{
    "PAPER'S NUMBER OF TABLES": 18,
    "S5.T1": {
        "caption": "TABLE I: Detailed information for datasets",
        "table": "<table id=\"S5.T1.st1.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T1.st1.3.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.4.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st1.3.4.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S5.T1.st1.3.4.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st1.3.4.2.1\" class=\"ltx_text ltx_font_bold\">#samples</span></td>\n<td id=\"S5.T1.st1.3.4.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st1.3.4.3.1\" class=\"ltx_text ltx_font_bold\">#features</span></td>\n<td id=\"S5.T1.st1.3.4.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st1.3.4.4.1\" class=\"ltx_text ltx_font_bold\">Task</span></td>\n</tr>\n<tr id=\"S5.T1.st1.3.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\">gisette</td>\n<td id=\"S5.T1.st1.3.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">6,000</td>\n<td id=\"S5.T1.st1.3.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">5,000</td>\n<td id=\"S5.T1.st1.3.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">binary-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.3.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.6.1\" class=\"ltx_td ltx_align_center\">covtype</td>\n<td id=\"S5.T1.st1.3.6.2\" class=\"ltx_td ltx_align_center\">581,012</td>\n<td id=\"S5.T1.st1.3.6.3\" class=\"ltx_td ltx_align_center\">54</td>\n<td id=\"S5.T1.st1.3.6.4\" class=\"ltx_td ltx_align_center\">binary-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.3.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.7.1\" class=\"ltx_td ltx_align_center\">phishing</td>\n<td id=\"S5.T1.st1.3.7.2\" class=\"ltx_td ltx_align_center\">11,055</td>\n<td id=\"S5.T1.st1.3.7.3\" class=\"ltx_td ltx_align_center\">68</td>\n<td id=\"S5.T1.st1.3.7.4\" class=\"ltx_td ltx_align_center\">binary-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.3.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.8.1\" class=\"ltx_td ltx_align_center\">UJIIndoorLoc</td>\n<td id=\"S5.T1.st1.3.8.2\" class=\"ltx_td ltx_align_center\">21,048</td>\n<td id=\"S5.T1.st1.3.8.3\" class=\"ltx_td ltx_align_center\">529</td>\n<td id=\"S5.T1.st1.3.8.4\" class=\"ltx_td ltx_align_center\">regression</td>\n</tr>\n<tr id=\"S5.T1.st1.3.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.9.1\" class=\"ltx_td ltx_align_center\">Superconduct</td>\n<td id=\"S5.T1.st1.3.9.2\" class=\"ltx_td ltx_align_center\">21,263</td>\n<td id=\"S5.T1.st1.3.9.3\" class=\"ltx_td ltx_align_center\">81</td>\n<td id=\"S5.T1.st1.3.9.4\" class=\"ltx_td ltx_align_center\">regression</td>\n</tr>\n<tr id=\"S5.T1.st1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.1.1.2\" class=\"ltx_td ltx_align_center\">MNIST</td>\n<td id=\"S5.T1.st1.1.1.3\" class=\"ltx_td ltx_align_center\">60,000</td>\n<td id=\"S5.T1.st1.1.1.1\" class=\"ltx_td ltx_align_center\">28<math id=\"S5.T1.st1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T1.st1.1.1.1.m1.1a\"><mo id=\"S5.T1.st1.1.1.1.m1.1.1\" xref=\"S5.T1.st1.1.1.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.st1.1.1.1.m1.1b\"><times id=\"S5.T1.st1.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.st1.1.1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.st1.1.1.1.m1.1c\">\\times</annotation></semantics></math>28</td>\n<td id=\"S5.T1.st1.1.1.4\" class=\"ltx_td ltx_align_center\">multi-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.2.2.2\" class=\"ltx_td ltx_align_center\">KMNIST</td>\n<td id=\"S5.T1.st1.2.2.3\" class=\"ltx_td ltx_align_center\">60,000</td>\n<td id=\"S5.T1.st1.2.2.1\" class=\"ltx_td ltx_align_center\">28<math id=\"S5.T1.st1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T1.st1.2.2.1.m1.1a\"><mo id=\"S5.T1.st1.2.2.1.m1.1.1\" xref=\"S5.T1.st1.2.2.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.st1.2.2.1.m1.1b\"><times id=\"S5.T1.st1.2.2.1.m1.1.1.cmml\" xref=\"S5.T1.st1.2.2.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.st1.2.2.1.m1.1c\">\\times</annotation></semantics></math>28</td>\n<td id=\"S5.T1.st1.2.2.4\" class=\"ltx_td ltx_align_center\">multi-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">Fashion-MNIST</td>\n<td id=\"S5.T1.st1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">60,000</td>\n<td id=\"S5.T1.st1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">28<math id=\"S5.T1.st1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T1.st1.3.3.1.m1.1a\"><mo id=\"S5.T1.st1.3.3.1.m1.1.1\" xref=\"S5.T1.st1.3.3.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.st1.3.3.1.m1.1b\"><times id=\"S5.T1.st1.3.3.1.m1.1.1.cmml\" xref=\"S5.T1.st1.3.3.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.st1.3.3.1.m1.1c\">\\times</annotation></semantics></math>28</td>\n<td id=\"S5.T1.st1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">multi-classification</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression."
        ]
    },
    "S5.T1.st1": {
        "caption": "(a) Public datasets",
        "table": "<table id=\"S5.T1.st1.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T1.st1.3.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.4.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st1.3.4.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S5.T1.st1.3.4.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st1.3.4.2.1\" class=\"ltx_text ltx_font_bold\">#samples</span></td>\n<td id=\"S5.T1.st1.3.4.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st1.3.4.3.1\" class=\"ltx_text ltx_font_bold\">#features</span></td>\n<td id=\"S5.T1.st1.3.4.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st1.3.4.4.1\" class=\"ltx_text ltx_font_bold\">Task</span></td>\n</tr>\n<tr id=\"S5.T1.st1.3.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\">gisette</td>\n<td id=\"S5.T1.st1.3.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">6,000</td>\n<td id=\"S5.T1.st1.3.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">5,000</td>\n<td id=\"S5.T1.st1.3.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">binary-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.3.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.6.1\" class=\"ltx_td ltx_align_center\">covtype</td>\n<td id=\"S5.T1.st1.3.6.2\" class=\"ltx_td ltx_align_center\">581,012</td>\n<td id=\"S5.T1.st1.3.6.3\" class=\"ltx_td ltx_align_center\">54</td>\n<td id=\"S5.T1.st1.3.6.4\" class=\"ltx_td ltx_align_center\">binary-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.3.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.7.1\" class=\"ltx_td ltx_align_center\">phishing</td>\n<td id=\"S5.T1.st1.3.7.2\" class=\"ltx_td ltx_align_center\">11,055</td>\n<td id=\"S5.T1.st1.3.7.3\" class=\"ltx_td ltx_align_center\">68</td>\n<td id=\"S5.T1.st1.3.7.4\" class=\"ltx_td ltx_align_center\">binary-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.3.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.8.1\" class=\"ltx_td ltx_align_center\">UJIIndoorLoc</td>\n<td id=\"S5.T1.st1.3.8.2\" class=\"ltx_td ltx_align_center\">21,048</td>\n<td id=\"S5.T1.st1.3.8.3\" class=\"ltx_td ltx_align_center\">529</td>\n<td id=\"S5.T1.st1.3.8.4\" class=\"ltx_td ltx_align_center\">regression</td>\n</tr>\n<tr id=\"S5.T1.st1.3.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.9.1\" class=\"ltx_td ltx_align_center\">Superconduct</td>\n<td id=\"S5.T1.st1.3.9.2\" class=\"ltx_td ltx_align_center\">21,263</td>\n<td id=\"S5.T1.st1.3.9.3\" class=\"ltx_td ltx_align_center\">81</td>\n<td id=\"S5.T1.st1.3.9.4\" class=\"ltx_td ltx_align_center\">regression</td>\n</tr>\n<tr id=\"S5.T1.st1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.1.1.2\" class=\"ltx_td ltx_align_center\">MNIST</td>\n<td id=\"S5.T1.st1.1.1.3\" class=\"ltx_td ltx_align_center\">60,000</td>\n<td id=\"S5.T1.st1.1.1.1\" class=\"ltx_td ltx_align_center\">28<math id=\"S5.T1.st1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T1.st1.1.1.1.m1.1a\"><mo id=\"S5.T1.st1.1.1.1.m1.1.1\" xref=\"S5.T1.st1.1.1.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.st1.1.1.1.m1.1b\"><times id=\"S5.T1.st1.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.st1.1.1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.st1.1.1.1.m1.1c\">\\times</annotation></semantics></math>28</td>\n<td id=\"S5.T1.st1.1.1.4\" class=\"ltx_td ltx_align_center\">multi-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.2.2.2\" class=\"ltx_td ltx_align_center\">KMNIST</td>\n<td id=\"S5.T1.st1.2.2.3\" class=\"ltx_td ltx_align_center\">60,000</td>\n<td id=\"S5.T1.st1.2.2.1\" class=\"ltx_td ltx_align_center\">28<math id=\"S5.T1.st1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T1.st1.2.2.1.m1.1a\"><mo id=\"S5.T1.st1.2.2.1.m1.1.1\" xref=\"S5.T1.st1.2.2.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.st1.2.2.1.m1.1b\"><times id=\"S5.T1.st1.2.2.1.m1.1.1.cmml\" xref=\"S5.T1.st1.2.2.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.st1.2.2.1.m1.1c\">\\times</annotation></semantics></math>28</td>\n<td id=\"S5.T1.st1.2.2.4\" class=\"ltx_td ltx_align_center\">multi-classification</td>\n</tr>\n<tr id=\"S5.T1.st1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">Fashion-MNIST</td>\n<td id=\"S5.T1.st1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">60,000</td>\n<td id=\"S5.T1.st1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">28<math id=\"S5.T1.st1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T1.st1.3.3.1.m1.1a\"><mo id=\"S5.T1.st1.3.3.1.m1.1.1\" xref=\"S5.T1.st1.3.3.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.st1.3.3.1.m1.1b\"><times id=\"S5.T1.st1.3.3.1.m1.1.1.cmml\" xref=\"S5.T1.st1.3.3.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.st1.3.3.1.m1.1c\">\\times</annotation></semantics></math>28</td>\n<td id=\"S5.T1.st1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">multi-classification</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "S5.T1.st2": {
        "caption": "(b) Real-world federated datasets",
        "table": "<table id=\"S5.T1.st2.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T1.st2.4.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st2.4.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S5.T1.st2.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st2.4.1.2.1\" class=\"ltx_text ltx_font_bold\">#samples</span></td>\n<td id=\"S5.T1.st2.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st2.4.1.3.1\" class=\"ltx_text ltx_font_bold\">#features</span></td>\n<td id=\"S5.T1.st2.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.st2.4.1.4.1\" class=\"ltx_text ltx_font_bold\">Task</span></td>\n</tr>\n<tr id=\"S5.T1.st2.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\">NUS-WIDE</td>\n<td id=\"S5.T1.st2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">269,648</td>\n<td id=\"S5.T1.st2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">66+144+75+128+225</td>\n<td id=\"S5.T1.st2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">bin-class</td>\n</tr>\n<tr id=\"S5.T1.st2.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">MovieLens</td>\n<td id=\"S5.T1.st2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">1,000,209</td>\n<td id=\"S5.T1.st2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">9992+133</td>\n<td id=\"S5.T1.st2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">reg</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: Models used in FedOnce-L0",
        "table": "<table id=\"S5.T2.17\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T2.17.18\" class=\"ltx_tr\">\n<td id=\"S5.T2.17.18.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T2.17.18.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S5.T2.17.18.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T2.17.18.2.1\" class=\"ltx_text ltx_font_bold\">Guest Model</span></td>\n<td id=\"S5.T2.17.18.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T2.17.18.3.1\" class=\"ltx_text ltx_font_bold\">Host Model</span></td>\n</tr>\n<tr id=\"S5.T2.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">gisette</td>\n<td id=\"S5.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">FC(100<math id=\"S5.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.1.1.1.m1.1a\"><mo id=\"S5.T2.1.1.1.m1.1.1\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.1b\"><times id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.1c\">\\times</annotation></semantics></math>100<math id=\"S5.T2.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.2.2.2.m2.1a\"><mo id=\"S5.T2.2.2.2.m2.1.1\" xref=\"S5.T2.2.2.2.m2.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.2.2.2.m2.1b\"><times id=\"S5.T2.2.2.2.m2.1.1.cmml\" xref=\"S5.T2.2.2.2.m2.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.2.2.2.m2.1c\">\\times</annotation></semantics></math>50)</td>\n<td id=\"S5.T2.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">FC(30<math id=\"S5.T2.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.3.3.3.m1.1a\"><mo id=\"S5.T2.3.3.3.m1.1.1\" xref=\"S5.T2.3.3.3.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.3.3.3.m1.1b\"><times id=\"S5.T2.3.3.3.m1.1.1.cmml\" xref=\"S5.T2.3.3.3.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.3.3.3.m1.1c\">\\times</annotation></semantics></math>30)</td>\n</tr>\n<tr id=\"S5.T2.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T2.6.6.4\" class=\"ltx_td ltx_align_center\">covtype</td>\n<td id=\"S5.T2.5.5.2\" class=\"ltx_td ltx_align_center\">FC(200<math id=\"S5.T2.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.4.4.1.m1.1a\"><mo id=\"S5.T2.4.4.1.m1.1.1\" xref=\"S5.T2.4.4.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.4.4.1.m1.1b\"><times id=\"S5.T2.4.4.1.m1.1.1.cmml\" xref=\"S5.T2.4.4.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.4.4.1.m1.1c\">\\times</annotation></semantics></math>100<math id=\"S5.T2.5.5.2.m2.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.5.5.2.m2.1a\"><mo id=\"S5.T2.5.5.2.m2.1.1\" xref=\"S5.T2.5.5.2.m2.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.5.5.2.m2.1b\"><times id=\"S5.T2.5.5.2.m2.1.1.cmml\" xref=\"S5.T2.5.5.2.m2.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.5.5.2.m2.1c\">\\times</annotation></semantics></math>100)</td>\n<td id=\"S5.T2.6.6.3\" class=\"ltx_td ltx_align_center\">FC(30<math id=\"S5.T2.6.6.3.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.6.6.3.m1.1a\"><mo id=\"S5.T2.6.6.3.m1.1.1\" xref=\"S5.T2.6.6.3.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.6.6.3.m1.1b\"><times id=\"S5.T2.6.6.3.m1.1.1.cmml\" xref=\"S5.T2.6.6.3.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.6.6.3.m1.1c\">\\times</annotation></semantics></math>30)</td>\n</tr>\n<tr id=\"S5.T2.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T2.7.7.2\" class=\"ltx_td ltx_align_center\">phishing</td>\n<td id=\"S5.T2.7.7.1\" class=\"ltx_td ltx_align_center\">FC(30<math id=\"S5.T2.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.7.7.1.m1.1a\"><mo id=\"S5.T2.7.7.1.m1.1.1\" xref=\"S5.T2.7.7.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.7.7.1.m1.1b\"><times id=\"S5.T2.7.7.1.m1.1.1.cmml\" xref=\"S5.T2.7.7.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.7.7.1.m1.1c\">\\times</annotation></semantics></math>30)</td>\n<td id=\"S5.T2.7.7.3\" class=\"ltx_td ltx_align_center\">FC(30)</td>\n</tr>\n<tr id=\"S5.T2.11.11\" class=\"ltx_tr\">\n<td id=\"S5.T2.11.11.5\" class=\"ltx_td ltx_align_center\">UJIIndoorLoc</td>\n<td id=\"S5.T2.10.10.3\" class=\"ltx_td ltx_align_center\">FC(50<math id=\"S5.T2.8.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.8.8.1.m1.1a\"><mo id=\"S5.T2.8.8.1.m1.1.1\" xref=\"S5.T2.8.8.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.8.8.1.m1.1b\"><times id=\"S5.T2.8.8.1.m1.1.1.cmml\" xref=\"S5.T2.8.8.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.8.8.1.m1.1c\">\\times</annotation></semantics></math>50<math id=\"S5.T2.9.9.2.m2.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.9.9.2.m2.1a\"><mo id=\"S5.T2.9.9.2.m2.1.1\" xref=\"S5.T2.9.9.2.m2.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.9.9.2.m2.1b\"><times id=\"S5.T2.9.9.2.m2.1.1.cmml\" xref=\"S5.T2.9.9.2.m2.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.9.9.2.m2.1c\">\\times</annotation></semantics></math>50<math id=\"S5.T2.10.10.3.m3.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.10.10.3.m3.1a\"><mo id=\"S5.T2.10.10.3.m3.1.1\" xref=\"S5.T2.10.10.3.m3.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.10.10.3.m3.1b\"><times id=\"S5.T2.10.10.3.m3.1.1.cmml\" xref=\"S5.T2.10.10.3.m3.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.10.10.3.m3.1c\">\\times</annotation></semantics></math>30)</td>\n<td id=\"S5.T2.11.11.4\" class=\"ltx_td ltx_align_center\">FC(30<math id=\"S5.T2.11.11.4.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.11.11.4.m1.1a\"><mo id=\"S5.T2.11.11.4.m1.1.1\" xref=\"S5.T2.11.11.4.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.11.11.4.m1.1b\"><times id=\"S5.T2.11.11.4.m1.1.1.cmml\" xref=\"S5.T2.11.11.4.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.11.11.4.m1.1c\">\\times</annotation></semantics></math>10)</td>\n</tr>\n<tr id=\"S5.T2.13.13\" class=\"ltx_tr\">\n<td id=\"S5.T2.13.13.3\" class=\"ltx_td ltx_align_center\">Superconduct</td>\n<td id=\"S5.T2.12.12.1\" class=\"ltx_td ltx_align_center\">FC(30<math id=\"S5.T2.12.12.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.12.12.1.m1.1a\"><mo id=\"S5.T2.12.12.1.m1.1.1\" xref=\"S5.T2.12.12.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.12.12.1.m1.1b\"><times id=\"S5.T2.12.12.1.m1.1.1.cmml\" xref=\"S5.T2.12.12.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.12.12.1.m1.1c\">\\times</annotation></semantics></math>30)</td>\n<td id=\"S5.T2.13.13.2\" class=\"ltx_td ltx_align_center\">FC(30<math id=\"S5.T2.13.13.2.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.13.13.2.m1.1a\"><mo id=\"S5.T2.13.13.2.m1.1.1\" xref=\"S5.T2.13.13.2.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.13.13.2.m1.1b\"><times id=\"S5.T2.13.13.2.m1.1.1.cmml\" xref=\"S5.T2.13.13.2.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.13.13.2.m1.1c\">\\times</annotation></semantics></math>10)</td>\n</tr>\n<tr id=\"S5.T2.14.14\" class=\"ltx_tr\">\n<td id=\"S5.T2.14.14.2\" class=\"ltx_td ltx_align_center\">MNIST</td>\n<td id=\"S5.T2.14.14.1\" class=\"ltx_td ltx_align_center\">CNN0<math id=\"S5.T2.14.14.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T2.14.14.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T2.14.14.1.m1.1.1\" xref=\"S5.T2.14.14.1.m1.1.1.cmml\">→</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.14.14.1.m1.1b\"><ci id=\"S5.T2.14.14.1.m1.1.1.cmml\" xref=\"S5.T2.14.14.1.m1.1.1\">→</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.14.14.1.m1.1c\">\\rightarrow</annotation></semantics></math>FC(128)</td>\n<td id=\"S5.T2.14.14.3\" class=\"ltx_td ltx_align_center\">FC(128)</td>\n</tr>\n<tr id=\"S5.T2.15.15\" class=\"ltx_tr\">\n<td id=\"S5.T2.15.15.2\" class=\"ltx_td ltx_align_center\">KMNIST</td>\n<td id=\"S5.T2.15.15.1\" class=\"ltx_td ltx_align_center\">CNN0<math id=\"S5.T2.15.15.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T2.15.15.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T2.15.15.1.m1.1.1\" xref=\"S5.T2.15.15.1.m1.1.1.cmml\">→</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.15.15.1.m1.1b\"><ci id=\"S5.T2.15.15.1.m1.1.1.cmml\" xref=\"S5.T2.15.15.1.m1.1.1\">→</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.15.15.1.m1.1c\">\\rightarrow</annotation></semantics></math>FC(128)</td>\n<td id=\"S5.T2.15.15.3\" class=\"ltx_td ltx_align_center\">FC(128)</td>\n</tr>\n<tr id=\"S5.T2.16.16\" class=\"ltx_tr\">\n<td id=\"S5.T2.16.16.2\" class=\"ltx_td ltx_align_center\">Fashion-MNIST</td>\n<td id=\"S5.T2.16.16.1\" class=\"ltx_td ltx_align_center\">CNN0<math id=\"S5.T2.16.16.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rightarrow\" display=\"inline\"><semantics id=\"S5.T2.16.16.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T2.16.16.1.m1.1.1\" xref=\"S5.T2.16.16.1.m1.1.1.cmml\">→</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.16.16.1.m1.1b\"><ci id=\"S5.T2.16.16.1.m1.1.1.cmml\" xref=\"S5.T2.16.16.1.m1.1.1\">→</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.16.16.1.m1.1c\">\\rightarrow</annotation></semantics></math>FC(128)</td>\n<td id=\"S5.T2.16.16.3\" class=\"ltx_td ltx_align_center\">FC(128)</td>\n</tr>\n<tr id=\"S5.T2.17.19\" class=\"ltx_tr\">\n<td id=\"S5.T2.17.19.1\" class=\"ltx_td ltx_align_center\">NUS-WIDE</td>\n<td id=\"S5.T2.17.19.2\" class=\"ltx_td ltx_align_center\">FC(60)</td>\n<td id=\"S5.T2.17.19.3\" class=\"ltx_td ltx_align_center\">FC(50)</td>\n</tr>\n<tr id=\"S5.T2.17.17\" class=\"ltx_tr\">\n<td id=\"S5.T2.17.17.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">MovieLens</td>\n<td id=\"S5.T2.17.17.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">NCF(32<math id=\"S5.T2.17.17.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T2.17.17.1.m1.1a\"><mo id=\"S5.T2.17.17.1.m1.1.1\" xref=\"S5.T2.17.17.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.17.17.1.m1.1b\"><times id=\"S5.T2.17.17.1.m1.1.1.cmml\" xref=\"S5.T2.17.17.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.17.17.1.m1.1c\">\\times</annotation></semantics></math>16)/NCF(128)</td>\n<td id=\"S5.T2.17.17.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">FC(10)</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively."
        ]
    },
    "S5.T3": {
        "caption": "TABLE III: Models used in FedOnce-L1",
        "table": "<table id=\"S5.T3.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T3.4.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.4.5.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T3.4.5.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S5.T3.4.5.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T3.4.5.2.1\" class=\"ltx_text ltx_font_bold\">Guest Model</span></td>\n<td id=\"S5.T3.4.5.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T3.4.5.3.1\" class=\"ltx_text ltx_font_bold\">Host Model</span></td>\n</tr>\n<tr id=\"S5.T3.4.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.4.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\">gisette</td>\n<td id=\"S5.T3.4.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">FC(30)</td>\n<td id=\"S5.T3.4.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">FC(10)</td>\n</tr>\n<tr id=\"S5.T3.4.7\" class=\"ltx_tr\">\n<td id=\"S5.T3.4.7.1\" class=\"ltx_td ltx_align_center\">covtype</td>\n<td id=\"S5.T3.4.7.2\" class=\"ltx_td ltx_align_center\">FC(50)</td>\n<td id=\"S5.T3.4.7.3\" class=\"ltx_td ltx_align_center\">FC(30)</td>\n</tr>\n<tr id=\"S5.T3.4.8\" class=\"ltx_tr\">\n<td id=\"S5.T3.4.8.1\" class=\"ltx_td ltx_align_center\">phishing</td>\n<td id=\"S5.T3.4.8.2\" class=\"ltx_td ltx_align_center\">FC(30)</td>\n<td id=\"S5.T3.4.8.3\" class=\"ltx_td ltx_align_center\">FC(10)</td>\n</tr>\n<tr id=\"S5.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.2.3\" class=\"ltx_td ltx_align_center\">UJIIndoorLoc</td>\n<td id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_center\">FC(50<math id=\"S5.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T3.1.1.1.m1.1a\"><mo id=\"S5.T3.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.m1.1b\"><times id=\"S5.T3.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.m1.1c\">\\times</annotation></semantics></math>50)</td>\n<td id=\"S5.T3.2.2.2\" class=\"ltx_td ltx_align_center\">FC(20<math id=\"S5.T3.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T3.2.2.2.m1.1a\"><mo id=\"S5.T3.2.2.2.m1.1.1\" xref=\"S5.T3.2.2.2.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.2.2.2.m1.1b\"><times id=\"S5.T3.2.2.2.m1.1.1.cmml\" xref=\"S5.T3.2.2.2.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.2.2.2.m1.1c\">\\times</annotation></semantics></math>20)</td>\n</tr>\n<tr id=\"S5.T3.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.4.4.3\" class=\"ltx_td ltx_align_center\">Superconduct</td>\n<td id=\"S5.T3.3.3.1\" class=\"ltx_td ltx_align_center\">FC(50<math id=\"S5.T3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T3.3.3.1.m1.1a\"><mo id=\"S5.T3.3.3.1.m1.1.1\" xref=\"S5.T3.3.3.1.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.3.3.1.m1.1b\"><times id=\"S5.T3.3.3.1.m1.1.1.cmml\" xref=\"S5.T3.3.3.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.3.3.1.m1.1c\">\\times</annotation></semantics></math>50)</td>\n<td id=\"S5.T3.4.4.2\" class=\"ltx_td ltx_align_center\">FC(20<math id=\"S5.T3.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S5.T3.4.4.2.m1.1a\"><mo id=\"S5.T3.4.4.2.m1.1.1\" xref=\"S5.T3.4.4.2.m1.1.1.cmml\">×</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.4.4.2.m1.1b\"><times id=\"S5.T3.4.4.2.m1.1.1.cmml\" xref=\"S5.T3.4.4.2.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.4.4.2.m1.1c\">\\times</annotation></semantics></math>20)</td>\n</tr>\n<tr id=\"S5.T3.4.9\" class=\"ltx_tr\">\n<td id=\"S5.T3.4.9.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">MNIST</td>\n<td id=\"S5.T3.4.9.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">CNN1</td>\n<td id=\"S5.T3.4.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">FC(10)</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively."
        ]
    },
    "S5.T4": {
        "caption": "TABLE IV: Training time of FedOnce-L0 and SplitNN",
        "table": "<table id=\"S5.T4.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T4.4.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T4.4.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S5.T4.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S5.T4.4.1.2.1\" class=\"ltx_text ltx_font_bold\">Training Time(min)</span></td>\n</tr>\n<tr id=\"S5.T4.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T4.4.2.1.1\" class=\"ltx_text ltx_font_bold\">SplitNN</span></td>\n<td id=\"S5.T4.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T4.4.2.2.1\" class=\"ltx_text ltx_font_bold\">FedOnce-L0</span></td>\n</tr>\n<tr id=\"S5.T4.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">gisette</td>\n<td id=\"S5.T4.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.62</td>\n<td id=\"S5.T4.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.60</td>\n</tr>\n<tr id=\"S5.T4.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.4.1\" class=\"ltx_td ltx_align_center\">covtype</td>\n<td id=\"S5.T4.4.4.2\" class=\"ltx_td ltx_align_center\">171.12</td>\n<td id=\"S5.T4.4.4.3\" class=\"ltx_td ltx_align_center\">52.33</td>\n</tr>\n<tr id=\"S5.T4.4.5\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.5.1\" class=\"ltx_td ltx_align_center\">phishing</td>\n<td id=\"S5.T4.4.5.2\" class=\"ltx_td ltx_align_center\">4.07</td>\n<td id=\"S5.T4.4.5.3\" class=\"ltx_td ltx_align_center\">1.63</td>\n</tr>\n<tr id=\"S5.T4.4.6\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.6.1\" class=\"ltx_td ltx_align_center\">UJIIndoorLoc</td>\n<td id=\"S5.T4.4.6.2\" class=\"ltx_td ltx_align_center\">23.15</td>\n<td id=\"S5.T4.4.6.3\" class=\"ltx_td ltx_align_center\">8.96</td>\n</tr>\n<tr id=\"S5.T4.4.7\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.7.1\" class=\"ltx_td ltx_align_center\">Superconduct</td>\n<td id=\"S5.T4.4.7.2\" class=\"ltx_td ltx_align_center\">16.55</td>\n<td id=\"S5.T4.4.7.3\" class=\"ltx_td ltx_align_center\">7.54</td>\n</tr>\n<tr id=\"S5.T4.4.8\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.8.1\" class=\"ltx_td ltx_align_center\">MNIST</td>\n<td id=\"S5.T4.4.8.2\" class=\"ltx_td ltx_align_center\">53.58</td>\n<td id=\"S5.T4.4.8.3\" class=\"ltx_td ltx_align_center\">24.17</td>\n</tr>\n<tr id=\"S5.T4.4.9\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.9.1\" class=\"ltx_td ltx_align_center\">KMNIST</td>\n<td id=\"S5.T4.4.9.2\" class=\"ltx_td ltx_align_center\">56.47</td>\n<td id=\"S5.T4.4.9.3\" class=\"ltx_td ltx_align_center\">24.13</td>\n</tr>\n<tr id=\"S5.T4.4.10\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.10.1\" class=\"ltx_td ltx_align_center\">Fashion-MNIST</td>\n<td id=\"S5.T4.4.10.2\" class=\"ltx_td ltx_align_center\">54.58</td>\n<td id=\"S5.T4.4.10.3\" class=\"ltx_td ltx_align_center\">23.00</td>\n</tr>\n<tr id=\"S5.T4.4.11\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.11.1\" class=\"ltx_td ltx_align_center\">NUS-WIDE</td>\n<td id=\"S5.T4.4.11.2\" class=\"ltx_td ltx_align_center\">8.86</td>\n<td id=\"S5.T4.4.11.3\" class=\"ltx_td ltx_align_center\">5.17</td>\n</tr>\n<tr id=\"S5.T4.4.12\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.12.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">MovieLens</td>\n<td id=\"S5.T4.4.12.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">39.88</td>\n<td id=\"S5.T4.4.12.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">34.91</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5."
        ]
    },
    "S5.T5": {
        "caption": "",
        "table": "<table id=\"S5.T5.10.10\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T5.2.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T5.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T5.2.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S5.T5.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T5.2.2.2.4.1\" class=\"ltx_text\"><span id=\"S5.T5.2.2.2.4.1.1\" class=\"ltx_text\"></span> <span id=\"S5.T5.2.2.2.4.1.2\" class=\"ltx_text\">\n<span id=\"S5.T5.2.2.2.4.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S5.T5.2.2.2.4.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"S5.T5.2.2.2.4.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T5.2.2.2.4.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Host</span></span></span>\n<span id=\"S5.T5.2.2.2.4.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"S5.T5.2.2.2.4.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T5.2.2.2.4.1.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Party<sup id=\"S5.T5.2.2.2.4.1.2.1.2.1.1.1\" class=\"ltx_sup\"><span id=\"S5.T5.2.2.2.4.1.2.1.2.1.1.1.1\" class=\"ltx_text ltx_font_medium\">1</span></sup></span></span></span>\n</span></span> <span id=\"S5.T5.2.2.2.4.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"S5.T5.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S5.T5.2.2.2.5.1\" class=\"ltx_text ltx_font_bold\">Guest Model</span></td>\n<td id=\"S5.T5.2.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S5.T5.2.2.2.6.1\" class=\"ltx_text ltx_font_bold\">Host Model</span></td>\n<td id=\"S5.T5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T5.1.1.1.1.1\" class=\"ltx_text\"><math id=\"S5.T5.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics id=\"S5.T5.1.1.1.1.1.m1.1a\"><mi id=\"S5.T5.1.1.1.1.1.m1.1.1\" xref=\"S5.T5.1.1.1.1.1.m1.1.1.cmml\">d</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.1.1.1.1.1.m1.1b\"><ci id=\"S5.T5.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T5.1.1.1.1.1.m1.1.1\">𝑑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.1.1.1.1.1.m1.1c\">d</annotation></semantics></math></span></td>\n<td id=\"S5.T5.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T5.2.2.2.2.1\" class=\"ltx_text\"><math id=\"S5.T5.2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"f\" display=\"inline\"><semantics id=\"S5.T5.2.2.2.2.1.m1.1a\"><mi id=\"S5.T5.2.2.2.2.1.m1.1.1\" xref=\"S5.T5.2.2.2.2.1.m1.1.1.cmml\">f</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.2.2.2.2.1.m1.1b\"><ci id=\"S5.T5.2.2.2.2.1.m1.1.1.cmml\" xref=\"S5.T5.2.2.2.2.1.m1.1.1\">𝑓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.2.2.2.2.1.m1.1c\">f</annotation></semantics></math></span></td>\n<td id=\"S5.T5.2.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T5.2.2.2.7.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></td>\n</tr>\n<tr id=\"S5.T5.10.10.10\" class=\"ltx_tr\">\n<td id=\"S5.T5.3.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S5.T5.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\eta\" display=\"inline\"><semantics id=\"S5.T5.3.3.3.1.m1.1a\"><mi id=\"S5.T5.3.3.3.1.m1.1.1\" xref=\"S5.T5.3.3.3.1.m1.1.1.cmml\">η</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.3.3.3.1.m1.1b\"><ci id=\"S5.T5.3.3.3.1.m1.1.1.cmml\" xref=\"S5.T5.3.3.3.1.m1.1.1\">𝜂</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.3.3.3.1.m1.1c\">\\eta</annotation></semantics></math></td>\n<td id=\"S5.T5.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S5.T5.4.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"S5.T5.4.4.4.2.m1.1a\"><mi id=\"S5.T5.4.4.4.2.m1.1.1\" xref=\"S5.T5.4.4.4.2.m1.1.1.cmml\">λ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.4.4.4.2.m1.1b\"><ci id=\"S5.T5.4.4.4.2.m1.1.1.cmml\" xref=\"S5.T5.4.4.4.2.m1.1.1\">𝜆</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.4.4.4.2.m1.1c\">\\lambda</annotation></semantics></math></td>\n<td id=\"S5.T5.5.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S5.T5.5.5.5.3.m1.1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics id=\"S5.T5.5.5.5.3.m1.1a\"><mi id=\"S5.T5.5.5.5.3.m1.1.1\" xref=\"S5.T5.5.5.5.3.m1.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.5.5.5.3.m1.1b\"><ci id=\"S5.T5.5.5.5.3.m1.1.1.cmml\" xref=\"S5.T5.5.5.5.3.m1.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.5.5.5.3.m1.1c\">b</annotation></semantics></math></td>\n<td id=\"S5.T5.6.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S5.T5.6.6.6.4.m1.1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics id=\"S5.T5.6.6.6.4.m1.1a\"><mi id=\"S5.T5.6.6.6.4.m1.1.1\" xref=\"S5.T5.6.6.6.4.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.6.6.6.4.m1.1b\"><ci id=\"S5.T5.6.6.6.4.m1.1.1.cmml\" xref=\"S5.T5.6.6.6.4.m1.1.1\">𝑇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.6.6.6.4.m1.1c\">T</annotation></semantics></math></td>\n<td id=\"S5.T5.7.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S5.T5.7.7.7.5.m1.1\" class=\"ltx_Math\" alttext=\"\\eta\" display=\"inline\"><semantics id=\"S5.T5.7.7.7.5.m1.1a\"><mi id=\"S5.T5.7.7.7.5.m1.1.1\" xref=\"S5.T5.7.7.7.5.m1.1.1.cmml\">η</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.7.7.7.5.m1.1b\"><ci id=\"S5.T5.7.7.7.5.m1.1.1.cmml\" xref=\"S5.T5.7.7.7.5.m1.1.1\">𝜂</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.7.7.7.5.m1.1c\">\\eta</annotation></semantics></math></td>\n<td id=\"S5.T5.8.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S5.T5.8.8.8.6.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"S5.T5.8.8.8.6.m1.1a\"><mi id=\"S5.T5.8.8.8.6.m1.1.1\" xref=\"S5.T5.8.8.8.6.m1.1.1.cmml\">λ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.8.8.8.6.m1.1b\"><ci id=\"S5.T5.8.8.8.6.m1.1.1.cmml\" xref=\"S5.T5.8.8.8.6.m1.1.1\">𝜆</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.8.8.8.6.m1.1c\">\\lambda</annotation></semantics></math></td>\n<td id=\"S5.T5.9.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S5.T5.9.9.9.7.m1.1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics id=\"S5.T5.9.9.9.7.m1.1a\"><mi id=\"S5.T5.9.9.9.7.m1.1.1\" xref=\"S5.T5.9.9.9.7.m1.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.9.9.9.7.m1.1b\"><ci id=\"S5.T5.9.9.9.7.m1.1.1.cmml\" xref=\"S5.T5.9.9.9.7.m1.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.9.9.9.7.m1.1c\">b</annotation></semantics></math></td>\n<td id=\"S5.T5.10.10.10.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S5.T5.10.10.10.8.m1.1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics id=\"S5.T5.10.10.10.8.m1.1a\"><mi id=\"S5.T5.10.10.10.8.m1.1.1\" xref=\"S5.T5.10.10.10.8.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.10.10.10.8.m1.1b\"><ci id=\"S5.T5.10.10.10.8.m1.1.1.cmml\" xref=\"S5.T5.10.10.10.8.m1.1.1\">𝑇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.10.10.10.8.m1.1c\">T</annotation></semantics></math></td>\n</tr>\n<tr id=\"S5.T5.10.10.11\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.11.1\" class=\"ltx_td ltx_align_center ltx_border_t\">gisette</td>\n<td id=\"S5.T5.10.10.11.2\" class=\"ltx_td ltx_align_center ltx_border_t\">*</td>\n<td id=\"S5.T5.10.10.11.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1e-4</td>\n<td id=\"S5.T5.10.10.11.4\" class=\"ltx_td ltx_align_center ltx_border_t\">1e-5</td>\n<td id=\"S5.T5.10.10.11.5\" class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td id=\"S5.T5.10.10.11.6\" class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td id=\"S5.T5.10.10.11.7\" class=\"ltx_td ltx_align_center ltx_border_t\">1e-4</td>\n<td id=\"S5.T5.10.10.11.8\" class=\"ltx_td ltx_align_center ltx_border_t\">1e-4</td>\n<td id=\"S5.T5.10.10.11.9\" class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td id=\"S5.T5.10.10.11.10\" class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td id=\"S5.T5.10.10.11.11\" class=\"ltx_td ltx_align_center ltx_border_t\">3</td>\n<td id=\"S5.T5.10.10.11.12\" class=\"ltx_td ltx_align_center ltx_border_t\">1</td>\n<td id=\"S5.T5.10.10.11.13\" class=\"ltx_td ltx_align_center ltx_border_t\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.12\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.12.1\" class=\"ltx_td ltx_align_center\">covtype</td>\n<td id=\"S5.T5.10.10.12.2\" class=\"ltx_td ltx_align_center\">*</td>\n<td id=\"S5.T5.10.10.12.3\" class=\"ltx_td ltx_align_center\">1e-3</td>\n<td id=\"S5.T5.10.10.12.4\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.12.5\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.12.6\" class=\"ltx_td ltx_align_center\">30</td>\n<td id=\"S5.T5.10.10.12.7\" class=\"ltx_td ltx_align_center\">1e-3</td>\n<td id=\"S5.T5.10.10.12.8\" class=\"ltx_td ltx_align_center\">1e-3</td>\n<td id=\"S5.T5.10.10.12.9\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.12.10\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.12.11\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S5.T5.10.10.12.12\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"S5.T5.10.10.12.13\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.13\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.13.1\" class=\"ltx_td ltx_align_center\">phishing</td>\n<td id=\"S5.T5.10.10.13.2\" class=\"ltx_td ltx_align_center\">*</td>\n<td id=\"S5.T5.10.10.13.3\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.13.4\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.13.5\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.13.6\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.13.7\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.13.8\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.13.9\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.13.10\" class=\"ltx_td ltx_align_center\">300</td>\n<td id=\"S5.T5.10.10.13.11\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S5.T5.10.10.13.12\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"S5.T5.10.10.13.13\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.14\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.14.1\" class=\"ltx_td ltx_align_center\">UJIIndoorLoc</td>\n<td id=\"S5.T5.10.10.14.2\" class=\"ltx_td ltx_align_center\">*</td>\n<td id=\"S5.T5.10.10.14.3\" class=\"ltx_td ltx_align_center\">3e-4</td>\n<td id=\"S5.T5.10.10.14.4\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.14.5\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.14.6\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"S5.T5.10.10.14.7\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.14.8\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.14.9\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.14.10\" class=\"ltx_td ltx_align_center\">600</td>\n<td id=\"S5.T5.10.10.14.11\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S5.T5.10.10.14.12\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"S5.T5.10.10.14.13\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.15\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.15.1\" class=\"ltx_td ltx_align_center\">Superconduct</td>\n<td id=\"S5.T5.10.10.15.2\" class=\"ltx_td ltx_align_center\">*</td>\n<td id=\"S5.T5.10.10.15.3\" class=\"ltx_td ltx_align_center\">3e-4</td>\n<td id=\"S5.T5.10.10.15.4\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.15.5\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.15.6\" class=\"ltx_td ltx_align_center\">500</td>\n<td id=\"S5.T5.10.10.15.7\" class=\"ltx_td ltx_align_center\">3e-4</td>\n<td id=\"S5.T5.10.10.15.8\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.15.9\" class=\"ltx_td ltx_align_center\">128</td>\n<td id=\"S5.T5.10.10.15.10\" class=\"ltx_td ltx_align_center\">600</td>\n<td id=\"S5.T5.10.10.15.11\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S5.T5.10.10.15.12\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"S5.T5.10.10.15.13\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.16\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.16.1\" class=\"ltx_td ltx_align_center\">MNIST</td>\n<td id=\"S5.T5.10.10.16.2\" class=\"ltx_td ltx_align_center\">*</td>\n<td id=\"S5.T5.10.10.16.3\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.16.4\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.16.5\" class=\"ltx_td ltx_align_center\">128</td>\n<td id=\"S5.T5.10.10.16.6\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.16.7\" class=\"ltx_td ltx_align_center\">1e-3</td>\n<td id=\"S5.T5.10.10.16.8\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.16.9\" class=\"ltx_td ltx_align_center\">128</td>\n<td id=\"S5.T5.10.10.16.10\" class=\"ltx_td ltx_align_center\">200</td>\n<td id=\"S5.T5.10.10.16.11\" class=\"ltx_td ltx_align_center\">16</td>\n<td id=\"S5.T5.10.10.16.12\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S5.T5.10.10.16.13\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.17\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.17.1\" class=\"ltx_td ltx_align_center\">KMNIST</td>\n<td id=\"S5.T5.10.10.17.2\" class=\"ltx_td ltx_align_center\">*</td>\n<td id=\"S5.T5.10.10.17.3\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.17.4\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.17.5\" class=\"ltx_td ltx_align_center\">128</td>\n<td id=\"S5.T5.10.10.17.6\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.17.7\" class=\"ltx_td ltx_align_center\">1e-3</td>\n<td id=\"S5.T5.10.10.17.8\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.17.9\" class=\"ltx_td ltx_align_center\">128</td>\n<td id=\"S5.T5.10.10.17.10\" class=\"ltx_td ltx_align_center\">200</td>\n<td id=\"S5.T5.10.10.17.11\" class=\"ltx_td ltx_align_center\">16</td>\n<td id=\"S5.T5.10.10.17.12\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S5.T5.10.10.17.13\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.18\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.18.1\" class=\"ltx_td ltx_align_center\">Fashion-MNIST</td>\n<td id=\"S5.T5.10.10.18.2\" class=\"ltx_td ltx_align_center\">*</td>\n<td id=\"S5.T5.10.10.18.3\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.18.4\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.18.5\" class=\"ltx_td ltx_align_center\">128</td>\n<td id=\"S5.T5.10.10.18.6\" class=\"ltx_td ltx_align_center\">100</td>\n<td id=\"S5.T5.10.10.18.7\" class=\"ltx_td ltx_align_center\">1e-3</td>\n<td id=\"S5.T5.10.10.18.8\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.18.9\" class=\"ltx_td ltx_align_center\">128</td>\n<td id=\"S5.T5.10.10.18.10\" class=\"ltx_td ltx_align_center\">200</td>\n<td id=\"S5.T5.10.10.18.11\" class=\"ltx_td ltx_align_center\">16</td>\n<td id=\"S5.T5.10.10.18.12\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S5.T5.10.10.18.13\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.19\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.19.1\" class=\"ltx_td ltx_align_center\">NUS-WIDE</td>\n<td id=\"S5.T5.10.10.19.2\" class=\"ltx_td ltx_align_center\">*</td>\n<td id=\"S5.T5.10.10.19.3\" class=\"ltx_td ltx_align_center\">3e-4</td>\n<td id=\"S5.T5.10.10.19.4\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.19.5\" class=\"ltx_td ltx_align_center\">128</td>\n<td id=\"S5.T5.10.10.19.6\" class=\"ltx_td ltx_align_center\">50</td>\n<td id=\"S5.T5.10.10.19.7\" class=\"ltx_td ltx_align_center\">7e-5</td>\n<td id=\"S5.T5.10.10.19.8\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.19.9\" class=\"ltx_td ltx_align_center\">128</td>\n<td id=\"S5.T5.10.10.19.10\" class=\"ltx_td ltx_align_center\">50</td>\n<td id=\"S5.T5.10.10.19.11\" class=\"ltx_td ltx_align_center\">8</td>\n<td id=\"S5.T5.10.10.19.12\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"S5.T5.10.10.19.13\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.20\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.20.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" rowspan=\"2\"><span id=\"S5.T5.10.10.20.1.1\" class=\"ltx_text\">MovieLens</span></td>\n<td id=\"S5.T5.10.10.20.2\" class=\"ltx_td ltx_align_center\">0</td>\n<td id=\"S5.T5.10.10.20.3\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.20.4\" class=\"ltx_td ltx_align_center\">1e-5</td>\n<td id=\"S5.T5.10.10.20.5\" class=\"ltx_td ltx_align_center\">64</td>\n<td id=\"S5.T5.10.10.20.6\" class=\"ltx_td ltx_align_center\">30</td>\n<td id=\"S5.T5.10.10.20.7\" class=\"ltx_td ltx_align_center\">1e-4</td>\n<td id=\"S5.T5.10.10.20.8\" class=\"ltx_td ltx_align_center\">2e-4</td>\n<td id=\"S5.T5.10.10.20.9\" class=\"ltx_td ltx_align_center\">64</td>\n<td id=\"S5.T5.10.10.20.10\" class=\"ltx_td ltx_align_center\">40</td>\n<td id=\"S5.T5.10.10.20.11\" class=\"ltx_td ltx_align_center\">3</td>\n<td id=\"S5.T5.10.10.20.12\" class=\"ltx_td ltx_align_center\">1</td>\n<td id=\"S5.T5.10.10.20.13\" class=\"ltx_td ltx_align_center\">Adam</td>\n</tr>\n<tr id=\"S5.T5.10.10.21\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.21.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">1</td>\n<td id=\"S5.T5.10.10.21.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">1e-4</td>\n<td id=\"S5.T5.10.10.21.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">1e-5</td>\n<td id=\"S5.T5.10.10.21.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">64</td>\n<td id=\"S5.T5.10.10.21.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">30</td>\n<td id=\"S5.T5.10.10.21.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">5e-4</td>\n<td id=\"S5.T5.10.10.21.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">1e-5</td>\n<td id=\"S5.T5.10.10.21.8\" class=\"ltx_td ltx_align_center ltx_border_bb\">64</td>\n<td id=\"S5.T5.10.10.21.9\" class=\"ltx_td ltx_align_center ltx_border_bb\">100</td>\n<td id=\"S5.T5.10.10.21.10\" class=\"ltx_td ltx_align_center ltx_border_bb\">48</td>\n<td id=\"S5.T5.10.10.21.11\" class=\"ltx_td ltx_align_center ltx_border_bb\">1</td>\n<td id=\"S5.T5.10.10.21.12\" class=\"ltx_td ltx_align_center ltx_border_bb\">Adam</td>\n</tr>\n</table>\n\n",
        "footnotes": "1The party that is set as host party. j𝑗j: the hyperparameters when party j𝑗j is set as host party. * means the hyperparameters remain the same when each party is set as host party.",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "S5.T6": {
        "caption": "TABLE VI: Performance of FedOnce-L1 with different ε𝜀\\varepsilon",
        "table": "<table id=\"S5.T6.st1.13\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T6.st1.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st1.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S5.T6.st1.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st1.3.1.1.1\" class=\"ltx_text\"><math id=\"S5.T6.st1.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\varepsilon\" display=\"inline\"><semantics id=\"S5.T6.st1.3.1.1.1.m1.1a\"><mi id=\"S5.T6.st1.3.1.1.1.m1.1.1\" xref=\"S5.T6.st1.3.1.1.1.m1.1.1.cmml\">ε</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.3.1.1.1.m1.1b\"><ci id=\"S5.T6.st1.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T6.st1.3.1.1.1.m1.1.1\">𝜀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.3.1.1.1.m1.1c\">\\varepsilon</annotation></semantics></math></span></td>\n<td id=\"S5.T6.st1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" colspan=\"4\"><span id=\"S5.T6.st1.3.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></td>\n<td id=\"S5.T6.st1.3.1.4\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"></td>\n</tr>\n<tr id=\"S5.T6.st1.13.12\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.13.12.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.1.1\" class=\"ltx_text ltx_font_bold\">Party 1</span></td>\n<td id=\"S5.T6.st1.13.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.2.1\" class=\"ltx_text ltx_font_bold\">Party 2</span></td>\n<td id=\"S5.T6.st1.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.3.1\" class=\"ltx_text ltx_font_bold\">Party 3</span></td>\n<td id=\"S5.T6.st1.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.4.1\" class=\"ltx_text ltx_font_bold\">Party 4</span></td>\n<td id=\"S5.T6.st1.13.12.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.5.1\" class=\"ltx_text ltx_font_bold\">Range</span></td>\n</tr>\n<tr id=\"S5.T6.st1.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"4\"><span id=\"S5.T6.st1.4.2.2.1\" class=\"ltx_text\">Priv-Baseline</span></td>\n<td id=\"S5.T6.st1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">86.20%</td>\n<td id=\"S5.T6.st1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">86.09%</td>\n<td id=\"S5.T6.st1.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">75.14%</td>\n<td id=\"S5.T6.st1.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.00%</td>\n<td id=\"S5.T6.st1.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.00%<math id=\"S5.T6.st1.4.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.4.2.1.m1.1a\"><mo id=\"S5.T6.st1.4.2.1.m1.1.1\" xref=\"S5.T6.st1.4.2.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.4.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.4.2.1.m1.1.1.cmml\" xref=\"S5.T6.st1.4.2.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.4.2.1.m1.1c\">\\sim</annotation></semantics></math>86.20%</td>\n</tr>\n<tr id=\"S5.T6.st1.5.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st1.5.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.31%</td>\n<td id=\"S5.T6.st1.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.10%</td>\n<td id=\"S5.T6.st1.5.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">80.67%</td>\n<td id=\"S5.T6.st1.5.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">80.57%</td>\n<td id=\"S5.T6.st1.5.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">80.57%<math id=\"S5.T6.st1.5.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.5.3.1.m1.1a\"><mo id=\"S5.T6.st1.5.3.1.m1.1.1\" xref=\"S5.T6.st1.5.3.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.5.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.5.3.1.m1.1.1.cmml\" xref=\"S5.T6.st1.5.3.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.5.3.1.m1.1c\">\\sim</annotation></semantics></math>89.31%</td>\n</tr>\n<tr id=\"S5.T6.st1.6.4\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st1.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.49%</td>\n<td id=\"S5.T6.st1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.46%</td>\n<td id=\"S5.T6.st1.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">84.01%</td>\n<td id=\"S5.T6.st1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.08%</td>\n<td id=\"S5.T6.st1.6.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.08%<math id=\"S5.T6.st1.6.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.6.4.1.m1.1a\"><mo id=\"S5.T6.st1.6.4.1.m1.1.1\" xref=\"S5.T6.st1.6.4.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.6.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.6.4.1.m1.1.1.cmml\" xref=\"S5.T6.st1.6.4.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.6.4.1.m1.1c\">\\sim</annotation></semantics></math>89.49%</td>\n</tr>\n<tr id=\"S5.T6.st1.7.5\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.7.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st1.7.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.71%</td>\n<td id=\"S5.T6.st1.7.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.42%</td>\n<td id=\"S5.T6.st1.7.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">83.13%</td>\n<td id=\"S5.T6.st1.7.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.55%</td>\n<td id=\"S5.T6.st1.7.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.55%<math id=\"S5.T6.st1.7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.7.5.1.m1.1a\"><mo id=\"S5.T6.st1.7.5.1.m1.1.1\" xref=\"S5.T6.st1.7.5.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.7.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.7.5.1.m1.1.1.cmml\" xref=\"S5.T6.st1.7.5.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.7.5.1.m1.1c\">\\sim</annotation></semantics></math>89.71%</td>\n</tr>\n<tr id=\"S5.T6.st1.8.6\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"5\"><span id=\"S5.T6.st1.8.6.2.1\" class=\"ltx_text\">FedOnce-L1</span></td>\n<td id=\"S5.T6.st1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st1.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.81%</td>\n<td id=\"S5.T6.st1.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.05%</td>\n<td id=\"S5.T6.st1.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">78.88%</td>\n<td id=\"S5.T6.st1.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">79.51%</td>\n<td id=\"S5.T6.st1.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">78.88%<math id=\"S5.T6.st1.8.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.8.6.1.m1.1a\"><mo id=\"S5.T6.st1.8.6.1.m1.1.1\" xref=\"S5.T6.st1.8.6.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.8.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.8.6.1.m1.1.1.cmml\" xref=\"S5.T6.st1.8.6.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.8.6.1.m1.1c\">\\sim</annotation></semantics></math>88.81%</td>\n</tr>\n<tr id=\"S5.T6.st1.9.7\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.9.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st1.9.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.73%</td>\n<td id=\"S5.T6.st1.9.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.54%</td>\n<td id=\"S5.T6.st1.9.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">81.77%</td>\n<td id=\"S5.T6.st1.9.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.88%</td>\n<td id=\"S5.T6.st1.9.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">81.77%<math id=\"S5.T6.st1.9.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.9.7.1.m1.1a\"><mo id=\"S5.T6.st1.9.7.1.m1.1.1\" xref=\"S5.T6.st1.9.7.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.9.7.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.9.7.1.m1.1.1.cmml\" xref=\"S5.T6.st1.9.7.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.9.7.1.m1.1c\">\\sim</annotation></semantics></math>89.73%</td>\n</tr>\n<tr id=\"S5.T6.st1.10.8\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st1.10.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.25%</td>\n<td id=\"S5.T6.st1.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.67%</td>\n<td id=\"S5.T6.st1.10.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">83.53%</td>\n<td id=\"S5.T6.st1.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.77%</td>\n<td id=\"S5.T6.st1.10.8.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.77%<math id=\"S5.T6.st1.10.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.10.8.1.m1.1a\"><mo id=\"S5.T6.st1.10.8.1.m1.1.1\" xref=\"S5.T6.st1.10.8.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.10.8.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.10.8.1.m1.1.1.cmml\" xref=\"S5.T6.st1.10.8.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.10.8.1.m1.1c\">\\sim</annotation></semantics></math>89.25%</td>\n</tr>\n<tr id=\"S5.T6.st1.11.9\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.11.9.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st1.11.9.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.37%</td>\n<td id=\"S5.T6.st1.11.9.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.01%</td>\n<td id=\"S5.T6.st1.11.9.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.14%</td>\n<td id=\"S5.T6.st1.11.9.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.64%</td>\n<td id=\"S5.T6.st1.11.9.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.01%<math id=\"S5.T6.st1.11.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.11.9.1.m1.1a\"><mo id=\"S5.T6.st1.11.9.1.m1.1.1\" xref=\"S5.T6.st1.11.9.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.11.9.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.11.9.1.m1.1.1.cmml\" xref=\"S5.T6.st1.11.9.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.11.9.1.m1.1c\">\\sim</annotation></semantics></math>93.64%</td>\n</tr>\n<tr id=\"S5.T6.st1.13.11\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.12.10.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><math id=\"S5.T6.st1.12.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\infty\" display=\"inline\"><semantics id=\"S5.T6.st1.12.10.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T6.st1.12.10.1.m1.1.1\" xref=\"S5.T6.st1.12.10.1.m1.1.1.cmml\">∞</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.12.10.1.m1.1b\"><infinity id=\"S5.T6.st1.12.10.1.m1.1.1.cmml\" xref=\"S5.T6.st1.12.10.1.m1.1.1\"></infinity></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.12.10.1.m1.1c\">\\infty</annotation></semantics></math></td>\n<td id=\"S5.T6.st1.13.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">96.91%</td>\n<td id=\"S5.T6.st1.13.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">97.11%</td>\n<td id=\"S5.T6.st1.13.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">97.61%</td>\n<td id=\"S5.T6.st1.13.11.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">96.42%</td>\n<td id=\"S5.T6.st1.13.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">96.42%<math id=\"S5.T6.st1.13.11.2.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.13.11.2.m1.1a\"><mo id=\"S5.T6.st1.13.11.2.m1.1.1\" xref=\"S5.T6.st1.13.11.2.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.13.11.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.13.11.2.m1.1.1.cmml\" xref=\"S5.T6.st1.13.11.2.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.13.11.2.m1.1c\">\\sim</annotation></semantics></math>97.61%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired."
        ]
    },
    "S5.T6.st1": {
        "caption": "(a) Performance on gisette, δ=10−4𝛿superscript104\\delta=10^{-4}",
        "table": "<table id=\"S5.T6.st1.13\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T6.st1.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st1.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S5.T6.st1.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st1.3.1.1.1\" class=\"ltx_text\"><math id=\"S5.T6.st1.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\varepsilon\" display=\"inline\"><semantics id=\"S5.T6.st1.3.1.1.1.m1.1a\"><mi id=\"S5.T6.st1.3.1.1.1.m1.1.1\" xref=\"S5.T6.st1.3.1.1.1.m1.1.1.cmml\">ε</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.3.1.1.1.m1.1b\"><ci id=\"S5.T6.st1.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T6.st1.3.1.1.1.m1.1.1\">𝜀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.3.1.1.1.m1.1c\">\\varepsilon</annotation></semantics></math></span></td>\n<td id=\"S5.T6.st1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" colspan=\"4\"><span id=\"S5.T6.st1.3.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></td>\n<td id=\"S5.T6.st1.3.1.4\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"></td>\n</tr>\n<tr id=\"S5.T6.st1.13.12\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.13.12.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.1.1\" class=\"ltx_text ltx_font_bold\">Party 1</span></td>\n<td id=\"S5.T6.st1.13.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.2.1\" class=\"ltx_text ltx_font_bold\">Party 2</span></td>\n<td id=\"S5.T6.st1.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.3.1\" class=\"ltx_text ltx_font_bold\">Party 3</span></td>\n<td id=\"S5.T6.st1.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.4.1\" class=\"ltx_text ltx_font_bold\">Party 4</span></td>\n<td id=\"S5.T6.st1.13.12.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st1.13.12.5.1\" class=\"ltx_text ltx_font_bold\">Range</span></td>\n</tr>\n<tr id=\"S5.T6.st1.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"4\"><span id=\"S5.T6.st1.4.2.2.1\" class=\"ltx_text\">Priv-Baseline</span></td>\n<td id=\"S5.T6.st1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">86.20%</td>\n<td id=\"S5.T6.st1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">86.09%</td>\n<td id=\"S5.T6.st1.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">75.14%</td>\n<td id=\"S5.T6.st1.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.00%</td>\n<td id=\"S5.T6.st1.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.00%<math id=\"S5.T6.st1.4.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.4.2.1.m1.1a\"><mo id=\"S5.T6.st1.4.2.1.m1.1.1\" xref=\"S5.T6.st1.4.2.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.4.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.4.2.1.m1.1.1.cmml\" xref=\"S5.T6.st1.4.2.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.4.2.1.m1.1c\">\\sim</annotation></semantics></math>86.20%</td>\n</tr>\n<tr id=\"S5.T6.st1.5.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st1.5.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.31%</td>\n<td id=\"S5.T6.st1.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.10%</td>\n<td id=\"S5.T6.st1.5.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">80.67%</td>\n<td id=\"S5.T6.st1.5.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">80.57%</td>\n<td id=\"S5.T6.st1.5.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">80.57%<math id=\"S5.T6.st1.5.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.5.3.1.m1.1a\"><mo id=\"S5.T6.st1.5.3.1.m1.1.1\" xref=\"S5.T6.st1.5.3.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.5.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.5.3.1.m1.1.1.cmml\" xref=\"S5.T6.st1.5.3.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.5.3.1.m1.1c\">\\sim</annotation></semantics></math>89.31%</td>\n</tr>\n<tr id=\"S5.T6.st1.6.4\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st1.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.49%</td>\n<td id=\"S5.T6.st1.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.46%</td>\n<td id=\"S5.T6.st1.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">84.01%</td>\n<td id=\"S5.T6.st1.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.08%</td>\n<td id=\"S5.T6.st1.6.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.08%<math id=\"S5.T6.st1.6.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.6.4.1.m1.1a\"><mo id=\"S5.T6.st1.6.4.1.m1.1.1\" xref=\"S5.T6.st1.6.4.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.6.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.6.4.1.m1.1.1.cmml\" xref=\"S5.T6.st1.6.4.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.6.4.1.m1.1c\">\\sim</annotation></semantics></math>89.49%</td>\n</tr>\n<tr id=\"S5.T6.st1.7.5\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.7.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st1.7.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.71%</td>\n<td id=\"S5.T6.st1.7.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.42%</td>\n<td id=\"S5.T6.st1.7.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">83.13%</td>\n<td id=\"S5.T6.st1.7.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.55%</td>\n<td id=\"S5.T6.st1.7.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.55%<math id=\"S5.T6.st1.7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.7.5.1.m1.1a\"><mo id=\"S5.T6.st1.7.5.1.m1.1.1\" xref=\"S5.T6.st1.7.5.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.7.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.7.5.1.m1.1.1.cmml\" xref=\"S5.T6.st1.7.5.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.7.5.1.m1.1c\">\\sim</annotation></semantics></math>89.71%</td>\n</tr>\n<tr id=\"S5.T6.st1.8.6\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"5\"><span id=\"S5.T6.st1.8.6.2.1\" class=\"ltx_text\">FedOnce-L1</span></td>\n<td id=\"S5.T6.st1.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st1.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.81%</td>\n<td id=\"S5.T6.st1.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.05%</td>\n<td id=\"S5.T6.st1.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">78.88%</td>\n<td id=\"S5.T6.st1.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">79.51%</td>\n<td id=\"S5.T6.st1.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">78.88%<math id=\"S5.T6.st1.8.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.8.6.1.m1.1a\"><mo id=\"S5.T6.st1.8.6.1.m1.1.1\" xref=\"S5.T6.st1.8.6.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.8.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.8.6.1.m1.1.1.cmml\" xref=\"S5.T6.st1.8.6.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.8.6.1.m1.1c\">\\sim</annotation></semantics></math>88.81%</td>\n</tr>\n<tr id=\"S5.T6.st1.9.7\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.9.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st1.9.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.73%</td>\n<td id=\"S5.T6.st1.9.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.54%</td>\n<td id=\"S5.T6.st1.9.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">81.77%</td>\n<td id=\"S5.T6.st1.9.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.88%</td>\n<td id=\"S5.T6.st1.9.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">81.77%<math id=\"S5.T6.st1.9.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.9.7.1.m1.1a\"><mo id=\"S5.T6.st1.9.7.1.m1.1.1\" xref=\"S5.T6.st1.9.7.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.9.7.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.9.7.1.m1.1.1.cmml\" xref=\"S5.T6.st1.9.7.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.9.7.1.m1.1c\">\\sim</annotation></semantics></math>89.73%</td>\n</tr>\n<tr id=\"S5.T6.st1.10.8\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st1.10.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.25%</td>\n<td id=\"S5.T6.st1.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.67%</td>\n<td id=\"S5.T6.st1.10.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">83.53%</td>\n<td id=\"S5.T6.st1.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.77%</td>\n<td id=\"S5.T6.st1.10.8.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.77%<math id=\"S5.T6.st1.10.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.10.8.1.m1.1a\"><mo id=\"S5.T6.st1.10.8.1.m1.1.1\" xref=\"S5.T6.st1.10.8.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.10.8.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.10.8.1.m1.1.1.cmml\" xref=\"S5.T6.st1.10.8.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.10.8.1.m1.1c\">\\sim</annotation></semantics></math>89.25%</td>\n</tr>\n<tr id=\"S5.T6.st1.11.9\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.11.9.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st1.11.9.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.37%</td>\n<td id=\"S5.T6.st1.11.9.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.01%</td>\n<td id=\"S5.T6.st1.11.9.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.14%</td>\n<td id=\"S5.T6.st1.11.9.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.64%</td>\n<td id=\"S5.T6.st1.11.9.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.01%<math id=\"S5.T6.st1.11.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.11.9.1.m1.1a\"><mo id=\"S5.T6.st1.11.9.1.m1.1.1\" xref=\"S5.T6.st1.11.9.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.11.9.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.11.9.1.m1.1.1.cmml\" xref=\"S5.T6.st1.11.9.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.11.9.1.m1.1c\">\\sim</annotation></semantics></math>93.64%</td>\n</tr>\n<tr id=\"S5.T6.st1.13.11\" class=\"ltx_tr\">\n<td id=\"S5.T6.st1.12.10.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><math id=\"S5.T6.st1.12.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\infty\" display=\"inline\"><semantics id=\"S5.T6.st1.12.10.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T6.st1.12.10.1.m1.1.1\" xref=\"S5.T6.st1.12.10.1.m1.1.1.cmml\">∞</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.12.10.1.m1.1b\"><infinity id=\"S5.T6.st1.12.10.1.m1.1.1.cmml\" xref=\"S5.T6.st1.12.10.1.m1.1.1\"></infinity></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.12.10.1.m1.1c\">\\infty</annotation></semantics></math></td>\n<td id=\"S5.T6.st1.13.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">96.91%</td>\n<td id=\"S5.T6.st1.13.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">97.11%</td>\n<td id=\"S5.T6.st1.13.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">97.61%</td>\n<td id=\"S5.T6.st1.13.11.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">96.42%</td>\n<td id=\"S5.T6.st1.13.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">96.42%<math id=\"S5.T6.st1.13.11.2.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st1.13.11.2.m1.1a\"><mo id=\"S5.T6.st1.13.11.2.m1.1.1\" xref=\"S5.T6.st1.13.11.2.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st1.13.11.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st1.13.11.2.m1.1.1.cmml\" xref=\"S5.T6.st1.13.11.2.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st1.13.11.2.m1.1c\">\\sim</annotation></semantics></math>97.61%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "S5.T6.st2": {
        "caption": "(b) Performance on covtype, δ=10−5𝛿superscript105\\delta=10^{-5}",
        "table": "<table id=\"S5.T6.st2.13\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T6.st2.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S5.T6.st2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st2.3.1.1.1\" class=\"ltx_text\"><math id=\"S5.T6.st2.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\varepsilon\" display=\"inline\"><semantics id=\"S5.T6.st2.3.1.1.1.m1.1a\"><mi id=\"S5.T6.st2.3.1.1.1.m1.1.1\" xref=\"S5.T6.st2.3.1.1.1.m1.1.1.cmml\">ε</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.3.1.1.1.m1.1b\"><ci id=\"S5.T6.st2.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T6.st2.3.1.1.1.m1.1.1\">𝜀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.3.1.1.1.m1.1c\">\\varepsilon</annotation></semantics></math></span></td>\n<td id=\"S5.T6.st2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" colspan=\"4\"><span id=\"S5.T6.st2.3.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></td>\n<td id=\"S5.T6.st2.3.1.4\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"></td>\n</tr>\n<tr id=\"S5.T6.st2.13.12\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.13.12.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st2.13.12.1.1\" class=\"ltx_text ltx_font_bold\">Party 1</span></td>\n<td id=\"S5.T6.st2.13.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st2.13.12.2.1\" class=\"ltx_text ltx_font_bold\">Party 2</span></td>\n<td id=\"S5.T6.st2.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st2.13.12.3.1\" class=\"ltx_text ltx_font_bold\">Party 3</span></td>\n<td id=\"S5.T6.st2.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st2.13.12.4.1\" class=\"ltx_text ltx_font_bold\">Party 4</span></td>\n<td id=\"S5.T6.st2.13.12.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st2.13.12.5.1\" class=\"ltx_text ltx_font_bold\">Range</span></td>\n</tr>\n<tr id=\"S5.T6.st2.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"4\"><span id=\"S5.T6.st2.4.2.2.1\" class=\"ltx_text\">Priv-Baseline</span></td>\n<td id=\"S5.T6.st2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.60%</td>\n<td id=\"S5.T6.st2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">65.88%</td>\n<td id=\"S5.T6.st2.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">60.19%</td>\n<td id=\"S5.T6.st2.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">66.53%</td>\n<td id=\"S5.T6.st2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">60.19%<math id=\"S5.T6.st2.4.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st2.4.2.1.m1.1a\"><mo id=\"S5.T6.st2.4.2.1.m1.1.1\" xref=\"S5.T6.st2.4.2.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.4.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st2.4.2.1.m1.1.1.cmml\" xref=\"S5.T6.st2.4.2.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.4.2.1.m1.1c\">\\sim</annotation></semantics></math>74.60%</td>\n</tr>\n<tr id=\"S5.T6.st2.5.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st2.5.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">75.89%</td>\n<td id=\"S5.T6.st2.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">69.14%</td>\n<td id=\"S5.T6.st2.5.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">69.88%</td>\n<td id=\"S5.T6.st2.5.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">68.70%</td>\n<td id=\"S5.T6.st2.5.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">68.70%<math id=\"S5.T6.st2.5.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st2.5.3.1.m1.1a\"><mo id=\"S5.T6.st2.5.3.1.m1.1.1\" xref=\"S5.T6.st2.5.3.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.5.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st2.5.3.1.m1.1.1.cmml\" xref=\"S5.T6.st2.5.3.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.5.3.1.m1.1c\">\\sim</annotation></semantics></math>75.89%</td>\n</tr>\n<tr id=\"S5.T6.st2.6.4\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st2.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">78.27%</td>\n<td id=\"S5.T6.st2.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.75%</td>\n<td id=\"S5.T6.st2.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.46%</td>\n<td id=\"S5.T6.st2.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.87%</td>\n<td id=\"S5.T6.st2.6.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.46%<math id=\"S5.T6.st2.6.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st2.6.4.1.m1.1a\"><mo id=\"S5.T6.st2.6.4.1.m1.1.1\" xref=\"S5.T6.st2.6.4.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.6.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st2.6.4.1.m1.1.1.cmml\" xref=\"S5.T6.st2.6.4.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.6.4.1.m1.1c\">\\sim</annotation></semantics></math>78.27%</td>\n</tr>\n<tr id=\"S5.T6.st2.7.5\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.7.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st2.7.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">79.25%</td>\n<td id=\"S5.T6.st2.7.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.96%</td>\n<td id=\"S5.T6.st2.7.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.40%</td>\n<td id=\"S5.T6.st2.7.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.04%</td>\n<td id=\"S5.T6.st2.7.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.40%<math id=\"S5.T6.st2.7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st2.7.5.1.m1.1a\"><mo id=\"S5.T6.st2.7.5.1.m1.1.1\" xref=\"S5.T6.st2.7.5.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.7.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st2.7.5.1.m1.1.1.cmml\" xref=\"S5.T6.st2.7.5.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.7.5.1.m1.1c\">\\sim</annotation></semantics></math>79.25%</td>\n</tr>\n<tr id=\"S5.T6.st2.8.6\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"5\"><span id=\"S5.T6.st2.8.6.2.1\" class=\"ltx_text\">FedOnce-L1</span></td>\n<td id=\"S5.T6.st2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st2.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">76.88%</td>\n<td id=\"S5.T6.st2.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">70.74%</td>\n<td id=\"S5.T6.st2.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">70.11%</td>\n<td id=\"S5.T6.st2.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">71.56%</td>\n<td id=\"S5.T6.st2.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">70.11%<math id=\"S5.T6.st2.8.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st2.8.6.1.m1.1a\"><mo id=\"S5.T6.st2.8.6.1.m1.1.1\" xref=\"S5.T6.st2.8.6.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.8.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st2.8.6.1.m1.1.1.cmml\" xref=\"S5.T6.st2.8.6.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.8.6.1.m1.1c\">\\sim</annotation></semantics></math>76.88%</td>\n</tr>\n<tr id=\"S5.T6.st2.9.7\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.9.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st2.9.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">77.80%</td>\n<td id=\"S5.T6.st2.9.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.33%</td>\n<td id=\"S5.T6.st2.9.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.71%</td>\n<td id=\"S5.T6.st2.9.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">72.15%</td>\n<td id=\"S5.T6.st2.9.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">72.15%<math id=\"S5.T6.st2.9.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st2.9.7.1.m1.1a\"><mo id=\"S5.T6.st2.9.7.1.m1.1.1\" xref=\"S5.T6.st2.9.7.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.9.7.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st2.9.7.1.m1.1.1.cmml\" xref=\"S5.T6.st2.9.7.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.9.7.1.m1.1c\">\\sim</annotation></semantics></math>77.80%</td>\n</tr>\n<tr id=\"S5.T6.st2.10.8\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st2.10.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">79.23%</td>\n<td id=\"S5.T6.st2.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.18%</td>\n<td id=\"S5.T6.st2.10.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.66%</td>\n<td id=\"S5.T6.st2.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.98%</td>\n<td id=\"S5.T6.st2.10.8.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">73.98%<math id=\"S5.T6.st2.10.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st2.10.8.1.m1.1a\"><mo id=\"S5.T6.st2.10.8.1.m1.1.1\" xref=\"S5.T6.st2.10.8.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.10.8.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st2.10.8.1.m1.1.1.cmml\" xref=\"S5.T6.st2.10.8.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.10.8.1.m1.1c\">\\sim</annotation></semantics></math>79.23%</td>\n</tr>\n<tr id=\"S5.T6.st2.11.9\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.11.9.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st2.11.9.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">80.49%</td>\n<td id=\"S5.T6.st2.11.9.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">75.04%</td>\n<td id=\"S5.T6.st2.11.9.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.74%</td>\n<td id=\"S5.T6.st2.11.9.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.80%</td>\n<td id=\"S5.T6.st2.11.9.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.74%<math id=\"S5.T6.st2.11.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st2.11.9.1.m1.1a\"><mo id=\"S5.T6.st2.11.9.1.m1.1.1\" xref=\"S5.T6.st2.11.9.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.11.9.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st2.11.9.1.m1.1.1.cmml\" xref=\"S5.T6.st2.11.9.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.11.9.1.m1.1c\">\\sim</annotation></semantics></math>80.49%</td>\n</tr>\n<tr id=\"S5.T6.st2.13.11\" class=\"ltx_tr\">\n<td id=\"S5.T6.st2.12.10.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><math id=\"S5.T6.st2.12.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\infty\" display=\"inline\"><semantics id=\"S5.T6.st2.12.10.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T6.st2.12.10.1.m1.1.1\" xref=\"S5.T6.st2.12.10.1.m1.1.1.cmml\">∞</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.12.10.1.m1.1b\"><infinity id=\"S5.T6.st2.12.10.1.m1.1.1.cmml\" xref=\"S5.T6.st2.12.10.1.m1.1.1\"></infinity></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.12.10.1.m1.1c\">\\infty</annotation></semantics></math></td>\n<td id=\"S5.T6.st2.13.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.24%</td>\n<td id=\"S5.T6.st2.13.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.48%</td>\n<td id=\"S5.T6.st2.13.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.29%</td>\n<td id=\"S5.T6.st2.13.11.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.06%</td>\n<td id=\"S5.T6.st2.13.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">74.06%<math id=\"S5.T6.st2.13.11.2.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st2.13.11.2.m1.1a\"><mo id=\"S5.T6.st2.13.11.2.m1.1.1\" xref=\"S5.T6.st2.13.11.2.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st2.13.11.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st2.13.11.2.m1.1.1.cmml\" xref=\"S5.T6.st2.13.11.2.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st2.13.11.2.m1.1c\">\\sim</annotation></semantics></math>82.24%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "S5.T6.st3": {
        "caption": "(c) Performance on phishing, δ=10−5𝛿superscript105\\delta=10^{-5}",
        "table": "<table id=\"S5.T6.st3.13\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T6.st3.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st3.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S5.T6.st3.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st3.3.1.1.1\" class=\"ltx_text\"><math id=\"S5.T6.st3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\varepsilon\" display=\"inline\"><semantics id=\"S5.T6.st3.3.1.1.1.m1.1a\"><mi id=\"S5.T6.st3.3.1.1.1.m1.1.1\" xref=\"S5.T6.st3.3.1.1.1.m1.1.1.cmml\">ε</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.3.1.1.1.m1.1b\"><ci id=\"S5.T6.st3.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T6.st3.3.1.1.1.m1.1.1\">𝜀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.3.1.1.1.m1.1c\">\\varepsilon</annotation></semantics></math></span></td>\n<td id=\"S5.T6.st3.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" colspan=\"4\"><span id=\"S5.T6.st3.3.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></td>\n<td id=\"S5.T6.st3.3.1.4\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"></td>\n</tr>\n<tr id=\"S5.T6.st3.13.12\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.13.12.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st3.13.12.1.1\" class=\"ltx_text ltx_font_bold\">Party 1</span></td>\n<td id=\"S5.T6.st3.13.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st3.13.12.2.1\" class=\"ltx_text ltx_font_bold\">Party 2</span></td>\n<td id=\"S5.T6.st3.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st3.13.12.3.1\" class=\"ltx_text ltx_font_bold\">Party 3</span></td>\n<td id=\"S5.T6.st3.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st3.13.12.4.1\" class=\"ltx_text ltx_font_bold\">Party 4</span></td>\n<td id=\"S5.T6.st3.13.12.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st3.13.12.5.1\" class=\"ltx_text ltx_font_bold\">Range</span></td>\n</tr>\n<tr id=\"S5.T6.st3.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"4\"><span id=\"S5.T6.st3.4.2.2.1\" class=\"ltx_text\">Priv-Baseline</span></td>\n<td id=\"S5.T6.st3.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st3.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.69%</td>\n<td id=\"S5.T6.st3.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.22%</td>\n<td id=\"S5.T6.st3.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">83.98%</td>\n<td id=\"S5.T6.st3.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">83.64%</td>\n<td id=\"S5.T6.st3.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">83.64%<math id=\"S5.T6.st3.4.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st3.4.2.1.m1.1a\"><mo id=\"S5.T6.st3.4.2.1.m1.1.1\" xref=\"S5.T6.st3.4.2.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.4.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st3.4.2.1.m1.1.1.cmml\" xref=\"S5.T6.st3.4.2.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.4.2.1.m1.1c\">\\sim</annotation></semantics></math>87.69%</td>\n</tr>\n<tr id=\"S5.T6.st3.5.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st3.5.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.43%</td>\n<td id=\"S5.T6.st3.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.49%</td>\n<td id=\"S5.T6.st3.5.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.79%</td>\n<td id=\"S5.T6.st3.5.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.92%</td>\n<td id=\"S5.T6.st3.5.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.49%<math id=\"S5.T6.st3.5.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st3.5.3.1.m1.1a\"><mo id=\"S5.T6.st3.5.3.1.m1.1.1\" xref=\"S5.T6.st3.5.3.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.5.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st3.5.3.1.m1.1.1.cmml\" xref=\"S5.T6.st3.5.3.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.5.3.1.m1.1c\">\\sim</annotation></semantics></math>88.79%</td>\n</tr>\n<tr id=\"S5.T6.st3.6.4\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st3.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.31%</td>\n<td id=\"S5.T6.st3.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.51%</td>\n<td id=\"S5.T6.st3.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.55%</td>\n<td id=\"S5.T6.st3.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.29%</td>\n<td id=\"S5.T6.st3.6.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.29%<math id=\"S5.T6.st3.6.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st3.6.4.1.m1.1a\"><mo id=\"S5.T6.st3.6.4.1.m1.1.1\" xref=\"S5.T6.st3.6.4.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.6.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st3.6.4.1.m1.1.1.cmml\" xref=\"S5.T6.st3.6.4.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.6.4.1.m1.1c\">\\sim</annotation></semantics></math>90.31%</td>\n</tr>\n<tr id=\"S5.T6.st3.7.5\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.7.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st3.7.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.89%</td>\n<td id=\"S5.T6.st3.7.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.76%</td>\n<td id=\"S5.T6.st3.7.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.69%</td>\n<td id=\"S5.T6.st3.7.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.51%</td>\n<td id=\"S5.T6.st3.7.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.69%<math id=\"S5.T6.st3.7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st3.7.5.1.m1.1a\"><mo id=\"S5.T6.st3.7.5.1.m1.1.1\" xref=\"S5.T6.st3.7.5.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.7.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st3.7.5.1.m1.1.1.cmml\" xref=\"S5.T6.st3.7.5.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.7.5.1.m1.1c\">\\sim</annotation></semantics></math>90.89%</td>\n</tr>\n<tr id=\"S5.T6.st3.8.6\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"5\"><span id=\"S5.T6.st3.8.6.2.1\" class=\"ltx_text\">FedOnce-L1</span></td>\n<td id=\"S5.T6.st3.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st3.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.10%</td>\n<td id=\"S5.T6.st3.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.36%</td>\n<td id=\"S5.T6.st3.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.78%</td>\n<td id=\"S5.T6.st3.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.95%</td>\n<td id=\"S5.T6.st3.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.78%<math id=\"S5.T6.st3.8.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st3.8.6.1.m1.1a\"><mo id=\"S5.T6.st3.8.6.1.m1.1.1\" xref=\"S5.T6.st3.8.6.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.8.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st3.8.6.1.m1.1.1.cmml\" xref=\"S5.T6.st3.8.6.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.8.6.1.m1.1c\">\\sim</annotation></semantics></math>90.10%</td>\n</tr>\n<tr id=\"S5.T6.st3.9.7\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.9.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st3.9.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.10%</td>\n<td id=\"S5.T6.st3.9.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.24%</td>\n<td id=\"S5.T6.st3.9.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.90%</td>\n<td id=\"S5.T6.st3.9.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.16%</td>\n<td id=\"S5.T6.st3.9.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.24%<math id=\"S5.T6.st3.9.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st3.9.7.1.m1.1a\"><mo id=\"S5.T6.st3.9.7.1.m1.1.1\" xref=\"S5.T6.st3.9.7.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.9.7.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st3.9.7.1.m1.1.1.cmml\" xref=\"S5.T6.st3.9.7.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.9.7.1.m1.1c\">\\sim</annotation></semantics></math>91.16%</td>\n</tr>\n<tr id=\"S5.T6.st3.10.8\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st3.10.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.56%</td>\n<td id=\"S5.T6.st3.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.65%</td>\n<td id=\"S5.T6.st3.10.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.36%</td>\n<td id=\"S5.T6.st3.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.16%</td>\n<td id=\"S5.T6.st3.10.8.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.36%<math id=\"S5.T6.st3.10.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st3.10.8.1.m1.1a\"><mo id=\"S5.T6.st3.10.8.1.m1.1.1\" xref=\"S5.T6.st3.10.8.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.10.8.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st3.10.8.1.m1.1.1.cmml\" xref=\"S5.T6.st3.10.8.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.10.8.1.m1.1c\">\\sim</annotation></semantics></math>91.56%</td>\n</tr>\n<tr id=\"S5.T6.st3.11.9\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.11.9.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st3.11.9.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.78%</td>\n<td id=\"S5.T6.st3.11.9.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.12%</td>\n<td id=\"S5.T6.st3.11.9.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.63%</td>\n<td id=\"S5.T6.st3.11.9.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.30%</td>\n<td id=\"S5.T6.st3.11.9.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.63%<math id=\"S5.T6.st3.11.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st3.11.9.1.m1.1a\"><mo id=\"S5.T6.st3.11.9.1.m1.1.1\" xref=\"S5.T6.st3.11.9.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.11.9.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st3.11.9.1.m1.1.1.cmml\" xref=\"S5.T6.st3.11.9.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.11.9.1.m1.1c\">\\sim</annotation></semantics></math>91.30%</td>\n</tr>\n<tr id=\"S5.T6.st3.13.11\" class=\"ltx_tr\">\n<td id=\"S5.T6.st3.12.10.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><math id=\"S5.T6.st3.12.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\infty\" display=\"inline\"><semantics id=\"S5.T6.st3.12.10.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T6.st3.12.10.1.m1.1.1\" xref=\"S5.T6.st3.12.10.1.m1.1.1.cmml\">∞</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.12.10.1.m1.1b\"><infinity id=\"S5.T6.st3.12.10.1.m1.1.1.cmml\" xref=\"S5.T6.st3.12.10.1.m1.1.1\"></infinity></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.12.10.1.m1.1c\">\\infty</annotation></semantics></math></td>\n<td id=\"S5.T6.st3.13.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.95%</td>\n<td id=\"S5.T6.st3.13.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.28%</td>\n<td id=\"S5.T6.st3.13.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.21%</td>\n<td id=\"S5.T6.st3.13.11.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.90%</td>\n<td id=\"S5.T6.st3.13.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.90%<math id=\"S5.T6.st3.13.11.2.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st3.13.11.2.m1.1a\"><mo id=\"S5.T6.st3.13.11.2.m1.1.1\" xref=\"S5.T6.st3.13.11.2.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st3.13.11.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st3.13.11.2.m1.1.1.cmml\" xref=\"S5.T6.st3.13.11.2.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st3.13.11.2.m1.1c\">\\sim</annotation></semantics></math>91.95%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "S5.T6.st4": {
        "caption": "(d) Performance on UJIIndoorLoc, δ=10−5𝛿superscript105\\delta=10^{-5}",
        "table": "<table id=\"S5.T6.st4.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T6.st4.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st4.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S5.T6.st4.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st4.3.1.1.1\" class=\"ltx_text\"><math id=\"S5.T6.st4.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\varepsilon\" display=\"inline\"><semantics id=\"S5.T6.st4.3.1.1.1.m1.1a\"><mi id=\"S5.T6.st4.3.1.1.1.m1.1.1\" xref=\"S5.T6.st4.3.1.1.1.m1.1.1.cmml\">ε</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st4.3.1.1.1.m1.1b\"><ci id=\"S5.T6.st4.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T6.st4.3.1.1.1.m1.1.1\">𝜀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st4.3.1.1.1.m1.1c\">\\varepsilon</annotation></semantics></math></span></td>\n<td id=\"S5.T6.st4.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" colspan=\"4\"><span id=\"S5.T6.st4.3.1.3.1\" class=\"ltx_text ltx_font_bold\">RMSE</span></td>\n<td id=\"S5.T6.st4.3.1.4\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"></td>\n</tr>\n<tr id=\"S5.T6.st4.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st4.4.3.1.1\" class=\"ltx_text ltx_font_bold\">Party 1</span></td>\n<td id=\"S5.T6.st4.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st4.4.3.2.1\" class=\"ltx_text ltx_font_bold\">Party 2</span></td>\n<td id=\"S5.T6.st4.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st4.4.3.3.1\" class=\"ltx_text ltx_font_bold\">Party 3</span></td>\n<td id=\"S5.T6.st4.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st4.4.3.4.1\" class=\"ltx_text ltx_font_bold\">Party 4</span></td>\n<td id=\"S5.T6.st4.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st4.4.3.5.1\" class=\"ltx_text ltx_font_bold\">Range</span></td>\n</tr>\n<tr id=\"S5.T6.st4.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"4\"><span id=\"S5.T6.st4.4.4.1.1\" class=\"ltx_text\">Priv-Baseline</span></td>\n<td id=\"S5.T6.st4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.2326</td>\n<td id=\"S5.T6.st4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.2275</td>\n<td id=\"S5.T6.st4.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.2292</td>\n<td id=\"S5.T6.st4.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.2501</td>\n<td id=\"S5.T6.st4.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.2275-0.2501</td>\n</tr>\n<tr id=\"S5.T6.st4.4.5\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st4.4.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1487</td>\n<td id=\"S5.T6.st4.4.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1578</td>\n<td id=\"S5.T6.st4.4.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.2062</td>\n<td id=\"S5.T6.st4.4.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1943</td>\n<td id=\"S5.T6.st4.4.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1487-0.2062</td>\n</tr>\n<tr id=\"S5.T6.st4.4.6\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.6.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st4.4.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1053</td>\n<td id=\"S5.T6.st4.4.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1224</td>\n<td id=\"S5.T6.st4.4.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1475</td>\n<td id=\"S5.T6.st4.4.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1914</td>\n<td id=\"S5.T6.st4.4.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1053-0.1914</td>\n</tr>\n<tr id=\"S5.T6.st4.4.7\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st4.4.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1051</td>\n<td id=\"S5.T6.st4.4.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1198</td>\n<td id=\"S5.T6.st4.4.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1409</td>\n<td id=\"S5.T6.st4.4.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1345</td>\n<td id=\"S5.T6.st4.4.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1051-0.1345</td>\n</tr>\n<tr id=\"S5.T6.st4.4.8\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"5\"><span id=\"S5.T6.st4.4.8.1.1\" class=\"ltx_text\">FedOnce-L1</span></td>\n<td id=\"S5.T6.st4.4.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st4.4.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1066</td>\n<td id=\"S5.T6.st4.4.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1102</td>\n<td id=\"S5.T6.st4.4.8.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1087</td>\n<td id=\"S5.T6.st4.4.8.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1489</td>\n<td id=\"S5.T6.st4.4.8.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1066-0.1489</td>\n</tr>\n<tr id=\"S5.T6.st4.4.9\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.9.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st4.4.9.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0769</td>\n<td id=\"S5.T6.st4.4.9.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0811</td>\n<td id=\"S5.T6.st4.4.9.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0843</td>\n<td id=\"S5.T6.st4.4.9.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1362</td>\n<td id=\"S5.T6.st4.4.9.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0769-0.1362</td>\n</tr>\n<tr id=\"S5.T6.st4.4.10\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.10.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st4.4.10.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0714</td>\n<td id=\"S5.T6.st4.4.10.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0746</td>\n<td id=\"S5.T6.st4.4.10.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0664</td>\n<td id=\"S5.T6.st4.4.10.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1223</td>\n<td id=\"S5.T6.st4.4.10.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0664-0.1223</td>\n</tr>\n<tr id=\"S5.T6.st4.4.11\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.11.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st4.4.11.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0542</td>\n<td id=\"S5.T6.st4.4.11.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0614</td>\n<td id=\"S5.T6.st4.4.11.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0678</td>\n<td id=\"S5.T6.st4.4.11.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0730</td>\n<td id=\"S5.T6.st4.4.11.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0542-0.0730</td>\n</tr>\n<tr id=\"S5.T6.st4.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.st4.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><math id=\"S5.T6.st4.4.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\infty\" display=\"inline\"><semantics id=\"S5.T6.st4.4.2.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T6.st4.4.2.1.m1.1.1\" xref=\"S5.T6.st4.4.2.1.m1.1.1.cmml\">∞</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st4.4.2.1.m1.1b\"><infinity id=\"S5.T6.st4.4.2.1.m1.1.1.cmml\" xref=\"S5.T6.st4.4.2.1.m1.1.1\"></infinity></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st4.4.2.1.m1.1c\">\\infty</annotation></semantics></math></td>\n<td id=\"S5.T6.st4.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0355</td>\n<td id=\"S5.T6.st4.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0369</td>\n<td id=\"S5.T6.st4.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0402</td>\n<td id=\"S5.T6.st4.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0359</td>\n<td id=\"S5.T6.st4.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0355-0.0402</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "S5.T6.st5": {
        "caption": "(e) Performance on MNIST, δ=10−5𝛿superscript105\\delta=10^{-5}",
        "table": "<table id=\"S5.T6.st5.13\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T6.st5.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st5.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S5.T6.st5.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st5.3.1.1.1\" class=\"ltx_text\"><math id=\"S5.T6.st5.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\varepsilon\" display=\"inline\"><semantics id=\"S5.T6.st5.3.1.1.1.m1.1a\"><mi id=\"S5.T6.st5.3.1.1.1.m1.1.1\" xref=\"S5.T6.st5.3.1.1.1.m1.1.1.cmml\">ε</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.3.1.1.1.m1.1b\"><ci id=\"S5.T6.st5.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T6.st5.3.1.1.1.m1.1.1\">𝜀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.3.1.1.1.m1.1c\">\\varepsilon</annotation></semantics></math></span></td>\n<td id=\"S5.T6.st5.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" colspan=\"4\"><span id=\"S5.T6.st5.3.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></td>\n<td id=\"S5.T6.st5.3.1.4\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"></td>\n</tr>\n<tr id=\"S5.T6.st5.13.12\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.13.12.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st5.13.12.1.1\" class=\"ltx_text ltx_font_bold\">Party 1</span></td>\n<td id=\"S5.T6.st5.13.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st5.13.12.2.1\" class=\"ltx_text ltx_font_bold\">Party 2</span></td>\n<td id=\"S5.T6.st5.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st5.13.12.3.1\" class=\"ltx_text ltx_font_bold\">Party 3</span></td>\n<td id=\"S5.T6.st5.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st5.13.12.4.1\" class=\"ltx_text ltx_font_bold\">Party 4</span></td>\n<td id=\"S5.T6.st5.13.12.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st5.13.12.5.1\" class=\"ltx_text ltx_font_bold\">Range</span></td>\n</tr>\n<tr id=\"S5.T6.st5.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"4\"><span id=\"S5.T6.st5.4.2.2.1\" class=\"ltx_text\">Priv-Baseline</span></td>\n<td id=\"S5.T6.st5.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st5.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">45.29%</td>\n<td id=\"S5.T6.st5.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">39.98%</td>\n<td id=\"S5.T6.st5.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">50.28%</td>\n<td id=\"S5.T6.st5.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">44.49%</td>\n<td id=\"S5.T6.st5.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">39.98%<math id=\"S5.T6.st5.4.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st5.4.2.1.m1.1a\"><mo id=\"S5.T6.st5.4.2.1.m1.1.1\" xref=\"S5.T6.st5.4.2.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.4.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st5.4.2.1.m1.1.1.cmml\" xref=\"S5.T6.st5.4.2.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.4.2.1.m1.1c\">\\sim</annotation></semantics></math>50.28%</td>\n</tr>\n<tr id=\"S5.T6.st5.5.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.5.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st5.5.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">70.07%</td>\n<td id=\"S5.T6.st5.5.3.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">76.70%</td>\n<td id=\"S5.T6.st5.5.3.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">78.00%</td>\n<td id=\"S5.T6.st5.5.3.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">75.23%</td>\n<td id=\"S5.T6.st5.5.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">70.07%<math id=\"S5.T6.st5.5.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st5.5.3.1.m1.1a\"><mo id=\"S5.T6.st5.5.3.1.m1.1.1\" xref=\"S5.T6.st5.5.3.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.5.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st5.5.3.1.m1.1.1.cmml\" xref=\"S5.T6.st5.5.3.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.5.3.1.m1.1c\">\\sim</annotation></semantics></math>78.00%</td>\n</tr>\n<tr id=\"S5.T6.st5.6.4\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.6.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st5.6.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">83.64%</td>\n<td id=\"S5.T6.st5.6.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.92%</td>\n<td id=\"S5.T6.st5.6.4.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">84.00%</td>\n<td id=\"S5.T6.st5.6.4.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.60%</td>\n<td id=\"S5.T6.st5.6.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.60%<math id=\"S5.T6.st5.6.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st5.6.4.1.m1.1a\"><mo id=\"S5.T6.st5.6.4.1.m1.1.1\" xref=\"S5.T6.st5.6.4.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.6.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st5.6.4.1.m1.1.1.cmml\" xref=\"S5.T6.st5.6.4.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.6.4.1.m1.1c\">\\sim</annotation></semantics></math>84.00%</td>\n</tr>\n<tr id=\"S5.T6.st5.7.5\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.7.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st5.7.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">85.40%</td>\n<td id=\"S5.T6.st5.7.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">86.33%</td>\n<td id=\"S5.T6.st5.7.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">86.21%</td>\n<td id=\"S5.T6.st5.7.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">85.28%</td>\n<td id=\"S5.T6.st5.7.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">85.28%<math id=\"S5.T6.st5.7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st5.7.5.1.m1.1a\"><mo id=\"S5.T6.st5.7.5.1.m1.1.1\" xref=\"S5.T6.st5.7.5.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.7.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st5.7.5.1.m1.1.1.cmml\" xref=\"S5.T6.st5.7.5.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.7.5.1.m1.1c\">\\sim</annotation></semantics></math>86.33%</td>\n</tr>\n<tr id=\"S5.T6.st5.8.6\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"5\"><span id=\"S5.T6.st5.8.6.2.1\" class=\"ltx_text\">FedOnce-L1</span></td>\n<td id=\"S5.T6.st5.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st5.8.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.13%</td>\n<td id=\"S5.T6.st5.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">82.57%</td>\n<td id=\"S5.T6.st5.8.6.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">83.30%</td>\n<td id=\"S5.T6.st5.8.6.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">81.43%</td>\n<td id=\"S5.T6.st5.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">81.43%<math id=\"S5.T6.st5.8.6.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st5.8.6.1.m1.1a\"><mo id=\"S5.T6.st5.8.6.1.m1.1.1\" xref=\"S5.T6.st5.8.6.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.8.6.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st5.8.6.1.m1.1.1.cmml\" xref=\"S5.T6.st5.8.6.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.8.6.1.m1.1c\">\\sim</annotation></semantics></math>83.30%</td>\n</tr>\n<tr id=\"S5.T6.st5.9.7\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.9.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st5.9.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.01%</td>\n<td id=\"S5.T6.st5.9.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.51%</td>\n<td id=\"S5.T6.st5.9.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.91%</td>\n<td id=\"S5.T6.st5.9.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.02%</td>\n<td id=\"S5.T6.st5.9.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.01%<math id=\"S5.T6.st5.9.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st5.9.7.1.m1.1a\"><mo id=\"S5.T6.st5.9.7.1.m1.1.1\" xref=\"S5.T6.st5.9.7.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.9.7.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st5.9.7.1.m1.1.1.cmml\" xref=\"S5.T6.st5.9.7.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.9.7.1.m1.1c\">\\sim</annotation></semantics></math>87.91%</td>\n</tr>\n<tr id=\"S5.T6.st5.10.8\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.10.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st5.10.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.05%</td>\n<td id=\"S5.T6.st5.10.8.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.03%</td>\n<td id=\"S5.T6.st5.10.8.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">88.24%</td>\n<td id=\"S5.T6.st5.10.8.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.91%</td>\n<td id=\"S5.T6.st5.10.8.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">87.91%<math id=\"S5.T6.st5.10.8.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st5.10.8.1.m1.1a\"><mo id=\"S5.T6.st5.10.8.1.m1.1.1\" xref=\"S5.T6.st5.10.8.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.10.8.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st5.10.8.1.m1.1.1.cmml\" xref=\"S5.T6.st5.10.8.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.10.8.1.m1.1c\">\\sim</annotation></semantics></math>89.03%</td>\n</tr>\n<tr id=\"S5.T6.st5.11.9\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.11.9.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st5.11.9.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.52%</td>\n<td id=\"S5.T6.st5.11.9.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">90.63%</td>\n<td id=\"S5.T6.st5.11.9.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.61%</td>\n<td id=\"S5.T6.st5.11.9.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.32%</td>\n<td id=\"S5.T6.st5.11.9.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">89.32%<math id=\"S5.T6.st5.11.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st5.11.9.1.m1.1a\"><mo id=\"S5.T6.st5.11.9.1.m1.1.1\" xref=\"S5.T6.st5.11.9.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.11.9.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st5.11.9.1.m1.1.1.cmml\" xref=\"S5.T6.st5.11.9.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.11.9.1.m1.1c\">\\sim</annotation></semantics></math>90.63%</td>\n</tr>\n<tr id=\"S5.T6.st5.13.11\" class=\"ltx_tr\">\n<td id=\"S5.T6.st5.12.10.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><math id=\"S5.T6.st5.12.10.1.m1.1\" class=\"ltx_Math\" alttext=\"\\infty\" display=\"inline\"><semantics id=\"S5.T6.st5.12.10.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T6.st5.12.10.1.m1.1.1\" xref=\"S5.T6.st5.12.10.1.m1.1.1.cmml\">∞</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.12.10.1.m1.1b\"><infinity id=\"S5.T6.st5.12.10.1.m1.1.1.cmml\" xref=\"S5.T6.st5.12.10.1.m1.1.1\"></infinity></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.12.10.1.m1.1c\">\\infty</annotation></semantics></math></td>\n<td id=\"S5.T6.st5.13.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">92.05%</td>\n<td id=\"S5.T6.st5.13.11.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">92.97%</td>\n<td id=\"S5.T6.st5.13.11.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">93.11%</td>\n<td id=\"S5.T6.st5.13.11.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.74%</td>\n<td id=\"S5.T6.st5.13.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">91.74%<math id=\"S5.T6.st5.13.11.2.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S5.T6.st5.13.11.2.m1.1a\"><mo id=\"S5.T6.st5.13.11.2.m1.1.1\" xref=\"S5.T6.st5.13.11.2.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st5.13.11.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.st5.13.11.2.m1.1.1.cmml\" xref=\"S5.T6.st5.13.11.2.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st5.13.11.2.m1.1c\">\\sim</annotation></semantics></math>93.11%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "S5.T6.st6": {
        "caption": "(f) Performance on Superconduct, δ=10−5𝛿superscript105\\delta=10^{-5}",
        "table": "<table id=\"S5.T6.st6.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T6.st6.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st6.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S5.T6.st6.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"2\"><span id=\"S5.T6.st6.3.1.1.1\" class=\"ltx_text\"><math id=\"S5.T6.st6.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\varepsilon\" display=\"inline\"><semantics id=\"S5.T6.st6.3.1.1.1.m1.1a\"><mi id=\"S5.T6.st6.3.1.1.1.m1.1.1\" xref=\"S5.T6.st6.3.1.1.1.m1.1.1.cmml\">ε</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st6.3.1.1.1.m1.1b\"><ci id=\"S5.T6.st6.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T6.st6.3.1.1.1.m1.1.1\">𝜀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st6.3.1.1.1.m1.1c\">\\varepsilon</annotation></semantics></math></span></td>\n<td id=\"S5.T6.st6.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" colspan=\"4\"><span id=\"S5.T6.st6.3.1.3.1\" class=\"ltx_text ltx_font_bold\">RMSE</span></td>\n<td id=\"S5.T6.st6.3.1.4\" class=\"ltx_td ltx_border_tt\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"></td>\n</tr>\n<tr id=\"S5.T6.st6.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st6.4.3.1.1\" class=\"ltx_text ltx_font_bold\">Party 1</span></td>\n<td id=\"S5.T6.st6.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st6.4.3.2.1\" class=\"ltx_text ltx_font_bold\">Party 2</span></td>\n<td id=\"S5.T6.st6.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st6.4.3.3.1\" class=\"ltx_text ltx_font_bold\">Party 3</span></td>\n<td id=\"S5.T6.st6.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st6.4.3.4.1\" class=\"ltx_text ltx_font_bold\">Party 4</span></td>\n<td id=\"S5.T6.st6.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><span id=\"S5.T6.st6.4.3.5.1\" class=\"ltx_text ltx_font_bold\">Range</span></td>\n</tr>\n<tr id=\"S5.T6.st6.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"4\"><span id=\"S5.T6.st6.4.4.1.1\" class=\"ltx_text\">Priv-Baseline</span></td>\n<td id=\"S5.T6.st6.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st6.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1668</td>\n<td id=\"S5.T6.st6.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1667</td>\n<td id=\"S5.T6.st6.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1783</td>\n<td id=\"S5.T6.st6.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1373</td>\n<td id=\"S5.T6.st6.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1373-0.1783</td>\n</tr>\n<tr id=\"S5.T6.st6.4.5\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st6.4.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1253</td>\n<td id=\"S5.T6.st6.4.5.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1387</td>\n<td id=\"S5.T6.st6.4.5.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1348</td>\n<td id=\"S5.T6.st6.4.5.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1246</td>\n<td id=\"S5.T6.st6.4.5.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1246-0.1387</td>\n</tr>\n<tr id=\"S5.T6.st6.4.6\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.6.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st6.4.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1173</td>\n<td id=\"S5.T6.st6.4.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1256</td>\n<td id=\"S5.T6.st6.4.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1299</td>\n<td id=\"S5.T6.st6.4.6.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1230</td>\n<td id=\"S5.T6.st6.4.6.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1173-0.1299</td>\n</tr>\n<tr id=\"S5.T6.st6.4.7\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st6.4.7.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1125</td>\n<td id=\"S5.T6.st6.4.7.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1140</td>\n<td id=\"S5.T6.st6.4.7.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1308</td>\n<td id=\"S5.T6.st6.4.7.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1148</td>\n<td id=\"S5.T6.st6.4.7.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1125-0.1308</td>\n</tr>\n<tr id=\"S5.T6.st6.4.8\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\" rowspan=\"5\"><span id=\"S5.T6.st6.4.8.1.1\" class=\"ltx_text\">FedOnce-L1</span></td>\n<td id=\"S5.T6.st6.4.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">2</td>\n<td id=\"S5.T6.st6.4.8.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1080</td>\n<td id=\"S5.T6.st6.4.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1043</td>\n<td id=\"S5.T6.st6.4.8.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1059</td>\n<td id=\"S5.T6.st6.4.8.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1110</td>\n<td id=\"S5.T6.st6.4.8.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1043-0.1110</td>\n</tr>\n<tr id=\"S5.T6.st6.4.9\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.9.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">4</td>\n<td id=\"S5.T6.st6.4.9.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1071</td>\n<td id=\"S5.T6.st6.4.9.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1003</td>\n<td id=\"S5.T6.st6.4.9.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1009</td>\n<td id=\"S5.T6.st6.4.9.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1052</td>\n<td id=\"S5.T6.st6.4.9.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1003-0.1052</td>\n</tr>\n<tr id=\"S5.T6.st6.4.10\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.10.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">6</td>\n<td id=\"S5.T6.st6.4.10.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0985</td>\n<td id=\"S5.T6.st6.4.10.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1001</td>\n<td id=\"S5.T6.st6.4.10.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0984</td>\n<td id=\"S5.T6.st6.4.10.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0973</td>\n<td id=\"S5.T6.st6.4.10.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0973-0.1001</td>\n</tr>\n<tr id=\"S5.T6.st6.4.11\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.11.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">8</td>\n<td id=\"S5.T6.st6.4.11.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1020</td>\n<td id=\"S5.T6.st6.4.11.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.1075</td>\n<td id=\"S5.T6.st6.4.11.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0985</td>\n<td id=\"S5.T6.st6.4.11.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0956</td>\n<td id=\"S5.T6.st6.4.11.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0956-0.1075</td>\n</tr>\n<tr id=\"S5.T6.st6.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.st6.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\"><math id=\"S5.T6.st6.4.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\infty\" display=\"inline\"><semantics id=\"S5.T6.st6.4.2.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T6.st6.4.2.1.m1.1.1\" xref=\"S5.T6.st6.4.2.1.m1.1.1.cmml\">∞</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.st6.4.2.1.m1.1b\"><infinity id=\"S5.T6.st6.4.2.1.m1.1.1.cmml\" xref=\"S5.T6.st6.4.2.1.m1.1.1\"></infinity></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.st6.4.2.1.m1.1c\">\\infty</annotation></semantics></math></td>\n<td id=\"S5.T6.st6.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0832</td>\n<td id=\"S5.T6.st6.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0833</td>\n<td id=\"S5.T6.st6.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0841</td>\n<td id=\"S5.T6.st6.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0801</td>\n<td id=\"S5.T6.st6.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.6pt;padding-right:2.6pt;\">0.0801-0.0841</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "S5.T7": {
        "caption": "TABLE VII: Performance of FedOnce on different unsupervised learning methods",
        "table": "<table id=\"S5.T7.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T7.4.1\" class=\"ltx_tr\">\n<td id=\"S5.T7.4.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T7.4.1.1.1\" class=\"ltx_text ltx_font_bold\">Feature Type</span></td>\n<td id=\"S5.T7.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T7.4.1.2.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S5.T7.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S5.T7.4.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></td>\n</tr>\n<tr id=\"S5.T7.4.2\" class=\"ltx_tr\">\n<td id=\"S5.T7.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T7.4.2.1.1\" class=\"ltx_text ltx_font_bold\">FedOnce-L0</span></td>\n<td id=\"S5.T7.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T7.4.2.2.1\" class=\"ltx_text ltx_font_bold\">FedOnce-PCA</span></td>\n</tr>\n<tr id=\"S5.T7.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T7.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S5.T7.4.3.1.1\" class=\"ltx_text\">Multi-variant</span></td>\n<td id=\"S5.T7.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">gisette</td>\n<td id=\"S5.T7.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">96.51%</td>\n<td id=\"S5.T7.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">94.10%</td>\n</tr>\n<tr id=\"S5.T7.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T7.4.4.1\" class=\"ltx_td ltx_align_center\">phishing</td>\n<td id=\"S5.T7.4.4.2\" class=\"ltx_td ltx_align_center\">92.78%</td>\n<td id=\"S5.T7.4.4.3\" class=\"ltx_td ltx_align_center\">92.41%</td>\n</tr>\n<tr id=\"S5.T7.4.5\" class=\"ltx_tr\">\n<td id=\"S5.T7.4.5.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" rowspan=\"2\"><span id=\"S5.T7.4.5.1.1\" class=\"ltx_text\">Image</span></td>\n<td id=\"S5.T7.4.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">MNIST</td>\n<td id=\"S5.T7.4.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">98.05%</td>\n<td id=\"S5.T7.4.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">25.98%</td>\n</tr>\n<tr id=\"S5.T7.4.6\" class=\"ltx_tr\">\n<td id=\"S5.T7.4.6.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">KMNIST</td>\n<td id=\"S5.T7.4.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">91.73%</td>\n<td id=\"S5.T7.4.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">25.59%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations."
        ]
    },
    "S5.T8": {
        "caption": "TABLE VIII: Performance of FedOnce-L0 with Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} on representations (bold values mean outperforming Solo)",
        "table": "<table id=\"S5.T8.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T8.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T8.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"3\"><span id=\"S5.T8.3.2.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></td>\n<td id=\"S5.T8.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"6\"><span id=\"S5.T8.3.2.2.1\" class=\"ltx_text ltx_font_bold\">Accuracy/RMSE</span></td>\n</tr>\n<tr id=\"S5.T8.3.1\" class=\"ltx_tr\">\n<td id=\"S5.T8.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" colspan=\"5\"><span id=\"S5.T8.3.1.1.1\" class=\"ltx_text ltx_font_bold\">FedOnce-L0 w/ different <math id=\"S5.T8.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma_{r}\" display=\"inline\"><semantics id=\"S5.T8.3.1.1.1.m1.1a\"><msub id=\"S5.T8.3.1.1.1.m1.1.1\" xref=\"S5.T8.3.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T8.3.1.1.1.m1.1.1.2\" xref=\"S5.T8.3.1.1.1.m1.1.1.2.cmml\">σ</mi><mi id=\"S5.T8.3.1.1.1.m1.1.1.3\" xref=\"S5.T8.3.1.1.1.m1.1.1.3.cmml\">r</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.3.1.1.1.m1.1b\"><apply id=\"S5.T8.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T8.3.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T8.3.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T8.3.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T8.3.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T8.3.1.1.1.m1.1.1.2\">𝜎</ci><ci id=\"S5.T8.3.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T8.3.1.1.1.m1.1.1.3\">𝑟</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.3.1.1.1.m1.1c\">\\sigma_{r}</annotation></semantics></math></span></td>\n<td id=\"S5.T8.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\" rowspan=\"2\"><span id=\"S5.T8.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Solo</span></td>\n</tr>\n<tr id=\"S5.T8.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T8.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.0</td>\n<td id=\"S5.T8.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.1</td>\n<td id=\"S5.T8.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.5</td>\n<td id=\"S5.T8.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.0</td>\n<td id=\"S5.T8.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">1.5</td>\n</tr>\n<tr id=\"S5.T8.3.4\" class=\"ltx_tr\">\n<td id=\"S5.T8.3.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">NUS-WIDE</td>\n<td id=\"S5.T8.3.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T8.3.4.2.1\" class=\"ltx_text ltx_font_bold\">84.76%</span></td>\n<td id=\"S5.T8.3.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T8.3.4.3.1\" class=\"ltx_text ltx_font_bold\">84.64%</span></td>\n<td id=\"S5.T8.3.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T8.3.4.4.1\" class=\"ltx_text ltx_font_bold\">82.75%</span></td>\n<td id=\"S5.T8.3.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T8.3.4.5.1\" class=\"ltx_text ltx_font_bold\">81.45%</span></td>\n<td id=\"S5.T8.3.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">81.05%</td>\n<td id=\"S5.T8.3.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">80.71%</td>\n</tr>\n<tr id=\"S5.T8.3.5\" class=\"ltx_tr\">\n<td id=\"S5.T8.3.5.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">MovieLens</td>\n<td id=\"S5.T8.3.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T8.3.5.2.1\" class=\"ltx_text ltx_font_bold\">0.9373</span></td>\n<td id=\"S5.T8.3.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T8.3.5.3.1\" class=\"ltx_text ltx_font_bold\">0.9578</span></td>\n<td id=\"S5.T8.3.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T8.3.5.4.1\" class=\"ltx_text ltx_font_bold\">0.9690</span></td>\n<td id=\"S5.T8.3.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\"><span id=\"S5.T8.3.5.5.1\" class=\"ltx_text ltx_font_bold\">0.9679</span></td>\n<td id=\"S5.T8.3.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.9969</td>\n<td id=\"S5.T8.3.5.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:4.0pt;padding-right:4.0pt;\">0.9835</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0)."
        ]
    },
    "S6.T9": {
        "caption": "",
        "table": "<table id=\"S6.T9.1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S6.T9.1.1.2\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T9.1.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S6.T9.1.1.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T9.1.1.2.2.1\" class=\"ltx_text ltx_font_bold\">Label Owner<sup id=\"S6.T9.1.1.2.2.1.1\" class=\"ltx_sup\"><span id=\"S6.T9.1.1.2.2.1.1.1\" class=\"ltx_text ltx_font_medium\">1</span></sup></span></td>\n<td id=\"S6.T9.1.1.2.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T9.1.1.2.3.1\" class=\"ltx_text ltx_font_bold\">Privacy<sup id=\"S6.T9.1.1.2.3.1.1\" class=\"ltx_sup\"><span id=\"S6.T9.1.1.2.3.1.1.1\" class=\"ltx_text ltx_font_medium\">2</span></sup></span></td>\n<td id=\"S6.T9.1.1.2.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T9.1.1.2.4.1\" class=\"ltx_text ltx_font_bold\">Model<sup id=\"S6.T9.1.1.2.4.1.1\" class=\"ltx_sup\"><span id=\"S6.T9.1.1.2.4.1.1.1\" class=\"ltx_text ltx_font_medium\">3</span></sup></span></td>\n<td id=\"S6.T9.1.1.2.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S6.T9.1.1.2.5.1\" class=\"ltx_text ltx_font_bold\">Comm. Rounds<sup id=\"S6.T9.1.1.2.5.1.1\" class=\"ltx_sup\"><span id=\"S6.T9.1.1.2.5.1.1.1\" class=\"ltx_text ltx_font_medium\">4</span></sup></span></td>\n</tr>\n<tr id=\"S6.T9.1.1.3\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FDML <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite>\n</td>\n<td id=\"S6.T9.1.1.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">All</td>\n<td id=\"S6.T9.1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">N/A</td>\n<td id=\"S6.T9.1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">NN</td>\n<td id=\"S6.T9.1.1.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">Multi</td>\n</tr>\n<tr id=\"S6.T9.1.1.4\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.4.1\" class=\"ltx_td ltx_align_center\">SplitNN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n<td id=\"S6.T9.1.1.4.2\" class=\"ltx_td ltx_align_center\">Single</td>\n<td id=\"S6.T9.1.1.4.3\" class=\"ltx_td ltx_align_center\">N/A</td>\n<td id=\"S6.T9.1.1.4.4\" class=\"ltx_td ltx_align_center\">NN</td>\n<td id=\"S6.T9.1.1.4.5\" class=\"ltx_td ltx_align_center\">Multi</td>\n</tr>\n<tr id=\"S6.T9.1.1.5\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.5.1\" class=\"ltx_td ltx_align_center\">SecureBoost <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>\n</td>\n<td id=\"S6.T9.1.1.5.2\" class=\"ltx_td ltx_align_center\">Single</td>\n<td id=\"S6.T9.1.1.5.3\" class=\"ltx_td ltx_align_center\">HE</td>\n<td id=\"S6.T9.1.1.5.4\" class=\"ltx_td ltx_align_center\">GBDT</td>\n<td id=\"S6.T9.1.1.5.5\" class=\"ltx_td ltx_align_center\">Multi</td>\n</tr>\n<tr id=\"S6.T9.1.1.6\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.6.1\" class=\"ltx_td ltx_align_center\">Pivot<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">12</a>]</cite><sup id=\"S6.T9.1.1.6.1.1\" class=\"ltx_sup\">5</sup>\n</td>\n<td id=\"S6.T9.1.1.6.2\" class=\"ltx_td ltx_align_center\">Single</td>\n<td id=\"S6.T9.1.1.6.3\" class=\"ltx_td ltx_align_center\">MPC/HE</td>\n<td id=\"S6.T9.1.1.6.4\" class=\"ltx_td ltx_align_center\">GBDT/LR</td>\n<td id=\"S6.T9.1.1.6.5\" class=\"ltx_td ltx_align_center\">Multi</td>\n</tr>\n<tr id=\"S6.T9.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.1.1\" class=\"ltx_td ltx_align_center\">VF<sup id=\"S6.T9.1.1.1.1.1\" class=\"ltx_sup\">2</sup>Boost<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite><sup id=\"S6.T9.1.1.1.1.2\" class=\"ltx_sup\">5</sup>\n</td>\n<td id=\"S6.T9.1.1.1.2\" class=\"ltx_td ltx_align_center\">Single</td>\n<td id=\"S6.T9.1.1.1.3\" class=\"ltx_td ltx_align_center\">HE</td>\n<td id=\"S6.T9.1.1.1.4\" class=\"ltx_td ltx_align_center\">GBDT</td>\n<td id=\"S6.T9.1.1.1.5\" class=\"ltx_td ltx_align_center\">Multi</td>\n</tr>\n<tr id=\"S6.T9.1.1.7\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.7.1\" class=\"ltx_td ltx_align_center\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">24</a>, <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite></td>\n<td id=\"S6.T9.1.1.7.2\" class=\"ltx_td ltx_align_center\">Single</td>\n<td id=\"S6.T9.1.1.7.3\" class=\"ltx_td ltx_align_center\">DP-S</td>\n<td id=\"S6.T9.1.1.7.4\" class=\"ltx_td ltx_align_center\">FW</td>\n<td id=\"S6.T9.1.1.7.5\" class=\"ltx_td ltx_align_center\">Multi</td>\n</tr>\n<tr id=\"S6.T9.1.1.8\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.8.1\" class=\"ltx_td ltx_align_center\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">25</a>]</cite></td>\n<td id=\"S6.T9.1.1.8.2\" class=\"ltx_td ltx_align_center\">Single</td>\n<td id=\"S6.T9.1.1.8.3\" class=\"ltx_td ltx_align_center\">DP-S</td>\n<td id=\"S6.T9.1.1.8.4\" class=\"ltx_td ltx_align_center\">LR/HTL</td>\n<td id=\"S6.T9.1.1.8.5\" class=\"ltx_td ltx_align_center\">Multi</td>\n</tr>\n<tr id=\"S6.T9.1.1.9\" class=\"ltx_tr\">\n<td id=\"S6.T9.1.1.9.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T9.1.1.9.1.1\" class=\"ltx_text ltx_font_bold\">FedOnce</span></td>\n<td id=\"S6.T9.1.1.9.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T9.1.1.9.2.1\" class=\"ltx_text ltx_font_bold\">Single</span></td>\n<td id=\"S6.T9.1.1.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T9.1.1.9.3.1\" class=\"ltx_text ltx_font_bold\">DP-M</span></td>\n<td id=\"S6.T9.1.1.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T9.1.1.9.4.1\" class=\"ltx_text ltx_font_bold\">NN</span></td>\n<td id=\"S6.T9.1.1.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T9.1.1.9.5.1\" class=\"ltx_text ltx_font_bold\">Single</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "1 The number of parties that own the labels. Single: Only one party owns the labels, All: all parties own the labels \n2 Privacy mechanism used in the algorithm. N/A: Not specified in details, HE: homomorphic encryption, MPC: secure multi-party computation, DP-S: differential privacy with simple division among parties, DP-M: differential privacy with moments division among parties; \n3 Model supported by the algorithm. NN: neural networks, GBDT: gradient boosting decision trees, LR: logistic regression, FW: Floyd–Warshall algorithm, HTL: hypothesis transfer learning. \n4 Communication rounds required for the algorithm. Multi: multiple rounds are required, Single: single round is required. \n5 Pivot [12] and VF2Boost [11] have the same accuracy as SecureBoost with additional techniques on encryption.",
        "references": [
            "Considering the model training, communication issues have limited the practical adoption of vertical federated learning in many real-world applications [9]. For example, such limitation becomes non-negligible when parties are 1) mobile devices communicating through Cellular networks, 2) IoT devices communicating through Wi-Fi networks, 3) seagoing ships communicating through satellites. These applications, featuring unstable connections and expensive communication cost, cannot be handled by existing vertical federated learning algorithms for two major reasons, including 1) high stability requirement: existing vertical federated learning approaches require all the parties to stay online or even synchronous during the entire training process, which is unrealistic for parties connected by unstable networks; 2) high communication cost: current studies in vertical federated learning usually incur large communication overhead, leading to the considerable economic cost. For example, in each training iteration, SecureBoost [10], VF2Boost [11], and Pivot [12] exchange encrypted intermediate results (e,g, gradients) between the host party and guest parties. Similarly, when training each batch of data, SplitNN [13] transfers outputs and gradients of a common layer between the host party and guest parties. All these approaches require batch/iteration-level synchronization and high communication cost.",
            "FedOnce does not rely on specific unsupervised learning algorithms. In this paper, we choose NAT [18], which is a simple and effective unsupervised learning algorithm suitable for both image datasets and multivariate datasets. In NAT, in order to learn a representation R:n×d:𝑅𝑛𝑑R:n\\times d of n𝑛n samples with d𝑑d dimensions, a random representation C:n×d:𝐶𝑛𝑑C:n\\times d is generated. Denoting the neural network model to generate R𝑅R as fθ​(⋅)subscript𝑓𝜃⋅f_{\\theta}(\\cdot), the goal of NAT can be expressed as the Equation 1.",
            "SplitNN [13] is a simple and effective vertical federated learning222Though some studies categorize SplitNN as split learning, we denote SplitNN as a vertical federated learning algorithm for simplicity. model based on neural network. As explained in Fig. 2, models are split into multiple parties. Each guest party holds a local model; the host party holds a local model and an aggregation model. In forward propagation, first, all the parties forward propagate independently using their local models. Then, the outputs of local models on guest parties are sent to the host party. The host party concatenates the outputs from all the parties (including itself) and feeds the concatenated output into the aggregation model θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The aggregation model continues forward propagating to produce the final prediction. In the backpropagation, first, the loss and gradients are calculated based on the final prediction. Then, the gradients w.r.t. to θ1,θ2,θ3subscript𝜃1subscript𝜃2subscript𝜃3\\theta_{1},\\theta_{2},\\theta_{3} are calculated by backpropagating through θa​g​gsubscript𝜃𝑎𝑔𝑔\\theta_{agg}. The gradients w.r.t. θ2,θ3subscript𝜃2subscript𝜃3\\theta_{2},\\theta_{3} are sent to two guest parties, respectively. Finally, all the parties perform backpropagation on their local models. In each iteration of training, both forward propagation and backpropagation are performed for one time. Therefore, the training incurs two communication rounds between the host party and each guest party every iteration, which is impractical when parties are connected by unstable and expensive networks. This pitfall of SplitNN motivates our design of a one-shot vertical federated learning framework.",
            "Datasets. We evaluate FedOnce on eight public datasets and two real-world federated datasets, whose details are summarized in Table I. Public datasets: Public datasets are used to simulate the scenario where each party has the same number of features. gisette, phishing, and covtype are binary classification datasets obtained from LIBSVM333https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets. UJIIndoorLoc and Superconduct are regression datasets obtained from UCI444https://archive.ics.uci.edu/ml/datasets.php. MNIST, KMNIST and Fashion-MNIST are multi-class classification datasets obtained from TorchVision555https://pytorch.org/docs/stable/torchvision/datasets.html. Real-world federated datasets: 1) NUS-WIDE [37] is a real-world multi-view image datasets, used to simulate a security company training a model based on image data collected from different surveillance cameras. In NUS-WIDE, each image is stored in the form of five types of low-level features. NUS-WIDE provides 81 labels, among which we pick the most occurring label sky to conduct binary classification. 2) MovieLens [38] is a real-world recommendation dataset, used to simulate a film production company training a recommendation model based on the data from a movie rating website and a movie streaming website. MovieLens contains 9992 one-hot identity features and 133 auxiliary features. We set movie ratings as labels and regard the task as regression.",
            "Baselines.\nTo evaluate the communication efficiency of FedOnce-L0, we adopt five baselines: 1) Solo: each party trains locally with its own data and real labels. 2) Combine: all the data and labels are trained centrally. 3) SplitNN [13]: a vertical federated learning algorithm for neural networks. 4) SecureBoost [10]: a vertical federated learning framework for gradient boosting decision trees (GBDT). Notably, VF2Boost [11] and Pivot [12], producing the same accuracy as SecureBoost with additional techniques on encryption, are not individually compared. Since FedOnce-L0 provides no protection on the released model during the training process, for a fair comparison, we ignore the cryptographic overhead when calculating the communication size of SecureBoost. 5) Linear-Combine: a linear model is trained on centralized datasets. Specifically, we train logistic regression for classification tasks and train ridge regression for regression tasks on centralized datasets similarly to Combine. Among these baselines, SecureBoost is not evaluated on MNIST, KMNIST and Fashion-MNIST since GBDT is unsuitable for 2D features.",
            "Models.\nThe models used for FedOnce-L0 and FedOnce-L1 on each dataset are summarized in Table II and Table III, respectively. The sizes of hidden layers are carefully tuned for each dataset. FC(m×n𝑚𝑛m\\times n) indicates fully connected layers with two hidden layers which have m𝑚m and n𝑛n nodes, respectively. CNN0 and CNN1 indicates two kinds of convolutional neural networks whose structures are shown in Fig. 4. NCF(m×n𝑚𝑛m\\times n) indicates neural collaborative filtering [41] with two hidden layers which have m𝑚m and n𝑛n nodes, respectively.",
            "Hyperparameters For each dataset, we summarize the hyperparameters of FedOnce-L0 in Table V and hyperparameters of FedOnce-L1 in Appendix A. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs.",
            "Training Time.\nFor each dataset, we record the training time of SplitNN and FedOnce-L0 in Table IV. As observed from the table, FedOnce-L0 generally has a lower training time than SplitNN; this is because the local models in FedOnce can be trained in parallel, while all the local models are connected with the aggregation model and trained in each iteration in SplitNN. We also highlight that the communication time is measured in a shared-memory environment of a single machine. In reality, the communication time can also be a huge burden for SplitNN which requires much more communication size as demonstrated in Fig. 5.",
            "From Fig. 8, we can make two observations. First, as the number of parties k𝑘k increases, FedOnce-L0, whose performance remains stable while the performance of Solo degrades rapidly, is scalable. Second, even at a large number of parties, FedOnce consistently outperforms SecureBoost and SplitNN with the same communication size.",
            "FedOnce is suitable for different unsupervised learning methods. Among these unsupervised learning methods, we compare the performance of FedOnce with NAT [18] (FedOnce-L0) and Principal Component Analysis [42] (FedOnce-PCA). Specifically, fixing both the dimensions of representative features and the number of principal components the same, we present the results of four typical datasets covering image features and multi-variant features in Table VII. Without loss of generality, party 𝒫1subscript𝒫1\\mathcal{P}_{1} is selected as the host party. The choice of K𝐾K is identical to that in Section 5.2.",
            "From Table VII, we observe that FedOnce-PCA can achieve close performance to FedOnce-L0 on multi-variant datasets, but has very poor performance on image datasets. This is because PCA is incapable to extract useful representations from complex features like images. On the contrary, NAT is a suitable unsupervised learning method for FedOnce that can handle different types of features. Generally, more “advanced” unsupervised learning algorithms tend to produce a better performance on FedOnce. The performance of unsupervised learning methods can be evaluated by commonly used metrics in the literature. For example, NAT is evaluated by the performance of a linear classifier on the learned representations.",
            "Performance. Despite the much lower privacy loss compared to simple division, FedOnce-L1 still suffers significant performance loss at a large k𝑘k like other vertical federated learning algorithms. Therefore, we set k=4𝑘4k=4 and report the performance under different overall privacy budgets ε𝜀\\varepsilon on six public datasets. Fashion-MNIST and KMNIST, which fail to produce reasonable accuracy, are not included in this experiment. The results are summarized in Table VI.",
            "Two observations can be made from Table VI. First, FedOnce-L1 significantly outperforms Priv-Baseline with the same privacy budget ε𝜀\\varepsilon. Second, compared with FedOnce-L1 with ε=∞𝜀\\varepsilon=\\infty (no noise is added), FedOnce-L1 has a close performance under a modest ε𝜀\\varepsilon in most datasets. Additionally, the performance of FedOnce-L1 with a small ε𝜀\\varepsilon (e.g., ε=2𝜀2\\varepsilon=2) can be significantly affected by the noise, implying more advanced privacy mechanisms are desired.",
            "In this experiment, to preserve the privacy of representations, we investigate the effect of adding noise to representations on the performance of FedOnce-L0. Specifically, independent Gaussian noise of scale σrsubscript𝜎𝑟\\sigma_{r} is added to all the representations. We report the average performance of all the parties on two real-world federated datasets in Table VIII.",
            "As can be observed from Table VIII, the performance of FedOnce-L0 drops below Solo at a relatively large scale of noise (e.g., σr=1.5subscript𝜎𝑟1.5\\sigma_{r}=1.5) on both datasets. This observation indicates that techniques like privacy-preserving data releasing can potentially be used to protect representations, whereas the added noise must be restricted to a small scale (e.g., σr<1.0subscript𝜎𝑟1.0\\sigma_{r}<1.0).",
            "In this section, we review the literature in three aspects and summarize the related work in Table IX.",
            "Nevertheless, existing studies on global differential privacy under vertical federated learning suffer significant performance loss. One study [48] achieves differential privacy by objective perturbation, which is often intractable in practice compared to gradient perturbation according to [49]. The other two studies [24, 25] apply differential privacy based on gradient perturbation, but they both focus on inner-party privacy instead of inter-party privacy. Specifically, in both studies, the simple composition is directly applied in the analysis of privacy loss across parties, leading to excessive noise. In this paper, we apply moments accountant to analyze the inter-party privacy loss, thus effectively reducing the overall privacy loss compared to simple composition.",
            "Communication-Efficient Federated Learning.\nMost existing studies in communication-efficient federated learning [50, 51, 52, 14, 15] focus on horizontal federated learning. These approaches, requiring each party to train independently, cannot be applied to vertical federated learning where only one party holds the labels. Though some approaches [40, 53, 10] study the communication efficiency in vertical federated learning, they all require multiple communication rounds and a certain level of synchronization. As observed from Table IX, vertical federated learning with one-shot communication remains unexplored.",
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    },
    "A1.T10": {
        "caption": "TABLE X: Hyperparameters of FedOnce-L1 on each dataset",
        "table": "<table id=\"A1.T10.12\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A1.T10.4.4\" class=\"ltx_tr\">\n<td id=\"A1.T10.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"2\"><span id=\"A1.T10.4.4.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Dataset</span></td>\n<td id=\"A1.T10.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"2\"><span id=\"A1.T10.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"><math id=\"A1.T10.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\varepsilon\" display=\"inline\"><semantics id=\"A1.T10.1.1.1.1.m1.1a\"><mi id=\"A1.T10.1.1.1.1.m1.1.1\" xref=\"A1.T10.1.1.1.1.m1.1.1.cmml\">ε</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.1.1.1.1.m1.1b\"><ci id=\"A1.T10.1.1.1.1.m1.1.1.cmml\" xref=\"A1.T10.1.1.1.1.m1.1.1\">𝜀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.1.1.1.1.m1.1c\">\\varepsilon</annotation></semantics></math></span></td>\n<td id=\"A1.T10.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"4\"><span id=\"A1.T10.4.4.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Guest Model</span></td>\n<td id=\"A1.T10.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" colspan=\"4\"><span id=\"A1.T10.4.4.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Host Model</span></td>\n<td id=\"A1.T10.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"2\"><span id=\"A1.T10.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\"><math id=\"A1.T10.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"d\" display=\"inline\"><semantics id=\"A1.T10.2.2.2.1.m1.1a\"><mi id=\"A1.T10.2.2.2.1.m1.1.1\" xref=\"A1.T10.2.2.2.1.m1.1.1.cmml\">d</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.2.2.2.1.m1.1b\"><ci id=\"A1.T10.2.2.2.1.m1.1.1.cmml\" xref=\"A1.T10.2.2.2.1.m1.1.1\">𝑑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.2.2.2.1.m1.1c\">d</annotation></semantics></math></span></td>\n<td id=\"A1.T10.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"2\"><span id=\"A1.T10.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\"><math id=\"A1.T10.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"f\" display=\"inline\"><semantics id=\"A1.T10.3.3.3.1.m1.1a\"><mi id=\"A1.T10.3.3.3.1.m1.1.1\" xref=\"A1.T10.3.3.3.1.m1.1.1.cmml\">f</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.3.3.3.1.m1.1b\"><ci id=\"A1.T10.3.3.3.1.m1.1.1.cmml\" xref=\"A1.T10.3.3.3.1.m1.1.1\">𝑓</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.3.3.3.1.m1.1c\">f</annotation></semantics></math></span></td>\n<td id=\"A1.T10.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"2\"><span id=\"A1.T10.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\"><math id=\"A1.T10.4.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\Omega\" display=\"inline\"><semantics id=\"A1.T10.4.4.4.1.m1.1a\"><mi mathvariant=\"normal\" id=\"A1.T10.4.4.4.1.m1.1.1\" xref=\"A1.T10.4.4.4.1.m1.1.1.cmml\">Ω</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.4.4.4.1.m1.1b\"><ci id=\"A1.T10.4.4.4.1.m1.1.1.cmml\" xref=\"A1.T10.4.4.4.1.m1.1.1\">Ω</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.4.4.4.1.m1.1c\">\\Omega</annotation></semantics></math></span></td>\n</tr>\n<tr id=\"A1.T10.12.12\" class=\"ltx_tr\">\n<td id=\"A1.T10.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"A1.T10.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\eta\" display=\"inline\"><semantics id=\"A1.T10.5.5.1.m1.1a\"><mi mathsize=\"90%\" id=\"A1.T10.5.5.1.m1.1.1\" xref=\"A1.T10.5.5.1.m1.1.1.cmml\">η</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.5.5.1.m1.1b\"><ci id=\"A1.T10.5.5.1.m1.1.1.cmml\" xref=\"A1.T10.5.5.1.m1.1.1\">𝜂</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.5.5.1.m1.1c\">\\eta</annotation></semantics></math></td>\n<td id=\"A1.T10.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"A1.T10.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"A1.T10.6.6.2.m1.1a\"><mi mathsize=\"90%\" id=\"A1.T10.6.6.2.m1.1.1\" xref=\"A1.T10.6.6.2.m1.1.1.cmml\">λ</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.6.6.2.m1.1b\"><ci id=\"A1.T10.6.6.2.m1.1.1.cmml\" xref=\"A1.T10.6.6.2.m1.1.1\">𝜆</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.6.6.2.m1.1c\">\\lambda</annotation></semantics></math></td>\n<td id=\"A1.T10.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"A1.T10.7.7.3.m1.1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics id=\"A1.T10.7.7.3.m1.1a\"><mi mathsize=\"90%\" id=\"A1.T10.7.7.3.m1.1.1\" xref=\"A1.T10.7.7.3.m1.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.7.7.3.m1.1b\"><ci id=\"A1.T10.7.7.3.m1.1.1.cmml\" xref=\"A1.T10.7.7.3.m1.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.7.7.3.m1.1c\">b</annotation></semantics></math></td>\n<td id=\"A1.T10.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"A1.T10.8.8.4.m1.1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics id=\"A1.T10.8.8.4.m1.1a\"><mi mathsize=\"90%\" id=\"A1.T10.8.8.4.m1.1.1\" xref=\"A1.T10.8.8.4.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.8.8.4.m1.1b\"><ci id=\"A1.T10.8.8.4.m1.1.1.cmml\" xref=\"A1.T10.8.8.4.m1.1.1\">𝑇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.8.8.4.m1.1c\">T</annotation></semantics></math></td>\n<td id=\"A1.T10.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"A1.T10.9.9.5.m1.1\" class=\"ltx_Math\" alttext=\"\\eta\" display=\"inline\"><semantics id=\"A1.T10.9.9.5.m1.1a\"><mi mathsize=\"90%\" id=\"A1.T10.9.9.5.m1.1.1\" xref=\"A1.T10.9.9.5.m1.1.1.cmml\">η</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.9.9.5.m1.1b\"><ci id=\"A1.T10.9.9.5.m1.1.1.cmml\" xref=\"A1.T10.9.9.5.m1.1.1\">𝜂</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.9.9.5.m1.1c\">\\eta</annotation></semantics></math></td>\n<td id=\"A1.T10.10.10.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"A1.T10.10.10.6.m1.1\" class=\"ltx_Math\" alttext=\"\\lambda\" display=\"inline\"><semantics id=\"A1.T10.10.10.6.m1.1a\"><mi mathsize=\"90%\" id=\"A1.T10.10.10.6.m1.1.1\" xref=\"A1.T10.10.10.6.m1.1.1.cmml\">λ</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.10.10.6.m1.1b\"><ci id=\"A1.T10.10.10.6.m1.1.1.cmml\" xref=\"A1.T10.10.10.6.m1.1.1\">𝜆</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.10.10.6.m1.1c\">\\lambda</annotation></semantics></math></td>\n<td id=\"A1.T10.11.11.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"A1.T10.11.11.7.m1.1\" class=\"ltx_Math\" alttext=\"b\" display=\"inline\"><semantics id=\"A1.T10.11.11.7.m1.1a\"><mi mathsize=\"90%\" id=\"A1.T10.11.11.7.m1.1.1\" xref=\"A1.T10.11.11.7.m1.1.1.cmml\">b</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.11.11.7.m1.1b\"><ci id=\"A1.T10.11.11.7.m1.1.1.cmml\" xref=\"A1.T10.11.11.7.m1.1.1\">𝑏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.11.11.7.m1.1c\">b</annotation></semantics></math></td>\n<td id=\"A1.T10.12.12.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><math id=\"A1.T10.12.12.8.m1.1\" class=\"ltx_Math\" alttext=\"T\" display=\"inline\"><semantics id=\"A1.T10.12.12.8.m1.1a\"><mi mathsize=\"90%\" id=\"A1.T10.12.12.8.m1.1.1\" xref=\"A1.T10.12.12.8.m1.1.1.cmml\">T</mi><annotation-xml encoding=\"MathML-Content\" id=\"A1.T10.12.12.8.m1.1b\"><ci id=\"A1.T10.12.12.8.m1.1.1.cmml\" xref=\"A1.T10.12.12.8.m1.1.1\">𝑇</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T10.12.12.8.m1.1c\">T</annotation></semantics></math></td>\n</tr>\n<tr id=\"A1.T10.12.13\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.13.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"4\"><span id=\"A1.T10.12.13.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">gisette</span></td>\n<td id=\"A1.T10.12.13.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2</span></td>\n<td id=\"A1.T10.12.13.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.13.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.13.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.13.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.13.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.13.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.13.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.13.10\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.13.11\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n<td id=\"A1.T10.12.13.12\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.13.13\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.13.13.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.0</span></td>\n</tr>\n<tr id=\"A1.T10.12.14\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.14.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n<td id=\"A1.T10.12.14.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.2</span></td>\n<td id=\"A1.T10.12.14.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.14.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.14.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.14.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.2</span></td>\n<td id=\"A1.T10.12.14.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.14.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.14.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">30</span></td>\n<td id=\"A1.T10.12.14.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n<td id=\"A1.T10.12.14.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.14.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.14.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.0</span></td>\n</tr>\n<tr id=\"A1.T10.12.15\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.15.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.15.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.15.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.15.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.15.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.15.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.6</span></td>\n<td id=\"A1.T10.12.15.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.15.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.15.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">15</span></td>\n<td id=\"A1.T10.12.15.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n<td id=\"A1.T10.12.15.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.15.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.15.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.16\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.16.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td id=\"A1.T10.12.16.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.16.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.16.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.16.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.16.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.6</span></td>\n<td id=\"A1.T10.12.16.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.16.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.16.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">40</span></td>\n<td id=\"A1.T10.12.16.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n<td id=\"A1.T10.12.16.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.16.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.16.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.17\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.17.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"4\"><span id=\"A1.T10.12.17.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">phishing</span></td>\n<td id=\"A1.T10.12.17.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2</span></td>\n<td id=\"A1.T10.12.17.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.17.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.17.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.17.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.17.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.17.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.17.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.17.10\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">30</span></td>\n<td id=\"A1.T10.12.17.11\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n<td id=\"A1.T10.12.17.12\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.17.13\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.17.13.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.0</span></td>\n</tr>\n<tr id=\"A1.T10.12.18\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.18.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n<td id=\"A1.T10.12.18.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.2</span></td>\n<td id=\"A1.T10.12.18.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.18.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.18.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.18.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.2</span></td>\n<td id=\"A1.T10.12.18.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.18.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.18.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">30</span></td>\n<td id=\"A1.T10.12.18.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n<td id=\"A1.T10.12.18.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.18.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.18.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.0</span></td>\n</tr>\n<tr id=\"A1.T10.12.19\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.19.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.19.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.19.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.19.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.19.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.19.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.1</span></td>\n<td id=\"A1.T10.12.19.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.19.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">32</span></td>\n<td id=\"A1.T10.12.19.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">30</span></td>\n<td id=\"A1.T10.12.19.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n<td id=\"A1.T10.12.19.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.19.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.19.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.0</span></td>\n</tr>\n<tr id=\"A1.T10.12.20\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.20.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td id=\"A1.T10.12.20.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.20.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.20.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.20.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.20.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.20.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.20.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.20.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">40</span></td>\n<td id=\"A1.T10.12.20.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">3</span></td>\n<td id=\"A1.T10.12.20.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.20.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.20.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.21\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.21.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"4\"><span id=\"A1.T10.12.21.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"><span id=\"A1.T10.12.21.1.1.1\" class=\"ltx_text\"></span> <span id=\"A1.T10.12.21.1.1.2\" class=\"ltx_text\">\n<span id=\"A1.T10.12.21.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"A1.T10.12.21.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"A1.T10.12.21.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">MNIST</span></span>\n<span id=\"A1.T10.12.21.1.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"A1.T10.12.21.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fashion-MNIST</span></span>\n</span></span> <span id=\"A1.T10.12.21.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"A1.T10.12.21.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2</span></td>\n<td id=\"A1.T10.12.21.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td id=\"A1.T10.12.21.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.21.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">256</span></td>\n<td id=\"A1.T10.12.21.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.21.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.8</span></td>\n<td id=\"A1.T10.12.21.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.21.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">256</span></td>\n<td id=\"A1.T10.12.21.10\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">15</span></td>\n<td id=\"A1.T10.12.21.11\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td id=\"A1.T10.12.21.12\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.21.13\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.21.13.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.22\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.22.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n<td id=\"A1.T10.12.22.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td id=\"A1.T10.12.22.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.22.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">256</span></td>\n<td id=\"A1.T10.12.22.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.22.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.9</span></td>\n<td id=\"A1.T10.12.22.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.22.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">256</span></td>\n<td id=\"A1.T10.12.22.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">15</span></td>\n<td id=\"A1.T10.12.22.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td id=\"A1.T10.12.22.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.22.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.22.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.23\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.23.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.23.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td id=\"A1.T10.12.23.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.23.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">256</span></td>\n<td id=\"A1.T10.12.23.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">7</span></td>\n<td id=\"A1.T10.12.23.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.9</span></td>\n<td id=\"A1.T10.12.23.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.23.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">256</span></td>\n<td id=\"A1.T10.12.23.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">15</span></td>\n<td id=\"A1.T10.12.23.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td id=\"A1.T10.12.23.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.23.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.23.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.24\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.24.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td id=\"A1.T10.12.24.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td id=\"A1.T10.12.24.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.24.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">512</span></td>\n<td id=\"A1.T10.12.24.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.24.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.9</span></td>\n<td id=\"A1.T10.12.24.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.24.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">512</span></td>\n<td id=\"A1.T10.12.24.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">30</span></td>\n<td id=\"A1.T10.12.24.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td id=\"A1.T10.12.24.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.24.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.24.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.25\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.25.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\" rowspan=\"4\"><span id=\"A1.T10.12.25.1.1\" class=\"ltx_text\" style=\"font-size:90%;\"><span id=\"A1.T10.12.25.1.1.1\" class=\"ltx_text\"></span> <span id=\"A1.T10.12.25.1.1.2\" class=\"ltx_text\">\n<span id=\"A1.T10.12.25.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"A1.T10.12.25.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"A1.T10.12.25.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">UJIIndoorLoc</span></span>\n<span id=\"A1.T10.12.25.1.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"A1.T10.12.25.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Superconduct</span></span>\n</span></span> <span id=\"A1.T10.12.25.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"A1.T10.12.25.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2</span></td>\n<td id=\"A1.T10.12.25.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.4</span></td>\n<td id=\"A1.T10.12.25.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.25.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.25.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.25.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td id=\"A1.T10.12.25.8\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.25.9\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.25.10\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">10</span></td>\n<td id=\"A1.T10.12.25.11\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.25.12\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.25.13\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.25.13.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.26\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.26.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">4</span></td>\n<td id=\"A1.T10.12.26.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.4</span></td>\n<td id=\"A1.T10.12.26.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.26.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.26.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">12</span></td>\n<td id=\"A1.T10.12.26.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td id=\"A1.T10.12.26.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.26.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.26.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">20</span></td>\n<td id=\"A1.T10.12.26.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.26.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.26.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.26.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.27\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.27.1\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.27.2\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.4</span></td>\n<td id=\"A1.T10.12.27.3\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.27.4\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.27.5\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">12</span></td>\n<td id=\"A1.T10.12.27.6\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.5</span></td>\n<td id=\"A1.T10.12.27.7\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.27.8\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.27.9\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">30</span></td>\n<td id=\"A1.T10.12.27.10\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.27.11\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.27.12\" class=\"ltx_td ltx_align_center\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.27.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n<tr id=\"A1.T10.12.28\" class=\"ltx_tr\">\n<td id=\"A1.T10.12.28.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">8</span></td>\n<td id=\"A1.T10.12.28.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.4</span></td>\n<td id=\"A1.T10.12.28.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.28.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.28.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">12</span></td>\n<td id=\"A1.T10.12.28.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.3</span></td>\n<td id=\"A1.T10.12.28.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0</span></td>\n<td id=\"A1.T10.12.28.8\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.8.1\" class=\"ltx_text\" style=\"font-size:90%;\">128</span></td>\n<td id=\"A1.T10.12.28.9\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.9.1\" class=\"ltx_text\" style=\"font-size:90%;\">60</span></td>\n<td id=\"A1.T10.12.28.10\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.10.1\" class=\"ltx_text\" style=\"font-size:90%;\">6</span></td>\n<td id=\"A1.T10.12.28.11\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.11.1\" class=\"ltx_text\" style=\"font-size:90%;\">1</span></td>\n<td id=\"A1.T10.12.28.12\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T10.12.28.12.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.5</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "For each dataset, we summarize the hyperparameters of FedOnce-L1 in Table X. In the table, η𝜂\\eta refers to the learning rate, λ𝜆\\lambda refers to weight decay, b𝑏b refers to batch size, T𝑇T refers to the number of epochs, d𝑑d refers to the dimension of representations. f𝑓f indicates the frequency of permutation matrix P𝑃P to be updated. For example, if the update frequency is 3, P𝑃P will be updated every three epochs. ε𝜀\\varepsilon refers to the overall privacy budget. ΩΩ\\Omega refers to the clipping norm. SGD refers to stochastic gradient descent without momentum, i.e., m​o​m​e​n​t​u​m=0𝑚𝑜𝑚𝑒𝑛𝑡𝑢𝑚0momentum=0. We adopt the SGD optimizer in FedOnce-L1 because our analysis of differential privacy is based on SGD."
        ]
    }
}