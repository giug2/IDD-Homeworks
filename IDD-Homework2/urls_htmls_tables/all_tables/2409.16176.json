{
    "id_table_1": {
        "caption": "TABLE I:  The embedding models that we compared and evaluated, and the fixed size  d d d italic_d  of the embeddings they generate.",
        "table": "S3.F1.2",
        "footnotes": [],
        "references": [
            "We prepare a  description string  for each CAPEC attack pattern and ATT&CK ICS technique by concatenating their name, ID, and description, as displayed in Fig.  1 . An embedding model,    \\Phi roman_ , then tokenizes each input  description string  to a list of tokens and transforms those tokens to a vector in  R d superscript R d \\mathbb{R}^{d} blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT . Although most of the  description strings  are quite short and fit within the maximum sequence length of the models, truncation is performed for the few longer descriptions where necessary. Our specific embedding models and their corresponding dimensions are listed in Table  I ."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  CAPEC-to-ATT&CK Results",
        "table": "S3.F1.4",
        "footnotes": [],
        "references": [
            "The next task is to use the vector embedding of the  description strings  of CAPEC attack patterns and ATT&CK ICS techniques to generate a mapping between the taxonomies, i.e., identify the list of  k k k italic_k  entries from one taxonomy that are most similar to a given entry from the other taxonomy.  Here, we present two methods of generating the required mapping  (i) nearest-neighbour mapping, and (ii) RAG-based mapping. These are compared in Fig.  2 . As we will show in this section, the RAG-based approach builds directly off the nearest-neighbor approach, refining its output to improve precision.",
            "We propose a method that improves upon the simple embedding-based retrieval method by utilizing LLMs in an approach similar to standard RAG techniques  [ 22 ] . This RAG pipeline relies upon both an embedding model and a generative language modelthe nearest-neighbor mapping function  f k ,  subscript f k  f_{k,\\Phi} italic_f start_POSTSUBSCRIPT italic_k , roman_ end_POSTSUBSCRIPT  is in fact the first step of the RAG pipeline. As shown in Fig.  2 , an individual CAPEC attack pattern  c c c italic_c  is fed into the pipeline as input along with a parameter  k k k italic_k , and the resulting techniques  f k ,   ( c ) subscript f k  c f_{k,\\Phi}(c) italic_f start_POSTSUBSCRIPT italic_k , roman_ end_POSTSUBSCRIPT ( italic_c )  are retrieved. While we did store the intermediate, low-dimensional embedded representations in a vector database, the taxonomies are currently small enough that all embeddings can alternatively be kept in memory. Once retrieved, the techniques are ranked according to their proximity to  c c c italic_c , and then passed along to the LLM in a prompt. Because of the significant instability and sensitivity of LLMs to small changes in their inputs  [ 23 ] , prompt engineering is necessary to improve their robustness. We utilized the open-source 8B-parameter instruction fine-tuned variant of Metas Llama 3 for this purpose  [ 24 ] ."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  ATT&CK-to-CAPEC Results",
        "table": "S3.T1.4",
        "footnotes": [],
        "references": []
    },
    "id_table_4": {
        "caption": "",
        "table": "S5.T2.12",
        "footnotes": [],
        "references": []
    },
    "id_table_5": {
        "caption": "",
        "table": "S5.T3.12",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}