{
    "S2.T1": {
        "caption": "Table 1: Results on GLUE test sets. Metrics differ per task (explained in Appendix A) but the best result is highlighted.",
        "table": null,
        "footnotes": [],
        "references": [
            "We first use the three meta-learning algorithms with PPS sampling and present in Table 1 the experimental results on the GLUE test set. Generally, the meta-learning algorithms achieve better performance than the strong baseline models, with Reptile performing the best."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Effect of task distributions. We report the accuracy or Matthews correlation on development sets.",
        "table": null,
        "footnotes": [],
        "references": [
            "As we have mentioned above, we propose three different choices of the task distribution p​(T)𝑝𝑇p(T) in this paper. Here we train Reptile with these task distributions and test models’ performance on the development set as shown in Table 2."
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Effect of the number of update steps and the inner learning rate α𝛼\\alpha.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this part, we test the effect of the number of update steps k𝑘k and the learning rate in the inner learning loop. The experimental results on the development sets are shown in Table 3. We find that setting k𝑘k to 5 is the optimal strategy and more or fewer update steps may lead to worse performance.",
            "We also vary the inner learning rate α𝛼\\alpha and investigate its impact. The results are listed in Table 3. We can see that larger α𝛼\\alpha may degrade the performance because the resulting gradients deviate a lot from normal ones.\nThe above two ablations studies demonstrate the importance of making the meta-gradient informative."
        ]
    },
    "A0.T4": {
        "caption": "Table 4: Basic information and statistics of the GLUE and SciTail datasets Williams et al. (2018).",
        "table": null,
        "footnotes": [],
        "references": [
            "Basically, the GLUE dataset  Wang et al. (2019) consists of three types of tasks: single-sentence classification, similarity and paraphrase tasks, and inference tasks, as shown in Table 4."
        ]
    },
    "A3.T5": {
        "caption": "Table 5: Accuracy numbers on the 10 probing tasks Conneau et al. (2018).",
        "table": null,
        "footnotes": [],
        "references": [
            "A probing task is a classification problem that requires the model to make predictions related to certain linguistic properties of sentences. The abbreviations for the 10 tasks are listed in Table 5. Basically, these tasks are set to test the model’s abilities to capture surface, syntactic or semantic information. We refer the reader to Conneau et al. (2018) for details. We freeze all the parameters of the models and only train the classification layer for the probing tasks."
        ]
    }
}