{
    "id_table_1": {
        "caption": "Table 1:  Comparison of different datasets for speech separation and enhancement. Geometry refers to the room geometry. Occlusion indicates whether objects obstruct the sound source and microphone. Material shows whether the material properties of objects are considered. Scalability refers to the ability to increase the amount of data. Cost represents the difficulty of simulating or collecting data. Tools indicates whether there are toolkits for data collection. Src Type indicates the source type. SS and SE denote the speech separation and enhancement tasks.",
        "table": "S2.T1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "To advance research in moving sound sources, particularly in speech separation and enhancement tasks, there is a pressing need for a high-fidelity, low-cost, and comprehensive synthetic toolkit and data asset for moving sound sources. To address these issues, we developed  SonicSim , a synthetic toolkit capable of accurately simulating RIRs for moving sound sources, as shown in Figure  1 . This toolkit is built upon the embodied AI simulation platform, Habitat-sim  (Savva et al.,  2019 ) , which can import a variety of 3D environments and accurately simulate the acoustic characteristics of rooms. The core strength of SonicSim lies in its precise simulation of RIRs  (Chen et al.,  2022a )  and highly customizable simulation of moving sound sources, which significantly expands the scale of data collection for moving sound source research.",
            "In this section, we systematically compare the SonicSim toolkit and the generated SonicSet dataset with existing real-world datasets, synthetic datasets, and simulation toolkits (see Table  1 ).",
            "The customizable dataset generator, SonicSim, is specifically designed to generate datasets for speech tasks. Built on Habitat-sim  (Savva et al.,  2019 ) , SonicSim leverages its highly realistic audio renderer (demonstrated in  (Chen et al.,  2022a ) ) and high-performance 3D simulator to produce high-quality audio data adapted to various acoustic environments. SonicSim provides a rich set of annotations, including source and microphone position maps, clean audio, and audio with reverberation and noise, without incurring additional data collection costs. More importantly, SonicSim provides users with extensive control over the dataset generation process, allowing customization of scene layouts, scene materials, source and microphone positions, and microphone types, while ensuring physical realism through its physics engine. By adjusting these configurations, SonicSim can flexibly generate diverse acoustic environment data to meet the specific requirements of various tasks. The following main functions are included in SonicSim, as shown in Figure  1 .",
            "To create realistic synthetic mixed audio, we set the length of each mixed audio clip to 60 seconds. Each speech segment was composed of 3-5 full audio clips with the same speaker randomly selected from LibriSpeech, arranged into a 60-second audio sequence by randomly selecting start positions within 0-8 seconds for each clip. This approach ensured various overlap rates in the mixed audio while utilizing SonicSim for moving sound source synthesis. For noise data, 6-8 segments of environmental noise were randomly selected from the divided FSD50K dataset, and 6-8 segments of musical noise were randomly chosen from the divided FMA dataset. These noise segments were arranged into 60-second audio clips by choosing random start positions within 0-4 seconds. We used Loudness Units relative to Full Scale (LUFS) to adjust volume levels, setting the speech at -17 LUFS, environmental noise at -21 LUFS, and musical noise at -24 LUFS. When generating the mixed audio, different speakers were randomly chosen from the speech data, and a segment of environmental noise was randomly selected to create a mixed audio clip with background noise, while a segment of musical noise was randomly chosen to create a mixed audio clip with a musical noise. These mixed audio samples were used to test the robustness of speech separation and enhancement models in various noisy environments. For each dataset group (three speech clips + one environmental noise clip + one musical noise clip), we simulated within the same scene, where the speech clips were simulated as moving sources and the noise was simulated as static sources. We present an example of SonicSet in Figure  5  in Appendix  B  and provide the corresponding audio metadata in the JSON file  1  in Appendix  B .",
            "Speech separation aims to isolate individual speech signals from a mixture containing multiple speakers, as shown in Figure  4 (a). This task is critical for real-world applications such as meetings and telephone conversations. A detailed explanation of the task pipeline is provided in Appendix  C.1 .",
            "The speech enhancement task aims to extract high-quality target speech from a noisy signal, reducing or eliminating background noise, as shown in Figure  4 (b). This task is crucial in applications such as speech recognition, speech communication, and hearing aids. A detailed explanation of this task pipeline is available in Appendix  D.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparative performance evaluation of models trained on different datasets using real-recorded audio with  environmental noise . The results are reported separately for  trained on LRS2-2Mix ,  trained on Libri2Mix  and  trained on SonicSet , distinguished by a slash. The relative length is indicated below the value by horizontal bars.",
        "table": "A1.T7.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Based on SonicSim, we constructed a multi-scene, large-scale, and high-quality moving sound source dataset called  SonicSet  (see Figure  2 ): 1)  Multi-scene : SonicSet utilizes 90 scenes from the Matterport3D dataset  (Chang et al.,  2017 ) , covering a wide range of real-world environments, such as homes, offices, and churches; 2)  Large-scale : SonicSet integrates 360 hours of speech audio from the LibriSpeech  (Panayotov et al.,  2015 ) , combined with environmental noise from FSD50K  (Fonseca et al.,  2021 )  and musical noise from the FMA dataset  (Defferrard et al.,  2017 ) ; 3)  High-quality : The RIRs of the synthetic audio closely resemble real-world environments by simulating reflection and diffraction across various materials, resulting in higher-quality reverberated audio.",
            "We selected several popular models that have demonstrated excellent performance in speech separation as benchmarks, including Conv-TasNet  (Luo & Mesgarani,  2019 ) , DPRNN  (Luo et al.,  2020 ) , DPTNet  (Chen et al.,  2020a ) , SuDORM-RF  (Tzinis et al.,  2020 ) , A-FRCNN  (Hu et al.,  2021 ) , SKIM  (Li et al.,  2022d ) , TDANet  (Li et al.,  2023 ) , BSRNN  (Luo & Yu,  2023b ) , TF-GridNet  (Wang et al.,  2023 ) , Mossformer  (Zhao & Ma,  2023 ) , and Mossformer2  (Zhao et al.,  2024 ) . For detailed information about the benchmark models, see Appendix  C.2 . The training object of the models is available in Appendix  C.3 . The hyperparameter configuration and all training settings are available in Appendix  C.4 . The PyTorch implementations and pre-trained weights for all benchmark models are publicly available 4 4 4 https://github.com/JusperLee/SonicSim/tree/main/separation .",
            "Comparison on real-recorded datasets . To validate the acoustic simulation gap between synthetic datasets and real data, we constructed a real-world moving sound source dataset consisting of 5 hours of recorded audio (details in Appendix  C.5 ), encompassing various complex acoustic environments and different types of background noise (environmental and musical noise). In Tables  2  and  3 , we trained models using the LRS2-2Mix and Libri2Mix datasets to compare their performance with models trained on the SonicSet dataset. The LRS2-2Mix dataset  (Li et al.,  2023 )  contains real-world noise and reverberation. The Libri2Mix dataset  (Cosentino et al.,  2020 )  is currently the largest synthetic speech separation dataset, and its data scale is closest to SonicSet among public speech separation datasets. We trained different models on these datasets and tested them on the real-world recorded datasets. The results demonstrated that models trained on the SonicSet dataset achieved overall the best separation performance on the real-world recorded datasets. This finding indicates that the SonicSet dataset excels in simulating more realistic acoustic scenes, making it effective for model training and evaluation.",
            "In the speech enhancement experiments, we selected several popular models: DCCRN  (Hu et al.,  2020 ) , Fullband  (Hao et al.,  2021 ) , FullSubNet  (Hao et al.,  2021 ) , Fast-FullSubNet  (Hao & Li,  2022 ) , FullSubNet+  (Chen et al.,  2022b ) , TaylorSENet  (Li et al.,  2022a ) , GaGNet  (Li et al.,  2022c ) , G2Net  (Li et al.,  2022b )  and Inter-SubNet  (Chen et al.,  2023 ) . The details of the models are available in Appendix  D.2 . The training object of the model is available in Appendix  D.3 . The hyperparameter configuration and all training settings are available in Appendix  D.4 ."
        ]
    }
}