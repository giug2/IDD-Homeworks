{
    "id_table_1": {
        "caption": "Table 1:  Examples of generative CQs and their matched ground truth CQ with highest cosine similarity scores for two domain ontology engineering tasks",
        "table": "S4.T1.3",
        "footnotes": [],
        "references": [
            "Figure  1  illustrates the main steps in the RAG pipeline: domain knowledge indexing, relevant data retrieval, and response generation.",
            "These components constitute the following template with four variables to be determined for each ontology engineering task in our approach.  Variables are enclosed in curly braces as shown in Listing  1 .",
            "We take two ontology engineering tasks to evaluate our approach for generating CQs.  The first is the construction of a knowledge graph of empirical research in requirement engineering(RE), namely KG-EmpiRE  [ 8 ] .  In this paper, the authors constructed the KG-EmpiRE with the purpose of providing the community with the state and evolution of empirical research in RE.  The KG-EmpiRE was evaluated against 77 CQs manually derived by three domain experts from a published visionary paper about how researchers should conduct empirical research in RE  [ 13 ] .  The second is the construction of a core reference ontology in HumanComputer Interaction (HCI), namely HCIO  [ 5 ] .  In this paper, the authors constructed the HCIO with the purpose of clarifying the main concepts involved in the HCI phenomenon.  The HCIO was evaluated against 15 CQs identified by ontology engineers using the methods described in SABiO  [ 2 ] .  Examples of these CQs identified by domain experts could be found in Table  1  with column name  C  Q g  t C subscript Q g t CQ_{gt} italic_C italic_Q start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT .",
            "As mentioned in Section  3.1 , the selection of documents for the knowledge base in our RAG pipeline is very important.",
            "Table  1  shows some examples of the  C  Q g  e  n C subscript Q g e n CQ_{gen} italic_C italic_Q start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT  and their matched  C  Q g  t C subscript Q g t CQ_{gt} italic_C italic_Q start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT  with cosine similarity scores for two domain engineering tasks.",
            "From the perspective of zero-shot prompting, we observe that the precision in HCI is much higher than the precision in RE, which we think is due to the degree of abstraction of the target ontology or knowledge graph.  The more concrete the ontology/KG is, the more domain knowledge is required for LLMs to perform well in the tasks of generating CQs for ontology engineering.  For instance, the purpose of the HCIO is to develop a reference ontology to represent the core concepts in HCI, while the purpose of the KG-EmpiRE is to construct a knowledge graph to capture the state and evolution in RE.  More domain knowledge such as specific methods used in RE is required to construct the KG-EmpiRE so as to answer the example  C  Q g  t C subscript Q g t CQ_{gt} italic_C italic_Q start_POSTSUBSCRIPT italic_g italic_t end_POSTSUBSCRIPT  shown in Table  1 ,  How often are which empirical methods used over time? .  Therefore, our RAG approach outperforms zero-shot prompting in generating CQs for KG-EmpiRE, while zero-shot prompting outperforms the proposed RAG approach in generating CQs for HCIO."
        ]
    },
    "global_footnotes": []
}