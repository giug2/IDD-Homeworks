{
    "S5.T1": {
        "caption": "Table 1: F1 scores on WSD benchmarks and accuracy on WiC are shown for the three sources (top) and for the different meta-embedding methods (bottom).",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE2</th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE3</th>\n<th id=\"S5.T1.1.1.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE07</th>\n<th id=\"S5.T1.1.1.1.5\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE13</th>\n<th id=\"S5.T1.1.1.1.6\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE15</th>\n<th id=\"S5.T1.1.1.1.7\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">ALL</th>\n<th id=\"S5.T1.1.1.1.8\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">WiC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">LMMS</th>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\">76.34</td>\n<td id=\"S5.T1.1.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\">75.57</td>\n<td id=\"S5.T1.1.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\">68.13</td>\n<td id=\"S5.T1.1.2.1.5\" class=\"ltx_td ltx_align_right ltx_border_t\">75.12</td>\n<td id=\"S5.T1.1.2.1.6\" class=\"ltx_td ltx_align_right ltx_border_t\">77.01</td>\n<td id=\"S5.T1.1.2.1.7\" class=\"ltx_td ltx_align_right ltx_border_t\">75.44</td>\n<td id=\"S5.T1.1.2.1.8\" class=\"ltx_td ltx_align_right ltx_border_t\">69.30</td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">ARES</th>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_right\">78.05</td>\n<td id=\"S5.T1.1.3.2.3\" class=\"ltx_td ltx_align_right\">77.08</td>\n<td id=\"S5.T1.1.3.2.4\" class=\"ltx_td ltx_align_right\">70.99</td>\n<td id=\"S5.T1.1.3.2.5\" class=\"ltx_td ltx_align_right\">77.31</td>\n<td id=\"S5.T1.1.3.2.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T1.1.3.2.6.1\" class=\"ltx_text ltx_font_bold\">83.17</span></td>\n<td id=\"S5.T1.1.3.2.7\" class=\"ltx_td ltx_align_right\">77.91</td>\n<td id=\"S5.T1.1.3.2.8\" class=\"ltx_td ltx_align_right\">68.50</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SBERT</th>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_right\">53.11</td>\n<td id=\"S5.T1.1.4.3.3\" class=\"ltx_td ltx_align_right\">52.22</td>\n<td id=\"S5.T1.1.4.3.4\" class=\"ltx_td ltx_align_right\">41.37</td>\n<td id=\"S5.T1.1.4.3.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T1.1.4.3.5.1\" class=\"ltx_text ltx_font_bold\">78.77</span></td>\n<td id=\"S5.T1.1.4.3.6\" class=\"ltx_td ltx_align_right\">55.12</td>\n<td id=\"S5.T1.1.4.3.7\" class=\"ltx_td ltx_align_right\">59.85</td>\n<td id=\"S5.T1.1.4.3.8\" class=\"ltx_td ltx_align_right\">71.14</td>\n</tr>\n<tr id=\"S5.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">AVG</th>\n<td id=\"S5.T1.1.5.4.2\" class=\"ltx_td ltx_align_right ltx_border_t\">79.36</td>\n<td id=\"S5.T1.1.5.4.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T1.1.5.4.3.1\" class=\"ltx_text ltx_font_bold\">77.46</span></td>\n<td id=\"S5.T1.1.5.4.4\" class=\"ltx_td ltx_align_right ltx_border_t\">70.33</td>\n<td id=\"S5.T1.1.5.4.5\" class=\"ltx_td ltx_align_right ltx_border_t\">77.86</td>\n<td id=\"S5.T1.1.5.4.6\" class=\"ltx_td ltx_align_right ltx_border_t\">80.82</td>\n<td id=\"S5.T1.1.5.4.7\" class=\"ltx_td ltx_align_right ltx_border_t\">78.17</td>\n<td id=\"S5.T1.1.5.4.8\" class=\"ltx_td ltx_align_right ltx_border_t\">71.16</td>\n</tr>\n<tr id=\"S5.T1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">CONC</th>\n<td id=\"S5.T1.1.6.5.2\" class=\"ltx_td ltx_align_right\">78.22</td>\n<td id=\"S5.T1.1.6.5.3\" class=\"ltx_td ltx_align_right\">77.14</td>\n<td id=\"S5.T1.1.6.5.4\" class=\"ltx_td ltx_align_right\">70.99</td>\n<td id=\"S5.T1.1.6.5.5\" class=\"ltx_td ltx_align_right\">77.37</td>\n<td id=\"S5.T1.1.6.5.6\" class=\"ltx_td ltx_align_right\">82.97</td>\n<td id=\"S5.T1.1.6.5.7\" class=\"ltx_td ltx_align_right\">77.97</td>\n<td id=\"S5.T1.1.6.5.8\" class=\"ltx_td ltx_align_right\">70.38</td>\n</tr>\n<tr id=\"S5.T1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SVD</th>\n<td id=\"S5.T1.1.7.6.2\" class=\"ltx_td ltx_align_right\">75.02</td>\n<td id=\"S5.T1.1.7.6.3\" class=\"ltx_td ltx_align_right\">74.22</td>\n<td id=\"S5.T1.1.7.6.4\" class=\"ltx_td ltx_align_right\">67.25</td>\n<td id=\"S5.T1.1.7.6.5\" class=\"ltx_td ltx_align_right\">72.81</td>\n<td id=\"S5.T1.1.7.6.6\" class=\"ltx_td ltx_align_right\">74.85</td>\n<td id=\"S5.T1.1.7.6.7\" class=\"ltx_td ltx_align_right\">73.80</td>\n<td id=\"S5.T1.1.7.6.8\" class=\"ltx_td ltx_align_right\">63.01</td>\n</tr>\n<tr id=\"S5.T1.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AEME</th>\n<td id=\"S5.T1.1.8.7.2\" class=\"ltx_td ltx_align_right\">78.53</td>\n<td id=\"S5.T1.1.8.7.3\" class=\"ltx_td ltx_align_right\">76.92</td>\n<td id=\"S5.T1.1.8.7.4\" class=\"ltx_td ltx_align_right\">69.01</td>\n<td id=\"S5.T1.1.8.7.5\" class=\"ltx_td ltx_align_right\">76.09</td>\n<td id=\"S5.T1.1.8.7.6\" class=\"ltx_td ltx_align_right\">78.96</td>\n<td id=\"S5.T1.1.8.7.7\" class=\"ltx_td ltx_align_right\">77.03</td>\n<td id=\"S5.T1.1.8.7.8\" class=\"ltx_td ltx_align_right\">70.69</td>\n</tr>\n<tr id=\"S5.T1.1.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">NPMS</th>\n<td id=\"S5.T1.1.9.8.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T1.1.9.8.2.1\" class=\"ltx_text ltx_font_bold\">79.93</span></td>\n<td id=\"S5.T1.1.9.8.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">77.30</td>\n<td id=\"S5.T1.1.9.8.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T1.1.9.8.4.1\" class=\"ltx_text ltx_font_bold\">71.65</span></td>\n<td id=\"S5.T1.1.9.8.5\" class=\"ltx_td ltx_align_right ltx_border_bb\">77.49</td>\n<td id=\"S5.T1.1.9.8.6\" class=\"ltx_td ltx_align_right ltx_border_bb\">81.21</td>\n<td id=\"S5.T1.1.9.8.7\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T1.1.9.8.7.1\" class=\"ltx_text ltx_font_bold\">78.37</span></td>\n<td id=\"S5.T1.1.9.8.8\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T1.1.9.8.8.1\" class=\"ltx_text ltx_font_bold\">71.47</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table 1 compares the performance of NPMS against the meta-embedding methods described in §​ 4.3 on WSD and WiC.\nWe see that NPMS obtains the overall best performance for WSD (ALL) as well as on WiC.\nAmong the three sources, ARES reports the best performance for WSD (ALL), while SBERT does so for WiC.\nIn SE2, SE07 datasets NPMS reports the best performance, whereas AVG, SBERT and ARES do so respectively in SE3, SE13 and SE15.\nAmong the baseline methods, we see AVG to report the best results, which is closely followed by CONC.\nPoor performance of SVD shows the challenge of applying dimensionality reduction methods on CONC due to missing sense embeddings.\nAlthough AEME has reported the SoTA performance for word-level meta-embedding, applying it directly on sense embeddings is suboptimal.\nThis shows the difference between word- vs. sense-level meta-embedding learning problems, and calls for sense-specific meta-embedding learning methods.",
            "The performance of a meta-embedding depends on the source embeddings used.\nTherefore, we evaluate the ability of NPMS to create meta-sense embeddings from diverse source sense embeddings that have different dimensionalities and created from different MLMs.\nDue space limitations, in Table 2 we compare NPMS against AVG, which reported the best performance among all other meta-embedding learning methods in Table 1.\nFrom Table 2, we see that when the dimensionalities of the two source sense embeddings are identical (i.e. 2048 dimensional LMMS + ARES or LMMS + SBERT configurations) or similar (i.e. 2048 dimensional ARES + 2048 dimensional SBERT configuration), AVG closely matches the performance of NPMS in WSD and WiC evaluations.\nHowever, we see a drastically different trend when the two sources are not BERT-based (e.g. XLNet, RoBERTa) or when they have significantly different dimensionalities (1024 dimensional LMMS (XLNet), LMMS (RoBERTa) and 50 dimensional DeConf).\nIn such settings, we see that NPMS to performs significantly better than AVG across all WSD benchmarks as well as on WiC.\nRecall that AVG assumes (a) the source embedding spaces to be orthogonal,\nand (b) applies zero-padding to the smaller dimensional source embeddings to make them aligned with the rest of the source embeddings.\nBoth of those assumptions do not hold true when the source embeddings are created from diverse MLMs or have significantly different numbers of dimensions, which leads to suboptimal performances in AVG.\nOn the other hand, NPMS does not directly compare source sense embeddings, but instead consider neighbourhoods computed from the source sense embeddings.\nMoreover, zero-padding is not required in NPMS because the contextual alignment step ensures the proper alignment between the contextual embedding and meta-sense embedding spaces.\nThese advantages of NPMS are clearly evident from Table 2."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Meta-sense embedding of sources with different dimensionalities (shown in brackets) and MLMs.",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<td id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\">SE2</td>\n<td id=\"S5.T2.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\">SE3</td>\n<td id=\"S5.T2.1.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\">SE07</td>\n<td id=\"S5.T2.1.1.1.5\" class=\"ltx_td ltx_align_right ltx_border_tt\">SE13</td>\n<td id=\"S5.T2.1.1.1.6\" class=\"ltx_td ltx_align_right ltx_border_tt\">SE15</td>\n<td id=\"S5.T2.1.1.1.7\" class=\"ltx_td ltx_align_right ltx_border_tt\">ALL</td>\n<td id=\"S5.T2.1.1.1.8\" class=\"ltx_td ltx_align_right ltx_border_tt\">WiC</td>\n</tr>\n<tr id=\"S5.T2.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"8\">LMMS(BERT) [2048] + ARES (BERT) [2048]</th>\n</tr>\n<tr id=\"S5.T2.1.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AVG</th>\n<td id=\"S5.T2.1.3.3.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.3.3.2.1\" class=\"ltx_text ltx_font_bold\">78.79</span></td>\n<td id=\"S5.T2.1.3.3.3\" class=\"ltx_td ltx_align_right\">77.03</td>\n<td id=\"S5.T2.1.3.3.4\" class=\"ltx_td ltx_align_right\">69.89</td>\n<td id=\"S5.T2.1.3.3.5\" class=\"ltx_td ltx_align_right\">77.13</td>\n<td id=\"S5.T2.1.3.3.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.3.3.6.1\" class=\"ltx_text ltx_font_bold\">81.80</span></td>\n<td id=\"S5.T2.1.3.3.7\" class=\"ltx_td ltx_align_right\">77.83</td>\n<td id=\"S5.T2.1.3.3.8\" class=\"ltx_td ltx_align_right\">70.22</td>\n</tr>\n<tr id=\"S5.T2.1.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">NPMS</th>\n<td id=\"S5.T2.1.4.4.2\" class=\"ltx_td ltx_align_right\">78.53</td>\n<td id=\"S5.T2.1.4.4.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.4.4.3.1\" class=\"ltx_text ltx_font_bold\">77.14</span></td>\n<td id=\"S5.T2.1.4.4.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.4.4.4.1\" class=\"ltx_text ltx_font_bold\">71.87</span></td>\n<td id=\"S5.T2.1.4.4.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.4.4.5.1\" class=\"ltx_text ltx_font_bold\">77.37</span></td>\n<td id=\"S5.T2.1.4.4.6\" class=\"ltx_td ltx_align_right\">81.60</td>\n<td id=\"S5.T2.1.4.4.7\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.4.4.7.1\" class=\"ltx_text ltx_font_bold\">77.93</span></td>\n<td id=\"S5.T2.1.4.4.8\" class=\"ltx_td ltx_align_right\">70.22</td>\n</tr>\n<tr id=\"S5.T2.1.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"8\">ARES (BERT) [2048] + SBERT [2048]</th>\n</tr>\n<tr id=\"S5.T2.1.6.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AVG</th>\n<td id=\"S5.T2.1.6.6.2\" class=\"ltx_td ltx_align_right\">78.57</td>\n<td id=\"S5.T2.1.6.6.3\" class=\"ltx_td ltx_align_right\">77.35</td>\n<td id=\"S5.T2.1.6.6.4\" class=\"ltx_td ltx_align_right\">71.21</td>\n<td id=\"S5.T2.1.6.6.5\" class=\"ltx_td ltx_align_right\">78.10</td>\n<td id=\"S5.T2.1.6.6.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.6.6.6.1\" class=\"ltx_text ltx_font_bold\">81.70</span></td>\n<td id=\"S5.T2.1.6.6.7\" class=\"ltx_td ltx_align_right\">78.13</td>\n<td id=\"S5.T2.1.6.6.8\" class=\"ltx_td ltx_align_right\">71.32</td>\n</tr>\n<tr id=\"S5.T2.1.7.7\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">NPMS</th>\n<td id=\"S5.T2.1.7.7.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.7.7.2.1\" class=\"ltx_text ltx_font_bold\">78.79</span></td>\n<td id=\"S5.T2.1.7.7.3\" class=\"ltx_td ltx_align_right\">77.41</td>\n<td id=\"S5.T2.1.7.7.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.7.7.4.1\" class=\"ltx_text ltx_font_bold\">71.65</span></td>\n<td id=\"S5.T2.1.7.7.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.7.7.5.1\" class=\"ltx_text ltx_font_bold\">78.53</span></td>\n<td id=\"S5.T2.1.7.7.6\" class=\"ltx_td ltx_align_right\">81.41</td>\n<td id=\"S5.T2.1.7.7.7\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.7.7.7.1\" class=\"ltx_text ltx_font_bold\">78.30</span></td>\n<td id=\"S5.T2.1.7.7.8\" class=\"ltx_td ltx_align_right\">71.32</td>\n</tr>\n<tr id=\"S5.T2.1.8.8\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"8\">LMMS (BERT) [2048] + SBERT [2048]</th>\n</tr>\n<tr id=\"S5.T2.1.9.9\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AVG</th>\n<td id=\"S5.T2.1.9.9.2\" class=\"ltx_td ltx_align_right\">77.70</td>\n<td id=\"S5.T2.1.9.9.3\" class=\"ltx_td ltx_align_right\">76.16</td>\n<td id=\"S5.T2.1.9.9.4\" class=\"ltx_td ltx_align_right\">68.79</td>\n<td id=\"S5.T2.1.9.9.5\" class=\"ltx_td ltx_align_right\">78.04</td>\n<td id=\"S5.T2.1.9.9.6\" class=\"ltx_td ltx_align_right\">77.69</td>\n<td id=\"S5.T2.1.9.9.7\" class=\"ltx_td ltx_align_right\">76.82</td>\n<td id=\"S5.T2.1.9.9.8\" class=\"ltx_td ltx_align_right\">69.59</td>\n</tr>\n<tr id=\"S5.T2.1.10.10\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">NPMS</th>\n<td id=\"S5.T2.1.10.10.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.10.10.2.1\" class=\"ltx_text ltx_font_bold\">78.05</span></td>\n<td id=\"S5.T2.1.10.10.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.10.10.3.1\" class=\"ltx_text ltx_font_bold\">76.86</span></td>\n<td id=\"S5.T2.1.10.10.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.10.10.4.1\" class=\"ltx_text ltx_font_bold\">69.89</span></td>\n<td id=\"S5.T2.1.10.10.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.10.10.5.1\" class=\"ltx_text ltx_font_bold\">78.28</span></td>\n<td id=\"S5.T2.1.10.10.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.10.10.6.1\" class=\"ltx_text ltx_font_bold\">78.28</span></td>\n<td id=\"S5.T2.1.10.10.7\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.10.10.7.1\" class=\"ltx_text ltx_font_bold\">77.32</span></td>\n<td id=\"S5.T2.1.10.10.8\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.10.10.8.1\" class=\"ltx_text ltx_font_bold\">71.79</span></td>\n</tr>\n<tr id=\"S5.T2.1.11.11\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"8\">LMMS(XLNet) [1024] + DeConf [50]</th>\n</tr>\n<tr id=\"S5.T2.1.12.12\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AVG</th>\n<td id=\"S5.T2.1.12.12.2\" class=\"ltx_td ltx_align_right\">40.80</td>\n<td id=\"S5.T2.1.12.12.3\" class=\"ltx_td ltx_align_right\">35.68</td>\n<td id=\"S5.T2.1.12.12.4\" class=\"ltx_td ltx_align_right\">21.32</td>\n<td id=\"S5.T2.1.12.12.5\" class=\"ltx_td ltx_align_right\">41.61</td>\n<td id=\"S5.T2.1.12.12.6\" class=\"ltx_td ltx_align_right\">43.93</td>\n<td id=\"S5.T2.1.12.12.7\" class=\"ltx_td ltx_align_right\">38.89</td>\n<td id=\"S5.T2.1.12.12.8\" class=\"ltx_td ltx_align_right\">66.46</td>\n</tr>\n<tr id=\"S5.T2.1.13.13\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.13.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">NPMS</th>\n<td id=\"S5.T2.1.13.13.2\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.13.13.2.1\" class=\"ltx_text ltx_font_bold\">50.88</span></td>\n<td id=\"S5.T2.1.13.13.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.13.13.3.1\" class=\"ltx_text ltx_font_bold\">41.68</span></td>\n<td id=\"S5.T2.1.13.13.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.13.13.4.1\" class=\"ltx_text ltx_font_bold\">40.66</span></td>\n<td id=\"S5.T2.1.13.13.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.13.13.5.1\" class=\"ltx_text ltx_font_bold\">53.04</span></td>\n<td id=\"S5.T2.1.13.13.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.13.13.6.1\" class=\"ltx_text ltx_font_bold\">53.13</span></td>\n<td id=\"S5.T2.1.13.13.7\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.13.13.7.1\" class=\"ltx_text ltx_font_bold\">48.70</span></td>\n<td id=\"S5.T2.1.13.13.8\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T2.1.13.13.8.1\" class=\"ltx_text ltx_font_bold\">69.26</span></td>\n</tr>\n<tr id=\"S5.T2.1.14.14\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.14.14.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" colspan=\"8\">LMMS(RoBERTa) [1024] + DeConf [50]</th>\n</tr>\n<tr id=\"S5.T2.1.15.15\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.15.15.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">AVG</th>\n<td id=\"S5.T2.1.15.15.2\" class=\"ltx_td ltx_align_right\">39.35</td>\n<td id=\"S5.T2.1.15.15.3\" class=\"ltx_td ltx_align_right\">34.97</td>\n<td id=\"S5.T2.1.15.15.4\" class=\"ltx_td ltx_align_right\">26.15</td>\n<td id=\"S5.T2.1.15.15.5\" class=\"ltx_td ltx_align_right\">41.48</td>\n<td id=\"S5.T2.1.15.15.6\" class=\"ltx_td ltx_align_right\">42.47</td>\n<td id=\"S5.T2.1.15.15.7\" class=\"ltx_td ltx_align_right\">38.33</td>\n<td id=\"S5.T2.1.15.15.8\" class=\"ltx_td ltx_align_right\">66.46</td>\n</tr>\n<tr id=\"S5.T2.1.16.16\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.16.16.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">NPMS</th>\n<td id=\"S5.T2.1.16.16.2\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T2.1.16.16.2.1\" class=\"ltx_text ltx_font_bold\">48.77</span></td>\n<td id=\"S5.T2.1.16.16.3\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T2.1.16.16.3.1\" class=\"ltx_text ltx_font_bold\">44.81</span></td>\n<td id=\"S5.T2.1.16.16.4\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T2.1.16.16.4.1\" class=\"ltx_text ltx_font_bold\">39.34</span></td>\n<td id=\"S5.T2.1.16.16.5\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T2.1.16.16.5.1\" class=\"ltx_text ltx_font_bold\">53.41</span></td>\n<td id=\"S5.T2.1.16.16.6\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T2.1.16.16.6.1\" class=\"ltx_text ltx_font_bold\">53.52</span></td>\n<td id=\"S5.T2.1.16.16.7\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T2.1.16.16.7.1\" class=\"ltx_text ltx_font_bold\">48.89</span></td>\n<td id=\"S5.T2.1.16.16.8\" class=\"ltx_td ltx_align_right ltx_border_bb\"><span id=\"S5.T2.1.16.16.8.1\" class=\"ltx_text ltx_font_bold\">69.75</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "The performance of a meta-embedding depends on the source embeddings used.\nTherefore, we evaluate the ability of NPMS to create meta-sense embeddings from diverse source sense embeddings that have different dimensionalities and created from different MLMs.\nDue space limitations, in Table 2 we compare NPMS against AVG, which reported the best performance among all other meta-embedding learning methods in Table 1.\nFrom Table 2, we see that when the dimensionalities of the two source sense embeddings are identical (i.e. 2048 dimensional LMMS + ARES or LMMS + SBERT configurations) or similar (i.e. 2048 dimensional ARES + 2048 dimensional SBERT configuration), AVG closely matches the performance of NPMS in WSD and WiC evaluations.\nHowever, we see a drastically different trend when the two sources are not BERT-based (e.g. XLNet, RoBERTa) or when they have significantly different dimensionalities (1024 dimensional LMMS (XLNet), LMMS (RoBERTa) and 50 dimensional DeConf).\nIn such settings, we see that NPMS to performs significantly better than AVG across all WSD benchmarks as well as on WiC.\nRecall that AVG assumes (a) the source embedding spaces to be orthogonal,\nand (b) applies zero-padding to the smaller dimensional source embeddings to make them aligned with the rest of the source embeddings.\nBoth of those assumptions do not hold true when the source embeddings are created from diverse MLMs or have significantly different numbers of dimensions, which leads to suboptimal performances in AVG.\nOn the other hand, NPMS does not directly compare source sense embeddings, but instead consider neighbourhoods computed from the source sense embeddings.\nMoreover, zero-padding is not required in NPMS because the contextual alignment step ensures the proper alignment between the contextual embedding and meta-sense embedding spaces.\nThese advantages of NPMS are clearly evident from Table 2."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Effect of learning a projection matrix between meta-sense vs. BERT embedding spaces.",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Method</th>\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WSD (ALL)</th>\n<th id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WiC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">SVD with proj.</th>\n<td id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">74.80</td>\n<td id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">66.93</td>\n</tr>\n<tr id=\"S5.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">SVD without proj.</th>\n<td id=\"S5.T3.1.3.2.2\" class=\"ltx_td ltx_align_center\">35.90</td>\n<td id=\"S5.T3.1.3.2.3\" class=\"ltx_td ltx_align_center\">60.34</td>\n</tr>\n<tr id=\"S5.T3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">AEME with proj.</th>\n<td id=\"S5.T3.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">76.02</td>\n<td id=\"S5.T3.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">68.65</td>\n</tr>\n<tr id=\"S5.T3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">AEME without proj.</th>\n<td id=\"S5.T3.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">41.60</td>\n<td id=\"S5.T3.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">53.61</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table 3 shows the importance of learning a projection matrix via (6) between meta-sense and contextualised embeddings, for SVD and AEME.\nWe see that the performance of both of those methods drop significantly without the projection matrix learning step.\nEven with projection matrices, SVD and AEME do not outperform simpler baselines such as AVG or CONC.\nOn the other hand, NPMS does not require such a projection matrix learning step and consistently outperforms all those methods across multiple WSD and WiC benchmarks."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Ablation between the PIP-loss (Lpipsubscript𝐿pipL_{\\rm pip}) and contextual alignment loss (Lcontsubscript𝐿contL_{\\rm cont}).",
        "table": "<table id=\"S5.T4.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.2.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.3.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_tt\"/>\n<th id=\"S5.T4.2.3.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE2</th>\n<th id=\"S5.T4.2.3.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE3</th>\n<th id=\"S5.T4.2.3.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE07</th>\n<th id=\"S5.T4.2.3.1.5\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE13</th>\n<th id=\"S5.T4.2.3.1.6\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">SE15</th>\n<th id=\"S5.T4.2.3.1.7\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">ALL</th>\n<th id=\"S5.T4.2.3.1.8\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">WiC</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.2.4.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Both</th>\n<td id=\"S5.T4.2.4.1.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.2.4.1.2.1\" class=\"ltx_text ltx_font_bold\">79.93</span></td>\n<td id=\"S5.T4.2.4.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.2.4.1.3.1\" class=\"ltx_text ltx_font_bold\">77.30</span></td>\n<td id=\"S5.T4.2.4.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\">71.65</td>\n<td id=\"S5.T4.2.4.1.5\" class=\"ltx_td ltx_align_right ltx_border_t\">77.49</td>\n<td id=\"S5.T4.2.4.1.6\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.2.4.1.6.1\" class=\"ltx_text ltx_font_bold\">81.21</span></td>\n<td id=\"S5.T4.2.4.1.7\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.2.4.1.7.1\" class=\"ltx_text ltx_font_bold\">78.37</span></td>\n<td id=\"S5.T4.2.4.1.8\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T4.2.4.1.8.1\" class=\"ltx_text ltx_font_bold\">71.47</span></td>\n</tr>\n<tr id=\"S5.T4.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<math id=\"S5.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"L_{\\rm pip}\" display=\"inline\"><semantics id=\"S5.T4.1.1.1.m1.1a\"><msub id=\"S5.T4.1.1.1.m1.1.1\" xref=\"S5.T4.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T4.1.1.1.m1.1.1.2\" xref=\"S5.T4.1.1.1.m1.1.1.2.cmml\">L</mi><mi id=\"S5.T4.1.1.1.m1.1.1.3\" xref=\"S5.T4.1.1.1.m1.1.1.3.cmml\">pip</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.1.1.1.m1.1b\"><apply id=\"S5.T4.1.1.1.m1.1.1.cmml\" xref=\"S5.T4.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T4.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T4.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T4.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T4.1.1.1.m1.1.1.2\">𝐿</ci><ci id=\"S5.T4.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T4.1.1.1.m1.1.1.3\">pip</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.1.1.1.m1.1c\">L_{\\rm pip}</annotation></semantics></math> only</th>\n<td id=\"S5.T4.1.1.2\" class=\"ltx_td ltx_align_right\">79.80</td>\n<td id=\"S5.T4.1.1.3\" class=\"ltx_td ltx_align_right\">77.03</td>\n<td id=\"S5.T4.1.1.4\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T4.1.1.4.1\" class=\"ltx_text ltx_font_bold\">71.87</span></td>\n<td id=\"S5.T4.1.1.5\" class=\"ltx_td ltx_align_right\">77.49</td>\n<td id=\"S5.T4.1.1.6\" class=\"ltx_td ltx_align_right\">80.72</td>\n<td id=\"S5.T4.1.1.7\" class=\"ltx_td ltx_align_right\">78.20</td>\n<td id=\"S5.T4.1.1.8\" class=\"ltx_td ltx_align_right\">70.69</td>\n</tr>\n<tr id=\"S5.T4.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">\n<math id=\"S5.T4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"L_{\\rm cont}\" display=\"inline\"><semantics id=\"S5.T4.2.2.1.m1.1a\"><msub id=\"S5.T4.2.2.1.m1.1.1\" xref=\"S5.T4.2.2.1.m1.1.1.cmml\"><mi id=\"S5.T4.2.2.1.m1.1.1.2\" xref=\"S5.T4.2.2.1.m1.1.1.2.cmml\">L</mi><mi id=\"S5.T4.2.2.1.m1.1.1.3\" xref=\"S5.T4.2.2.1.m1.1.1.3.cmml\">cont</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.2.2.1.m1.1b\"><apply id=\"S5.T4.2.2.1.m1.1.1.cmml\" xref=\"S5.T4.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T4.2.2.1.m1.1.1.1.cmml\" xref=\"S5.T4.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T4.2.2.1.m1.1.1.2.cmml\" xref=\"S5.T4.2.2.1.m1.1.1.2\">𝐿</ci><ci id=\"S5.T4.2.2.1.m1.1.1.3.cmml\" xref=\"S5.T4.2.2.1.m1.1.1.3\">cont</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.2.2.1.m1.1c\">L_{\\rm cont}</annotation></semantics></math> only</th>\n<td id=\"S5.T4.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_b\">79.54</td>\n<td id=\"S5.T4.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_b\">77.19</td>\n<td id=\"S5.T4.2.2.4\" class=\"ltx_td ltx_align_right ltx_border_b\">70.77</td>\n<td id=\"S5.T4.2.2.5\" class=\"ltx_td ltx_align_right ltx_border_b\"><span id=\"S5.T4.2.2.5.1\" class=\"ltx_text ltx_font_bold\">77.86</span></td>\n<td id=\"S5.T4.2.2.6\" class=\"ltx_td ltx_align_right ltx_border_b\">80.33</td>\n<td id=\"S5.T4.2.2.7\" class=\"ltx_td ltx_align_right ltx_border_b\">78.12</td>\n<td id=\"S5.T4.2.2.8\" class=\"ltx_td ltx_align_right ltx_border_b\">71.32</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "To understand the contributions of the two loss terms PIP-loss (Lpipsubscript𝐿pipL_{\\rm pip}) and contextual alignment loss (Lcontsubscript𝐿contL_{\\rm cont}), we conduct an ablation study where we train NPMS with three sources using only one of the two losses at a time.\nFrom Table 4, we see that in both WiC and WSD (ALL, SE2, SE3, SE15), the best performance is obtained by using both losses.\nEach loss contributes differently in different datasets, although the overall difference between the two losses is non-significant (according to a paired Student’s t𝑡t-test with p<0.05𝑝0.05p<0.05).\nThis is particularly encouraging because PIP-loss can be computed without having access to a sense labelled corpus such as SemCor.\nSuch resources might not be available in specialised domains such as medical or legal texts.\nTherefore, in such cases we can still apply NPMS trained using only the PIP-loss.\nAlthough we considered a linearly-weighted combination of the two losses in (5), we believe further improvements might be possible by exploring more complex (nonlinear) combinations of the two losses.\nHowever, exploring such combinations is beyond the scope of current paper and is deferred to future work."
        ]
    }
}