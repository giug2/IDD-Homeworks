{
    "S3.T1": {
        "caption": "Table 1:  Results of 5-way 1-shot and 5-way 5-shot on HuffPost headlines dataset, 10-way 1-shot, 10-way 5-shot, 15-way 1-shot and 15-way 5-shot on Banking77 and Clinc150 datasets (cross domain) by using our proposed method and all baselines.",
        "table": null,
        "footnotes": [],
        "references": [
            "The total results of experiments are reported in table 1. By observing these experimental results, we obtain the following conclusions:",
            "(1) Whether it is for texts with minimal semantics (Huffpost), fine-grained categorized (Banking77) or cross domains (Clinc150), our proposed method AMGS has an average improvement of 0.2-6.5% over the state-of-the-art model on both 1-shot and 5-shot classification. In particular, compared with our AMGS and MAML Finn et¬†al. (2017), Reptile Nichol et¬†al. (2018), we can draw the following observations from the Table 1: (i) Our proposed method achieves better performance on all tasks. In especial, in the 15-Way 5-shot task on Clinc150 dataset, our proposed method outperforms the best counterpart by 18.9%. (ii) MAML and Reptile perform better on fine-grained classification Banking77 dataset with more similar categories than on cross-domain Clinc150 dataset with less similar categories, and have a smaller gap with our AMGS. To verify that our AMGS perform better than MAML on alleviating overfitting, we plot their accuracy learning curves in Figure 3. In the figure, the training procudure of our AMGS is more stable than that of MAML from the beginning to the end. Besides, the gap between the accuracy of seen classes and unseen classes of our AMGS is less than that of MAML. These results are demonstrated that our AMGS can make model more stable in meta-training and more readily generalizeds to unseen classes by addressing the overfitting issue."
        ]
    },
    "S4.T2": {
        "caption": "Table 2:  Ablation study results for different strategies of meta-learner on Banking77 and Clinc150 (cross domain) datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this section, we further investigate the impact of the different strategies for meta-learner. To compare with our strategy, we design three other comparison strategies. As shown in Table 2, \"AMGS w que\", \"AMGS w sup\", \"AMGS w que+sup\" respectively represent the meta-learner in AMGS only use the gradients of the query set, support set and both query and support set. None of these three strategies pay attention to distinguishing the positive or the negative of the gradients. Comparing our strategy with \"AMGS w que+sup\" strategy, we have improved significantly more on 1-shot task than on 5-shot task. From all the results, our adaptive meta-learner which filters the impact of the negative gradient achieves the better performance among these compared strategies."
        ]
    },
    "S4.T3": {
        "caption": "Table 3:  Ablation study results of MTP in meta-training and meta-testing fast adaptation phase (testing) on Banking77 and Clinc150 (cross domain) datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "As shown in Table 3, we first eliminate MTP in training stage. After losing a richer distribution of tasks, the performances of AMGS decrease by about 0.8%, which verifies the effectiveness of MTP in meta-training phase. Further, we explore MTP in meta-testing fast adaptation phase. The empirical results demonstrate that after joining the auxiliary task in meta-testing, the model performances have increased by about 0.5%. The testing auxiliary task makes the primary task more robust on the support set, and has some suppression effects on the occurrence of overfitting. All these results demonstrate that MTP task have a certain effect on Banking77 and Clinc150 datasets, but it can not significantly improve the experimental results.",
            "As Table 3 shows, we explore the effect of different masking probabilities and strategies on Banking77. In the table, \"Mask\" means that we replace the token with [MASK] in MTP, \"Same\" means that we keep the target token unchanged and \"Random\" means that the token is replaced with the random token except itself. From the table, we can see that the masking strategy in BERT pre-training is not the best choice in the few-shot text classification. Therefore, in this paper, we attempt to alter the masking strategy which 100% changes the target token to [MASK]."
        ]
    },
    "A1.T4": {
        "caption": "Table 4:  Ablation study results of different masking strategies on the validation episodes of Banking77.",
        "table": null,
        "footnotes": [],
        "references": [
            "The results of validation episodes have shown in Table 4. We explore a large scale trade-off œÅùúå\\rho. demonstrating MTP has the greatest contribution when the trade-off œÅùúå\\rho equals 0.001. Especially, œÅùúå\\rho equals 0 means we remove the impact of MTP, which verifies the effectiveness of our MTP."
        ]
    },
    "A2.T5": {
        "caption": "Table 5:  Sensitivity study results of 10-way 5-shot on the validation episodes of Banking77.",
        "table": null,
        "footnotes": [],
        "references": []
    }
}