{
    "PAPER'S NUMBER OF TABLES": 4,
    "S5.T1": {
        "caption": "Table 1: Comparisons of offline false accept and reject rates for various\noptimizers on non-IID data.",
        "table": "<table id=\"S5.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T1.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Optimizer</span></th>\n<th id=\"S5.T1.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T1.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">FA [%]</span></th>\n<th id=\"S5.T1.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T1.3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">FR [%]</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.3.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S5.T1.3.2.1.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedAvg</span></td>\n<td id=\"S5.T1.3.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S5.T1.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></td>\n<td id=\"S5.T1.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T1.3.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">8.76</span></td>\n</tr>\n<tr id=\"S5.T1.3.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.3.2.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T1.3.3.2.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedAvg</span><span id=\"S5.T1.3.3.2.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> + NAG</span>\n</td>\n<td id=\"S5.T1.3.3.2.2\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T1.3.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></td>\n<td id=\"S5.T1.3.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.3.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.09</span></td>\n</tr>\n<tr id=\"S5.T1.3.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.4.3.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T1.3.4.3.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedAdam</span></td>\n<td id=\"S5.T1.3.4.3.2\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T1.3.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.19</span></td>\n<td id=\"S5.T1.3.4.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.3.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.95</span></td>\n</tr>\n<tr id=\"S5.T1.3.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.5.4.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S5.T1.3.5.4.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedAdam</span><span id=\"S5.T1.3.5.4.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> + NAG</span>\n</td>\n<td id=\"S5.T1.3.5.4.2\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T1.3.5.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.21</span></td>\n<td id=\"S5.T1.3.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.3.5.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.68</span></td>\n</tr>\n<tr id=\"S5.T1.3.6.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.6.5.1\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T1.3.6.5.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedYogi</span></td>\n<td id=\"S5.T1.3.6.5.2\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T1.3.6.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.19</span></td>\n<td id=\"S5.T1.3.6.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T1.3.6.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.39</span></td>\n</tr>\n<tr id=\"S5.T1.3.7.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.7.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span id=\"S5.T1.3.7.6.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">FedYogi</span><span id=\"S5.T1.3.7.6.1.2\" class=\"ltx_text\" style=\"font-size:90%;\"> + NAG</span>\n</td>\n<td id=\"S5.T1.3.7.6.2\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S5.T1.3.7.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td id=\"S5.T1.3.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T1.3.7.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.11</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Optimization techniques were explored for non-IID training. First, the\nalgorithms described in Section ",
                "3",
                " were tuned via grid\nsearches. For ",
                "FedAvg",
                ", ",
                "η",
                "s",
                "=",
                "1.0",
                "subscript",
                "𝜂",
                "𝑠",
                "1.0",
                "\\eta_{s}=1.0",
                " and a momentum value of ",
                "0.99",
                "0.99",
                "0.99",
                " was\nfound to work best. ",
                "FedAdam",
                " and ",
                "FedYogi",
                " both converged well\nwith ",
                "β",
                "1",
                "=",
                "0.9",
                "subscript",
                "𝛽",
                "1",
                "0.9",
                "\\beta_{1}=0.9",
                " and ",
                "β",
                "2",
                "=",
                "0.999",
                "subscript",
                "𝛽",
                "2",
                "0.999",
                "\\beta_{2}=0.999",
                ", though Adam worked with the default\n",
                "ϵ",
                "=",
                "10",
                "−",
                "8",
                "italic-ϵ",
                "superscript",
                "10",
                "8",
                "\\epsilon=10^{-8}",
                " and ",
                "η",
                "s",
                "=",
                "10",
                "−",
                "3",
                "subscript",
                "𝜂",
                "𝑠",
                "superscript",
                "10",
                "3",
                "\\eta_{s}=10^{-3}",
                " while Yogi worked best with a larger\n",
                "ϵ",
                "=",
                "10",
                "−",
                "3",
                "italic-ϵ",
                "superscript",
                "10",
                "3",
                "\\epsilon=10^{-3}",
                ", ",
                "η",
                "s",
                "=",
                "0.1",
                "subscript",
                "𝜂",
                "𝑠",
                "0.1",
                "\\eta_{s}=0.1",
                ", and initial accumulator value of ",
                "10",
                "−",
                "6",
                "superscript",
                "10",
                "6",
                "10^{-6}",
                ".\nExperiments were also performed in which Nesterov accelerated gradients were\nsubstituted for classical momenta.",
                "Table ",
                "1",
                " compares non-IID training with each server\noptimization algorithm. Exponentially decayed client learning rates were used\nwith ",
                "FedAdam",
                " and ",
                "FedYogi",
                ", while ",
                "FedAvg",
                " worked better\nwith constant client learning rates. The adaptive optimizers had a decisive\nadvantage over ",
                "FedAvg",
                " on FR. Replacing classical momentum with NAG\nbenefitted ",
                "FedAvg",
                " and ",
                "FedAdam",
                ", but ",
                "FedYogi",
                " with\nclassical momentum had the lowest FR overall.",
                "Theoretical and empirical results indicate that client learning rate (LR) decay\nimproves convergence on non-IID\ndata ",
                "[",
                "20",
                ", ",
                "24",
                ", ",
                "21",
                "]",
                ". Fixed client learning\nrates (with ",
                "η",
                "c",
                "=",
                "0.02",
                "subscript",
                "𝜂",
                "𝑐",
                "0.02",
                "{\\eta}_{c}=0.02",
                ") were compared with exponentially-decayed client\nlearning rate schedules, in which ",
                "η",
                "c",
                "subscript",
                "𝜂",
                "𝑐",
                "{\\eta}_{c}",
                " was reduced by a constant factor,\n",
                "Γ",
                "η",
                ",",
                "c",
                "subscript",
                "Γ",
                "𝜂",
                "𝑐",
                "{\\Gamma}_{\\eta,c}",
                ", after every ",
                "N",
                "Γ",
                "subscript",
                "𝑁",
                "Γ",
                "N_{\\Gamma}",
                " steps. Hyperparameter scans found\nthat the eval loss was minimized with an initial learning rate\n",
                "η",
                "c",
                ",",
                "0",
                "=",
                "0.02",
                "subscript",
                "𝜂",
                "𝑐",
                "0",
                "0.02",
                "{\\eta}_{c,0}=0.02",
                ", ",
                "Γ",
                "η",
                ",",
                "c",
                "=",
                "0.9",
                "subscript",
                "Γ",
                "𝜂",
                "𝑐",
                "0.9",
                "{\\Gamma}_{\\eta,c}=0.9",
                ", and ",
                "N",
                "Γ",
                "=",
                "1000",
                "subscript",
                "𝑁",
                "Γ",
                "1000",
                "N_{\\Gamma}=1000",
                ".",
                "Learning rate comparisons are shown in Table ",
                "2",
                ", for the\n",
                "FedYogi",
                " optimizer. LR decay significantly improves both IID and non-IID\ntraining. The difference is most pronounced for non-IID training, where the FR\ndecreases from 2.35% to 1.39% given a fixed FA=0.2%."
            ]
        ]
    },
    "S5.T2": {
        "caption": "Table 2: A comparison of client learning rate schedules.",
        "table": "<table id=\"S5.T2.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S5.T2.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">LR schedule</span></th>\n<th id=\"S5.T2.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T2.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">FR (IID) [%]</span></th>\n<th id=\"S5.T2.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T2.3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">FR (Non-IID) [%]</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span id=\"S5.T2.3.2.1.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Constant</span></th>\n<td id=\"S5.T2.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T2.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.14</span></td>\n<td id=\"S5.T2.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T2.3.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.35</span></td>\n</tr>\n<tr id=\"S5.T2.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S5.T2.3.3.2.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Exponential</span></th>\n<td id=\"S5.T2.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.3.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.73</span></td>\n<td id=\"S5.T2.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T2.3.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.39</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Optimization techniques were explored for non-IID training. First, the\nalgorithms described in Section ",
                "3",
                " were tuned via grid\nsearches. For ",
                "FedAvg",
                ", ",
                "η",
                "s",
                "=",
                "1.0",
                "subscript",
                "𝜂",
                "𝑠",
                "1.0",
                "\\eta_{s}=1.0",
                " and a momentum value of ",
                "0.99",
                "0.99",
                "0.99",
                " was\nfound to work best. ",
                "FedAdam",
                " and ",
                "FedYogi",
                " both converged well\nwith ",
                "β",
                "1",
                "=",
                "0.9",
                "subscript",
                "𝛽",
                "1",
                "0.9",
                "\\beta_{1}=0.9",
                " and ",
                "β",
                "2",
                "=",
                "0.999",
                "subscript",
                "𝛽",
                "2",
                "0.999",
                "\\beta_{2}=0.999",
                ", though Adam worked with the default\n",
                "ϵ",
                "=",
                "10",
                "−",
                "8",
                "italic-ϵ",
                "superscript",
                "10",
                "8",
                "\\epsilon=10^{-8}",
                " and ",
                "η",
                "s",
                "=",
                "10",
                "−",
                "3",
                "subscript",
                "𝜂",
                "𝑠",
                "superscript",
                "10",
                "3",
                "\\eta_{s}=10^{-3}",
                " while Yogi worked best with a larger\n",
                "ϵ",
                "=",
                "10",
                "−",
                "3",
                "italic-ϵ",
                "superscript",
                "10",
                "3",
                "\\epsilon=10^{-3}",
                ", ",
                "η",
                "s",
                "=",
                "0.1",
                "subscript",
                "𝜂",
                "𝑠",
                "0.1",
                "\\eta_{s}=0.1",
                ", and initial accumulator value of ",
                "10",
                "−",
                "6",
                "superscript",
                "10",
                "6",
                "10^{-6}",
                ".\nExperiments were also performed in which Nesterov accelerated gradients were\nsubstituted for classical momenta.",
                "Table ",
                "1",
                " compares non-IID training with each server\noptimization algorithm. Exponentially decayed client learning rates were used\nwith ",
                "FedAdam",
                " and ",
                "FedYogi",
                ", while ",
                "FedAvg",
                " worked better\nwith constant client learning rates. The adaptive optimizers had a decisive\nadvantage over ",
                "FedAvg",
                " on FR. Replacing classical momentum with NAG\nbenefitted ",
                "FedAvg",
                " and ",
                "FedAdam",
                ", but ",
                "FedYogi",
                " with\nclassical momentum had the lowest FR overall.",
                "Theoretical and empirical results indicate that client learning rate (LR) decay\nimproves convergence on non-IID\ndata ",
                "[",
                "20",
                ", ",
                "24",
                ", ",
                "21",
                "]",
                ". Fixed client learning\nrates (with ",
                "η",
                "c",
                "=",
                "0.02",
                "subscript",
                "𝜂",
                "𝑐",
                "0.02",
                "{\\eta}_{c}=0.02",
                ") were compared with exponentially-decayed client\nlearning rate schedules, in which ",
                "η",
                "c",
                "subscript",
                "𝜂",
                "𝑐",
                "{\\eta}_{c}",
                " was reduced by a constant factor,\n",
                "Γ",
                "η",
                ",",
                "c",
                "subscript",
                "Γ",
                "𝜂",
                "𝑐",
                "{\\Gamma}_{\\eta,c}",
                ", after every ",
                "N",
                "Γ",
                "subscript",
                "𝑁",
                "Γ",
                "N_{\\Gamma}",
                " steps. Hyperparameter scans found\nthat the eval loss was minimized with an initial learning rate\n",
                "η",
                "c",
                ",",
                "0",
                "=",
                "0.02",
                "subscript",
                "𝜂",
                "𝑐",
                "0",
                "0.02",
                "{\\eta}_{c,0}=0.02",
                ", ",
                "Γ",
                "η",
                ",",
                "c",
                "=",
                "0.9",
                "subscript",
                "Γ",
                "𝜂",
                "𝑐",
                "0.9",
                "{\\Gamma}_{\\eta,c}=0.9",
                ", and ",
                "N",
                "Γ",
                "=",
                "1000",
                "subscript",
                "𝑁",
                "Γ",
                "1000",
                "N_{\\Gamma}=1000",
                ".",
                "Learning rate comparisons are shown in Table ",
                "2",
                ", for the\n",
                "FedYogi",
                " optimizer. LR decay significantly improves both IID and non-IID\ntraining. The difference is most pronounced for non-IID training, where the FR\ndecreases from 2.35% to 1.39% given a fixed FA=0.2%."
            ]
        ]
    },
    "S5.T3": {
        "caption": "Table 3: FA and FR comparisons for models trained on IID and non-IID data with\ndifferent data augmentations.",
        "table": "<table id=\"S5.T3.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S5.T3.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Data Augmentation</span></th>\n<th id=\"S5.T3.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S5.T3.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Data type</span></th>\n<th id=\"S5.T3.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T3.3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">FA [%]</span></th>\n<th id=\"S5.T3.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T3.3.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">FR [%]</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span id=\"S5.T3.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">No augmentation</span></th>\n<th id=\"S5.T3.3.2.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span id=\"S5.T3.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">IID</span></th>\n<td id=\"S5.T3.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T3.3.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.17</span></td>\n<td id=\"S5.T3.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T3.3.2.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.20</span></td>\n</tr>\n<tr id=\"S5.T3.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T3.3.3.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">No augmentation</span></th>\n<th id=\"S5.T3.3.3.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T3.3.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Non-IID</span></th>\n<td id=\"S5.T3.3.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td id=\"S5.T3.3.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.3.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">3.19</span></td>\n</tr>\n<tr id=\"S5.T3.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T3.3.4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MTR</span></th>\n<th id=\"S5.T3.3.4.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T3.3.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">IID</span></th>\n<td id=\"S5.T3.3.4.3.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.13</span></td>\n<td id=\"S5.T3.3.4.3.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.4.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.96</span></td>\n</tr>\n<tr id=\"S5.T3.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T3.3.5.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">MTR</span></th>\n<th id=\"S5.T3.3.5.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T3.3.5.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Non-IID</span></th>\n<td id=\"S5.T3.3.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.5.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.18</span></td>\n<td id=\"S5.T3.3.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.5.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.15</span></td>\n</tr>\n<tr id=\"S5.T3.3.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T3.3.6.5.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">SpecAugment</span></th>\n<th id=\"S5.T3.3.6.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T3.3.6.5.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">IID</span></th>\n<td id=\"S5.T3.3.6.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.6.5.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td id=\"S5.T3.3.6.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T3.3.6.5.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.73</span></td>\n</tr>\n<tr id=\"S5.T3.3.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S5.T3.3.7.6.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">SpecAugment</span></th>\n<th id=\"S5.T3.3.7.6.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S5.T3.3.7.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">Non-IID</span></th>\n<td id=\"S5.T3.3.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.3.7.6.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.19</span></td>\n<td id=\"S5.T3.3.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T3.3.7.6.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.39</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Two common speech data augmentation methods—MTR ",
                "[",
                "37",
                "]",
                " and\nSpecAugment ",
                "[",
                "38",
                "]",
                "—were tuned for non-IID training.",
                "MTR is an acoustic room simulator that generates noise files which can be\napplied to spectrogram inputs. Based on ",
                "a priori",
                " distributions, MTR\ngenerates random room sizes and dimensions, speaker and noise source positions,\nsignal to noise ratios, and reverberation times ",
                "[",
                "39",
                "]",
                ". The technique is\neffective for far-field speech recognition and has been used previously for\nkeyword spotting ",
                "[",
                "10",
                ", ",
                "8",
                "]",
                ".",
                "In the simulation experiments, MTR was used to create up to 100 noised replica\nof each clean utterance from vendor data. In order to keep a constant number of\ntraining examples per simulated client, MTR configurations were randomly sampled\nevery time a given simulated client device was used for training.",
                "Unfortunately, MTR is infeasible for on-device training: users would have to\ndownload additional noise data (for additive noise) and room simulation\nconfigurations (for reverberations). The extra data processing would also\nlengthen client training times.",
                "Spectrum augmentation (SpecAugment) is a fast and lightweight alternative for\nspeech data augmentation. It has been used previously for keyword\nspotting ",
                "[",
                "40",
                "]",
                ", and has been used to achieve state-of-the-art ASR\nperformance ",
                "[",
                "41",
                "]",
                ". The augmentation policy broadly consists of\nthree components: (1) ",
                "Time Masking",
                ", in which consecutive time frames in\nthe spectrogram are masked and replaced with Gaussian-distributed noise, (2)\n",
                "Frequency Masking",
                ", in which adjacent bins of the spectrogram are\nzeroed, and (3) ",
                "Time Warping",
                ", in which features are linearly displaced\nalong the temporal axis.",
                "SpecAugment is an ideal on-device alternative to MTR, as it requires no config\nfiles and minimally increases training time. Tuning in non-IID data simulations\nfound an optimal configuration of 2 time masks of up to 60 frames along with 2\nfrequency masks of up to 15 bins. TimeWarp was not used.",
                "Augmentation strategies for IID and non-IID FL are compared in\nTable ",
                "3",
                ". SpecAugment reduced the FR with respect to MTR and\nno augmentation on both data distributions. Thus, we can reduce communication\ncosts and on-device processing time with SpecAugment while also improving\nperformance."
            ]
        ]
    },
    "S5.T4": {
        "caption": "Table 4: FR comparisons for on-device labeling strategies.",
        "table": "<table id=\"S5.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S5.T4.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Labeling</span></th>\n<th id=\"S5.T4.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T4.3.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">FR (IID) [%]</span></th>\n<th id=\"S5.T4.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T4.3.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">FR (Non-IID) [%]</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\"><span id=\"S5.T4.3.2.1.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Supervised</span></th>\n<td id=\"S5.T4.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T4.3.2.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.73</span></td>\n<td id=\"S5.T4.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S5.T4.3.2.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.39</span></td>\n</tr>\n<tr id=\"S5.T4.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\"><span id=\"S5.T4.3.3.2.1.1\" class=\"ltx_text ltx_font_typewriter\" style=\"font-size:90%;\">Teacher</span></th>\n<td id=\"S5.T4.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T4.3.3.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.12</span></td>\n<td id=\"S5.T4.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T4.3.3.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.07</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "High-quality labeling can be difficult to obtain on-device, since peeking at\ndata is impossible by design in FL, and user feedback signals are unreliable or\ninfrequent. Given the obstacles to on-device labeling, teacher student training\ncan be used to adapt a model trained on the server (with manually labeled data)\nto the on-device unlabeled data domain ",
                "[",
                "42",
                ", ",
                "43",
                ", ",
                "44",
                "]",
                ".\nModels were trained on both IID and non-IID data with supervised and\nteacher-generated labels. For the semi-supervised setting, the teacher model\narchitecture was identical to the student, but was trained on additional data in\na centralized setting.",
                "Table ",
                "4",
                " compares teacher student training with supervised\ntraining. While the FR increases when moving to semi-supervised labels, it is\nexpected that the matched data available in true on-device data, coupled with\na limited number of samples labeled with user feedback signals, will close the\nperformance gap."
            ]
        ]
    }
}