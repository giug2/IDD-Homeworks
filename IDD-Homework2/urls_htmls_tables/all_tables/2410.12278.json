{
    "id_table_1": {
        "caption": "Table 1:  Average corpus distance between non-hallucinated samples and hallucinated generations over six generators. Lower the distance the better a method is (   \\downarrow  ).",
        "table": "S3.T1.4.4",
        "footnotes": [],
        "references": [
            "In this paper, we propose a generic approach to curate synthetic datasets for training hallucination detectors (hallucination datasets), as shown in Figure  1 . Our approach features a two-step  Generation-Selection  pipeline: we first generate a group of hallucinated candidates for a given input through an LLM (generator) and then select the best candidate through an LLM (judge) based on a given criteria. The synthetic datasets are then used as training data to develop  post-hoc  hallucination detectors. To answer the two key questions, we propose two design features in the generation step to customize hallucination generation and improve dataset quality:  Hallucination Pattern Guidance (HPG)  and  Language Style Alignment (LSA) .",
            "We manually curate three hallucination patterns for our experiments, including non-sensical response, inconsistent entity, and irrelevant content. Each hallucination pattern is associated with a demonstration example. 1 1 1 We also run the experiments using five automatically curated hallucination patterns. The generalization evaluation is deferred to Table  10  and  11  in Appendix  E .  Details on the hallucination patterns are in Appendix  D . We generate three hallucination candidates per sample for each pattern. As a result, each synthetic dataset contains 4000 samples, including 1000 non-hallucinated responses and 3000 hallucinated responses. Detailed parameters for the generation pipeline and fine-tuning are deferred to Appendix  A .",
            "Distances between hallucinated and non-hallucinated responses are reported in Table  1 . The distances are consistently smaller with our approach,  demonstrating that our approach generates hallucinated samples closer to the real human non-hallucinated samples . Specifically, compared with SimPrompt, our hallucinated responses are 12.0%, 11.5% and 6.8% closer to the good ones on average for OpenDialKG, ReDial, and SalesBot, respectively. The distance improvement is even larger when compared with HaluEval. The reduced corpus distance implies that our approach guides the generation to resemble the language features of non-hallucinated samples. This resemblance makes it more difficult for a detector to focus on trivial language features irrelevant to detecting hallucinations.",
            "We run the generalization investigation based off datasets generated using the automatically curated hallucination patterns, and the results are reported in Tables  10  and  11 . In line with the findings in the main, it is observed that detectors trained using our data pipeline possess better OTG and OGG generalization and robustness across the board. Such results confirm that our pipeline is agnostic to the source of hallucination patterns."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Hallucination detection performance by category. Reported are average F1 scores over six generators and five data mixture strategies. OP indicates that the data of the column pattern is removed from the training data.  Bold  indicates the best in class, and  underline  signals the second.",
        "table": "S3.T2.1.1",
        "footnotes": [],
        "references": [
            "Generator is a prompted LLM performing the task of hallucinated sample generation. We utilize LLMs to generate hallucinated data, as they are proven to generate high-quality text while following instructions. The key here is to properly design the prompts for hallucination guidance and language style alignment. Besides, it is important to carefully specify the persona in the system prompt to work around the safety policies in place that prevent LLMs from generating hallucinations. We adopt the chain-of-thought (CoT)  (Wei et al.,  2022 )  prompt and ask the generator to provide rationale for the generated samples, inspired by  Peng et al. ( 2023 ) . Our generator prompt is structured as follows: The prompt starts with a definition of persona customized for target tasks, which is followed by a section of HPG consisting of the pattern description and one demonstration example. The next is the LSA section, which comprises itemized guidelines for text generation. The prompt ends with an input and brief instructions on the output format. The details of prompts are deferred to Section  2.2  and Section  2.3 .",
            "Table  2  reports the average of F1 scores recorded by ICL detectors and supervised detectors (Vanilla and Mixture). For more detailed results by individual models, refer to Table  6  in the appendix. For ICL detectors, the LLM is assessed on the synthetic dataset generated by the same LLM. For supervised detectors, the reported performance is in-generator performancethe detectors are assessed on the test dataset generated by the same LLM as the training dataset. We find that ICL detectors still face significant challenges identifying hallucinations generated by themselves; the average F1 score is only 0.613 across the board. Fine-tuned detectors, in contrast, exhibit stronger performance consistently. For vanilla fine-tuned detectors trained on synthetic datasets generated by specific LLMs, the average F1 scores for six models are 0.920, 0.932, and 0.963 on OpenDialKG, ReDial, and SalesBot, respectively.",
            "For the detection performance, Table  6  and Table  7  provide detailed hallucination detection performance by hallucination category, as a supplement to the performance reported in Table  2 . As for the generalization investigation, detailed results on the out-of-generator generalization and robustness are reported in Table  8  and results for cross-task generalization are provided in Table   9 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Out-of-generator & out-of-task generalization. The reported metrics in the performance panel are F1 scores on out-of-generator and out-of-task datasets, and the metric for robustness is the standard deviation of F1 scores on out-of-generator datasets.  Bold  indicates the best in class, and  underline  signals the second in each section.",
        "table": "S3.T3.3.3",
        "footnotes": [],
        "references": [
            "Generator is a prompted LLM performing the task of hallucinated sample generation. We utilize LLMs to generate hallucinated data, as they are proven to generate high-quality text while following instructions. The key here is to properly design the prompts for hallucination guidance and language style alignment. Besides, it is important to carefully specify the persona in the system prompt to work around the safety policies in place that prevent LLMs from generating hallucinations. We adopt the chain-of-thought (CoT)  (Wei et al.,  2022 )  prompt and ask the generator to provide rationale for the generated samples, inspired by  Peng et al. ( 2023 ) . Our generator prompt is structured as follows: The prompt starts with a definition of persona customized for target tasks, which is followed by a section of HPG consisting of the pattern description and one demonstration example. The next is the LSA section, which comprises itemized guidelines for text generation. The prompt ends with an input and brief instructions on the output format. The details of prompts are deferred to Section  2.2  and Section  2.3 .",
            "The HPG module needs a set of predetermined hallucination patterns. Each pattern consists of a short description and a demonstration, including an input, a non-hallucinated output, and a hallucinated output of the pattern. With the HPG section, the generator follows the instruction to generate hallucinated candidates in a controlled rather than open-ended manner. Our approach relies on human judgment to determine the hallucination patterns in their target applications. Such patterns can be generic,  e.g. , overconfidence and non-factuality, or task-specific, such as confusing between entities in response. Practitioners have the flexibility to include the most common and relevant hallucinations by simply writing out descriptions and curating demonstrations. Moreover, it is worth noting that the predefined pattern can go beyond the conventional definition of hallucination and include any undesired LLM behaviors that we want to detect. For example, in the experiment to be presented in Section  3 , we include the pattern of nonsensical responses, where the generated responses bear no meaning in the context.",
            "As shown in the performance panel of table  3 , supervised detectors continue delivering superior performance than ICL detectors across the board, though slightly underperform the in-the-generator detectors. Besides, we observe that supervised detectors trained with mixed data outperform vanilla supervised detectors on out-of-generator datasets by 0.032 on average. This indicates that data mixture is a simple yet effective approach to increasing the OGG ability. Moreover, mixture trained detectors outperform both SimPrompt and HaluEval by 0.011 and 0.112 respectively.",
            "Performance is reported in Table  3 . Vanilla trained detectors outperform the ICL detectors by 0.257 on average on the three tasks; Mixture trained detectors outperform by the margin of 0.247. Compared with in-domain scenarios, the out-of-task performance degrades by 0.066 and 0.071 using mixture and vanilla trained supervised detectors, respectively. The findings suggest that supervised detectors produced based off our data pipeline are superior alternatives to ICL in the scenarios of light-weight development, offering plug-and-play capability with great generalization."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Ablation study. Column names denote the generation setup for train datasets and row names denote the setup for test datasets.",
        "table": "S3.T4.1.1",
        "footnotes": [],
        "references": [
            "We test this conjecture in Table  4  with an ablation on LSA and HPG components. Note that values on the diagonal are higher as expected since the train and test sets are more similar. We observe that the average performance on test hallucinations generated with both LSA and HPG is much lower than the ones w/o LSA or w/o HPG, supporting the conjecture that the synthetic samples become easier to detect without LSA and HPG. Particularly, w/o HPG, the generated hallucinations become too trivial for the detector to detect, with an average F1 of 0.973 versus 0.908 w/ HPG across three benchmarks. LSA also makes the hallucinations harder to detect, though with a lower effect compared to HPG (0.917 average F1 w/o versus 0.908 w/ LSA)."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Hallucination pattern guidance information.",
        "table": "A4.T5.1",
        "footnotes": [],
        "references": [
            "We use three identical hallucination patterns to generate synthetic datasets for all three benchmark tasks. The details of the hallucination patterns are summarized in Table  5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Detailed results for hallucination detection performance by category. Reported are F1 scores.",
        "table": "A5.T6.1.1",
        "footnotes": [],
        "references": [
            "Table  2  reports the average of F1 scores recorded by ICL detectors and supervised detectors (Vanilla and Mixture). For more detailed results by individual models, refer to Table  6  in the appendix. For ICL detectors, the LLM is assessed on the synthetic dataset generated by the same LLM. For supervised detectors, the reported performance is in-generator performancethe detectors are assessed on the test dataset generated by the same LLM as the training dataset. We find that ICL detectors still face significant challenges identifying hallucinations generated by themselves; the average F1 score is only 0.613 across the board. Fine-tuned detectors, in contrast, exhibit stronger performance consistently. For vanilla fine-tuned detectors trained on synthetic datasets generated by specific LLMs, the average F1 scores for six models are 0.920, 0.932, and 0.963 on OpenDialKG, ReDial, and SalesBot, respectively.",
            "Supervised detectors with data mixture perform slightly worse than vanilla supervised detectors on average (0.938 versus 0.912). The same pattern is observed in each hallucination pattern category and benchmark task. Since we control the sample size, the degraded performance suggests that synthetic datasets generated by different LLMs are distant in distribution, so merging datasets without scaling the sample size is at the cost of model performance. Interestingly, the conclusion holds even for datasets generated by the LLMs in the same family (see Table  6  in Appendix  E ). We conjecture that model size plays a role in generation distributions, as models in the same family usually share the training corpus.",
            "For the detection performance, Table  6  and Table  7  provide detailed hallucination detection performance by hallucination category, as a supplement to the performance reported in Table  2 . As for the generalization investigation, detailed results on the out-of-generator generalization and robustness are reported in Table  8  and results for cross-task generalization are provided in Table   9 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Detailed results for out-of-pattern performance using manual hallucination patterns. Reported are F1 scores.",
        "table": "A5.T7.1.1",
        "footnotes": [],
        "references": [
            "For the detection performance, Table  6  and Table  7  provide detailed hallucination detection performance by hallucination category, as a supplement to the performance reported in Table  2 . As for the generalization investigation, detailed results on the out-of-generator generalization and robustness are reported in Table  8  and results for cross-task generalization are provided in Table   9 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Detailed results for out-of-generator generalization using manual hallucination patterns. Reported are F1 scores. IG stands for in-the-Generator performance while OG stands for out-of-generator.",
        "table": "A5.T8.3.3",
        "footnotes": [],
        "references": [
            "For the detection performance, Table  6  and Table  7  provide detailed hallucination detection performance by hallucination category, as a supplement to the performance reported in Table  2 . As for the generalization investigation, detailed results on the out-of-generator generalization and robustness are reported in Table  8  and results for cross-task generalization are provided in Table   9 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Results of out-of-task generalization using manually curated hallucination patterns. Reported are average F1 scores across six generators using vanilla trained detectors.",
        "table": "A5.T9.24.24",
        "footnotes": [],
        "references": [
            "For the detection performance, Table  6  and Table  7  provide detailed hallucination detection performance by hallucination category, as a supplement to the performance reported in Table  2 . As for the generalization investigation, detailed results on the out-of-generator generalization and robustness are reported in Table  8  and results for cross-task generalization are provided in Table   9 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Detailed results for out-of-generator generalization using automatically generated hallucination patterns. Reported are F1 scores. IG stands for in-the-Generator performance while OG stands for out-of-generator.",
        "table": "A5.T10.3.3",
        "footnotes": [],
        "references": [
            "We manually curate three hallucination patterns for our experiments, including non-sensical response, inconsistent entity, and irrelevant content. Each hallucination pattern is associated with a demonstration example. 1 1 1 We also run the experiments using five automatically curated hallucination patterns. The generalization evaluation is deferred to Table  10  and  11  in Appendix  E .  Details on the hallucination patterns are in Appendix  D . We generate three hallucination candidates per sample for each pattern. As a result, each synthetic dataset contains 4000 samples, including 1000 non-hallucinated responses and 3000 hallucinated responses. Detailed parameters for the generation pipeline and fine-tuning are deferred to Appendix  A .",
            "We run the generalization investigation based off datasets generated using the automatically curated hallucination patterns, and the results are reported in Tables  10  and  11 . In line with the findings in the main, it is observed that detectors trained using our data pipeline possess better OTG and OGG generalization and robustness across the board. Such results confirm that our pipeline is agnostic to the source of hallucination patterns."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Results of out-of-task generalization using automatically generated hallucination patterns. Reported are average F1 scores across six generators using vanilla trained detectors.",
        "table": "A5.T11.6.6",
        "footnotes": [],
        "references": [
            "We manually curate three hallucination patterns for our experiments, including non-sensical response, inconsistent entity, and irrelevant content. Each hallucination pattern is associated with a demonstration example. 1 1 1 We also run the experiments using five automatically curated hallucination patterns. The generalization evaluation is deferred to Table  10  and  11  in Appendix  E .  Details on the hallucination patterns are in Appendix  D . We generate three hallucination candidates per sample for each pattern. As a result, each synthetic dataset contains 4000 samples, including 1000 non-hallucinated responses and 3000 hallucinated responses. Detailed parameters for the generation pipeline and fine-tuning are deferred to Appendix  A .",
            "We run the generalization investigation based off datasets generated using the automatically curated hallucination patterns, and the results are reported in Tables  10  and  11 . In line with the findings in the main, it is observed that detectors trained using our data pipeline possess better OTG and OGG generalization and robustness across the board. Such results confirm that our pipeline is agnostic to the source of hallucination patterns."
        ]
    }
}