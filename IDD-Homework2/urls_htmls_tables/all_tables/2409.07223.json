{
    "id_table_1": {
        "caption": "Table 1:  The parameters of the three problems in Section  5.1  and Algorithm  1 . Notation  a . b k formulae-sequence a subscript b k a.b_{k} italic_a . italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  denotes a number  a . b  10 k formulae-sequence a b superscript 10 k a.b\\times 10^{k} italic_a . italic_b  10 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT  and the dash   - -  means that the parameter does not exist in the problem.",
        "table": "A5.EGx1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "where  M M \\mathcal{M} caligraphic_M  is a  d d d italic_d -dimensional Riemannian manifold,  F : M  R : F  M R F:\\mathcal{M}\\rightarrow\\mathbb{R} italic_F : caligraphic_M  blackboard_R  is continuously differentiable but not necessarily convex, and it covers both important cases of the expected risk ( 1.2a ) or the empirical risk ( 1.2b )",
            "The problem whose objective function has the form of ( 1.2b ) arises from various applications, including but not limited to principal eigenvector computation over sphere manifold  [ GH15 ] , Frechet mean computation of points over symmetric positive definite matrix manifold  [ Bha07 ,  ZJRS16 ,  HMJG21 ]  or over hyperbolic manifold  [ Bon13 ] , low-rank matrix completion problem  [ MS13 ,  MKJS19 ] , low-dimensional multitask feature learning  [ JM18 ] , and hyperbolic structured prediction over hyperbolic manifold  [ MCR20 ] .",
            "Federated learning (FL), as a fairly promising distributed learning architecture, allows multiple agents to collaborate with a server to solve such problems ( 1.1 )  [ MMR + 23 ] . An important advantage of FL is that the data is held by each agent without being shared with the server and FL thus guarantees the privacy of the agents to some extent. Another remarkable feature of FL which distinguishes it from traditional distributed learning  [ TBA86 ]  is that each active agent is allowed to perform multiple local updates between two consecutive outer iterations, which can sufficiently make use of the computation ability of agents and reduce communication expense between the server and the agents.",
            "In the following, we focus on the expected risk minimization ( 1.2a ) and propose a generic FL algorithm with  S S S italic_S  agents to solve the problem defined on Riemannian manifolds. The resulting conclusions also hold for the empirical risk minimization ( 1.2b ).",
            "The counterparts in the Riemannian setting are made in Definitions  2.1   [ HAG18 ]  and  2.2   [ HW22 ] . The first property is called  L L L italic_L -Lipschitz continuous differentiability (Definition  2.1 ) in the Riemannian setting and the second is used as a generalization of the notion of  L L L italic_L -smoothness (Definition  2.2 ) to the Riemannian setting.",
            "Aggregation ( 3.8 ) combined with ( 3.7 ) can be viewed as a generalization of ( 3.1 ) combined with ( 3.4 ). Specific to each agent  j j j italic_j , it only needs to upload",
            "Summarizing the discussion above, this paper proposes a  R iemannian  Fed erated  A veraing  G radient  S tream (RFedAGS) algorithm, as stated in Algorithm  1 , which can be viewed as a generalization of FedAvg since RFedAGS is equivalent to FedAvg when the manifold  M M \\mathcal{M} caligraphic_M  is a Euclidean space.",
            "The convergence analysis is established based on Assumptions  4.1 ,  4.2 , and  4.3 , which are standard and have been used in federated learning, stochastic gradient methods, and Riemannian optimization; see e.g.,  [ HAG15 ,  HGA15 ,  TFBJ18 ,  ZC18 ,  WJ21 ,  HKMC19 ,  SKM19 ] .",
            "x  = arg  min x  M  F  ( x ) superscript x subscript arg min x M F x x^{*}=\\operatorname*{arg\\,min}_{x\\in\\mathcal{M}}F(x) italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_x  caligraphic_M end_POSTSUBSCRIPT italic_F ( italic_x ) , the outer iterates  { x ~ t } t  1 subscript subscript ~ x t t 1 \\{\\tilde{x}_{t}\\}_{t\\geq 1} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t  1 end_POSTSUBSCRIPT  and the inner iterates  { { { x t , k j } j = 1 S } k  0 } t  1 subscript subscript superscript subscript superscript subscript x t k j j 1 S k 0 t 1 \\{\\{\\{x_{t,k}^{j}\\}_{j=1}^{S}\\}_{k\\geq 0}\\}_{t\\geq 1} { { { italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k  0 end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t  1 end_POSTSUBSCRIPT  generated by Algorithm  1  remain in a compact and connected subset  W  M W M \\mathcal{W}\\subseteq\\mathcal{M} caligraphic_W  caligraphic_M 2 2 2 Here, the number of outer iterations  T T T italic_T  is assumed to be infinity. ;",
            "The existence of a totally retractive neighborhood  W W \\mathcal{W} caligraphic_W  of  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is guaranteed  [ HAG15 ] , and such assumptions as Assumptions  4.1 ( 1 ) and ( 2 ) have been used in, e.g.,  [ HAG15 ,  TFBJ18 ,  SKM19 ] . Assumptions  4.1 ( 3 ) and ( 5 ) are the standard requirements for analyzing convergence in the Euclidean setting, see, e.g.,  [ ZC18 ,  Sti19 ,  WJ21 ] , and thus we make the counterparts in the Riemannian setting. For commonly-encountered manifolds, e.g., Stiefel manifolds, Grassmann manifolds, and fixed rank matrix manifolds, we can construct an isometric vector transport by parallelization  [ HGA15 ,  HAG15 ] . For another manifold whose exponential map is computationally cheap, e.g., unit sphere manifolds, symmetric positive definite matrix manifolds, and Hyperbolic manifolds, the parallel transport is an alternative of the isometric vector transport  [ AMS08 ,  Bou23 ] . In machine learning, the step sizes are usually not large, and thus we assume that they are bounded from above by a constant  A A A italic_A .",
            "In the Euclidean setting, the convergence properties are based on the  L L L italic_L -smoothness of the objective function. We follow this approach in the Riemannian setting. Under Assumption  4.1 ( 4 ),  L L L italic_L -retraction smoothness of  F F F italic_F  implies that at the  t t t italic_t -th outer iteration, the following holds:",
            "Further, taking expectation over the randomness at the  t t t italic_t -th outer iteration conditioned on  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for Inequality ( 4.1 ) yields",
            "where  E t  [  ] subscript E t delimited-[]  \\mathbb{E}_{t}[\\cdot] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  ]  means the expectation over the randomness of the  t t t italic_t -outer iteration, and satisfies  E  [ F  ( x ~ t + 1 ) ] = E 1  E 2  ...  E t  [ F  ( x ~ t + 1 ) ] E delimited-[] F subscript ~ x t 1 subscript E 1 subscript E 2 ... subscript E t delimited-[] F subscript ~ x t 1 \\mathbb{E}[F(\\tilde{x}_{t+1})]=\\mathbb{E}_{1}\\mathbb{E}_{2}\\dots\\mathbb{E}_{t}% [F(\\tilde{x}_{t+1})] blackboard_E [ italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ] = blackboard_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ... blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [ italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ]  with  E  [  ] E delimited-[]  \\mathbb{E}[\\cdot] blackboard_E [  ]  being the total expectation since  x ~ t + 1 subscript ~ x t 1 \\tilde{x}_{t+1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT  completely determined by the independent random realizations  { { {   , k , s j } s  B  , k j }  = 1 t } k = 0 K  1 } j = 1 S \\{\\{\\{\\xi_{\\tau,k,s}^{j}\\}_{{s\\in\\mathcal{B}_{\\tau,k}^{j}}}\\}_{\\tau=1}^{t}\\}_{% k=0}^{K-1}\\}_{j=1}^{S} { { { italic_ start_POSTSUBSCRIPT italic_ , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_s  caligraphic_B start_POSTSUBSCRIPT italic_ , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K - 1 end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT . The subsequent convergence analyses are based on ( 4.2 ), and thus this paper focuses on bounding the terms on the right-hand side, as stated in Lemma  4.1 ,  4.2 ,  4.3  and  4.4 , whose proofs can be found in Appendices  A ,  B ,  C  and  D . An upper bound of the second term is given in Lemma  4.1 .",
            "The iterates  { x ~ t } subscript ~ x t \\{\\tilde{x}_{t}\\} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT }  generated by Algorithm  1  satisfy that",
            "At the  t t t italic_t -th outer iteration of Algorithm  1  with a fixed step size   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  within the inner iteration of each agent, we have that",
            "At the  k k k italic_k -th inner iteration of the  t t t italic_t -th outer iteration of Algorithm  1 , for each agent  j = 1 , 2 , ... , S j 1 2 ... S j=1,2,\\dots,S italic_j = 1 , 2 , ... , italic_S  and  k = 1 , 2 , ... , K  1 k 1 2 ... K 1 k=1,2,\\dots,K-1 italic_k = 1 , 2 , ... , italic_K - 1 , we have",
            "where the expectation is taken over the randomness at the  t t t italic_t -th outer iteration conditioned on  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,  M = ( C 2 2 + A 2  C 1 2  C 3 2 ) M superscript subscript C 2 2 superscript A 2 superscript subscript C 1 2 superscript subscript C 3 2 M=(C_{2}^{2}+A^{2}C_{1}^{2}C_{3}^{2}) italic_M = ( italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  is a positive constant,  A A A italic_A  is stated in Assumption  4.1 ( 6 ),  C 1 subscript C 1 C_{1} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is a constant such that   grad  F  ( x )   C 1 norm grad F x subscript C 1 \\|\\mathrm{grad}F(x)\\|\\leq C_{1}  roman_grad italic_F ( italic_x )   italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  for all  x  W x W x\\in\\mathcal{W} italic_x  caligraphic_W  (as Assumption  4.1 ( 6 )), and  C 2 subscript C 2 C_{2} italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  and  C 3 subscript C 3 C_{3} italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT  are the same as that in Lemma  4.3   3 3 3 In particular, when  M M \\mathcal{M} caligraphic_M  reduces to a Euclidean space, e.g.,  M = R d M superscript R d \\mathcal{M}=\\mathbb{R}^{d} caligraphic_M = blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , we have  M = 1 M 1 M=1 italic_M = 1 . .",
            "Next, this paper gives the first convergent result of the proposed RFedAGS, as stated in Theorem  4.1  built on the inequality ( 4.2 ), which claims the fact that the cost values at the consecutive iterates generated by RFedAGS are sufficient descent in some extent.",
            "If we run Algorithm  1  with a fixed step size   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and a fixed batch size  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  within parallel steps.",
            "Under conditions that we considered, it follows from Lemma  4.1  that",
            "which implies ( 4.9 ) holds for  K = 1 K 1 K=1 italic_K = 1 . For  K > 1 K 1 K>1 italic_K > 1 , plugging ( 4.6 ) and ( 4.10 ) into ( 4.2 ) yields",
            "From Theorem  4.1 , if     t   2    L  H  (    t , K , S ) < B   t   grad  F  ( x ~ t )  2 subscript    t superscript  2  L H subscript    t K S subscript   B t superscript norm grad F subscript ~ x t 2 {\\bar{\\alpha}_{t}\\sigma^{2}}\\delta LH(\\bar{\\alpha}_{t},K,S)<\\bar{B}_{t}\\|% \\mathrm{grad}\\,F(\\tilde{x}_{t})\\|^{2} over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ italic_L italic_H ( over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_K , italic_S ) < over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , then the cost values at the consecutive iterates are strictly decreasing in the sense of expectation. In addition, it is also observed that when  K = 1 K 1 K=1 italic_K = 1 , meaning all agents perform only one-step local update, the second term on the right-hand side of ( 4.9 ) equals to  L     t 2   2 2  S  B   t L superscript subscript    t 2 superscript  2 2 S subscript   B t \\frac{L\\bar{\\alpha}_{t}^{2}\\sigma^{2}}{2S\\bar{B}_{t}} divide start_ARG italic_L over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_S over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG , which decreases as the batch size  B   t subscript   B t \\bar{B}_{t} over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  increases. In fact, in this case, the proposed Algorithm  1  reduces to standard stochastic gradient method (at this time,  S  B   t S subscript   B t S\\bar{B}_{t} italic_S over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  can be viewed as the new batch size), and the results are consistent with the existing works, e.g., [ Bon13 ,  BCN18 ] .",
            "Theorem  4.1  provides that the cost values at the consecutive iterates generated by the proposed RFedAGS are bounded by the squared norm of gradient plus a term controlled by the step sizes. Subsequently, we further require that the step sizes are fixed under Conditions ( 4.7 ) and ( 4.8b ), which makes us convenient to characterize the stronger convergence properties, see Theorem  4.2 , Corollary  4.1  and Theorem  4.3 .",
            "If we run Algorithm  1  with a fixed step size   t =    subscript  t    \\alpha_{t}=\\bar{\\alpha} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG , a fixed batch size  B t = B   subscript B t   B B_{t}=\\bar{B} italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG  satisfying ( 4.7 ) and ( 4.8b ). Then the resulting sequence of iterates  { x ~ t } t = 1 T superscript subscript subscript ~ x t t 1 T \\{\\tilde{x}_{t}\\}_{t=1}^{T} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  satisfies",
            "with  H  (    , K , S ) H    K S H(\\bar{\\alpha},K,S) italic_H ( over  start_ARG italic_ end_ARG , italic_K , italic_S )  being the same as the one in Theorem  4.1  and  x   arg  min x  M  F  ( x ) superscript x subscript arg min x M F x x^{*}\\in\\operatorname*{arg\\,min}_{x\\in\\mathcal{M}}F(x) italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_x  caligraphic_M end_POSTSUBSCRIPT italic_F ( italic_x ) .",
            "Based on Theorem  4.1 , taking total expectation and summing over  t = 1 , 2 , ... , T t 1 2 ... T t=1,2,\\dots,T italic_t = 1 , 2 , ... , italic_T  for ( 4.9 ) yields",
            "A direct consequence of Theorem  4.2  is that for a fixed  K K K italic_K  and sufficient small   > 0 italic- 0 \\epsilon>0 italic_ > 0 , ensuring  1 T  E  [  t = 1 T  grad  F  ( x ~ t )  2 ]   1 T E delimited-[] superscript subscript t 1 T superscript norm grad F subscript ~ x t 2 italic- \\frac{1}{T}\\mathbb{E}[\\sum_{t=1}^{T}\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]\\leq\\epsilon divide start_ARG 1 end_ARG start_ARG italic_T end_ARG blackboard_E [  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  italic_  requires  T  O  ( 1  2 ) T O 1 superscript italic- 2 T\\geq\\mathcal{O}(\\frac{1}{{\\epsilon}^{2}}) italic_T  caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) , as stated in Corollary  4.1 .",
            "It follows that for ( 4.11 )",
            "On the other hand, setting  T  ( F  ( x ~ 1 )  F  ( x  ) )  B    L  M 2  S 3  ( 2  K  1 ) 2  ( K  1 ) 2 9   2  K 4 T F subscript ~ x 1 F superscript x   B L superscript M 2 superscript S 3 superscript 2 K 1 2 superscript K 1 2 9 superscript  2 superscript K 4 T\\geq\\frac{(F(\\tilde{x}_{1})-F(x^{*}))\\bar{B}LM^{2}S^{3}(2K-1)^{2}(K-1)^{2}}{9% \\sigma^{2}K^{4}} italic_T  divide start_ARG ( italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ) over  start_ARG italic_B end_ARG italic_L italic_M start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( 2 italic_K - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_K - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 9 italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_K start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG  is sufficient to ensure ( 4.16 ) with     t =     subscript    t superscript    \\bar{\\alpha}_{t}=\\bar{\\alpha}^{*} over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . Therefore, setting",
            "with  H  (    , K , S ) H    K S H(\\bar{\\alpha},K,S) italic_H ( over  start_ARG italic_ end_ARG , italic_K , italic_S )  being the same as the one in Theorem  4.1 .",
            "with  Q = K     2   2  L 2  B    H  (    , K , S ) Q K superscript    2 superscript  2 L 2   B H    K S {Q}=\\frac{K\\bar{\\alpha}^{2}\\sigma^{2}L}{2\\bar{B}}H(\\bar{\\alpha},K,S) italic_Q = divide start_ARG italic_K over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_L end_ARG start_ARG 2 over  start_ARG italic_B end_ARG end_ARG italic_H ( over  start_ARG italic_ end_ARG , italic_K , italic_S ) . Combining ( 4.1 ) and the condition for        \\bar{\\alpha} over  start_ARG italic_ end_ARG  yields the desired result.",
            "Theorem  4.2 , Corollary  4.1  and Theorem  4.3  require that the step sizes and batch sizes for all agents in all steps are the same, which results in the bound of the expected average squared gradient norms (Theorem  4.2 ) or the expected optimal gap (Theorem  4.3 ) do not vanish as  T    T T\\rightarrow\\infty italic_T   . To improve the results, we impose the decaying step sizes in each outer iteration while satisfying some standard conditions in stochastic (sub)gradient methods. Moreover, the batch sizes are not required to be fixed but only bounded. The formal statement refers to Theorem  4.4  and  4.5 .",
            "If we run Algorithm  1  with decaying step sizes   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and not fixed but bounded batch sizes  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for outer iterations satisfying ( 4.7 ), ( 4.8b ) and  B low  B   t  B up subscript B low subscript   B t subscript B up B_{\\mathrm{low}}\\leq\\bar{B}_{t}\\leq{B}_{\\mathrm{up}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_B start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT  with  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  and  B up subscript B up B_{\\mathrm{up}} italic_B start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT  being positive integers, then the resulting sequence of iterates  { x ~ t } t = 1 T superscript subscript subscript ~ x t t 1 T \\{\\tilde{x}_{t}\\}_{t=1}^{T} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  satisfies",
            "Dividing both sides of ( 4.12 ) by   t = 1 T  t superscript subscript t 1 T subscript  t \\sum_{t=1}^{T}\\alpha_{t}  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  directly results in ( 4.20 ), i.e.,",
            "Under conditions ( 4.21 ) for step sizes, we have",
            "It is stated by Theorem  4.4  that if one uses decaying step sizes with respect to outer iterations satisfying ( 4.21 ), there exists at least one accumulation of the iterates generated by Algorithm  1  which is a critical point in the sense of expectation. In addition, if one takes     t =  0 / (  + t ) subscript    t subscript  0  t \\bar{\\alpha}_{t}=\\alpha_{0}/(\\beta+t) over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT / ( italic_ + italic_t ) , then Condition ( 4.21 ) is satisfied, where   0 subscript  0 \\alpha_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and    \\beta italic_  are positive constants.",
            "By ( 4.18 ) , at the  t t t italic_t -th iteration, we have",
            "An important question of RFedAGS is whether multiple inner iterations, i.e.,  K > 1 K 1 K>1 italic_K > 1 , bring benefits. In other words, is the optimal choice of  K K K italic_K , denoted by  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , greater than  1 1 1 1 5 5 5 View the bound of ( 4.11 ) (or ( 4.20 )) as a function  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) of  K K K italic_K . The optimal choice of  K K K italic_K  is defined as  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) with an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . ? As shown in Theorems  4.6  and  4.7 , the optimal  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  can be greater than  1 1 1 1  under some reasonable conditions. Such results are generalized from  [ ZC18 ] .",
            "We run Algorithm  1  with a fixed batch size  B t , k = B   subscript B t k   B B_{t,k}=\\bar{B} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG  and a fixed step size   t , k =    subscript  t k    \\alpha_{t,k}=\\bar{\\alpha} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG  satisfying Conditions ( 4.7 ) and ( 4.8b ). Under the same conditions as Theorem  4.2 , if the number of outer iteration  T T T italic_T  satisfies",
            "Since the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is prescribed, there exists an upper bound for  K K K italic_K , denoted by  K ~ ~ K \\tilde{K} over~ start_ARG italic_K end_ARG , such that Condition ( 4.8b ) holds for all  K < K ~ K ~ K K<\\tilde{K} italic_K < over~ start_ARG italic_K end_ARG . It follows from ( 4.11 ) that",
            "We run Algorithm  1  with batch sizes  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and decaying step sizes   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfying Conditions ( 4.7 ) and ( 4.8b ). Under the same conditions as Theorem  4.4 , if the number of outer iterations  T T T italic_T  satisfies",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 ).",
            "The experiments conducted in this paper are focused on the empirical risk minimization ( 1.2b ). Under assumptions of i.i.d. and full agent participation, the empirical risk minimization ( 1.2b ) can be equivalently rewritten to",
            "Table  1  gives the parameters used in the problems and Algorithm  1  and Figures  1(a)  1(f)  illuminate the simulation results of the three problems.",
            "In terms of fixed step size cases, the RFedAGS shows the linear convergence for the three problems, which is consistent with the theoretical result (Theorem  4.3 ) as the three problems locally satisfy RPL condition. However, due to that the right-hand side of Inequality ( 4.17 ) in Theorem  4.3  does not vanish as  T T T italic_T  goes to   \\infty  , the solutions given by RFedAGS may not be of high accuracy. All of these observations are verified in Figures  1(a) - 1(c) . To find a highly accurate solution, applying decaying step sizes is a commonly used therapy in the machine learning community. In theory (refer to Theorem  4.5 ), using the decaying step sizes satisfying condition ( LABEL:Conv:RPL_decay:0 ) makes the expected optimal gaps vanish as  T T T italic_T  goes to   \\infty  . Numerically, using the decaying step sizes ( 5.2 ) for the three problems in RFedAGS, does find a higher accurate solution compared to the fixed step size cases since the excess risk is smaller implying the solutions are close to the minimizers, refer to Figures  1(d) - 1(f) . On the other hand, it should be noticed that as the growth of  K K K italic_K , the number of inner iterations, the convergence speed is significantly improved from the theoretical results. Meanwhile,  K K K italic_K  must be not too large since too large  K K K italic_K  makes upper bounds large for expected optimal gaps, refer to Inequalities ( 4.17 ) and ( LABEL:Conv:RPL_decay:0 ). This analysis is verified by Figure  1 . At each outer iteration, the server needs to communicate with all the agents. Therefore, the communication cost between the server and agents is also reduced as the growth of  K K K italic_K  in a reasonable range.",
            "where the last equality is due to ( A ), ( A.3 ) and ( B.1 ). Subsequently, we have",
            "where the equality is due to   x , y  = 1 2  [  x  2 +  y  2   x  y  2 ] x y 1 2 delimited-[] superscript norm x 2 superscript norm y 2 superscript norm x y 2 \\left<x,y\\right>=\\frac{1}{2}[\\|x\\|^{2}+\\|y\\|^{2}-\\|x-y\\|^{2}]  italic_x , italic_y  = divide start_ARG 1 end_ARG start_ARG 2 end_ARG [  italic_x  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT +  italic_y  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT -  italic_x - italic_y  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] , the inequality due to  x t , 0 j = x ~ t superscript subscript x t 0 j subscript ~ x t x_{t,0}^{j}=\\tilde{x}_{t} italic_x start_POSTSUBSCRIPT italic_t , 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT = over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  with  j = 1 , ... , S j 1 ... S j=1,\\dots,S italic_j = 1 , ... , italic_S , and the  L L L italic_L -Lipschitz continuous differentiability (Definition  2.1 ) of  F F F italic_F .",
            "where the third equality is due to the inverse function Theorem  C.1 . Noting that the map  P  ,   (  ) subscript P    P_{\\cdot,\\cdot}(\\cdot) italic_P start_POSTSUBSCRIPT  ,  end_POSTSUBSCRIPT (  )  is defined in  T W = { ( x , y ,  ) : x , y  W ,   R x  1  ( W ) } subscript T W conditional-set x y  formulae-sequence x y W  superscript subscript R x 1 W \\mathrm{T}_{\\mathcal{W}}=\\{(x,y,\\eta):x,y\\in\\mathcal{W},\\eta\\in\\mathrm{R}_{x}^% {-1}(\\mathcal{W})\\} roman_T start_POSTSUBSCRIPT caligraphic_W end_POSTSUBSCRIPT = { ( italic_x , italic_y , italic_ ) : italic_x , italic_y  caligraphic_W , italic_  roman_R start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( caligraphic_W ) } , which is a compact set, according to Assumption  4.1 ( 1 ) and  4.1 ( 2 ), thus, smoothness of the retraction implies that the Jacobin and Hessian of  P  ,   (  ) subscript P    P_{\\cdot,\\cdot}(\\cdot) italic_P start_POSTSUBSCRIPT  ,  end_POSTSUBSCRIPT (  )  with respect to the third variable is uniformly bounded in norm on the compact set. We, thus, use  C 2 , C 3 > 0 subscript C 2 subscript C 3 0 C_{2},C_{3}>0 italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT > 0  to denote bounds on the operator norms of the Jacobin and Hessian of  P  ,   (  ) subscript P    P_{\\cdot,\\cdot}(\\cdot) italic_P start_POSTSUBSCRIPT  ,  end_POSTSUBSCRIPT (  )  with respect to the third variable in the compact set. Noting that",
            "where we used  R x ~ t  1  ( x ~ t ) = 0 x ~ t superscript subscript R subscript ~ x t 1 subscript ~ x t subscript 0 subscript ~ x t \\mathrm{R}_{\\tilde{x}_{t}}^{-1}(\\tilde{x}_{t})=0_{\\tilde{x}_{t}} roman_R start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) = 0 start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT . It follows from ( C.1 ) that",
            "where we used Assumptions  4.1 ( 1 ) and  4.1 ( 6 ) implying that for all  x  W x W x\\in\\mathcal{W} italic_x  caligraphic_W  and    \\xi italic_ , there exists  C 1 > 0 subscript C 1 0 C_{1}>0 italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0  such that   grad  f  ( x ,  )   C 1 norm grad f x  subscript C 1 \\|\\mathrm{grad}f(x,\\xi)\\|\\leq C_{1}  roman_grad italic_f ( italic_x , italic_ )   italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , and that   t , k  A subscript  t k A \\alpha_{t,k}\\leq A italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT  italic_A . On the other hand, similar to the analysis of ( A ) and ( A ), it follows that"
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  the best NMSE scores (lower is better) on testing set for different subspace dimension  r r r italic_r . Here a number  a . b k formulae-sequence a subscript b k a.b_{k} italic_a . italic_b start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  means  a . b  10 k formulae-sequence a b superscript 10 k a.b\\times 10^{k} italic_a . italic_b  10 start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT . The numbers in parentheses represent their corresponding  x x x italic_x -axis coordinates in Figure  3 .",
        "table": "A5.EGx2",
        "footnotes": [
            ""
        ],
        "references": [
            "where  M M \\mathcal{M} caligraphic_M  is a  d d d italic_d -dimensional Riemannian manifold,  F : M  R : F  M R F:\\mathcal{M}\\rightarrow\\mathbb{R} italic_F : caligraphic_M  blackboard_R  is continuously differentiable but not necessarily convex, and it covers both important cases of the expected risk ( 1.2a ) or the empirical risk ( 1.2b )",
            "The problem whose objective function has the form of ( 1.2b ) arises from various applications, including but not limited to principal eigenvector computation over sphere manifold  [ GH15 ] , Frechet mean computation of points over symmetric positive definite matrix manifold  [ Bha07 ,  ZJRS16 ,  HMJG21 ]  or over hyperbolic manifold  [ Bon13 ] , low-rank matrix completion problem  [ MS13 ,  MKJS19 ] , low-dimensional multitask feature learning  [ JM18 ] , and hyperbolic structured prediction over hyperbolic manifold  [ MCR20 ] .",
            "In the following, we focus on the expected risk minimization ( 1.2a ) and propose a generic FL algorithm with  S S S italic_S  agents to solve the problem defined on Riemannian manifolds. The resulting conclusions also hold for the empirical risk minimization ( 1.2b ).",
            "The remainder of this paper is organized as follows. Section  2  introduces preliminaries related to Riemannian optimization. Section  3  discusses the update strategy of FedAvg in detail and develops its counterpart update strategy suitable for the Riemannian setting, and the resulting algorithm is called RFedAGS. Subsequently, Section  4  analyzes the convergence properties for general non-convex problems and RPL problems with two step size schemes: fixed step size and decaying step sizes. Section  5  reports extensive numerical experiment results to evaluate the performance of the proposed RFedAGS. Finally, Section  6  gives conclusions of this paper.",
            "The counterparts in the Riemannian setting are made in Definitions  2.1   [ HAG18 ]  and  2.2   [ HW22 ] . The first property is called  L L L italic_L -Lipschitz continuous differentiability (Definition  2.1 ) in the Riemannian setting and the second is used as a generalization of the notion of  L L L italic_L -smoothness (Definition  2.2 ) to the Riemannian setting.",
            "which is called Frechet mean of points  { x t , K j } j = 1 S superscript subscript superscript subscript x t K j j 1 S \\{x_{t,K}^{j}\\}_{j=1}^{S} { italic_x start_POSTSUBSCRIPT italic_t , italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT . However, exactly solving Problem ( 3.2 ) is computationally expensive in most cases, not to mention to apply it to federated learning framework. Thus, approximating the solution of Problem ( 3.2 ) becomes a reasonable choice. Note that the Riemannian gradient of the cost ( 3.2 ) is given by",
            "which is an approximation to the solution of Problem ( 3.2 ). Compared with ( 3.2 ), the approximation significantly reduces the computational complexity. Aggregation ( 3.3 ), called tangent mean, was firstly introduced by  [ LM23 ]  to federated learning on Riemannian manifolds, and subsequently used in  [ HHJM24 ] .",
            "The convergence analysis is established based on Assumptions  4.1 ,  4.2 , and  4.3 , which are standard and have been used in federated learning, stochastic gradient methods, and Riemannian optimization; see e.g.,  [ HAG15 ,  HGA15 ,  TFBJ18 ,  ZC18 ,  WJ21 ,  HKMC19 ,  SKM19 ] .",
            "The existence of a totally retractive neighborhood  W W \\mathcal{W} caligraphic_W  of  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is guaranteed  [ HAG15 ] , and such assumptions as Assumptions  4.1 ( 1 ) and ( 2 ) have been used in, e.g.,  [ HAG15 ,  TFBJ18 ,  SKM19 ] . Assumptions  4.1 ( 3 ) and ( 5 ) are the standard requirements for analyzing convergence in the Euclidean setting, see, e.g.,  [ ZC18 ,  Sti19 ,  WJ21 ] , and thus we make the counterparts in the Riemannian setting. For commonly-encountered manifolds, e.g., Stiefel manifolds, Grassmann manifolds, and fixed rank matrix manifolds, we can construct an isometric vector transport by parallelization  [ HGA15 ,  HAG15 ] . For another manifold whose exponential map is computationally cheap, e.g., unit sphere manifolds, symmetric positive definite matrix manifolds, and Hyperbolic manifolds, the parallel transport is an alternative of the isometric vector transport  [ AMS08 ,  Bou23 ] . In machine learning, the step sizes are usually not large, and thus we assume that they are bounded from above by a constant  A A A italic_A .",
            "Next, we make assumptions about the first and second moments of the stochastic gradients  grad  f  ( x ;  ) grad f x  \\mathrm{grad}f(x;\\xi) roman_grad italic_f ( italic_x ; italic_ ) , as stated in Assumptions  4.2  and  4.3 , which are standard in literature; see e.g.,  [ ZC18 ,  HKMC19 ,  WJ21 ] .",
            "where  E t  [  ] subscript E t delimited-[]  \\mathbb{E}_{t}[\\cdot] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  ]  means the expectation over the randomness of the  t t t italic_t -outer iteration, and satisfies  E  [ F  ( x ~ t + 1 ) ] = E 1  E 2  ...  E t  [ F  ( x ~ t + 1 ) ] E delimited-[] F subscript ~ x t 1 subscript E 1 subscript E 2 ... subscript E t delimited-[] F subscript ~ x t 1 \\mathbb{E}[F(\\tilde{x}_{t+1})]=\\mathbb{E}_{1}\\mathbb{E}_{2}\\dots\\mathbb{E}_{t}% [F(\\tilde{x}_{t+1})] blackboard_E [ italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ] = blackboard_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ... blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [ italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ]  with  E  [  ] E delimited-[]  \\mathbb{E}[\\cdot] blackboard_E [  ]  being the total expectation since  x ~ t + 1 subscript ~ x t 1 \\tilde{x}_{t+1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT  completely determined by the independent random realizations  { { {   , k , s j } s  B  , k j }  = 1 t } k = 0 K  1 } j = 1 S \\{\\{\\{\\xi_{\\tau,k,s}^{j}\\}_{{s\\in\\mathcal{B}_{\\tau,k}^{j}}}\\}_{\\tau=1}^{t}\\}_{% k=0}^{K-1}\\}_{j=1}^{S} { { { italic_ start_POSTSUBSCRIPT italic_ , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_s  caligraphic_B start_POSTSUBSCRIPT italic_ , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K - 1 end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT . The subsequent convergence analyses are based on ( 4.2 ), and thus this paper focuses on bounding the terms on the right-hand side, as stated in Lemma  4.1 ,  4.2 ,  4.3  and  4.4 , whose proofs can be found in Appendices  A ,  B ,  C  and  D . An upper bound of the second term is given in Lemma  4.1 .",
            "For the first term of the right-hand side of ( 4.2 ), Lemma  4.2  gives an upper bound.",
            "In order to further bound  E t  [  grad  F  ( x ~ t ) , R x ~ t  1  ( x ~ t + 1 )  ] subscript E t delimited-[] grad F subscript ~ x t superscript subscript R subscript ~ x t 1 subscript ~ x t 1 \\mathbb{E}_{t}[\\left<\\mathrm{grad}F(\\tilde{x}_{t}),\\mathrm{R}_{\\tilde{x}_{t}}^% {-1}(\\tilde{x}_{t+1})\\right>] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , roman_R start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT )  ]  for  K > 1 K 1 K>1 italic_K > 1 , from Lemma  4.2 , it is necessary to estimate the bounds for  E t  [  R x ~ t  1  ( x t , k j )  2 ] subscript E t delimited-[] superscript norm superscript subscript R subscript ~ x t 1 superscript subscript x t k j 2 \\mathbb{E}_{t}[\\|\\mathrm{R}_{\\tilde{x}_{t}}^{-1}(x_{t,k}^{j})\\|^{2}] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  roman_R start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] , as theoretically discussed in Lemma  4.4  which states that for agent  j j j italic_j , the distance between the  k k k italic_k -th local update  x t , k j superscript subscript x t k j x_{t,k}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  and the the  t t t italic_t -th outer iterate  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  are controlled by the sum of squared step sizes. Intuitively, the distance increases as the number of local iterations grows, which is shown in Lemma  4.4 . Meanwhile, it also reflects the drift between an agents local update parameter  x t , k j superscript subscript x t k j x_{t,k}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  and the global parameter  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . A general result is provided in Lemma  4.3 .",
            "Therefore, based on Lemma  4.4 , the first term of the right-hand side of ( 4.2 ) is bounded by",
            "Next, this paper gives the first convergent result of the proposed RFedAGS, as stated in Theorem  4.1  built on the inequality ( 4.2 ), which claims the fact that the cost values at the consecutive iterates generated by RFedAGS are sufficient descent in some extent.",
            "In particular, for  K = 1 K 1 K=1 italic_K = 1 , from ( 4.5 ), ( 4.2 ) and the inequality above, we have",
            "which implies ( 4.9 ) holds for  K = 1 K 1 K=1 italic_K = 1 . For  K > 1 K 1 K>1 italic_K > 1 , plugging ( 4.6 ) and ( 4.10 ) into ( 4.2 ) yields",
            "Theorem  4.1  provides that the cost values at the consecutive iterates generated by the proposed RFedAGS are bounded by the squared norm of gradient plus a term controlled by the step sizes. Subsequently, we further require that the step sizes are fixed under Conditions ( 4.7 ) and ( 4.8b ), which makes us convenient to characterize the stronger convergence properties, see Theorem  4.2 , Corollary  4.1  and Theorem  4.3 .",
            "A direct consequence of Theorem  4.2  is that for a fixed  K K K italic_K  and sufficient small   > 0 italic- 0 \\epsilon>0 italic_ > 0 , ensuring  1 T  E  [  t = 1 T  grad  F  ( x ~ t )  2 ]   1 T E delimited-[] superscript subscript t 1 T superscript norm grad F subscript ~ x t 2 italic- \\frac{1}{T}\\mathbb{E}[\\sum_{t=1}^{T}\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]\\leq\\epsilon divide start_ARG 1 end_ARG start_ARG italic_T end_ARG blackboard_E [  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  italic_  requires  T  O  ( 1  2 ) T O 1 superscript italic- 2 T\\geq\\mathcal{O}(\\frac{1}{{\\epsilon}^{2}}) italic_T  caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) , as stated in Corollary  4.1 .",
            "Under the condition of Theorem  4.2 , if the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is given as",
            "Under the same conditions as Theorem  4.2  together with assuming that the function  F F F italic_F  satisfies the RPL condition",
            "Theorem  4.2 , Corollary  4.1  and Theorem  4.3  require that the step sizes and batch sizes for all agents in all steps are the same, which results in the bound of the expected average squared gradient norms (Theorem  4.2 ) or the expected optimal gap (Theorem  4.3 ) do not vanish as  T    T T\\rightarrow\\infty italic_T   . To improve the results, we impose the decaying step sizes in each outer iteration while satisfying some standard conditions in stochastic (sub)gradient methods. Moreover, the batch sizes are not required to be fixed but only bounded. The formal statement refers to Theorem  4.4  and  4.5 .",
            "Dividing both sides of ( 4.12 ) by   t = 1 T  t superscript subscript t 1 T subscript  t \\sum_{t=1}^{T}\\alpha_{t}  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  directly results in ( 4.20 ), i.e.,",
            "Under conditions ( 4.21 ) for step sizes, we have",
            "Suppose  lim inf T   E  [  grad  F  ( x ~ t )  2 ] = 0 subscript limit-infimum  T E delimited-[] superscript norm grad F subscript ~ x t 2 0 \\liminf_{T\\rightarrow\\infty}\\mathbb{E}[\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]% \\not=0 lim inf start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT blackboard_E [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = 0 . Then there exist a positive constant   > 0 italic- 0 \\epsilon>0 italic_ > 0  and an integer  t 0 > 0 subscript t 0 0 t_{0}>0 italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT > 0  such that for all  t > t 0 t subscript t 0 t>t_{0} italic_t > italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ,  E  [  grad  F  ( x ~ t )  2 ] >  E delimited-[] superscript norm grad F subscript ~ x t 2 italic- \\mathbb{E}[\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]>\\epsilon blackboard_E [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] > italic_ . Therefore,  lim T   E  [  t = 1 T  t  t = 1 T  t   grad  F  ( x ~ t )  2 ]  lim T    t = 1 T  t    t = 1 T  t =  > 0 subscript  T E delimited-[] superscript subscript t 1 T subscript  t superscript subscript t 1 T subscript  t superscript norm grad F subscript ~ x t 2 subscript  T superscript subscript t 1 T subscript  t italic- superscript subscript t 1 T subscript  t italic- 0 \\lim_{T\\rightarrow\\infty}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\frac{\\alpha_{t}}{\\sum_% {t=1}^{T}\\alpha_{t}}\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}\\right]\\geq\\lim_{T% \\rightarrow\\infty}\\sum_{t=1}^{T}\\frac{\\alpha_{t}\\epsilon}{\\sum_{t=1}^{T}\\alpha% _{t}}=\\epsilon>0 roman_lim start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT blackboard_E [  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  roman_lim start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ end_ARG start_ARG  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG = italic_ > 0 , which contradicts with ( 4.23 ).",
            "It is stated by Theorem  4.4  that if one uses decaying step sizes with respect to outer iterations satisfying ( 4.21 ), there exists at least one accumulation of the iterates generated by Algorithm  1  which is a critical point in the sense of expectation. In addition, if one takes     t =  0 / (  + t ) subscript    t subscript  0  t \\bar{\\alpha}_{t}=\\alpha_{0}/(\\beta+t) over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT / ( italic_ + italic_t ) , then Condition ( 4.21 ) is satisfied, where   0 subscript  0 \\alpha_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and    \\beta italic_  are positive constants.",
            "Now we are ready to prove ( 4.25 ) by induction. To begin with, for  t = 1 t 1 t=1 italic_t = 1 , ( 4.25 ) follows from the definition of    \\nu italic_ . Next, assuming ( 4.25 ) holds for some  t  1 t 1 t\\geq 1 italic_t  1 . From ( LABEL:Conv:RPL_decay:3 ) and denoting  t ^ =  + t ^ t  t \\hat{t}=\\gamma+t over^ start_ARG italic_t end_ARG = italic_ + italic_t , it follows that",
            "If    \\kappa italic_  is chosen to be  1 +  ~   ( K  1 +  ) 1 ~   K 1  \\frac{1+\\tilde{\\delta}}{\\mu(K-1+\\delta)} divide start_ARG 1 + over~ start_ARG italic_ end_ARG end_ARG start_ARG italic_ ( italic_K - 1 + italic_ ) end_ARG  for a constant   ~ > 0 ~  0 \\tilde{\\delta}>0 over~ start_ARG italic_ end_ARG > 0  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfies ( 4.7 ) and ( 4.8b ), then    \\nu italic_  in ( 4.26 ) becomes",
            "Therefore, we conclude that if  0 <  < 1 / 3 0  1 3 0<\\delta<1/3 0 < italic_ < 1 / 3 , the first two terms on the right-hand side of ( 4.28 ) decrease as  K K K italic_K  grows while the choice of large  K K K italic_K  would not influence    \\nu italic_  much, which implies there exists a  K > 1 K 1 K>1 italic_K > 1  such that    \\nu italic_  is minimum. However, choosing a large batch size  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  reduces    \\nu italic_  in general and thus accelerates the convergence speed by ( 4.25 ).",
            "An important question of RFedAGS is whether multiple inner iterations, i.e.,  K > 1 K 1 K>1 italic_K > 1 , bring benefits. In other words, is the optimal choice of  K K K italic_K , denoted by  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , greater than  1 1 1 1 5 5 5 View the bound of ( 4.11 ) (or ( 4.20 )) as a function  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) of  K K K italic_K . The optimal choice of  K K K italic_K  is defined as  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) with an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . ? As shown in Theorems  4.6  and  4.7 , the optimal  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  can be greater than  1 1 1 1  under some reasonable conditions. Such results are generalized from  [ ZC18 ] .",
            "We run Algorithm  1  with a fixed batch size  B t , k = B   subscript B t k   B B_{t,k}=\\bar{B} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG  and a fixed step size   t , k =    subscript  t k    \\alpha_{t,k}=\\bar{\\alpha} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG  satisfying Conditions ( 4.7 ) and ( 4.8b ). Under the same conditions as Theorem  4.2 , if the number of outer iteration  T T T italic_T  satisfies",
            "holds for  K  K ~ K ~ K K\\leq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG . Define the right hand side of the above equation as  Q 1  ( K ) := ( a 1 / K + a 2  K + a 3  ( 2  K  1 )  ( K  1 ) )  K / ( K  1 +  ) assign subscript Q 1 K subscript a 1 K subscript a 2 K subscript a 3 2 K 1 K 1 K K 1  Q_{1}(K):=(a_{1}/K+a_{2}K+a_{3}(2K-1)(K-1))K/(K-1+\\delta) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K ) := ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT / italic_K + italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_K + italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( 2 italic_K - 1 ) ( italic_K - 1 ) ) italic_K / ( italic_K - 1 + italic_ )  with  a 1 = 2  ( F  ( x ~ 1 )  F  ( x  ) ) / ( T     ) subscript a 1 2 F subscript ~ x 1 F superscript x T    a_{1}=2(F(\\tilde{x}_{1})-F(x^{*}))/(T\\bar{\\alpha}) italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2 ( italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ) / ( italic_T over  start_ARG italic_ end_ARG ) ,  a 2 =     L   2 / ( S  B   ) subscript a 2    L superscript  2 S   B a_{2}=\\bar{\\alpha}L\\sigma^{2}/(S\\bar{B}) italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG italic_L italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( italic_S over  start_ARG italic_B end_ARG )  and  a 3 =    2   2  L 2  M / ( 3  B   ) subscript a 3 superscript    2 superscript  2 superscript L 2 M 3   B a_{3}=\\bar{\\alpha}^{2}\\sigma^{2}L^{2}M/(3\\bar{B}) italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_M / ( 3 over  start_ARG italic_B end_ARG ) . It follows that  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K ) . Inequality ( 4.29 ) implies  ( 1 /  )  a 1 > ( 3  1 /  )  a 2 + 6  a 3 1  subscript a 1 3 1  subscript a 2 6 subscript a 3 \\left(1/\\delta\\right)a_{1}>(3-1/\\delta)a_{2}+6a_{3} ( 1 / italic_ ) italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > ( 3 - 1 / italic_ ) italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + 6 italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , which yields  Q 1  ( 2 ) < Q 1  ( 1 ) subscript Q 1 2 subscript Q 1 1 Q_{1}(2)<Q_{1}(1) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 2 ) < italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 1 ) . Therefore, we have inequality  K  > 1 superscript K 1 K^{*}>1 italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT > 1 , which completes the proof.",
            "Since the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is prescribed, there exists an upper bound for  K K K italic_K , denoted by  K ~ ~ K \\tilde{K} over~ start_ARG italic_K end_ARG , such that Condition ( 4.8b ) holds for all  K < K ~ K ~ K K<\\tilde{K} italic_K < over~ start_ARG italic_K end_ARG . It follows from ( 4.20 ) that",
            "It is noted from Theorems  4.6  and  4.7  that the larger  F  ( x ~ 1 )  F  ( x  ) F subscript ~ x 1 F superscript x F(\\tilde{x}_{1})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , larger batch sizes, and smaller step sizes, make the conditions ( 4.8b ), ( 4.29 ) and ( 4.30 ) easier to be satisfied. Therefore, when the initial guess  x ~ 1 subscript ~ x 1 \\tilde{x}_{1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is far away from the minimizer  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , using a large  K K K italic_K  is reasonable.",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 ).",
            "The experiments conducted in this paper are focused on the empirical risk minimization ( 1.2b ). Under assumptions of i.i.d. and full agent participation, the empirical risk minimization ( 1.2b ) can be equivalently rewritten to",
            "In terms of fixed step size cases, the RFedAGS shows the linear convergence for the three problems, which is consistent with the theoretical result (Theorem  4.3 ) as the three problems locally satisfy RPL condition. However, due to that the right-hand side of Inequality ( 4.17 ) in Theorem  4.3  does not vanish as  T T T italic_T  goes to   \\infty  , the solutions given by RFedAGS may not be of high accuracy. All of these observations are verified in Figures  1(a) - 1(c) . To find a highly accurate solution, applying decaying step sizes is a commonly used therapy in the machine learning community. In theory (refer to Theorem  4.5 ), using the decaying step sizes satisfying condition ( LABEL:Conv:RPL_decay:0 ) makes the expected optimal gaps vanish as  T T T italic_T  goes to   \\infty  . Numerically, using the decaying step sizes ( 5.2 ) for the three problems in RFedAGS, does find a higher accurate solution compared to the fixed step size cases since the excess risk is smaller implying the solutions are close to the minimizers, refer to Figures  1(d) - 1(f) . On the other hand, it should be noticed that as the growth of  K K K italic_K , the number of inner iterations, the convergence speed is significantly improved from the theoretical results. Meanwhile,  K K K italic_K  must be not too large since too large  K K K italic_K  makes upper bounds large for expected optimal gaps, refer to Inequalities ( 4.17 ) and ( LABEL:Conv:RPL_decay:0 ). This analysis is verified by Figure  1 . At each outer iteration, the server needs to communicate with all the agents. Therefore, the communication cost between the server and agents is also reduced as the growth of  K K K italic_K  in a reasonable range.",
            "We investigate the efficacy of RFedAGS for synthetic datasets generated by the approach in  Case 6  of  [ MKJS19 ] . Specifically, for each task  T i  j subscript T i j \\mathcal{T}_{ij} caligraphic_T start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT , (i) the number of instances  d i  j subscript d i j d_{ij} italic_d start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  is randomly chosen between  10 10 10 10  and  50 50 50 50 ; (ii) the training instances  X i  j  R d i  j  m subscript X i j superscript R subscript d i j m \\mathbf{X}_{ij}\\in\\mathbb{R}^{d_{ij}\\times m} bold_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  italic_m end_POSTSUPERSCRIPT  with  m = 100 m 100 m=100 italic_m = 100  are given from the standard Gaussian distribution; (iii) the subspace  U  superscript U \\mathbf{U}^{*} bold_U start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  for the problem is a generated point in  St  ( 5 , 100 ) St 5 100 \\mathrm{St}(5,100) roman_St ( 5 , 100 )  with the dimension  r = 5 r 5 r=5 italic_r = 5 ; (iv) the labels  y i  j subscript y i j y_{ij} italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  for training instances for  T i  j subscript T i j \\mathcal{T}_{ij} caligraphic_T start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  are generated by  y i  j = X i  j  U   ( U  ) T  w i  j subscript y i j subscript X i j superscript U superscript superscript U T subscript w i j y_{ij}=\\mathbf{X}_{ij}\\mathbf{U}^{*}(\\mathbf{U}^{*})^{T}w_{ij} italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = bold_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT bold_U start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( bold_U start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  with  w i  j subscript w i j w_{ij} italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  being generated by the standard Gaussian distribution, and subsequently the labels are perturbed by a random Gaussian noise with zero mean and  10  6 superscript 10 6 10^{-6} 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT  standard deviation. Figures  2(a)  and  2(b)  show the results with  S = 20 S 20 S=20 italic_S = 20 ,  N = 50 N 50 N=50 italic_N = 50 ,   = 0  0 \\lambda=0 italic_ = 0 , the fixed step size     = 0.003    0.003 \\bar{\\alpha}=0.003 over  start_ARG italic_ end_ARG = 0.003  and the fixed batch size  B   = 25   B 25 \\bar{B}=25 over  start_ARG italic_B end_ARG = 25 . We also observed a similar result: the number of inner iterations significantly influences the convergence. It should be worthily mentioned that the results demonstrate RFedAGS has a linear convergence rate.",
            "where for each task  T i  j subscript T i j \\mathcal{T}_{ij} caligraphic_T start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ,  y ^ i  j subscript ^ y i j \\hat{y}_{ij} over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  and  y i  j subscript y i j y_{ij} italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  are respectively the predicted labels and the true labels,  MSE  ( y ^ , y ) MSE ^ y y \\mathrm{MSE}(\\hat{y},y) roman_MSE ( over^ start_ARG italic_y end_ARG , italic_y )  is the mean square error, and  var  ( y ) var y \\mathrm{var}(y) roman_var ( italic_y )  is the variance of the total true labels. Fixed step size     = 1.0  10  6    1.0 superscript 10 6 \\bar{\\alpha}=1.0\\times 10^{-6} over  start_ARG italic_ end_ARG = 1.0  10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT  is used here and the remaining parameters are set as   = 1.0  10  3  1.0 superscript 10 3 \\lambda=1.0\\times 10^{-3} italic_ = 1.0  10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT , and  B   = 18   B 18 \\bar{B}=18 over  start_ARG italic_B end_ARG = 18 . The results with multiple values of  K K K italic_K , the number of inner iterations, and the subspace dimension  r r r italic_r  are reported in Figure  3  and Table  2 .",
            "where the first equality follows from ( 3.7 ) and the inequality follows from the fact    i = 1 n x i  2  n   i = 1 n  x i  2 superscript norm superscript subscript i 1 n subscript x i 2 n superscript subscript i 1 n superscript norm subscript x i 2 \\|\\sum_{i=1}^{n}x_{i}\\|^{2}\\leq n\\sum_{i=1}^{n}\\|x_{i}\\|^{2}   start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  italic_n  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Under Assumption  4.2 , taking the expectation for fixed  t t t italic_t ,  k k k italic_k  and  j j j italic_j  yields",
            "where the third equality is due to that  {  t , k , s j } superscript subscript  t k s j \\{\\xi_{t,k,s}^{j}\\} { italic_ start_POSTSUBSCRIPT italic_t , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT }  with  j = 1 , ... , S j 1 ... S j=1,\\dots,S italic_j = 1 , ... , italic_S  and  s  B t , k j s superscript subscript B t k j s\\in\\mathcal{B}_{t,k}^{j} italic_s  caligraphic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  are independent random variables for fixed  t t t italic_t  and  k k k italic_k , the fourth equality is due to Assumption  4.2 , and the inequality is due to Assumption  4.3 . On the other hand, for fixed  k k k italic_k , we have",
            "where the equality is due to   x , y  = 1 2  [  x  2 +  y  2   x  y  2 ] x y 1 2 delimited-[] superscript norm x 2 superscript norm y 2 superscript norm x y 2 \\left<x,y\\right>=\\frac{1}{2}[\\|x\\|^{2}+\\|y\\|^{2}-\\|x-y\\|^{2}]  italic_x , italic_y  = divide start_ARG 1 end_ARG start_ARG 2 end_ARG [  italic_x  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT +  italic_y  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT -  italic_x - italic_y  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] , the inequality due to  x t , 0 j = x ~ t superscript subscript x t 0 j subscript ~ x t x_{t,0}^{j}=\\tilde{x}_{t} italic_x start_POSTSUBSCRIPT italic_t , 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT = over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  with  j = 1 , ... , S j 1 ... S j=1,\\dots,S italic_j = 1 , ... , italic_S , and the  L L L italic_L -Lipschitz continuous differentiability (Definition  2.1 ) of  F F F italic_F .",
            "where the third equality is due to the inverse function Theorem  C.1 . Noting that the map  P  ,   (  ) subscript P    P_{\\cdot,\\cdot}(\\cdot) italic_P start_POSTSUBSCRIPT  ,  end_POSTSUBSCRIPT (  )  is defined in  T W = { ( x , y ,  ) : x , y  W ,   R x  1  ( W ) } subscript T W conditional-set x y  formulae-sequence x y W  superscript subscript R x 1 W \\mathrm{T}_{\\mathcal{W}}=\\{(x,y,\\eta):x,y\\in\\mathcal{W},\\eta\\in\\mathrm{R}_{x}^% {-1}(\\mathcal{W})\\} roman_T start_POSTSUBSCRIPT caligraphic_W end_POSTSUBSCRIPT = { ( italic_x , italic_y , italic_ ) : italic_x , italic_y  caligraphic_W , italic_  roman_R start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( caligraphic_W ) } , which is a compact set, according to Assumption  4.1 ( 1 ) and  4.1 ( 2 ), thus, smoothness of the retraction implies that the Jacobin and Hessian of  P  ,   (  ) subscript P    P_{\\cdot,\\cdot}(\\cdot) italic_P start_POSTSUBSCRIPT  ,  end_POSTSUBSCRIPT (  )  with respect to the third variable is uniformly bounded in norm on the compact set. We, thus, use  C 2 , C 3 > 0 subscript C 2 subscript C 3 0 C_{2},C_{3}>0 italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT > 0  to denote bounds on the operator norms of the Jacobin and Hessian of  P  ,   (  ) subscript P    P_{\\cdot,\\cdot}(\\cdot) italic_P start_POSTSUBSCRIPT  ,  end_POSTSUBSCRIPT (  )  with respect to the third variable in the compact set. Noting that"
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A5.EGx3",
        "footnotes": [],
        "references": [
            "In light of these limitations, this paper proposes a new server aggregation, called the average of gradient stream, and a corresponding Riemannian FL framework, namely RFedAGS, on generic Riemannian manifolds. The gradient stream generated by agent  i i i italic_i  is all mini-batch gradients generated in the local training process by agent  i i i italic_i . The proposed server aggregation aims to average these gradient streams uploaded by all agents in a legal way in Riemannian manifolds. It will be seen in Section  3  that the proposed server aggregation is another generalization of FedAvg to the Riemannian setting. Meanwhile, based on this aggregation, extensive convergence results are given in this paper for proposed RFedAGS. Moreover, it is pointed out that in the Euclidean setting, there exist some works, e.g.,  [ KKM + 20 ,  RCZ + 21 ,  YFL21 ] , in which agents upload the mini-batch gradient stream to the server at each outer iteration and the proposed aggregation can be viewed as a generalization of these to the Riemannian setting.",
            "The remainder of this paper is organized as follows. Section  2  introduces preliminaries related to Riemannian optimization. Section  3  discusses the update strategy of FedAvg in detail and develops its counterpart update strategy suitable for the Riemannian setting, and the resulting algorithm is called RFedAGS. Subsequently, Section  4  analyzes the convergence properties for general non-convex problems and RPL problems with two step size schemes: fixed step size and decaying step sizes. Section  5  reports extensive numerical experiment results to evaluate the performance of the proposed RFedAGS. Finally, Section  6  gives conclusions of this paper.",
            "which is called Frechet mean of points  { x t , K j } j = 1 S superscript subscript superscript subscript x t K j j 1 S \\{x_{t,K}^{j}\\}_{j=1}^{S} { italic_x start_POSTSUBSCRIPT italic_t , italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT . However, exactly solving Problem ( 3.2 ) is computationally expensive in most cases, not to mention to apply it to federated learning framework. Thus, approximating the solution of Problem ( 3.2 ) becomes a reasonable choice. Note that the Riemannian gradient of the cost ( 3.2 ) is given by",
            "which is an approximation to the solution of Problem ( 3.2 ). Compared with ( 3.2 ), the approximation significantly reduces the computational complexity. Aggregation ( 3.3 ), called tangent mean, was firstly introduced by  [ LM23 ]  to federated learning on Riemannian manifolds, and subsequently used in  [ HHJM24 ] .",
            "Tangent mean ( 3.3 ) is an approximation of the average of all received local parameters to Riemannian manifolds. Nevertheless, this paper gives an another server aggregation which is from the perspective of the outer loop. This make us think of the outer loop as a pseudo-gradient descent update.",
            "For Riemannian manifolds, from ( 3.3 ), the search direction is given by",
            "Combining ( 3.5 ) with ( 3.6 ) shows that exactly expanding the expression of the search direction   t subscript  t \\eta_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , which involves multiple consecutive exponential mappings, is difficult in general since the exponential mapping is short of linearity. Consequently, this makes the convergence analysis more challenging, when multiple steps SGD are involved in local updates. In view of the discussion above, this paper resorts to another aggregation which can not only realize server aggregation efficiently but also analyze algorithm convergence conveniently. In our opinion, this aggregation is a more essential generalization from the Euclidean setting to the Riemannian setting.",
            "Noting that from ( 3.4 ), the search direction at the  t t t italic_t -th outer iteration is given by the average of mini-batch gradients of all agents, which is here called gradient stream. Adopting the idea in the Riemannian setting, directly combining the mini-batch gradients located in different tangent spaces is not well defined. Fortunately, with the aid of vector transport, the combination can be defined. Specifically, the search direction is given by",
            "Using ( 3.7 ), the server aggregation is given by",
            "Aggregation ( 3.8 ) combined with ( 3.7 ) can be viewed as a generalization of ( 3.1 ) combined with ( 3.4 ). Specific to each agent  j j j italic_j , it only needs to upload",
            "From the perspective of geometry, tangent mean ( 3.3 ) projects the final inner iterates  x t + K j superscript subscript x t K j x_{t+K}^{j} italic_x start_POSTSUBSCRIPT italic_t + italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  back to the tangent space at  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , then averages them and finally retracts the average into the manifold. While in aggregation ( 3.8 ), the intermediary negative mini-batch-gradients   1 B t , k   s  B t , k j grad  f  ( x t , k j ,  t , k , s j ) 1 subscript B t k subscript s subscript superscript B j t k grad f superscript subscript x t k j superscript subscript  t k s j -\\frac{1}{B_{t,k}}\\sum_{s\\in\\mathcal{B}^{j}_{t,k}}\\mathrm{grad}f(x_{t,k}^{j},% \\xi_{t,k,s}^{j}) - divide start_ARG 1 end_ARG start_ARG italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT end_ARG  start_POSTSUBSCRIPT italic_s  caligraphic_B start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_grad italic_f ( italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_t , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )  are transported to the tangent space at  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  in some way, then averages them and finally retracts the average into the manifold. In particular, letting the proposed aggregation ( 3.8 ) use the exponential map and parallel transport, the two aggregations coincide when (i)  M = R d M superscript R d \\mathcal{M}=\\mathbb{R}^{d} caligraphic_M = blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ; or (ii)  K = 1 K 1 K=1 italic_K = 1 .",
            "The convergence analysis is established based on Assumptions  4.1 ,  4.2 , and  4.3 , which are standard and have been used in federated learning, stochastic gradient methods, and Riemannian optimization; see e.g.,  [ HAG15 ,  HGA15 ,  TFBJ18 ,  ZC18 ,  WJ21 ,  HKMC19 ,  SKM19 ] .",
            "The existence of a totally retractive neighborhood  W W \\mathcal{W} caligraphic_W  of  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is guaranteed  [ HAG15 ] , and such assumptions as Assumptions  4.1 ( 1 ) and ( 2 ) have been used in, e.g.,  [ HAG15 ,  TFBJ18 ,  SKM19 ] . Assumptions  4.1 ( 3 ) and ( 5 ) are the standard requirements for analyzing convergence in the Euclidean setting, see, e.g.,  [ ZC18 ,  Sti19 ,  WJ21 ] , and thus we make the counterparts in the Riemannian setting. For commonly-encountered manifolds, e.g., Stiefel manifolds, Grassmann manifolds, and fixed rank matrix manifolds, we can construct an isometric vector transport by parallelization  [ HGA15 ,  HAG15 ] . For another manifold whose exponential map is computationally cheap, e.g., unit sphere manifolds, symmetric positive definite matrix manifolds, and Hyperbolic manifolds, the parallel transport is an alternative of the isometric vector transport  [ AMS08 ,  Bou23 ] . In machine learning, the step sizes are usually not large, and thus we assume that they are bounded from above by a constant  A A A italic_A .",
            "Next, we make assumptions about the first and second moments of the stochastic gradients  grad  f  ( x ;  ) grad f x  \\mathrm{grad}f(x;\\xi) roman_grad italic_f ( italic_x ; italic_ ) , as stated in Assumptions  4.2  and  4.3 , which are standard in literature; see e.g.,  [ ZC18 ,  HKMC19 ,  WJ21 ] .",
            "It is observed in Assumption  4.3  that a larger batch size  B B B italic_B  results in smaller variance of the mini-batch gradient, which is in line with observation in practice, and a more general form  ( E [  1 B  s  B grad f ( x ;  s )  grad F ( x )  2 ]    grad F ( x )  2 +  2 B \\big{(}\\mathbb{E}[\\|\\frac{1}{B}\\sum_{s\\in\\mathcal{B}}\\mathrm{grad}f(x;\\xi_{s})% -\\mathrm{grad}F(x)\\|^{2}]\\leq\\beta\\|\\mathrm{grad}F(x)\\|^{2}+\\frac{\\sigma^{2}}{B} ( blackboard_E [  divide start_ARG 1 end_ARG start_ARG italic_B end_ARG  start_POSTSUBSCRIPT italic_s  caligraphic_B end_POSTSUBSCRIPT roman_grad italic_f ( italic_x ; italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) - roman_grad italic_F ( italic_x )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  italic_  roman_grad italic_F ( italic_x )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_B end_ARG  with constant    0 ) \\beta\\geq 0\\big{)} italic_  0 )  is used in  [ HKMC19 ,  WJ21 ] .",
            "where  E t  [  ] subscript E t delimited-[]  \\mathbb{E}_{t}[\\cdot] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  ]  means the expectation over the randomness of the  t t t italic_t -outer iteration, and satisfies  E  [ F  ( x ~ t + 1 ) ] = E 1  E 2  ...  E t  [ F  ( x ~ t + 1 ) ] E delimited-[] F subscript ~ x t 1 subscript E 1 subscript E 2 ... subscript E t delimited-[] F subscript ~ x t 1 \\mathbb{E}[F(\\tilde{x}_{t+1})]=\\mathbb{E}_{1}\\mathbb{E}_{2}\\dots\\mathbb{E}_{t}% [F(\\tilde{x}_{t+1})] blackboard_E [ italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ] = blackboard_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ... blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [ italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ]  with  E  [  ] E delimited-[]  \\mathbb{E}[\\cdot] blackboard_E [  ]  being the total expectation since  x ~ t + 1 subscript ~ x t 1 \\tilde{x}_{t+1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT  completely determined by the independent random realizations  { { {   , k , s j } s  B  , k j }  = 1 t } k = 0 K  1 } j = 1 S \\{\\{\\{\\xi_{\\tau,k,s}^{j}\\}_{{s\\in\\mathcal{B}_{\\tau,k}^{j}}}\\}_{\\tau=1}^{t}\\}_{% k=0}^{K-1}\\}_{j=1}^{S} { { { italic_ start_POSTSUBSCRIPT italic_ , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_s  caligraphic_B start_POSTSUBSCRIPT italic_ , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K - 1 end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT . The subsequent convergence analyses are based on ( 4.2 ), and thus this paper focuses on bounding the terms on the right-hand side, as stated in Lemma  4.1 ,  4.2 ,  4.3  and  4.4 , whose proofs can be found in Appendices  A ,  B ,  C  and  D . An upper bound of the second term is given in Lemma  4.1 .",
            "In order to further bound  E t  [  grad  F  ( x ~ t ) , R x ~ t  1  ( x ~ t + 1 )  ] subscript E t delimited-[] grad F subscript ~ x t superscript subscript R subscript ~ x t 1 subscript ~ x t 1 \\mathbb{E}_{t}[\\left<\\mathrm{grad}F(\\tilde{x}_{t}),\\mathrm{R}_{\\tilde{x}_{t}}^% {-1}(\\tilde{x}_{t+1})\\right>] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , roman_R start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT )  ]  for  K > 1 K 1 K>1 italic_K > 1 , from Lemma  4.2 , it is necessary to estimate the bounds for  E t  [  R x ~ t  1  ( x t , k j )  2 ] subscript E t delimited-[] superscript norm superscript subscript R subscript ~ x t 1 superscript subscript x t k j 2 \\mathbb{E}_{t}[\\|\\mathrm{R}_{\\tilde{x}_{t}}^{-1}(x_{t,k}^{j})\\|^{2}] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  roman_R start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] , as theoretically discussed in Lemma  4.4  which states that for agent  j j j italic_j , the distance between the  k k k italic_k -th local update  x t , k j superscript subscript x t k j x_{t,k}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  and the the  t t t italic_t -th outer iterate  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  are controlled by the sum of squared step sizes. Intuitively, the distance increases as the number of local iterations grows, which is shown in Lemma  4.4 . Meanwhile, it also reflects the drift between an agents local update parameter  x t , k j superscript subscript x t k j x_{t,k}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  and the global parameter  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . A general result is provided in Lemma  4.3 .",
            "When  M M \\mathcal{M} caligraphic_M  reduces into a Euclidean space, e.g.,  M = R d M superscript R d \\mathcal{M}=\\mathbb{R}^{d} caligraphic_M = blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , the constants in Lemma  4.3  will become  C 2 = 1 subscript C 2 1 C_{2}=1 italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 1  and  C 3 = 0 subscript C 3 0 C_{3}=0 italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = 0 . In this case, the results correspondingly becomes   x ~ t  x t , k j  2  k    = 0 k  1  t ,  2   G F  ( x t ,  j )  2 . superscript norm subscript ~ x t superscript subscript x t k j 2 k superscript subscript  0 k 1 superscript subscript  t  2 superscript norm subscript G F superscript subscript x t  j 2 \\|\\tilde{x}_{t}-x_{t,k}^{j}\\|^{2}\\leq k\\sum_{\\tau=0}^{k-1}\\alpha_{t,\\tau}^{2}% \\|\\mathcal{G}_{F}(x_{t,\\tau}^{j})\\|^{2}.  over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  italic_k  start_POSTSUBSCRIPT italic_ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t , italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  caligraphic_G start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t , italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .  In Lemma  4.3 , if one uses  1 B t , k  1   s  B t , k  1 j grad  f  ( x t , k  1 j ;  t , k  1 , s j ) 1 subscript B t k 1 subscript s superscript subscript B t k 1 j grad f superscript subscript x t k 1 j superscript subscript  t k 1 s j \\frac{1}{B_{t,k-1}}\\sum_{s\\in\\mathcal{B}_{t,k-1}^{j}}\\mathrm{grad}f(x_{t,k-1}^% {j};\\xi_{t,k-1,s}^{j}) divide start_ARG 1 end_ARG start_ARG italic_B start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT end_ARG  start_POSTSUBSCRIPT italic_s  caligraphic_B start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_grad italic_f ( italic_x start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ; italic_ start_POSTSUBSCRIPT italic_t , italic_k - 1 , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )  to replace  G F  ( x t , k  1 j ) subscript G F superscript subscript x t k 1 j \\mathcal{G}_{F}(x_{t,k-1}^{j}) caligraphic_G start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) , then the desired result is obtained in Lemma  4.4 .",
            "where the expectation is taken over the randomness at the  t t t italic_t -th outer iteration conditioned on  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,  M = ( C 2 2 + A 2  C 1 2  C 3 2 ) M superscript subscript C 2 2 superscript A 2 superscript subscript C 1 2 superscript subscript C 3 2 M=(C_{2}^{2}+A^{2}C_{1}^{2}C_{3}^{2}) italic_M = ( italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  is a positive constant,  A A A italic_A  is stated in Assumption  4.1 ( 6 ),  C 1 subscript C 1 C_{1} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is a constant such that   grad  F  ( x )   C 1 norm grad F x subscript C 1 \\|\\mathrm{grad}F(x)\\|\\leq C_{1}  roman_grad italic_F ( italic_x )   italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  for all  x  W x W x\\in\\mathcal{W} italic_x  caligraphic_W  (as Assumption  4.1 ( 6 )), and  C 2 subscript C 2 C_{2} italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  and  C 3 subscript C 3 C_{3} italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT  are the same as that in Lemma  4.3   3 3 3 In particular, when  M M \\mathcal{M} caligraphic_M  reduces to a Euclidean space, e.g.,  M = R d M superscript R d \\mathcal{M}=\\mathbb{R}^{d} caligraphic_M = blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , we have  M = 1 M 1 M=1 italic_M = 1 . .",
            "Theorem  4.1  provides that the cost values at the consecutive iterates generated by the proposed RFedAGS are bounded by the squared norm of gradient plus a term controlled by the step sizes. Subsequently, we further require that the step sizes are fixed under Conditions ( 4.7 ) and ( 4.8b ), which makes us convenient to characterize the stronger convergence properties, see Theorem  4.2 , Corollary  4.1  and Theorem  4.3 .",
            "Theorem  4.3  gives an upper bound of the expected optimal gap if the objective satisfies the Riemannian Polyak-ojasiewicz (RPL) condition.",
            "Theorem  4.2 , Corollary  4.1  and Theorem  4.3  require that the step sizes and batch sizes for all agents in all steps are the same, which results in the bound of the expected average squared gradient norms (Theorem  4.2 ) or the expected optimal gap (Theorem  4.3 ) do not vanish as  T    T T\\rightarrow\\infty italic_T   . To improve the results, we impose the decaying step sizes in each outer iteration while satisfying some standard conditions in stochastic (sub)gradient methods. Moreover, the batch sizes are not required to be fixed but only bounded. The formal statement refers to Theorem  4.4  and  4.5 .",
            "Suppose  lim inf T   E  [  grad  F  ( x ~ t )  2 ] = 0 subscript limit-infimum  T E delimited-[] superscript norm grad F subscript ~ x t 2 0 \\liminf_{T\\rightarrow\\infty}\\mathbb{E}[\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]% \\not=0 lim inf start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT blackboard_E [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = 0 . Then there exist a positive constant   > 0 italic- 0 \\epsilon>0 italic_ > 0  and an integer  t 0 > 0 subscript t 0 0 t_{0}>0 italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT > 0  such that for all  t > t 0 t subscript t 0 t>t_{0} italic_t > italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ,  E  [  grad  F  ( x ~ t )  2 ] >  E delimited-[] superscript norm grad F subscript ~ x t 2 italic- \\mathbb{E}[\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]>\\epsilon blackboard_E [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] > italic_ . Therefore,  lim T   E  [  t = 1 T  t  t = 1 T  t   grad  F  ( x ~ t )  2 ]  lim T    t = 1 T  t    t = 1 T  t =  > 0 subscript  T E delimited-[] superscript subscript t 1 T subscript  t superscript subscript t 1 T subscript  t superscript norm grad F subscript ~ x t 2 subscript  T superscript subscript t 1 T subscript  t italic- superscript subscript t 1 T subscript  t italic- 0 \\lim_{T\\rightarrow\\infty}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\frac{\\alpha_{t}}{\\sum_% {t=1}^{T}\\alpha_{t}}\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}\\right]\\geq\\lim_{T% \\rightarrow\\infty}\\sum_{t=1}^{T}\\frac{\\alpha_{t}\\epsilon}{\\sum_{t=1}^{T}\\alpha% _{t}}=\\epsilon>0 roman_lim start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT blackboard_E [  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  roman_lim start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ end_ARG start_ARG  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG = italic_ > 0 , which contradicts with ( 4.23 ).",
            "Under the same conditions as Theorem  4.3  except for that the step size sequence and the batch size sequence satisfy",
            "Denote the right-hand side of the inequality above by  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) . Therefore,  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) . A sufficient condition for  K  > 1 superscript K 1 K^{*}>1 italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT > 1  is  Q 2  ( 2 ) < Q 2  ( 1 ) subscript Q 2 2 subscript Q 2 1 Q_{2}(2)<Q_{2}(1) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 2 ) < italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 ) , which is guaranteed by ( 4.30 ).",
            "It is noted from Theorems  4.6  and  4.7  that the larger  F  ( x ~ 1 )  F  ( x  ) F subscript ~ x 1 F superscript x F(\\tilde{x}_{1})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , larger batch sizes, and smaller step sizes, make the conditions ( 4.8b ), ( 4.29 ) and ( 4.30 ) easier to be satisfied. Therefore, when the initial guess  x ~ 1 subscript ~ x 1 \\tilde{x}_{1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is far away from the minimizer  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , using a large  K K K italic_K  is reasonable.",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 ).",
            "In this section, RFedAvg means that each agent performs SGD to update the local parameter and the server uses the tangent mean ( 3.3 ) to aggregate the next global parameter, refer to  [ LM23 , Algorithm 2] . The related Riemannian operations for considered manifold are discussed in Appendix  E  and the implementation is from the Manopt package  [ BMAS14 ]  except the isometric vector transports of Stiefel manifold and Grassmann manifold 6 6 6 The isometric vector transports on Stiefel manifold and Grassmann manifold here are provided in  [ HAG18 ] , which are also called transporter by parallelization  [ HGA15 ] . Excess risk is defined by  F  ( x ~ T )  F  ( x  ) F superscript ~ x T F superscript x F(\\tilde{x}^{T})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , where  F  ( x  ) F superscript x F(x^{*}) italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  is obtained by a centralized Riemannian optimization method with high accuracy, i.e., the norm of the final gradient is smaller than  10  6 superscript 10 6 10^{-6} 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT .",
            "In terms of fixed step size cases, the RFedAGS shows the linear convergence for the three problems, which is consistent with the theoretical result (Theorem  4.3 ) as the three problems locally satisfy RPL condition. However, due to that the right-hand side of Inequality ( 4.17 ) in Theorem  4.3  does not vanish as  T T T italic_T  goes to   \\infty  , the solutions given by RFedAGS may not be of high accuracy. All of these observations are verified in Figures  1(a) - 1(c) . To find a highly accurate solution, applying decaying step sizes is a commonly used therapy in the machine learning community. In theory (refer to Theorem  4.5 ), using the decaying step sizes satisfying condition ( LABEL:Conv:RPL_decay:0 ) makes the expected optimal gaps vanish as  T T T italic_T  goes to   \\infty  . Numerically, using the decaying step sizes ( 5.2 ) for the three problems in RFedAGS, does find a higher accurate solution compared to the fixed step size cases since the excess risk is smaller implying the solutions are close to the minimizers, refer to Figures  1(d) - 1(f) . On the other hand, it should be noticed that as the growth of  K K K italic_K , the number of inner iterations, the convergence speed is significantly improved from the theoretical results. Meanwhile,  K K K italic_K  must be not too large since too large  K K K italic_K  makes upper bounds large for expected optimal gaps, refer to Inequalities ( 4.17 ) and ( LABEL:Conv:RPL_decay:0 ). This analysis is verified by Figure  1 . At each outer iteration, the server needs to communicate with all the agents. Therefore, the communication cost between the server and agents is also reduced as the growth of  K K K italic_K  in a reasonable range.",
            "where the Grassmann manifold  Gr  ( r , m ) Gr r m \\mathrm{Gr}(r,m) roman_Gr ( italic_r , italic_m )  is equipped with the quotient manifold structure  Gr  ( r , m ) = St  ( r , m ) / O  ( r ) Gr r m St r m O r \\mathrm{Gr}(r,m)=\\mathrm{St}(r,m)/\\mathbb{O}(r) roman_Gr ( italic_r , italic_m ) = roman_St ( italic_r , italic_m ) / blackboard_O ( italic_r )  with  O  ( r ) O r \\mathbb{O}(r) blackboard_O ( italic_r )  being the orthogonal group,  U  St  ( r , m ) U St r m \\mathbf{U}\\in\\mathrm{St}(r,m) bold_U  roman_St ( italic_r , italic_m )  is a representative of  U  Gr  ( r , m ) U Gr r m \\mathcal{U}\\in\\mathrm{Gr}(r,m) caligraphic_U  roman_Gr ( italic_r , italic_m ) , and for a given  U U \\mathbf{U} bold_U ,  w i  j  U subscript w i j U w_{ij\\mathbf{U}} italic_w start_POSTSUBSCRIPT italic_i italic_j bold_U end_POSTSUBSCRIPT  is the least-squares solution to  min w i  j  R r  0.5   X i  j  U  w i  j  y i  j  F 2 +    w i  j  F 2 subscript subscript w i j superscript R r 0.5 superscript subscript norm subscript X i j U subscript w i j subscript y i j F 2  superscript subscript norm subscript w i j F 2 \\min_{w_{ij}\\in\\mathbb{R}^{r}}0.5\\|\\mathbf{X}_{ij}\\mathbf{U}w_{ij}-y_{ij}\\|_{F% }^{2}+\\lambda\\|w_{ij}\\|_{F}^{2} roman_min start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT end_POSTSUBSCRIPT 0.5  bold_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT bold_U italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT - italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_  italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , which has a closed form for    0  0 \\lambda\\geq 0 italic_  0 . Note that Problem ( 5.3 ) is defined on Grassmann manifold  Gr  ( r , m ) Gr r m \\mathrm{Gr}(r,m) roman_Gr ( italic_r , italic_m ) , but numerically implemented with matrix  U U \\mathbf{U} bold_U  in Stiefel manifold  St  ( r , m ) St r m \\mathrm{St}(r,m) roman_St ( italic_r , italic_m ) .",
            "where for each task  T i  j subscript T i j \\mathcal{T}_{ij} caligraphic_T start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT ,  y ^ i  j subscript ^ y i j \\hat{y}_{ij} over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  and  y i  j subscript y i j y_{ij} italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  are respectively the predicted labels and the true labels,  MSE  ( y ^ , y ) MSE ^ y y \\mathrm{MSE}(\\hat{y},y) roman_MSE ( over^ start_ARG italic_y end_ARG , italic_y )  is the mean square error, and  var  ( y ) var y \\mathrm{var}(y) roman_var ( italic_y )  is the variance of the total true labels. Fixed step size     = 1.0  10  6    1.0 superscript 10 6 \\bar{\\alpha}=1.0\\times 10^{-6} over  start_ARG italic_ end_ARG = 1.0  10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT  is used here and the remaining parameters are set as   = 1.0  10  3  1.0 superscript 10 3 \\lambda=1.0\\times 10^{-3} italic_ = 1.0  10 start_POSTSUPERSCRIPT - 3 end_POSTSUPERSCRIPT , and  B   = 18   B 18 \\bar{B}=18 over  start_ARG italic_B end_ARG = 18 . The results with multiple values of  K K K italic_K , the number of inner iterations, and the subspace dimension  r r r italic_r  are reported in Figure  3  and Table  2 .",
            "A direct observation is that RFedAGS is comparable to some centralized methods (RSD, RCG, and RLBFGS) in terms of function value. In terms of NMSE, the smaller whose value is the more accurate the model is, the performance of RFedAGS is comparable to these centralized methods. Meanwhile, larger  K K K italic_K  significantly improves the convergence speed. Specifically, cases  K = 4 K 4 K=4 italic_K = 4 ,  K = 8 K 8 K=8 italic_K = 8 , and  K = 10 K 10 K=10 italic_K = 10  reduced at least one half the number of iterations compared to the case  K = 1 K 1 K=1 italic_K = 1  when  r = 4 , 5 r 4 5 r=4,5 italic_r = 4 , 5 . We also noted that the performance of RFedAvg is very close to the performance of RFedAGS with  K = 1 K 1 K=1 italic_K = 1 , which is not surprising since the two methods are direct generalizations of the classical FedAvg from two aspects as discussed in Section  3 . To the best of our knowledge, under the same setting: full participation and  K > 1 K 1 K>1 italic_K > 1 , RFedAvg is short of theoretical utility guarantees. On the other hand, the implementation of RFedAvg depends on the inverse of the exponential mapping  Exp  1 superscript Exp 1 \\mathrm{Exp}^{-1} roman_Exp start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT , which is expensive to compute in some manifolds. For example, a closed form of  Exp  1 superscript Exp 1 \\mathrm{Exp}^{-1} roman_Exp start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT  is unknown on the Stiefel manifold, and only iterative methods  [ Bry17 ,  ZH22 ]  are developed to compute it, which makes the computational cost highly expensive. Nevertheless, RFedAGS does not encounter these issues.",
            "where the first equality follows from ( 3.7 ) and the inequality follows from the fact    i = 1 n x i  2  n   i = 1 n  x i  2 superscript norm superscript subscript i 1 n subscript x i 2 n superscript subscript i 1 n superscript norm subscript x i 2 \\|\\sum_{i=1}^{n}x_{i}\\|^{2}\\leq n\\sum_{i=1}^{n}\\|x_{i}\\|^{2}   start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  italic_n  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Under Assumption  4.2 , taking the expectation for fixed  t t t italic_t ,  k k k italic_k  and  j j j italic_j  yields",
            "where ( A.3 ) is due to that  x t , k j superscript subscript x t k j x_{t,k}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT but not  x t , k  1 j superscript subscript x t k 1 j x_{t,k-1}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT depends on the randomness of the  ( k  1 ) k 1 (k-1) ( italic_k - 1 ) -th inner iteration and ( A.4 ) is due to that   t , k , s j superscript subscript  t k s j \\xi_{t,k,s}^{j} italic_ start_POSTSUBSCRIPT italic_t , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  depends on the randomness of the  k k k italic_k -th inner iteration. Hence, combining Assumption  4.3  and ( A ), we have",
            "where the third equality is due to that  {  t , k , s j } superscript subscript  t k s j \\{\\xi_{t,k,s}^{j}\\} { italic_ start_POSTSUBSCRIPT italic_t , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT }  with  j = 1 , ... , S j 1 ... S j=1,\\dots,S italic_j = 1 , ... , italic_S  and  s  B t , k j s superscript subscript B t k j s\\in\\mathcal{B}_{t,k}^{j} italic_s  caligraphic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  are independent random variables for fixed  t t t italic_t  and  k k k italic_k , the fourth equality is due to Assumption  4.2 , and the inequality is due to Assumption  4.3 . On the other hand, for fixed  k k k italic_k , we have",
            "where the first equality follows ( A.4 ), the inequality is due to ( A ), and the last equality is due to ( A.3 ). Combining ( A ) together with ( A ) yields",
            "where the last equality is due to ( A ), ( A.3 ) and ( B.1 ). Subsequently, we have",
            "The proof of Lemma  4.3  relies on the following inverse function theorem  [ Lee12 , Theorem 4.5]  on manifolds. For completeness, we re-state it here.",
            "Now we are ready to prove Lemma  4.3 .",
            "From Lemma  4.3  with the update strategy"
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A5.EGx4",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows. Section  2  introduces preliminaries related to Riemannian optimization. Section  3  discusses the update strategy of FedAvg in detail and develops its counterpart update strategy suitable for the Riemannian setting, and the resulting algorithm is called RFedAGS. Subsequently, Section  4  analyzes the convergence properties for general non-convex problems and RPL problems with two step size schemes: fixed step size and decaying step sizes. Section  5  reports extensive numerical experiment results to evaluate the performance of the proposed RFedAGS. Finally, Section  6  gives conclusions of this paper.",
            "Noting that from ( 3.4 ), the search direction at the  t t t italic_t -th outer iteration is given by the average of mini-batch gradients of all agents, which is here called gradient stream. Adopting the idea in the Riemannian setting, directly combining the mini-batch gradients located in different tangent spaces is not well defined. Fortunately, with the aid of vector transport, the combination can be defined. Specifically, the search direction is given by",
            "Aggregation ( 3.8 ) combined with ( 3.7 ) can be viewed as a generalization of ( 3.1 ) combined with ( 3.4 ). Specific to each agent  j j j italic_j , it only needs to upload",
            "The convergence analysis is established based on Assumptions  4.1 ,  4.2 , and  4.3 , which are standard and have been used in federated learning, stochastic gradient methods, and Riemannian optimization; see e.g.,  [ HAG15 ,  HGA15 ,  TFBJ18 ,  ZC18 ,  WJ21 ,  HKMC19 ,  SKM19 ] .",
            "The existence of a totally retractive neighborhood  W W \\mathcal{W} caligraphic_W  of  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is guaranteed  [ HAG15 ] , and such assumptions as Assumptions  4.1 ( 1 ) and ( 2 ) have been used in, e.g.,  [ HAG15 ,  TFBJ18 ,  SKM19 ] . Assumptions  4.1 ( 3 ) and ( 5 ) are the standard requirements for analyzing convergence in the Euclidean setting, see, e.g.,  [ ZC18 ,  Sti19 ,  WJ21 ] , and thus we make the counterparts in the Riemannian setting. For commonly-encountered manifolds, e.g., Stiefel manifolds, Grassmann manifolds, and fixed rank matrix manifolds, we can construct an isometric vector transport by parallelization  [ HGA15 ,  HAG15 ] . For another manifold whose exponential map is computationally cheap, e.g., unit sphere manifolds, symmetric positive definite matrix manifolds, and Hyperbolic manifolds, the parallel transport is an alternative of the isometric vector transport  [ AMS08 ,  Bou23 ] . In machine learning, the step sizes are usually not large, and thus we assume that they are bounded from above by a constant  A A A italic_A .",
            "Next, we make assumptions about the first and second moments of the stochastic gradients  grad  f  ( x ;  ) grad f x  \\mathrm{grad}f(x;\\xi) roman_grad italic_f ( italic_x ; italic_ ) , as stated in Assumptions  4.2  and  4.3 , which are standard in literature; see e.g.,  [ ZC18 ,  HKMC19 ,  WJ21 ] .",
            "It is observed in Assumption  4.3  that a larger batch size  B B B italic_B  results in smaller variance of the mini-batch gradient, which is in line with observation in practice, and a more general form  ( E [  1 B  s  B grad f ( x ;  s )  grad F ( x )  2 ]    grad F ( x )  2 +  2 B \\big{(}\\mathbb{E}[\\|\\frac{1}{B}\\sum_{s\\in\\mathcal{B}}\\mathrm{grad}f(x;\\xi_{s})% -\\mathrm{grad}F(x)\\|^{2}]\\leq\\beta\\|\\mathrm{grad}F(x)\\|^{2}+\\frac{\\sigma^{2}}{B} ( blackboard_E [  divide start_ARG 1 end_ARG start_ARG italic_B end_ARG  start_POSTSUBSCRIPT italic_s  caligraphic_B end_POSTSUBSCRIPT roman_grad italic_f ( italic_x ; italic_ start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ) - roman_grad italic_F ( italic_x )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  italic_  roman_grad italic_F ( italic_x )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + divide start_ARG italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_B end_ARG  with constant    0 ) \\beta\\geq 0\\big{)} italic_  0 )  is used in  [ HKMC19 ,  WJ21 ] .",
            "In the Euclidean setting, the convergence properties are based on the  L L L italic_L -smoothness of the objective function. We follow this approach in the Riemannian setting. Under Assumption  4.1 ( 4 ),  L L L italic_L -retraction smoothness of  F F F italic_F  implies that at the  t t t italic_t -th outer iteration, the following holds:",
            "Further, taking expectation over the randomness at the  t t t italic_t -th outer iteration conditioned on  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for Inequality ( 4.1 ) yields",
            "where  E t  [  ] subscript E t delimited-[]  \\mathbb{E}_{t}[\\cdot] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  ]  means the expectation over the randomness of the  t t t italic_t -outer iteration, and satisfies  E  [ F  ( x ~ t + 1 ) ] = E 1  E 2  ...  E t  [ F  ( x ~ t + 1 ) ] E delimited-[] F subscript ~ x t 1 subscript E 1 subscript E 2 ... subscript E t delimited-[] F subscript ~ x t 1 \\mathbb{E}[F(\\tilde{x}_{t+1})]=\\mathbb{E}_{1}\\mathbb{E}_{2}\\dots\\mathbb{E}_{t}% [F(\\tilde{x}_{t+1})] blackboard_E [ italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ] = blackboard_E start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ... blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [ italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT ) ]  with  E  [  ] E delimited-[]  \\mathbb{E}[\\cdot] blackboard_E [  ]  being the total expectation since  x ~ t + 1 subscript ~ x t 1 \\tilde{x}_{t+1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT  completely determined by the independent random realizations  { { {   , k , s j } s  B  , k j }  = 1 t } k = 0 K  1 } j = 1 S \\{\\{\\{\\xi_{\\tau,k,s}^{j}\\}_{{s\\in\\mathcal{B}_{\\tau,k}^{j}}}\\}_{\\tau=1}^{t}\\}_{% k=0}^{K-1}\\}_{j=1}^{S} { { { italic_ start_POSTSUBSCRIPT italic_ , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_s  caligraphic_B start_POSTSUBSCRIPT italic_ , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_ = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_k = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K - 1 end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_S end_POSTSUPERSCRIPT . The subsequent convergence analyses are based on ( 4.2 ), and thus this paper focuses on bounding the terms on the right-hand side, as stated in Lemma  4.1 ,  4.2 ,  4.3  and  4.4 , whose proofs can be found in Appendices  A ,  B ,  C  and  D . An upper bound of the second term is given in Lemma  4.1 .",
            "For the first term of the right-hand side of ( 4.2 ), Lemma  4.2  gives an upper bound.",
            "In order to further bound  E t  [  grad  F  ( x ~ t ) , R x ~ t  1  ( x ~ t + 1 )  ] subscript E t delimited-[] grad F subscript ~ x t superscript subscript R subscript ~ x t 1 subscript ~ x t 1 \\mathbb{E}_{t}[\\left<\\mathrm{grad}F(\\tilde{x}_{t}),\\mathrm{R}_{\\tilde{x}_{t}}^% {-1}(\\tilde{x}_{t+1})\\right>] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , roman_R start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t + 1 end_POSTSUBSCRIPT )  ]  for  K > 1 K 1 K>1 italic_K > 1 , from Lemma  4.2 , it is necessary to estimate the bounds for  E t  [  R x ~ t  1  ( x t , k j )  2 ] subscript E t delimited-[] superscript norm superscript subscript R subscript ~ x t 1 superscript subscript x t k j 2 \\mathbb{E}_{t}[\\|\\mathrm{R}_{\\tilde{x}_{t}}^{-1}(x_{t,k}^{j})\\|^{2}] blackboard_E start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT [  roman_R start_POSTSUBSCRIPT over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] , as theoretically discussed in Lemma  4.4  which states that for agent  j j j italic_j , the distance between the  k k k italic_k -th local update  x t , k j superscript subscript x t k j x_{t,k}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  and the the  t t t italic_t -th outer iterate  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  are controlled by the sum of squared step sizes. Intuitively, the distance increases as the number of local iterations grows, which is shown in Lemma  4.4 . Meanwhile, it also reflects the drift between an agents local update parameter  x t , k j superscript subscript x t k j x_{t,k}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  and the global parameter  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . A general result is provided in Lemma  4.3 .",
            "When  M M \\mathcal{M} caligraphic_M  reduces into a Euclidean space, e.g.,  M = R d M superscript R d \\mathcal{M}=\\mathbb{R}^{d} caligraphic_M = blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , the constants in Lemma  4.3  will become  C 2 = 1 subscript C 2 1 C_{2}=1 italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 1  and  C 3 = 0 subscript C 3 0 C_{3}=0 italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = 0 . In this case, the results correspondingly becomes   x ~ t  x t , k j  2  k    = 0 k  1  t ,  2   G F  ( x t ,  j )  2 . superscript norm subscript ~ x t superscript subscript x t k j 2 k superscript subscript  0 k 1 superscript subscript  t  2 superscript norm subscript G F superscript subscript x t  j 2 \\|\\tilde{x}_{t}-x_{t,k}^{j}\\|^{2}\\leq k\\sum_{\\tau=0}^{k-1}\\alpha_{t,\\tau}^{2}% \\|\\mathcal{G}_{F}(x_{t,\\tau}^{j})\\|^{2}.  over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT - italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  italic_k  start_POSTSUBSCRIPT italic_ = 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_k - 1 end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t , italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  caligraphic_G start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t , italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .  In Lemma  4.3 , if one uses  1 B t , k  1   s  B t , k  1 j grad  f  ( x t , k  1 j ;  t , k  1 , s j ) 1 subscript B t k 1 subscript s superscript subscript B t k 1 j grad f superscript subscript x t k 1 j superscript subscript  t k 1 s j \\frac{1}{B_{t,k-1}}\\sum_{s\\in\\mathcal{B}_{t,k-1}^{j}}\\mathrm{grad}f(x_{t,k-1}^% {j};\\xi_{t,k-1,s}^{j}) divide start_ARG 1 end_ARG start_ARG italic_B start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT end_ARG  start_POSTSUBSCRIPT italic_s  caligraphic_B start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT end_POSTSUBSCRIPT roman_grad italic_f ( italic_x start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ; italic_ start_POSTSUBSCRIPT italic_t , italic_k - 1 , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )  to replace  G F  ( x t , k  1 j ) subscript G F superscript subscript x t k 1 j \\mathcal{G}_{F}(x_{t,k-1}^{j}) caligraphic_G start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT ) , then the desired result is obtained in Lemma  4.4 .",
            "where the expectation is taken over the randomness at the  t t t italic_t -th outer iteration conditioned on  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,  M = ( C 2 2 + A 2  C 1 2  C 3 2 ) M superscript subscript C 2 2 superscript A 2 superscript subscript C 1 2 superscript subscript C 3 2 M=(C_{2}^{2}+A^{2}C_{1}^{2}C_{3}^{2}) italic_M = ( italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  is a positive constant,  A A A italic_A  is stated in Assumption  4.1 ( 6 ),  C 1 subscript C 1 C_{1} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is a constant such that   grad  F  ( x )   C 1 norm grad F x subscript C 1 \\|\\mathrm{grad}F(x)\\|\\leq C_{1}  roman_grad italic_F ( italic_x )   italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  for all  x  W x W x\\in\\mathcal{W} italic_x  caligraphic_W  (as Assumption  4.1 ( 6 )), and  C 2 subscript C 2 C_{2} italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  and  C 3 subscript C 3 C_{3} italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT  are the same as that in Lemma  4.3   3 3 3 In particular, when  M M \\mathcal{M} caligraphic_M  reduces to a Euclidean space, e.g.,  M = R d M superscript R d \\mathcal{M}=\\mathbb{R}^{d} caligraphic_M = blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , we have  M = 1 M 1 M=1 italic_M = 1 . .",
            "In particular, for  K > 1 K 1 K>1 italic_K > 1 , if   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , it follows from Lemma  4.4  that",
            "Therefore, based on Lemma  4.4 , the first term of the right-hand side of ( 4.2 ) is bounded by",
            "Next, this paper gives the first convergent result of the proposed RFedAGS, as stated in Theorem  4.1  built on the inequality ( 4.2 ), which claims the fact that the cost values at the consecutive iterates generated by RFedAGS are sufficient descent in some extent.",
            "where    ( 0 , 1 )  0 1 \\delta\\in(0,1) italic_  ( 0 , 1 )  is some constant 4 4 4 Noting that ( 4.8a ) implies ( 4.7 ), thus  ( 4.8b ) implies ( 4.7 ). On the other hand,  ( 4.7 ) allows a larger step size than that of ( 4.8b ), which is one of the reasons that we discuss the case of  K = 1 K 1 K=1 italic_K = 1  separately. , then it holds that",
            "Under conditions that we considered, it follows from Lemma  4.1  that",
            "In particular, for  K = 1 K 1 K=1 italic_K = 1 , from ( 4.5 ), ( 4.2 ) and the inequality above, we have",
            "which implies ( 4.9 ) holds for  K = 1 K 1 K=1 italic_K = 1 . For  K > 1 K 1 K>1 italic_K > 1 , plugging ( 4.6 ) and ( 4.10 ) into ( 4.2 ) yields",
            "From Theorem  4.1 , if     t   2    L  H  (    t , K , S ) < B   t   grad  F  ( x ~ t )  2 subscript    t superscript  2  L H subscript    t K S subscript   B t superscript norm grad F subscript ~ x t 2 {\\bar{\\alpha}_{t}\\sigma^{2}}\\delta LH(\\bar{\\alpha}_{t},K,S)<\\bar{B}_{t}\\|% \\mathrm{grad}\\,F(\\tilde{x}_{t})\\|^{2} over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ italic_L italic_H ( over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_K , italic_S ) < over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , then the cost values at the consecutive iterates are strictly decreasing in the sense of expectation. In addition, it is also observed that when  K = 1 K 1 K=1 italic_K = 1 , meaning all agents perform only one-step local update, the second term on the right-hand side of ( 4.9 ) equals to  L     t 2   2 2  S  B   t L superscript subscript    t 2 superscript  2 2 S subscript   B t \\frac{L\\bar{\\alpha}_{t}^{2}\\sigma^{2}}{2S\\bar{B}_{t}} divide start_ARG italic_L over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_S over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG , which decreases as the batch size  B   t subscript   B t \\bar{B}_{t} over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  increases. In fact, in this case, the proposed Algorithm  1  reduces to standard stochastic gradient method (at this time,  S  B   t S subscript   B t S\\bar{B}_{t} italic_S over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  can be viewed as the new batch size), and the results are consistent with the existing works, e.g., [ Bon13 ,  BCN18 ] .",
            "Theorem  4.1  provides that the cost values at the consecutive iterates generated by the proposed RFedAGS are bounded by the squared norm of gradient plus a term controlled by the step sizes. Subsequently, we further require that the step sizes are fixed under Conditions ( 4.7 ) and ( 4.8b ), which makes us convenient to characterize the stronger convergence properties, see Theorem  4.2 , Corollary  4.1  and Theorem  4.3 .",
            "If we run Algorithm  1  with a fixed step size   t =    subscript  t    \\alpha_{t}=\\bar{\\alpha} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG , a fixed batch size  B t = B   subscript B t   B B_{t}=\\bar{B} italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG  satisfying ( 4.7 ) and ( 4.8b ). Then the resulting sequence of iterates  { x ~ t } t = 1 T superscript subscript subscript ~ x t t 1 T \\{\\tilde{x}_{t}\\}_{t=1}^{T} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  satisfies",
            "with  H  (    , K , S ) H    K S H(\\bar{\\alpha},K,S) italic_H ( over  start_ARG italic_ end_ARG , italic_K , italic_S )  being the same as the one in Theorem  4.1  and  x   arg  min x  M  F  ( x ) superscript x subscript arg min x M F x x^{*}\\in\\operatorname*{arg\\,min}_{x\\in\\mathcal{M}}F(x) italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_x  caligraphic_M end_POSTSUBSCRIPT italic_F ( italic_x ) .",
            "Based on Theorem  4.1 , taking total expectation and summing over  t = 1 , 2 , ... , T t 1 2 ... T t=1,2,\\dots,T italic_t = 1 , 2 , ... , italic_T  for ( 4.9 ) yields",
            "A direct consequence of Theorem  4.2  is that for a fixed  K K K italic_K  and sufficient small   > 0 italic- 0 \\epsilon>0 italic_ > 0 , ensuring  1 T  E  [  t = 1 T  grad  F  ( x ~ t )  2 ]   1 T E delimited-[] superscript subscript t 1 T superscript norm grad F subscript ~ x t 2 italic- \\frac{1}{T}\\mathbb{E}[\\sum_{t=1}^{T}\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]\\leq\\epsilon divide start_ARG 1 end_ARG start_ARG italic_T end_ARG blackboard_E [  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  italic_  requires  T  O  ( 1  2 ) T O 1 superscript italic- 2 T\\geq\\mathcal{O}(\\frac{1}{{\\epsilon}^{2}}) italic_T  caligraphic_O ( divide start_ARG 1 end_ARG start_ARG italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG ) , as stated in Corollary  4.1 .",
            "Under the condition of Theorem  4.2 , if the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is given as",
            "such that ( 4.8b ) holds, the following holds that",
            "It follows that for ( 4.11 )",
            "On the other hand, setting  T  ( F  ( x ~ 1 )  F  ( x  ) )  B    L  M 2  S 3  ( 2  K  1 ) 2  ( K  1 ) 2 9   2  K 4 T F subscript ~ x 1 F superscript x   B L superscript M 2 superscript S 3 superscript 2 K 1 2 superscript K 1 2 9 superscript  2 superscript K 4 T\\geq\\frac{(F(\\tilde{x}_{1})-F(x^{*}))\\bar{B}LM^{2}S^{3}(2K-1)^{2}(K-1)^{2}}{9% \\sigma^{2}K^{4}} italic_T  divide start_ARG ( italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ) over  start_ARG italic_B end_ARG italic_L italic_M start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( 2 italic_K - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_K - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 9 italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_K start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG  is sufficient to ensure ( 4.16 ) with     t =     subscript    t superscript    \\bar{\\alpha}_{t}=\\bar{\\alpha}^{*} over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . Therefore, setting",
            "Theorem  4.3  gives an upper bound of the expected optimal gap if the objective satisfies the Riemannian Polyak-ojasiewicz (RPL) condition.",
            "Under the same conditions as Theorem  4.2  together with assuming that the function  F F F italic_F  satisfies the RPL condition",
            "where  x  = arg  min x  M  F  ( x ) superscript x subscript arg min x M F x x^{*}=\\operatorname*{arg\\,min}_{x\\in\\mathcal{M}}F(x) italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_x  caligraphic_M end_POSTSUBSCRIPT italic_F ( italic_x )  and    \\mu italic_  is a positive constant. Under Conditions ( 4.7 ), ( 4.8b ) and     < 1   ( K  1 +  )    1  K 1  \\bar{\\alpha}<\\frac{1}{\\mu(K-1+\\delta)} over  start_ARG italic_ end_ARG < divide start_ARG 1 end_ARG start_ARG italic_ ( italic_K - 1 + italic_ ) end_ARG , we have",
            "with  H  (    , K , S ) H    K S H(\\bar{\\alpha},K,S) italic_H ( over  start_ARG italic_ end_ARG , italic_K , italic_S )  being the same as the one in Theorem  4.1 .",
            "At the  t t t italic_t -th iteration, by RPL condition, we have  F  ( x ~ t )  F  ( x  )  1 2     grad  F  ( x ~ t )  F subscript ~ x t F superscript x 1 2  norm grad F subscript ~ x t F(\\tilde{x}_{t})-F(x^{*})\\leq\\frac{1}{2\\mu}\\|\\mathrm{grad}F(\\tilde{x}_{t})\\| italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  divide start_ARG 1 end_ARG start_ARG 2 italic_ end_ARG  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  . Combining this with ( 4.9 ) and taking total expectation on both sides gives rise to",
            "with  Q = K     2   2  L 2  B    H  (    , K , S ) Q K superscript    2 superscript  2 L 2   B H    K S {Q}=\\frac{K\\bar{\\alpha}^{2}\\sigma^{2}L}{2\\bar{B}}H(\\bar{\\alpha},K,S) italic_Q = divide start_ARG italic_K over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_L end_ARG start_ARG 2 over  start_ARG italic_B end_ARG end_ARG italic_H ( over  start_ARG italic_ end_ARG , italic_K , italic_S ) . Combining ( 4.1 ) and the condition for        \\bar{\\alpha} over  start_ARG italic_ end_ARG  yields the desired result.",
            "Theorem  4.2 , Corollary  4.1  and Theorem  4.3  require that the step sizes and batch sizes for all agents in all steps are the same, which results in the bound of the expected average squared gradient norms (Theorem  4.2 ) or the expected optimal gap (Theorem  4.3 ) do not vanish as  T    T T\\rightarrow\\infty italic_T   . To improve the results, we impose the decaying step sizes in each outer iteration while satisfying some standard conditions in stochastic (sub)gradient methods. Moreover, the batch sizes are not required to be fixed but only bounded. The formal statement refers to Theorem  4.4  and  4.5 .",
            "If we run Algorithm  1  with decaying step sizes   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and not fixed but bounded batch sizes  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for outer iterations satisfying ( 4.7 ), ( 4.8b ) and  B low  B   t  B up subscript B low subscript   B t subscript B up B_{\\mathrm{low}}\\leq\\bar{B}_{t}\\leq{B}_{\\mathrm{up}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_B start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT  with  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  and  B up subscript B up B_{\\mathrm{up}} italic_B start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT  being positive integers, then the resulting sequence of iterates  { x ~ t } t = 1 T superscript subscript subscript ~ x t t 1 T \\{\\tilde{x}_{t}\\}_{t=1}^{T} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  satisfies",
            "Dividing both sides of ( 4.12 ) by   t = 1 T  t superscript subscript t 1 T subscript  t \\sum_{t=1}^{T}\\alpha_{t}  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  directly results in ( 4.20 ), i.e.,",
            "Under conditions ( 4.21 ) for step sizes, we have",
            "Suppose  lim inf T   E  [  grad  F  ( x ~ t )  2 ] = 0 subscript limit-infimum  T E delimited-[] superscript norm grad F subscript ~ x t 2 0 \\liminf_{T\\rightarrow\\infty}\\mathbb{E}[\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]% \\not=0 lim inf start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT blackboard_E [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = 0 . Then there exist a positive constant   > 0 italic- 0 \\epsilon>0 italic_ > 0  and an integer  t 0 > 0 subscript t 0 0 t_{0}>0 italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT > 0  such that for all  t > t 0 t subscript t 0 t>t_{0} italic_t > italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ,  E  [  grad  F  ( x ~ t )  2 ] >  E delimited-[] superscript norm grad F subscript ~ x t 2 italic- \\mathbb{E}[\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]>\\epsilon blackboard_E [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] > italic_ . Therefore,  lim T   E  [  t = 1 T  t  t = 1 T  t   grad  F  ( x ~ t )  2 ]  lim T    t = 1 T  t    t = 1 T  t =  > 0 subscript  T E delimited-[] superscript subscript t 1 T subscript  t superscript subscript t 1 T subscript  t superscript norm grad F subscript ~ x t 2 subscript  T superscript subscript t 1 T subscript  t italic- superscript subscript t 1 T subscript  t italic- 0 \\lim_{T\\rightarrow\\infty}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\frac{\\alpha_{t}}{\\sum_% {t=1}^{T}\\alpha_{t}}\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}\\right]\\geq\\lim_{T% \\rightarrow\\infty}\\sum_{t=1}^{T}\\frac{\\alpha_{t}\\epsilon}{\\sum_{t=1}^{T}\\alpha% _{t}}=\\epsilon>0 roman_lim start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT blackboard_E [  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  roman_lim start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ end_ARG start_ARG  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG = italic_ > 0 , which contradicts with ( 4.23 ).",
            "It is stated by Theorem  4.4  that if one uses decaying step sizes with respect to outer iterations satisfying ( 4.21 ), there exists at least one accumulation of the iterates generated by Algorithm  1  which is a critical point in the sense of expectation. In addition, if one takes     t =  0 / (  + t ) subscript    t subscript  0  t \\bar{\\alpha}_{t}=\\alpha_{0}/(\\beta+t) over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT / ( italic_ + italic_t ) , then Condition ( 4.21 ) is satisfied, where   0 subscript  0 \\alpha_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and    \\beta italic_  are positive constants.",
            "Under the same conditions as Theorem  4.3  except for that the step size sequence and the batch size sequence satisfy",
            "By ( 4.18 ) , at the  t t t italic_t -th iteration, we have",
            "Now we are ready to prove ( 4.25 ) by induction. To begin with, for  t = 1 t 1 t=1 italic_t = 1 , ( 4.25 ) follows from the definition of    \\nu italic_ . Next, assuming ( 4.25 ) holds for some  t  1 t 1 t\\geq 1 italic_t  1 . From ( LABEL:Conv:RPL_decay:3 ) and denoting  t ^ =  + t ^ t  t \\hat{t}=\\gamma+t over^ start_ARG italic_t end_ARG = italic_ + italic_t , it follows that",
            "If    \\kappa italic_  is chosen to be  1 +  ~   ( K  1 +  ) 1 ~   K 1  \\frac{1+\\tilde{\\delta}}{\\mu(K-1+\\delta)} divide start_ARG 1 + over~ start_ARG italic_ end_ARG end_ARG start_ARG italic_ ( italic_K - 1 + italic_ ) end_ARG  for a constant   ~ > 0 ~  0 \\tilde{\\delta}>0 over~ start_ARG italic_ end_ARG > 0  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfies ( 4.7 ) and ( 4.8b ), then    \\nu italic_  in ( 4.26 ) becomes",
            "Therefore, we conclude that if  0 <  < 1 / 3 0  1 3 0<\\delta<1/3 0 < italic_ < 1 / 3 , the first two terms on the right-hand side of ( 4.28 ) decrease as  K K K italic_K  grows while the choice of large  K K K italic_K  would not influence    \\nu italic_  much, which implies there exists a  K > 1 K 1 K>1 italic_K > 1  such that    \\nu italic_  is minimum. However, choosing a large batch size  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  reduces    \\nu italic_  in general and thus accelerates the convergence speed by ( 4.25 ).",
            "An important question of RFedAGS is whether multiple inner iterations, i.e.,  K > 1 K 1 K>1 italic_K > 1 , bring benefits. In other words, is the optimal choice of  K K K italic_K , denoted by  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , greater than  1 1 1 1 5 5 5 View the bound of ( 4.11 ) (or ( 4.20 )) as a function  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) of  K K K italic_K . The optimal choice of  K K K italic_K  is defined as  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) with an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . ? As shown in Theorems  4.6  and  4.7 , the optimal  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  can be greater than  1 1 1 1  under some reasonable conditions. Such results are generalized from  [ ZC18 ] .",
            "We run Algorithm  1  with a fixed batch size  B t , k = B   subscript B t k   B B_{t,k}=\\bar{B} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG  and a fixed step size   t , k =    subscript  t k    \\alpha_{t,k}=\\bar{\\alpha} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG  satisfying Conditions ( 4.7 ) and ( 4.8b ). Under the same conditions as Theorem  4.2 , if the number of outer iteration  T T T italic_T  satisfies",
            "Since the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is prescribed, there exists an upper bound for  K K K italic_K , denoted by  K ~ ~ K \\tilde{K} over~ start_ARG italic_K end_ARG , such that Condition ( 4.8b ) holds for all  K < K ~ K ~ K K<\\tilde{K} italic_K < over~ start_ARG italic_K end_ARG . It follows from ( 4.11 ) that",
            "holds for  K  K ~ K ~ K K\\leq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG . Define the right hand side of the above equation as  Q 1  ( K ) := ( a 1 / K + a 2  K + a 3  ( 2  K  1 )  ( K  1 ) )  K / ( K  1 +  ) assign subscript Q 1 K subscript a 1 K subscript a 2 K subscript a 3 2 K 1 K 1 K K 1  Q_{1}(K):=(a_{1}/K+a_{2}K+a_{3}(2K-1)(K-1))K/(K-1+\\delta) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K ) := ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT / italic_K + italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_K + italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( 2 italic_K - 1 ) ( italic_K - 1 ) ) italic_K / ( italic_K - 1 + italic_ )  with  a 1 = 2  ( F  ( x ~ 1 )  F  ( x  ) ) / ( T     ) subscript a 1 2 F subscript ~ x 1 F superscript x T    a_{1}=2(F(\\tilde{x}_{1})-F(x^{*}))/(T\\bar{\\alpha}) italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2 ( italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ) / ( italic_T over  start_ARG italic_ end_ARG ) ,  a 2 =     L   2 / ( S  B   ) subscript a 2    L superscript  2 S   B a_{2}=\\bar{\\alpha}L\\sigma^{2}/(S\\bar{B}) italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG italic_L italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( italic_S over  start_ARG italic_B end_ARG )  and  a 3 =    2   2  L 2  M / ( 3  B   ) subscript a 3 superscript    2 superscript  2 superscript L 2 M 3   B a_{3}=\\bar{\\alpha}^{2}\\sigma^{2}L^{2}M/(3\\bar{B}) italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_M / ( 3 over  start_ARG italic_B end_ARG ) . It follows that  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K ) . Inequality ( 4.29 ) implies  ( 1 /  )  a 1 > ( 3  1 /  )  a 2 + 6  a 3 1  subscript a 1 3 1  subscript a 2 6 subscript a 3 \\left(1/\\delta\\right)a_{1}>(3-1/\\delta)a_{2}+6a_{3} ( 1 / italic_ ) italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > ( 3 - 1 / italic_ ) italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + 6 italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , which yields  Q 1  ( 2 ) < Q 1  ( 1 ) subscript Q 1 2 subscript Q 1 1 Q_{1}(2)<Q_{1}(1) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 2 ) < italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 1 ) . Therefore, we have inequality  K  > 1 superscript K 1 K^{*}>1 italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT > 1 , which completes the proof.",
            "We run Algorithm  1  with batch sizes  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and decaying step sizes   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfying Conditions ( 4.7 ) and ( 4.8b ). Under the same conditions as Theorem  4.4 , if the number of outer iterations  T T T italic_T  satisfies",
            "Since the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is prescribed, there exists an upper bound for  K K K italic_K , denoted by  K ~ ~ K \\tilde{K} over~ start_ARG italic_K end_ARG , such that Condition ( 4.8b ) holds for all  K < K ~ K ~ K K<\\tilde{K} italic_K < over~ start_ARG italic_K end_ARG . It follows from ( 4.20 ) that",
            "Denote the right-hand side of the inequality above by  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) . Therefore,  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) . A sufficient condition for  K  > 1 superscript K 1 K^{*}>1 italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT > 1  is  Q 2  ( 2 ) < Q 2  ( 1 ) subscript Q 2 2 subscript Q 2 1 Q_{2}(2)<Q_{2}(1) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 2 ) < italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 ) , which is guaranteed by ( 4.30 ).",
            "It is noted from Theorems  4.6  and  4.7  that the larger  F  ( x ~ 1 )  F  ( x  ) F subscript ~ x 1 F superscript x F(\\tilde{x}_{1})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , larger batch sizes, and smaller step sizes, make the conditions ( 4.8b ), ( 4.29 ) and ( 4.30 ) easier to be satisfied. Therefore, when the initial guess  x ~ 1 subscript ~ x 1 \\tilde{x}_{1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is far away from the minimizer  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , using a large  K K K italic_K  is reasonable.",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 ).",
            "In terms of fixed step size cases, the RFedAGS shows the linear convergence for the three problems, which is consistent with the theoretical result (Theorem  4.3 ) as the three problems locally satisfy RPL condition. However, due to that the right-hand side of Inequality ( 4.17 ) in Theorem  4.3  does not vanish as  T T T italic_T  goes to   \\infty  , the solutions given by RFedAGS may not be of high accuracy. All of these observations are verified in Figures  1(a) - 1(c) . To find a highly accurate solution, applying decaying step sizes is a commonly used therapy in the machine learning community. In theory (refer to Theorem  4.5 ), using the decaying step sizes satisfying condition ( LABEL:Conv:RPL_decay:0 ) makes the expected optimal gaps vanish as  T T T italic_T  goes to   \\infty  . Numerically, using the decaying step sizes ( 5.2 ) for the three problems in RFedAGS, does find a higher accurate solution compared to the fixed step size cases since the excess risk is smaller implying the solutions are close to the minimizers, refer to Figures  1(d) - 1(f) . On the other hand, it should be noticed that as the growth of  K K K italic_K , the number of inner iterations, the convergence speed is significantly improved from the theoretical results. Meanwhile,  K K K italic_K  must be not too large since too large  K K K italic_K  makes upper bounds large for expected optimal gaps, refer to Inequalities ( 4.17 ) and ( LABEL:Conv:RPL_decay:0 ). This analysis is verified by Figure  1 . At each outer iteration, the server needs to communicate with all the agents. Therefore, the communication cost between the server and agents is also reduced as the growth of  K K K italic_K  in a reasonable range.",
            "where the first equality follows from ( 3.7 ) and the inequality follows from the fact    i = 1 n x i  2  n   i = 1 n  x i  2 superscript norm superscript subscript i 1 n subscript x i 2 n superscript subscript i 1 n superscript norm subscript x i 2 \\|\\sum_{i=1}^{n}x_{i}\\|^{2}\\leq n\\sum_{i=1}^{n}\\|x_{i}\\|^{2}   start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  italic_n  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Under Assumption  4.2 , taking the expectation for fixed  t t t italic_t ,  k k k italic_k  and  j j j italic_j  yields",
            "where ( A.3 ) is due to that  x t , k j superscript subscript x t k j x_{t,k}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT but not  x t , k  1 j superscript subscript x t k 1 j x_{t,k-1}^{j} italic_x start_POSTSUBSCRIPT italic_t , italic_k - 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT depends on the randomness of the  ( k  1 ) k 1 (k-1) ( italic_k - 1 ) -th inner iteration and ( A.4 ) is due to that   t , k , s j superscript subscript  t k s j \\xi_{t,k,s}^{j} italic_ start_POSTSUBSCRIPT italic_t , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  depends on the randomness of the  k k k italic_k -th inner iteration. Hence, combining Assumption  4.3  and ( A ), we have",
            "where the third equality is due to that  {  t , k , s j } superscript subscript  t k s j \\{\\xi_{t,k,s}^{j}\\} { italic_ start_POSTSUBSCRIPT italic_t , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT }  with  j = 1 , ... , S j 1 ... S j=1,\\dots,S italic_j = 1 , ... , italic_S  and  s  B t , k j s superscript subscript B t k j s\\in\\mathcal{B}_{t,k}^{j} italic_s  caligraphic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  are independent random variables for fixed  t t t italic_t  and  k k k italic_k , the fourth equality is due to Assumption  4.2 , and the inequality is due to Assumption  4.3 . On the other hand, for fixed  k k k italic_k , we have",
            "where the first equality follows ( A.4 ), the inequality is due to ( A ), and the last equality is due to ( A.3 ). Combining ( A ) together with ( A ) yields",
            "The proof of Lemma  4.3  relies on the following inverse function theorem  [ Lee12 , Theorem 4.5]  on manifolds. For completeness, we re-state it here.",
            "Now we are ready to prove Lemma  4.3 .",
            "where the third equality is due to the inverse function Theorem  C.1 . Noting that the map  P  ,   (  ) subscript P    P_{\\cdot,\\cdot}(\\cdot) italic_P start_POSTSUBSCRIPT  ,  end_POSTSUBSCRIPT (  )  is defined in  T W = { ( x , y ,  ) : x , y  W ,   R x  1  ( W ) } subscript T W conditional-set x y  formulae-sequence x y W  superscript subscript R x 1 W \\mathrm{T}_{\\mathcal{W}}=\\{(x,y,\\eta):x,y\\in\\mathcal{W},\\eta\\in\\mathrm{R}_{x}^% {-1}(\\mathcal{W})\\} roman_T start_POSTSUBSCRIPT caligraphic_W end_POSTSUBSCRIPT = { ( italic_x , italic_y , italic_ ) : italic_x , italic_y  caligraphic_W , italic_  roman_R start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - 1 end_POSTSUPERSCRIPT ( caligraphic_W ) } , which is a compact set, according to Assumption  4.1 ( 1 ) and  4.1 ( 2 ), thus, smoothness of the retraction implies that the Jacobin and Hessian of  P  ,   (  ) subscript P    P_{\\cdot,\\cdot}(\\cdot) italic_P start_POSTSUBSCRIPT  ,  end_POSTSUBSCRIPT (  )  with respect to the third variable is uniformly bounded in norm on the compact set. We, thus, use  C 2 , C 3 > 0 subscript C 2 subscript C 3 0 C_{2},C_{3}>0 italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT > 0  to denote bounds on the operator norms of the Jacobin and Hessian of  P  ,   (  ) subscript P    P_{\\cdot,\\cdot}(\\cdot) italic_P start_POSTSUBSCRIPT  ,  end_POSTSUBSCRIPT (  )  with respect to the third variable in the compact set. Noting that",
            "From Lemma  4.3  with the update strategy",
            "where we used Assumptions  4.1 ( 1 ) and  4.1 ( 6 ) implying that for all  x  W x W x\\in\\mathcal{W} italic_x  caligraphic_W  and    \\xi italic_ , there exists  C 1 > 0 subscript C 1 0 C_{1}>0 italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0  such that   grad  f  ( x ,  )   C 1 norm grad f x  subscript C 1 \\|\\mathrm{grad}f(x,\\xi)\\|\\leq C_{1}  roman_grad italic_f ( italic_x , italic_ )   italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , and that   t , k  A subscript  t k A \\alpha_{t,k}\\leq A italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT  italic_A . On the other hand, similar to the analysis of ( A ) and ( A ), it follows that"
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A5.EGx5",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows. Section  2  introduces preliminaries related to Riemannian optimization. Section  3  discusses the update strategy of FedAvg in detail and develops its counterpart update strategy suitable for the Riemannian setting, and the resulting algorithm is called RFedAGS. Subsequently, Section  4  analyzes the convergence properties for general non-convex problems and RPL problems with two step size schemes: fixed step size and decaying step sizes. Section  5  reports extensive numerical experiment results to evaluate the performance of the proposed RFedAGS. Finally, Section  6  gives conclusions of this paper.",
            "Combining ( 3.5 ) with ( 3.6 ) shows that exactly expanding the expression of the search direction   t subscript  t \\eta_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , which involves multiple consecutive exponential mappings, is difficult in general since the exponential mapping is short of linearity. Consequently, this makes the convergence analysis more challenging, when multiple steps SGD are involved in local updates. In view of the discussion above, this paper resorts to another aggregation which can not only realize server aggregation efficiently but also analyze algorithm convergence conveniently. In our opinion, this aggregation is a more essential generalization from the Euclidean setting to the Riemannian setting.",
            "The existence of a totally retractive neighborhood  W W \\mathcal{W} caligraphic_W  of  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is guaranteed  [ HAG15 ] , and such assumptions as Assumptions  4.1 ( 1 ) and ( 2 ) have been used in, e.g.,  [ HAG15 ,  TFBJ18 ,  SKM19 ] . Assumptions  4.1 ( 3 ) and ( 5 ) are the standard requirements for analyzing convergence in the Euclidean setting, see, e.g.,  [ ZC18 ,  Sti19 ,  WJ21 ] , and thus we make the counterparts in the Riemannian setting. For commonly-encountered manifolds, e.g., Stiefel manifolds, Grassmann manifolds, and fixed rank matrix manifolds, we can construct an isometric vector transport by parallelization  [ HGA15 ,  HAG15 ] . For another manifold whose exponential map is computationally cheap, e.g., unit sphere manifolds, symmetric positive definite matrix manifolds, and Hyperbolic manifolds, the parallel transport is an alternative of the isometric vector transport  [ AMS08 ,  Bou23 ] . In machine learning, the step sizes are usually not large, and thus we assume that they are bounded from above by a constant  A A A italic_A .",
            "In particular, for  K = 1 K 1 K=1 italic_K = 1 , from ( 4.5 ), ( 4.2 ) and the inequality above, we have",
            "Theorem  4.2 , Corollary  4.1  and Theorem  4.3  require that the step sizes and batch sizes for all agents in all steps are the same, which results in the bound of the expected average squared gradient norms (Theorem  4.2 ) or the expected optimal gap (Theorem  4.3 ) do not vanish as  T    T T\\rightarrow\\infty italic_T   . To improve the results, we impose the decaying step sizes in each outer iteration while satisfying some standard conditions in stochastic (sub)gradient methods. Moreover, the batch sizes are not required to be fixed but only bounded. The formal statement refers to Theorem  4.4  and  4.5 .",
            "Now we are ready to prove ( 4.25 ) by induction. To begin with, for  t = 1 t 1 t=1 italic_t = 1 , ( 4.25 ) follows from the definition of    \\nu italic_ . Next, assuming ( 4.25 ) holds for some  t  1 t 1 t\\geq 1 italic_t  1 . From ( LABEL:Conv:RPL_decay:3 ) and denoting  t ^ =  + t ^ t  t \\hat{t}=\\gamma+t over^ start_ARG italic_t end_ARG = italic_ + italic_t , it follows that",
            "Therefore, we conclude that if  0 <  < 1 / 3 0  1 3 0<\\delta<1/3 0 < italic_ < 1 / 3 , the first two terms on the right-hand side of ( 4.28 ) decrease as  K K K italic_K  grows while the choice of large  K K K italic_K  would not influence    \\nu italic_  much, which implies there exists a  K > 1 K 1 K>1 italic_K > 1  such that    \\nu italic_  is minimum. However, choosing a large batch size  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  reduces    \\nu italic_  in general and thus accelerates the convergence speed by ( 4.25 ).",
            "In terms of fixed step size cases, the RFedAGS shows the linear convergence for the three problems, which is consistent with the theoretical result (Theorem  4.3 ) as the three problems locally satisfy RPL condition. However, due to that the right-hand side of Inequality ( 4.17 ) in Theorem  4.3  does not vanish as  T T T italic_T  goes to   \\infty  , the solutions given by RFedAGS may not be of high accuracy. All of these observations are verified in Figures  1(a) - 1(c) . To find a highly accurate solution, applying decaying step sizes is a commonly used therapy in the machine learning community. In theory (refer to Theorem  4.5 ), using the decaying step sizes satisfying condition ( LABEL:Conv:RPL_decay:0 ) makes the expected optimal gaps vanish as  T T T italic_T  goes to   \\infty  . Numerically, using the decaying step sizes ( 5.2 ) for the three problems in RFedAGS, does find a higher accurate solution compared to the fixed step size cases since the excess risk is smaller implying the solutions are close to the minimizers, refer to Figures  1(d) - 1(f) . On the other hand, it should be noticed that as the growth of  K K K italic_K , the number of inner iterations, the convergence speed is significantly improved from the theoretical results. Meanwhile,  K K K italic_K  must be not too large since too large  K K K italic_K  makes upper bounds large for expected optimal gaps, refer to Inequalities ( 4.17 ) and ( LABEL:Conv:RPL_decay:0 ). This analysis is verified by Figure  1 . At each outer iteration, the server needs to communicate with all the agents. Therefore, the communication cost between the server and agents is also reduced as the growth of  K K K italic_K  in a reasonable range.",
            "where the Grassmann manifold  Gr  ( r , m ) Gr r m \\mathrm{Gr}(r,m) roman_Gr ( italic_r , italic_m )  is equipped with the quotient manifold structure  Gr  ( r , m ) = St  ( r , m ) / O  ( r ) Gr r m St r m O r \\mathrm{Gr}(r,m)=\\mathrm{St}(r,m)/\\mathbb{O}(r) roman_Gr ( italic_r , italic_m ) = roman_St ( italic_r , italic_m ) / blackboard_O ( italic_r )  with  O  ( r ) O r \\mathbb{O}(r) blackboard_O ( italic_r )  being the orthogonal group,  U  St  ( r , m ) U St r m \\mathbf{U}\\in\\mathrm{St}(r,m) bold_U  roman_St ( italic_r , italic_m )  is a representative of  U  Gr  ( r , m ) U Gr r m \\mathcal{U}\\in\\mathrm{Gr}(r,m) caligraphic_U  roman_Gr ( italic_r , italic_m ) , and for a given  U U \\mathbf{U} bold_U ,  w i  j  U subscript w i j U w_{ij\\mathbf{U}} italic_w start_POSTSUBSCRIPT italic_i italic_j bold_U end_POSTSUBSCRIPT  is the least-squares solution to  min w i  j  R r  0.5   X i  j  U  w i  j  y i  j  F 2 +    w i  j  F 2 subscript subscript w i j superscript R r 0.5 superscript subscript norm subscript X i j U subscript w i j subscript y i j F 2  superscript subscript norm subscript w i j F 2 \\min_{w_{ij}\\in\\mathbb{R}^{r}}0.5\\|\\mathbf{X}_{ij}\\mathbf{U}w_{ij}-y_{ij}\\|_{F% }^{2}+\\lambda\\|w_{ij}\\|_{F}^{2} roman_min start_POSTSUBSCRIPT italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT end_POSTSUBSCRIPT 0.5  bold_X start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT bold_U italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT - italic_y start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_  italic_w start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , which has a closed form for    0  0 \\lambda\\geq 0 italic_  0 . Note that Problem ( 5.3 ) is defined on Grassmann manifold  Gr  ( r , m ) Gr r m \\mathrm{Gr}(r,m) roman_Gr ( italic_r , italic_m ) , but numerically implemented with matrix  U U \\mathbf{U} bold_U  in Stiefel manifold  St  ( r , m ) St r m \\mathrm{St}(r,m) roman_St ( italic_r , italic_m ) ."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A5.EGx6",
        "footnotes": [],
        "references": [
            "The remainder of this paper is organized as follows. Section  2  introduces preliminaries related to Riemannian optimization. Section  3  discusses the update strategy of FedAvg in detail and develops its counterpart update strategy suitable for the Riemannian setting, and the resulting algorithm is called RFedAGS. Subsequently, Section  4  analyzes the convergence properties for general non-convex problems and RPL problems with two step size schemes: fixed step size and decaying step sizes. Section  5  reports extensive numerical experiment results to evaluate the performance of the proposed RFedAGS. Finally, Section  6  gives conclusions of this paper.",
            "Combining ( 3.5 ) with ( 3.6 ) shows that exactly expanding the expression of the search direction   t subscript  t \\eta_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , which involves multiple consecutive exponential mappings, is difficult in general since the exponential mapping is short of linearity. Consequently, this makes the convergence analysis more challenging, when multiple steps SGD are involved in local updates. In view of the discussion above, this paper resorts to another aggregation which can not only realize server aggregation efficiently but also analyze algorithm convergence conveniently. In our opinion, this aggregation is a more essential generalization from the Euclidean setting to the Riemannian setting.",
            "where the expectation is taken over the randomness at the  t t t italic_t -th outer iteration conditioned on  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ,  M = ( C 2 2 + A 2  C 1 2  C 3 2 ) M superscript subscript C 2 2 superscript A 2 superscript subscript C 1 2 superscript subscript C 3 2 M=(C_{2}^{2}+A^{2}C_{1}^{2}C_{3}^{2}) italic_M = ( italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_A start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  is a positive constant,  A A A italic_A  is stated in Assumption  4.1 ( 6 ),  C 1 subscript C 1 C_{1} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is a constant such that   grad  F  ( x )   C 1 norm grad F x subscript C 1 \\|\\mathrm{grad}F(x)\\|\\leq C_{1}  roman_grad italic_F ( italic_x )   italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  for all  x  W x W x\\in\\mathcal{W} italic_x  caligraphic_W  (as Assumption  4.1 ( 6 )), and  C 2 subscript C 2 C_{2} italic_C start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  and  C 3 subscript C 3 C_{3} italic_C start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT  are the same as that in Lemma  4.3   3 3 3 In particular, when  M M \\mathcal{M} caligraphic_M  reduces to a Euclidean space, e.g.,  M = R d M superscript R d \\mathcal{M}=\\mathbb{R}^{d} caligraphic_M = blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT , we have  M = 1 M 1 M=1 italic_M = 1 . .",
            "which implies ( 4.9 ) holds for  K = 1 K 1 K=1 italic_K = 1 . For  K > 1 K 1 K>1 italic_K > 1 , plugging ( 4.6 ) and ( 4.10 ) into ( 4.2 ) yields",
            "On the other hand, setting  T  ( F  ( x ~ 1 )  F  ( x  ) )  B    L  M 2  S 3  ( 2  K  1 ) 2  ( K  1 ) 2 9   2  K 4 T F subscript ~ x 1 F superscript x   B L superscript M 2 superscript S 3 superscript 2 K 1 2 superscript K 1 2 9 superscript  2 superscript K 4 T\\geq\\frac{(F(\\tilde{x}_{1})-F(x^{*}))\\bar{B}LM^{2}S^{3}(2K-1)^{2}(K-1)^{2}}{9% \\sigma^{2}K^{4}} italic_T  divide start_ARG ( italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ) over  start_ARG italic_B end_ARG italic_L italic_M start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( 2 italic_K - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_K - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 9 italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_K start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG  is sufficient to ensure ( 4.16 ) with     t =     subscript    t superscript    \\bar{\\alpha}_{t}=\\bar{\\alpha}^{*} over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . Therefore, setting",
            "If    \\kappa italic_  is chosen to be  1 +  ~   ( K  1 +  ) 1 ~   K 1  \\frac{1+\\tilde{\\delta}}{\\mu(K-1+\\delta)} divide start_ARG 1 + over~ start_ARG italic_ end_ARG end_ARG start_ARG italic_ ( italic_K - 1 + italic_ ) end_ARG  for a constant   ~ > 0 ~  0 \\tilde{\\delta}>0 over~ start_ARG italic_ end_ARG > 0  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfies ( 4.7 ) and ( 4.8b ), then    \\nu italic_  in ( 4.26 ) becomes",
            "An important question of RFedAGS is whether multiple inner iterations, i.e.,  K > 1 K 1 K>1 italic_K > 1 , bring benefits. In other words, is the optimal choice of  K K K italic_K , denoted by  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , greater than  1 1 1 1 5 5 5 View the bound of ( 4.11 ) (or ( 4.20 )) as a function  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) of  K K K italic_K . The optimal choice of  K K K italic_K  is defined as  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) with an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . ? As shown in Theorems  4.6  and  4.7 , the optimal  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  can be greater than  1 1 1 1  under some reasonable conditions. Such results are generalized from  [ ZC18 ] .",
            "It is noted from Theorems  4.6  and  4.7  that the larger  F  ( x ~ 1 )  F  ( x  ) F subscript ~ x 1 F superscript x F(\\tilde{x}_{1})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , larger batch sizes, and smaller step sizes, make the conditions ( 4.8b ), ( 4.29 ) and ( 4.30 ) easier to be satisfied. Therefore, when the initial guess  x ~ 1 subscript ~ x 1 \\tilde{x}_{1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is far away from the minimizer  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , using a large  K K K italic_K  is reasonable.",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 ).",
            "where we used Assumptions  4.1 ( 1 ) and  4.1 ( 6 ) implying that for all  x  W x W x\\in\\mathcal{W} italic_x  caligraphic_W  and    \\xi italic_ , there exists  C 1 > 0 subscript C 1 0 C_{1}>0 italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > 0  such that   grad  f  ( x ,  )   C 1 norm grad f x  subscript C 1 \\|\\mathrm{grad}f(x,\\xi)\\|\\leq C_{1}  roman_grad italic_f ( italic_x , italic_ )   italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , and that   t , k  A subscript  t k A \\alpha_{t,k}\\leq A italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT  italic_A . On the other hand, similar to the analysis of ( A ) and ( A ), it follows that"
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A5.EGx7",
        "footnotes": [],
        "references": [
            "Using ( 3.7 ), the server aggregation is given by",
            "Aggregation ( 3.8 ) combined with ( 3.7 ) can be viewed as a generalization of ( 3.1 ) combined with ( 3.4 ). Specific to each agent  j j j italic_j , it only needs to upload",
            "where    ( 0 , 1 )  0 1 \\delta\\in(0,1) italic_  ( 0 , 1 )  is some constant 4 4 4 Noting that ( 4.8a ) implies ( 4.7 ), thus  ( 4.8b ) implies ( 4.7 ). On the other hand,  ( 4.7 ) allows a larger step size than that of ( 4.8b ), which is one of the reasons that we discuss the case of  K = 1 K 1 K=1 italic_K = 1  separately. , then it holds that",
            "Theorem  4.1  provides that the cost values at the consecutive iterates generated by the proposed RFedAGS are bounded by the squared norm of gradient plus a term controlled by the step sizes. Subsequently, we further require that the step sizes are fixed under Conditions ( 4.7 ) and ( 4.8b ), which makes us convenient to characterize the stronger convergence properties, see Theorem  4.2 , Corollary  4.1  and Theorem  4.3 .",
            "If we run Algorithm  1  with a fixed step size   t =    subscript  t    \\alpha_{t}=\\bar{\\alpha} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG , a fixed batch size  B t = B   subscript B t   B B_{t}=\\bar{B} italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG  satisfying ( 4.7 ) and ( 4.8b ). Then the resulting sequence of iterates  { x ~ t } t = 1 T superscript subscript subscript ~ x t t 1 T \\{\\tilde{x}_{t}\\}_{t=1}^{T} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  satisfies",
            "where  x  = arg  min x  M  F  ( x ) superscript x subscript arg min x M F x x^{*}=\\operatorname*{arg\\,min}_{x\\in\\mathcal{M}}F(x) italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_x  caligraphic_M end_POSTSUBSCRIPT italic_F ( italic_x )  and    \\mu italic_  is a positive constant. Under Conditions ( 4.7 ), ( 4.8b ) and     < 1   ( K  1 +  )    1  K 1  \\bar{\\alpha}<\\frac{1}{\\mu(K-1+\\delta)} over  start_ARG italic_ end_ARG < divide start_ARG 1 end_ARG start_ARG italic_ ( italic_K - 1 + italic_ ) end_ARG , we have",
            "If we run Algorithm  1  with decaying step sizes   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and not fixed but bounded batch sizes  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for outer iterations satisfying ( 4.7 ), ( 4.8b ) and  B low  B   t  B up subscript B low subscript   B t subscript B up B_{\\mathrm{low}}\\leq\\bar{B}_{t}\\leq{B}_{\\mathrm{up}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_B start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT  with  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  and  B up subscript B up B_{\\mathrm{up}} italic_B start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT  being positive integers, then the resulting sequence of iterates  { x ~ t } t = 1 T superscript subscript subscript ~ x t t 1 T \\{\\tilde{x}_{t}\\}_{t=1}^{T} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  satisfies",
            "If    \\kappa italic_  is chosen to be  1 +  ~   ( K  1 +  ) 1 ~   K 1  \\frac{1+\\tilde{\\delta}}{\\mu(K-1+\\delta)} divide start_ARG 1 + over~ start_ARG italic_ end_ARG end_ARG start_ARG italic_ ( italic_K - 1 + italic_ ) end_ARG  for a constant   ~ > 0 ~  0 \\tilde{\\delta}>0 over~ start_ARG italic_ end_ARG > 0  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfies ( 4.7 ) and ( 4.8b ), then    \\nu italic_  in ( 4.26 ) becomes",
            "An important question of RFedAGS is whether multiple inner iterations, i.e.,  K > 1 K 1 K>1 italic_K > 1 , bring benefits. In other words, is the optimal choice of  K K K italic_K , denoted by  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , greater than  1 1 1 1 5 5 5 View the bound of ( 4.11 ) (or ( 4.20 )) as a function  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) of  K K K italic_K . The optimal choice of  K K K italic_K  is defined as  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) with an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . ? As shown in Theorems  4.6  and  4.7 , the optimal  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  can be greater than  1 1 1 1  under some reasonable conditions. Such results are generalized from  [ ZC18 ] .",
            "We run Algorithm  1  with a fixed batch size  B t , k = B   subscript B t k   B B_{t,k}=\\bar{B} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG  and a fixed step size   t , k =    subscript  t k    \\alpha_{t,k}=\\bar{\\alpha} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG  satisfying Conditions ( 4.7 ) and ( 4.8b ). Under the same conditions as Theorem  4.2 , if the number of outer iteration  T T T italic_T  satisfies",
            "We run Algorithm  1  with batch sizes  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and decaying step sizes   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfying Conditions ( 4.7 ) and ( 4.8b ). Under the same conditions as Theorem  4.4 , if the number of outer iterations  T T T italic_T  satisfies",
            "It is noted from Theorems  4.6  and  4.7  that the larger  F  ( x ~ 1 )  F  ( x  ) F subscript ~ x 1 F superscript x F(\\tilde{x}_{1})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , larger batch sizes, and smaller step sizes, make the conditions ( 4.8b ), ( 4.29 ) and ( 4.30 ) easier to be satisfied. Therefore, when the initial guess  x ~ 1 subscript ~ x 1 \\tilde{x}_{1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is far away from the minimizer  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , using a large  K K K italic_K  is reasonable.",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 ).",
            "In terms of fixed step size cases, the RFedAGS shows the linear convergence for the three problems, which is consistent with the theoretical result (Theorem  4.3 ) as the three problems locally satisfy RPL condition. However, due to that the right-hand side of Inequality ( 4.17 ) in Theorem  4.3  does not vanish as  T T T italic_T  goes to   \\infty  , the solutions given by RFedAGS may not be of high accuracy. All of these observations are verified in Figures  1(a) - 1(c) . To find a highly accurate solution, applying decaying step sizes is a commonly used therapy in the machine learning community. In theory (refer to Theorem  4.5 ), using the decaying step sizes satisfying condition ( LABEL:Conv:RPL_decay:0 ) makes the expected optimal gaps vanish as  T T T italic_T  goes to   \\infty  . Numerically, using the decaying step sizes ( 5.2 ) for the three problems in RFedAGS, does find a higher accurate solution compared to the fixed step size cases since the excess risk is smaller implying the solutions are close to the minimizers, refer to Figures  1(d) - 1(f) . On the other hand, it should be noticed that as the growth of  K K K italic_K , the number of inner iterations, the convergence speed is significantly improved from the theoretical results. Meanwhile,  K K K italic_K  must be not too large since too large  K K K italic_K  makes upper bounds large for expected optimal gaps, refer to Inequalities ( 4.17 ) and ( LABEL:Conv:RPL_decay:0 ). This analysis is verified by Figure  1 . At each outer iteration, the server needs to communicate with all the agents. Therefore, the communication cost between the server and agents is also reduced as the growth of  K K K italic_K  in a reasonable range.",
            "where the first equality follows from ( 3.7 ) and the inequality follows from the fact    i = 1 n x i  2  n   i = 1 n  x i  2 superscript norm superscript subscript i 1 n subscript x i 2 n superscript subscript i 1 n superscript norm subscript x i 2 \\|\\sum_{i=1}^{n}x_{i}\\|^{2}\\leq n\\sum_{i=1}^{n}\\|x_{i}\\|^{2}   start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  italic_n  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Under Assumption  4.2 , taking the expectation for fixed  t t t italic_t ,  k k k italic_k  and  j j j italic_j  yields"
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A5.EGx8",
        "footnotes": [],
        "references": [
            "Aggregation ( 3.8 ) combined with ( 3.7 ) can be viewed as a generalization of ( 3.1 ) combined with ( 3.4 ). Specific to each agent  j j j italic_j , it only needs to upload",
            "From the perspective of geometry, tangent mean ( 3.3 ) projects the final inner iterates  x t + K j superscript subscript x t K j x_{t+K}^{j} italic_x start_POSTSUBSCRIPT italic_t + italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT  back to the tangent space at  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , then averages them and finally retracts the average into the manifold. While in aggregation ( 3.8 ), the intermediary negative mini-batch-gradients   1 B t , k   s  B t , k j grad  f  ( x t , k j ,  t , k , s j ) 1 subscript B t k subscript s subscript superscript B j t k grad f superscript subscript x t k j superscript subscript  t k s j -\\frac{1}{B_{t,k}}\\sum_{s\\in\\mathcal{B}^{j}_{t,k}}\\mathrm{grad}f(x_{t,k}^{j},% \\xi_{t,k,s}^{j}) - divide start_ARG 1 end_ARG start_ARG italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT end_ARG  start_POSTSUBSCRIPT italic_s  caligraphic_B start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT roman_grad italic_f ( italic_x start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT , italic_ start_POSTSUBSCRIPT italic_t , italic_k , italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_j end_POSTSUPERSCRIPT )  are transported to the tangent space at  x ~ t subscript ~ x t \\tilde{x}_{t} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  in some way, then averages them and finally retracts the average into the manifold. In particular, letting the proposed aggregation ( 3.8 ) use the exponential map and parallel transport, the two aggregations coincide when (i)  M = R d M superscript R d \\mathcal{M}=\\mathbb{R}^{d} caligraphic_M = blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT ; or (ii)  K = 1 K 1 K=1 italic_K = 1 .",
            "where    ( 0 , 1 )  0 1 \\delta\\in(0,1) italic_  ( 0 , 1 )  is some constant 4 4 4 Noting that ( 4.8a ) implies ( 4.7 ), thus  ( 4.8b ) implies ( 4.7 ). On the other hand,  ( 4.7 ) allows a larger step size than that of ( 4.8b ), which is one of the reasons that we discuss the case of  K = 1 K 1 K=1 italic_K = 1  separately. , then it holds that",
            "Theorem  4.1  provides that the cost values at the consecutive iterates generated by the proposed RFedAGS are bounded by the squared norm of gradient plus a term controlled by the step sizes. Subsequently, we further require that the step sizes are fixed under Conditions ( 4.7 ) and ( 4.8b ), which makes us convenient to characterize the stronger convergence properties, see Theorem  4.2 , Corollary  4.1  and Theorem  4.3 .",
            "If we run Algorithm  1  with a fixed step size   t =    subscript  t    \\alpha_{t}=\\bar{\\alpha} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG , a fixed batch size  B t = B   subscript B t   B B_{t}=\\bar{B} italic_B start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG  satisfying ( 4.7 ) and ( 4.8b ). Then the resulting sequence of iterates  { x ~ t } t = 1 T superscript subscript subscript ~ x t t 1 T \\{\\tilde{x}_{t}\\}_{t=1}^{T} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  satisfies",
            "such that ( 4.8b ) holds, the following holds that",
            "where  x  = arg  min x  M  F  ( x ) superscript x subscript arg min x M F x x^{*}=\\operatorname*{arg\\,min}_{x\\in\\mathcal{M}}F(x) italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_x  caligraphic_M end_POSTSUBSCRIPT italic_F ( italic_x )  and    \\mu italic_  is a positive constant. Under Conditions ( 4.7 ), ( 4.8b ) and     < 1   ( K  1 +  )    1  K 1  \\bar{\\alpha}<\\frac{1}{\\mu(K-1+\\delta)} over  start_ARG italic_ end_ARG < divide start_ARG 1 end_ARG start_ARG italic_ ( italic_K - 1 + italic_ ) end_ARG , we have",
            "If we run Algorithm  1  with decaying step sizes   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , and not fixed but bounded batch sizes  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for outer iterations satisfying ( 4.7 ), ( 4.8b ) and  B low  B   t  B up subscript B low subscript   B t subscript B up B_{\\mathrm{low}}\\leq\\bar{B}_{t}\\leq{B}_{\\mathrm{up}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  italic_B start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT  with  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  and  B up subscript B up B_{\\mathrm{up}} italic_B start_POSTSUBSCRIPT roman_up end_POSTSUBSCRIPT  being positive integers, then the resulting sequence of iterates  { x ~ t } t = 1 T superscript subscript subscript ~ x t t 1 T \\{\\tilde{x}_{t}\\}_{t=1}^{T} { over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT  satisfies",
            "By ( 4.18 ) , at the  t t t italic_t -th iteration, we have",
            "If    \\kappa italic_  is chosen to be  1 +  ~   ( K  1 +  ) 1 ~   K 1  \\frac{1+\\tilde{\\delta}}{\\mu(K-1+\\delta)} divide start_ARG 1 + over~ start_ARG italic_ end_ARG end_ARG start_ARG italic_ ( italic_K - 1 + italic_ ) end_ARG  for a constant   ~ > 0 ~  0 \\tilde{\\delta}>0 over~ start_ARG italic_ end_ARG > 0  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfies ( 4.7 ) and ( 4.8b ), then    \\nu italic_  in ( 4.26 ) becomes",
            "Therefore, we conclude that if  0 <  < 1 / 3 0  1 3 0<\\delta<1/3 0 < italic_ < 1 / 3 , the first two terms on the right-hand side of ( 4.28 ) decrease as  K K K italic_K  grows while the choice of large  K K K italic_K  would not influence    \\nu italic_  much, which implies there exists a  K > 1 K 1 K>1 italic_K > 1  such that    \\nu italic_  is minimum. However, choosing a large batch size  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  reduces    \\nu italic_  in general and thus accelerates the convergence speed by ( 4.25 ).",
            "We run Algorithm  1  with a fixed batch size  B t , k = B   subscript B t k   B B_{t,k}=\\bar{B} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG  and a fixed step size   t , k =    subscript  t k    \\alpha_{t,k}=\\bar{\\alpha} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG  satisfying Conditions ( 4.7 ) and ( 4.8b ). Under the same conditions as Theorem  4.2 , if the number of outer iteration  T T T italic_T  satisfies",
            "Since the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is prescribed, there exists an upper bound for  K K K italic_K , denoted by  K ~ ~ K \\tilde{K} over~ start_ARG italic_K end_ARG , such that Condition ( 4.8b ) holds for all  K < K ~ K ~ K K<\\tilde{K} italic_K < over~ start_ARG italic_K end_ARG . It follows from ( 4.11 ) that",
            "We run Algorithm  1  with batch sizes  B t , k = B   t subscript B t k subscript   B t B_{t,k}=\\bar{B}_{t} italic_B start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and decaying step sizes   t , k =    t subscript  t k subscript    t \\alpha_{t,k}=\\bar{\\alpha}_{t} italic_ start_POSTSUBSCRIPT italic_t , italic_k end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfying Conditions ( 4.7 ) and ( 4.8b ). Under the same conditions as Theorem  4.4 , if the number of outer iterations  T T T italic_T  satisfies",
            "Since the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is prescribed, there exists an upper bound for  K K K italic_K , denoted by  K ~ ~ K \\tilde{K} over~ start_ARG italic_K end_ARG , such that Condition ( 4.8b ) holds for all  K < K ~ K ~ K K<\\tilde{K} italic_K < over~ start_ARG italic_K end_ARG . It follows from ( 4.20 ) that",
            "It is noted from Theorems  4.6  and  4.7  that the larger  F  ( x ~ 1 )  F  ( x  ) F subscript ~ x 1 F superscript x F(\\tilde{x}_{1})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , larger batch sizes, and smaller step sizes, make the conditions ( 4.8b ), ( 4.29 ) and ( 4.30 ) easier to be satisfied. Therefore, when the initial guess  x ~ 1 subscript ~ x 1 \\tilde{x}_{1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is far away from the minimizer  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , using a large  K K K italic_K  is reasonable."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S3.E6",
        "footnotes": [],
        "references": [
            "which implies ( 4.9 ) holds for  K = 1 K 1 K=1 italic_K = 1 . For  K > 1 K 1 K>1 italic_K > 1 , plugging ( 4.6 ) and ( 4.10 ) into ( 4.2 ) yields",
            "From Theorem  4.1 , if     t   2    L  H  (    t , K , S ) < B   t   grad  F  ( x ~ t )  2 subscript    t superscript  2  L H subscript    t K S subscript   B t superscript norm grad F subscript ~ x t 2 {\\bar{\\alpha}_{t}\\sigma^{2}}\\delta LH(\\bar{\\alpha}_{t},K,S)<\\bar{B}_{t}\\|% \\mathrm{grad}\\,F(\\tilde{x}_{t})\\|^{2} over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ italic_L italic_H ( over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , italic_K , italic_S ) < over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , then the cost values at the consecutive iterates are strictly decreasing in the sense of expectation. In addition, it is also observed that when  K = 1 K 1 K=1 italic_K = 1 , meaning all agents perform only one-step local update, the second term on the right-hand side of ( 4.9 ) equals to  L     t 2   2 2  S  B   t L superscript subscript    t 2 superscript  2 2 S subscript   B t \\frac{L\\bar{\\alpha}_{t}^{2}\\sigma^{2}}{2S\\bar{B}_{t}} divide start_ARG italic_L over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 2 italic_S over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG , which decreases as the batch size  B   t subscript   B t \\bar{B}_{t} over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  increases. In fact, in this case, the proposed Algorithm  1  reduces to standard stochastic gradient method (at this time,  S  B   t S subscript   B t S\\bar{B}_{t} italic_S over  start_ARG italic_B end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  can be viewed as the new batch size), and the results are consistent with the existing works, e.g., [ Bon13 ,  BCN18 ] .",
            "Based on Theorem  4.1 , taking total expectation and summing over  t = 1 , 2 , ... , T t 1 2 ... T t=1,2,\\dots,T italic_t = 1 , 2 , ... , italic_T  for ( 4.9 ) yields",
            "At the  t t t italic_t -th iteration, by RPL condition, we have  F  ( x ~ t )  F  ( x  )  1 2     grad  F  ( x ~ t )  F subscript ~ x t F superscript x 1 2  norm grad F subscript ~ x t F(\\tilde{x}_{t})-F(x^{*})\\leq\\frac{1}{2\\mu}\\|\\mathrm{grad}F(\\tilde{x}_{t})\\| italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT )  divide start_ARG 1 end_ARG start_ARG 2 italic_ end_ARG  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  . Combining this with ( 4.9 ) and taking total expectation on both sides gives rise to",
            "holds for  K  K ~ K ~ K K\\leq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG . Define the right hand side of the above equation as  Q 1  ( K ) := ( a 1 / K + a 2  K + a 3  ( 2  K  1 )  ( K  1 ) )  K / ( K  1 +  ) assign subscript Q 1 K subscript a 1 K subscript a 2 K subscript a 3 2 K 1 K 1 K K 1  Q_{1}(K):=(a_{1}/K+a_{2}K+a_{3}(2K-1)(K-1))K/(K-1+\\delta) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K ) := ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT / italic_K + italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_K + italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( 2 italic_K - 1 ) ( italic_K - 1 ) ) italic_K / ( italic_K - 1 + italic_ )  with  a 1 = 2  ( F  ( x ~ 1 )  F  ( x  ) ) / ( T     ) subscript a 1 2 F subscript ~ x 1 F superscript x T    a_{1}=2(F(\\tilde{x}_{1})-F(x^{*}))/(T\\bar{\\alpha}) italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2 ( italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ) / ( italic_T over  start_ARG italic_ end_ARG ) ,  a 2 =     L   2 / ( S  B   ) subscript a 2    L superscript  2 S   B a_{2}=\\bar{\\alpha}L\\sigma^{2}/(S\\bar{B}) italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG italic_L italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( italic_S over  start_ARG italic_B end_ARG )  and  a 3 =    2   2  L 2  M / ( 3  B   ) subscript a 3 superscript    2 superscript  2 superscript L 2 M 3   B a_{3}=\\bar{\\alpha}^{2}\\sigma^{2}L^{2}M/(3\\bar{B}) italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_M / ( 3 over  start_ARG italic_B end_ARG ) . It follows that  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K ) . Inequality ( 4.29 ) implies  ( 1 /  )  a 1 > ( 3  1 /  )  a 2 + 6  a 3 1  subscript a 1 3 1  subscript a 2 6 subscript a 3 \\left(1/\\delta\\right)a_{1}>(3-1/\\delta)a_{2}+6a_{3} ( 1 / italic_ ) italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > ( 3 - 1 / italic_ ) italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + 6 italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , which yields  Q 1  ( 2 ) < Q 1  ( 1 ) subscript Q 1 2 subscript Q 1 1 Q_{1}(2)<Q_{1}(1) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 2 ) < italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 1 ) . Therefore, we have inequality  K  > 1 superscript K 1 K^{*}>1 italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT > 1 , which completes the proof.",
            "It is noted from Theorems  4.6  and  4.7  that the larger  F  ( x ~ 1 )  F  ( x  ) F subscript ~ x 1 F superscript x F(\\tilde{x}_{1})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , larger batch sizes, and smaller step sizes, make the conditions ( 4.8b ), ( 4.29 ) and ( 4.30 ) easier to be satisfied. Therefore, when the initial guess  x ~ 1 subscript ~ x 1 \\tilde{x}_{1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is far away from the minimizer  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , using a large  K K K italic_K  is reasonable.",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 )."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A5.EGx9",
        "footnotes": [],
        "references": [
            "which implies ( 4.9 ) holds for  K = 1 K 1 K=1 italic_K = 1 . For  K > 1 K 1 K>1 italic_K > 1 , plugging ( 4.6 ) and ( 4.10 ) into ( 4.2 ) yields"
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A5.EGx10",
        "footnotes": [],
        "references": [
            "It follows that for ( 4.11 )",
            "An important question of RFedAGS is whether multiple inner iterations, i.e.,  K > 1 K 1 K>1 italic_K > 1 , bring benefits. In other words, is the optimal choice of  K K K italic_K , denoted by  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , greater than  1 1 1 1 5 5 5 View the bound of ( 4.11 ) (or ( 4.20 )) as a function  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) of  K K K italic_K . The optimal choice of  K K K italic_K  is defined as  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) with an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . ? As shown in Theorems  4.6  and  4.7 , the optimal  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  can be greater than  1 1 1 1  under some reasonable conditions. Such results are generalized from  [ ZC18 ] .",
            "Since the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is prescribed, there exists an upper bound for  K K K italic_K , denoted by  K ~ ~ K \\tilde{K} over~ start_ARG italic_K end_ARG , such that Condition ( 4.8b ) holds for all  K < K ~ K ~ K K<\\tilde{K} italic_K < over~ start_ARG italic_K end_ARG . It follows from ( 4.11 ) that",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 )."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A5.EGx11",
        "footnotes": [],
        "references": [
            "Dividing both sides of ( 4.12 ) by   t = 1 T  t superscript subscript t 1 T subscript  t \\sum_{t=1}^{T}\\alpha_{t}  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  directly results in ( 4.20 ), i.e.,"
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A5.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A5.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "S4.E4",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A5.EGx14",
        "footnotes": [],
        "references": [
            "On the other hand, setting  T  ( F  ( x ~ 1 )  F  ( x  ) )  B    L  M 2  S 3  ( 2  K  1 ) 2  ( K  1 ) 2 9   2  K 4 T F subscript ~ x 1 F superscript x   B L superscript M 2 superscript S 3 superscript 2 K 1 2 superscript K 1 2 9 superscript  2 superscript K 4 T\\geq\\frac{(F(\\tilde{x}_{1})-F(x^{*}))\\bar{B}LM^{2}S^{3}(2K-1)^{2}(K-1)^{2}}{9% \\sigma^{2}K^{4}} italic_T  divide start_ARG ( italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ) over  start_ARG italic_B end_ARG italic_L italic_M start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_S start_POSTSUPERSCRIPT 3 end_POSTSUPERSCRIPT ( 2 italic_K - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_K - 1 ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG 9 italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_K start_POSTSUPERSCRIPT 4 end_POSTSUPERSCRIPT end_ARG  is sufficient to ensure ( 4.16 ) with     t =     subscript    t superscript    \\bar{\\alpha}_{t}=\\bar{\\alpha}^{*} over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . Therefore, setting"
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "A5.EGx15",
        "footnotes": [],
        "references": [
            "In terms of fixed step size cases, the RFedAGS shows the linear convergence for the three problems, which is consistent with the theoretical result (Theorem  4.3 ) as the three problems locally satisfy RPL condition. However, due to that the right-hand side of Inequality ( 4.17 ) in Theorem  4.3  does not vanish as  T T T italic_T  goes to   \\infty  , the solutions given by RFedAGS may not be of high accuracy. All of these observations are verified in Figures  1(a) - 1(c) . To find a highly accurate solution, applying decaying step sizes is a commonly used therapy in the machine learning community. In theory (refer to Theorem  4.5 ), using the decaying step sizes satisfying condition ( LABEL:Conv:RPL_decay:0 ) makes the expected optimal gaps vanish as  T T T italic_T  goes to   \\infty  . Numerically, using the decaying step sizes ( 5.2 ) for the three problems in RFedAGS, does find a higher accurate solution compared to the fixed step size cases since the excess risk is smaller implying the solutions are close to the minimizers, refer to Figures  1(d) - 1(f) . On the other hand, it should be noticed that as the growth of  K K K italic_K , the number of inner iterations, the convergence speed is significantly improved from the theoretical results. Meanwhile,  K K K italic_K  must be not too large since too large  K K K italic_K  makes upper bounds large for expected optimal gaps, refer to Inequalities ( 4.17 ) and ( LABEL:Conv:RPL_decay:0 ). This analysis is verified by Figure  1 . At each outer iteration, the server needs to communicate with all the agents. Therefore, the communication cost between the server and agents is also reduced as the growth of  K K K italic_K  in a reasonable range."
        ]
    },
    "id_table_18": {
        "caption": "",
        "table": "A5.EGx16",
        "footnotes": [],
        "references": [
            "By ( 4.18 ) , at the  t t t italic_t -th iteration, we have"
        ]
    },
    "id_table_19": {
        "caption": "",
        "table": "A5.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A5.EGx18",
        "footnotes": [],
        "references": [
            "Dividing both sides of ( 4.12 ) by   t = 1 T  t superscript subscript t 1 T subscript  t \\sum_{t=1}^{T}\\alpha_{t}  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  directly results in ( 4.20 ), i.e.,",
            "An important question of RFedAGS is whether multiple inner iterations, i.e.,  K > 1 K 1 K>1 italic_K > 1 , bring benefits. In other words, is the optimal choice of  K K K italic_K , denoted by  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , greater than  1 1 1 1 5 5 5 View the bound of ( 4.11 ) (or ( 4.20 )) as a function  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) of  K K K italic_K . The optimal choice of  K K K italic_K  is defined as  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  (or  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) ) with an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . ? As shown in Theorems  4.6  and  4.7 , the optimal  K  superscript K K^{*} italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  can be greater than  1 1 1 1  under some reasonable conditions. Such results are generalized from  [ ZC18 ] .",
            "Since the step size        \\bar{\\alpha} over  start_ARG italic_ end_ARG  is prescribed, there exists an upper bound for  K K K italic_K , denoted by  K ~ ~ K \\tilde{K} over~ start_ARG italic_K end_ARG , such that Condition ( 4.8b ) holds for all  K < K ~ K ~ K K<\\tilde{K} italic_K < over~ start_ARG italic_K end_ARG . It follows from ( 4.20 ) that",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 )."
        ]
    },
    "id_table_21": {
        "caption": "",
        "table": "S4.E6",
        "footnotes": [],
        "references": [
            "Under conditions ( 4.21 ) for step sizes, we have",
            "It is stated by Theorem  4.4  that if one uses decaying step sizes with respect to outer iterations satisfying ( 4.21 ), there exists at least one accumulation of the iterates generated by Algorithm  1  which is a critical point in the sense of expectation. In addition, if one takes     t =  0 / (  + t ) subscript    t subscript  0  t \\bar{\\alpha}_{t}=\\alpha_{0}/(\\beta+t) over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT / ( italic_ + italic_t ) , then Condition ( 4.21 ) is satisfied, where   0 subscript  0 \\alpha_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  and    \\beta italic_  are positive constants."
        ]
    },
    "id_table_22": {
        "caption": "",
        "table": "A5.EGx19",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A5.EGx20",
        "footnotes": [],
        "references": [
            "Suppose  lim inf T   E  [  grad  F  ( x ~ t )  2 ] = 0 subscript limit-infimum  T E delimited-[] superscript norm grad F subscript ~ x t 2 0 \\liminf_{T\\rightarrow\\infty}\\mathbb{E}[\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]% \\not=0 lim inf start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT blackboard_E [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] = 0 . Then there exist a positive constant   > 0 italic- 0 \\epsilon>0 italic_ > 0  and an integer  t 0 > 0 subscript t 0 0 t_{0}>0 italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT > 0  such that for all  t > t 0 t subscript t 0 t>t_{0} italic_t > italic_t start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ,  E  [  grad  F  ( x ~ t )  2 ] >  E delimited-[] superscript norm grad F subscript ~ x t 2 italic- \\mathbb{E}[\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}]>\\epsilon blackboard_E [  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] > italic_ . Therefore,  lim T   E  [  t = 1 T  t  t = 1 T  t   grad  F  ( x ~ t )  2 ]  lim T    t = 1 T  t    t = 1 T  t =  > 0 subscript  T E delimited-[] superscript subscript t 1 T subscript  t superscript subscript t 1 T subscript  t superscript norm grad F subscript ~ x t 2 subscript  T superscript subscript t 1 T subscript  t italic- superscript subscript t 1 T subscript  t italic- 0 \\lim_{T\\rightarrow\\infty}\\mathbb{E}\\left[\\sum_{t=1}^{T}\\frac{\\alpha_{t}}{\\sum_% {t=1}^{T}\\alpha_{t}}\\|\\mathrm{grad}F(\\tilde{x}_{t})\\|^{2}\\right]\\geq\\lim_{T% \\rightarrow\\infty}\\sum_{t=1}^{T}\\frac{\\alpha_{t}\\epsilon}{\\sum_{t=1}^{T}\\alpha% _{t}}=\\epsilon>0 roman_lim start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT blackboard_E [  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG  roman_grad italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  roman_lim start_POSTSUBSCRIPT italic_T   end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT italic_ end_ARG start_ARG  start_POSTSUBSCRIPT italic_t = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG = italic_ > 0 , which contradicts with ( 4.23 )."
        ]
    },
    "id_table_24": {
        "caption": "",
        "table": "S4.E9",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A5.EGx21",
        "footnotes": [],
        "references": [
            "Now we are ready to prove ( 4.25 ) by induction. To begin with, for  t = 1 t 1 t=1 italic_t = 1 , ( 4.25 ) follows from the definition of    \\nu italic_ . Next, assuming ( 4.25 ) holds for some  t  1 t 1 t\\geq 1 italic_t  1 . From ( LABEL:Conv:RPL_decay:3 ) and denoting  t ^ =  + t ^ t  t \\hat{t}=\\gamma+t over^ start_ARG italic_t end_ARG = italic_ + italic_t , it follows that",
            "Therefore, we conclude that if  0 <  < 1 / 3 0  1 3 0<\\delta<1/3 0 < italic_ < 1 / 3 , the first two terms on the right-hand side of ( 4.28 ) decrease as  K K K italic_K  grows while the choice of large  K K K italic_K  would not influence    \\nu italic_  much, which implies there exists a  K > 1 K 1 K>1 italic_K > 1  such that    \\nu italic_  is minimum. However, choosing a large batch size  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  reduces    \\nu italic_  in general and thus accelerates the convergence speed by ( 4.25 )."
        ]
    },
    "id_table_26": {
        "caption": "",
        "table": "A5.EGx22",
        "footnotes": [],
        "references": [
            "If    \\kappa italic_  is chosen to be  1 +  ~   ( K  1 +  ) 1 ~   K 1  \\frac{1+\\tilde{\\delta}}{\\mu(K-1+\\delta)} divide start_ARG 1 + over~ start_ARG italic_ end_ARG end_ARG start_ARG italic_ ( italic_K - 1 + italic_ ) end_ARG  for a constant   ~ > 0 ~  0 \\tilde{\\delta}>0 over~ start_ARG italic_ end_ARG > 0  such that  a   1 subscript   a 1 \\bar{a}_{1} over  start_ARG italic_a end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  satisfies ( 4.7 ) and ( 4.8b ), then    \\nu italic_  in ( 4.26 ) becomes"
        ]
    },
    "id_table_27": {
        "caption": "",
        "table": "A5.EGx23",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "A5.EGx24",
        "footnotes": [],
        "references": [
            "Therefore, we conclude that if  0 <  < 1 / 3 0  1 3 0<\\delta<1/3 0 < italic_ < 1 / 3 , the first two terms on the right-hand side of ( 4.28 ) decrease as  K K K italic_K  grows while the choice of large  K K K italic_K  would not influence    \\nu italic_  much, which implies there exists a  K > 1 K 1 K>1 italic_K > 1  such that    \\nu italic_  is minimum. However, choosing a large batch size  B low subscript B low B_{\\mathrm{low}} italic_B start_POSTSUBSCRIPT roman_low end_POSTSUBSCRIPT  reduces    \\nu italic_  in general and thus accelerates the convergence speed by ( 4.25 )."
        ]
    },
    "id_table_29": {
        "caption": "",
        "table": "A5.EGx25",
        "footnotes": [],
        "references": [
            "holds for  K  K ~ K ~ K K\\leq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG . Define the right hand side of the above equation as  Q 1  ( K ) := ( a 1 / K + a 2  K + a 3  ( 2  K  1 )  ( K  1 ) )  K / ( K  1 +  ) assign subscript Q 1 K subscript a 1 K subscript a 2 K subscript a 3 2 K 1 K 1 K K 1  Q_{1}(K):=(a_{1}/K+a_{2}K+a_{3}(2K-1)(K-1))K/(K-1+\\delta) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K ) := ( italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT / italic_K + italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_K + italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ( 2 italic_K - 1 ) ( italic_K - 1 ) ) italic_K / ( italic_K - 1 + italic_ )  with  a 1 = 2  ( F  ( x ~ 1 )  F  ( x  ) ) / ( T     ) subscript a 1 2 F subscript ~ x 1 F superscript x T    a_{1}=2(F(\\tilde{x}_{1})-F(x^{*}))/(T\\bar{\\alpha}) italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 2 ( italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) ) / ( italic_T over  start_ARG italic_ end_ARG ) ,  a 2 =     L   2 / ( S  B   ) subscript a 2    L superscript  2 S   B a_{2}=\\bar{\\alpha}L\\sigma^{2}/(S\\bar{B}) italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG italic_L italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / ( italic_S over  start_ARG italic_B end_ARG )  and  a 3 =    2   2  L 2  M / ( 3  B   ) subscript a 3 superscript    2 superscript  2 superscript L 2 M 3   B a_{3}=\\bar{\\alpha}^{2}\\sigma^{2}L^{2}M/(3\\bar{B}) italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT = over  start_ARG italic_ end_ARG start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_M / ( 3 over  start_ARG italic_B end_ARG ) . It follows that  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 1  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 1 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{1}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K ) . Inequality ( 4.29 ) implies  ( 1 /  )  a 1 > ( 3  1 /  )  a 2 + 6  a 3 1  subscript a 1 3 1  subscript a 2 6 subscript a 3 \\left(1/\\delta\\right)a_{1}>(3-1/\\delta)a_{2}+6a_{3} ( 1 / italic_ ) italic_a start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT > ( 3 - 1 / italic_ ) italic_a start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + 6 italic_a start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , which yields  Q 1  ( 2 ) < Q 1  ( 1 ) subscript Q 1 2 subscript Q 1 1 Q_{1}(2)<Q_{1}(1) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 2 ) < italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( 1 ) . Therefore, we have inequality  K  > 1 superscript K 1 K^{*}>1 italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT > 1 , which completes the proof.",
            "It is noted from Theorems  4.6  and  4.7  that the larger  F  ( x ~ 1 )  F  ( x  ) F subscript ~ x 1 F superscript x F(\\tilde{x}_{1})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , larger batch sizes, and smaller step sizes, make the conditions ( 4.8b ), ( 4.29 ) and ( 4.30 ) easier to be satisfied. Therefore, when the initial guess  x ~ 1 subscript ~ x 1 \\tilde{x}_{1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is far away from the minimizer  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , using a large  K K K italic_K  is reasonable.",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 )."
        ]
    },
    "id_table_30": {
        "caption": "",
        "table": "A5.EGx26",
        "footnotes": [],
        "references": [
            "Denote the right-hand side of the inequality above by  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) . Therefore,  K   arg  min K  { 1 , 2 , ... , K ~ }  Q 2  ( K ) superscript K subscript arg min K 1 2 ... ~ K subscript Q 2 K K^{*}\\in\\operatorname*{arg\\,min}_{K\\in\\{1,2,\\dots,\\tilde{K}\\}}Q_{2}(K) italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  start_OPERATOR roman_arg roman_min end_OPERATOR start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... , over~ start_ARG italic_K end_ARG } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K ) . A sufficient condition for  K  > 1 superscript K 1 K^{*}>1 italic_K start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT > 1  is  Q 2  ( 2 ) < Q 2  ( 1 ) subscript Q 2 2 subscript Q 2 1 Q_{2}(2)<Q_{2}(1) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 2 ) < italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( 1 ) , which is guaranteed by ( 4.30 ).",
            "It is noted from Theorems  4.6  and  4.7  that the larger  F  ( x ~ 1 )  F  ( x  ) F subscript ~ x 1 F superscript x F(\\tilde{x}_{1})-F(x^{*}) italic_F ( over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) - italic_F ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , larger batch sizes, and smaller step sizes, make the conditions ( 4.8b ), ( 4.29 ) and ( 4.30 ) easier to be satisfied. Therefore, when the initial guess  x ~ 1 subscript ~ x 1 \\tilde{x}_{1} over~ start_ARG italic_x end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  is far away from the minimizer  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , using a large  K K K italic_K  is reasonable.",
            "Inequalities ( 4.11 ) and ( 4.20 ) are guaranteed to hold for  K K K italic_K  smaller than an integer  K ~ > 1 ~ K 1 \\tilde{K}>1 over~ start_ARG italic_K end_ARG > 1 . For  K  K ~ K ~ K K\\geq\\tilde{K} italic_K  over~ start_ARG italic_K end_ARG , it is still an open question whether or not these two inequalities hold. Suppose that these two inequalities hold for any integer  K K K italic_K . Then one can still verify that the minimizers of  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  are finite and the conclusions in Theorem  4.6  and  4.7  hold. Specifically,  Q 1  ( K ) subscript Q 1 K Q_{1}(K) italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  Q 2  ( K ) subscript Q 2 K Q_{2}(K) italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  in the proofs of Theorems  4.6  and  4.7  go to   \\infty   as  K K K italic_K  goes to   \\infty  . Thus, the integer programmings  min K  { 1 , 2 , ... }  Q 1  ( K ) subscript K 1 2 ... subscript Q 1 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{1}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_K )  and  min K  { 1 , 2 , ... }  Q 2  ( K ) subscript K 1 2 ... subscript Q 2 K \\min_{K\\in\\{1,2,\\dots\\}}Q_{2}(K) roman_min start_POSTSUBSCRIPT italic_K  { 1 , 2 , ... } end_POSTSUBSCRIPT italic_Q start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_K )  have finite minimizers, which are also greater than  1 1 1 1  under Conditions ( 4.29 ) and ( 4.30 )."
        ]
    },
    "id_table_31": {
        "caption": "",
        "table": "S4.E11",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A5.EGx27",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "S4.E12",
        "footnotes": [],
        "references": []
    },
    "id_table_34": {
        "caption": "",
        "table": "A5.EGx28",
        "footnotes": [],
        "references": []
    },
    "id_table_35": {
        "caption": "",
        "table": "A5.EGx29",
        "footnotes": [],
        "references": []
    },
    "id_table_36": {
        "caption": "",
        "table": "A5.EGx30",
        "footnotes": [],
        "references": []
    },
    "id_table_37": {
        "caption": "",
        "table": "A5.EGx31",
        "footnotes": [],
        "references": []
    },
    "id_table_38": {
        "caption": "",
        "table": "A5.EGx32",
        "footnotes": [],
        "references": []
    },
    "id_table_39": {
        "caption": "",
        "table": "A5.EGx33",
        "footnotes": [],
        "references": []
    },
    "id_table_40": {
        "caption": "",
        "table": "A5.EGx34",
        "footnotes": [],
        "references": []
    },
    "id_table_41": {
        "caption": "",
        "table": "A5.EGx35",
        "footnotes": [],
        "references": []
    },
    "id_table_42": {
        "caption": "",
        "table": "A5.EGx36",
        "footnotes": [],
        "references": []
    },
    "id_table_43": {
        "caption": "",
        "table": "A5.EGx37",
        "footnotes": [],
        "references": []
    },
    "id_table_44": {
        "caption": "",
        "table": "A5.EGx38",
        "footnotes": [],
        "references": []
    },
    "id_table_45": {
        "caption": "",
        "table": "A5.EGx39",
        "footnotes": [],
        "references": []
    },
    "id_table_46": {
        "caption": "",
        "table": "S4.E20",
        "footnotes": [],
        "references": []
    },
    "id_table_47": {
        "caption": "",
        "table": "A5.EGx40",
        "footnotes": [],
        "references": []
    },
    "id_table_48": {
        "caption": "",
        "table": "A5.EGx41",
        "footnotes": [],
        "references": []
    },
    "id_table_49": {
        "caption": "",
        "table": "A5.EGx42",
        "footnotes": [],
        "references": []
    },
    "id_table_50": {
        "caption": "",
        "table": "A5.EGx43",
        "footnotes": [],
        "references": []
    },
    "id_table_51": {
        "caption": "",
        "table": "S4.E24",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_52": {
        "caption": "",
        "table": "A5.EGx44",
        "footnotes": [],
        "references": []
    },
    "id_table_53": {
        "caption": "",
        "table": "A5.EGx45",
        "footnotes": [],
        "references": []
    },
    "id_table_54": {
        "caption": "",
        "table": "S4.E27",
        "footnotes": [],
        "references": []
    },
    "id_table_55": {
        "caption": "",
        "table": "A5.EGx46",
        "footnotes": [],
        "references": []
    },
    "id_table_56": {
        "caption": "",
        "table": "A5.EGx47",
        "footnotes": [],
        "references": []
    },
    "id_table_57": {
        "caption": "",
        "table": "A5.EGx48",
        "footnotes": [],
        "references": []
    },
    "id_table_58": {
        "caption": "",
        "table": "A5.EGx49",
        "footnotes": [],
        "references": []
    },
    "id_table_59": {
        "caption": "",
        "table": "A5.EGx50",
        "footnotes": [],
        "references": []
    },
    "id_table_60": {
        "caption": "",
        "table": "A5.EGx51",
        "footnotes": [],
        "references": []
    },
    "id_table_61": {
        "caption": "",
        "table": "A5.EGx52",
        "footnotes": [],
        "references": []
    },
    "id_table_62": {
        "caption": "",
        "table": "A5.EGx53",
        "footnotes": [],
        "references": []
    },
    "id_table_63": {
        "caption": "",
        "table": "S5.T1.40",
        "footnotes": [],
        "references": []
    },
    "id_table_64": {
        "caption": "",
        "table": "S5.E3",
        "footnotes": [],
        "references": []
    },
    "id_table_65": {
        "caption": "",
        "table": "A5.EGx54",
        "footnotes": [],
        "references": []
    },
    "id_table_66": {
        "caption": "",
        "table": "S5.T2.39.31",
        "footnotes": [],
        "references": []
    },
    "id_table_67": {
        "caption": "",
        "table": "A5.EGx55",
        "footnotes": [],
        "references": []
    },
    "id_table_68": {
        "caption": "",
        "table": "A5.EGx56",
        "footnotes": [],
        "references": []
    },
    "id_table_69": {
        "caption": "",
        "table": "A5.EGx57",
        "footnotes": [],
        "references": []
    },
    "id_table_70": {
        "caption": "",
        "table": "A5.EGx58",
        "footnotes": [],
        "references": []
    },
    "id_table_71": {
        "caption": "",
        "table": "A5.EGx59",
        "footnotes": [],
        "references": []
    },
    "id_table_72": {
        "caption": "",
        "table": "A5.EGx60",
        "footnotes": [],
        "references": []
    },
    "id_table_73": {
        "caption": "",
        "table": "A5.EGx61",
        "footnotes": [],
        "references": []
    },
    "id_table_74": {
        "caption": "",
        "table": "A5.EGx62",
        "footnotes": [],
        "references": []
    },
    "id_table_75": {
        "caption": "",
        "table": "A5.EGx63",
        "footnotes": [],
        "references": []
    },
    "id_table_76": {
        "caption": "",
        "table": "A5.EGx64",
        "footnotes": [],
        "references": []
    },
    "id_table_77": {
        "caption": "",
        "table": "A5.EGx65",
        "footnotes": [],
        "references": []
    },
    "id_table_78": {
        "caption": "",
        "table": "A5.EGx66",
        "footnotes": [],
        "references": []
    },
    "id_table_79": {
        "caption": "",
        "table": "A5.EGx67",
        "footnotes": [],
        "references": []
    },
    "id_table_80": {
        "caption": "",
        "table": "A5.EGx68",
        "footnotes": [],
        "references": []
    },
    "id_table_81": {
        "caption": "",
        "table": "A5.EGx69",
        "footnotes": [],
        "references": []
    },
    "id_table_82": {
        "caption": "",
        "table": "A5.EGx70",
        "footnotes": [],
        "references": []
    },
    "id_table_83": {
        "caption": "",
        "table": "A5.EGx71",
        "footnotes": [],
        "references": []
    }
}