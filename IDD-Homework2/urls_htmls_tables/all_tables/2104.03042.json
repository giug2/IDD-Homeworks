{
    "PAPER'S NUMBER OF TABLES": 5,
    "S5.T1": {
        "caption": "Table 1: Android phones used from the AWS Device Farm",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_tt\"><span id=\"S5.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Device Name</span></th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Type</span></th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><span id=\"S5.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">OS Version</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Google Pixel 4</td>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Phone</td>\n<td id=\"S5.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10</td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Google Pixel 3</td>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">Phone</td>\n<td id=\"S5.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">10</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Google Pixel 2</td>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">Phone</td>\n<td id=\"S5.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">9</td>\n</tr>\n<tr id=\"S5.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Samsung Galaxy Tab S6</td>\n<td id=\"S5.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">Tablet</td>\n<td id=\"S5.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">9</td>\n</tr>\n<tr id=\"S5.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_l ltx_border_r\">Samsung Galaxy Tab S4</td>\n<td id=\"S5.T1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">Tablet</td>\n<td id=\"S5.T1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">8.1.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Deployment Setup. We run the Flower Server configured with the FedAvg strategy and host it on a cloud virtual machine. Two sets of edge devices are used in the evaluation: Android smartphones and Nvidia Jetson TX2 accelerators. To scale our experiments to a reasonably large number of Android clients with different OS versions, we deploy Flower Clients on the Amazon AWS Device Farm, which enables testing applications on real Android devices accessed through AWS. TableÂ 1 list the smartphones from AWS Device Farm used in our evaluation. Nvidia Jetson TX2 devices support full-fledged PyTorch â€“ this means we could successfully port existing PyTorch training pipelines to implement FL clients on them."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Flower supports implementation of FL clients on any device that has on-device training support. Here we show various FL experiments on Android and Nvidia Jetson devices.",
        "table": "<table id=\"S5.T2.st1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.st1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.st1.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">\n<table id=\"S5.T2.st1.3.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.st1.3.1.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Local</span></td>\n</tr>\n<tr id=\"S5.T2.st1.3.1.1.1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.1.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Epochs (E)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S5.T2.st1.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T2.st1.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></th>\n<th id=\"S5.T2.st1.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<table id=\"S5.T2.st1.3.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.st1.3.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Convergence</span></td>\n</tr>\n<tr id=\"S5.T2.st1.3.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.3.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Time (mins)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S5.T2.st1.3.1.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<table id=\"S5.T2.st1.3.1.1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.st1.3.1.1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Energy</span></td>\n</tr>\n<tr id=\"S5.T2.st1.3.1.1.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.4.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Consumed (kJ)</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.st1.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.st1.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S5.T2.st1.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.48</td>\n<td id=\"S5.T2.st1.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">17.63</td>\n<td id=\"S5.T2.st1.3.2.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">10.21</td>\n</tr>\n<tr id=\"S5.T2.st1.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.st1.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">5</th>\n<td id=\"S5.T2.st1.3.3.2.2\" class=\"ltx_td ltx_align_center\">0.64</td>\n<td id=\"S5.T2.st1.3.3.2.3\" class=\"ltx_td ltx_align_center\">36.83</td>\n<td id=\"S5.T2.st1.3.3.2.4\" class=\"ltx_td ltx_nopad_r ltx_align_center\">50.54</td>\n</tr>\n<tr id=\"S5.T2.st1.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.st1.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T2.st1.3.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.67</td>\n<td id=\"S5.T2.st1.3.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">80.32</td>\n<td id=\"S5.T2.st1.3.4.3.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">100.95</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "System Costs of FL. In TableÂ 2(b), we present various performance metrics obtained on Nvidia TX2 and Android devices. First, we train a ResNet-18 model on the CIFAR-10 dataset on 10 Nvidia TX2 clients. In TableÂ 2(a), we vary the number of local training epochs (Eğ¸E) performed on each client in a round of FL. Our results show that choosing a higher Eğ¸E results in better FL accuracy, however it also comes at the expense of significant increase in total training time and overall energy consumption across the clients. While the accuracy metrics in TableÂ 2(a) could have been obtained in a simulated setup, quantifying the training time and energy costs on real clients would not have been possible without a real on-device deployment. As reducing the energy and carbon footprint of training ML models is a major challenge for the community, Flower can assist researchers in choosing an optimal value of Eğ¸E to obtain the best trade-off between accuracy and energy consumption.",
            "Next, we train a 2-layer DNN classifier (Head Model) on top of a pre-trained MobileNetV2 Base Model on Android clients for the Office-31 dataset. In TableÂ 2(b), we vary the number of Android clients (Cğ¶C) participating in FL, while keeping the local training epochs (Eğ¸E) on each client fixed to 5. We observe that by increasing the number of clients, we can train a more accurate object recognition model. Intuitively, as more clients participate in the training, the model gets exposed to more diverse training examples, thereby increasing its generalizability to unseen test samples. However, this accuracy gain comes at the expense of high energy consumption â€“ the more clients we use, the higher the total energy consumption of FL. Again, based on this analysis obtained using Flower, researchers can choose an appropriate number of clients to find a balance between accuracy and energy consumption.",
            "For this experiment, we use Nvidia Jetson TX2 as the client device, which has one Pascal GPU and six CPU cores. We repeat the experiment shown in TableÂ 2(a), but instead of using the embedded GPU for training, we train the ResNet-18 model on a CPU. In TableÂ 3, we show that CPU training with local epochs E=10ğ¸10E=10 would take 1.27Ã—1.27\\times more time to obtain the same accuracy as the GPU training. This implies that even a single client device with low compute resources (e.g., a CPU) can become a bottleneck and significantly increase the FL training time."
        ]
    },
    "S5.T2.st1": {
        "caption": "(a) Performance metrics with Nvidia Jetson TX2 as we vary the number of local epochs. Number of clients Cğ¶C is set to 10 and the model is trained for 40 rounds.",
        "table": "<table id=\"S5.T2.st1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.st1.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.st1.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">\n<table id=\"S5.T2.st1.3.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.st1.3.1.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Local</span></td>\n</tr>\n<tr id=\"S5.T2.st1.3.1.1.1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.1.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Epochs (E)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S5.T2.st1.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T2.st1.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></th>\n<th id=\"S5.T2.st1.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<table id=\"S5.T2.st1.3.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.st1.3.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Convergence</span></td>\n</tr>\n<tr id=\"S5.T2.st1.3.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.3.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Time (mins)</span></td>\n</tr>\n</table>\n</th>\n<th id=\"S5.T2.st1.3.1.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<table id=\"S5.T2.st1.3.1.1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.st1.3.1.1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Energy</span></td>\n</tr>\n<tr id=\"S5.T2.st1.3.1.1.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.st1.3.1.1.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st1.3.1.1.4.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Consumed (kJ)</span></td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.st1.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.st1.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">1</th>\n<td id=\"S5.T2.st1.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.48</td>\n<td id=\"S5.T2.st1.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">17.63</td>\n<td id=\"S5.T2.st1.3.2.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">10.21</td>\n</tr>\n<tr id=\"S5.T2.st1.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.st1.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">5</th>\n<td id=\"S5.T2.st1.3.3.2.2\" class=\"ltx_td ltx_align_center\">0.64</td>\n<td id=\"S5.T2.st1.3.3.2.3\" class=\"ltx_td ltx_align_center\">36.83</td>\n<td id=\"S5.T2.st1.3.3.2.4\" class=\"ltx_td ltx_nopad_r ltx_align_center\">50.54</td>\n</tr>\n<tr id=\"S5.T2.st1.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.st1.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T2.st1.3.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.67</td>\n<td id=\"S5.T2.st1.3.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">80.32</td>\n<td id=\"S5.T2.st1.3.4.3.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">100.95</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In this paper, we present our exploration of on-device training of FL workloads on Android smartphones and Nvidia Jetson series embedded devices, using the Flower frameworkÂ Beutel etÂ al. (2020). Flower offers a stable implementation of the core components of an FL system, and provides higher-level abstractions to enable researchers to experiment and implement new ideas on top of a reliable stack. We first demonstrate how we use the language-, platform- and ML framework-agnostic capabilities of Flower to support on-device training of FL workloads on edge devices with heterogeneous hardware and software stacks. We then deploy these FL clients on various embedded devices as well as on Android smartphones hosted in the Amazon AWS Device Farm (https://aws.amazon.com/device-farm/). Finally, we present an evaluation to compute various system-related metrics of FL and highlight how this quantification could lead to the design of more efficient FL algorithms.",
            "Deployment Setup. We run the Flower Server configured with the FedAvg strategy and host it on a cloud virtual machine. Two sets of edge devices are used in the evaluation: Android smartphones and Nvidia Jetson TX2 accelerators. To scale our experiments to a reasonably large number of Android clients with different OS versions, we deploy Flower Clients on the Amazon AWS Device Farm, which enables testing applications on real Android devices accessed through AWS. TableÂ 1 list the smartphones from AWS Device Farm used in our evaluation. Nvidia Jetson TX2 devices support full-fledged PyTorch â€“ this means we could successfully port existing PyTorch training pipelines to implement FL clients on them.",
            "System Costs of FL. In TableÂ 2(b), we present various performance metrics obtained on Nvidia TX2 and Android devices. First, we train a ResNet-18 model on the CIFAR-10 dataset on 10 Nvidia TX2 clients. In TableÂ 2(a), we vary the number of local training epochs (Eğ¸E) performed on each client in a round of FL. Our results show that choosing a higher Eğ¸E results in better FL accuracy, however it also comes at the expense of significant increase in total training time and overall energy consumption across the clients. While the accuracy metrics in TableÂ 2(a) could have been obtained in a simulated setup, quantifying the training time and energy costs on real clients would not have been possible without a real on-device deployment. As reducing the energy and carbon footprint of training ML models is a major challenge for the community, Flower can assist researchers in choosing an optimal value of Eğ¸E to obtain the best trade-off between accuracy and energy consumption.",
            "Next, we train a 2-layer DNN classifier (Head Model) on top of a pre-trained MobileNetV2 Base Model on Android clients for the Office-31 dataset. In TableÂ 2(b), we vary the number of Android clients (Cğ¶C) participating in FL, while keeping the local training epochs (Eğ¸E) on each client fixed to 5. We observe that by increasing the number of clients, we can train a more accurate object recognition model. Intuitively, as more clients participate in the training, the model gets exposed to more diverse training examples, thereby increasing its generalizability to unseen test samples. However, this accuracy gain comes at the expense of high energy consumption â€“ the more clients we use, the higher the total energy consumption of FL. Again, based on this analysis obtained using Flower, researchers can choose an appropriate number of clients to find a balance between accuracy and energy consumption.",
            "For this experiment, we use Nvidia Jetson TX2 as the client device, which has one Pascal GPU and six CPU cores. We repeat the experiment shown in TableÂ 2(a), but instead of using the embedded GPU for training, we train the ResNet-18 model on a CPU. In TableÂ 3, we show that CPU training with local epochs E=10ğ¸10E=10 would take 1.27Ã—1.27\\times more time to obtain the same accuracy as the GPU training. This implies that even a single client device with low compute resources (e.g., a CPU) can become a bottleneck and significantly increase the FL training time.",
            "Once we obtain this quantification of computational heterogeneity, we can design better federated optimization algorithms. As an example, we implement a modified version of FedAvg where each client device is assigned a cutoff time (Ï„ğœ\\tau) after which it must send its model parameters to the server, irrespective of whether it has finished its local epochs or not. This strategy has parallels with the FedProx algorithm Li etÂ al. (2018) which also accepts partial results from clients. However, the key advantage of using Flower is that we can compute and assign a processor-specific cutoff time for each client. For example, on average it takes 1.99 minutes to complete an FL round on the TX2 GPU. If we set the same time as a cutoff for CPU clients (Ï„=1.99ğœ1.99\\tau=1.99 mins) as shown in TableÂ 3, we obtain the same convergence time as GPU, at the expense of 3% accuracy drop. With Ï„=2.23ğœ2.23\\tau=2.23, a better balance between accuracy and training time could be obtained on a CPU."
        ]
    },
    "S5.T2.st2": {
        "caption": "(b) Performance metrics with Android clients as we vary the number of clients. Local epochs Eğ¸E is fixed to 5 in this experiment and the model is trained for 20 rounds.",
        "table": "<table id=\"S5.T2.st2.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.st2.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.st2.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\">\n<table id=\"S5.T2.st2.3.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.st2.3.1.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.st2.3.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st2.3.1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Number</span></td>\n</tr>\n<tr id=\"S5.T2.st2.3.1.1.1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.st2.3.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st2.3.1.1.1.1.2.1.1\" class=\"ltx_text ltx_font_bold\">of Clients (C)</span></td>\n</tr>\n</table>\n</th>\n<td id=\"S5.T2.st2.3.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S5.T2.st2.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></td>\n<td id=\"S5.T2.st2.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">\n<table id=\"S5.T2.st2.3.1.1.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.st2.3.1.1.3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.st2.3.1.1.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st2.3.1.1.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Convergence</span></td>\n</tr>\n<tr id=\"S5.T2.st2.3.1.1.3.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.st2.3.1.1.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st2.3.1.1.3.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Time (mins)</span></td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T2.st2.3.1.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\">\n<table id=\"S5.T2.st2.3.1.1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T2.st2.3.1.1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.st2.3.1.1.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st2.3.1.1.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Energy</span></td>\n</tr>\n<tr id=\"S5.T2.st2.3.1.1.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.st2.3.1.1.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T2.st2.3.1.1.4.1.2.1.1\" class=\"ltx_text ltx_font_bold\">Consumed (kJ)</span></td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"S5.T2.st2.3.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.st2.3.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">4</th>\n<td id=\"S5.T2.st2.3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.84</td>\n<td id=\"S5.T2.st2.3.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">30.7</td>\n<td id=\"S5.T2.st2.3.2.2.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">10.4</td>\n</tr>\n<tr id=\"S5.T2.st2.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.st2.3.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">7</th>\n<td id=\"S5.T2.st2.3.3.3.2\" class=\"ltx_td ltx_align_center\">0.85</td>\n<td id=\"S5.T2.st2.3.3.3.3\" class=\"ltx_td ltx_align_center\">31.3</td>\n<td id=\"S5.T2.st2.3.3.3.4\" class=\"ltx_td ltx_nopad_r ltx_align_center\">19.72</td>\n</tr>\n<tr id=\"S5.T2.st2.3.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.st2.3.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T2.st2.3.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.87</td>\n<td id=\"S5.T2.st2.3.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">31.8</td>\n<td id=\"S5.T2.st2.3.4.4.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">28.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In this paper, we present our exploration of on-device training of FL workloads on Android smartphones and Nvidia Jetson series embedded devices, using the Flower frameworkÂ Beutel etÂ al. (2020). Flower offers a stable implementation of the core components of an FL system, and provides higher-level abstractions to enable researchers to experiment and implement new ideas on top of a reliable stack. We first demonstrate how we use the language-, platform- and ML framework-agnostic capabilities of Flower to support on-device training of FL workloads on edge devices with heterogeneous hardware and software stacks. We then deploy these FL clients on various embedded devices as well as on Android smartphones hosted in the Amazon AWS Device Farm (https://aws.amazon.com/device-farm/). Finally, we present an evaluation to compute various system-related metrics of FL and highlight how this quantification could lead to the design of more efficient FL algorithms.",
            "Deployment Setup. We run the Flower Server configured with the FedAvg strategy and host it on a cloud virtual machine. Two sets of edge devices are used in the evaluation: Android smartphones and Nvidia Jetson TX2 accelerators. To scale our experiments to a reasonably large number of Android clients with different OS versions, we deploy Flower Clients on the Amazon AWS Device Farm, which enables testing applications on real Android devices accessed through AWS. TableÂ 1 list the smartphones from AWS Device Farm used in our evaluation. Nvidia Jetson TX2 devices support full-fledged PyTorch â€“ this means we could successfully port existing PyTorch training pipelines to implement FL clients on them.",
            "System Costs of FL. In TableÂ 2(b), we present various performance metrics obtained on Nvidia TX2 and Android devices. First, we train a ResNet-18 model on the CIFAR-10 dataset on 10 Nvidia TX2 clients. In TableÂ 2(a), we vary the number of local training epochs (Eğ¸E) performed on each client in a round of FL. Our results show that choosing a higher Eğ¸E results in better FL accuracy, however it also comes at the expense of significant increase in total training time and overall energy consumption across the clients. While the accuracy metrics in TableÂ 2(a) could have been obtained in a simulated setup, quantifying the training time and energy costs on real clients would not have been possible without a real on-device deployment. As reducing the energy and carbon footprint of training ML models is a major challenge for the community, Flower can assist researchers in choosing an optimal value of Eğ¸E to obtain the best trade-off between accuracy and energy consumption.",
            "Next, we train a 2-layer DNN classifier (Head Model) on top of a pre-trained MobileNetV2 Base Model on Android clients for the Office-31 dataset. In TableÂ 2(b), we vary the number of Android clients (Cğ¶C) participating in FL, while keeping the local training epochs (Eğ¸E) on each client fixed to 5. We observe that by increasing the number of clients, we can train a more accurate object recognition model. Intuitively, as more clients participate in the training, the model gets exposed to more diverse training examples, thereby increasing its generalizability to unseen test samples. However, this accuracy gain comes at the expense of high energy consumption â€“ the more clients we use, the higher the total energy consumption of FL. Again, based on this analysis obtained using Flower, researchers can choose an appropriate number of clients to find a balance between accuracy and energy consumption.",
            "For this experiment, we use Nvidia Jetson TX2 as the client device, which has one Pascal GPU and six CPU cores. We repeat the experiment shown in TableÂ 2(a), but instead of using the embedded GPU for training, we train the ResNet-18 model on a CPU. In TableÂ 3, we show that CPU training with local epochs E=10ğ¸10E=10 would take 1.27Ã—1.27\\times more time to obtain the same accuracy as the GPU training. This implies that even a single client device with low compute resources (e.g., a CPU) can become a bottleneck and significantly increase the FL training time.",
            "Once we obtain this quantification of computational heterogeneity, we can design better federated optimization algorithms. As an example, we implement a modified version of FedAvg where each client device is assigned a cutoff time (Ï„ğœ\\tau) after which it must send its model parameters to the server, irrespective of whether it has finished its local epochs or not. This strategy has parallels with the FedProx algorithm Li etÂ al. (2018) which also accepts partial results from clients. However, the key advantage of using Flower is that we can compute and assign a processor-specific cutoff time for each client. For example, on average it takes 1.99 minutes to complete an FL round on the TX2 GPU. If we set the same time as a cutoff for CPU clients (Ï„=1.99ğœ1.99\\tau=1.99 mins) as shown in TableÂ 3, we obtain the same convergence time as GPU, at the expense of 3% accuracy drop. With Ï„=2.23ğœ2.23\\tau=2.23, a better balance between accuracy and training time could be obtained on a CPU."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Effect of computational heterogeneity on FL training times. Using Flower, we can compute a hardware-specific cutoff Ï„ğœ\\tau (in minutes) for each processor, and find a balance between FL accuracy and training time. Ï„=0ğœ0\\tau=0 indicates no cutoff time.",
        "table": "<table id=\"S5.T3.8\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.8.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.4.5\" class=\"ltx_td ltx_border_tt\"></td>\n<th id=\"S5.T3.5.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">\n<table id=\"S5.T3.5.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.5.1.1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.5.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.5.1.1.1.2.1.1\" class=\"ltx_text ltx_font_bold\">GPU</span></td>\n</tr>\n<tr id=\"S5.T3.5.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.5.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span id=\"S5.T3.5.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">(</span><math id=\"S5.T3.5.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\tau\" display=\"inline\"><semantics id=\"S5.T3.5.1.1.1.1.1.m1.1a\"><mi id=\"S5.T3.5.1.1.1.1.1.m1.1.1\" xref=\"S5.T3.5.1.1.1.1.1.m1.1.1.cmml\">Ï„</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.5.1.1.1.1.1.m1.1b\"><ci id=\"S5.T3.5.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.5.1.1.1.1.1.m1.1.1\">ğœ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.5.1.1.1.1.1.m1.1c\">\\tau</annotation></semantics></math><span id=\"S5.T3.5.1.1.1.1.1.2\" class=\"ltx_text ltx_font_bold\"> = 0)</span>\n</td>\n</tr>\n</table>\n</th>\n<th id=\"S5.T3.6.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<table id=\"S5.T3.6.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.6.2.2.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"S5.T3.6.2.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">CPU</span></td>\n</tr>\n<tr id=\"S5.T3.6.2.2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">\n<span id=\"S5.T3.6.2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">(</span><math id=\"S5.T3.6.2.2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\tau\" display=\"inline\"><semantics id=\"S5.T3.6.2.2.1.1.1.m1.1a\"><mi id=\"S5.T3.6.2.2.1.1.1.m1.1.1\" xref=\"S5.T3.6.2.2.1.1.1.m1.1.1.cmml\">Ï„</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.6.2.2.1.1.1.m1.1b\"><ci id=\"S5.T3.6.2.2.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.6.2.2.1.1.1.m1.1.1\">ğœ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.6.2.2.1.1.1.m1.1c\">\\tau</annotation></semantics></math><span id=\"S5.T3.6.2.2.1.1.1.2\" class=\"ltx_text ltx_font_bold\"> = 0)</span>\n</td>\n</tr>\n</table>\n</th>\n<th id=\"S5.T3.7.3.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">\n<table id=\"S5.T3.7.3.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.7.3.3.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.7.3.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T3.7.3.3.1.2.1.1\" class=\"ltx_text ltx_font_bold\">CPU</span></td>\n</tr>\n<tr id=\"S5.T3.7.3.3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.7.3.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">\n<span id=\"S5.T3.7.3.3.1.1.1.1\" class=\"ltx_text ltx_font_bold\">(</span><math id=\"S5.T3.7.3.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\tau\" display=\"inline\"><semantics id=\"S5.T3.7.3.3.1.1.1.m1.1a\"><mi id=\"S5.T3.7.3.3.1.1.1.m1.1.1\" xref=\"S5.T3.7.3.3.1.1.1.m1.1.1.cmml\">Ï„</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.7.3.3.1.1.1.m1.1b\"><ci id=\"S5.T3.7.3.3.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.7.3.3.1.1.1.m1.1.1\">ğœ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.7.3.3.1.1.1.m1.1c\">\\tau</annotation></semantics></math><span id=\"S5.T3.7.3.3.1.1.1.2\" class=\"ltx_text ltx_font_bold\"> = 2.23)</span>\n</td>\n</tr>\n</table>\n</th>\n<th id=\"S5.T3.8.4.4\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\">\n<table id=\"S5.T3.8.4.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.8.4.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.4.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"S5.T3.8.4.4.1.2.1.1\" class=\"ltx_text ltx_font_bold\">CPU</span></td>\n</tr>\n<tr id=\"S5.T3.8.4.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.4.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">\n<span id=\"S5.T3.8.4.4.1.1.1.1\" class=\"ltx_text ltx_font_bold\">(</span><math id=\"S5.T3.8.4.4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\tau\" display=\"inline\"><semantics id=\"S5.T3.8.4.4.1.1.1.m1.1a\"><mi id=\"S5.T3.8.4.4.1.1.1.m1.1.1\" xref=\"S5.T3.8.4.4.1.1.1.m1.1.1.cmml\">Ï„</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.8.4.4.1.1.1.m1.1b\"><ci id=\"S5.T3.8.4.4.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.8.4.4.1.1.1.m1.1.1\">ğœ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.8.4.4.1.1.1.m1.1c\">\\tau</annotation></semantics></math><span id=\"S5.T3.8.4.4.1.1.1.2\" class=\"ltx_text ltx_font_bold\"> = 1.99)</span>\n</td>\n</tr>\n</table>\n</th>\n</tr>\n<tr id=\"S5.T3.8.5.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.5.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Accuracy</td>\n<td id=\"S5.T3.8.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.67</td>\n<td id=\"S5.T3.8.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.67</td>\n<td id=\"S5.T3.8.5.1.4\" class=\"ltx_td ltx_align_left ltx_border_t\">0.66</td>\n<td id=\"S5.T3.8.5.1.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">0.63</td>\n</tr>\n<tr id=\"S5.T3.8.6.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.6.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<table id=\"S5.T3.8.6.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.8.6.2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.6.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Training</td>\n</tr>\n<tr id=\"S5.T3.8.6.2.1.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.6.2.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">time (mins)</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T3.8.6.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">80.32</td>\n<td id=\"S5.T3.8.6.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">\n<table id=\"S5.T3.8.6.2.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.8.6.2.3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.6.2.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">102</td>\n</tr>\n<tr id=\"S5.T3.8.6.2.3.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.6.2.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(1.27x)</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T3.8.6.2.4\" class=\"ltx_td ltx_align_left ltx_border_bb\">\n<table id=\"S5.T3.8.6.2.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.8.6.2.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.6.2.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">89.15</td>\n</tr>\n<tr id=\"S5.T3.8.6.2.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.6.2.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(1.11x)</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T3.8.6.2.5\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb\">\n<table id=\"S5.T3.8.6.2.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T3.8.6.2.5.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.6.2.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">80.34</td>\n</tr>\n<tr id=\"S5.T3.8.6.2.5.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.8.6.2.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(1.0x)</td>\n</tr>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "For this experiment, we use Nvidia Jetson TX2 as the client device, which has one Pascal GPU and six CPU cores. We repeat the experiment shown in TableÂ 2(a), but instead of using the embedded GPU for training, we train the ResNet-18 model on a CPU. In TableÂ 3, we show that CPU training with local epochs E=10ğ¸10E=10 would take 1.27Ã—1.27\\times more time to obtain the same accuracy as the GPU training. This implies that even a single client device with low compute resources (e.g., a CPU) can become a bottleneck and significantly increase the FL training time.",
            "Once we obtain this quantification of computational heterogeneity, we can design better federated optimization algorithms. As an example, we implement a modified version of FedAvg where each client device is assigned a cutoff time (Ï„ğœ\\tau) after which it must send its model parameters to the server, irrespective of whether it has finished its local epochs or not. This strategy has parallels with the FedProx algorithm Li etÂ al. (2018) which also accepts partial results from clients. However, the key advantage of using Flower is that we can compute and assign a processor-specific cutoff time for each client. For example, on average it takes 1.99 minutes to complete an FL round on the TX2 GPU. If we set the same time as a cutoff for CPU clients (Ï„=1.99ğœ1.99\\tau=1.99 mins) as shown in TableÂ 3, we obtain the same convergence time as GPU, at the expense of 3% accuracy drop. With Ï„=2.23ğœ2.23\\tau=2.23, a better balance between accuracy and training time could be obtained on a CPU."
        ]
    }
}