{
    "id_table_1": {
        "caption": "Table 1 :  Example of original and processed data.  The original format includes detailed patient background, symptoms, diagnosis, history, plan of action, and dialogue. The processed format extracts essential patient known knowledge and reformulates doctor question statements for clarity and consistency.",
        "table": "S3.T1.5",
        "footnotes": [],
        "references": [
            "Figure  1  illustrates our system framework. Original medical notes are split into train and test sets. The train set builds a vectorized knowledge base. The test set, parsed by an LLM parser, produces ground truth pairs. The LLM healthcare chain generates questions using additional tools. A virtual patient simulator provides partial patient statements and answers. The generated questions are evaluated by an LLM judge based on various metrics. Finally, answers are assessed using summary-based metrics like NER2NER and ROUGE.",
            "HealthQ necessitates a general data processing mechanism due to the unstructured nature of medical notes and the often ambiguous doctor-patient conversations. Patients may provide simple yes or no answers, leaving the context or symptoms embedded in the doctors questions. To accurately evaluate the questioning capabilities of LLM healthcare chains, we require a customized data format that aligns with our evaluation framework. This involves clearly separating patient known knowledgewhich includes the patients feelings, self-reported symptoms, and background historyfrom the doctors question statements. These pairs are essential for our virtual patient mechanism, enabling the assessment of how effectively doctors ask questions to gather useful information for diagnosis. To illustrate how the dataset format changed through our processing, Table  1  shows an example of the original and processed data formats.",
            "As depicted in Figure  1 , we split the original medical notes into training and test sets. The training set is transformed and stored in a vector database (VDB), which the LLM Chain can use for searching and retrieval. However, the LLM Chain has no knowledge of any new patients in the test set. The test set is parsed by the LLM to simulate virtual patients, allowing us to evaluate how effectively these LLM healthcare chains, acting as doctors, can ask relevant questions and make judgments based on the simulated patient responses."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  LLM Judge Interrogation Metrics (Scores range from 0 to 10)",
        "table": "S3.T2.4",
        "footnotes": [],
        "references": [
            "LLM Judge Interrogation:  This metric evaluates the questions generated by the LLM healthcare chains based on specificity, usefulness, relevance, coverage, and fluency, as shown in Table  2 . An LLM judge  J J J italic_J  assesses each question  Q Q Q italic_Q  using the patient knowledge  K K K italic_K , providing scores according to these criteria. The interrogation metrics are calculated as follows:",
            "Evaluate the Question ( Q Q Q italic_Q ) using Judge ( J  ( Q , K ) J Q K J(Q,K) italic_J ( italic_Q , italic_K ) ) :  Evaluate the generated question using an LLM judge based on criteria such as relevance, specificity, and informativeness, as detailed in Table  2 .",
            "Individual Data Point Analysis:  This analysis examines the relationship between the interrogation score for each individual question generated by the LLM healthcare chains and the informativeness of the corresponding response from the virtual patient. Specifically, it assesses whether a high score in interrogation metrics (see Table  2 ) for a single question leads to a more comprehensive and informative answer for that specific interaction.",
            "Figure  2  presents the kernel density estimation (KDE) and radar plots comparing the distributions of interrogation metrics across LLM healthcare chains, providing further insights into  RQ2 . The KDE plots reveal that advanced chains, such as RAG and RAG with reflection, exhibit broader and higher-density distributions in key metrics like specificity, relevance, and usefulness. This reflects their adaptive questioning capabilities and iterative reasoning processes. Chains incorporating reflection and CoT techniques show distinct improvements, particularly in relevance and specificity, due to their ability to refine questions based on previous outputs. The radar plot consolidates these findings, with self-consistency-enhanced chains (RAG_reflection_CoT_SC) displaying more balanced performance across all metrics. In contrast, the hardcoded chain has narrow distributions, indicating rigid question generation, while the ReAct chain struggles with fluency and coverage, possibly due to limitations in its toolset. These observations support the idea that well-designed chains, especially those employing reflection and CoT methods, are more effective in generating questions that lead to informative patient responses, aligning with the goals outlined in  RQ2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Comparison of Summary-Based Metrics across LLM Chains",
        "table": "S3.T3.st1.4",
        "footnotes": [],
        "references": [
            "The summary-based metrics presented in Table  3(b)  are particularly relevant to  RQ1 , which assesses how well the induced answers summarize all relevant information. The results show that state-of-the-art chains like RAG_reflection and RAG_Reflection_CoT_SC consistently outperform more primitive methods, such as the hardcoded chain, across nearly all evaluated metrics. This is especially evident in the ROUGE and NER2NER metrics, where these advanced chains demonstrate superior performance. For instance, the RAG_reflection_CoT_SC chain achieves the highest overall scores in several categories, reflecting its ability to generate contextually relevant and informative questions that lead to more comprehensive and accurate patient responses. The lower performance of the ReAct chain could be attributed to its design in LangChain, emphasizing answering rather than questioning, along with the limited number of tools provided to it. These findings suggest that well-formulated questions by the more advanced chains lead to better information extraction, thereby validating the effectiveness of our evaluation framework in summarizing all relevant information, as addressed in  RQ1 .",
            "Figures  3(a)  and  3(b)  offer deeper insights into the relationship between question quality and answer quality using normalized mutual information (MI) matrices. In Figure  3(a) , MI is computed between two metrics,  J m subscript J m J_{m} italic_J start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  and  J n subscript J n J_{n} italic_J start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , for each individual interaction:",
            "where  p p p italic_p  indexes individual interactions or patient cases. This figure shows strong dependence between metrics such as Usefulness and Relevance ( M  I = 0.50 M I 0.50 MI=0.50 italic_M italic_I = 0.50 ), reflecting their shared focus on capturing relevant aspects of the patients condition. In contrast, pairs with less dependency such as those between Fluency and Coverage (MI = 0.38) indicate weaker relevance. In this case, fluency does not necessarily imply the completeness of aspects covered in the question. In Figure  3(b) , MI is calculated based on aggregated scores across all interactions within each LLM chain type:"
        ]
    },
    "id_table_4": {
        "caption": "(a)   Information Extraction Quality - NER2NER",
        "table": "S3.T3.st2.4",
        "footnotes": [],
        "references": [
            "The results of these analyses, presented through correlation and mutual information matrices in Section  4 , provide insights into how well-formulated questions can enhance patient interactions and improve healthcare outcomes.",
            "To ensure the feasibility of our approach for local implementations in clinics and hospitals, all components of the LLM health chains are based on open-source models. The primary LLM used is the Mixtral 8 8 8 https://huggingface.co/mistralai/Mixtral-8x7B-v0.1  8X7b, served on Groq 9 9 9 https://wow.groq.com/ . On branches of Chain of Thought Self-Consistency (CoT-SC) processes, a temperature of 0.3 is used, while a temperature of 0 is applied for all other models. The NER component utilizes an open-source biomedical NER model  [ 35 ]  to extract medical entities to make set comparisons. We employ FAISS  [ 36 ]  with its default settings as the VDB index package, and the MiniLM 10 10 10 https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2  model for text embeddings. Data preprocessingincluding the extraction of patient known knowledge ( K K K italic_K ), virtual patient simulation, and question evaluation by the LLM judgeis conducted using Claude-3-Opus 11 11 11 https://claude.ai/ . The experiments are conducted on a dataset comprising 128 test cases (64 from each of the two datasets). We benchmark seven different LLM health chains, generating a total of 796 patient status-doctor statement pairs. For comparative analysis, we group the data by the LLM chain type and summarize the mean scores in Table  4 .",
            "For  RQ2 , which examines the correlation between the quality of questions generated by LLM chains and the resulting answers, the LLM judge interrogation results in Table  4  show that most LLM healthcare chains substantially outperform the hardcoded workflow across all interrogation metrics. The basic RAG chain shows notable improvements in specificity, usefulness, relevance, and fluency compared to the hardcoded method, highlighting its more flexible and adaptive approach to generating questions. Chains that incorporate reflection and iterative reasoning techniques, such as those using reflection and CoT, exhibit further enhancements, particularly in relevance and coverage. The reflection mechanism enables the chains to reassess and refine the context of the questions, leading to more targeted and contextually appropriate queries. The addition of self-consistency checks further boosts performance, with the RAG_CoT_SC chain showing the highest scores in relevance and fluency. This improvement is attributed to the iterative reasoning and self-consistency processes, which help in formulating more coherent and relevant questions. The reaction-based chain, while showing some improvements over the hardcoded method, still lags behind the more advanced RAG variants, particularly in specificity and coverage. This analysis demonstrates that chains performing well in interrogation metrics tend to produce better quality answers, thus validating the connection explored in  RQ2 ."
        ]
    },
    "id_table_5": {
        "caption": "(b)   Summarization Performance - ROUGE",
        "table": "S3.T4.4",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "Ziyu Wang and Hao Li contributed equally to this work.",
        "Ziyu Wang and Hao Li contributed equally to this work.",
        "Corresponding author."
    ]
}