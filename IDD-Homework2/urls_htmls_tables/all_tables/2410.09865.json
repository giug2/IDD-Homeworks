{
    "id_table_1": {
        "caption": "Table 1:  Ours vs. shows the proportion of users who prefer our method over the alternative. An MPS above 1.00 and results above 50% in the user study indicate our method outplays the counterpart. FS, FER Acc., FAU Acc., EA and FF denote FaceScore  Liao et al. ( 2024 ) , FER accuracy, facial action unit accuracy, expression alignment and face fidelity, respectively.",
        "table": "S5.T1.8.8",
        "footnotes": [
            ""
        ],
        "references": [
            "We investigate the effectiveness of the synthetic data across various learning paradigms, demonstrating consistent and modest improvement in model performance. As shown in Fig.  1 (c), training with the synthetic data yields significant performance boosts across various learning paradigms. Notably, pre-training on the synthetic data (Fig.  1 (b)) with MoCo v3  Chen et al. ( 2021 )  yields a significant performance boost of +2.90% on AffectNet, surpassing real-world data pre-training. In supervised learning, SynFER improves accuracy by +1.55% for the state-of-the-art FER model, POSTER++  Mao et al. ( 2024 ) , on AffectNet. We further explore performance scaling of the synthetic data, revealing further gains as dataset size increases.",
            "We begin by introducing i) the overall synthetic pipeline for generating facial expression image-label pairs. Next, we detail ii) our approach for producing high-fidelity facial expression images, which are controlled through high-level text descriptions (Sec. 4.2.1 ), fine-grained facial action units corresponding to localized facial muscles (Sec. 4.2.3 ), and a semantic guidance technique (Sec. 4.3 ). Finally, we introduce iii) the FER annotation crafter (FERAnno), a crucial component that thoroughly understands the synthetic facial expression data and automatically generates accurate annotations accordingly(Sec. 4.4 ). This pipeline ensures both precision and reliability in facial expression generation and labeling.",
            "Layout Initialization.  During inference, we select a random face image  x s superscript x s x^{s} italic_x start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT  from FEText and invert it to initialize the noise sample  x T s superscript subscript x T s x_{T}^{s} italic_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT (Eq.  1 ). Since early diffusion stages shape the global layout of the image  Zhang et al. ( 2023 ); Pan et al. ( 2023 ); Mao et al. ( 2023 ) , this strategy helps preserve the natural facial structure, ensuring the generated images are coherent, high-quality, and visually consistent with real-world expressions.",
            "Image Inversion.  To extract facial features and cross-attention maps with the diffusion model    subscript italic-  \\epsilon_{\\theta} italic_ start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT , we first inverse the generated image  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  back to the noise sample  x t subscript x t x_{t} italic_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  at a denoising timestep  t t t italic_t , following a predefined scheduler, as described in Eq.  1 . To preserve facial details, we set  t = 1 t 1 t=1 italic_t = 1  during the inversion process, ensuring that the facial features remain as close as possible to the original generated image  x 0 subscript x 0 x_{0} italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . This partially denoised sample is then passed through the trained denoising network, allowing us to extract rich facial features and cross-attention maps from intermediate layers, which are critical for capturing detailed facial patterns.",
            "We conduct extensive experiments to evaluate both the generation quality of our synthetic data (Sec.  5.1 ) and its effectiveness in FER tasks (Sec.  5.2 ). For more details on experimental setup, implementation details are provided in the appendix.",
            "We compute FID between the synthesis images and the test set of the AffectNet  Mollahosseini et al. ( 2017 ) . HPSv2  Wu et al. ( 2023b )  and MPS  Zhang et al. ( 2024a )  evaluate the human preferences of the overall synthesis images, while FaceScore  Liao et al. ( 2024 )  measures the quality of the generated faces. Tab.  1  shows that our method outperforms popular diffusion models and the SOTA facial expression generation method FineFace  Varanka et al. ( 2024 ) , across all metrics of image quality, human preference and facial expression accuracy. Notably, the advantages of SynFER in both FE Acc. and AU Acc. indicate its outstanding controllability in facial expression generation.",
            "As shown in Fig.  10 , we provide comparisons between the over-smoothing synthetic images and the more natural ones. Due to the large amount of super-resolution data in FEText, it can be seen that solely performing fine-tuning on the entire FEText significantly degrades the realism of the images, while the proposed two-stage fine-tuning strategies in Sec.  4.2.2  could effectively prevent over-smoothing.",
            "More examples from FEText are shown in Fig.  11 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Linear probe performance comparisons of SSL models on three FER datasets.",
        "table": "S5.T3.fig1.1.1",
        "footnotes": [],
        "references": [
            "We begin by introducing i) the overall synthetic pipeline for generating facial expression image-label pairs. Next, we detail ii) our approach for producing high-fidelity facial expression images, which are controlled through high-level text descriptions (Sec. 4.2.1 ), fine-grained facial action units corresponding to localized facial muscles (Sec. 4.2.3 ), and a semantic guidance technique (Sec. 4.3 ). Finally, we introduce iii) the FER annotation crafter (FERAnno), a crucial component that thoroughly understands the synthetic facial expression data and automatically generates accurate annotations accordingly(Sec. 4.4 ). This pipeline ensures both precision and reliability in facial expression generation and labeling.",
            "We introduce the overall pipeline for FER data synthesis (Fig.  2 ). The process starts with a coarse human portrait description assigned to a specific facial expression. ChatGPT enriches this description with details such as facial appearance, subtle facial muscle movements, and contextual cues. Simultaneously, facial action unit annotations are generated based on prior FAU-FE knowledge  Ekman & Friesen ( 1978 ) , aligning them with emotion categories to serve as explicit control signals for guiding the facial expression image synthesis. Once the facial expression label, facial action unit labels, and expanded textual prompt are prepared, these inputs condition our diffusion model to generate high-fidelity FER images, guided by semantic guidance to ensure accurate FER semantic. During the denoising process, FERAnno automatically produces pseudo labels for the generated images. To further improve labeling accuracy, we ensemble our FERAnno with existing FER models, which collaborate to vote on the accuracy of the predefined FER labels. In cases where discrepancies arise, the predefined label is refined by averaging the predictions from the ensemble experts. This mechanism effectively reduces the risk of inconsistent or uncertain annotations, ensuring that the final synthesis data is precise and dependable for downstream applications.",
            "To facilitate our diffusion model to generate high-fidelity facial expressions, a straightforward approach is to fine-tune the model directly on the proposed FEText using the diffusion loss in Eq.   3 . However, since FEText contains images processed through a super-resolution model, this direct fine-tuning strategy may lead to over-smoothing in the generated images. To address this, we introduce a two-stage fine-tuning paradigm. In the first stage, the diffusion model is trained on the entire FEText dataset to capture facial expression-related semantics. Then, the second stage mitigates over-smoothing by specifically fine-tuning our diffusion model on the CelebA-HQ and FFHQ subsets of FEText, which consist of native high-resolution images. This two-step approach ensures that our model learns expressive facial details while preserving image sharpness. The fine-tuned model then serves as the foundation for controllable facial expression generation, incorporating facial action unit injection (Sec.  4.2.3 ) and semantic guidance (Sec.  4.3 ).",
            "We conduct extensive experiments to evaluate both the generation quality of our synthetic data (Sec.  5.1 ) and its effectiveness in FER tasks (Sec.  5.2 ). For more details on experimental setup, implementation details are provided in the appendix.",
            "As shown in Fig.  10 , we provide comparisons between the over-smoothing synthetic images and the more natural ones. Due to the large amount of super-resolution data in FEText, it can be seen that solely performing fine-tuning on the entire FEText significantly degrades the realism of the images, while the proposed two-stage fine-tuning strategies in Sec.  4.2.2  could effectively prevent over-smoothing."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparison of supervised learning models (with and without our synthetic data) and the label calibrator FERAnno.",
        "table": "S5.T3.fig2.1.1",
        "footnotes": [],
        "references": [
            "We begin by introducing i) the overall synthetic pipeline for generating facial expression image-label pairs. Next, we detail ii) our approach for producing high-fidelity facial expression images, which are controlled through high-level text descriptions (Sec. 4.2.1 ), fine-grained facial action units corresponding to localized facial muscles (Sec. 4.2.3 ), and a semantic guidance technique (Sec. 4.3 ). Finally, we introduce iii) the FER annotation crafter (FERAnno), a crucial component that thoroughly understands the synthetic facial expression data and automatically generates accurate annotations accordingly(Sec. 4.4 ). This pipeline ensures both precision and reliability in facial expression generation and labeling.",
            "To address the lack of facial expression image-text pairs for diffusion model training, we introduce FEText (Fig.  3 ), the first hybrid image-text dataset for FER. It combines face images from FFHQ  Karras et al. ( 2019 ) , CelebA-HQ  Karras et al. ( 2017 ) , AffectNet  Mollahosseini et al. ( 2017 )  and SFEW  Dhall et al. ( 2011 ) , each paired with captions generated by a multi-modal large language model (MLLM). FEText includes 400,000 curated pairs tailored for facial expression tasks.",
            "To facilitate our diffusion model to generate high-fidelity facial expressions, a straightforward approach is to fine-tune the model directly on the proposed FEText using the diffusion loss in Eq.   3 . However, since FEText contains images processed through a super-resolution model, this direct fine-tuning strategy may lead to over-smoothing in the generated images. To address this, we introduce a two-stage fine-tuning paradigm. In the first stage, the diffusion model is trained on the entire FEText dataset to capture facial expression-related semantics. Then, the second stage mitigates over-smoothing by specifically fine-tuning our diffusion model on the CelebA-HQ and FFHQ subsets of FEText, which consist of native high-resolution images. This two-step approach ensures that our model learns expressive facial details while preserving image sharpness. The fine-tuned model then serves as the foundation for controllable facial expression generation, incorporating facial action unit injection (Sec.  4.2.3 ) and semantic guidance (Sec.  4.3 ).",
            "While fine-tuning the diffusion model using facial expression captions provides general language-based guidance for facial expression generation, it lacks the precision needed to capture fine-grained facial details, such as localized muscle movements. To overcome this limitation, we propose to incorporate more explicit control signals through Facial Action Units (FAUs), each of which represents a specific facial muscle movement. Inspired by IP-Adapter  Ye et al. ( 2023 ) , we apply a decoupled cross-attention module to integrate FAU embeddings with the diffusion models generation process. These embeddings are derived by mapping discrete FAU labels into high-dimensional representations using a Multi-Layer Perceptron, referred to as the AU adapter. FAU labels for each image in the FEText dataset are annotated using the widely adopted FAU detection model, OpenGraphAU  Luo et al. ( 2022 ) . With the diffusion models parameters frozen, we train the AU adapter to guide the model in recovering facial images based on the annotated FAU labels, using the objective in Eq.  3 .",
            "Self-supervised Representation Learning.  We trained self-supervised learning (SSL) models, including BYOL  Grill et al. ( 2020 ) , MoCo v3  Chen et al. ( 2021 ) , and SimCLR  Chen et al. ( 2020 ) , using real-world data, our synthetic data, and a combination of both. The linear probe performance of these models was evaluated on three widely used facial expression recognition (FER) datasets: RAF-DB  Li et al. ( 2017 ) , AffectNet  Mollahosseini et al. ( 2017 ) , and SFEW  Dhall et al. ( 2011 ) , with results reported in Tab.  3 . All SSL models were trained with a ResNet-50 architecture  He et al. ( 2016 ) . Notably, state-of-the-art methods in self-supervised facial representation learning, such as MCF  Wang et al. ( 2023 ) , FRA  Gao & Patras ( 2024 ) , and PCL  Liu et al. ( 2023c ) , were pre-trained on much larger face datasets like LAION-Face  Zheng et al. ( 2022 ) , VGGFace2  Cao et al. ( 2018 ) , and VoxCeleb  Nagrani et al. ( 2020 ) . However, these models underperformed on FER tasks compared to ours, highlighting that existing large-scale face datasets may lack the high-quality and diverse facial expression patterns required for accurate FER. Results demonstrate that combining real-world and synthetic data consistently boosts SSL baselines. Remarkably, even when MoCo v3 was trained solely on our synthetic data, it achieved a 2.12% improvement on RAF-DB, underscoring the effectiveness of our approach in capturing critical facial expression details that are essential for FER.",
            "We validate the effectiveness of SynFER for supervised representation learning by evaluating its performance on RAF-DB and AffectNet (Tab.  3 ). We compare with SOTA FER models, including Ada-DF  Liu et al. ( 2023a ) , POSTER++  Mao et al. ( 2024 ) , and APViT  Xue et al. ( 2022 ) . The results demonstrate that incorporating synthetic data consistently enhances both baseline models and the latest SOTAs in supervised facial expression recognition. Notably, APViT benefits from the synthetic data with improvements of 0.27% on RAF-DB and 0.32% on AffectNet. While the improvements in supervised learning are more modest compared to self-supervised learning, they remain consistent. This is likely due to the stricter distribution alignment required in supervised learning between synthetic training data and real-world test data. In the following section on scaling behavior analysis, we provide further insights, showcasing the use of the distribution alignment technique, Real-Fake  Yuan et al. ( 2023 ) , to alleviate this problem.",
            "Reliability of FERAnno.  We assess the reliability of FERAnno as a label calibrator by evaluating its performance on two FER datasets and visualizing its attention maps in Tab.  3  and Fig.  6 . FERAnno significantly outperforms previous SOTAs, achieving a +0.51% improvement on RAF-DB and a +1.34% improvement on AffectNet over the second-best models. The attention maps in Fig.  6  further demonstrate FERAnnos ability to accurately locate facial expression-related facial features, such as jaw-dropping and furrowed eyebrows, highlighting the diffusion models great semantic understanding and fine-grained facial expression recognition.",
            "Others.  As all the methods for comparisons in supervised learning (Tab.  3 ), few-shot learning (Tab.  5 ) and multi-modal fine-tuning (Tab.  5 ) are open-source, we thus only need to rewrite the corresponding code for dataset reading to incorporate the synthetic data. We follow the default setting in each open-source code of the compared methods."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance comparisons with SOTA few-shot learning methods on 5-way few-shot FER tasks with a 95% confidence interval. (*) indicates training with both real-world data and our synthesis data.",
        "table": "S5.T5.fig1.1.1",
        "footnotes": [],
        "references": [
            "We begin by introducing i) the overall synthetic pipeline for generating facial expression image-label pairs. Next, we detail ii) our approach for producing high-fidelity facial expression images, which are controlled through high-level text descriptions (Sec. 4.2.1 ), fine-grained facial action units corresponding to localized facial muscles (Sec. 4.2.3 ), and a semantic guidance technique (Sec. 4.3 ). Finally, we introduce iii) the FER annotation crafter (FERAnno), a crucial component that thoroughly understands the synthetic facial expression data and automatically generates accurate annotations accordingly(Sec. 4.4 ). This pipeline ensures both precision and reliability in facial expression generation and labeling.",
            "To facilitate our diffusion model to generate high-fidelity facial expressions, a straightforward approach is to fine-tune the model directly on the proposed FEText using the diffusion loss in Eq.   3 . However, since FEText contains images processed through a super-resolution model, this direct fine-tuning strategy may lead to over-smoothing in the generated images. To address this, we introduce a two-stage fine-tuning paradigm. In the first stage, the diffusion model is trained on the entire FEText dataset to capture facial expression-related semantics. Then, the second stage mitigates over-smoothing by specifically fine-tuning our diffusion model on the CelebA-HQ and FFHQ subsets of FEText, which consist of native high-resolution images. This two-step approach ensures that our model learns expressive facial details while preserving image sharpness. The fine-tuned model then serves as the foundation for controllable facial expression generation, incorporating facial action unit injection (Sec.  4.2.3 ) and semantic guidance (Sec.  4.3 ).",
            "To ensure semantic alignment between each synthesized face image and its assigned facial expression label, we introduce FERAnno, a label calibration framework designed to validate the consistency of the generated data. By analyzing the facial patterns of each synthesized image, FERAnno categorizes them and compares the post-categorized labels with their pre-assigned facial expression labels. This verification process helps identify and filter out samples with mismatched labels, preventing them from negatively impacting downstream FER model training. Specifically, FERAnno is a diffusion-based label calibrator equipped with a deep understanding of facial semantics. It leverages the multi-scale intermediate features and cross-attention maps inherent in the diffusion model to predict accurate FER labels, as depicted in Fig.  4 . This ensures only high-quality, correctly labeled samples are included in the training pipeline, leading to more reliable model performance.",
            "Multi-scale Features and Attention Maps Fusion.  Given that the multi-scale feature maps  F F \\mathcal{F} caligraphic_F  capture global information essential for image generation, and the cross-attention maps provide class-discriminative information as well as relationships between object locations  Tang et al. ( 2022 ); Caron et al. ( 2021 ) , FERAnno fuses both features and attention maps within a dual-branch encoder architecture for pseudo-label annotation. An overview of this architecture is shown in Fig.  4 .",
            "As shown in Fig.  10 , we provide comparisons between the over-smoothing synthetic images and the more natural ones. Due to the large amount of super-resolution data in FEText, it can be seen that solely performing fine-tuning on the entire FEText significantly degrades the realism of the images, while the proposed two-stage fine-tuning strategies in Sec.  4.2.2  could effectively prevent over-smoothing."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Zero-shot performance comparison of CLIP for FER, reporting both Weighted Average Recall (WAR) and Unweighted Average Recall (UAR) as in previous works  Zhao et al. ( 2024 ) . (*) indicates models trained with both real-world and synthetic data.",
        "table": "S5.T5.fig2.1.1",
        "footnotes": [
            ""
        ],
        "references": [
            "We conduct extensive experiments to evaluate both the generation quality of our synthetic data (Sec.  5.1 ) and its effectiveness in FER tasks (Sec.  5.2 ). For more details on experimental setup, implementation details are provided in the appendix.",
            "Few-shot Learning.  Addressing the challenge of limited labeled FER data across different scenarios, we explore the potential of synthetic data to enhance few-shot learning, as presented in Tab.  5 . Following the protocol established by CDNet  Zou et al. ( 2022 ) , we train models on five basic expression datasets and evaluate them on three compound expression datasets: CFEE_C  Du et al. ( 2014 ) , EmotionNet_C  Fabian Benitez-Quiroz et al. ( 2016 ) , and RAF_C  Li et al. ( 2017 ) . To benchmark our approach, we compare it against SOTA few-shot learning methods, including InfoPatch  Liu et al. ( 2021 ) , LR+DC  Yang et al. ( 2021 ) , and STARTUP  Phoo & Hariharan ( 2021 ) . The results clearly demonstrate that integrating synthetic data consistently enhances few-shot FER performance across key metrics. This highlights the ability of synthetic data, with its broader range of FER patterns, to bridge the gap in data-limited scenarios, allowing models to better generalize to complex, real-world expressions in few-shot tasks.",
            "Multi-modal Fine-tuning.  The synthesis data encompasses multiple modalities, including generated images, textual prompts, and FER labels. To assess its impact on multi-modal fine-tuning for FER, we focus on fine-tuning the vision-language foundation model CLIP  Wang et al. ( 2022 ) , as its performance on face-related tasks is widely regarded as sub-optimal  Guo et al. ( 2023 ); Chen et al. ( 2024 ) . Building on Exp-CLIP  Zhao et al. ( 2024 ) , we fine-tune the models on CAER-S  Lee et al. ( 2019 )  and evaluate their zero-shot performances on the datasets outlined in Tab.  5 . Our results show that the inclusion of detailed textual prompts and a larger training image set significantly enhances the generalization ability of Exp-CLIP in understanding facial expressions, achieving significant improvements such as +2.58% UAR on the AffectNet dataset.",
            "Effectiveness of FAU Control.  We validate the effectiveness of SynFER by examining how Facial Action Units (FAUs) enhance the generation process and refine facial details. As illustrated in Fig.  5 , samples generated with FAU control (third column) exhibit facial expressions that more accurately match their assigned labels compared to those generated with only text guidance (second column). For example, the fear expression, driven by FAUs like Inner Brow Raiser and Lip Stretcher, becomes more distinct (third column, second row), making it easier to differentiate from other emotions such as surprise. Similarly, disgust is more pronounced with FAUs like Lid Tightener. Without FAU control, facial expressions (second column) tend to blur, as different categories show overlapping features. Quantitative results in Tab.  6  highlight the impact of FAU control: FER accuracy increases from 34.62% to 48.74%, and FAU detection accuracy rises from 88.91% to 92.37%. This also translates into improved downstream performance on RAF-DB and AffectNet.",
            "Effectiveness of Semantic Guidance.  We further explore the impact of semantic guidance (SG) on both generation quality and supervised representation learning, as shown in Fig.  5  and Tab.  6 . By updating text embeddings to better align with the target facial expression category, SG improves the accuracy of the generated expressions by 6.4%, compared to static text and FAUs. The samples in the last column of Fig.  5  show more exaggerated facial expressions than those in the third column, with SG enhancing the intensity.",
            "Others.  As all the methods for comparisons in supervised learning (Tab.  3 ), few-shot learning (Tab.  5 ) and multi-modal fine-tuning (Tab.  5 ) are open-source, we thus only need to rewrite the corresponding code for dataset reading to incorporate the synthetic data. We follow the default setting in each open-source code of the compared methods."
        ]
    },
    "id_table_6": {
        "caption": "Table 7:  Implementation details on self-supervised pre-training. RRC and RHF denote random resize crop and random horizontal flip, respectively.",
        "table": "S5.F5.fig1.1.1",
        "footnotes": [],
        "references": [
            "Effectiveness of FAU Control.  We validate the effectiveness of SynFER by examining how Facial Action Units (FAUs) enhance the generation process and refine facial details. As illustrated in Fig.  5 , samples generated with FAU control (third column) exhibit facial expressions that more accurately match their assigned labels compared to those generated with only text guidance (second column). For example, the fear expression, driven by FAUs like Inner Brow Raiser and Lip Stretcher, becomes more distinct (third column, second row), making it easier to differentiate from other emotions such as surprise. Similarly, disgust is more pronounced with FAUs like Lid Tightener. Without FAU control, facial expressions (second column) tend to blur, as different categories show overlapping features. Quantitative results in Tab.  6  highlight the impact of FAU control: FER accuracy increases from 34.62% to 48.74%, and FAU detection accuracy rises from 88.91% to 92.37%. This also translates into improved downstream performance on RAF-DB and AffectNet.",
            "Effectiveness of Semantic Guidance.  We further explore the impact of semantic guidance (SG) on both generation quality and supervised representation learning, as shown in Fig.  5  and Tab.  6 . By updating text embeddings to better align with the target facial expression category, SG improves the accuracy of the generated expressions by 6.4%, compared to static text and FAUs. The samples in the last column of Fig.  5  show more exaggerated facial expressions than those in the third column, with SG enhancing the intensity.",
            "Reliability of FERAnno.  We assess the reliability of FERAnno as a label calibrator by evaluating its performance on two FER datasets and visualizing its attention maps in Tab.  3  and Fig.  6 . FERAnno significantly outperforms previous SOTAs, achieving a +0.51% improvement on RAF-DB and a +1.34% improvement on AffectNet over the second-best models. The attention maps in Fig.  6  further demonstrate FERAnnos ability to accurately locate facial expression-related facial features, such as jaw-dropping and furrowed eyebrows, highlighting the diffusion models great semantic understanding and fine-grained facial expression recognition."
        ]
    },
    "id_table_7": {
        "caption": "Table 8:  FAU annotations to generate specific classes of facial expression images.",
        "table": "A1.T7.1.1",
        "footnotes": [],
        "references": [
            "Synthetic Data Scaling Analysis.  Following  Tian et al. ( 2024 ); Fan et al. ( 2024 ) , we investigate the scaling behavior of synthetic data in both self-supervised and supervised learning paradigms. To highlight the potential of synthetic FER data, we train models exclusively on synthetic images, without combining real-world data. The results in Fig.  7  (a)-(b) show a stronger scaling effect in self-supervised learning compared to supervised learning, where performance improves significantly with more data. This difference is likely due to the need for better distribution alignment in supervised learning  Yuan et al. ( 2023 ) . While SynFER focuses on addressing FER data scarcity, aligning the synthetic data distribution with real-world data is crucial for supervised tasks. To further explore this, we apply the Real-Fake technique  Yuan et al. ( 2023 )  for real and synthetic data distribution alignment, and present the results in Fig.  7  (c). Compared to standard supervised learning, Real-Fake demonstrates a clear performance boost."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.T8.1",
        "footnotes": [],
        "references": [
            "Step Size    \\lambda italic_  in Semantic Guidance.  For hyper-parameter analysis, we consider five configurations of the step size    \\lambda italic_  in semantic guidance. Due to computational resource constraints, we provide results of self-supervised learning with MoCo v3  Chen et al. ( 2021 )  on 0.2M synthetic data for pre-training and report the linear probe performances on RAF-DB  Li et al. ( 2017 ) . Experiment results are shown in Fig.  8 . It can be seen that when    \\lambda italic_  is relatively small, its influence on the performance is relatively small. However, as    \\lambda italic_  continues to increase, the downstream performance is severely degraded. This is because an excessive    \\lambda italic_  would lead to severely disrupted images, as shown in Fig.  9 ."
        ]
    }
}