{
    "id_table_1": {
        "caption": "Table 1:  Comparisons of Systems with Different Human-Human Interaction Generators on the Replica Dataset.  Baseline refers to generations without human-human interactions. The best results are highlighted in bold.",
        "table": "S4.T2.4.4.4",
        "footnotes": [],
        "references": [
            "We evaluate the performance of Sitcom-Crafter using open-source 3D scenes. The experimental results demonstrate that Sitcom-Crafter can generate high-quality, diverse, and well-physics-constrained human motions (see examples in Fig.  1 ). Our key contributions in this work are as follows:  1)  we develop a comprehensive human motion generation system, Sitcom-Crafter, which supports the synthesis of diverse types of human motions guided by both 3D scene structures and long plot contexts. The system consists of three motion generation modules and five augmentation modules that provide a flexible approach to motion generation.  2)  We introduce a novel self-supervised, scene-aware human-human interaction generation method within the generation modules. By synthesizing binary SDF points around the motion region, we incorporate surrounding scene information into the generator, addressing the motion-scene collision problem prevalent in existing methods. Additionally, we unify motion representation using marker points across the different generation modules, ensuring seamless integration and compatibility of the generated motions.  3)  We design five augmentation modules to enhance the cohesiveness and quality of the generated motions and improve the systems user-friendliness. These include modules for plot interpretation and command distribution, motion synchronization, collision revision, hand pose retrieval, and motion retargeting. Experimental evaluations on open-source 3D scenes demonstrate that our system effectively synthesizes high-quality, diverse, and well-physics-constrained human motions.",
            "Our human-human interaction generation module builds upon InterGen  (Liang et al.,  2024 ) , but introduces several key modifications to better integrate with the overall system and address motion-scene collision issues. These changes include a different motion representation, additional network conditions (last frame condition and synthetic SDF points condition), a body regressor that converts marker points to body mesh, data canonicalization, and modified training losses and strategies. We will cover some of these aspects here, while leaving the details of other components to Appendix  B.1 .",
            "We therefore improved the random point-based height assignment by adopting a random plane-based height assignment. Specifically, we randomly sample  K K K italic_K  patterns (such as rotated ellipses and rectangles), denoted as  Pat Pat \\mathrm{Pat} roman_Pat , and set a uniform height value for the columns of points within each pattern. This is formulated as  P o  b  j = { P o  b  j 1 , P o  b  j 2 , ... , P o  b  j K } subscript P o b j superscript subscript P o b j 1 superscript subscript P o b j 2 ... superscript subscript P o b j K P_{obj}=\\{P_{obj}^{1},P_{obj}^{2},...,P_{obj}^{K}\\} italic_P start_POSTSUBSCRIPT italic_o italic_b italic_j end_POSTSUBSCRIPT = { italic_P start_POSTSUBSCRIPT italic_o italic_b italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_P start_POSTSUBSCRIPT italic_o italic_b italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT , ... , italic_P start_POSTSUBSCRIPT italic_o italic_b italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT } , where  P o  b  j i superscript subscript P o b j i P_{obj}^{i} italic_P start_POSTSUBSCRIPT italic_o italic_b italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  denotes points within a patterns region, and  P o  b  j i = { p  p z < T Pat k  ( p x , p y )  Pat k } superscript subscript P o b j i conditional-set p subscript p z subscript T subscript Pat k subscript p x subscript p y subscript Pat k P_{obj}^{i}=\\{p\\mid p_{z}<T_{\\mathrm{Pat}_{k}}\\wedge(p_{x},p_{y})\\in\\mathrm{%  Pat}_{k}\\} italic_P start_POSTSUBSCRIPT italic_o italic_b italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT = { italic_p  italic_p start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT < italic_T start_POSTSUBSCRIPT roman_Pat start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT  ( italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )  roman_Pat start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }  with  T Pat k = Rand  ( T f  l  o  o  r , T c  e  i  l  i  n  g ) subscript T subscript Pat k Rand subscript T f l o o r subscript T c e i l i n g T_{\\mathrm{Pat}_{k}}=\\mathrm{Rand}(T_{floor},T_{ceiling}) italic_T start_POSTSUBSCRIPT roman_Pat start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT = roman_Rand ( italic_T start_POSTSUBSCRIPT italic_f italic_l italic_o italic_o italic_r end_POSTSUBSCRIPT , italic_T start_POSTSUBSCRIPT italic_c italic_e italic_i italic_l italic_i italic_n italic_g end_POSTSUBSCRIPT )  shared by all points within the pattern region. This approach ensures that the objects have flat surfaces parallel to the floor. While extending the setting to sloped surfaces more closely mimicking real-world conditions could be explored, we leave this as future work. By incorporating this synthetic surrounding object information into the generator, our human-human interaction module successfully avoids collisions when applied to a 3D scene. Specifically, we pass these synthesized SDF points into the generator as additional conditions. The condition is a concatenation of the points positions (relative to one persons pelvis) and their values, forming a 4-dimensional vector for each point  p c  o  n  d = { p x , y , z , p v  a  l  u  e } subscript p c o n d subscript p x y z subscript p v a l u e p_{cond}=\\{p_{x,y,z},p_{value}\\} italic_p start_POSTSUBSCRIPT italic_c italic_o italic_n italic_d end_POSTSUBSCRIPT = { italic_p start_POSTSUBSCRIPT italic_x , italic_y , italic_z end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_v italic_a italic_l italic_u italic_e end_POSTSUBSCRIPT } . These vectors are processed by several blocks of 3D convolutions and then flattened into an embedding vector passed into the generator. Details are provided in Appendix  B.1 .",
            "Data Canonicalization.  In InterGen  (Liang et al.,  2024 ) , data canonicalization is achieved by moving one character to the global coordinate origin, aligning the first frames direction along the Y-axis, and adaptively transforming the second characters position and rotation.  E.g. , given the joint positions  j g p  R 22  3 superscript subscript j g p superscript R 22 3 \\bm{j}_{g}^{p}\\in\\mathbb{R}^{22\\times 3} bold_italic_j start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT 22  3 end_POSTSUPERSCRIPT  (see Appendix  A  for definitions), the positions  { j g p } N A subscript superscript superscript subscript j g p A N \\{\\bm{j}_{g}^{p}\\}^{A}_{N} { bold_italic_j start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT } start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT  of character A are centered around the origin, while character Bs positions  { j g p } N B superscript subscript superscript subscript j g p N B \\{\\bm{j}_{g}^{p}\\}_{N}^{B} { bold_italic_j start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT , relative to character A, are more dispersed. Our experiments (see Table  4 ) reveal that this approach introduces a bias: the character near the origin is easier to model due to its concentrated joint positions, while the other character, with more dispersed positions, is harder to learn. This issue is even more pronounced in our marker-based representation, where we must model the global positions of character Bs 67 markers (see Appendix  B.1 ).",
            "Second, if plot  c c c italic_c  is available, by carefully illustrating the format and types of orders that can be interpreted by the generation modules through prompts, denoted as  prompt o  r  d  e  r subscript prompt o r d e r \\mathrm{prompt}_{order} roman_prompt start_POSTSUBSCRIPT italic_o italic_r italic_d italic_e italic_r end_POSTSUBSCRIPT , the language model can extract key orders for different characters from the input plot context. These orders are recognizable and executable by our three motion generation models. This process is formulated as  { Orders A , Orders B , ... } = Lang  ( c , prompt o  r  d  e  r ) subscript Orders A subscript Orders B ... Lang c subscript prompt o r d e r \\{\\mathrm{Orders_{A}},\\mathrm{Orders_{B}},\\ldots\\}=\\mathrm{Lang}(c,\\mathrm{%  prompt}_{order}) { roman_Orders start_POSTSUBSCRIPT roman_A end_POSTSUBSCRIPT , roman_Orders start_POSTSUBSCRIPT roman_B end_POSTSUBSCRIPT , ... } = roman_Lang ( italic_c , roman_prompt start_POSTSUBSCRIPT italic_o italic_r italic_d italic_e italic_r end_POSTSUBSCRIPT ) , where  A , B , ... A B ... A,B,\\ldots italic_A , italic_B , ...  represent different characters. The language model also rechecks the produced commands to ensure format correctness. These commands are then distributed to the appropriate generation modules. More details of this module, including specific prompts and how generation modules interpret and execute commands, are provided in Appendix  C.1 .",
            "Evaluation Metrics.  We evaluated physical compliance of the generated motions using the following metrics: Foot Velocity to detect foot sliding (FS) artifacts, Foot Penetration (FP) to measure foot markers distances from the grounds default height, Human-Scene Penetration (HSP) to quantify negative SDF points of marker points indicating collision with scene objects, and Human-Human Perturbation (HHP) to measure intersecting vertices between characters. The calculation of these physical metrics can be directly referred to in the corresponding physics-constraint loss functions designed in Appendix  B.1 . Additionally, for a comprehensive evaluation, we followed  Guo et al. ( 2022 )  and employed other metrics (experiments with these metrics are shown in the Appendix): Frechet Inception Distance (FID) to compare data distribution between generated and ground-truth motions, R Precision Top N (R-P TN) to assess text-motion matching, Diversity to evaluate the variance in generated motions, MModality to gauge motion diversity under the same text guidance, and MM Dist to measure the cosine similarity between motions and texts.",
            "B.1: Human-Human Interaction Generation   B.1",
            "C.1: Plot Generation, Comprehension, and Command Distribution   C.1",
            "We train two regressors: a marker-to-SMPL regressor and a marker-to-SMPLX regressor. Both regressors are trained on the training and validation sets of the InterHuman dataset  (Liang et al.,  2024 )  and tested on the test set. The optimization loss function used in  Zhang & Tang ( 2022 )  primarily involves the MSE loss between the indexed marker points of the converted SMPL model and the input marker points (excluding the regulation loss on hand poses). From the visualization of the predicted results shown in Fig.  10 , it is evident that while the predicted SMPL body model achieves similarly positioned marker points, there are noticeable distortions, such as rotations and windings on the body mesh (e.g., in the neck region).",
            "Upon investigation, we found that these distortions were due to the regressed joint rotations increasing periodically (by  2   2  2\\pi 2 italic_ ). To address this, for the marker-to-SMPL regressor, we added additional regression losses on body joint rotations   b subscript  b \\theta_{b} italic_ start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT , root translation  t r subscript t r t_{r} italic_t start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , root rotation   r subscript  r \\theta_{r} italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , and body shape    \\beta italic_ . For the marker-to-SMPLX regressor, since the body shape parameters and root translation are not mutually compatible, we only added regression losses on body joint rotations   b subscript  b \\theta_{b} italic_ start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT  and root rotation   r subscript  r \\theta_{r} italic_ start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT , assigning them a 0.1 loss weight to avoid potential overfitting. The regression performance is illustrated in Fig.  11 .",
            "Given a long plot context, through our plot comprehension module (details in Section  3.3  and Appendix  C.1 ), the human locomotion generation module receives two kinds of instructions:  None None \\mathrm{None} roman_None  (indicating a random route point in the scene) or  [ ObjectName ] delimited-[] ObjectName \\mathrm{[ObjectName]} [ roman_ObjectName ]  (indicating a route point near a specific scene object). For  None None \\mathrm{None} roman_None , the point sampling is random and unconstrained, whereas for  [ ObjectName ] delimited-[] ObjectName \\mathrm{[ObjectName]} [ roman_ObjectName ] , sampling occurs within a larger 3D bounding box around the object. To ensure the sampling result is not within unwalkable regions ( e.g. , inside the object), the system pre-bakes a 2D navigation plane indicating walkable areas. Illegal points are filtered out, and sampling repeats if necessary. For sampling around objects, the 3D box scope dynamically increases to avoid dead loops. If multiple objects share the same  ObjectName ObjectName \\mathrm{ObjectName} roman_ObjectName , one is selected at random. Thus, for a locomotion instruction like  [ None , Chair , Table ] None Chair Table [\\mathrm{None},\\mathrm{Chair},\\mathrm{Table}] [ roman_None , roman_Chair , roman_Table ] , the system process is as shown in Fig.  12 . This module also aids human-human interaction generation by ensuring that the two characters are not too far apart when initiating interaction. To achieve this, an additional route point is added around one character, compelling them to move closer to the other, resulting in more cohesive and natural interaction.",
            "For instructing human-scene interaction generation, the instructional text format is  [ [ ObjectName ] , [ MotionType ] ] delimited-[] ObjectName delimited-[] MotionType [\\mathrm{[ObjectName],[MotionType]}] [ [ roman_ObjectName ] , [ roman_MotionType ] ] . This involves first sampling a point near the target object as described above, and then executing the specific motion. Given an instruction like  [ Chair , Sit ] Chair Sit [\\mathrm{Chair},\\mathrm{Sit}] [ roman_Chair , roman_Sit ] , the system generates results as shown in Fig.  12 .",
            "For random plot generation based on the input 3D scene information, the prompt fed into the language model is shown in Fig.  13 . By providing examples of expected outputs and passing the category names of objects present in the 3D scene, the language model functions effectively as a scripter, returning satisfactory orders. These orders include  [ None , ObjectName ] None ObjectName [\\mathrm{None},\\mathrm{ObjectName}] [ roman_None , roman_ObjectName ]  for human locomotion generation,  [ MotionType ] delimited-[] MotionType [\\mathrm{MotionType}] [ roman_MotionType ]  for human-scene interaction generation, and orders prefixed with HHI: such as  [ HHI : Two  persons  hug  each  other  ... ] delimited-[] : HHI Two persons hug each other ... [\\mathrm{HHI:~{}Two~{}persons~{}hug~{}each~{}other~{}...}] [ roman_HHI : roman_Two roman_persons roman_hug roman_each roman_other ... ] . An example of the language models generation result is shown in Fig.  14 .",
            "Due to the inherent stochasticity of the language model, the extracted orders sometimes fail to meet the specifications required by the generation modules. Therefore, we utilize the language model again to revise the initial orders, adhering to a series of established rules shown in Fig.  15 . This additional revision process ensures that the instructional orders are more satisfactory. An example of the language models generation result is shown in Fig.  16 . In practice, we also append this rule prompt after the generation prompt in random plot generation to ensure successful output.",
            "Fig.  17  displays a hand motion sequence before and after retrieval. To ensure the consistency of hand motion, we also apply Slerp interpolation, as detailed in Appendix  C.3 .",
            "Then, we apply a similar process to character B, but slow down its former motions and speed up its latter motions. By slowing down/speeding up the former part and speeding up/slowing down the latter part, the total sequence length  L L L italic_L  remains unchanged. We then re-calculate  L h  u  m  a  n  P  e  n  e subscript L h u m a n P e n e \\mathcal{L}_{humanPene} caligraphic_L start_POSTSUBSCRIPT italic_h italic_u italic_m italic_a italic_n italic_P italic_e italic_n italic_e end_POSTSUBSCRIPT  to check if the human-human collision is alleviated. If so, new collision sequences  COL COL \\mathrm{COL} roman_COL  are identified, and the process starts from the first new sequence; otherwise, we move to the next collided sequence  [ col s  ( n + 1 ) , col e  ( n + 1 ) ] subscript col s n 1 subscript col e n 1 [\\mathrm{col_{s(n+1)}},\\mathrm{col_{e(n+1)}}] [ roman_col start_POSTSUBSCRIPT roman_s ( roman_n + 1 ) end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT roman_e ( roman_n + 1 ) end_POSTSUBSCRIPT ] . In this way, the system effectively alleviates human-human collisions among human locomotion or human-scene interaction results. Fig.  18  shows the results before and after applying the collision revision module. However, it should be noted that the collision revision will fail when there is no space for changing motion lanes, for example the narrow aisle as shown in Fig.  19 .",
            "Fig.  20  demonstrates the unrealistic rendering results when merely applying a texture map onto the SMPL/SMPL-X body model, compared to the more natural results of existing 3D digital human assets. To ensure that the generated motions retarget well to the 3D asset, we carefully adjust the shifts of the translations of the pelvis joint and the rotations of 21 body joints and 30 hand joints between the SMPL-X body model (all generated marker points are converted to SMPL-X parameters using our marker-to-SMPLX regressor) and the 3D digital asset. This process is performed using the Keemap  (Keeline,  2023 )  package in Blender. The retargeted results are shown in Fig.  21 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparisons with other human-human interaction methods on the InterHuman dataset.  Real represents the performance of real data. The best results are in bold.",
        "table": "S4.T2.8.4.4",
        "footnotes": [],
        "references": [
            "Before delving into our system, we recommend first reviewing Appendix  A  for an understanding of the methods closely related to our approach and some denotation symbols, which are placed there due to page length constraints. Fig.  2  demonstrates the entire workflow of Sitcom-Crafter. The initial input is a 3D scene with semantic information about its objects. The plot context can either be automatically generated based on the 3D scene information or manually designed by users. A language model is then employed to comprehend the plot context and transform it into recognizable commands for character motions. These commands are distributed to three motion generation modules responsible for crafting different types of motions: human locomotion, human-scene interaction, and human-human interaction. During the generation process, a motion synchronization module ensures consistency between synthesized frames from different generation modules. A hand pose retrieval module supplements hand poses to enhance the expressiveness of the human motions. To increase the realism and naturalness of the motions, a collision revision module adjusts the generated motions to prevent collisions between characters, and a motion retargeting module maps the generated motions to existing high-quality 3D human assets for improved visual fidelity.",
            "In this section, we will introduce our human-human interaction generation module, while the details and designs of the other two motion generation modules are provided in Appendix  B.2 .",
            "To address the first issue, we use Spherical Linear Interpolation (Slerp)  (Shoemake,  1985 )  for smooth rotation transitions and linear interpolation for other transitions. For the second one, we set the motion length based on the maximum order number of the two characters. If one character has fewer orders, we extend his motion by generating redundant frames. Details are provided in Appendix  C.2 .",
            "In Table  2 , we present the physical metrics comparisons between systems employing different human-human interaction methods. The results generated by commands without human-human interaction serve as the baseline, which should approximate the upper bound of FS, FP, and HSP performance. This is because the human locomotion and human-scene interaction modules are shared across all compared systems, and these two modules have been well-tuned to avoid physical inconsistencies.",
            "Comparisons of Human-Human Interaction Generation on Synthetic Implicit 3D Scenes.  We further isolate the human-human interaction module to evaluate its performance on our synthetic implicit 3D scenes, as described in Section  3.2 . We primarily focus on physical compliance metrics. For metrics related to motion diversity, alignment with guidance text, and other aspects, please refer to Appendix  E . From Table  2 , we observe that our human-human interaction module surpasses the other two methods in all physical metrics, validating the effectiveness of our design.",
            "B.2: Human Locomotion and Human-Scene Interaction Generation   B.2",
            "C.2: Motion Synchronization   C.2",
            "For the SDF grid points, given their dimension of  R 4  S h  o  r  S h  o  r  S v  e  r superscript R 4 subscript S h o r subscript S h o r subscript S v e r \\mathbb{R}^{4\\times S_{hor}\\times S_{hor}\\times S_{ver}} blackboard_R start_POSTSUPERSCRIPT 4  italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT  italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT  italic_S start_POSTSUBSCRIPT italic_v italic_e italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  as mentioned in Section  3.2 , we first process them with three 3D convolution layers, each with a stride of 2, reducing the dimensionality to  R 4  S h  o  r 8  S h  o  r 8  S v  e  r 8 superscript R 4 subscript S h o r 8 subscript S h o r 8 subscript S v e r 8 \\mathbb{R}^{4\\times\\frac{S_{hor}}{8}\\times\\frac{S_{hor}}{8}\\times\\frac{S_{ver}%  }{8}} blackboard_R start_POSTSUPERSCRIPT 4  divide start_ARG italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT end_ARG start_ARG 8 end_ARG  divide start_ARG italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT end_ARG start_ARG 8 end_ARG  divide start_ARG italic_S start_POSTSUBSCRIPT italic_v italic_e italic_r end_POSTSUBSCRIPT end_ARG start_ARG 8 end_ARG end_POSTSUPERSCRIPT  to conserve memory. We then flatten and encode the result using an MLP layer to match the embedding space. The encoded marker embedding and SDF embedding are directly added to the text embedding and timestep embedding. The combined embedding is then utilized to guide the generation by influencing the AdaLN layers scale and shift factors within the transformer blocks of the diffusion model, as described in  Peebles & Xie ( 2022 ) .",
            "Given a long plot context, through our plot comprehension module (details in Section  3.3  and Appendix  C.1 ), the human locomotion generation module receives two kinds of instructions:  None None \\mathrm{None} roman_None  (indicating a random route point in the scene) or  [ ObjectName ] delimited-[] ObjectName \\mathrm{[ObjectName]} [ roman_ObjectName ]  (indicating a route point near a specific scene object). For  None None \\mathrm{None} roman_None , the point sampling is random and unconstrained, whereas for  [ ObjectName ] delimited-[] ObjectName \\mathrm{[ObjectName]} [ roman_ObjectName ] , sampling occurs within a larger 3D bounding box around the object. To ensure the sampling result is not within unwalkable regions ( e.g. , inside the object), the system pre-bakes a 2D navigation plane indicating walkable areas. Illegal points are filtered out, and sampling repeats if necessary. For sampling around objects, the 3D box scope dynamically increases to avoid dead loops. If multiple objects share the same  ObjectName ObjectName \\mathrm{ObjectName} roman_ObjectName , one is selected at random. Thus, for a locomotion instruction like  [ None , Chair , Table ] None Chair Table [\\mathrm{None},\\mathrm{Chair},\\mathrm{Table}] [ roman_None , roman_Chair , roman_Table ] , the system process is as shown in Fig.  12 . This module also aids human-human interaction generation by ensuring that the two characters are not too far apart when initiating interaction. To achieve this, an additional route point is added around one character, compelling them to move closer to the other, resulting in more cohesive and natural interaction.",
            "For instructing human-scene interaction generation, the instructional text format is  [ [ ObjectName ] , [ MotionType ] ] delimited-[] ObjectName delimited-[] MotionType [\\mathrm{[ObjectName],[MotionType]}] [ [ roman_ObjectName ] , [ roman_MotionType ] ] . This involves first sampling a point near the target object as described above, and then executing the specific motion. Given an instruction like  [ Chair , Sit ] Chair Sit [\\mathrm{Chair},\\mathrm{Sit}] [ roman_Chair , roman_Sit ] , the system generates results as shown in Fig.  12 .",
            "To ensure consistency between motions from different modules, we adopt the Slerp technique  Shoemake ( 1985 ) . This is primarily applied to transitions between other motions and human-human interactions, or vice versa, where there can be noticeable motion jumps despite conditioning the generation on the last frame (as mentioned in Section  3.2 ). Specifically, we designate the first and last four frames of generated human-human interactions as buffer areas for interpolation, using Slerp for rotation interpolation and linear interpolation for translation. This results in smoother motion transitions.",
            "where there are four locomotion orders before the human-human interaction. Suppose each order consists of five clips, then there is a total of 1.25 seconds of motion sequence at 40 FPS per order (see related details in Section  A ). Since Character B has more locomotion orders than Character A before the human-human interaction order, we set the total generated frame length before the human-human interaction in this segment to 41.25=5 seconds. This means when A or B reaches the target route point, they generate additional redundant frames at that point until the total motion length reaches 5s. These redundant frames will appear as hovering at the end (as shown in Fig.  2 ). In this way, the system prevents one character from ending their motion and remaining static until the other character finishes, ensuring better naturalness and smoothness in the interactions.",
            "The collision revision module of our system primarily functions as a post-processing step. Since the human-human interaction generator has taken the other characters position into account during its training and with marker condition (see Section  3.2 ), the collision revision here focuses on human-human collisions that occur in the results from the human locomotion generation module and the human-scene interaction generation module. These modules do not consider other characters positions during motion generation, leading to frequent human-human collisions in their results.",
            "The core solution of the collision revision module is to adjust the speed of one characters motion and slow down the other characters motion to avoid collisions. The strategy is as follows: suppose there are two sequences,  Seq A subscript Seq A \\mathrm{Seq}_{A} roman_Seq start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT  and  Seq B subscript Seq B \\mathrm{Seq}_{B} roman_Seq start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT , for two characters, respectively. The sequence length is identical and denoted as  L L L italic_L . To determine the collided frames, we use  L h  u  m  a  n  P  e  n  e subscript L h u m a n P e n e \\mathcal{L}_{humanPene} caligraphic_L start_POSTSUBSCRIPT italic_h italic_u italic_m italic_a italic_n italic_P italic_e italic_n italic_e end_POSTSUBSCRIPT  mentioned in Section  3.2  to identify the collided frames (a threshold is set to filter out frames with trivial human-human collisions). Suppose the collided sequences are  COL = { [ col s1 , col e1 ] , [ col s2 , col e2 ] , ... , [ col sN , col eN ] } COL subscript col s1 subscript col e1 subscript col s2 subscript col e2 ... subscript col sN subscript col eN \\mathrm{COL}=\\{[\\mathrm{col_{s1}},\\mathrm{col_{e1}}],[\\mathrm{col_{s2}},%  \\mathrm{col_{e2}}],...,[\\mathrm{col_{sN}},\\mathrm{col_{eN}}]\\} roman_COL = { [ roman_col start_POSTSUBSCRIPT s1 end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT e1 end_POSTSUBSCRIPT ] , [ roman_col start_POSTSUBSCRIPT s2 end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT e2 end_POSTSUBSCRIPT ] , ... , [ roman_col start_POSTSUBSCRIPT roman_sN end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT roman_eN end_POSTSUBSCRIPT ] } , where  [ col sn , col en ] subscript col sn subscript col en [\\mathrm{col_{sn}},\\mathrm{col_{en}}] [ roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT ]  denotes the start and end frame index of the  n t  h subscript n t h n_{th} italic_n start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT  collided sequence. We apply collision revision iteratively from the first collided sequence. For character A, we speed up its motions during  [ 0 , col sn + col en 2 ] 0 subscript col sn subscript col en 2 [0,\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2}] [ 0 , divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ]  by downsampling the frame length from  col sn + col en 2 subscript col sn subscript col en 2 \\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2} divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG  to  ( col sn + col en 2  col en  col sn 2 ) subscript col sn subscript col en 2 subscript col en subscript col sn 2 (\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2}-\\frac{\\mathrm{col_{en}}-\\mathrm%  {col_{sn}}}{2}) ( divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG - divide start_ARG roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT - roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ) , and slow down its motions during  [ col sn + col en 2 , L ] subscript col sn subscript col en 2 L [\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2},L] [ divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG , italic_L ]  by upsampling the frame length from  ( L  col sn + col en 2 ) L subscript col sn subscript col en 2 (L-\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2}) ( italic_L - divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG )  to  ( L  col sn + col en 2 + col en  col sn 2 ) L subscript col sn subscript col en 2 subscript col en subscript col sn 2 (L-\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2}+\\frac{\\mathrm{col_{en}}-%  \\mathrm{col_{sn}}}{2}) ( italic_L - divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG + divide start_ARG roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT - roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ) .",
            "Fig.  20  demonstrates the unrealistic rendering results when merely applying a texture map onto the SMPL/SMPL-X body model, compared to the more natural results of existing 3D digital human assets. To ensure that the generated motions retarget well to the 3D asset, we carefully adjust the shifts of the translations of the pelvis joint and the rotations of 21 body joints and 30 hand joints between the SMPL-X body model (all generated marker points are converted to SMPL-X parameters using our marker-to-SMPLX regressor) and the 3D digital asset. This process is performed using the Keemap  (Keeline,  2023 )  package in Blender. The retargeted results are shown in Fig.  21 .",
            "As discussed in Section  3.2 , our marker-based body representation allows the generator to utilize data from various sources and formats. Here, we explore how this capability affects the quality of generated motions. Table  8  presents results exploring how this capability influences the quality of generated motions. We observe that incorporating more data from Inter-X improves physical metrics, while R-Precision and MM Dist metrics suffer. We attribute this to the differing distributions between Inter-X and InterHuman data, which may cause the motion encoder trained on InterHuman to perform suboptimally on Inter-X, thus affecting these evaluation metrics.",
            "Figures  22  and  23  present additional visual comparisons of different human-human interaction modules. Fig.  24  and Fig.  25  showcases further visual generation results guided by long plots."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparisons between our marker format generator and other human-human interaction generators on the InterHuman dataset.  Following InterGen, we run all evaluations 20 times.   plus-or-minus \\pm   indicates the 95% confidence interval. Bold indicates the best result. * denotes our replication results, while other compared results are cited from InterGen. The compared variant of our method here only uses the marker format without additional conditions or losses mentioned in Section  3.2 .",
        "table": "A5.T3.75.73",
        "footnotes": [],
        "references": [
            "To address the collision problem, we propose a self-supervised strategy to incorporate 3D scene information into existing human-human interaction data by synthesizing Signed Distance Function (SDF) points (pipeline shown in Fig.  3 ). Specifically, we preprocess existing human-human interaction data to obtain the mesh of their motion sequences, denoted as  M  ( X ) = { M  ( x 1 ) , M  ( x 2 ) , ... , M  ( x N ) } M X M subscript x 1 M subscript x 2 ... M subscript x N M(\\bm{X})=\\{M(\\bm{x}_{1}),M(\\bm{x}_{2}),...,M(\\bm{x}_{N})\\} italic_M ( bold_italic_X ) = { italic_M ( bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_M ( bold_italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , ... , italic_M ( bold_italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ) } , where  M  (  ) M  M(\\cdot) italic_M (  )  denotes the transformation from body representation  x i subscript x i \\bm{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  to body mesh (achieved using the SMPL/SMPL-X parameters  (Loper et al.,  2015 ; Pavlakos et al.,  2019 )  provided in ground-truth motion data). We then project the vertices  V V V italic_V  of this sequence mesh to the ground plane and calculate the convex hull  H H H italic_H  of the projected 2D points, representing the truly walkable region in the ground 2D space. Next, we construct a  S h  o  r  S h  o  r  S v  e  r subscript S h o r subscript S h o r subscript S v e r S_{hor}\\times S_{hor}\\times S_{ver} italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT  italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT  italic_S start_POSTSUBSCRIPT italic_v italic_e italic_r end_POSTSUBSCRIPT  grid of 3D points ( P  R S h  o  r  S h  o  r  S v  e  r P superscript R subscript S h o r subscript S h o r subscript S v e r P\\in\\mathbb{R}^{S_{hor}\\times S_{hor}\\times S_{ver}} italic_P  blackboard_R start_POSTSUPERSCRIPT italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT  italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT  italic_S start_POSTSUBSCRIPT italic_v italic_e italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ), where  S h  o  r subscript S h o r S_{hor} italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT  and  S v  e  r subscript S v e r S_{ver} italic_S start_POSTSUBSCRIPT italic_v italic_e italic_r end_POSTSUBSCRIPT  denote the horizontal and vertical dimensions, respectively, with the grid center aligned with the pelvis position of one character. These points have values of either 1 or -1, representing outside scene objects and inside them, thus forming binary SDF points. Initially, all points are set to 1, indicating all regions are walkable. We then set the points  { P f  l  o  o  r , P c  e  i  l  i  n  g } subscript P f l o o r subscript P c e i l i n g \\{P_{floor},P_{ceiling}\\} { italic_P start_POSTSUBSCRIPT italic_f italic_l italic_o italic_o italic_r end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_c italic_e italic_i italic_l italic_i italic_n italic_g end_POSTSUBSCRIPT }  representing the floor and ceiling to -1:  { P f  l  o  o  r , P c  e  i  l  i  n  g } = { ( p x , p y , p z )  p z  T f  l  o  o  r  p z  T c  e  i  l  i  n  g } subscript P f l o o r subscript P c e i l i n g conditional-set subscript p x subscript p y subscript p z subscript p z subscript T f l o o r subscript p z subscript T c e i l i n g \\{P_{floor},P_{ceiling}\\}=\\{(p_{x},p_{y},p_{z})\\mid p_{z}\\leq T_{floor}\\vee p_%  {z}\\geq T_{ceiling}\\} { italic_P start_POSTSUBSCRIPT italic_f italic_l italic_o italic_o italic_r end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_c italic_e italic_i italic_l italic_i italic_n italic_g end_POSTSUBSCRIPT } = { ( italic_p start_POSTSUBSCRIPT italic_x end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT )  italic_p start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT  italic_T start_POSTSUBSCRIPT italic_f italic_l italic_o italic_o italic_r end_POSTSUBSCRIPT  italic_p start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT  italic_T start_POSTSUBSCRIPT italic_c italic_e italic_i italic_l italic_i italic_n italic_g end_POSTSUBSCRIPT } , where  T f  l  o  o  r subscript T f l o o r T_{floor} italic_T start_POSTSUBSCRIPT italic_f italic_l italic_o italic_o italic_r end_POSTSUBSCRIPT  and  T c  e  i  l  i  n  g subscript T c e i l i n g T_{ceiling} italic_T start_POSTSUBSCRIPT italic_c italic_e italic_i italic_l italic_i italic_n italic_g end_POSTSUBSCRIPT  are the heights of the ground and ceiling.",
            "Hand Pose Retrieval.  The default configuration of the 67 marker points  (Loper et al.,  2014 )  includes only two markers at the wrist positions, which limits the expressiveness of human hand motions. To enhance this aspect, we employ a postprocessing step that retrieves hand poses from a dataset. Specifically, we utilize CLIP  (Radford et al.,  2021 )  to encode all the prompts in the Inter-X dataset  (Xu et al.,  2024 )  (which is in SMPL-X format, including hand poses) during a pre-encoding phase. In the inference phase, we encode the guided text similarly and compute the cosine similarity between the embedding of the inference text and the embeddings from the dataset, selecting the motion clip with the highest similarity. Next, we perform either random segmentation or upsampling on the selected motion clip to align its frames with the generated motion sequence. These retrieved hand poses are then integrated into the generated motion sequence. The effectiveness of this approach is demonstrated in results displayed in Appendix  C.3 .",
            "Comparisons of Human-Human Interaction Generation on Synthetic Implicit 3D Scenes.  We further isolate the human-human interaction module to evaluate its performance on our synthetic implicit 3D scenes, as described in Section  3.2 . We primarily focus on physical compliance metrics. For metrics related to motion diversity, alignment with guidance text, and other aspects, please refer to Appendix  E . From Table  2 , we observe that our human-human interaction module surpasses the other two methods in all physical metrics, validating the effectiveness of our design.",
            "C.3: Hand Pose Retrieval   C.3",
            "For the SDF grid points, given their dimension of  R 4  S h  o  r  S h  o  r  S v  e  r superscript R 4 subscript S h o r subscript S h o r subscript S v e r \\mathbb{R}^{4\\times S_{hor}\\times S_{hor}\\times S_{ver}} blackboard_R start_POSTSUPERSCRIPT 4  italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT  italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT  italic_S start_POSTSUBSCRIPT italic_v italic_e italic_r end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  as mentioned in Section  3.2 , we first process them with three 3D convolution layers, each with a stride of 2, reducing the dimensionality to  R 4  S h  o  r 8  S h  o  r 8  S v  e  r 8 superscript R 4 subscript S h o r 8 subscript S h o r 8 subscript S v e r 8 \\mathbb{R}^{4\\times\\frac{S_{hor}}{8}\\times\\frac{S_{hor}}{8}\\times\\frac{S_{ver}%  }{8}} blackboard_R start_POSTSUPERSCRIPT 4  divide start_ARG italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT end_ARG start_ARG 8 end_ARG  divide start_ARG italic_S start_POSTSUBSCRIPT italic_h italic_o italic_r end_POSTSUBSCRIPT end_ARG start_ARG 8 end_ARG  divide start_ARG italic_S start_POSTSUBSCRIPT italic_v italic_e italic_r end_POSTSUBSCRIPT end_ARG start_ARG 8 end_ARG end_POSTSUPERSCRIPT  to conserve memory. We then flatten and encode the result using an MLP layer to match the embedding space. The encoded marker embedding and SDF embedding are directly added to the text embedding and timestep embedding. The combined embedding is then utilized to guide the generation by influencing the AdaLN layers scale and shift factors within the transformer blocks of the diffusion model, as described in  Peebles & Xie ( 2022 ) .",
            "Given a long plot context, through our plot comprehension module (details in Section  3.3  and Appendix  C.1 ), the human locomotion generation module receives two kinds of instructions:  None None \\mathrm{None} roman_None  (indicating a random route point in the scene) or  [ ObjectName ] delimited-[] ObjectName \\mathrm{[ObjectName]} [ roman_ObjectName ]  (indicating a route point near a specific scene object). For  None None \\mathrm{None} roman_None , the point sampling is random and unconstrained, whereas for  [ ObjectName ] delimited-[] ObjectName \\mathrm{[ObjectName]} [ roman_ObjectName ] , sampling occurs within a larger 3D bounding box around the object. To ensure the sampling result is not within unwalkable regions ( e.g. , inside the object), the system pre-bakes a 2D navigation plane indicating walkable areas. Illegal points are filtered out, and sampling repeats if necessary. For sampling around objects, the 3D box scope dynamically increases to avoid dead loops. If multiple objects share the same  ObjectName ObjectName \\mathrm{ObjectName} roman_ObjectName , one is selected at random. Thus, for a locomotion instruction like  [ None , Chair , Table ] None Chair Table [\\mathrm{None},\\mathrm{Chair},\\mathrm{Table}] [ roman_None , roman_Chair , roman_Table ] , the system process is as shown in Fig.  12 . This module also aids human-human interaction generation by ensuring that the two characters are not too far apart when initiating interaction. To achieve this, an additional route point is added around one character, compelling them to move closer to the other, resulting in more cohesive and natural interaction.",
            "For random plot generation based on the input 3D scene information, the prompt fed into the language model is shown in Fig.  13 . By providing examples of expected outputs and passing the category names of objects present in the 3D scene, the language model functions effectively as a scripter, returning satisfactory orders. These orders include  [ None , ObjectName ] None ObjectName [\\mathrm{None},\\mathrm{ObjectName}] [ roman_None , roman_ObjectName ]  for human locomotion generation,  [ MotionType ] delimited-[] MotionType [\\mathrm{MotionType}] [ roman_MotionType ]  for human-scene interaction generation, and orders prefixed with HHI: such as  [ HHI : Two  persons  hug  each  other  ... ] delimited-[] : HHI Two persons hug each other ... [\\mathrm{HHI:~{}Two~{}persons~{}hug~{}each~{}other~{}...}] [ roman_HHI : roman_Two roman_persons roman_hug roman_each roman_other ... ] . An example of the language models generation result is shown in Fig.  14 .",
            "To ensure consistency between motions from different modules, we adopt the Slerp technique  Shoemake ( 1985 ) . This is primarily applied to transitions between other motions and human-human interactions, or vice versa, where there can be noticeable motion jumps despite conditioning the generation on the last frame (as mentioned in Section  3.2 ). Specifically, we designate the first and last four frames of generated human-human interactions as buffer areas for interpolation, using Slerp for rotation interpolation and linear interpolation for translation. This results in smoother motion transitions.",
            "Fig.  17  displays a hand motion sequence before and after retrieval. To ensure the consistency of hand motion, we also apply Slerp interpolation, as detailed in Appendix  C.3 .",
            "The collision revision module of our system primarily functions as a post-processing step. Since the human-human interaction generator has taken the other characters position into account during its training and with marker condition (see Section  3.2 ), the collision revision here focuses on human-human collisions that occur in the results from the human locomotion generation module and the human-scene interaction generation module. These modules do not consider other characters positions during motion generation, leading to frequent human-human collisions in their results.",
            "The core solution of the collision revision module is to adjust the speed of one characters motion and slow down the other characters motion to avoid collisions. The strategy is as follows: suppose there are two sequences,  Seq A subscript Seq A \\mathrm{Seq}_{A} roman_Seq start_POSTSUBSCRIPT italic_A end_POSTSUBSCRIPT  and  Seq B subscript Seq B \\mathrm{Seq}_{B} roman_Seq start_POSTSUBSCRIPT italic_B end_POSTSUBSCRIPT , for two characters, respectively. The sequence length is identical and denoted as  L L L italic_L . To determine the collided frames, we use  L h  u  m  a  n  P  e  n  e subscript L h u m a n P e n e \\mathcal{L}_{humanPene} caligraphic_L start_POSTSUBSCRIPT italic_h italic_u italic_m italic_a italic_n italic_P italic_e italic_n italic_e end_POSTSUBSCRIPT  mentioned in Section  3.2  to identify the collided frames (a threshold is set to filter out frames with trivial human-human collisions). Suppose the collided sequences are  COL = { [ col s1 , col e1 ] , [ col s2 , col e2 ] , ... , [ col sN , col eN ] } COL subscript col s1 subscript col e1 subscript col s2 subscript col e2 ... subscript col sN subscript col eN \\mathrm{COL}=\\{[\\mathrm{col_{s1}},\\mathrm{col_{e1}}],[\\mathrm{col_{s2}},%  \\mathrm{col_{e2}}],...,[\\mathrm{col_{sN}},\\mathrm{col_{eN}}]\\} roman_COL = { [ roman_col start_POSTSUBSCRIPT s1 end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT e1 end_POSTSUBSCRIPT ] , [ roman_col start_POSTSUBSCRIPT s2 end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT e2 end_POSTSUBSCRIPT ] , ... , [ roman_col start_POSTSUBSCRIPT roman_sN end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT roman_eN end_POSTSUBSCRIPT ] } , where  [ col sn , col en ] subscript col sn subscript col en [\\mathrm{col_{sn}},\\mathrm{col_{en}}] [ roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT ]  denotes the start and end frame index of the  n t  h subscript n t h n_{th} italic_n start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT  collided sequence. We apply collision revision iteratively from the first collided sequence. For character A, we speed up its motions during  [ 0 , col sn + col en 2 ] 0 subscript col sn subscript col en 2 [0,\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2}] [ 0 , divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ]  by downsampling the frame length from  col sn + col en 2 subscript col sn subscript col en 2 \\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2} divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG  to  ( col sn + col en 2  col en  col sn 2 ) subscript col sn subscript col en 2 subscript col en subscript col sn 2 (\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2}-\\frac{\\mathrm{col_{en}}-\\mathrm%  {col_{sn}}}{2}) ( divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG - divide start_ARG roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT - roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ) , and slow down its motions during  [ col sn + col en 2 , L ] subscript col sn subscript col en 2 L [\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2},L] [ divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG , italic_L ]  by upsampling the frame length from  ( L  col sn + col en 2 ) L subscript col sn subscript col en 2 (L-\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2}) ( italic_L - divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG )  to  ( L  col sn + col en 2 + col en  col sn 2 ) L subscript col sn subscript col en 2 subscript col en subscript col sn 2 (L-\\frac{\\mathrm{col_{sn}}+\\mathrm{col_{en}}}{2}+\\frac{\\mathrm{col_{en}}-%  \\mathrm{col_{sn}}}{2}) ( italic_L - divide start_ARG roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT + roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG + divide start_ARG roman_col start_POSTSUBSCRIPT roman_en end_POSTSUBSCRIPT - roman_col start_POSTSUBSCRIPT roman_sn end_POSTSUBSCRIPT end_ARG start_ARG 2 end_ARG ) .",
            "In our human-human interaction module, the size of the SDF points box was set to  3  3  3 3 3 3 3\\times 3\\times 3 3  3  3  meters, consisting of  128  128  128 128 128 128 128\\times 128\\times 128 128  128  128  points. The ground threshold  T f  l  o  o  r subscript T f l o o r T_{floor} italic_T start_POSTSUBSCRIPT italic_f italic_l italic_o italic_o italic_r end_POSTSUBSCRIPT  was set to the height of the ground in the raw data, while the ceiling threshold  T c  e  i  l  i  n  g subscript T c e i l i n g T_{ceiling} italic_T start_POSTSUBSCRIPT italic_c italic_e italic_i italic_l italic_i italic_n italic_g end_POSTSUBSCRIPT  was randomized between the maximum height of the motion body meshes and the top height of the SDF box. The number of implicitly synthesized objects,  K K K italic_K , is uniformly chosen at random from the range [0,10]. The loss weights for each function in Eq.  3  were determined through hyperparameter tuning to achieve optimal performance, resulting in values of 1, 3, 0.001, 30, 30, 0.01, 3, 1, 1, respectively. We conducted training over 1,000 epochs in  Phase1 Phase1 \\mathrm{Phase}1 Phase1  with a batch size of 20 per GPU and a learning rate of 1e -4 . In  Phase2 Phase2 \\mathrm{Phase}2 Phase2 , we trained for 500 epochs using the same batch size but reduced the learning rate to 1e -5 . For  Phase3 Phase3 \\mathrm{Phase}3 Phase3 , we decreased the batch size to 6 per GPU due to increased memory costs from  L h  u  m  a  n  P  e  n  e subscript L h u m a n P e n e \\mathcal{L}_{humanPene} caligraphic_L start_POSTSUBSCRIPT italic_h italic_u italic_m italic_a italic_n italic_P italic_e italic_n italic_e end_POSTSUBSCRIPT  and further lowered the learning rate to 1e -6 . Training was performed using 4 RTX 4090 GPUs, totaling 384 GPU hours for training on InterGen, and extended to 1000 GPU hours when incorporating training on Inter-X.",
            "Table  3  presents additional metrics evaluating the motion quality and diversity of different human-human interaction modules, complementing the physics compliance metrics discussed in the main paper. To test the feasibility of using marker points as a human-human interaction representationdistinct from InterGen  (Liang et al.,  2024 ) we removed the conditions from our method and aligned the training settings, such as the number of epochs, with InterGen for a fair comparison. The results indicate that switching to the marker points format maintains motion quality and diversity, with metrics close to those of InterGen, thus validating the feasibility of using marker points as a motion representation. We also attempted to adapt ComMDM  (Shafir et al.,  2024 )  to a larger human-human interaction dataset, but it did not yield significant improvements in the metrics. We attribute this to ComMDMs heavy reliance on the pretrained MDM  (Tevet et al.,  2023 )  model, where most layers were frozen, making it difficult to capture the distribution of a larger dataset.",
            "As discussed in Section  3.2 , our marker-based body representation allows the generator to utilize data from various sources and formats. Here, we explore how this capability affects the quality of generated motions. Table  8  presents results exploring how this capability influences the quality of generated motions. We observe that incorporating more data from Inter-X improves physical metrics, while R-Precision and MM Dist metrics suffer. We attribute this to the differing distributions between Inter-X and InterHuman data, which may cause the motion encoder trained on InterHuman to perform suboptimally on Inter-X, thus affecting these evaluation metrics.",
            "Figures  22  and  23  present additional visual comparisons of different human-human interaction modules. Fig.  24  and Fig.  25  showcases further visual generation results guided by long plots."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Comparisons between different canonicalization strategies.  A represents the character centered around the origin, while B represents the character positioned relative to A. R-P T1 denotes R-Precision Top 1. The best results are highlighted in bold.",
        "table": "A5.T4.9.9",
        "footnotes": [],
        "references": [
            "Data Canonicalization.  In InterGen  (Liang et al.,  2024 ) , data canonicalization is achieved by moving one character to the global coordinate origin, aligning the first frames direction along the Y-axis, and adaptively transforming the second characters position and rotation.  E.g. , given the joint positions  j g p  R 22  3 superscript subscript j g p superscript R 22 3 \\bm{j}_{g}^{p}\\in\\mathbb{R}^{22\\times 3} bold_italic_j start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT 22  3 end_POSTSUPERSCRIPT  (see Appendix  A  for definitions), the positions  { j g p } N A subscript superscript superscript subscript j g p A N \\{\\bm{j}_{g}^{p}\\}^{A}_{N} { bold_italic_j start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT } start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT  of character A are centered around the origin, while character Bs positions  { j g p } N B superscript subscript superscript subscript j g p N B \\{\\bm{j}_{g}^{p}\\}_{N}^{B} { bold_italic_j start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_p end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT , relative to character A, are more dispersed. Our experiments (see Table  4 ) reveal that this approach introduces a bias: the character near the origin is easier to model due to its concentrated joint positions, while the other character, with more dispersed positions, is harder to learn. This issue is even more pronounced in our marker-based representation, where we must model the global positions of character Bs 67 markers (see Appendix  B.1 ).",
            "To address this, we propose a new canonicalization method that reduces the complexity of learning divergent distributions, as shown in Fig.  4 . We set each characters pelvis joint as its local origin and adjust their marker positions relative to that. This approach concentrates both characters marker distributions around their local origins, leaving only the pelvis joints global transition as the primary divergent distribution to the model. This adjustment simplifies the learning process, reducing the challenge from 67 divergent distributions to just one,  i.e. , character Bs pelvis joint position. The final motion frame representation of character B is then  x i  R ( 67 + 1 )  3 subscript x i superscript R 67 1 3 \\bm{x}_{i}\\in\\mathbb{R}^{(67+1)\\times 3} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT ( 67 + 1 )  3 end_POSTSUPERSCRIPT , consisting of 67 marker positions and one pelvis joint position.",
            "To mitigate collisions, we instead propose a postprocessing collision revision module. Specifically, we identify frames in the human locomotion results where collisions with other characters occur. Subsequently, we adjust the speed of the human motion by either downsampling or upsampling frames. This adjustment helps in avoiding collisions more effectively. For detailed methodologies, please refer to Appendix  C.4 .",
            "C.4: Collision Revision   C.4",
            "For random plot generation based on the input 3D scene information, the prompt fed into the language model is shown in Fig.  13 . By providing examples of expected outputs and passing the category names of objects present in the 3D scene, the language model functions effectively as a scripter, returning satisfactory orders. These orders include  [ None , ObjectName ] None ObjectName [\\mathrm{None},\\mathrm{ObjectName}] [ roman_None , roman_ObjectName ]  for human locomotion generation,  [ MotionType ] delimited-[] MotionType [\\mathrm{MotionType}] [ roman_MotionType ]  for human-scene interaction generation, and orders prefixed with HHI: such as  [ HHI : Two  persons  hug  each  other  ... ] delimited-[] : HHI Two persons hug each other ... [\\mathrm{HHI:~{}Two~{}persons~{}hug~{}each~{}other~{}...}] [ roman_HHI : roman_Two roman_persons roman_hug roman_each roman_other ... ] . An example of the language models generation result is shown in Fig.  14 .",
            "Canonicalization Strategies.  We compare the performance of different canonicalization strategies as illustrated in Fig.  4 . The experiments were conducted using training for  Phase1 Phase1 \\mathrm{Phase}1 Phase1  and  Phase2 Phase2 \\mathrm{Phase}2 Phase2  ( Phase3 Phase3 \\mathrm{Phase}3 Phase3  was excluded as it does not impact the conclusions and helps reduce memory usage). From the results in Table  4 , it is evident that the initial canonicalization strategy introduced significant bias, with the human-scene perturbation metric values of Character A and B being 0.508 and 4.791, respectively. After transitioning to the improved canonicalization strategy, most metrics showed enhanced performance, and the bias was largely mitigated, with values of 0.443 and 1.471 for the human-scene perturbation metric. This demonstrates the effectiveness of the improved canonicalization approach.",
            "Figures  22  and  23  present additional visual comparisons of different human-human interaction modules. Fig.  24  and Fig.  25  showcases further visual generation results guided by long plots."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Ablation study of the effectiveness of motion and SDF conditions.  Conditions are added cumulatively rather than individually. The best results are highlighted in bold.",
        "table": "A6.T5.9.9",
        "footnotes": [],
        "references": [
            "Self-Supervised SDF Condition for Scene-Aware Collision Avoidance.  Prior human-human interaction generation methods, such as InterGen, do not consider the 3D scene information surrounding characters in the formulation of its denoiser  D  (  ) D  D(\\cdot) italic_D (  ) . If human-human interaction motions generated by InterGen are directly applied to a 3D scene, severe collisions between characters and scene objects can occur (as shown in Fig.  5 ). A straightforward solution is to introduce additional 3D scene conditions into the network. However, there is currently a lack of human-human interaction data with 3D scene information, and collecting such data is costly.",
            "Motion Retargeting.  The original SMPL/SMPL-X body mesh lacks color attributes or patterns. While applying a texture map can enhance naturalness, the smooth surface of the mesh can still yield unrealistic results for features like hair or protruding parts such as clothes. To achieve greater realism, we utilize 3D character assets provided by Mixamo  (Adobe,  2015 )  and employ the Keemap  (Keeline,  2023 )  package within Blender for retargeting SMPL/SMPL-X motions to existing high-quality 3D characters. More details can be found in Appendix  C.5 .",
            "Fig.  5  provides visual comparisons of different human-human interaction modules. It is evident that without surrounding scene awareness, the other two models tend to penetrate the 3D environment, whereas our model better avoids such issues. The figure also highlights the ablation of the SDF condition (detailed in Appendix  F ), which fails to prevent penetration, thus confirming the effectiveness of our design. Additionally, Fig.  6  illustrates the complete system pipeline guided by long plots, showcasing our systems capability to support various types of human motion generation. Further visual comparisons and pipeline generation results are available in Appendix  H . For dynamic video views of the generated motions and comparisons, please refer to our  Supplementary Videos .",
            "C.5: Motion Retargeting   C.5",
            "Due to the inherent stochasticity of the language model, the extracted orders sometimes fail to meet the specifications required by the generation modules. Therefore, we utilize the language model again to revise the initial orders, adhering to a series of established rules shown in Fig.  15 . This additional revision process ensures that the instructional orders are more satisfactory. An example of the language models generation result is shown in Fig.  16 . In practice, we also append this rule prompt after the generation prompt in random plot generation to ensure successful output.",
            "Motion and SDF Conditions.  In Table  5 , we assess the impact of motion and SDF conditions on the human-human interaction module. Without these conditions, there is a notable increase in human-scene collisions, with an HSP value of 15.897. When the conditions are applied, there is a clear improvement in physics-consistency performance, though this comes at the cost of reduced diversity (Diversity and MModality) in the generated human motions due to the constraints imposed by the conditions. Additionally, while the SDF condition improves human-scene collision mitigation, the motion condition also contributes positively. We attribute this benefit to the first-frame motion conditions role in defining the range of subsequent motions, which helps prevent the generation of extreme motions that could lead to significant collisions.",
            "Figures  22  and  23  present additional visual comparisons of different human-human interaction modules. Fig.  24  and Fig.  25  showcases further visual generation results guided by long plots."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Ablation study of the effectiveness of different conditions.  Losses are added cumulatively rather than individually. Base Losses refers to losses excluding collision-related terms as shown in Eq.  3 . The best results are highlighted in bold.",
        "table": "A6.T6.13.13",
        "footnotes": [],
        "references": [
            "Fig.  5  provides visual comparisons of different human-human interaction modules. It is evident that without surrounding scene awareness, the other two models tend to penetrate the 3D environment, whereas our model better avoids such issues. The figure also highlights the ablation of the SDF condition (detailed in Appendix  F ), which fails to prevent penetration, thus confirming the effectiveness of our design. Additionally, Fig.  6  illustrates the complete system pipeline guided by long plots, showcasing our systems capability to support various types of human motion generation. Further visual comparisons and pipeline generation results are available in Appendix  H . For dynamic video views of the generated motions and comparisons, please refer to our  Supplementary Videos .",
            "Due to the inherent stochasticity of the language model, the extracted orders sometimes fail to meet the specifications required by the generation modules. Therefore, we utilize the language model again to revise the initial orders, adhering to a series of established rules shown in Fig.  15 . This additional revision process ensures that the instructional orders are more satisfactory. An example of the language models generation result is shown in Fig.  16 . In practice, we also append this rule prompt after the generation prompt in random plot generation to ensure successful output.",
            "Loss Functions.  We assess the effectiveness of several key loss functions in our human-human interaction module:  L s  c  e  n  e  P  e  n  e subscript L s c e n e P e n e \\mathcal{L}_{scenePene} caligraphic_L start_POSTSUBSCRIPT italic_s italic_c italic_e italic_n italic_e italic_P italic_e italic_n italic_e end_POSTSUBSCRIPT ,  L s  c  e  n  e  R  e  g subscript L s c e n e R e g \\mathcal{L}_{sceneReg} caligraphic_L start_POSTSUBSCRIPT italic_s italic_c italic_e italic_n italic_e italic_R italic_e italic_g end_POSTSUBSCRIPT ,  L h  u  m  a  n  P  e  n  e subscript L h u m a n P e n e \\mathcal{L}_{humanPene} caligraphic_L start_POSTSUBSCRIPT italic_h italic_u italic_m italic_a italic_n italic_P italic_e italic_n italic_e end_POSTSUBSCRIPT , and  L h  u  m  a  n  R  e  g subscript L h u m a n R e g \\mathcal{L}_{humanReg} caligraphic_L start_POSTSUBSCRIPT italic_h italic_u italic_m italic_a italic_n italic_R italic_e italic_g end_POSTSUBSCRIPT . The comparisons in Table  6  demonstrate the benefits of incorporating human-scene and human-human collision-related losses, as indicated by improved physical compliance metrics. However, we observe that the human-human collision-related loss negatively impacts R-Precision, MM Dist, and FID scores. This degradation can be attributed to the inclusion of noise in the training data, where characters occasionally collide. The additional penalty from the human-human collision loss can cause a mismatch between the generated motion distribution and the training data distribution, leading to higher FID scores. Additionally, since R-Precision and MM Dist evaluations rely on motion encoders trained on the initial data, biased generated results may not be well encoded, affecting these metrics."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Ablation study of different training strategies.  + denotes phased training, while & denotes merged training phases. The best results are highlighted in bold.",
        "table": "A6.T7.9.9",
        "footnotes": [],
        "references": [
            "Additionally, InterGen treats human-human interaction generation as commutative, meaning that  { X A , X B } superscript X A superscript X B \\{\\bm{X}^{A},\\bm{X}^{B}\\} { bold_italic_X start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT , bold_italic_X start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT }  (where  A A A italic_A  and  B B B italic_B  denote different characters) and  { X B , X A } superscript X B superscript X A \\{\\bm{X}^{B},\\bm{X}^{A}\\} { bold_italic_X start_POSTSUPERSCRIPT italic_B end_POSTSUPERSCRIPT , bold_italic_X start_POSTSUPERSCRIPT italic_A end_POSTSUPERSCRIPT }  are considered equivalent despite their different orders. Based on this, InterGen designed an interactively cooperative transformer-style network where motions  A A A italic_A  and  B B B italic_B  are generated by the same weight-sharing denoiser  D  (  ) D  D(\\cdot) italic_D (  ) , conditioned on timestep  t t t italic_t , hidden states of the counterpart  h h \\bm{h} bold_italic_h , and text guidance  c c c italic_c . The denoising step for motion  A A A italic_A  is formulated as  D  ( x ( t )  A , h , t , c ) D superscript x t A h t c D(\\bm{x}^{(t)A},\\bm{h},t,c) italic_D ( bold_italic_x start_POSTSUPERSCRIPT ( italic_t ) italic_A end_POSTSUPERSCRIPT , bold_italic_h , italic_t , italic_c ) , where  x ( t )  A superscript x t A \\bm{x}^{(t)A} bold_italic_x start_POSTSUPERSCRIPT ( italic_t ) italic_A end_POSTSUPERSCRIPT  represents the  t t  h subscript t t h t_{th} italic_t start_POSTSUBSCRIPT italic_t italic_h end_POSTSUBSCRIPT  denoising steps result. While this formulation achieves generally good results, we observed that due to the ambiguity in person order during inference (both start from random Gaussian noise), the two characters motions sometimes collapse into a single overlapped motion. Examples of this issue are shown in Fig.  7 .",
            "Fig.  17  displays a hand motion sequence before and after retrieval. To ensure the consistency of hand motion, we also apply Slerp interpolation, as detailed in Appendix  C.3 .",
            "Training Strategies.  We examine the necessity of the three-step training process for the human-human interaction generator. Table  7  presents the results of different training strategies. The results indicate a significant deterioration in generation quality when combining all training phases. Specifically, the combined training phases lead to a marked increase in FID (exceeding 22) and a decrease in R-Precision. We attribute these quality degradations to the early introduction of physical constraints, which restrict the generators motion learning capability. This constraint compels the generator to consistently avoid collisions, resulting in overly cautious motions with subtle movements or even static poses. This underscores the importance of the phased training approach."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Explorations of different data scales.  The best results are highlighted in bold.",
        "table": "A6.T8.9.9",
        "footnotes": [],
        "references": [
            "First Frame Marker Condition and SDF Condition.  Fig.  8  illustrates how the first frame marker condition and SDF points are integrated into the generator. For the frame condition, we use a GRU (Gated Recurrent Unit) layer to encode the input into the same embedding dimension as the text embedding and timestep embedding. Although we only use one frame condition in this paper, the GRU layer allows for the potential to condition the generation on a sequence of frames, enhancing the networks extensibility.",
            "Then, we apply a similar process to character B, but slow down its former motions and speed up its latter motions. By slowing down/speeding up the former part and speeding up/slowing down the latter part, the total sequence length  L L L italic_L  remains unchanged. We then re-calculate  L h  u  m  a  n  P  e  n  e subscript L h u m a n P e n e \\mathcal{L}_{humanPene} caligraphic_L start_POSTSUBSCRIPT italic_h italic_u italic_m italic_a italic_n italic_P italic_e italic_n italic_e end_POSTSUBSCRIPT  to check if the human-human collision is alleviated. If so, new collision sequences  COL COL \\mathrm{COL} roman_COL  are identified, and the process starts from the first new sequence; otherwise, we move to the next collided sequence  [ col s  ( n + 1 ) , col e  ( n + 1 ) ] subscript col s n 1 subscript col e n 1 [\\mathrm{col_{s(n+1)}},\\mathrm{col_{e(n+1)}}] [ roman_col start_POSTSUBSCRIPT roman_s ( roman_n + 1 ) end_POSTSUBSCRIPT , roman_col start_POSTSUBSCRIPT roman_e ( roman_n + 1 ) end_POSTSUBSCRIPT ] . In this way, the system effectively alleviates human-human collisions among human locomotion or human-scene interaction results. Fig.  18  shows the results before and after applying the collision revision module. However, it should be noted that the collision revision will fail when there is no space for changing motion lanes, for example the narrow aisle as shown in Fig.  19 .",
            "As discussed in Section  3.2 , our marker-based body representation allows the generator to utilize data from various sources and formats. Here, we explore how this capability affects the quality of generated motions. Table  8  presents results exploring how this capability influences the quality of generated motions. We observe that incorporating more data from Inter-X improves physical metrics, while R-Precision and MM Dist metrics suffer. We attribute this to the differing distributions between Inter-X and InterHuman data, which may cause the motion encoder trained on InterHuman to perform suboptimally on Inter-X, thus affecting these evaluation metrics."
        ]
    },
    "global_footnotes": []
}