{
    "id_table_1": {
        "caption": "TABLE I:  Performance comparisons with 6 explicit approaches: Test accuracy (%) and the drop (   \\nabla  ) with respect to baseline under different privacy budget    \\varepsilon italic_  (  = 10  5  superscript 10 5 \\delta=10^{-5} italic_ = 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT , BL: Baseline).",
        "table": "S3.E1",
        "footnotes": [],
        "references": [
            "Compared to traditional learning solutions that directly access to private data and lead to privacy leakage in released model (in red in Fig.  1 ), the privacy-preserving learning solutions usually add privacy protection strategies or avoid released model (in green in Fig.  1 ) directly access to private data during training. Towards this end, many existing approaches have been proposed, which are mainly based on differential privacy  [ 11 ] . According to the privacy-preserving strategy, they can be roughly grouped into two categories: the implicit category and explicit category.",
            "Inspired by this fact, we propose a teacher-student learning approach to train privacy-preserving student networks via discriminative-generative distillation, which applies  discriminative and generative models  to distill private knowledge and then explores  generated synthetic data  to perform knowledge transfer (Fig.  1 ). The objective is to enable an effective learning that achieves a promising trade-off between high model utility and strong privacy protection. As shown in Fig.  2 , the student is trained by using two streams. First, discriminative stream trains a baseline classifier on all private data and an ensemble of separate teachers on disjoint subsets, while generative stream takes the baseline classifier as a fixed discriminator and trains a generator in a data-free manner. Massive synthetic data are then generated with the generator and used to train a variational autoencoder (VAE)  [ 25 ] . After that, a few of the synthetic data are fed into the teacher ensemble to query labels with Laplacian aggregation, while most of the synthetic data are fed into VAE to achieve massive data triples by perturbing the latent codes. Finally, a semi-supervised learning is performed by simultaneously handling two tasks: knowledge transfer via supervised data classification, and knowledge enhancement via self-supervised model regularization.",
            "We can see that the risk of privacy leakage can be effectively suppressed due to the isolation between the released student model and private data. The supervised energy term can enforce knowledge transfer on the class-related characteristics, while the unsupervised energy term performs self-supervised regularization to enhance knowledge. Towards this end, we solve Eq. ( 1 ) via three steps, including: 1) data-free generator learning to get synthetic data  D ^ ^ D \\hat{\\mathcal{D}} over^ start_ARG caligraphic_D end_ARG  and train a VAE  {  e ,  d } subscript italic- e subscript italic- d \\{\\phi_{e},\\phi_{d}\\} { italic_ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT } , 2) teacher ensemble learning to achieve the labels  L ^ s subscript ^ L s \\hat{\\mathcal{L}}_{s} over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  by differentially private aggregation, and 3) semi-supervised student learning to get  W s subscript W s \\mathbb{W}_{s} blackboard_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT .",
            "where the mapping operator  M  (  ) M  \\mathcal{M}(\\cdot) caligraphic_M (  )  projects the code into decoder input,  n j  superscript subscript n j \\mathbf{n}_{j}^{*} bold_n start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is random perturbation noise along tangent direction (  =  *=\\parallel  =  ) or normal direction (   =   perpendicular-to *=\\perp  =  ). Then, the semi-supervised student learning is performed with  { D ^ s , L ^ s } subscript ^ D s subscript ^ L s \\{\\hat{\\mathcal{D}}_{s},\\hat{\\mathcal{L}}_{s}\\} { over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT , over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT }  and  T T \\mathcal{T} caligraphic_T . The supervised energy in Eq. ( 1 ) can be formulated as"
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Test accuracy comparisons with 7 implicit approaches on CIFAR10 under different    \\varepsilon italic_  (  = 10  5  superscript 10 5 \\delta=10^{-5} italic_ = 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT ).",
        "table": "S3.E2",
        "footnotes": [],
        "references": [
            "Inspired by this fact, we propose a teacher-student learning approach to train privacy-preserving student networks via discriminative-generative distillation, which applies  discriminative and generative models  to distill private knowledge and then explores  generated synthetic data  to perform knowledge transfer (Fig.  1 ). The objective is to enable an effective learning that achieves a promising trade-off between high model utility and strong privacy protection. As shown in Fig.  2 , the student is trained by using two streams. First, discriminative stream trains a baseline classifier on all private data and an ensemble of separate teachers on disjoint subsets, while generative stream takes the baseline classifier as a fixed discriminator and trains a generator in a data-free manner. Massive synthetic data are then generated with the generator and used to train a variational autoencoder (VAE)  [ 25 ] . After that, a few of the synthetic data are fed into the teacher ensemble to query labels with Laplacian aggregation, while most of the synthetic data are fed into VAE to achieve massive data triples by perturbing the latent codes. Finally, a semi-supervised learning is performed by simultaneously handling two tasks: knowledge transfer via supervised data classification, and knowledge enhancement via self-supervised model regularization.",
            "calculate the loss function with Eq. ( 2 ) on mini-batch:   i L  ( x i ) subscript i L subscript x i \\sum_{i}{\\mathcal{L}(\\mathbf{x}_{i})}  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT caligraphic_L ( bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S3.E6",
        "footnotes": [],
        "references": [
            "where  V k  (  ) subscript V k  \\mathcal{V}_{k}(\\cdot) caligraphic_V start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (  )  counts the votes of the query being predicted as class  k k k italic_k  by all  n n n italic_n  teachers, the final predicted label  l l l italic_l  is noisy and used to supervise the student training, a low privacy budget   0 subscript  0 \\varepsilon_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  is used to adjust privacy protection and  L  a  p  ( 2 /  0 ) L a p 2 subscript  0 Lap(2/\\varepsilon_{0}) italic_L italic_a italic_p ( 2 / italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  denotes the Laplacian distribution with location  0 0  and scale  2 /  0 2 subscript  0 2/\\varepsilon_{0} 2 / italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . For student training, each example from the query data  D ^ s subscript ^ D s \\hat{\\mathcal{D}}_{s} over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  is fed into the teacher ensemble and then the prediction is privately aggregated via Laplacian aggregation in Eq. ( 3 ), leading to  L ^ s = { l i } i = 1 | D ^ s | subscript ^ L s superscript subscript subscript l i i 1 subscript ^ D s \\hat{\\mathcal{L}}_{s}=\\{l_{i}\\}_{i=1}^{|\\hat{\\mathcal{D}}_{s}|} over^ start_ARG caligraphic_L end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = { italic_l start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | end_POSTSUPERSCRIPT . Directly using the maximum value of vote counts as labels may leak privacy, so we add random noise to the voting results to introduce ambiguity. Intuitively, this means that multiple teachers jointly determine the query result, making it difficult for adversary to recover the training data. In addition to this, our approach can provide the same or stronger privacy guarantee than many state-of-the-arts  [ 19 ,  23 ,  24 ,  66 ,  67 ,  45 ,  17 ,  18 ]  while reducing accuracy degradation by knowledge enhancement with an extra model regularization. It also means that our approach will have a less privacy cost when delivering student models with the same accuracy.",
            "Privacy Analysis.  According to the learning process in two streams, the total privacy budget contains two parts. Discriminative privacy budget is computed as PATE  [ 19 ,  21 ] , achieving   0 subscript  0 \\varepsilon_{0} italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT -differential privacy via Eq. ( 3 ) and getting  ( | D ^ s |   0 2 +  0   2  | D ^ s |  log   ,  )  limit-from subscript ^ D s superscript subscript  0 2 subscript  0 2 subscript ^ D s   (|\\hat{\\mathcal{D}}_{s}|\\varepsilon_{0}^{2}+\\varepsilon_{0}\\sqrt{-2|\\hat{% \\mathcal{D}}_{s}|\\log{\\delta}},\\delta)- ( | over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT square-root start_ARG - 2 | over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | roman_log italic_ end_ARG , italic_ ) - differential privacy over  | D ^ s | subscript ^ D s |\\hat{\\mathcal{D}}_{s}| | over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT |  queries for all    ( 0 , 1 )  0 1 \\delta\\in(0,1) italic_  ( 0 , 1 )   [ 69 ] . Generative privacy budget is computed according to the latent code perturbation in VAE construction. By taking the synthetic data in generative stream as a sequence, we achieve   1 subscript  1 \\varepsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -differential privacy by adding Laplacian noise with scale  2  c /  1 2 c subscript  1 2c/\\varepsilon_{1} 2 italic_c / italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  to the normalized latent codes, where  c c c italic_c  is the dimension of latent codes. It could be explained as follow. According to Laplacian mechanism and post-processing theorem  [ 69 ] , we have: for any two different images  x j subscript x j \\mathbf{x}_{j} bold_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  and  x j  superscript subscript x j  \\mathbf{x}_{j}^{\\prime} bold_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  as well as possible reconstructed output  x ^ j subscript ^ x j \\hat{\\mathbf{x}}_{j} over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , the VAE reconstruction mechanism  A A \\mathcal{A} caligraphic_A  satisfies  Pr  [ A  ( x j ) = x ^ j ]  exp  (  1 )  Pr  [ A  ( x j  ) = x ^ j ] Pr A subscript x j subscript ^ x j  subscript  1 Pr A superscript subscript x j  subscript ^ x j \\operatorname{Pr}[\\mathcal{A}(\\mathbf{x}_{j})=\\hat{\\mathbf{x}}_{j}]\\leq\\exp(% \\varepsilon_{1})\\cdot\\operatorname{Pr}\\left[\\mathcal{A}\\left(\\mathbf{x}_{j}^{% \\prime}\\right)=\\hat{\\mathbf{x}}_{j}\\right] roman_Pr [ caligraphic_A ( bold_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) = over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ]  roman_exp ( italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT )  roman_Pr [ caligraphic_A ( bold_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) = over^ start_ARG bold_x end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ]  where  Pr  [  ] Pr  \\operatorname{Pr}[\\cdot] roman_Pr [  ]  is the probability function. Then, we have the theorem.",
            "Label Query.  To study query effect on the trade-off between model accuracy and privacy protection, we compare the student learning under 27, 750, 1000 and 3000 queries. We treat the label of a generated example by the teacher ensemble as a query. The query number determines the privacy budget and failure probability, and we use differential privacy with moments accountant  [ 19 ]  as metric. More queries will cost a larger privacy budget and fixed query number will lead to constant privacy cost. The results are shown in Fig.  3 . They are as expected where a higher privacy budget leads to a higher model accuracy. Besides, in our approach, the private information that the delivered student can directly access is the noisy teachers prediction outputs who pass through Laplacian aggregation. The results also reveal that our student learning by dicriminative-generative distillation can be performed robustly and consistently under different label queries and providing a certain number of examples ( e.g. , 750) can lead to an impressive accuracy of 95.2%. It is very helpful in many practical applications where a few of samples are available for sharing. Therefore, our approach can effectively learn privacy-preserving student models and control accuracy drop."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S3.T1.12.12",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Generator Learning.  To study the effect of data generation, we conduct student learning on MNIST, FMNIST and CIFAR10 with the raw private data as well as synthetic data generated by four generative approaches, including ACGAN  [ 76 ] , WGAN  [ 46 ] , InfoGAN  [ 77 ]  and our Data-Free learned generator. The results are shown in Fig.  4  and some generated examples can be seen in Fig.  5 . It is easy to distinguish the images generated by ACGAN, WGAN and InfoGAN, implying that these generators learned with private data may expose data privacy in spite of achieving higher accuracy. By contrast, the synthetic images generated by data-free learned generator are hardly identified by human. Thus, the data-free learned generator effectively protects privacy while delivering comparable accuracy since it matches the distribution of private data in discriminative space."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S3.E7",
        "footnotes": [],
        "references": [
            "In this way, the synthetic data  D ^ ^ D \\hat{\\mathcal{D}} over^ start_ARG caligraphic_D end_ARG  generated by the learned generator have a similar distribution to private data without compromising privacy. Fig.  5  shows some examples. The synthetic data are very helpful for student learning, which can greatly improve accuracy compared to using public data and reduce accuracy loss compared to using private data directly. With  D ^ ^ D \\hat{\\mathcal{D}} over^ start_ARG caligraphic_D end_ARG , we train a VAE  {  e ,  d } subscript italic- e subscript italic- d \\{\\phi_{e},\\phi_{d}\\} { italic_ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT , italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT } , where the encoder   e subscript italic- e \\phi_{e} italic_ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  with parameters  W e subscript W e \\mathbb{W}_{e} blackboard_W start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT  and decoder   d subscript italic- d \\phi_{d} italic_ start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  with parameters  W d subscript W d \\mathbb{W}_{d} blackboard_W start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  are constructed with convolutional neural networks like  [ 65 ] .",
            "Generator Learning.  To study the effect of data generation, we conduct student learning on MNIST, FMNIST and CIFAR10 with the raw private data as well as synthetic data generated by four generative approaches, including ACGAN  [ 76 ] , WGAN  [ 46 ] , InfoGAN  [ 77 ]  and our Data-Free learned generator. The results are shown in Fig.  4  and some generated examples can be seen in Fig.  5 . It is easy to distinguish the images generated by ACGAN, WGAN and InfoGAN, implying that these generators learned with private data may expose data privacy in spite of achieving higher accuracy. By contrast, the synthetic images generated by data-free learned generator are hardly identified by human. Thus, the data-free learned generator effectively protects privacy while delivering comparable accuracy since it matches the distribution of private data in discriminative space.",
            "In practice, a learning process of our approach can achieve several major models, including: 1) the baseline model (serving as the fixed discriminator) and the ensemble of teachers that are kept privately, 2) the data-free learned generator that can be delivered to provide valuable synthetic data for training more further models in a more privacy-preserving manner than using private or other generative data (as shown in Fig.  5  and discussed above), and 3) the student that is delivered for privacy-preserving deployment. We take the student learned on MNIST under  ( 10.0 , 10  5 ) 10.0 superscript 10 5 (10.0,10^{-5}) ( 10.0 , 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT ) -differential privacy as an example and study its privacy-preserving ability against three reconstruction attacks, including reconstruction with data-free learned generator  [ 62 ] , model inversion attack with confidence information and basic countermeasures  [ 7 ] , and adversarial model inversion attack with background knowledge alignment  [ 8 ] . Some reconstructed results by these three attacks are shown in the left of Fig.  9 , from the first to the third row, respectively. We can see that the reconstructed images are very different from the original images and hardly distinguished which number they are by human. Thus, the student can well protect data privacy whilst delivering a high accuracy of 97.4%. To further verify the privacy-preserving ability of our students, we investigate an especial case by conducting inversion attack  [ 8 ]  against binary classification models that are trained on a subset of MNIST containing only 0s and 1s with GS-WGAN, DataLens and our DGD. The results are shown in the right of Fig.  9 . The reconstruction results of GS-WGAN and DataLens can be distinguished by human, while our model can provide better protection against reconstruction attacks. From these results, we can safely claim that our approach can provide effective privacy-preserving model learning and the learned models are suitable particularly for practical application on privacy-conscious scenarios."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S4.T2.5.5",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "where  W s   W s superscript subscript W s subscript W s \\mathbb{W}_{s}^{-}\\subset\\mathbb{W}_{s} blackboard_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  blackboard_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT  is backbone parameters of the student for extracting features. We can see that the unsupervised energy Eq. ( 6 ) includes normal regularization, tangent regularization and entropy regularization. The first two regularization terms enhance model robustness against perturbations along orthogonal and parallel directions to the underlying data manifold respectively, while entropy regularization ensures the student output more determinate predictions. This tangent-normal adversarial regularization by adding perturbation to the latent layer can make the student vary smoothly along tangent space and have strong robustness along normal space  [ 65 ] .",
            "Beyond the generator learning method, we further check the impact of synthetic data. Towards this end, via the generator trained with the baseline on MNIST as the fixed discriminator, we generate 8 synthetic datasets with various amounts to train students and report their performance in Fig.  6 . We can find that the model accuracy increases by training on more synthetic data and gets smooth after the used synthetic data reaches 60K that is equal to the number of private training examples. Therefore, we generate the same number of synthetic data as the private training data to provide a good trade-off between model performance and training efficiency."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "id1.1",
        "footnotes": [],
        "references": [
            "where  O O \\mathcal{O} caligraphic_O  denotes the subset of possible outputs. Eq. ( 7 ) indicates that  A  ( D ^ u ) A subscript ^ D u \\mathcal{A}(\\hat{\\mathcal{D}}_{u}) caligraphic_A ( over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT )  satisfies   1 subscript  1 \\varepsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT -differential privacy according to the definition of differential privacy  [ 69 ] . Further, according to the composition theorem  [ 69 ] , our approach finally satisfies  ( | D ^ s |   0 2 +  0   2  | D ^ s |  log   +  1 ,  ) subscript ^ D s superscript subscript  0 2 subscript  0 2 subscript ^ D s  subscript  1  (|\\hat{\\mathcal{D}}_{s}|\\varepsilon_{0}^{2}+\\varepsilon_{0}\\sqrt{-2|\\hat{% \\mathcal{D}}_{s}|\\log{\\delta}}+\\varepsilon_{1},\\delta) ( | over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT square-root start_ARG - 2 | over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | roman_log italic_ end_ARG + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ ) -differential privacy and gives the differential privacy guarantee.",
            "VAE-based Regularization.  To check the effect of VAE on student learning, we modify our approach for comparing to PATE  [ 19 ]  with Laplacian aggregation as well as its improved variant PATE+  [ 21 ]  with Gaussian aggregation under the same experimental settings. Towards this end, we remove the generator and feed private training data (simulating public unlabeled data like  [ 19 ,  21 ] ) into VAE to learn students with Laplacian and Gaussian aggregation, leading to two modified approaches denoted as  DGD  and  DGD +, respectively. We train various students on MNIST and SVHN under different privacy budget and conduct the comparisons. In our experiments, a few of training data serve as queries in noisy aggregation and the remaining most of the training data are fed into VAE where each example is reconstructed into a synthetic triple. The results are reported in Fig.  7 , where using VAE in our  DGD  and  DGD + can consistently improve model accuracy over PATE and PATE+ without sacrificing privacy guarantee, respectively. For example,  DGD + delivers an accuracy of 92.7% on SVHN that is very close to 92.8% achieved with baseline, implying the effectiveness of VAE-based regularization, since it can provide self-supervised knowledge enhancement to compensate the accuracy drop. We also achieve higher accuracy by Gaussian aggregation than Laplacian aggregation ( i.e. , PATE+ vs. PATE, and  DGD + vs.  DGD ) as stated in  [ 21 ] , implying the significance of noisy aggregation, introduced next."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "id2.1",
        "footnotes": [],
        "references": [
            "Teacher Ensemble.  We check the effect of teacher number on the accuracy of teacher ensemble. The top left of Fig.  8  shows the results on three datasets for evaluating the effect on simple and complex classification tasks. We find that the accuracy increases along with teacher number within a certain range, indicating that the model performance can be boosted by increasing teacher number in a certain range. It is very helpful in real-world applications like federated learning  [ 68 ,  78 ]  where the private model can be improved by adding the sharing data parties. The performance starts to degrade when the teacher number increases to a certain value. Then the amount of partitioned training data for each teacher starts to become inadequate for learning, suggesting careful selection of the teacher number.",
            "Noisy Aggregation.  During the student learning, the obtained query labels are disturbed by noise with a scale of  2 /  0 2 subscript  0 2/\\varepsilon_{0} 2 / italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . Theoretically, the higher the noise scale is, the lower privacy cost and better privacy protection is. However, too high noise scale may cause label distortion, making student difficult to learn useful knowledge and unsuitable for practical deployment. We study how the noise scale  2 /  0 2 subscript  0 2/\\varepsilon_{0} 2 / italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  affects the privacy cost and report in the bottom left of Fig.  8 . We can observe that the privacy cost declines rapidly with the noise scale within a certain range, has a short rise in the middle, and then keeps smooth. We suspect the main reason is the calculation process of privacy protection metric with moments accountant. Thus, we could select a noise scale ( e.g. ,  2 /  0 = 30 2 subscript  0 30 2/\\varepsilon_{0}=30 2 / italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 30 ) to provide a good trade-off between privacy protection and model accuracy.",
            "To further study the effect of noisy aggregation, we conduct experiments on MNIST and FMNIST and report the results in the top right of Fig.  8  where DGD+ improves DGD with Gaussian aggregation. It shows that the improvement of all students accuracy starts rapid and gets smooth when increasing the privacy budget. Moreover, as expected, the accuracy of students trained with Gaussian aggregation is remarkably improved over Laplacian aggregation, suggesting that more advanced noisy aggregation mechanisms can be incorporated into our framework to facilitate performance.",
            "In theory, our approach trains students from VAE-reconstructed synthetic data in generative stream and noisy labels in discriminative stream, where the reconstructed synthetic data are achieved by inputting synthetic data into VAE and reconstructing from noisy latent codes. As discussed in  III-E , it contains two parts of privacy budgets and totally achieves  ( | D ^ s |   0 2 +  0   2  | D ^ s |  log   +  1 ,  )  limit-from subscript ^ D s superscript subscript  0 2 subscript  0 2 subscript ^ D s  subscript  1  (|\\hat{\\mathcal{D}}_{s}|\\varepsilon_{0}^{2}+\\varepsilon_{0}\\sqrt{-2|\\hat{% \\mathcal{D}}_{s}|\\log{\\delta}}+\\varepsilon_{1},\\delta)- ( | over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT square-root start_ARG - 2 | over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT | roman_log italic_ end_ARG + italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_ ) - differential privacy over  | D ^ s | subscript ^ D s |\\hat{\\mathcal{D}}_{s}| | over^ start_ARG caligraphic_D end_ARG start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT |  queries for all    ( 0 , 1 )  0 1 \\delta\\in(0,1) italic_  ( 0 , 1 ) . In our experiments, generative privacy budget   1 = 0.01 subscript  1 0.01 \\varepsilon_{1}=0.01 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.01  is very small and can be ignored in the total privacy budget. Discriminative privacy budget dominates the total privacy budget,  e.g. , having a much higher privacy budget of 5.80 under   0 = 1 / 20 subscript  0 1 20 \\varepsilon_{0}=1/20 italic_ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 1 / 20 ,   = 10  5  superscript 10 5 \\delta=10^{-5} italic_ = 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT  over 400 queries. For discriminative stream, we first use differential privacy with advanced composition  [ 69 ]  to track privacy loss, seeing the red curve in the bottom right of Fig.  8 . To better track the privacy loss, we further use differential privacy with moment accountant  [ 12 ]  and add an advanced limit  [ 19 ]  to get a lower privacy budget. The results can be seen the blue curve in the bottom right of Fig.  8 . We can see that the privacy guarantee is satisfied in all metrics,  e.g. , a privacy budget of 10.1 with advanced composition or 8.03 with moments accountant under 1000 queries, leading to an effective trade-off between privacy-preserving model learning and accuracy drop control."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "id3.1",
        "footnotes": [],
        "references": [
            "In practice, a learning process of our approach can achieve several major models, including: 1) the baseline model (serving as the fixed discriminator) and the ensemble of teachers that are kept privately, 2) the data-free learned generator that can be delivered to provide valuable synthetic data for training more further models in a more privacy-preserving manner than using private or other generative data (as shown in Fig.  5  and discussed above), and 3) the student that is delivered for privacy-preserving deployment. We take the student learned on MNIST under  ( 10.0 , 10  5 ) 10.0 superscript 10 5 (10.0,10^{-5}) ( 10.0 , 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT ) -differential privacy as an example and study its privacy-preserving ability against three reconstruction attacks, including reconstruction with data-free learned generator  [ 62 ] , model inversion attack with confidence information and basic countermeasures  [ 7 ] , and adversarial model inversion attack with background knowledge alignment  [ 8 ] . Some reconstructed results by these three attacks are shown in the left of Fig.  9 , from the first to the third row, respectively. We can see that the reconstructed images are very different from the original images and hardly distinguished which number they are by human. Thus, the student can well protect data privacy whilst delivering a high accuracy of 97.4%. To further verify the privacy-preserving ability of our students, we investigate an especial case by conducting inversion attack  [ 8 ]  against binary classification models that are trained on a subset of MNIST containing only 0s and 1s with GS-WGAN, DataLens and our DGD. The results are shown in the right of Fig.  9 . The reconstruction results of GS-WGAN and DataLens can be distinguished by human, while our model can provide better protection against reconstruction attacks. From these results, we can safely claim that our approach can provide effective privacy-preserving model learning and the learned models are suitable particularly for practical application on privacy-conscious scenarios."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "id4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "id5.1",
        "footnotes": [],
        "references": []
    }
}