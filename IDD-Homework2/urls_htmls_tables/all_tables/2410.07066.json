{
    "id_table_1": {
        "caption": "Table 1 :  Comparison of previous survey papers",
        "table": "S1.T1.4",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In Section  2 , we introduce the capabilities and applications of several key DGMs in the literature. Specifically, we introduce Variational Autoencoders (VAEs) in Section  2.3 , Generative Adversarial Networks (GANs) in Section  2.4 , Normalizing Flows (or flow-based generative models) in Section  2.5 , Score-based generative models in Section  2.6  and Diffusion models in Section  2.7 . In Section  3 , we introduce state-of-the-art transportation research using DGMs. Particularly, we introduce applications of DGM 1) for generating realistic new data samples that can be applied in data synthesis, trajectory generation, and missing data imputation in Section  3.1 , 2) for estimating and predicting distributions at three different levels of analyses in transportation research (agent-level, link-level, and region-level) in Section  3.2 , and 3) for understanding underlying dynamics and learning unsupervised representations of data for applications like anomaly detection and mode choice analysis in Section  3.3 .  Readers primarily interested in the current practices can begin with Section  3  for a review of the latest literature in transportation research using DGMs, and then refer back to Section  2  for an introduction to the models themselves.  In Section  4 , we provide a tutorial that offers practical guidance on implementing DGMs in transportation research. We introduce two examples: 1) generating travel survey data in Section  4.1 , and 2) generating highway traffic speed contour data in Section  4.2 . In Section  5 , we identify and discuss the challenges and opportunities associated with using DGMs in the transportation domain, emphasizing the importance of addressing these challenges for the successful adoption of DGMs. Finally, in Section  6 , we summarize and conclude the paper.",
            "Equation ( 2.10 ) demonstrates that the ELBO is a  lower bound  to the log-likelihood of the data. Therefore, maximizing the ELBO with respect to the parameters of the encoder and decoder,   italic- \\phi italic_  and    \\theta italic_ , also maximizes the true log-likelihood ( log  p   ( x ) subscript p  x \\log p_{\\theta}(\\mathbf{x}) roman_log italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_x ) ). The ELBO can be efficiently estimated using stochastic gradient-based optimization methods without involving the intractable posterior  p   ( z | x ) subscript p  conditional z x p_{\\theta}(\\mathbf{z}|\\mathbf{x}) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x ) . Finally, the maximizing objective function of VAE can be expressed as:",
            "In this section, we present practical examples of how DGMs can be applied in transportation research. To reach a broader audience, we provide two hands-on tutorials: 1) Generating Household Travel Survey Data in Section  4.1 , and 2) Generating Highway Traffic Speed Contour (Time-Space Diagram) in Section  4.2 . Importantly, all data and code used in these tutorialsincluding preprocessing scripts, model training, inference code, and pre-trained model parametersare available in our GitHub repository:  https://github.com/benchoi93/DGMinTransportation . The tutorial code is implemented in Python, with PyTorch serving as the primary library for the tutorials. Additional requirements are detailed in the associated GitHub repository.",
            "The VAE loss function consists of two primary components as shown in Equation ( 2.11 ): the  reconstruction loss  and the  regularization loss . The reconstruction loss measures how well the model can reproduce the original data. Since the encoder and decoder are both deterministic functions when input is given, the reconstruction loss term,  E z  q   ( z | x )  [ log  p   ( x | z ) ] subscript E similar-to z subscript q italic- conditional z x delimited-[] subscript p  conditional x z \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\left[\\log p_{% \\theta}(\\mathbf{x}|\\mathbf{z})\\right] blackboard_E start_POSTSUBSCRIPT bold_z  italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_x | bold_z ) ] , can be re-written as  E x  [ log  p   ( x ^ | x ) ] subscript E x delimited-[] subscript p  conditional ^ x x \\mathbb{E}_{\\mathbf{x}}\\left[\\log p_{\\theta}(\\mathbf{\\hat{x}}|\\mathbf{x})\\right] blackboard_E start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( over^ start_ARG bold_x end_ARG | bold_x ) ] . If we assume that this distribution has a Gaussian form:",
            "Unlike VAEs, GANs do not require complex derivations of the loss function. The code implementation of the Equations ( 2.13 ) and ( 2.14 ) can be written intuitively. In practice, this involves using binary cross-entropy loss. The discriminator tries to identify if the given input is from the real data set or the generated data set. On the contrary, the generator tries to overcome the identification from the discriminator. Similar to the VAE, we constructed the 3-layer neural network for (a) generator and (b) discriminator as shown in Figure  10 . The noise vector (latent vector) is  D D D italic_D -dimensional vector where  D = 64 D 64 D=64 italic_D = 64 . The Pytorch code implementation is as follows:",
            "We use a flow-based generative model inspired by RealNVP  (Dinh et al.,  2016 ) , which leverages affine coupling layers and alternating masking strategies to transform complex data distributions into a standard normal distribution. For simplicity, and since the data dimensionality is not too large, the model in this tutorial does not include permutation layers between coupling layers and batch normalization that were proposed in the original RealNVP paper. A more detailed implementation of RealNVP will be discussed in Section  4.2.4 . Despite these simplifications, our model maintains the core idea of using invertible affine transformations to compute the log-likelihood and optimize the data distribution. The training objective is to maximize the likelihood of the observed data under this transformation, which can be expressed as minimizing the negative log-likelihood. The original loss function for the normalizing flow is the same as stated in Equation ( 2.21 ). For the affine coupling layers, we use the same equations as described from Equation ( 2.23 ) to Equation ( 2.26 ). We use much simpler networks for scaling and translation network as shown in Figure  11 . The code Pytorch implementation is as follows:",
            "In the tutorial code, we used the Denoising Diffusion Probabilistic Model (DDPM) as the reference for generating data  (Ho et al.,  2020 ) . Instead of using the traditional U-Net architecture for noise prediction, we implemented a simpler neural network structure, as shown in Figure  12 . The U-Net architecture is commonly used in diffusion models due to its powerful ability to capture multi-scale features in image data. However, in this tutorial, we implemented a simpler neural network to reduce computational complexity and facilitate a focus on the fundamental concepts of diffusion models, such as stepwise noise estimation and progressive denoising. The loss function for training is calculated based on Equation ( 2.39 ) presented above, which measures the models ability to predict noise accurately at each step of the diffusion process. The Pytorch code implementation is as follows:",
            "where the term  x ~  x ~ x x \\tilde{\\mathbf{x}}-\\mathbf{x} over~ start_ARG bold_x end_ARG - bold_x  corresponds to the variable  noise . We use a neural network with three linear layers and ReLU activations to obtain the estimated score as shown in Figure  13 . In this tutorial, we train the NCSN using sequential sigma noise levels rather than randomly mixing them. This approach diverges from the typical method of random sampling of sigma values but was chosen to provide a clearer illustration of the overall training process. Given the relatively low dimension of the dataset, the results show that the sequential method is also feasible. The Pytorch code implementation is as follows:",
            "Figure  14  presents the results of generating HTS data. In this section, we aim to share the empirical insights gained during the training of the models.",
            "The raw data from the MOTION project comprises segments of vehicle trajectory information. In this tutorial, our focus is on the reconstruction of traffic speed contours. To facilitate this, the trajectory data is transformed into velocity data corresponding to each specific time and position. Given that the calculation of velocity data based on trajectories inevitably results in the presence of zero or NaN values (except in the unlikely event of a vehicle being present at every possible position throughout the entire timespan), preprocessing was necessary. This preprocessing predominantly involved interpolation. The left and right of Figure  15  show the time-space traffic contours diagram before and after the interpolation.",
            "We utilize the result of Equation ( 4.5 ) to compute the loss of the VAE. The code structure and implementation of the loss function are consistent with the approach presented in Section  4.1.2 . Figure  16  illustrates the structure of the neural network for traffic speed contour generation. Figure  16  (a) illustrates the structure of the encoder. The encoder encodes the data to Mean and Variance with a  D D D italic_D -dimensional vector. In this section,  D = 64 D 64 D=64 italic_D = 64  is applied. Figure  16  (a) is the structure of the decoder that generates the image from the latent vector.",
            "The loss function and model architecture of the GAN for traffic speed contour estimation follows the implementation outlined in Section  4.1.3 . We employ binary cross-entropy loss for both the discriminator and the generator.",
            "Figure  17  presents the detailed neural network architecture of the GAN, which closely resembles the VAE structure depicted in Figure  16 . The discriminator shown in Figure  17  (a) outputs the decision for the input if it is real or fake. The generator, shown in Figure  17  (b) outputs the image from the  D D D italic_D -dimensional noise vector. Here,  D = 64 D 64 D=64 italic_D = 64  is applied. This similarity of the neural network structure between GAN and VAE enables us to qualitatively compare the performance of these two models. The models discussed in subsequent sections, such as Normalizing Flows, Score-based Generative Models, and Diffusion Models, possess more intricate and distinctive structures. Consequently, evaluating these models within the same neural network framework is impractical. However, the architectures of the VAE and GAN can be designed to be similar, which fulfills our purpose of comparison.",
            "As we discussed in Section  2.5 , among various Flow-based Generative Models such as  Dinh et al. ( 2014 ); Rezende and Mohamed ( 2015 ); Kingma and Dhariwal ( 2018 ); Durkan et al. ( 2019 ) , we implemented RealNVP  (Dinh et al.,  2016 )  because of its ability to generate high-dimensional distribution due to its flexibility. As we can see in Equation ( 2.21 ), we need to calculate the  log  p 0  ( z 0 ) subscript p 0 subscript z 0 \\log p_{0}(z_{0}) roman_log italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  in Equation ( 2.20 ) to get the likelihood of the data. In the implementation of RealNVP, we can simplify the log-likelihood  log  ( p  ( z ) ) p z \\log\\left(p(\\mathbf{z})\\right) roman_log ( italic_p ( bold_z ) )  by assuming  z z z italic_z  is sampled from the standard normal distribution. The other term in Equation ( 2.20 ), the summation of the determinant of the Jacobian, is simplified by applying a coupling layer as described through Equation ( 2.22 ) to Equation ( 2.26 ). Thus, the summation of the determinant of the Jacobian can be simplified as   j s  ( x 1 : d ) j subscript j s subscript subscript x : 1 d j \\sum_{j}s(x_{1:d})_{j}  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_s ( italic_x start_POSTSUBSCRIPT 1 : italic_d end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT . In our code application, the regularization loss for batch normalization and scale parameters are applied following the contents in RealNVP  (Dinh et al.,  2016 ) .",
            "The neural network structure of RealNVP is shown in Figure  18 . The neural network is composed of multiple layers of its Backbone Block, which is shown in Figure  18  (a). Each Backbone Block consists of convolution, Instance 2D Normalization, and ReLU layer. Each Backbone Block is the smallest unit for the scale and translation network. In the implementation in the current paper, we stacked 22 layers of scale and translation network. The dimensions and number of channels of the features change at each layer, as detailed in Figure  18  (b). In the generation stage, the computation proceeds in an inverse way.",
            "The neural network that is used for the DDPM is illustrated in Figure  19 . Figure  19  (a), Figure  19  (b), and Figure  19  (c) show the structure of the Residual Block, Backbone Block, and U-Net of DDPM, respectively. The U-Net structure that is applied in the neural network in DDPM enables one to learn the information from high-level features or features from previous layers. The residual block contains the time embedding as an input, which identifies the stage of the current generation process. A similar mechanism can also be found in the Score-based model, as the score-based model creates an image using the scheduled    \\sigma italic_ . The neural network in DDPM also includes the attention mechanism that captures the relationship between features. The attention mechanism was applied inside the Backbone block, between the Residual block and the group normalization.",
            "In the code implementation of NCSN in traffic speed contour, we had minor adjustments from the code from Section  4.1.6 . By definition,  x ~ ~ x \\tilde{\\mathbf{x}} over~ start_ARG bold_x end_ARG  is defined as  x ~ = x +    ~ x x   italic- \\tilde{\\mathbf{x}}=\\mathbf{x}+\\sigma\\cdot\\epsilon over~ start_ARG bold_x end_ARG = bold_x + italic_  italic_ . Then   x log  q   ( x ~ | x ) subscript  x subscript q  conditional ~ x x \\nabla_{\\mathbf{x}}\\log q_{\\sigma}\\left(\\tilde{\\mathbf{x}}|\\mathbf{x}\\right)  start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( over~ start_ARG bold_x end_ARG | bold_x )  in Equation ( 4.7 ) is transformed as   /  italic-  \\nicefrac{{\\epsilon}}{{\\sigma}} / start_ARG italic_ end_ARG start_ARG italic_ end_ARG . Practically,   2 superscript  2 \\sigma^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  is multiplied by the objective function as the weight of each stage. The detailed implementation can be found in the code.",
            "Figure  21  presents the 64 images of ground truth speed contour data and outputs of 64 samples generated by each DGM. It is worth noting that the results generated by the DGMs in Figure  21  are not samples that directly match the ground truth. These images are instead \"plausible\" representations generated from noise. Therefore, readers should keep in mind that good images are those that closely resemble the ground truth, with clear and well-defined traffic speed patterns across the contours. In contrast, poor images may appear overly blurred, and noisy, or suffer from mode collapse, where the generated outputs fail to capture the underlying traffic dynamics.",
            "Evaluating DGMs involves assessing how well the models is able to generate data that match the underlying distribution of the observed data. A critical limitation in transportation data analysis is that the amount of available data is small and deep learning models could overfit to the training data without proper evaluation and validation. In this context, scoring rules provide a principled way to evaluate these models by quantifying how well the predicted probabilities align with the actual outcomes  (Gneiting and Raftery,  2007 ) . For instance, the  Log Score  measures the negative log-likelihood of the generative data under the probabilistic distribution derived by DGMs. The  Continuously Ranked Probabilistic Score  (CRPS) and  Energy Score  (ES) measure how well the cumulative distribution obtained produces continuous univariate and multivariate data, respectively. A second challenge is that many types of traffic data are governed by physics. For example, the probability distribution of the trajectory of a car needs to be governed by the continuous variation of acceleration and turning angle. If one only considers location information in building the DGMs, the trained model will hardly reproduce realistic trajectories, which poses further problems in applying DGMs as data generators for downstream tasks. Another example is the traffic speed data that we introduced in Sec  4.2 . The speed in this case is produced by microscopic traffic flow. Treating these data as an image will lose the physical nature behind it, and unrealistic data that violate traffic flow patterns could be generated. A third challenge is how to define a proper probability distribution for complex structured data. Taking the travel survey data presented in Sec  4.1  as an example, multiple constraints exist at different hierarchical levels (see e.g.,  Sun et al. ( 2018 ) ): the composition of a household will be governed by certain structures; individuals within the same household exhibit strong interactions/correlation in term of social demographic features; trips of an individual have to satisfy certain sequence orders and in the meanwhile trips chains of different individuals will also be coupled (e.g., a trip of an adult to drop off a child at school). To model such data, one often has to introduce strong but unrealistic assumptions to simplify the probabilistic model, such as assuming household members are independent of each other. These assumptions make the computational part easier, but in the meanwhile the quality of data will be undermined.",
            "Synthetic data generation is one of the core applications of DGMs, as discussed in Section  3.1.1 . The use of synthetic data generated by DGMs offers several significant advantages for privacy protection in transportation, highlighting the potential of synthetic data to balance the need for data utility with stringent privacy requirements. One of the primary benefits of synthetic data generated by DGMs is its ability to preserve the statistical or distributional properties and patterns of real-world transportation and mobility data while excluding personally identifiable information. This means that the generated synthetic data can reflect the trends, behaviors, and characteristics found in actual data without risking individual privacy. For instance, in trajectory data containing personal origin-destination pairs and commuting patterns  (Choi et al.,  2018 ) , synthetic data can accurately represent peak travel times, popular routes, and average journey durations without including any specific details about individual travelers. This capability ensures that the generated synthetic data is useful for analysis, model training, and validation. Researchers and practitioners can derive meaningful insights for transportation policy and develop effective models for traffic management without compromising the quality and accuracy of the data. For example, urban planners can use synthetic data to simulate the impact of new infrastructure projects on traffic flow, while data scientists can train machine learning models to predict congestion or optimize public transport schedules. Furthermore, synthetic data enables safe data sharing and collaboration between different organizations, stakeholders, and researchers. For instance, transportation agencies who collected privacy-sensitive trajectory data, activity sequence, and time use data, can share synthetic datasets generated by DGMs with third-party developers, policymakers, and academic institutions without exposing sensitive information. This can foster innovation, accelerate research, and support the development of new solutions and services, all while maintaining privacy.",
            "Therefore, the overall loss function of minimizing the negative log-likelihood in Equation ( A.1 ) is decomposed into several losses,  L T subscript L T L_{T} italic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ,  L t  1 subscript L t 1 L_{t-1} italic_L start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , and  L 0 subscript L 0 L_{0} italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . Here,  L T subscript L T L_{T} italic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  is constant since both  q  ( x T | x 0 ) q conditional subscript x T subscript x 0 q\\left(\\mathbf{x}_{T}|\\mathbf{x}_{0}\\right) italic_q ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  and  p  ( x T ) p subscript x T p\\left(\\mathbf{x}_{T}\\right) italic_p ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )  are fixed, and therefore, we can ignore this term. Also, in  Ho and Ermon ( 2016 ) ,  L 0 subscript L 0 L_{0} italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  is explicitly defined by using the characteristics of the image generation problem, and as a result,  L 0 subscript L 0 L_{0} italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  can be interpreted as a reconstruction loss of a problem-specific decoder. As a result, the actual learning process of the diffusion model is related to  L t  1 subscript L t 1 L_{t-1} italic_L start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT .",
            "where    \\cdots   include the terms irrelevant to  x t  1 subscript x t 1 \\mathbf{x}_{t-1} bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT . Since  q  ( x t  1 | x t , x 0 ) q conditional subscript x t 1 subscript x t subscript x 0 q\\left(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}\\right) italic_q ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  is a Gaussian distribution, we can find the mean and the variance from Equation ( A.11 ):",
            "The dynamics of Brownian motion as described in Equation ( B.1 ) can be further derived by considering the potential energy in the system. Specifically, the force acting on a particle can be expressed in terms of the gradient of potential energy; i.e.,   V  ( x )  x =  V  ( x ) V x x  V x \\frac{\\partial V(x)}{\\partial x}=\\nabla V(x) divide start_ARG  italic_V ( italic_x ) end_ARG start_ARG  italic_x end_ARG =  italic_V ( italic_x ) . This leads to a reformulation of the equation as:"
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Trip chain data generated from example daily travel in Figure  8",
        "table": "S4.T2.4",
        "footnotes": [
            ""
        ],
        "references": [
            "In Section  2 , we introduce the capabilities and applications of several key DGMs in the literature. Specifically, we introduce Variational Autoencoders (VAEs) in Section  2.3 , Generative Adversarial Networks (GANs) in Section  2.4 , Normalizing Flows (or flow-based generative models) in Section  2.5 , Score-based generative models in Section  2.6  and Diffusion models in Section  2.7 . In Section  3 , we introduce state-of-the-art transportation research using DGMs. Particularly, we introduce applications of DGM 1) for generating realistic new data samples that can be applied in data synthesis, trajectory generation, and missing data imputation in Section  3.1 , 2) for estimating and predicting distributions at three different levels of analyses in transportation research (agent-level, link-level, and region-level) in Section  3.2 , and 3) for understanding underlying dynamics and learning unsupervised representations of data for applications like anomaly detection and mode choice analysis in Section  3.3 .  Readers primarily interested in the current practices can begin with Section  3  for a review of the latest literature in transportation research using DGMs, and then refer back to Section  2  for an introduction to the models themselves.  In Section  4 , we provide a tutorial that offers practical guidance on implementing DGMs in transportation research. We introduce two examples: 1) generating travel survey data in Section  4.1 , and 2) generating highway traffic speed contour data in Section  4.2 . In Section  5 , we identify and discuss the challenges and opportunities associated with using DGMs in the transportation domain, emphasizing the importance of addressing these challenges for the successful adoption of DGMs. Finally, in Section  6 , we summarize and conclude the paper.",
            "However, Equation ( 2.8 ) is intractable due to the integral over the  z z \\mathbf{z} bold_z  space. Even though Monte Carlo methods can be used to approximate the integral and apply maximum likelihood estimation, they may result in suboptimal generation due to poor-quality samples being assigned higher likelihoods. To address this issue, VAEs incorporate the encoder  q   ( z | x ) subscript q italic- conditional z x q_{\\phi}(\\mathbf{z}|\\mathbf{x}) italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x )  into their objective function, reformulated as:",
            "The first term in Equation ( 2.9 ),  E z  [ log  p   ( x | z ) ] subscript E z delimited-[] subscript p  conditional x z \\mathbbm{E}_{z}\\left[\\log p_{\\theta}(\\mathbf{x}|\\mathbf{z})\\right] blackboard_E start_POSTSUBSCRIPT italic_z end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_x | bold_z ) ] , is the likelihood of  x x \\mathbf{x} bold_x  generated from the decoder based on the sampled  z z \\mathbf{z} bold_z  from the posterior distribution. This term can be estimated through sampling using the reparameterization trick, which will be discussed later. The second term is the KL divergence between the approximate posterior distribution  q   ( z | x ) subscript q italic- conditional z x q_{\\phi}(\\mathbf{z}|\\mathbf{x}) italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x )  and the prior distribution  p  ( z ) p z p(\\mathbf{z}) italic_p ( bold_z ) . Assuming both distributions follow a tractable distribution (typically Gaussian), this term can be computed in closed form. The third term is intractable because the true posterior distribution  p   ( z | x ) subscript p  conditional z x p_{\\theta}(\\mathbf{z}|\\mathbf{x}) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x )  involves evaluation of intractable Equation ( 2.9 )  ( p   ( z | x ) = p   ( x | z )  p  ( z ) p   ( x ) ) subscript p  conditional z x subscript p  conditional x z p z subscript p  x \\left(p_{\\theta}(\\mathbf{z}|\\mathbf{x})=\\frac{p_{\\theta}(\\mathbf{x}|\\mathbf{z}% )p(\\mathbf{z})}{{p_{\\theta}(\\mathbf{x})}}\\right) ( italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x ) = divide start_ARG italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_x | bold_z ) italic_p ( bold_z ) end_ARG start_ARG italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_x ) end_ARG ) . However, the third term is always non-negative due to the properties of KL divergence. Therefore, a new objective function called the Evidence Lower BOund (ELBO),  L  ( x ;  ,  ) L x  italic- \\mathcal{L}(\\mathbf{x};\\theta,\\phi) caligraphic_L ( bold_x ; italic_ , italic_ ) , can be defined as:",
            "Equation ( 2.10 ) demonstrates that the ELBO is a  lower bound  to the log-likelihood of the data. Therefore, maximizing the ELBO with respect to the parameters of the encoder and decoder,   italic- \\phi italic_  and    \\theta italic_ , also maximizes the true log-likelihood ( log  p   ( x ) subscript p  x \\log p_{\\theta}(\\mathbf{x}) roman_log italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_x ) ). The ELBO can be efficiently estimated using stochastic gradient-based optimization methods without involving the intractable posterior  p   ( z | x ) subscript p  conditional z x p_{\\theta}(\\mathbf{z}|\\mathbf{x}) italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x ) . Finally, the maximizing objective function of VAE can be expressed as:",
            "Adversarial Autoencoders (AAEs)  (Makhzani et al.,  2015 )  represent an innovative fusion of VAEs (VAEs) and Generative Adversarial Networks (GANs; See Section  2.4  for more details), designed to enhance the capabilities of generative models. The primary advantage of AAEs lies in their ability to impose arbitrary prior distributions on the latent space, going beyond the Gaussian priors typically used in VAEs. This flexibility allows for the modeling of complex data distributions more effectively. AAEs consist of a standard autoencoder architecture, complemented by an adversarial network that enforces the latent space to conform to the chosen prior distribution. To be more specific, the fundamental condition in VAE loss is that KL divergence between the distribution of encoded input data and prior can be calculated. The function of KL divergence in loss term is making the distribution of encoded input data ( q   ( z | x ) subscript q italic- conditional z x q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}\\right) italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x ) ) the same as the prior distribution  p  ( z ) p z p(\\mathbf{z}) italic_p ( bold_z ) , which is the same logic of GAN. Therefore, AAE utilizes the discriminator term of GAN to replace the KL divergence of VAE, making it possible to use arbitrary prior distribution. Through adversarial training, AAEs generate sharper, more detailed outputs compared to traditional VAEs, especially noticeable in tasks like image generation. They are versatile in applications, ranging from semi-supervised learning to unsupervised clustering and anomaly detection. Additionally, AAEs offer better control over the generation process, including conditional generation and the learning of disentangled representations, making them suitable for tasks requiring precise control and interpretability. The integration of adversarial principles into autoencoders has thus positioned AAEs as a powerful and flexible tool in the realm of generative modeling, addressing key challenges of traditional VAEs and opening new avenues in machine learning research.",
            "First, the inverse function of Equation ( 2.22 ) is as follows:",
            "As a result, the Jacobian matrix shown in Equation ( 2.24 ) is a lower-triangular matrix. Therefore, the determinant of the Jacobian matrix can be calculated as follows:",
            "A significant challenge with this approach is that it involves direct access to   x log  p  ( x ) subscript  x p x \\nabla_{\\mathbf{x}}\\log p(\\mathbf{x})  start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT roman_log italic_p ( bold_x ) , which is not practically feasible. To address this issue, a substantial body of research has been dedicated to developing alternative methodologies, collectively known as score matching  (Hyvarinen and Dayan,  2005 ; Vincent,  2011 ; Song et al.,  2020a )  that minimizes Equation ( 2.27 ) without having to have access to the ground-truth data score.",
            "From Equation ( 2.32 ), we can derive the equation for  q  ( x 0 : T ) q subscript x : 0 T q(\\mathbf{x}_{0:T}) italic_q ( bold_x start_POSTSUBSCRIPT 0 : italic_T end_POSTSUBSCRIPT )  representing the diffusion trajectory from the original data  x 0 subscript x 0 \\mathbf{x}_{0} bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  to the complete noise  x T = z subscript x T z \\mathbf{x}_{T}=z bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = italic_z  based on properties of Markov chain and chain rules as follows:",
            "Therefore, the overall loss function of minimizing the negative log-likelihood in Equation ( 2.37 ) is decomposed into several losses,  L T subscript L T L_{T} italic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ,  L t  1 subscript L t 1 L_{t-1} italic_L start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , and  L 0 subscript L 0 L_{0} italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . Here,  L T subscript L T L_{T} italic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  is constant since both  q  ( x T | x 0 ) q conditional subscript x T subscript x 0 q\\left(\\mathbf{x}_{T}|\\mathbf{x}_{0}\\right) italic_q ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  and  p  ( x T ) p subscript x T p\\left(\\mathbf{x}_{T}\\right) italic_p ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )  are fixed, and therefore, we can ignore this term. Also, in  Ho and Ermon ( 2016 ) ,  L 0 subscript L 0 L_{0} italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  is explicitly defined by using the characteristics of the image generation problem, and as a result,  L 0 subscript L 0 L_{0} italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  can be interpreted as a reconstruction loss of a problem-specific decoder. As a result, the actual learning process of the diffusion model is related to  L t  1 subscript L t 1 L_{t-1} italic_L start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT .",
            "Since the diffusion process follows Gaussian distribution, the true reverse process,  q  ( x t  1 | x t , x 0 ) q conditional subscript x t 1 subscript x t subscript x 0 q\\left(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}\\right) italic_q ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , can be assumed to follow a Gaussian distribution if  T T T italic_T  is sufficiently large, or  T    T T\\rightarrow\\infty italic_T   . Let  q  ( x t  1 | x t , x 0 ) = N  ( x t  1 ;  ~ t  ( x t , x 0 ) ,  ~ t  I ) q conditional subscript x t 1 subscript x t subscript x 0 N subscript x t 1 subscript ~  t subscript x t subscript x 0 subscript ~  t I q\\left(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}\\right)=\\mathcal{N}(% \\mathbf{x}_{t-1};\\tilde{\\boldsymbol{\\mu}}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}),% \\tilde{\\beta}_{t}\\mathbbm{I}) italic_q ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = caligraphic_N ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; over~ start_ARG bold_italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_I ) . To derive explicit form of   ~ t  ( x t , x 0 ) subscript ~  t subscript x t subscript x 0 \\tilde{\\boldsymbol{\\mu}}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}) over~ start_ARG bold_italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  and   ~ t subscript ~  t \\tilde{\\beta}_{t} over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , first, we should derive a closed-form equation for sampling  x t subscript x t \\mathbf{x}_{t} bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  at an arbitrary timestep  t t t italic_t  from Equation ( 2.32 ) as follows:",
            "However, most DGMs cannot yet simultaneously satisfy these three key requirements, a challenge often referred to as the  Generative Learning Trilemma   (Xiao et al.,  2021 ) . This trilemma highlights the inherent trade-offs between high-quality sample generation, mode coverage and diversity, and fast, computationally efficient sampling. For example, Generative Adversarial Networks (GANs) have the advantage of generating high-quality samples rapidly. As discussed in Section  2.4 , GANs are renowned for producing realistic samples through a single forward pass of the generator network, making them computationally efficient at inference time. However, common shortcomings of GANs include mode collapse, where the model fails to capture the full diversity of the data distribution, generating samples from only some modes but not all. On the other hand, Variational Autoencoders (VAEs) and Normalizing Flows usually perform well in terms of mode coverage and fast computation time. VAEs promote diverse and representative sampling by modeling the entire data distribution, and they allow for quick sample generation by direct decoding from latent variables. Normalizing Flows also enables efficient sampling and exact density estimation. However, the quality of the samples generated by VAEs and Normalizing Flows may not be as high as those produced by other models. Diffusion Models present another approach, capable of covering diverse modes and generating high-quality samples. Recent advances have enabled diffusion models to produce samples that rival or even surpass GANs in terms of quality and diversity. However, diffusion models typically consist of a large number of denoising steps (often ranging from 500 to 1000), which require substantial computation to generate a single sample. This makes the sampling process slow and computationally expensive compared to other models.",
            "In this section, we present practical examples of how DGMs can be applied in transportation research. To reach a broader audience, we provide two hands-on tutorials: 1) Generating Household Travel Survey Data in Section  4.1 , and 2) Generating Highway Traffic Speed Contour (Time-Space Diagram) in Section  4.2 . Importantly, all data and code used in these tutorialsincluding preprocessing scripts, model training, inference code, and pre-trained model parametersare available in our GitHub repository:  https://github.com/benchoi93/DGMinTransportation . The tutorial code is implemented in Python, with PyTorch serving as the primary library for the tutorials. Additional requirements are detailed in the associated GitHub repository.",
            "The original household travel survey includes sociodemographic data and completed travel data for the households. For simplicity, we exclude the sociodemographic data and focus solely on travel data. An example of the HTS travel data used in this study is shown in Figure  8 . This travel data that are composed of three different trips can be transformed into the tabular data as the Table  2 . To simplify the problem, we selected the types of origin, activity, mode choice, and destination as target features for data generation. Table  3  provides a detailed description of the chosen features. In our tutorial example, each DGM is designed to generate each row that contains the data in Table  3  by learning the joint distribution of the selected features. The data includes origin and destination types divided into five categories: home, work, school, transfer points, and other locations. Transportation modes are categorized into nine discrete integers: 0 for the start of the day, 1 for the end of the day, 2 for staying at a location, and 3 through 8 representing various modes of transport such as walking, public transit, driving (driver and passenger), cycling, and taxis. Lastly, activity types include eight categories: work, education, leisure, shopping, and escort activities. It is important to note that the goal of this tutorial is not generating the complete daily travel, but rather focusing on individual trips. This allows the model to capture the relationships between features. During model evaluation, we present qualitative assessments with marginal distributions of each feature. This qualitative approach prioritizes interpretability and understanding over direct numerical performance metrics. Nevertheless, the model is trained to capture the joint probability distribution, emphasizing its ability to generate realistic combinations of attributes.",
            "The VAE loss function consists of two primary components as shown in Equation ( 2.11 ): the  reconstruction loss  and the  regularization loss . The reconstruction loss measures how well the model can reproduce the original data. Since the encoder and decoder are both deterministic functions when input is given, the reconstruction loss term,  E z  q   ( z | x )  [ log  p   ( x | z ) ] subscript E similar-to z subscript q italic- conditional z x delimited-[] subscript p  conditional x z \\mathbb{E}_{\\mathbf{z}\\sim q_{\\phi}(\\mathbf{z}|\\mathbf{x})}\\left[\\log p_{% \\theta}(\\mathbf{x}|\\mathbf{z})\\right] blackboard_E start_POSTSUBSCRIPT bold_z  italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x ) end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_x | bold_z ) ] , can be re-written as  E x  [ log  p   ( x ^ | x ) ] subscript E x delimited-[] subscript p  conditional ^ x x \\mathbb{E}_{\\mathbf{x}}\\left[\\log p_{\\theta}(\\mathbf{\\hat{x}}|\\mathbf{x})\\right] blackboard_E start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT [ roman_log italic_p start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( over^ start_ARG bold_x end_ARG | bold_x ) ] . If we assume that this distribution has a Gaussian form:",
            "Unlike VAEs, GANs do not require complex derivations of the loss function. The code implementation of the Equations ( 2.13 ) and ( 2.14 ) can be written intuitively. In practice, this involves using binary cross-entropy loss. The discriminator tries to identify if the given input is from the real data set or the generated data set. On the contrary, the generator tries to overcome the identification from the discriminator. Similar to the VAE, we constructed the 3-layer neural network for (a) generator and (b) discriminator as shown in Figure  10 . The noise vector (latent vector) is  D D D italic_D -dimensional vector where  D = 64 D 64 D=64 italic_D = 64 . The Pytorch code implementation is as follows:",
            "We use a flow-based generative model inspired by RealNVP  (Dinh et al.,  2016 ) , which leverages affine coupling layers and alternating masking strategies to transform complex data distributions into a standard normal distribution. For simplicity, and since the data dimensionality is not too large, the model in this tutorial does not include permutation layers between coupling layers and batch normalization that were proposed in the original RealNVP paper. A more detailed implementation of RealNVP will be discussed in Section  4.2.4 . Despite these simplifications, our model maintains the core idea of using invertible affine transformations to compute the log-likelihood and optimize the data distribution. The training objective is to maximize the likelihood of the observed data under this transformation, which can be expressed as minimizing the negative log-likelihood. The original loss function for the normalizing flow is the same as stated in Equation ( 2.21 ). For the affine coupling layers, we use the same equations as described from Equation ( 2.23 ) to Equation ( 2.26 ). We use much simpler networks for scaling and translation network as shown in Figure  11 . The code Pytorch implementation is as follows:",
            "In the tutorial code, we used the Denoising Diffusion Probabilistic Model (DDPM) as the reference for generating data  (Ho et al.,  2020 ) . Instead of using the traditional U-Net architecture for noise prediction, we implemented a simpler neural network structure, as shown in Figure  12 . The U-Net architecture is commonly used in diffusion models due to its powerful ability to capture multi-scale features in image data. However, in this tutorial, we implemented a simpler neural network to reduce computational complexity and facilitate a focus on the fundamental concepts of diffusion models, such as stepwise noise estimation and progressive denoising. The loss function for training is calculated based on Equation ( 2.39 ) presented above, which measures the models ability to predict noise accurately at each step of the diffusion process. The Pytorch code implementation is as follows:",
            "The original objective function of the Score-based Model is defined as Equation ( 2.28 ). Here we show how the transformation to Equation ( 2.29 ) proceeds when we are using the normal distribution for the noise  q   ( x ~ | x ) subscript q  conditional ~ x x q_{\\sigma}\\left(\\tilde{\\mathbf{x}}|\\mathbf{x}\\right) italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( over~ start_ARG bold_x end_ARG | bold_x ) . In this case, the equation can be simplified as shown follows:",
            "We utilize the result of Equation ( 4.5 ) to compute the loss of the VAE. The code structure and implementation of the loss function are consistent with the approach presented in Section  4.1.2 . Figure  16  illustrates the structure of the neural network for traffic speed contour generation. Figure  16  (a) illustrates the structure of the encoder. The encoder encodes the data to Mean and Variance with a  D D D italic_D -dimensional vector. In this section,  D = 64 D 64 D=64 italic_D = 64  is applied. Figure  16  (a) is the structure of the decoder that generates the image from the latent vector.",
            "As we discussed in Section  2.5 , among various Flow-based Generative Models such as  Dinh et al. ( 2014 ); Rezende and Mohamed ( 2015 ); Kingma and Dhariwal ( 2018 ); Durkan et al. ( 2019 ) , we implemented RealNVP  (Dinh et al.,  2016 )  because of its ability to generate high-dimensional distribution due to its flexibility. As we can see in Equation ( 2.21 ), we need to calculate the  log  p 0  ( z 0 ) subscript p 0 subscript z 0 \\log p_{0}(z_{0}) roman_log italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ( italic_z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  in Equation ( 2.20 ) to get the likelihood of the data. In the implementation of RealNVP, we can simplify the log-likelihood  log  ( p  ( z ) ) p z \\log\\left(p(\\mathbf{z})\\right) roman_log ( italic_p ( bold_z ) )  by assuming  z z z italic_z  is sampled from the standard normal distribution. The other term in Equation ( 2.20 ), the summation of the determinant of the Jacobian, is simplified by applying a coupling layer as described through Equation ( 2.22 ) to Equation ( 2.26 ). Thus, the summation of the determinant of the Jacobian can be simplified as   j s  ( x 1 : d ) j subscript j s subscript subscript x : 1 d j \\sum_{j}s(x_{1:d})_{j}  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_s ( italic_x start_POSTSUBSCRIPT 1 : italic_d end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT . In our code application, the regularization loss for batch normalization and scale parameters are applied following the contents in RealNVP  (Dinh et al.,  2016 ) .",
            "The loss function of DDPM is simplified as the difference between the predicted noise and the real noise as stated in Equation ( 2.39 ). The criteria of the difference, i.e., the norm of difference, can be varied by the problem. Our code is also designed to select one of three types of loss, which is  L 1 subscript L 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , Mean Square Error (MSE) loss, and Huber loss. For simplicity, we inserted the code block only including the MSE loss.",
            "The neural network of NCSN is formed with a block-wise structure with a Residual Block. Figure  20  (a) and Figure  20  (b) illustrate the structure of the Residual Block and whole neural network for score function, respectively. Skip-connection is applied in each block to maintain the low-level feature information.",
            "Figure  21  presents the 64 images of ground truth speed contour data and outputs of 64 samples generated by each DGM. It is worth noting that the results generated by the DGMs in Figure  21  are not samples that directly match the ground truth. These images are instead \"plausible\" representations generated from noise. Therefore, readers should keep in mind that good images are those that closely resemble the ground truth, with clear and well-defined traffic speed patterns across the contours. In contrast, poor images may appear overly blurred, and noisy, or suffer from mode collapse, where the generated outputs fail to capture the underlying traffic dynamics.",
            "Evaluating DGMs involves assessing how well the models is able to generate data that match the underlying distribution of the observed data. A critical limitation in transportation data analysis is that the amount of available data is small and deep learning models could overfit to the training data without proper evaluation and validation. In this context, scoring rules provide a principled way to evaluate these models by quantifying how well the predicted probabilities align with the actual outcomes  (Gneiting and Raftery,  2007 ) . For instance, the  Log Score  measures the negative log-likelihood of the generative data under the probabilistic distribution derived by DGMs. The  Continuously Ranked Probabilistic Score  (CRPS) and  Energy Score  (ES) measure how well the cumulative distribution obtained produces continuous univariate and multivariate data, respectively. A second challenge is that many types of traffic data are governed by physics. For example, the probability distribution of the trajectory of a car needs to be governed by the continuous variation of acceleration and turning angle. If one only considers location information in building the DGMs, the trained model will hardly reproduce realistic trajectories, which poses further problems in applying DGMs as data generators for downstream tasks. Another example is the traffic speed data that we introduced in Sec  4.2 . The speed in this case is produced by microscopic traffic flow. Treating these data as an image will lose the physical nature behind it, and unrealistic data that violate traffic flow patterns could be generated. A third challenge is how to define a proper probability distribution for complex structured data. Taking the travel survey data presented in Sec  4.1  as an example, multiple constraints exist at different hierarchical levels (see e.g.,  Sun et al. ( 2018 ) ): the composition of a household will be governed by certain structures; individuals within the same household exhibit strong interactions/correlation in term of social demographic features; trips of an individual have to satisfy certain sequence orders and in the meanwhile trips chains of different individuals will also be coupled (e.g., a trip of an adult to drop off a child at school). To model such data, one often has to introduce strong but unrealistic assumptions to simplify the probabilistic model, such as assuming household members are independent of each other. These assumptions make the computational part easier, but in the meanwhile the quality of data will be undermined.",
            "In transportation applications, integrating causality into DGMs can significantly enhance their effectiveness. Conventional generative models primarily rely on training data, which limits their effectiveness in new or changing environments. In contrast, causal generative models leverage underlying cause-effect relationships, enhancing their ability to generalize across different settings and maintain robustness in out-of-distribution (OOD) scenarios. By learning causality, these models can adapt to covariate shiftswhere input data distributions vary between training and deploymentwith fewer samples, reusing many of their components without the need for retraining. For example, a causal generative model trained to understand the relationship between traffic density and travel time in one city can adapt to another city with different traffic patterns. Additionally, causal generative models maintain reliable predictions under OOD conditions such as major social events, natural disasters, or unexpected road closures by understanding and leveraging causal structures. This dual capability of improved generalization and OOD robustness makes causal generative models particularly valuable for developing scalable and effective transportation solutions in diverse and dynamically changing environments, as discussed in Section  5.2 . Moreover, considering causality can improve the interpretability of DGMs. This capability not only helps stakeholders understand and trust the models outputs but also allows them to control the generative process to produce targeted data. Traditional deep learning models are often perceived as black boxes, providing limited insights into the reasons behind certain outputs. Conversely, causal generative models focus on causal mechanisms, which offer clearer explanations for transportation phenomena. For example, in the context of traffic safety, a causal generative model can not only predict the likelihood of accidents at a particular intersection but also explain contributing factors to output such as poor road design, inadequate road signs, or high pedestrian activity. This interpretability is essential for developing trustworthy AI, as discussed in Section  5.5  Since the model is interpretable based on these causal factors, it also allows for the control of the generative process by directly manipulating these variables. This kind of controllable generative model, grounded in causality, can be used to generate data representing specific scenarios, enabling transportation planners to simulate and evaluate the effects of interventions. This combination of interpretability and controllability makes causal generative models highly valuable in practical transportation applications.",
            "Using Equation ( 2.34 ) and Equation ( 2.35 ), we can further derive:",
            "Since the diffusion process follows Gaussian distribution, the true reverse process,  q  ( x t  1 | x t , x 0 ) q conditional subscript x t 1 subscript x t subscript x 0 q\\left(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}\\right) italic_q ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , can be assumed to follow a Gaussian distribution if  T T T italic_T  is sufficiently large, or  T    T T\\rightarrow\\infty italic_T   . Let  q  ( x t  1 | x t , x 0 ) = N  ( x t  1 ;  ~ t  ( x t , x 0 ) ,  ~ t  I ) q conditional subscript x t 1 subscript x t subscript x 0 N subscript x t 1 subscript ~  t subscript x t subscript x 0 subscript ~  t I q\\left(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}\\right)=\\mathcal{N}(% \\mathbf{x}_{t-1};\\tilde{\\boldsymbol{\\mu}}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}),% \\tilde{\\beta}_{t}\\mathbbm{I}) italic_q ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = caligraphic_N ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; over~ start_ARG bold_italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_I ) . To derive explicit form of   ~ t  ( x t , x 0 ) subscript ~  t subscript x t subscript x 0 \\tilde{\\boldsymbol{\\mu}}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}) over~ start_ARG bold_italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  and   ~ t subscript ~  t \\tilde{\\beta}_{t} over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , first, we should derive a closed-form equation for sampling  x t subscript x t \\mathbf{x}_{t} bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  at an arbitrary timestep  t t t italic_t  from Equation ( 2.32 ) as follows:"
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Details of each attribute in generated data",
        "table": "S4.T3.4",
        "footnotes": [],
        "references": [
            "In Section  2 , we introduce the capabilities and applications of several key DGMs in the literature. Specifically, we introduce Variational Autoencoders (VAEs) in Section  2.3 , Generative Adversarial Networks (GANs) in Section  2.4 , Normalizing Flows (or flow-based generative models) in Section  2.5 , Score-based generative models in Section  2.6  and Diffusion models in Section  2.7 . In Section  3 , we introduce state-of-the-art transportation research using DGMs. Particularly, we introduce applications of DGM 1) for generating realistic new data samples that can be applied in data synthesis, trajectory generation, and missing data imputation in Section  3.1 , 2) for estimating and predicting distributions at three different levels of analyses in transportation research (agent-level, link-level, and region-level) in Section  3.2 , and 3) for understanding underlying dynamics and learning unsupervised representations of data for applications like anomaly detection and mode choice analysis in Section  3.3 .  Readers primarily interested in the current practices can begin with Section  3  for a review of the latest literature in transportation research using DGMs, and then refer back to Section  2  for an introduction to the models themselves.  In Section  4 , we provide a tutorial that offers practical guidance on implementing DGMs in transportation research. We introduce two examples: 1) generating travel survey data in Section  4.1 , and 2) generating highway traffic speed contour data in Section  4.2 . In Section  5 , we identify and discuss the challenges and opportunities associated with using DGMs in the transportation domain, emphasizing the importance of addressing these challenges for the successful adoption of DGMs. Finally, in Section  6 , we summarize and conclude the paper.",
            "From Equation ( 2.32 ), we can derive the equation for  q  ( x 0 : T ) q subscript x : 0 T q(\\mathbf{x}_{0:T}) italic_q ( bold_x start_POSTSUBSCRIPT 0 : italic_T end_POSTSUBSCRIPT )  representing the diffusion trajectory from the original data  x 0 subscript x 0 \\mathbf{x}_{0} bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  to the complete noise  x T = z subscript x T z \\mathbf{x}_{T}=z bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT = italic_z  based on properties of Markov chain and chain rules as follows:",
            "Therefore, the overall loss function of minimizing the negative log-likelihood in Equation ( 2.37 ) is decomposed into several losses,  L T subscript L T L_{T} italic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ,  L t  1 subscript L t 1 L_{t-1} italic_L start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT , and  L 0 subscript L 0 L_{0} italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT . Here,  L T subscript L T L_{T} italic_L start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  is constant since both  q  ( x T | x 0 ) q conditional subscript x T subscript x 0 q\\left(\\mathbf{x}_{T}|\\mathbf{x}_{0}\\right) italic_q ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  and  p  ( x T ) p subscript x T p\\left(\\mathbf{x}_{T}\\right) italic_p ( bold_x start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT )  are fixed, and therefore, we can ignore this term. Also, in  Ho and Ermon ( 2016 ) ,  L 0 subscript L 0 L_{0} italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  is explicitly defined by using the characteristics of the image generation problem, and as a result,  L 0 subscript L 0 L_{0} italic_L start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  can be interpreted as a reconstruction loss of a problem-specific decoder. As a result, the actual learning process of the diffusion model is related to  L t  1 subscript L t 1 L_{t-1} italic_L start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT .",
            "Since the diffusion process follows Gaussian distribution, the true reverse process,  q  ( x t  1 | x t , x 0 ) q conditional subscript x t 1 subscript x t subscript x 0 q\\left(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}\\right) italic_q ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , can be assumed to follow a Gaussian distribution if  T T T italic_T  is sufficiently large, or  T    T T\\rightarrow\\infty italic_T   . Let  q  ( x t  1 | x t , x 0 ) = N  ( x t  1 ;  ~ t  ( x t , x 0 ) ,  ~ t  I ) q conditional subscript x t 1 subscript x t subscript x 0 N subscript x t 1 subscript ~  t subscript x t subscript x 0 subscript ~  t I q\\left(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}\\right)=\\mathcal{N}(% \\mathbf{x}_{t-1};\\tilde{\\boldsymbol{\\mu}}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}),% \\tilde{\\beta}_{t}\\mathbbm{I}) italic_q ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = caligraphic_N ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; over~ start_ARG bold_italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_I ) . To derive explicit form of   ~ t  ( x t , x 0 ) subscript ~  t subscript x t subscript x 0 \\tilde{\\boldsymbol{\\mu}}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}) over~ start_ARG bold_italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  and   ~ t subscript ~  t \\tilde{\\beta}_{t} over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , first, we should derive a closed-form equation for sampling  x t subscript x t \\mathbf{x}_{t} bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  at an arbitrary timestep  t t t italic_t  from Equation ( 2.32 ) as follows:",
            "The original household travel survey includes sociodemographic data and completed travel data for the households. For simplicity, we exclude the sociodemographic data and focus solely on travel data. An example of the HTS travel data used in this study is shown in Figure  8 . This travel data that are composed of three different trips can be transformed into the tabular data as the Table  2 . To simplify the problem, we selected the types of origin, activity, mode choice, and destination as target features for data generation. Table  3  provides a detailed description of the chosen features. In our tutorial example, each DGM is designed to generate each row that contains the data in Table  3  by learning the joint distribution of the selected features. The data includes origin and destination types divided into five categories: home, work, school, transfer points, and other locations. Transportation modes are categorized into nine discrete integers: 0 for the start of the day, 1 for the end of the day, 2 for staying at a location, and 3 through 8 representing various modes of transport such as walking, public transit, driving (driver and passenger), cycling, and taxis. Lastly, activity types include eight categories: work, education, leisure, shopping, and escort activities. It is important to note that the goal of this tutorial is not generating the complete daily travel, but rather focusing on individual trips. This allows the model to capture the relationships between features. During model evaluation, we present qualitative assessments with marginal distributions of each feature. This qualitative approach prioritizes interpretability and understanding over direct numerical performance metrics. Nevertheless, the model is trained to capture the joint probability distribution, emphasizing its ability to generate realistic combinations of attributes.",
            "Unlike VAEs, GANs do not require complex derivations of the loss function. The code implementation of the Equations ( 2.13 ) and ( 2.14 ) can be written intuitively. In practice, this involves using binary cross-entropy loss. The discriminator tries to identify if the given input is from the real data set or the generated data set. On the contrary, the generator tries to overcome the identification from the discriminator. Similar to the VAE, we constructed the 3-layer neural network for (a) generator and (b) discriminator as shown in Figure  10 . The noise vector (latent vector) is  D D D italic_D -dimensional vector where  D = 64 D 64 D=64 italic_D = 64 . The Pytorch code implementation is as follows:",
            "We use a flow-based generative model inspired by RealNVP  (Dinh et al.,  2016 ) , which leverages affine coupling layers and alternating masking strategies to transform complex data distributions into a standard normal distribution. For simplicity, and since the data dimensionality is not too large, the model in this tutorial does not include permutation layers between coupling layers and batch normalization that were proposed in the original RealNVP paper. A more detailed implementation of RealNVP will be discussed in Section  4.2.4 . Despite these simplifications, our model maintains the core idea of using invertible affine transformations to compute the log-likelihood and optimize the data distribution. The training objective is to maximize the likelihood of the observed data under this transformation, which can be expressed as minimizing the negative log-likelihood. The original loss function for the normalizing flow is the same as stated in Equation ( 2.21 ). For the affine coupling layers, we use the same equations as described from Equation ( 2.23 ) to Equation ( 2.26 ). We use much simpler networks for scaling and translation network as shown in Figure  11 . The code Pytorch implementation is as follows:",
            "In the tutorial code, we used the Denoising Diffusion Probabilistic Model (DDPM) as the reference for generating data  (Ho et al.,  2020 ) . Instead of using the traditional U-Net architecture for noise prediction, we implemented a simpler neural network structure, as shown in Figure  12 . The U-Net architecture is commonly used in diffusion models due to its powerful ability to capture multi-scale features in image data. However, in this tutorial, we implemented a simpler neural network to reduce computational complexity and facilitate a focus on the fundamental concepts of diffusion models, such as stepwise noise estimation and progressive denoising. The loss function for training is calculated based on Equation ( 2.39 ) presented above, which measures the models ability to predict noise accurately at each step of the diffusion process. The Pytorch code implementation is as follows:",
            "where the term  x ~  x ~ x x \\tilde{\\mathbf{x}}-\\mathbf{x} over~ start_ARG bold_x end_ARG - bold_x  corresponds to the variable  noise . We use a neural network with three linear layers and ReLU activations to obtain the estimated score as shown in Figure  13 . In this tutorial, we train the NCSN using sequential sigma noise levels rather than randomly mixing them. This approach diverges from the typical method of random sampling of sigma values but was chosen to provide a clearer illustration of the overall training process. Given the relatively low dimension of the dataset, the results show that the sequential method is also feasible. The Pytorch code implementation is as follows:",
            "The loss function and model architecture of the GAN for traffic speed contour estimation follows the implementation outlined in Section  4.1.3 . We employ binary cross-entropy loss for both the discriminator and the generator.",
            "The loss function of DDPM is simplified as the difference between the predicted noise and the real noise as stated in Equation ( 2.39 ). The criteria of the difference, i.e., the norm of difference, can be varied by the problem. Our code is also designed to select one of three types of loss, which is  L 1 subscript L 1 L_{1} italic_L start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , Mean Square Error (MSE) loss, and Huber loss. For simplicity, we inserted the code block only including the MSE loss.",
            "Synthetic data generation is one of the core applications of DGMs, as discussed in Section  3.1.1 . The use of synthetic data generated by DGMs offers several significant advantages for privacy protection in transportation, highlighting the potential of synthetic data to balance the need for data utility with stringent privacy requirements. One of the primary benefits of synthetic data generated by DGMs is its ability to preserve the statistical or distributional properties and patterns of real-world transportation and mobility data while excluding personally identifiable information. This means that the generated synthetic data can reflect the trends, behaviors, and characteristics found in actual data without risking individual privacy. For instance, in trajectory data containing personal origin-destination pairs and commuting patterns  (Choi et al.,  2018 ) , synthetic data can accurately represent peak travel times, popular routes, and average journey durations without including any specific details about individual travelers. This capability ensures that the generated synthetic data is useful for analysis, model training, and validation. Researchers and practitioners can derive meaningful insights for transportation policy and develop effective models for traffic management without compromising the quality and accuracy of the data. For example, urban planners can use synthetic data to simulate the impact of new infrastructure projects on traffic flow, while data scientists can train machine learning models to predict congestion or optimize public transport schedules. Furthermore, synthetic data enables safe data sharing and collaboration between different organizations, stakeholders, and researchers. For instance, transportation agencies who collected privacy-sensitive trajectory data, activity sequence, and time use data, can share synthetic datasets generated by DGMs with third-party developers, policymakers, and academic institutions without exposing sensitive information. This can foster innovation, accelerate research, and support the development of new solutions and services, all while maintaining privacy.",
            "Using Equation ( 2.34 ) and Equation ( 2.35 ), we can further derive:",
            "Since the diffusion process follows Gaussian distribution, the true reverse process,  q  ( x t  1 | x t , x 0 ) q conditional subscript x t 1 subscript x t subscript x 0 q\\left(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}\\right) italic_q ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , can be assumed to follow a Gaussian distribution if  T T T italic_T  is sufficiently large, or  T    T T\\rightarrow\\infty italic_T   . Let  q  ( x t  1 | x t , x 0 ) = N  ( x t  1 ;  ~ t  ( x t , x 0 ) ,  ~ t  I ) q conditional subscript x t 1 subscript x t subscript x 0 N subscript x t 1 subscript ~  t subscript x t subscript x 0 subscript ~  t I q\\left(\\mathbf{x}_{t-1}|\\mathbf{x}_{t},\\mathbf{x}_{0}\\right)=\\mathcal{N}(% \\mathbf{x}_{t-1};\\tilde{\\boldsymbol{\\mu}}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}),% \\tilde{\\beta}_{t}\\mathbbm{I}) italic_q ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT | bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) = caligraphic_N ( bold_x start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT ; over~ start_ARG bold_italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT ) , over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT blackboard_I ) . To derive explicit form of   ~ t  ( x t , x 0 ) subscript ~  t subscript x t subscript x 0 \\tilde{\\boldsymbol{\\mu}}_{t}(\\mathbf{x}_{t},\\mathbf{x}_{0}) over~ start_ARG bold_italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ( bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT )  and   ~ t subscript ~  t \\tilde{\\beta}_{t} over~ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , first, we should derive a closed-form equation for sampling  x t subscript x t \\mathbf{x}_{t} bold_x start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  at an arbitrary timestep  t t t italic_t  from Equation ( 2.32 ) as follows:"
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Pseudocode for data preprocessing",
        "table": "S4.T4.34",
        "footnotes": [],
        "references": [
            "In Section  2 , we introduce the capabilities and applications of several key DGMs in the literature. Specifically, we introduce Variational Autoencoders (VAEs) in Section  2.3 , Generative Adversarial Networks (GANs) in Section  2.4 , Normalizing Flows (or flow-based generative models) in Section  2.5 , Score-based generative models in Section  2.6  and Diffusion models in Section  2.7 . In Section  3 , we introduce state-of-the-art transportation research using DGMs. Particularly, we introduce applications of DGM 1) for generating realistic new data samples that can be applied in data synthesis, trajectory generation, and missing data imputation in Section  3.1 , 2) for estimating and predicting distributions at three different levels of analyses in transportation research (agent-level, link-level, and region-level) in Section  3.2 , and 3) for understanding underlying dynamics and learning unsupervised representations of data for applications like anomaly detection and mode choice analysis in Section  3.3 .  Readers primarily interested in the current practices can begin with Section  3  for a review of the latest literature in transportation research using DGMs, and then refer back to Section  2  for an introduction to the models themselves.  In Section  4 , we provide a tutorial that offers practical guidance on implementing DGMs in transportation research. We introduce two examples: 1) generating travel survey data in Section  4.1 , and 2) generating highway traffic speed contour data in Section  4.2 . In Section  5 , we identify and discuss the challenges and opportunities associated with using DGMs in the transportation domain, emphasizing the importance of addressing these challenges for the successful adoption of DGMs. Finally, in Section  6 , we summarize and conclude the paper.",
            "Adversarial Autoencoders (AAEs)  (Makhzani et al.,  2015 )  represent an innovative fusion of VAEs (VAEs) and Generative Adversarial Networks (GANs; See Section  2.4  for more details), designed to enhance the capabilities of generative models. The primary advantage of AAEs lies in their ability to impose arbitrary prior distributions on the latent space, going beyond the Gaussian priors typically used in VAEs. This flexibility allows for the modeling of complex data distributions more effectively. AAEs consist of a standard autoencoder architecture, complemented by an adversarial network that enforces the latent space to conform to the chosen prior distribution. To be more specific, the fundamental condition in VAE loss is that KL divergence between the distribution of encoded input data and prior can be calculated. The function of KL divergence in loss term is making the distribution of encoded input data ( q   ( z | x ) subscript q italic- conditional z x q_{\\phi}\\left(\\mathbf{z}|\\mathbf{x}\\right) italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( bold_z | bold_x ) ) the same as the prior distribution  p  ( z ) p z p(\\mathbf{z}) italic_p ( bold_z ) , which is the same logic of GAN. Therefore, AAE utilizes the discriminator term of GAN to replace the KL divergence of VAE, making it possible to use arbitrary prior distribution. Through adversarial training, AAEs generate sharper, more detailed outputs compared to traditional VAEs, especially noticeable in tasks like image generation. They are versatile in applications, ranging from semi-supervised learning to unsupervised clustering and anomaly detection. Additionally, AAEs offer better control over the generation process, including conditional generation and the learning of disentangled representations, making them suitable for tasks requiring precise control and interpretability. The integration of adversarial principles into autoencoders has thus positioned AAEs as a powerful and flexible tool in the realm of generative modeling, addressing key challenges of traditional VAEs and opening new avenues in machine learning research.",
            "As a result, the Jacobian matrix shown in Equation ( 2.24 ) is a lower-triangular matrix. Therefore, the determinant of the Jacobian matrix can be calculated as follows:",
            "However, most DGMs cannot yet simultaneously satisfy these three key requirements, a challenge often referred to as the  Generative Learning Trilemma   (Xiao et al.,  2021 ) . This trilemma highlights the inherent trade-offs between high-quality sample generation, mode coverage and diversity, and fast, computationally efficient sampling. For example, Generative Adversarial Networks (GANs) have the advantage of generating high-quality samples rapidly. As discussed in Section  2.4 , GANs are renowned for producing realistic samples through a single forward pass of the generator network, making them computationally efficient at inference time. However, common shortcomings of GANs include mode collapse, where the model fails to capture the full diversity of the data distribution, generating samples from only some modes but not all. On the other hand, Variational Autoencoders (VAEs) and Normalizing Flows usually perform well in terms of mode coverage and fast computation time. VAEs promote diverse and representative sampling by modeling the entire data distribution, and they allow for quick sample generation by direct decoding from latent variables. Normalizing Flows also enables efficient sampling and exact density estimation. However, the quality of the samples generated by VAEs and Normalizing Flows may not be as high as those produced by other models. Diffusion Models present another approach, capable of covering diverse modes and generating high-quality samples. Recent advances have enabled diffusion models to produce samples that rival or even surpass GANs in terms of quality and diversity. However, diffusion models typically consist of a large number of denoising steps (often ranging from 500 to 1000), which require substantial computation to generate a single sample. This makes the sampling process slow and computationally expensive compared to other models.",
            "In this section, we present practical examples of how DGMs can be applied in transportation research. To reach a broader audience, we provide two hands-on tutorials: 1) Generating Household Travel Survey Data in Section  4.1 , and 2) Generating Highway Traffic Speed Contour (Time-Space Diagram) in Section  4.2 . Importantly, all data and code used in these tutorialsincluding preprocessing scripts, model training, inference code, and pre-trained model parametersare available in our GitHub repository:  https://github.com/benchoi93/DGMinTransportation . The tutorial code is implemented in Python, with PyTorch serving as the primary library for the tutorials. Additional requirements are detailed in the associated GitHub repository.",
            "Unlike VAEs, GANs do not require complex derivations of the loss function. The code implementation of the Equations ( 2.13 ) and ( 2.14 ) can be written intuitively. In practice, this involves using binary cross-entropy loss. The discriminator tries to identify if the given input is from the real data set or the generated data set. On the contrary, the generator tries to overcome the identification from the discriminator. Similar to the VAE, we constructed the 3-layer neural network for (a) generator and (b) discriminator as shown in Figure  10 . The noise vector (latent vector) is  D D D italic_D -dimensional vector where  D = 64 D 64 D=64 italic_D = 64 . The Pytorch code implementation is as follows:",
            "We use a flow-based generative model inspired by RealNVP  (Dinh et al.,  2016 ) , which leverages affine coupling layers and alternating masking strategies to transform complex data distributions into a standard normal distribution. For simplicity, and since the data dimensionality is not too large, the model in this tutorial does not include permutation layers between coupling layers and batch normalization that were proposed in the original RealNVP paper. A more detailed implementation of RealNVP will be discussed in Section  4.2.4 . Despite these simplifications, our model maintains the core idea of using invertible affine transformations to compute the log-likelihood and optimize the data distribution. The training objective is to maximize the likelihood of the observed data under this transformation, which can be expressed as minimizing the negative log-likelihood. The original loss function for the normalizing flow is the same as stated in Equation ( 2.21 ). For the affine coupling layers, we use the same equations as described from Equation ( 2.23 ) to Equation ( 2.26 ). We use much simpler networks for scaling and translation network as shown in Figure  11 . The code Pytorch implementation is as follows:",
            "Figure  14  presents the results of generating HTS data. In this section, we aim to share the empirical insights gained during the training of the models.",
            "The entire process of interpolation, which is based on Edies definition of traffic flow dynamics, is outlined in Table  4 . The smoothing process, known as the adaptive smoothing method, is designed to filter out noisy fluctuations while considering the primary direction of flow propagation. This method employs spatiotemporal correlation analysis to identify the dominant characteristic line. The adaptive smoothing method uses a spatio-temporal low-pass filter that allows only low-frequency Fourier components to pass through, smoothing out high-frequency components. The filter eliminates high-frequency noises over a timescale shorter than    \\tau italic_  and spatial noise over a length scale shorter than    \\sigma italic_ . The values for    \\tau italic_  and    \\sigma italic_  were determined through empirical trials to ensure the data is not overly blurred while effectively passing the main propagation. The filter captures two main propagation types: free flow and congested traffic flow. These two propagations have different coefficients of velocity and direction, reflecting the distinct properties of each traffic flow type. The property of each traffic flow is reflected based on the different values of each coefficient.",
            "We utilize the result of Equation ( 4.5 ) to compute the loss of the VAE. The code structure and implementation of the loss function are consistent with the approach presented in Section  4.1.2 . Figure  16  illustrates the structure of the neural network for traffic speed contour generation. Figure  16  (a) illustrates the structure of the encoder. The encoder encodes the data to Mean and Variance with a  D D D italic_D -dimensional vector. In this section,  D = 64 D 64 D=64 italic_D = 64  is applied. Figure  16  (a) is the structure of the decoder that generates the image from the latent vector.",
            "The loss function and model architecture of the GAN for traffic speed contour estimation follows the implementation outlined in Section  4.1.3 . We employ binary cross-entropy loss for both the discriminator and the generator.",
            "In the code implementation of NCSN in traffic speed contour, we had minor adjustments from the code from Section  4.1.6 . By definition,  x ~ ~ x \\tilde{\\mathbf{x}} over~ start_ARG bold_x end_ARG  is defined as  x ~ = x +    ~ x x   italic- \\tilde{\\mathbf{x}}=\\mathbf{x}+\\sigma\\cdot\\epsilon over~ start_ARG bold_x end_ARG = bold_x + italic_  italic_ . Then   x log  q   ( x ~ | x ) subscript  x subscript q  conditional ~ x x \\nabla_{\\mathbf{x}}\\log q_{\\sigma}\\left(\\tilde{\\mathbf{x}}|\\mathbf{x}\\right)  start_POSTSUBSCRIPT bold_x end_POSTSUBSCRIPT roman_log italic_q start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( over~ start_ARG bold_x end_ARG | bold_x )  in Equation ( 4.7 ) is transformed as   /  italic-  \\nicefrac{{\\epsilon}}{{\\sigma}} / start_ARG italic_ end_ARG start_ARG italic_ end_ARG . Practically,   2 superscript  2 \\sigma^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  is multiplied by the objective function as the weight of each stage. The detailed implementation can be found in the code.",
            "Evaluating DGMs involves assessing how well the models is able to generate data that match the underlying distribution of the observed data. A critical limitation in transportation data analysis is that the amount of available data is small and deep learning models could overfit to the training data without proper evaluation and validation. In this context, scoring rules provide a principled way to evaluate these models by quantifying how well the predicted probabilities align with the actual outcomes  (Gneiting and Raftery,  2007 ) . For instance, the  Log Score  measures the negative log-likelihood of the generative data under the probabilistic distribution derived by DGMs. The  Continuously Ranked Probabilistic Score  (CRPS) and  Energy Score  (ES) measure how well the cumulative distribution obtained produces continuous univariate and multivariate data, respectively. A second challenge is that many types of traffic data are governed by physics. For example, the probability distribution of the trajectory of a car needs to be governed by the continuous variation of acceleration and turning angle. If one only considers location information in building the DGMs, the trained model will hardly reproduce realistic trajectories, which poses further problems in applying DGMs as data generators for downstream tasks. Another example is the traffic speed data that we introduced in Sec  4.2 . The speed in this case is produced by microscopic traffic flow. Treating these data as an image will lose the physical nature behind it, and unrealistic data that violate traffic flow patterns could be generated. A third challenge is how to define a proper probability distribution for complex structured data. Taking the travel survey data presented in Sec  4.1  as an example, multiple constraints exist at different hierarchical levels (see e.g.,  Sun et al. ( 2018 ) ): the composition of a household will be governed by certain structures; individuals within the same household exhibit strong interactions/correlation in term of social demographic features; trips of an individual have to satisfy certain sequence orders and in the meanwhile trips chains of different individuals will also be coupled (e.g., a trip of an adult to drop off a child at school). To model such data, one often has to introduce strong but unrealistic assumptions to simplify the probabilistic model, such as assuming household members are independent of each other. These assumptions make the computational part easier, but in the meanwhile the quality of data will be undermined.",
            "Using Equation ( 2.34 ) and Equation ( 2.35 ), we can further derive:",
            "In the context of probability distributions, the stochastic process described in Equation ( B.4 ) corresponds to a Fokker-Planck equation  (Fokker,  1914 ; Planck,  1917 ; Kadanoff,  2000 )  for the probability density function  p  ( x , t ) p x t p(x,t) italic_p ( italic_x , italic_t )  of finding the system in state  x x x italic_x  at time  t t t italic_t :",
            "Therefore, Equation ( B.4 ) can be re-written as:"
        ]
    }
}