{
    "A1.T7": {
        "caption": "Table 7: Prompt template to generate hallucinated responses.\n",
        "table": "<table id=\"A1.T7.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A1.T7.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Take a deep breath and work on this problem step by step.</td>\n</tr>\n<tr id=\"A1.T7.1.2\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.2.1\" class=\"ltx_td ltx_align_left\">I want you act as a chatbot in a conversation with human. Your job is to edit a detail in the True Response and generate</td>\n</tr>\n<tr id=\"A1.T7.1.3\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.3.1\" class=\"ltx_td ltx_align_left\">a Hallucinated Response that is inconsistent with the Dialogue History and Knowledge.</td>\n</tr>\n<tr id=\"A1.T7.1.4\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.4.1\" class=\"ltx_td ltx_align_left\">- Valid edit actions include removing, replacing or adding a short piece of information to the True</td>\n</tr>\n<tr id=\"A1.T7.1.5\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.5.1\" class=\"ltx_td ltx_align_left\">Response.</td>\n</tr>\n<tr id=\"A1.T7.1.6\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.6.1\" class=\"ltx_td ltx_align_left\">- If the True Response is faithful, please edit it to generate a Hallucinated Response.</td>\n</tr>\n<tr id=\"A1.T7.1.7\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.7.1\" class=\"ltx_td ltx_align_left\">- If the True Response has already contained hallucination, please edit it to generate an adversarial Hallucinated Response</td>\n</tr>\n<tr id=\"A1.T7.1.8\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.8.1\" class=\"ltx_td ltx_align_left\">that are more difficult to be detected.</td>\n</tr>\n<tr id=\"A1.T7.1.9\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.9.1\" class=\"ltx_td ltx_align_left\">- The generated Hallucinated Response should be ambiguous or complex or non-trivially implicit to be detected by a</td>\n</tr>\n<tr id=\"A1.T7.1.10\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.10.1\" class=\"ltx_td ltx_align_left\">human who has access to all the Knowledge and Dialogue History.</td>\n</tr>\n<tr id=\"A1.T7.1.11\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.11.1\" class=\"ltx_td ltx_align_left\">- The generated Hallucinated Response should contain similar number of words as the True Response. Do not make it</td>\n</tr>\n<tr id=\"A1.T7.1.12\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.12.1\" class=\"ltx_td ltx_align_left\">lengthy.</td>\n</tr>\n<tr id=\"A1.T7.1.13\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.13.1\" class=\"ltx_td ltx_align_left\">#Knowledge#: {Instructional prompt for target system}</td>\n</tr>\n<tr id=\"A1.T7.1.14\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.14.1\" class=\"ltx_td ltx_align_left\">#Dialogue History#: {dialogue history}</td>\n</tr>\n<tr id=\"A1.T7.1.15\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.15.1\" class=\"ltx_td ltx_align_left\">#True Response#: {system output}</td>\n</tr>\n<tr id=\"A1.T7.1.16\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.16.1\" class=\"ltx_td ltx_align_left\">Now, please generate your hallucinated response:</td>\n</tr>\n<tr id=\"A1.T7.1.17\" class=\"ltx_tr\">\n<td id=\"A1.T7.1.17.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">#Hallucinated Response#:</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table\u00a07 and Table\u00a08 include the prompt templates to generate hallucinated responses and faithful responses."
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Prompt template to generate faithful responses.\n",
        "table": "<table id=\"A1.T8.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A1.T8.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Take a deep breath and work on this problem step by step.</td>\n</tr>\n<tr id=\"A1.T8.1.2\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.2.1\" class=\"ltx_td ltx_align_left\">I want you act as a chatbot in a conversation with human.</td>\n</tr>\n<tr id=\"A1.T8.1.3\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.3.1\" class=\"ltx_td ltx_align_left\">Given a Response that contains hallucination, your job is to edit the Response lightly and generate a faithful Response</td>\n</tr>\n<tr id=\"A1.T8.1.4\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.4.1\" class=\"ltx_td ltx_align_left\">that is fully supported by with the Dialogue History and Knowledge.</td>\n</tr>\n<tr id=\"A1.T8.1.5\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.5.1\" class=\"ltx_td ltx_align_left\">- Valid edit actions include removing or replacing a short piece of information to the Response.</td>\n</tr>\n<tr id=\"A1.T8.1.6\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.6.1\" class=\"ltx_td ltx_align_left\">- Every token of the generated Response should be strictly verifiable by the Knowledge and Dialogue History. Even</td>\n</tr>\n<tr id=\"A1.T8.1.7\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.7.1\" class=\"ltx_td ltx_align_left\">commonsense information needs to be verifiable.</td>\n</tr>\n<tr id=\"A1.T8.1.8\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.8.1\" class=\"ltx_td ltx_align_left\">- Please keep the similar writing style as the Response. Do not make your response lengthy.</td>\n</tr>\n<tr id=\"A1.T8.1.9\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.9.1\" class=\"ltx_td ltx_align_left\">#Knowledge#: {Instructional prompt for target system}</td>\n</tr>\n<tr id=\"A1.T8.1.10\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.10.1\" class=\"ltx_td ltx_align_left\">#Dialogue History#: {dialogue history}</td>\n</tr>\n<tr id=\"A1.T8.1.11\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.11.1\" class=\"ltx_td ltx_align_left\">#Response#:{system output}</td>\n</tr>\n<tr id=\"A1.T8.1.12\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.12.1\" class=\"ltx_td ltx_align_left\">Now, please generate your faithful response:</td>\n</tr>\n<tr id=\"A1.T8.1.13\" class=\"ltx_tr\">\n<td id=\"A1.T8.1.13.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">#Faithful Response#:</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table\u00a07 and Table\u00a08 include the prompt templates to generate hallucinated responses and faithful responses."
        ]
    },
    "A1.T9": {
        "caption": "Table 9: Prompt template to generate \u2019Generic\u2019 responses for BEGIN dataset.\n",
        "table": "<table id=\"A1.T9.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A1.T9.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Take a deep breath and work on this problem step by step.</td>\n</tr>\n<tr id=\"A1.T9.1.2\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.2.1\" class=\"ltx_td ltx_align_left\">I want you act as a chatbot in a conversation with human.</td>\n</tr>\n<tr id=\"A1.T9.1.3\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.3.1\" class=\"ltx_td ltx_align_left\">Given a Response, your job is to rewrite it such that it is ostensibly about the same topic as the Response but becomes</td>\n</tr>\n<tr id=\"A1.T9.1.4\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.4.1\" class=\"ltx_td ltx_align_left\">vague and does not contain any factual statement.</td>\n</tr>\n<tr id=\"A1.T9.1.5\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.5.1\" class=\"ltx_td ltx_align_left\">Examples of rewritten Response includes but not limited to back-channeling, expressing uncertainty, or diverting the</td>\n</tr>\n<tr id=\"A1.T9.1.6\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.6.1\" class=\"ltx_td ltx_align_left\">conversation from ambiguous or controversial topics.\nDo not make your response lengthy.</td>\n</tr>\n<tr id=\"A1.T9.1.7\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.7.1\" class=\"ltx_td ltx_align_left\">#Knowledge#: {Instructional prompt for target system}</td>\n</tr>\n<tr id=\"A1.T9.1.8\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.8.1\" class=\"ltx_td ltx_align_left\">#Dialogue History#: {dialogue history}</td>\n</tr>\n<tr id=\"A1.T9.1.9\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.9.1\" class=\"ltx_td ltx_align_left\">#Response#: {system output}</td>\n</tr>\n<tr id=\"A1.T9.1.10\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.10.1\" class=\"ltx_td ltx_align_left\">Now, please generate your faithful response:</td>\n</tr>\n<tr id=\"A1.T9.1.11\" class=\"ltx_tr\">\n<td id=\"A1.T9.1.11.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">#Rewritten Response#:</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "is a knowledge-grounded dialog dataset featuring 12k responses from four dialogue systems distributed over 3 document-scale knowledge domains \u2013 Wizard of Wikipedia Dinan et\u00a0al. (2018), TopicalChat Gopalakrishnan et\u00a0al. (2023) and DoG\u00a0Zhou et\u00a0al. (2018) \u2014 all with mean knowledge snippets longer than OpenDialKG.\nIn addition, there are three response categories in BEGIN: Fully attributable, Not fully attributable, Generic. Generic category refers to response that are vague and do not provide any new information. Therefore, in addition to faithful and hallucination generation, we also ask LLM to generate responses under \"Generic\" category. The detailed prompt can be found in Appendix\u00a0A Table\u00a09.\nSince BEGIN only released the Dev and Test split, we adopt 1,228 system responses from Dev for both synthetic generation and development while reporting results on Test split.",
            "For BEGIN dataset, we also create a prompt to generate \"Generic\" responses, as shown in Table\u00a09"
        ]
    },
    "A10.T13": {
        "caption": "Table 13: Detection Results on Out-of-Distribution Data.\n",
        "table": "<table id=\"A10.T13.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A10.T13.1.1\" class=\"ltx_tr\">\n<td id=\"A10.T13.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Approach</td>\n<td id=\"A10.T13.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\">Macro-F1</td>\n</tr>\n<tr id=\"A10.T13.1.2\" class=\"ltx_tr\">\n<td id=\"A10.T13.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Random baseline</td>\n<td id=\"A10.T13.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">0.455</td>\n</tr>\n<tr id=\"A10.T13.1.3\" class=\"ltx_tr\">\n<td id=\"A10.T13.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Our approach</td>\n<td id=\"A10.T13.1.3.2\" class=\"ltx_td ltx_align_right\">0.518</td>\n</tr>\n<tr id=\"A10.T13.1.4\" class=\"ltx_tr\">\n<td id=\"A10.T13.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">GPT-4 (Internal)</td>\n<td id=\"A10.T13.1.4.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">0.543</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Given that the BEGIN dataset features significantly longer knowledge contexts in natural language text, as opposed to the list of triplets from the Freebase knowledge graph found in OpenDialKG, this setup simulates a substantial out-of-distribution evaluation. To adapt our binary classifiers for use with BEGIN, we collapse the \u201cgeneric\u201d and \u201cnot fully attributable\u201d labels into a single \u201challucination\u201d category, resulting in a modified binarized test set. The results are as follows in Table 13, which indicates that our approach leads to signifantly better performance in comparison to a naive random baseline, and shows competitive performance with GPT-4 based zero-shot detection in out-of-distribution scenario, emphasizing its generalization ability."
        ]
    },
    "A2.T10": {
        "caption": "Table 10: Prompt template to simulate the chatbot system for OpenDialKG.\n",
        "table": "<table id=\"A2.T10.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A2.T10.1.1\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">Take a deep breath and work on this problem step by step.</td>\n</tr>\n<tr id=\"A2.T10.1.2\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.2.1\" class=\"ltx_td ltx_align_left\">Given a Dialogue History and Knowledge, your job is to follow instructions in the Knowledge and generate a faithful</td>\n</tr>\n<tr id=\"A2.T10.1.3\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.3.1\" class=\"ltx_td ltx_align_left\">Response based on the Knowledge and Dialogue History.</td>\n</tr>\n<tr id=\"A2.T10.1.4\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.4.1\" class=\"ltx_td ltx_align_left\">#Knowledge#:</td>\n</tr>\n<tr id=\"A2.T10.1.5\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.5.1\" class=\"ltx_td ltx_align_left\">You are a chatbot. Your goal is to continue the conversation by responding to user&#8217;s last utterance.</td>\n</tr>\n<tr id=\"A2.T10.1.6\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.6.1\" class=\"ltx_td ltx_align_left\">You have the following knowledge that can be used to generate your response:</td>\n</tr>\n<tr id=\"A2.T10.1.7\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.7.1\" class=\"ltx_td ltx_align_left\">{KG knowledge}</td>\n</tr>\n<tr id=\"A2.T10.1.8\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.8.1\" class=\"ltx_td ltx_align_left\">#Dialogue History#:</td>\n</tr>\n<tr id=\"A2.T10.1.9\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.9.1\" class=\"ltx_td ltx_align_left\">{dialogue history}</td>\n</tr>\n<tr id=\"A2.T10.1.10\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.10.1\" class=\"ltx_td ltx_align_left\">Now, please generate your response:</td>\n</tr>\n<tr id=\"A2.T10.1.11\" class=\"ltx_tr\">\n<td id=\"A2.T10.1.11.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">#Response#:</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table\u00a010 contains the prompt template that we use to prompt GPT-4 for system responses on OpenDialKG."
        ]
    },
    "A6.T11": {
        "caption": "Table 11: Prompt template of zero-shot hallucination detector GPT-3.5-turbo for Table\u00a01.\n",
        "table": "<table id=\"A6.T11.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A6.T11.1.1\" class=\"ltx_tr\">\n<td id=\"A6.T11.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">&lt;DocumentGivenToAISystem&gt;: {Input/Document}&lt;/DocumentGivenToAISystem&gt;</td>\n</tr>\n<tr id=\"A6.T11.1.2\" class=\"ltx_tr\">\n<td id=\"A6.T11.1.2.1\" class=\"ltx_td ltx_align_left\">&lt;SummaryByAISystem&gt;: {System Output}&lt;/SummaryByAISystem&gt;</td>\n</tr>\n<tr id=\"A6.T11.1.3\" class=\"ltx_tr\">\n<td id=\"A6.T11.1.3.1\" class=\"ltx_td ltx_align_left\">Is the output Summary generated by the AI System Faithful to the Document given to it?</td>\n</tr>\n<tr id=\"A6.T11.1.4\" class=\"ltx_tr\">\n<td id=\"A6.T11.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Or is it Hallucinated? (Answer with +1 for Faithful or -1 for Hallucinated):</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Prompts are shown in Table\u00a011."
        ]
    },
    "A7.T12": {
        "caption": "Table 12: Prompt for GPT-4 (Internal) Zeroshot Approach (The Ternary version with Generic, the binary one omits the part concerned with Generic class)\n",
        "table": "<table id=\"A7.T12.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"A7.T12.1.1\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\">&lt;PromptGivenToExtBot&gt;: {Knowledge}&lt;/PromptGivenToExtBot&gt;</td>\n</tr>\n<tr id=\"A7.T12.1.2\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.2.1\" class=\"ltx_td ltx_align_left\">&lt;ConvHistoryBetweenUserAndExtBot&gt;: {System Output}&lt;/ConvHistoryBetweenUserAndExtBot&gt;</td>\n</tr>\n<tr id=\"A7.T12.1.3\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.3.1\" class=\"ltx_td ltx_align_left\">&lt;ResponseByExtBot&gt;: {System Output}&lt;/ResponseByExtBot&gt;</td>\n</tr>\n<tr id=\"A7.T12.1.4\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.4.1\" class=\"ltx_td ltx_align_left\">The Response here can be either Faithful to the Context (Prompt and ConvHistory) OR it\necan be hallucinated/contain hallucinations</td>\n</tr>\n<tr id=\"A7.T12.1.5\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.5.1\" class=\"ltx_td ltx_align_left\">(says something that is contradictory or not</td>\n</tr>\n<tr id=\"A7.T12.1.6\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.6.1\" class=\"ltx_td ltx_align_left\">entirely or close to likely supported by the context).</td>\n</tr>\n<tr id=\"A7.T12.1.7\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.7.1\" class=\"ltx_td ltx_align_left\">A third possibility is that it says something really generic and not really having a relevant truth value or sufficient relatibility to context,</td>\n</tr>\n<tr id=\"A7.T12.1.8\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.8.1\" class=\"ltx_td ltx_align_left\">such as smalltalk, obviously</td>\n</tr>\n<tr id=\"A7.T12.1.9\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.9.1\" class=\"ltx_td ltx_align_left\">true statements amongst other things.</td>\n</tr>\n<tr id=\"A7.T12.1.10\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.10.1\" class=\"ltx_td ltx_align_left\">Thus a Response can be Faithful, Hallucinated or Generic w.r.t the Prompt given to it and the ConvHistory.</td>\n</tr>\n<tr id=\"A7.T12.1.11\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.11.1\" class=\"ltx_td ltx_align_left\">Is the output Response given by the ExtBot Faithful to the Prompt given to it and the ConvHistory between User and ExtBot so far?</td>\n</tr>\n<tr id=\"A7.T12.1.12\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.12.1\" class=\"ltx_td ltx_align_left\">Or is it Hallucinated? Or is it Generic?</td>\n</tr>\n<tr id=\"A7.T12.1.13\" class=\"ltx_tr\">\n<td id=\"A7.T12.1.13.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">(Answer with 2 for Faithful, 1 for Generic or 0 for Hallucinated):</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Prompts are shown in Table\u00a012"
        ]
    },
    "S1.T1": {
        "caption": "Table 1: Performance evaluation of a GPT-3.5-based zero-shot hallucination detector across different generations of LLMs (see Appendix \u00a7E for prompt). This table illustrates a notable decline in detection efficacy when transitioning from older to more recent LLM iterations.",
        "table": "<table id=\"S1.T1.4\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S1.T1.4.5\" class=\"ltx_tr\">\n<td id=\"S1.T1.4.5.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Hallucination data</td>\n<td id=\"S1.T1.4.5.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">LLMs used in the data</td>\n<td id=\"S1.T1.4.5.3\" class=\"ltx_td ltx_align_right ltx_border_tt\">F1</td>\n</tr>\n<tr id=\"S1.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S1.T1.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">MNBM (&#8217;20)</td>\n<td id=\"S1.T1.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">GPT, Bert, Rnn, ConvNet</td>\n<td id=\"S1.T1.1.1.1\" class=\"ltx_td ltx_align_right ltx_border_t\"><math id=\"S1.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"0.780\" display=\"inline\"><semantics id=\"S1.T1.1.1.1.m1.1a\"><mn id=\"S1.T1.1.1.1.m1.1.1\" xref=\"S1.T1.1.1.1.m1.1.1.cmml\">0.780</mn><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.1.1.1.m1.1b\"><cn type=\"float\" id=\"S1.T1.1.1.1.m1.1.1.cmml\" xref=\"S1.T1.1.1.1.m1.1.1\">0.780</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.1.1.1.m1.1c\">0.780</annotation></semantics></math></td>\n</tr>\n<tr id=\"S1.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S1.T1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r\">FRANK (&#8217;21)</td>\n<td id=\"S1.T1.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_r\">PointerNet, bertS2S, Bart</td>\n<td id=\"S1.T1.2.2.1\" class=\"ltx_td ltx_align_right\"><math id=\"S1.T1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"0.694\" display=\"inline\"><semantics id=\"S1.T1.2.2.1.m1.1a\"><mn id=\"S1.T1.2.2.1.m1.1.1\" xref=\"S1.T1.2.2.1.m1.1.1.cmml\">0.694</mn><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.2.2.1.m1.1b\"><cn type=\"float\" id=\"S1.T1.2.2.1.m1.1.1.cmml\" xref=\"S1.T1.2.2.1.m1.1.1\">0.694</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.2.2.1.m1.1c\">0.694</annotation></semantics></math></td>\n</tr>\n<tr id=\"S1.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S1.T1.3.3.2\" class=\"ltx_td ltx_align_left ltx_border_r\">Seahorse (early &#8217;23)</td>\n<td id=\"S1.T1.3.3.3\" class=\"ltx_td ltx_align_right ltx_border_r\">T5, MT5, PALM</td>\n<td id=\"S1.T1.3.3.1\" class=\"ltx_td ltx_align_right\"><math id=\"S1.T1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"0.576\" display=\"inline\"><semantics id=\"S1.T1.3.3.1.m1.1a\"><mn id=\"S1.T1.3.3.1.m1.1.1\" xref=\"S1.T1.3.3.1.m1.1.1.cmml\">0.576</mn><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.3.3.1.m1.1b\"><cn type=\"float\" id=\"S1.T1.3.3.1.m1.1.1.cmml\" xref=\"S1.T1.3.3.1.m1.1.1\">0.576</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.3.3.1.m1.1c\">0.576</annotation></semantics></math></td>\n</tr>\n<tr id=\"S1.T1.4.4\" class=\"ltx_tr\">\n<td id=\"S1.T1.4.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">ScreenEval (late &#8217;23)</td>\n<td id=\"S1.T1.4.4.3\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\">GPT-4, longformer</td>\n<td id=\"S1.T1.4.4.1\" class=\"ltx_td ltx_align_right ltx_border_bb\"><math id=\"S1.T1.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"0.130\" display=\"inline\"><semantics id=\"S1.T1.4.4.1.m1.1a\"><mn id=\"S1.T1.4.4.1.m1.1.1\" xref=\"S1.T1.4.4.1.m1.1.1.cmml\">0.130</mn><annotation-xml encoding=\"MathML-Content\" id=\"S1.T1.4.4.1.m1.1b\"><cn type=\"float\" id=\"S1.T1.4.4.1.m1.1.1.cmml\" xref=\"S1.T1.4.4.1.m1.1.1\">0.130</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S1.T1.4.4.1.m1.1c\">0.130</annotation></semantics></math></td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "An effective hallucination detection system should be accurate, fast, and affordable. Cost-effectiveness is crucial because every check for hallucinations adds extra cost to the use of large language models (LLMs), which may already be substantially high. Moreover, the system must possess the flexibility to adapt to the rapidly evolving landscape of LLMs. As shown in Table\u00a01, newer iterations of LLMs generally exhibit enhanced capabilities in mitigating hallucinations, thereby escalating the complexity of the detection challenge. Unfortunately, many current methodologies are either i) costly in terms of compute Liu et\u00a0al. (2023); Manakul et\u00a0al. (2023b)\nor ii) depend on out-of-domain/external resources such as QA Honovich et\u00a0al. (2021); Fabbri et\u00a0al. (2022) or NLI annotation \u00a0Laban et\u00a0al. (2022); Honovich et\u00a0al. (2022), potentially compromising performance.",
            "Table 11: Prompt template of zero-shot hallucination detector GPT-3.5-turbo for Table\u00a01.\n"
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Macro-F1 and latency of hallucination detection methods over OpenDialKG-Eval. ",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T2.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.2.1\" class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td id=\"S3.T2.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">F1</td>\n<td id=\"S3.T2.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_tt\">Latency</td>\n</tr>\n<tr id=\"S3.T2.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">Zero-shot Detection</td>\n</tr>\n<tr id=\"S3.T2.1.4\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">SelfCheckGPT (QA) <cite class=\"ltx_cite ltx_citemacro_cite\">Manakul et&#160;al. (<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2023a</a>)</cite>\n</td>\n<td id=\"S3.T2.1.4.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.536</td>\n<td id=\"S3.T2.1.4.3\" class=\"ltx_td ltx_align_right ltx_border_t\">60.59 sec</td>\n</tr>\n<tr id=\"S3.T2.1.5\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_r\">SelfCheckGPT (NLI) <cite class=\"ltx_cite ltx_citemacro_cite\">Manakul et&#160;al. (<a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2023a</a>)</cite>\n</td>\n<td id=\"S3.T2.1.5.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.579</td>\n<td id=\"S3.T2.1.5.3\" class=\"ltx_td ltx_align_right\">0.93 sec</td>\n</tr>\n<tr id=\"S3.T2.1.6\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.6.1\" class=\"ltx_td ltx_align_left ltx_border_r\">G-Eval <cite class=\"ltx_cite ltx_citemacro_cite\">Liu et&#160;al. (<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>\n</td>\n<td id=\"S3.T2.1.6.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.608</td>\n<td id=\"S3.T2.1.6.3\" class=\"ltx_td ltx_align_right\">2.79 sec</td>\n</tr>\n<tr id=\"S3.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_r\">SCALE<sub id=\"S3.T2.1.1.1.1\" class=\"ltx_sub\"><span id=\"S3.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_italic\">XL</span></sub> <cite class=\"ltx_cite ltx_citemacro_cite\">Lattimer et&#160;al. (<a href=\"#bib.bib16\" title=\"\" class=\"ltx_ref\">2023</a>)</cite>\n</td>\n<td id=\"S3.T2.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.687</td>\n<td id=\"S3.T2.1.1.3\" class=\"ltx_td ltx_align_right\">0.22 sec</td>\n</tr>\n<tr id=\"S3.T2.1.7\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.7.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">T5-base Finetuned over Synthetic Data</td>\n</tr>\n<tr id=\"S3.T2.1.8\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.8.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">FADE <cite class=\"ltx_cite ltx_citemacro_cite\">Das et&#160;al. (<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">2022b</a>)</cite>\n</td>\n<td id=\"S3.T2.1.8.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.625</td>\n<td id=\"S3.T2.1.8.3\" class=\"ltx_td ltx_align_right ltx_border_t\">0.20 sec</td>\n</tr>\n<tr id=\"S3.T2.1.9\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.9.1\" class=\"ltx_td ltx_align_left ltx_border_r\">HaluEval <cite class=\"ltx_cite ltx_citemacro_cite\">Li et&#160;al. (<a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">2023a</a>)</cite>\n</td>\n<td id=\"S3.T2.1.9.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.702</td>\n<td id=\"S3.T2.1.9.3\" class=\"ltx_td ltx_align_right\">0.20 sec</td>\n</tr>\n<tr id=\"S3.T2.1.10\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.10.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Our approach</td>\n<td id=\"S3.T2.1.10.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\">0.762</td>\n<td id=\"S3.T2.1.10.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">0.20 sec</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table\u00a02 shows the performance of hallucination detection and latency per response on OpenDialKG-Eval. Latencies are profiled over AWS g5.xlarge instances with no batching sae for G-Eval which requires OpenAI API access.\nFrom the results, our approach not only out-performs T5 detectors finetuned over previous hallucination generation baselines, but more interestingly, it out-performs state-of-the-art zero-shot detection methods. Besides performance, finetuned models achieve significantly lower latency than all zero-shot baselines. We also show the results on BEGIN data. The results can be found in Table\u00a03, where similar observation can be found."
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Macro-F1 and latency of hallucination detection over BEGIN test split with three-class classification.\n",
        "table": "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.1.1\" class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td id=\"S3.T3.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">F1</td>\n<td id=\"S3.T3.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\">Latency</td>\n</tr>\n<tr id=\"S3.T3.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">Zero-shot Detection</td>\n<td id=\"S3.T3.1.2.2\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T3.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">GPT-4 (Internal)</td>\n<td id=\"S3.T3.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.323</td>\n<td id=\"S3.T3.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_t\">1.13 sec</td>\n</tr>\n<tr id=\"S3.T3.1.4\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\">T5-base Finetuned over Synthetic Data</td>\n<td id=\"S3.T3.1.4.2\" class=\"ltx_td ltx_border_t\"></td>\n</tr>\n<tr id=\"S3.T3.1.5\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">AugWow <cite class=\"ltx_cite ltx_citemacro_cite\">Gupta et&#160;al. (<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"S3.T3.1.5.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.378</td>\n<td id=\"S3.T3.1.5.3\" class=\"ltx_td ltx_align_right ltx_border_t\">0.20 sec</td>\n</tr>\n<tr id=\"S3.T3.1.6\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.6.1\" class=\"ltx_td ltx_align_left ltx_border_r\">BEGIN-Adv. <cite class=\"ltx_cite ltx_citemacro_cite\">Dziri et&#160;al. (<a href=\"#bib.bib6\" title=\"\" class=\"ltx_ref\">2022a</a>)</cite>\n</td>\n<td id=\"S3.T3.1.6.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.459</td>\n<td id=\"S3.T3.1.6.3\" class=\"ltx_td ltx_align_right\">0.20 sec</td>\n</tr>\n<tr id=\"S3.T3.1.7\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Our approach</td>\n<td id=\"S3.T3.1.7.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\">0.473</td>\n<td id=\"S3.T3.1.7.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">0.20 sec</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Table\u00a02 shows the performance of hallucination detection and latency per response on OpenDialKG-Eval. Latencies are profiled over AWS g5.xlarge instances with no batching sae for G-Eval which requires OpenAI API access.\nFrom the results, our approach not only out-performs T5 detectors finetuned over previous hallucination generation baselines, but more interestingly, it out-performs state-of-the-art zero-shot detection methods. Besides performance, finetuned models achieve significantly lower latency than all zero-shot baselines. We also show the results on BEGIN data. The results can be found in Table\u00a03, where similar observation can be found."
        ]
    },
    "S3.T4": {
        "caption": "Table 4: Results of ablation study. \u201cpos-F1\u201d and \u201cneg-F1\u201d represents F1 performance over faithful and Hallucination labels separately.\n",
        "table": "<table id=\"S3.T4.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T4.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T4.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Approach</td>\n<td id=\"S3.T4.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">pos-F1</td>\n<td id=\"S3.T4.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">neg-F1</td>\n<td id=\"S3.T4.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_tt\">F1</td>\n</tr>\n<tr id=\"S3.T4.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T4.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Our approach</td>\n<td id=\"S3.T4.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.812</td>\n<td id=\"S3.T4.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.713</td>\n<td id=\"S3.T4.1.2.4\" class=\"ltx_td ltx_align_right ltx_border_t\">0.762</td>\n</tr>\n<tr id=\"S3.T4.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T4.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_r\">w/o faithful generation</td>\n<td id=\"S3.T4.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.747</td>\n<td id=\"S3.T4.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_r\">0.618</td>\n<td id=\"S3.T4.1.3.4\" class=\"ltx_td ltx_align_right\">0.683</td>\n</tr>\n<tr id=\"S3.T4.1.4\" class=\"ltx_tr\">\n<td id=\"S3.T4.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">w/o hallucination generation</td>\n<td id=\"S3.T4.1.4.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\">0.517</td>\n<td id=\"S3.T4.1.4.3\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r\">0.502</td>\n<td id=\"S3.T4.1.4.4\" class=\"ltx_td ltx_align_right ltx_border_bb\">0.509</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "To analyze the significance of both hallucination and faithful response generation, we conduct an ablation study to replace one of the generation using system response. Results are shown in Table\u00a04. Results show that both categories of synthetic data are necessary to effectively fine-tune the detector."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Hallucination patterns appeared in OpenDialKG-Eval and our synthetic generated data for finetuning.",
        "table": "<table id=\"S4.T5.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T5.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">Pattern name</td>\n<td id=\"S4.T5.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">System</td>\n<td id=\"S4.T5.1.2.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">HaluEval</td>\n<td id=\"S4.T5.1.2.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_tt\">FADE</td>\n<td id=\"S4.T5.1.2.5\" class=\"ltx_td ltx_align_right ltx_border_tt\">Ours #</td>\n</tr>\n<tr id=\"S4.T5.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Adding attribute to an entity</td>\n<td id=\"S4.T5.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.540</td>\n<td id=\"S4.T5.1.3.3\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\"><span id=\"S4.T5.1.3.3.1\" class=\"ltx_text ltx_font_bold\">0.435</span></td>\n<td id=\"S4.T5.1.3.4\" class=\"ltx_td ltx_align_right ltx_border_r ltx_border_t\">0.156</td>\n<td id=\"S4.T5.1.3.5\" class=\"ltx_td ltx_align_right ltx_border_t\">0.530</td>\n</tr>\n<tr id=\"S4.T5.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Adding or updating relation</td>\n<td id=\"S4.T5.1.4.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.070</td>\n<td id=\"S4.T5.1.4.3\" class=\"ltx_td ltx_align_right ltx_border_r\">0.150</td>\n<td id=\"S4.T5.1.4.4\" class=\"ltx_td ltx_align_right ltx_border_r\">0.099</td>\n<td id=\"S4.T5.1.4.5\" class=\"ltx_td ltx_align_right\">0.220</td>\n</tr>\n<tr id=\"S4.T5.1.5\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Addding new entities</td>\n<td id=\"S4.T5.1.5.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.050</td>\n<td id=\"S4.T5.1.5.3\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"S4.T5.1.5.3.1\" class=\"ltx_text ltx_font_bold\">0.370</span></td>\n<td id=\"S4.T5.1.5.4\" class=\"ltx_td ltx_align_right ltx_border_r\"><span id=\"S4.T5.1.5.4.1\" class=\"ltx_text ltx_font_bold\">0.675</span></td>\n<td id=\"S4.T5.1.5.5\" class=\"ltx_td ltx_align_right\">0.160</td>\n</tr>\n<tr id=\"S4.T5.1.6\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.6.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Overclaim knowledge/affordance</td>\n<td id=\"S4.T5.1.6.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.027</td>\n<td id=\"S4.T5.1.6.3\" class=\"ltx_td ltx_align_right ltx_border_r\">0.011</td>\n<td id=\"S4.T5.1.6.4\" class=\"ltx_td ltx_align_right ltx_border_r\">0.010</td>\n<td id=\"S4.T5.1.6.5\" class=\"ltx_td ltx_align_right\">0.025</td>\n</tr>\n<tr id=\"S4.T5.1.7\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_r\">Inference error beyond above</td>\n<td id=\"S4.T5.1.7.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.004</td>\n<td id=\"S4.T5.1.7.3\" class=\"ltx_td ltx_align_right ltx_border_r\">0.011</td>\n<td id=\"S4.T5.1.7.4\" class=\"ltx_td ltx_align_right ltx_border_r\">0.018</td>\n<td id=\"S4.T5.1.7.5\" class=\"ltx_td ltx_align_right\">0.010</td>\n</tr>\n<tr id=\"S4.T5.1.8\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.8.1\" class=\"ltx_td ltx_align_left ltx_border_r\">None of the above</td>\n<td id=\"S4.T5.1.8.2\" class=\"ltx_td ltx_align_right ltx_border_r\">0.310</td>\n<td id=\"S4.T5.1.8.3\" class=\"ltx_td ltx_align_right ltx_border_r\">0.016</td>\n<td id=\"S4.T5.1.8.4\" class=\"ltx_td ltx_align_right ltx_border_r\">0.042</td>\n<td id=\"S4.T5.1.8.5\" class=\"ltx_td ltx_align_right\">0.050</td>\n</tr>\n<tr id=\"S4.T5.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\"><math id=\"S4.T5.1.1.1.m1.2\" class=\"ltx_Math\" alttext=\"\\text{KL}(\\bullet,\\text{System})\" display=\"inline\"><semantics id=\"S4.T5.1.1.1.m1.2a\"><mrow id=\"S4.T5.1.1.1.m1.2.3\" xref=\"S4.T5.1.1.1.m1.2.3.cmml\"><mtext id=\"S4.T5.1.1.1.m1.2.3.2\" xref=\"S4.T5.1.1.1.m1.2.3.2a.cmml\">KL</mtext><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T5.1.1.1.m1.2.3.1\" xref=\"S4.T5.1.1.1.m1.2.3.1.cmml\">&#8203;</mo><mrow id=\"S4.T5.1.1.1.m1.2.3.3.2\" xref=\"S4.T5.1.1.1.m1.2.3.3.1.cmml\"><mo stretchy=\"false\" id=\"S4.T5.1.1.1.m1.2.3.3.2.1\" xref=\"S4.T5.1.1.1.m1.2.3.3.1.cmml\">(</mo><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T5.1.1.1.m1.1.1\" xref=\"S4.T5.1.1.1.m1.1.1.cmml\">&#8729;</mo><mo id=\"S4.T5.1.1.1.m1.2.3.3.2.2\" xref=\"S4.T5.1.1.1.m1.2.3.3.1.cmml\">,</mo><mtext id=\"S4.T5.1.1.1.m1.2.2\" xref=\"S4.T5.1.1.1.m1.2.2a.cmml\">System</mtext><mo stretchy=\"false\" id=\"S4.T5.1.1.1.m1.2.3.3.2.3\" xref=\"S4.T5.1.1.1.m1.2.3.3.1.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.1.1.1.m1.2b\"><apply id=\"S4.T5.1.1.1.m1.2.3.cmml\" xref=\"S4.T5.1.1.1.m1.2.3\"><times id=\"S4.T5.1.1.1.m1.2.3.1.cmml\" xref=\"S4.T5.1.1.1.m1.2.3.1\"></times><ci id=\"S4.T5.1.1.1.m1.2.3.2a.cmml\" xref=\"S4.T5.1.1.1.m1.2.3.2\"><mtext id=\"S4.T5.1.1.1.m1.2.3.2.cmml\" xref=\"S4.T5.1.1.1.m1.2.3.2\">KL</mtext></ci><interval closure=\"open\" id=\"S4.T5.1.1.1.m1.2.3.3.1.cmml\" xref=\"S4.T5.1.1.1.m1.2.3.3.2\"><ci id=\"S4.T5.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.1.1.1.m1.1.1\">&#8729;</ci><ci id=\"S4.T5.1.1.1.m1.2.2a.cmml\" xref=\"S4.T5.1.1.1.m1.2.2\"><mtext id=\"S4.T5.1.1.1.m1.2.2.cmml\" xref=\"S4.T5.1.1.1.m1.2.2\">System</mtext></ci></interval></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.1.1.1.m1.2c\">\\text{KL}(\\bullet,\\text{System})</annotation></semantics></math></td>\n<td id=\"S4.T5.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\">-</td>\n<td id=\"S4.T5.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\">0.671</td>\n<td id=\"S4.T5.1.1.4\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_r ltx_border_t\">1.527</td>\n<td id=\"S4.T5.1.1.5\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">0.340</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Pattern distributions are both listed in Table\u00a0\n5 and Figure\u00a02. From the pattern distribution, it is interesting to see that our method has fewer hallucinations from entity replacing/swapping, the most dominant hallucination type is adding unverifiable attributes to an entity. This indicates that our methods generate responses which conform tighter to the real hallucination distribution in contrast to prior approaches. The KL Divergence between the categorical pattern distribution of our method and the system response based distribution is 0.3395, compared to the much greater 0.6706 (and 1.52) between the distribution of HaluEval (and FADE) vs the latter."
        ]
    },
    "S4.T6": {
        "caption": "Table 6: Human analysis of faithfulness of our generated synthetic responses in comparison to system outputs.\n",
        "table": "<table id=\"S4.T6.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S4.T6.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T6.1.1.1\" class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td id=\"S4.T6.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\">Faithfulness</td>\n</tr>\n<tr id=\"S4.T6.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T6.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">System output</td>\n<td id=\"S4.T6.1.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">41%</td>\n</tr>\n<tr id=\"S4.T6.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T6.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Faithful generation</td>\n<td id=\"S4.T6.1.3.2\" class=\"ltx_td ltx_align_right ltx_border_t\">51%</td>\n</tr>\n<tr id=\"S4.T6.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T6.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">Hallucination generation</td>\n<td id=\"S4.T6.1.4.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">5%</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "To more closely evaluate the effectiveness of rewriting, we did human annotation over 100 randomly sampled system responses along with our synthetically generated responses based on these system responses. Table \u00a06 shows the portion of faithful data within each type of responses:"
        ]
    }
}