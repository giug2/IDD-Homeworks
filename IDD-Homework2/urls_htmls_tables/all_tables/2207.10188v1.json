{
    "S3.T1": {
        "caption": "Table 1: Summary of notations.",
        "table": null,
        "footnotes": [],
        "references": []
    },
    "S4.T2": {
        "caption": "Table 2: Comparison of accuracy (%) with 95% confidence intervals (10 iterations) with bitwidth-dedicated and bitwidth-adaptive QAT methods. â€ â€ \\dagger denotes results from [6]. â€¡â€¡{\\ddagger} denotes results from a non-differentiated binarization function. FP stands for 32-bit Full-Precision. â€™-â€™ denotes results not provided.",
        "table": null,
        "footnotes": [],
        "references": [
            "Table\n2 shows the (meta-)test accuracy after (meta-)trained by QAT/bitwidth-adaptive QAT/MEBQAT in multiple model architectures and datasets. Here, bw,basubscriptğ‘ğ‘¤subscriptğ‘ğ‘b_{w},b_{a} are bitwidths used during testing. For each bitwidth setting, accuracy is averaged over one test epoch. Results of vanilla QAT come from individually trained models dedicated to a single bitwidth. All other results come from a single adaptable model, albeit with some prior work containing bitwidth-dedicated parts. Results show that MEBQAT achieves performance comparable to or better than the existing methods."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Comparison of training computation and storage costs.",
        "table": null,
        "footnotes": [],
        "references": [
            "We also tackle the limitations of prior bitwidth-adaptive QAT methods in scalability to the number of target bitwidths. Table 3 shows an overview of training and storage costs of various methods when compared with MEBQAT. Here, T(â‰ƒ|ğ’¯b|2)annotatedğ‘‡similar-to-or-equalsabsentsuperscriptsubscriptğ’¯ğ‘2T({\\simeq}{|\\mathcal{T}_{b}|}^{2}) represents the number of (test) bitwidths, Î˜Î˜\\Theta denotes the total model size, and Î¶ğœ\\zeta indicates the ratio of batch normalization layers respective to the entire model. Because MEBQAT is a meta-learning alternative to bitwidth adaptive learning, our method exhibits fast adaptation, requiring only a few train steps Mğ‘€M. In evaluation scenarios, 1<M(=4)â‰ªT(=73â€‹orâ€‹â€„75)1annotatedğ‘€absent4much-less-thanannotatedğ‘‡absent73or751<M(=4)\\ll T(=73\\;\\text{or}\\;75), showing that MEBQAT is up to 18 times more cost-efficient than other methods since it trains a single model with a single batch normalization layer for all different tasks. Note that computation costs are the same for all non-few-shot methods during testing since inference directly follows quantization. In other words, MEBQAT requires zero additional training during inference. Thus, MEBQAT exhibits much more training efficiency than other adaptive methods in non-few-shot scenarios."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Comparison of accuracy (%) to vanilla FOMAML and FOMAML + QAT, using 5-layer CNN in [9].",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 4 shows the meta-testing accuracy after meta-trained by FOMAML, FOMAML + QAT and MEBQAT-MAML. For each bitwidth setting, accuracy is averaged over 600 different sets of Nğ‘N target classes unseen in the previous phase. It is noteworthy that in some cases, MEBQAT-MAML exceeds the postulated upper bound of accuracy. In other words, although we hypothesized applying bitwidth-dedicated QAT directly to train individual models would have the highest accuracy, we found that in some cases, MEBQAT-MAML achieves performance exceeding the baseline."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Comparison of accuracy (%) to vanilla PN and PN + QAT, using 4-layer CNN in [30].",
        "table": null,
        "footnotes": [],
        "references": [
            "Table 5 shows the meta-testing accuracy after meta-trained by PN, PN + QAT and MEBQAT-PN. For each bitwidth setting, accuracy is averaged over 600 different sets of Nğ‘N target classes unseen in the previous phase. The results prove that MEBQAT is also compliant to metric-based meta-learning such that the base model can fit into any target bitwidth as well as target classes without fine-tuning in the test side."
        ]
    }
}