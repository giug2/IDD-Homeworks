{
    "S4.T1": {
        "caption": "Table 1: Ablations of a single network, evaluated on the VQA v2 validation set. We evaluate variations of our best “reference” model (first row), and show that it corresponds to a local optimum in the space of architectures and hyperparameters. Every row corresponds to one variation of that reference model. We train each variation with three different random seeds and report averages and standard deviations (±plus-or-minus\\pm).",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\">\n<tr id=\"S4.T1.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.1\" class=\"ltx_td ltx_align_justify\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span id=\"S4.T1.1.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.1.2.1.1.1\" class=\"ltx_p\"><span class=\"ltx_rule\" style=\"width:100%;height:0.4pt;background:black;display:inline-block;\"> </span></span>\n</span>\n</td>\n<td id=\"S4.T1.1.2.2\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T1.1.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\" colspan=\"5\"><span id=\"S4.T1.1.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">VQA v2 validation</span></td>\n</tr>\n<tr id=\"S4.T1.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.3.1\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T1.1.3.2\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T1.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" colspan=\"4\"><span id=\"S4.T1.1.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">VQA Score</span></td>\n<td id=\"S4.T1.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S4.T1.1.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Accuracy over</span></td>\n</tr>\n<tr id=\"S4.T1.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.4.1\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T1.1.4.2\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T1.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S4.T1.1.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">All</span></td>\n<td id=\"S4.T1.1.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S4.T1.1.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Yes/no</span></td>\n<td id=\"S4.T1.1.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S4.T1.1.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Numbers</span></td>\n<td id=\"S4.T1.1.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S4.T1.1.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Other</span></td>\n<td id=\"S4.T1.1.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S4.T1.1.4.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">balanced pairs</span></td>\n</tr>\n<tr id=\"S4.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_border_tt ltx_border_tt ltx_border_tt ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span id=\"S4.T1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T1.1.1.2.1.1\" class=\"ltx_p\"><span class=\"ltx_rule\" style=\"width:100%;height:0.4pt;background:black;display:inline-block;\"> </span></span>\n</span><span id=\"S4.T1.1.1.2.2\" class=\"ltx_text\" style=\"font-size:70%;\">\n</span><span id=\"S4.T1.1.1.2.3\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Reference model</span>\n</td>\n<td id=\"S4.T1.1.1.3\" class=\"ltx_td ltx_border_tt ltx_border_tt ltx_border_tt ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt ltx_border_tt ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span id=\"S4.T1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.15 </span><math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mo mathsize=\"70%\" id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S4.T1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:70%;\">0.08</span>\n</td>\n<td id=\"S4.T1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt ltx_border_tt ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S4.T1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.07</span></td>\n<td id=\"S4.T1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt ltx_border_tt ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S4.T1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">42.87</span></td>\n<td id=\"S4.T1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt ltx_border_tt ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S4.T1.1.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">55.81</span></td>\n<td id=\"S4.T1.1.1.7\" class=\"ltx_td ltx_border_tt ltx_border_tt ltx_border_tt ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n</tr>\n</table>\n",
        "footnotes": "",
        "references": [
            "We present an extensive set of experiments that compare our model as presented above (referred to as the reference model) with alternative architecture and hyperparameter values. The objective is to show that the proposed model corresponds to a local optimum in the space of architectures and parameters, and to evaluate the sensitivity of the final performance to each design choice. The following discussion follows the structure of Tables 1 and 2. Note the significant breadth and exhaustivity of the following experiments, which represent more than 3,000 GPU-hours of training time.",
            "Each experiment in this section uses a single network (i.e. no ensemble) that is a variation of our reference model (first row of Table 1). Each network is trained on the official training set of VQA v2 and on the additional questions from the Visual Genome unless specified. Results are reported on the validation test VQA v2 at the best epoch (highest overall VQA score). Each experiment (i.e. each row of Tables 1 and 2) is repeated 3 times, training the same network with different random seeds. We report the average and standard deviation over those three runs. The main performance metric is the standard VQA accuracy [6], i.e. the average ground truth score of the answers predicted for all questions222The ground truth score of a candidate answer j𝑗j to a question i𝑖i is a value si​j∈[0,1]subscript𝑠𝑖𝑗01s_{ij}\\in[0,1] provided with the dataset. It accounts for possible disagreement between annotators: si​j=1.0subscript𝑠𝑖𝑗1.0s_{ij}=1.0 if provided by m≥3𝑚3m\\geq 3 annotators, and s=m/3𝑠𝑚3s=m/3 otherwise. Those scores are further averaged in a 10–choose–9 manner [6].. We additionally report the metric of Accuracy over pairs [37] (last column of Tables 1 and 2). It is the ratio of balanced questions (i.e. questions associated with two images leading to two different answers) that are answered perfectly, i.e. with both predicted answers having a ground truth score of 1.01.01.0. This metric is significantly harder than the standard per-question score since it requires a correct answer to both images of the pair, discouraging blind guesses and reliance on language priors [14, 37].",
            "We now discuss the results of each ablative experiment from Tables 1 and 2 in turn.",
            "The use of additional training questions/answers from the Visual Genome (VG) [24] increase the performance on VQA v2 [14] in all question types. As mentioned above, we only use VG instances with a correct answer appearing the output vocabulary determined on VQA v2, and which use an image also used in VQA v2. Note that the +0.67%percent0.67+0.67\\% increase in performance is modest relative to the amount of additional training questions (an additional  485,000 over the  650,000 of VQA v2). Note that including VG questions relating to MS COCO images not used in VQA v2 resulted in slightly lower final performance (not reported in Table 1). We did not further investigate this issue.",
            "We compare the proposed shuffling of training data which keeps balanced pairs in the same mini-batches, with a standard, arbitrary random shuffling. The results in Table 1 are inconclusive: the overall VQA score is virtually identical either way. The accuracy over pairs however improves with the proposed shuffling. This is to be expected, as the purpose of the proposed method is to improve the learning of differentiating between balanced pairs. The cumulative ablation (row (5)5(5) in Table 2) confirms this advantage more clearly.",
            "We evaluate discarding the training questions that do not have their ground truth answer within the selected candidates. Early experiments have shown that those instances do still carry a useful training signal by drawing the predicted scores of the selected candidate answers towards zero. In Table 1, the VQA score remains virtually identical, but a very slight benefit is observed on the accuracy over pairs by retaining those instances.",
            "In other experiments (not reported in Table 1), we experimented with a tanh activation following the word embeddings as in [13, 22]. This had no significant effect, either with random or pretrained initializations.",
            "All of our non-linear layers are implemented as gated tanh (Section 3.7). These show a clear benefit over the gated ReLU, and even more so over simple ReLU or tanh activations. Note that we also experimented, without success, with various other combinations of highway [29], residual [15] and gating connections (not reported in Table 1). One benefit of gated layers is to double the number of learned parameters without increasing the dimension of the hidden states.",
            "Our architecture uses a simple element-wise product to combine the question and image representations. This proved far superior to a concatenation (not reported in Table 1), but we did not experiment with the various advanced forms of pooling proposed in the recent literature [13, 7]",
            "The size of mini-batches during the optimization proves to have a strong influence on the final performance. Mid-range values in {128,256¯,384¯,512¯,768}128¯256¯384¯512768\\{128,\\underline{256},\\underline{384},\\underline{512},768\\} proved superior to smaller mini-batches (including even smaller values), although they require significantly more memory and high-end GPUs. We observed the optimum mini-batch size to be stable across variations of the network architecture, through other experiments not reported in Table 1.",
            "Note that those experiments include the validation split of VQA v2 for training and use its test-dev split for evaluation, hence the higher overall performance compared to Tables 1 and 2.",
            "We compare in Table 3 the performance of our best model with existing methods. Ours is an ensemble of 30 networks identical to the reference model (first row of Table 1) with the exception of the dimension of the hidden states, increased here to 1,50015001,500. The issue of overfitting (Section 4.7) is mitigated by the large ensemble size. Compared to Table 1, this model also includes here the validation split of VQA v2 for training. Our model obtained the first place at the 2017 VQA Challenge [1]. It still surpasses all competing methods by a significant margin at the time of writing."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Cumulative ablations of a single network, evaluated on the VQA v2 validation set. The ablations of table rows are cumulative from top to bottom. The experimental setup is identical to the one used for Table 1.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle\">\n<tr id=\"S4.T2.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1\" class=\"ltx_td ltx_align_justify\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">\n<span id=\"S4.T2.1.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.2.1.1.1\" class=\"ltx_p\"><span class=\"ltx_rule\" style=\"width:100%;height:0.4pt;background:black;display:inline-block;\"> </span></span>\n</span>\n</td>\n<td id=\"S4.T2.1.2.2\" class=\"ltx_td\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></td>\n<td id=\"S4.T2.1.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"5\"><span id=\"S4.T2.1.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">VQA v2 validation</span></td>\n</tr>\n<tr id=\"S4.T2.1.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.1\" class=\"ltx_td\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></td>\n<td id=\"S4.T2.1.3.2\" class=\"ltx_td\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></td>\n<td id=\"S4.T2.1.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\" colspan=\"4\"><span id=\"S4.T2.1.3.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">VQA Score</span></td>\n<td id=\"S4.T2.1.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.3.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Accuracy over</span></td>\n</tr>\n<tr id=\"S4.T2.1.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.1\" class=\"ltx_td\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></td>\n<td id=\"S4.T2.1.4.2\" class=\"ltx_td\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></td>\n<td id=\"S4.T2.1.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.4.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">All</span></td>\n<td id=\"S4.T2.1.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.4.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Yes/no</span></td>\n<td id=\"S4.T2.1.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.4.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">Numbers</span></td>\n<td id=\"S4.T2.1.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.4.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">Other</span></td>\n<td id=\"S4.T2.1.4.7\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.4.7.1\" class=\"ltx_text\" style=\"font-size:70%;\">balanced pairs</span></td>\n</tr>\n<tr id=\"S4.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.1.2\" class=\"ltx_td ltx_align_justify ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">\n<span id=\"S4.T2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T2.1.1.2.1.1\" class=\"ltx_p\"><span class=\"ltx_rule\" style=\"width:100%;height:0.4pt;background:black;display:inline-block;\"> </span></span>\n</span><span id=\"S4.T2.1.1.2.2\" class=\"ltx_text\" style=\"font-size:70%;\">\n</span><span id=\"S4.T2.1.1.2.3\" class=\"ltx_text ltx_font_bold\" style=\"font-size:70%;\">Reference model</span>\n</td>\n<td id=\"S4.T2.1.1.3\" class=\"ltx_td ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></td>\n<td id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">\n<span id=\"S4.T2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">63.15 </span><math id=\"S4.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.m1.1a\"><mo mathsize=\"70%\" id=\"S4.T2.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S4.T2.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S4.T2.1.1.1.2\" class=\"ltx_text\" style=\"font-size:70%;\">0.08</span>\n</td>\n<td id=\"S4.T2.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">80.07</span></td>\n<td id=\"S4.T2.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.5.1\" class=\"ltx_text\" style=\"font-size:70%;\">42.87</span></td>\n<td id=\"S4.T2.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><span id=\"S4.T2.1.1.6.1\" class=\"ltx_text\" style=\"font-size:70%;\">55.81</span></td>\n<td id=\"S4.T2.1.1.7\" class=\"ltx_td ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></td>\n</tr>\n</table>\n",
        "footnotes": "",
        "references": [
            "Our key findings are summarized with the following characteristics of the proposed model, which enable its high performance (see also Table 2).",
            "We present an extensive set of experiments that compare our model as presented above (referred to as the reference model) with alternative architecture and hyperparameter values. The objective is to show that the proposed model corresponds to a local optimum in the space of architectures and parameters, and to evaluate the sensitivity of the final performance to each design choice. The following discussion follows the structure of Tables 1 and 2. Note the significant breadth and exhaustivity of the following experiments, which represent more than 3,000 GPU-hours of training time.",
            "Each experiment in this section uses a single network (i.e. no ensemble) that is a variation of our reference model (first row of Table 1). Each network is trained on the official training set of VQA v2 and on the additional questions from the Visual Genome unless specified. Results are reported on the validation test VQA v2 at the best epoch (highest overall VQA score). Each experiment (i.e. each row of Tables 1 and 2) is repeated 3 times, training the same network with different random seeds. We report the average and standard deviation over those three runs. The main performance metric is the standard VQA accuracy [6], i.e. the average ground truth score of the answers predicted for all questions222The ground truth score of a candidate answer j𝑗j to a question i𝑖i is a value si​j∈[0,1]subscript𝑠𝑖𝑗01s_{ij}\\in[0,1] provided with the dataset. It accounts for possible disagreement between annotators: si​j=1.0subscript𝑠𝑖𝑗1.0s_{ij}=1.0 if provided by m≥3𝑚3m\\geq 3 annotators, and s=m/3𝑠𝑚3s=m/3 otherwise. Those scores are further averaged in a 10–choose–9 manner [6].. We additionally report the metric of Accuracy over pairs [37] (last column of Tables 1 and 2). It is the ratio of balanced questions (i.e. questions associated with two images leading to two different answers) that are answered perfectly, i.e. with both predicted answers having a ground truth score of 1.01.01.0. This metric is significantly harder than the standard per-question score since it requires a correct answer to both images of the pair, discouraging blind guesses and reliance on language priors [14, 37].",
            "We now discuss the results of each ablative experiment from Tables 1 and 2 in turn.",
            "We compare the proposed shuffling of training data which keeps balanced pairs in the same mini-batches, with a standard, arbitrary random shuffling. The results in Table 1 are inconclusive: the overall VQA score is virtually identical either way. The accuracy over pairs however improves with the proposed shuffling. This is to be expected, as the purpose of the proposed method is to improve the learning of differentiating between balanced pairs. The cumulative ablation (row (5)5(5) in Table 2) confirms this advantage more clearly.",
            "The proposed soft scores perform significantly better than either of the binarized versions (Table 2).",
            "We then compare the sigmoid outputs of our reference model to the common choice of a softmax. Both use a cross-entropy loss. The softmax uses the single ground truth answer provided in the dataset, whereas the sigmoid uses the complete annotation data, occasionally with multiple correct answers marked for one question, due to disagreement between multiple annotators. The sigmoid performs significantly better than a softmax. This observation is confirmed in the cumulative ablations (Table 2)",
            "Note that those experiments include the validation split of VQA v2 for training and use its test-dev split for evaluation, hence the higher overall performance compared to Tables 1 and 2.",
            "We report in Table 2 a series of cumulative ablations of our reference model. We consider a series of characteristics of our model in the inverse of the order in which they could be incorporated into other VQA models. The results follow the trends observed with the individual ablations. Removing each proposed contribution steadily decreases the performance of the model. This set of experiments reveals that the most critical components of our model are the sigmoid outputs instead of a softmax, the soft scores used as ground truth targets, the image features from bottom-up attention [3], the gated tanh activations, the output layers initialized using GloVE and Google Images, and the smart shuffling of training data."
        ]
    },
    "S6.T3": {
        "caption": "Table 3: Comparison of our best model with competing methods. Excerpt from the official VQA v2 Leaderboard [1].",
        "table": "",
        "footnotes": "",
        "references": [
            "We compare in Table 3 the performance of our best model with existing methods. Ours is an ensemble of 30 networks identical to the reference model (first row of Table 1) with the exception of the dimension of the hidden states, increased here to 1,50015001,500. The issue of overfitting (Section 4.7) is mitigated by the large ensemble size. Compared to Table 1, this model also includes here the validation split of VQA v2 for training. Our model obtained the first place at the 2017 VQA Challenge [1]. It still surpasses all competing methods by a significant margin at the time of writing."
        ]
    }
}