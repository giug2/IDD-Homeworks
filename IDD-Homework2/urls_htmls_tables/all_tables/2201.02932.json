{
    "PAPER'S NUMBER OF TABLES": 4,
    "Sx4.T1": {
        "caption": "Table 1: Possible options for client device settings.",
        "table": "<table id=\"Sx4.T1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"Sx4.T1.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"Sx4.T1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"Sx4.T1.1.1.1.1.1.1\" class=\"ltx_text\">Device type</span></td>\n<td id=\"Sx4.T1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">iPhone8, iPhoneXR, iPad2,</td>\n</tr>\n<tr id=\"Sx4.T1.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"Sx4.T1.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Huawei-TAG-TL100, Google Pixel XL, Nexus5,</td>\n</tr>\n<tr id=\"Sx4.T1.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"Sx4.T1.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Samsung Galaxy Tab A, Amazon Fire 7 Tablet</td>\n</tr>\n<tr id=\"Sx4.T1.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"Sx4.T1.1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Data size</td>\n<td id=\"Sx4.T1.1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20, 25, 30, 35, 40, 45, 50, 55, 60</td>\n</tr>\n<tr id=\"Sx4.T1.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"Sx4.T1.1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"Sx4.T1.1.1.5.5.1.1\" class=\"ltx_text\">Location</span></td>\n<td id=\"Sx4.T1.1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Richmond (Virginia), San Francisco (California),</td>\n</tr>\n<tr id=\"Sx4.T1.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"Sx4.T1.1.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">Columbus (Ohio), and Toronto (Ontario)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We evaluate FedMarl using several popular DNN models, including LeNet (LeCun et al. 1998) on MNIST (mni 1998), VGG6 (Simonyan and Zisserman 2014) on CIFAR-10 (cif 2010) and ResNet-18 (He et al. 2016) on Fashion MNIST (fas 2015) for image classification, LSTM (Hochreiter and Schmidhuber 1997) on Shakespeare\ndataset (sha 2017) for text prediction. To simulate device heterogeneity, each client device is randomly assigned a device type, a location and a training set size, as summarized in\nTable 1. We collect training latency data over the above DNNs for each device type under each data size, and measure the time for sending each DNN model from the client devices at each location to the central server, as described in the motivation section.\nTo simulate the non-IID training data at each client device, we sort the training data by its label. For each client device, 80%percent8080\\% of its training data are from one random label, the rest of the training data are sampled uniformly from the remaining labels. The number of training data per device is generated by the power law (Li et al. 2020). All client devices adopt a uniform communication cost Bnt=1subscriptsuperscript𝐵𝑡𝑛1B^{t}_{n}=1 ∀t,nfor-all𝑡𝑛\\forall t,n. During each training round, N=10𝑁10N=10 client devices are randomly selected from a pool of K=100𝐾100K=100 devices. The training latency and communication latency for each device are sampled from the collected traces based on its device type, data size and location. Each client device first performs the probing training and reports its probing loss to the central server. Based on the feedback from the MARL agents, a subset of them will continue their local training process for E=5𝐸5E=5 epochs. The number of training round T𝑇T is set to T=20𝑇20T=20 for VGG6, and T=15𝑇15T=15 for LeNet, ResNet-18 and LSTM, respectively.\nEach MARL agent consists of a MLP of two layers with a hidden layer of 256 neurons. For the reward function (equation 4), we make w1=1.0subscript𝑤11.0w_{1}=1.0, w2=0.2subscript𝑤20.2w_{2}=0.2 and w3=0.1subscript𝑤30.1w_{3}=0.1. The utility function is defined to be U​(x)=10−201+e0.35​(1−x)𝑈𝑥10201superscript𝑒0.351𝑥U(x)=10-\\frac{20}{1+e^{0.35(1-x)}} for shaping the test accuracy. The sizes of the historical information Δ​TpΔsubscript𝑇𝑝\\Delta T_{p} and Δ​TcΔsubscript𝑇𝑐\\Delta T_{c} are set to 333 and 555, respectively. We train the VDN with 300, 200, 300, 200 episodes for LeNet, VGG6, ResNet-18 and LSTM until convergence. To demonstrate the advantage of the MARL over the single-agent RL, we also implement the FedMarl with single-agent RL and compare their convergence behaviors. We observe that MARL approach converges at a much fast speed than single-RL approach under the same training environment.",
            "We first evaluate the FedMarl performance by varying the pool size K𝐾K. Specifically, we utilize the MARL agents trained with K=100𝐾100K=100 and evaluate the performance under K=400𝐾400K=400 and K=800𝐾800K=800. Figure 7(a) depicts the total reward (defined in equation 4) of the algorithms on CIFAR-10. Remember that a higher reward indicates a better overall performance in terms of prediction accuracy, processing latency and bandwidth consumption. It can be seen that FedMarl outperforms the rest algorithms for each K𝐾K. In particular, FedMarl achieves a 1.36×1.36\\times and 1.32×1.32\\times higher total reward than the other algorithms on average under K=400𝐾400K=400 and K=800𝐾800K=800, respectively. Similarly, Figure 7(b) shows the performance of the algorithms under different amount of epochs E𝐸E for local training. FedMarl also obtains the best overall performance under different E𝐸E. Finally, we modify the FL system configuration by introducing new types of client devices. Specifically, in additional to the eight devices shown in Table 1, we introduce another six\nmobile devices to the device pool including iPhone 12, iPhone 7, Samsung Galaxy S21, Google Pixel 5, Samsung A11, Huawei P20 pro. We collect the latency traces for these new devices and evaluate the performances of the algorithms using the new device pool on CIFAR-10 (Figure 7(c)). The results show that the superior performance of the MARL agents can translate across different system settings. This is because the MARL agents only take normalized version of the inputs (equation 3) for generating the decisions, making them independent of a specific system configuration."
        ]
    },
    "Sx4.T2": {
        "caption": "Table 2: Accuracy performance of LeNet, VGG6, ResNet-18, LSTM on their datasets. FedMarl achieves the best accuracies across all the datasets.",
        "table": "<table id=\"Sx4.T2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Sx4.T2.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"Sx4.T2.1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t\"></th>\n<th id=\"Sx4.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">MNIST</th>\n<th id=\"Sx4.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">CIFAR-10</th>\n<th id=\"Sx4.T2.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">F-MNIST</th>\n<th id=\"Sx4.T2.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Shakespeare</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Sx4.T2.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"Sx4.T2.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"Sx4.T2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">FedMarl</span></td>\n<td id=\"Sx4.T2.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"Sx4.T2.1.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">96.91%</span></td>\n<td id=\"Sx4.T2.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"Sx4.T2.1.1.2.1.3.1\" class=\"ltx_text ltx_font_bold\">48.87%</span></td>\n<td id=\"Sx4.T2.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"Sx4.T2.1.1.2.1.4.1\" class=\"ltx_text ltx_font_bold\">96.14%</span></td>\n<td id=\"Sx4.T2.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"Sx4.T2.1.1.2.1.5.1\" class=\"ltx_text ltx_font_bold\">44.58%</span></td>\n</tr>\n<tr id=\"Sx4.T2.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"Sx4.T2.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedAvg</td>\n<td id=\"Sx4.T2.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">94.40%</td>\n<td id=\"Sx4.T2.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">43.49%</td>\n<td id=\"Sx4.T2.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">94.70%</td>\n<td id=\"Sx4.T2.1.1.3.2.5\" class=\"ltx_td ltx_align_center\">40.66%</td>\n</tr>\n<tr id=\"Sx4.T2.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"Sx4.T2.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">HeteroFL</td>\n<td id=\"Sx4.T2.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">96.86%</td>\n<td id=\"Sx4.T2.1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">47.74%</td>\n<td id=\"Sx4.T2.1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">96.08%</td>\n<td id=\"Sx4.T2.1.1.4.3.5\" class=\"ltx_td ltx_align_center\">43.98%</td>\n</tr>\n<tr id=\"Sx4.T2.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"Sx4.T2.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedProx</td>\n<td id=\"Sx4.T2.1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">95.85%</td>\n<td id=\"Sx4.T2.1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">47.32%</td>\n<td id=\"Sx4.T2.1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">95.73%</td>\n<td id=\"Sx4.T2.1.1.5.4.5\" class=\"ltx_td ltx_align_center\">43.66%</td>\n</tr>\n<tr id=\"Sx4.T2.1.1.6.5\" class=\"ltx_tr\">\n<td id=\"Sx4.T2.1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedProx-THS</td>\n<td id=\"Sx4.T2.1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">94.43%</td>\n<td id=\"Sx4.T2.1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">45.51%</td>\n<td id=\"Sx4.T2.1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">94.80%</td>\n<td id=\"Sx4.T2.1.1.6.5.5\" class=\"ltx_td ltx_align_center\">42.69%</td>\n</tr>\n<tr id=\"Sx4.T2.1.1.7.6\" class=\"ltx_tr\">\n<td id=\"Sx4.T2.1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedNova</td>\n<td id=\"Sx4.T2.1.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">96.07%</td>\n<td id=\"Sx4.T2.1.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">47.97%</td>\n<td id=\"Sx4.T2.1.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">95.93%</td>\n<td id=\"Sx4.T2.1.1.7.6.5\" class=\"ltx_td ltx_align_center\">44.10%</td>\n</tr>\n<tr id=\"Sx4.T2.1.1.8.7\" class=\"ltx_tr\">\n<td id=\"Sx4.T2.1.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedNova-THS</td>\n<td id=\"Sx4.T2.1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">95.30%</td>\n<td id=\"Sx4.T2.1.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">44.88%</td>\n<td id=\"Sx4.T2.1.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">94.42%</td>\n<td id=\"Sx4.T2.1.1.8.7.5\" class=\"ltx_td ltx_align_center\">42.41%</td>\n</tr>\n<tr id=\"Sx4.T2.1.1.9.8\" class=\"ltx_tr\">\n<td id=\"Sx4.T2.1.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Oort</td>\n<td id=\"Sx4.T2.1.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">96.09%</td>\n<td id=\"Sx4.T2.1.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">48.11%</td>\n<td id=\"Sx4.T2.1.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\">96.02%</td>\n<td id=\"Sx4.T2.1.1.9.8.5\" class=\"ltx_td ltx_align_center\">44.05%</td>\n</tr>\n<tr id=\"Sx4.T2.1.1.10.9\" class=\"ltx_tr\">\n<td id=\"Sx4.T2.1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">CS</td>\n<td id=\"Sx4.T2.1.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">95.89%</td>\n<td id=\"Sx4.T2.1.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">48.19%</td>\n<td id=\"Sx4.T2.1.1.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">96.14%</td>\n<td id=\"Sx4.T2.1.1.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_b\">43.97%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 2 and Figure 5 show the final test accuracy, total processing latencies and communication costs for all the algorithms. In Figure 5, all values are normalized by the values of the FedMarl. FedAvg, FedProx and FedNova make all the clients transmit their local updates, therefore there is no reduction on processing latency and communication cost. In contrast, Oort, CS, HeteroFL, FedProx-THS and FedNova-THS enables only partial client to report their local updates, which reduces the communication cost and processing latency. We notice that FedMarl, HeteroFL and FedNova outperform the rest algorithms on test accuracy,\nbut FedMarl achieves the optimal accuracy, processing latency and communication cost at the same time."
        ]
    },
    "Sx6.T3": {
        "caption": "Table 3: Performance of the trained MARL agents across different DNNs. All the results are normalized by the performance of agents trained with LeNet and evaluated on itself. Numbers in brackets show the performance after finetuning.",
        "table": "<table id=\"Sx6.T3.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Sx6.T3.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"Sx6.T3.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Source/Target</th>\n<th id=\"Sx6.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">LeNet</th>\n<th id=\"Sx6.T3.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">VGG6</th>\n<th id=\"Sx6.T3.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">ResNet-18</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Sx6.T3.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"Sx6.T3.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">LeNet</td>\n<td id=\"Sx6.T3.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.0 (1.0)</td>\n<td id=\"Sx6.T3.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.720 (0.798)</td>\n<td id=\"Sx6.T3.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.896 (0.930)</td>\n</tr>\n<tr id=\"Sx6.T3.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"Sx6.T3.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">VGG6</td>\n<td id=\"Sx6.T3.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.906 (0.974)</td>\n<td id=\"Sx6.T3.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.803 (0.803)</td>\n<td id=\"Sx6.T3.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.859 (0.927)</td>\n</tr>\n<tr id=\"Sx6.T3.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"Sx6.T3.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">ResNet-18</td>\n<td id=\"Sx6.T3.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.965 (0.989)</td>\n<td id=\"Sx6.T3.1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.731 (0.792)</td>\n<td id=\"Sx6.T3.1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.933 (0.933)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Next, we evaluate the generalizability of MARL agents over different DNN architectures and datasets. In particular, we train the MARL agents with a source DNN architecture (e.g., VGG6 on CIFAR-10), and evaluate their performance using a target DNN (e.g., ResNet-18 on Fashion MNIST). Table 3 shows the normalized rewards of the MARL agents. We notice that the MARL agents achieve a superior performance across different DNNs and datasets in general. For example, the MARL agents trained on ResNet-18 with Fashion MNIST can obtain a reward of 0.965 on LeNet with MNIST, which is comparable with the performance of the MARL agents trained with LeNet from scratch (1.0). Additionally, by finetuning the MARL agents with 10 episodes on the target DNN (Numbers in the brackets in Table 3), we notice further improvements on the reward. This demonstrates that the trained MARL agents can generalize to different DNN architectures and datasets."
        ]
    },
    "Sx6.T4": {
        "caption": "Table 4: ’A’,’L’,’B’ denote the accuracy, latency and communication cost. ",
        "table": "",
        "footnotes": "",
        "references": [
            "In this section, we investigate the impact of relative importance w1,w2,w3subscript𝑤1subscript𝑤2subscript𝑤3w_{1},w_{2},w_{3} in the reward function (equation 4) on the performance of FedMarl. Specifically, besides the original setting with [w1,w2,w3]=[1.0,0.2,0.1]subscript𝑤1subscript𝑤2subscript𝑤31.00.20.1[w_{1},w_{2},w_{3}]=[1.0,0.2,0.1],\nwe increase w2subscript𝑤2w_{2} from 0.2 to 0.5 and w3subscript𝑤3w_{3} from 0.1 to 0.3. This will force the MARL agents to learn a client selection algorithm with a lower processing latency and communication cost. As shown in Table 4, we observe that increasing w2subscript𝑤2w_{2} and w3subscript𝑤3w_{3} will lead to a lower total processing latency and communication cost at the price of lower accuracy. This indicates that FedMarl can adjust its behavior based on the relative importance of the objectives, which enables the application designers to customize the FedMarl based on their preferences."
        ]
    }
}