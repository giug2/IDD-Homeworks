{
    "id_table_1": {
        "caption": "Table 1:  Hyper-parameters for different datasets.",
        "table": "S3.T1.5.5",
        "footnotes": [],
        "references": [
            "Large-scale training data plays a key role in the remarkable success of modern deep learning methods across natural language processing  [ 1 ,  4 ,  15 ] , computer vision  [ 5 ,  2 ]  and multi-modal AI  [ 14 ,  11 ,  12 ] . However, training models with massive data is extremely resource-intensive in terms of computation, storage, and time, which poses a barrier for researchers with limited computational resources. To this end, Dataset Distillation (DD) has been proposed to distill a large-scale dataset into a small synthetic one so that the training effort can be reduced. As shown in  Fig.   1 , the goal of DD is that a model trained on the synthetic small dataset could obtain a comparable performance as a model trained on the original large dataset. The emergence of DD advances data-efficient model training, substantially reducing the costs of the tasks associated with data storage, hyper-parameter tuning, and architectural search  [ 6 ] .",
            "Difficulty-Aligned Technique.  During the distillation, the DATM will sample   t  superscript subscript  t \\theta_{t}^{*} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  and   t + M  superscript subscript  t M \\theta_{t+M}^{*} italic_ start_POSTSUBSCRIPT italic_t + italic_M end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  from the expert training trajectories    superscript  \\tau^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  to form the start and target parameters for the matching. The  M M M italic_M  is a preset hyper-parameter. As mentioned in  Sec.   1 , DATM demonstrates that matching early or late trajectories causes the synthetic data to learn easy or hard patterns. Therefore, the difficulty of the generated patterns could be controlled by restricting the matching range of the trajectories. In detail, a lower bound  T  superscript T T^{-} italic_T start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  and an upper bound  T + superscript T T^{+} italic_T start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  are separately set to establish a sample range, i.e., only the expert network parameters within  {  t  | T   t  T + } conditional-set superscript subscript  t superscript T t superscript T \\{\\theta_{t}^{*}|T^{-}\\leq t\\leq T^{+}\\} { italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT | italic_T start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT  italic_t  italic_T start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT }  could be sampled for the distillation. As a result, the segment of expert training trajectories utilized for the matching can be formulated as:",
            "Hyper-parameters.  The hyper-parameters of the M-DATM are reported in  Tab.   1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Ablation Results for M-DATM.",
        "table": "S3.T2.4",
        "footnotes": [],
        "references": [
            "In each distillation iteration, the   t  subscript superscript  t \\theta^{*}_{t} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and   t + M  subscript superscript  t M \\theta^{*}_{t+M} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + italic_M end_POSTSUBSCRIPT  are first sampled from expert training trajectories as start and target parameters. Then, the   ^ t + N subscript ^  t N \\hat{\\theta}_{t+N} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t + italic_N end_POSTSUBSCRIPT  can be obtained by classification with soft labels (  Eq.   3 ). At the end of the distillation iteration, the matching loss can be calculated by  Eq.   2 . It is then back-propagated to update the synthetic data  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as well as the soft labels  L i subscript L i L_{i} italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ,  D s  y  n = { ( x i , y ^ i = softmax ( L i ) ) \\mathcal{D}_{syn}=\\{(x_{i},\\hat{y}_{i}=\\text{softmax}(L_{i})) caligraphic_D start_POSTSUBSCRIPT italic_s italic_y italic_n end_POSTSUBSCRIPT = { ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = softmax ( italic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) } denotes the targeted synthetic dataset.",
            "During the challenge, we identify two problems when applying DATM to the DD task. In this subsection, we first elaborate on these issues, followed by a comprehensive analysis and the corresponding modifications (as shown in  Fig.   2 (a)) to solve them.",
            "Remove Soft Labels.  The first problem we met during the challenge is the performance gap between the evaluation script of DATM and that of the DD challenge. In other words, the test performance obtained from the official evaluation script of the DD challenge is much lower than that obtained from the DATM evaluation script. We have found that the problem is related to the soft labels technique in DATM. As shown by the red squares in  Fig.   2 (b), the soft labels learned by DATM do not achieve a precise one-to-one correspondence with the labels generated by the evaluation script (which are generated in the default order). Such  label inconsistency  will cause certain synthetic images to be assigned incorrect labels during evaluation, leading to a performance gap. To this end, we remove the soft labels techniques from DATM and directly optimize the synthetic dataset utilizing labels generated in the default order. Formally, the soft cross-entropy (SCE) loss  l s  o  f  t subscript l s o f t l_{soft} italic_l start_POSTSUBSCRIPT italic_s italic_o italic_f italic_t end_POSTSUBSCRIPT  in  Eq.   3  is replaced by the standard cross-entropy (CE) loss  l l l italic_l :",
            "Adjust Matching Range.  In the challenge, the other problem for DATM is its poor performance on Tiny ImageNet. Compared to CIFAR-100, Tiny ImageNet dataset contains richer information, with more classes (200 V.S. 100) and a higher resolution ( 64  64 64 64 64\\times 64 64  64  V.S.  32  32 32 32 32\\times 32 32  32 ), making it more challenging for existing DD methods. After removing the soft labels technique, we found that the original DATM, which matches a relatively late trajectory on Tiny ImageNet, could not be effectively optimized. As shown in  Fig.   2 (c), the loss function and accuracy repeatedly oscillate around the initial point, indicating that the synthetic dataset fails to capture discriminative information during the distillation. With this observation, we conjecture that the removal of soft labels further reduces the information capacity of the synthetic dataset, making it more challenging to capture the late trajectory information (hard patterns). Consequently, we reduce the matching range to  ( T  , T + ) = ( 0 , 20 ) superscript T superscript T 0 20 (T^{-},T^{+})=(0,20) ( italic_T start_POSTSUPERSCRIPT - end_POSTSUPERSCRIPT , italic_T start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT ) = ( 0 , 20 )  to let the synthetic dataset concentrate on easier patterns. This solution has proven effective, achieving the expected performance on Tiny ImageNet. Additionally, we conduct further experiments in  Sec.   3.3  to explore the optimal matching ranges for both CIFAR-100 and Tiny ImageNet datasets.",
            "Ablation Study.  In this subsection, we conduct ablation studies to show the contribution of the proposed two modifications. Specifically, the DATM means our baseline model training on the pre-normalized data as mentioned in  Sec.   3.2 . Removing the soft labels (DATM+M1) leads to significant improvement in performance on CIFAR-100, while performance on Tiny ImageNet remains relatively poor. Its noticeable that the DATM+M1 only removes the soft labels, but the matching range still follows the default settings of the original DATM. After adjusting the matching range to focus the synthetic dataset on easier patterns (DATM+M1+M2), the performances are improved on both datasets, with a remarkable improvement on Tiny ImageNet (+11.21%). The ablation study demonstrates that our two modifications are essential and effective."
        ]
    }
}