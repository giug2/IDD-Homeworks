{
    "S4.T1.3": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.3.3.4\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.3.3.5\">Multi-person</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.3.3.6\">Clothing, etc.</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.1.1.1\">Face Sim. <math alttext=\"\\uparrow(\\%)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T1.1.1.1.m1.1\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mrow id=\"S4.T1.1.1.1.m1.1b\"><mo id=\"S4.T1.1.1.1.m1.1.1\" stretchy=\"false\">&#8593;</mo><mrow id=\"S4.T1.1.1.1.m1.1.2\"><mo id=\"S4.T1.1.1.1.m1.1.2.1\" stretchy=\"false\">(</mo><mo id=\"S4.T1.1.1.1.m1.1.2.2\">%</mo><mo id=\"S4.T1.1.1.1.m1.1.2.3\" stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">\\uparrow(\\%)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T1.1.1.1.m1.1d\">&#8593; ( % )</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.2.2.2\">CLIP-T <math alttext=\"\\uparrow(\\%)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T1.2.2.2.m1.1\"><semantics id=\"S4.T1.2.2.2.m1.1a\"><mrow id=\"S4.T1.2.2.2.m1.1b\"><mo id=\"S4.T1.2.2.2.m1.1.1\" stretchy=\"false\">&#8593;</mo><mrow id=\"S4.T1.2.2.2.m1.1.2\"><mo id=\"S4.T1.2.2.2.m1.1.2.1\" stretchy=\"false\">(</mo><mo id=\"S4.T1.2.2.2.m1.1.2.2\">%</mo><mo id=\"S4.T1.2.2.2.m1.1.2.3\" stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.2.m1.1c\">\\uparrow(\\%)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T1.2.2.2.m1.1d\">&#8593; ( % )</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.3.3.3\">CLIP-I <math alttext=\"\\uparrow(\\%)\" class=\"ltx_math_unparsed\" display=\"inline\" id=\"S4.T1.3.3.3.m1.1\"><semantics id=\"S4.T1.3.3.3.m1.1a\"><mrow id=\"S4.T1.3.3.3.m1.1b\"><mo id=\"S4.T1.3.3.3.m1.1.1\" stretchy=\"false\">&#8593;</mo><mrow id=\"S4.T1.3.3.3.m1.1.2\"><mo id=\"S4.T1.3.3.3.m1.1.2.1\" stretchy=\"false\">(</mo><mo id=\"S4.T1.3.3.3.m1.1.2.2\">%</mo><mo id=\"S4.T1.3.3.3.m1.1.2.3\" stretchy=\"false\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.3.3.m1.1c\">\\uparrow(\\%)</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T1.3.3.3.m1.1d\">&#8593; ( % )</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.3.4.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.4.1.1\">MM-Diff <cite class=\"ltx_cite ltx_citemacro_citep\">(Wei et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.12576v1#bib.bib12\" title=\"\">2024</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.4.1.2\">&#10004;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.4.1.3\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.4.1.4\">60.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.4.1.5\">19.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.3.4.1.6\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T1.3.4.1.6.1\">78.46</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.5.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.5.2.1\">PhotoMaker-V2 <cite class=\"ltx_cite ltx_citemacro_citep\">(Li et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.12576v1#bib.bib8\" title=\"\">2024a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.5.2.2\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.5.2.3\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.5.2.4\">61.83</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.5.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.5.2.5.1\">23.71</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.5.2.6\">75.31</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.6.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.6.3.1\">IP-adapter-FaceID <cite class=\"ltx_cite ltx_citemacro_citep\">(Ye et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.12576v1#bib.bib21\" title=\"\">2023b</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.6.3.2\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.6.3.3\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.6.3.4\">66.66</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.6.3.5\">22.97</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.6.3.6\">68.72</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.7.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.7.4.1\">InstantID <cite class=\"ltx_cite ltx_citemacro_citep\">(Wang et&#160;al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.12576v1#bib.bib15\" title=\"\">2024a</a>)</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.7.4.2\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.7.4.3\">&#10007;</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.7.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.7.4.4.1\">73.90</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.7.4.5\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T1.3.7.4.5.1\">23.39</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.3.7.4.6\">72.50</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.8.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.3.8.5.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.8.5.1.1\">StoryMaker(Ours)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.3.8.5.2\">&#10004;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.3.8.5.3\">&#10004;</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.3.8.5.4\"><span class=\"ltx_text ltx_framed ltx_framed_underline\" id=\"S4.T1.3.8.5.4.1\">67.36</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.3.8.5.5\">21.75</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T1.3.8.5.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.3.8.5.6.1\">79.51</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1:  Quantitative comparisons on character-conditioned generation.The best results are in  bold .",
        "footnotes": [
            "Wei et al. [2024] \nZhichao Wei, Qingkun Su, Long Qin, and Weizhi Wang.\n\n Mm-diff: High-fidelity image personalization via multi-modal condition integration.\n\n arXiv preprint arXiv:2403.15059 , 2024.\n\n",
            "Li et al. [2024a] \nZhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, Ming-Ming Cheng, and Ying Shan.\n\n Photomaker: Customizing realistic human photos via stacked id embedding.\n\n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 8640–8650, 2024a.\n\n",
            "Ye et al. [2023b] \nHu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang.\n\n Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models.\n\n arXiv preprint arxiv:2308.06721 , 2023b.\n\n",
            "Wang et al. [2024a] \nQixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen.\n\n Instantid: Zero-shot identity-preserving generation in seconds.\n\n arXiv preprint arXiv:2401.07519 , 2024a.\n\n"
        ],
        "references": [
            "As shown in Table 1, we compare our StoryMaker with four tuning-free character generation models, including MM-Diff (Wei et al., 2024), PhotoMaker-V2 (Li et al., 2024a), InstantID (Wang et al., 2024a), and IP-Adapter-FaceID (Ye et al., 2023b). Our proposed StoryMaker achieves the highest CLIP-I score among previous methods due to the consistency of the entire portrait, including face, hairstyle, and clothing, though it has a relatively lower CLIP-T, slightly compromising text prompt adherence. For face similarity, our method outperforms others except for InstantID. We attribute InstantID’s superior performance to the extensive training data and the IdentityNet controlling module. It should be noted that among all evaluated methods, only MM-Diff and our method can preserve the ID of multiple persons. Moreover, StoryMaker is the only approach that maintains consistency not only in faces but also in clothing, hairstyles, and bodies."
        ]
    }
}