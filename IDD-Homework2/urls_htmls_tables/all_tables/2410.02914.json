{
    "id_table_1": {
        "caption": "Table 1:  Data Summary. #Corpus indicates the number of documents, while #Calibration and #Test indicate the number of queries. As SCIFACT lacks a calibration set, we randomly split its test set into calibration and test subsets.",
        "table": "S4.T1.2",
        "footnotes": [],
        "references": [
            "In this work, we address this limitation by introducing a novel score refinement method that employs a simple yet effective monotone transformation, inspired by ranking measures, to adjust the scores of any given information retrieval system.  By applying standard conformal prediction methods to these refined scores, we deliver significantly smaller retrieved sets while preserving their statistical guarantees, striking a crucial balance between efficiency and accuracy. An illustration of the proposed pipeline is shown in Figure  1 .  We validate the effectiveness of our method through experiments on three of BEIR  (Thakur et al.,  2021 )  benchmark datasets, demonstrating its ability to outperform competing approaches in producing compact sets that contain the relevant information.",
            "Datasets     For our evaluation, we utilized BEIR  (Thakur et al.,  2021 ) , a large collection of information retrieval benchmarks. Specifically, we focus on the following datasets: FEVER  (Thorne et al.,  2018 ) , SCIFACT  (Wadden et al.,  2020 ) , and FIQA  (Maia et al.,  2018 ) . Data statistics are presented at Table  1 .  It is important to note that each query within these datasets may have multiple relevant documents within the corpus. For this study, we adopted a pragmatic approach, considering the document with the highest score among the relevant documents as the ground truth. This ensures that a successful retrieval implies at least one relevant document is present in the inference set."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance comparison using BGE-large-1.5 on FIQA and SCIFACT across various values of    \\alpha italic_ .",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "While the conformal sets above use a calibrated threshold, other parameterizations are possible, such as setting the calibration parameter to the set size  K K K italic_K , as in ( 2 ). Furthermore, it is important to note that the description above merely presents conformal prediction in its simplest, most common form. However, there have been significant advancements in the field in recent years, leading to the development of more involved and efficient conformal methods  (Romano et al.,  2020 ; Angelopoulos et al.,  2020 )  and to extensions that provide guarantees beyond marginal coverage  (Angelopoulos et al.,  2022a ; Fisch et al.,  2020 ; Li et al.,  2023 ) .",
            "We first conduct experiments on the smaller SCIFACT dataset to optimize the hyperparameter    \\lambda italic_ . The results, shown in Figure  2 , reveal a favorable value for    \\lambda italic_ , prompting us to set   = 0.05  0.05 \\alpha=0.05 italic_ = 0.05 .",
            "Next, we conduct experiments on the large-scale FEVER dataset. As illustrated in Figure  3 , our score refinement method consistently outperforms other approaches by producing significantly smaller retrieved sets in experiments with BGE-large-1.5 across various values of    \\alpha italic_ , while maintaining comparable, albeit slightly lower, empirical coverage.  Results for the other datasets are summarized in Table  2 , consistent with our previous findings. We note that RAPS produced comparable results to APS, so we omit them for brevity. Additional results using E5-Mistral, which exhibit similar trends, are presented in Table  3  of the appendix, along with an ablation study comparing other simple transformations."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Empirical coverage and average group size for FIQA and SCIFACT, alpha values, and methods using the e5-mistral-7b-instruct.",
        "table": "A1.T3.1.1",
        "footnotes": [],
        "references": [
            "Next, we conduct experiments on the large-scale FEVER dataset. As illustrated in Figure  3 , our score refinement method consistently outperforms other approaches by producing significantly smaller retrieved sets in experiments with BGE-large-1.5 across various values of    \\alpha italic_ , while maintaining comparable, albeit slightly lower, empirical coverage.  Results for the other datasets are summarized in Table  2 , consistent with our previous findings. We note that RAPS produced comparable results to APS, so we omit them for brevity. Additional results using E5-Mistral, which exhibit similar trends, are presented in Table  3  of the appendix, along with an ablation study comparing other simple transformations.",
            "Here evaluate our method with the E5-Mistral embedder on SCIFACT and FIQA datasets. Results, presented in Table  3 , show our method consistently outperforms competitors. Moreover, using E5-Mistral leads to improved performance in both empirical coverage and average group size compared to BGE-large-1.5."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Ablation study comparing different score refinement methods with BGE-large-v1.5 encodings. The table shows empirical coverage and average group size for different datasets and methods. Bold values indicate the best performance for each    \\alpha italic_ .",
        "table": "A1.T4.1.1",
        "footnotes": [],
        "references": [
            "In addition to the aforementioned experiments, we compared our method against alternative transformations:  Max Score , where scores are normalized by dividing each by the maximum score, and  Z-Score , which standardizes the initial retrieved scores. The results, summarized in Table  4 , show that our score refinement transformation outperforms these other refinement methods in the vast majority of cases."
        ]
    },
    "global_footnotes": []
}