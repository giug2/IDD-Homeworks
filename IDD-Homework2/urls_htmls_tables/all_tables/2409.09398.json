{
    "id_table_1": {
        "caption": "TABLE I:  Laguage-queried TSE performance evaluation. bal. denotes AudioSet  balanced_train  and unbal. denotes AudioSet  unbalanced_train . The term Vanilla denotes target audio embeddings w/o any augmentation as condition embeddings for training and NI is noise injection for short. Except for  LASS  and  AudioSep , all the methods listed in this table are built upon  CLAPSep  with different training strategies. : reproduction in  [ 11 ] , : others in  [ 9 ]  is comprised of AudioCaps, Clotho v2, WavCaps, VGGSound.",
        "table": "S2.T1.4",
        "footnotes": [],
        "references": [
            "To address these challenges, we propose a retrieval-augmented, language-free training paradigm for language-queried TSE models as detailed in Fig.  1 . Before training, we use a large language model to generate diverse audio captions, which are encoded into text embeddings by the pre-trained CLAP text encoder and stored in an embedding cache. During training, we retrieve the most similar text embeddings based on the target audio embedding and use them as the condition embedding, guiding the TSE model in sound extraction.  This retrieval approach mitigates the modality mismatch and information leakage issues. Additionally, we apply noise injection  [ 12 ,  16 ,  14 ]  to the conditional embedding as an augmentation strategy, enhancing model generalizability.  Extensive experiments show that our retrieval-augmented approach significantly improves performance and generalizability compared to existing state-of-the-art methods. By relaxing the need for parallel data, our method scales easily for large-scale training and outperforms previous supervised training schemes across multiple benchmarks, achieving a 1-2 dB improvement in signal-to-distortion ratio (SDR).",
            "Although the  modality gap  is effectively addressed by Gaussian noise injection, the issue of  information leakage  remains a point of concern. We further propose a retrieval-augmented strategy to address both the modality gap and the information leakage problem simultaneously. As shown in Fig.  1 , before the model training, we first generate massive audio captions by prompting a large language model following  [ 19 ] . These audio captions are then encoded by the pre-trained CLAP text encoder to create an embedding cache  E  R M  D E superscript R M D {\\bm{E}}\\in{\\mathbb{R}}^{M\\times D} bold_italic_E  blackboard_R start_POSTSUPERSCRIPT italic_M  italic_D end_POSTSUPERSCRIPT , where  M M M italic_M  is the number of audio captions generated by LLM.",
            "where the retrieval function is detailed in Alg.  1  and  n 1  N  ( 0 ,  1  I )  R D similar-to subscript n 1 N 0 subscript italic- 1 I superscript R D {\\bm{n}}_{1}\\sim N(\\mathbf{0},\\epsilon_{1}{\\bm{I}})\\in{\\mathbb{R}}^{D} bold_italic_n start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  italic_N ( bold_0 , italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT bold_italic_I )  blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT . We explore the setting of   1 subscript italic- 1 \\epsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  in Section  III-C .",
            "By retrieving pre-encoded text embeddings, we address the modality gap because the queries are in the same text modality in both the training and inference phases. Meanwhile, by using retrieved condition embeddings, the TSE model does not directly see the target audio embeddings during its training, further avoiding information leakage. A more detailed procedure for retrieval-augmented language-free training of a language-queried target sound extraction model is given in Alg.  1 .",
            "We build the language-free training framework upon the state-of-the-art (SOTA) language-queried target sound extraction model, CLAPSep  [ 11 ] . For experiments on the  balanced_train  split, we train the model on a single RTX 3090 GPU with a batch size of 32 for a total of 96,450 steps. The initial learning rate is set to 1e-4, which decays exponentially by a factor of 0.32 at steps 31,150 and 64,300. For experiments on  D  S l  a  r  g  e D subscript S l a r g e \\mathcal{DS}_{large} caligraphic_D caligraphic_S start_POSTSUBSCRIPT italic_l italic_a italic_r italic_g italic_e end_POSTSUBSCRIPT , we train the model on four RTX 3090 GPUs with a global batch size of 64 for a total of 509,400 steps. The initial learning rate is set to 2e-4, which decays exponentially by a factor of 0.32 at steps 169,800 and 339,600. We adopt the negative SI - SDR between the model estimation and the target audio as an end-to-end loss function as detailed in equation  10 ."
        ]
    },
    "global_footnotes": []
}