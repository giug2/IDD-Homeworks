{
    "id_table_1": {
        "caption": "",
        "table": "A1.EGx1",
        "footnotes": [],
        "references": [
            "The rest of the work is structured as follows: after a recap on the related literature in Section  2 , the proposed approach is discussed and empirically verified in Section  3 . Next we explore the use of KNN Shapleys as hardness characterizers via a benchmark study in Section  4.1 , followed by a discussion on how to utilize them for synthetic data generation on real (Section  4.2 ) and simulated data (Section  4.3 ). The results are extensively discussed in Section  5 .",
            "where  C C C italic_C  is an arbitrary constant. The valuator   i subscript italic- i \\phi_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  can be interpreted as weighted sum of all the possible marginal contributions of the datapoint. Exact evaluation of the data Shapleys is prohibitively expensive because of the need to retrain the model  2 n  1 superscript 2 n 1 2^{n-1} 2 start_POSTSUPERSCRIPT italic_n - 1 end_POSTSUPERSCRIPT  times (once per subset) for each observation in the training data. It is possible to approximate the data Shapleys by considering only a small number of subsets  S  D  { i } S D i S\\subseteq D\\setminus\\{i\\} italic_S  italic_D  { italic_i }  in equation  1 , obtained via Monte-Carlo sampling of random permutations. Unfortunately, even state-of-the-art approximations based on this idea, such as  TMC-Shapley   (Ghorbani & Zou,  2019 )  or  Beta-Shapley   (Kwon & Zou,  2022 )  are computationally prohibitive for datasets with  n  1000 much-greater-than n 1000 n\\gg 1000 italic_n  1000 , making them difficult to use in practice. More generally, extensive work has been developed in the literature to make the computation of data Shapleys more efficient, such as  Jia et al. ( 2019 ) ,  Wang et al. ( 2023 )  and  Wang et al. ( 2024 ) .",
            "By generating synthetic data for difficult examples only, we aim to improve the performance of the model  A A \\mathcal{A} caligraphic_A  specifically on the most challenging parts of the data distribution. The proposed procedure is summarized visually in Figure  1 . In the next section, we provide a mathematical intuition on the interpretation of KNN Shapley as hardness charaterizers.",
            "It must be remarked that the value of  s train subscript s train s_{\\mathrm{train}} italic_s start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT  depends on  ( x test , y test ) subscript x test subscript y test (x_{\\mathrm{test}},y_{\\mathrm{test}}) ( italic_x start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT ) , as detailed in Table  2(b)  and equation  1 . The same procedure as equation  3  can be repeated for any  x train  R subscript x train R x_{\\mathrm{train}}\\in\\mathbb{R} italic_x start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT  blackboard_R , with results displayed in Figure  2(c) . As expected, the  1 1 1 1 NN Shapley decreases in the direction of increasing hardness, suggesting an association between low KNN Shapleys and hard regions of feature space.",
            "In  Seedat et al. ( 2024 ) , the problem of hardness characterization is approached quantitatively by comparing existing methods on how confidently they can identify different kinds of hard datapoints on a variety of OpenML datasets  (Vanschoren et al.,  2013 ) . Their toolkit supports two tabular datasets ( diabetes  and  cover ) and it allows to perturb a chosen proportion  p p p italic_p  of datapoints according to one of three hardness types ( mislabeling ,  out-of-distribution , or  atypical ;  cf.  Section  2.1 ). For this study, we use  p  { 0.05 , 0.1 , 0.15 , 0.2 } p 0.05 0.1 0.15 0.2 p\\in\\{0.05,0.1,0.15,0.2\\} italic_p  { 0.05 , 0.1 , 0.15 , 0.2 }  and the performance metric is the  Area Under the Precision Recall Curve  (AUPRC) of an MLP. Results are averaged across three independent runs per hardness type and then across the different hardness types. The KNN Shapleys-based characterizer is implemented by adding a custom class to the original GitHub repository ( https://github.com/seedatnabeel/H-CAT ). The results are plotted in Figure  3 .",
            "On the diabetes dataset, our characterizer outperforms existing methods for all choices of  p p p italic_p , although based on the critical difference diagram in Figure  3(a) , it can be noticed that performance is not significantly different from Data-IQ, the other method to offer native XGBoost support. On the cover dataset (see Figure  3(b) ), our characterizer ranks 7th overall, although once again performance is statistically compatible with Data-IQ. Based on the mathematical intuition discussed in Section  3.1 , KNN Shapleys are expected to be lower when a datapoint lies in a region of feature space where its target is  atypical  ( cf.  Section  2.1 ). When  p p p italic_p  is too large relative to the datasets difficulty, this atypicality is compromised, causing a deterioration in the performance of our characterizer.",
            "Data-IQs confidence scores and aleatoric uncertainties are estimated for each training observation as per Section  2.1 . Figure  4(b)  displays the relationship between the two: in the top left a large amount of easy points, with high confidence and low aleatoric uncertainty; in the bottom left a few hard points, with low confidence and low aleatoric uncertainty; lastly, the ambiguous points are located around the elbow, with high aleatoric uncertainty. Looking at the marginal distributions in Figure  4(c) , it is noticed that XGBoost is generally very confident in its predictions, causing the number of hard points to be low. Setting a low confidence threshold of  0.25 0.25 0.25 0.25 , a high confidence threshold of  0.75 0.75 0.75 0.75  and a low aleatoric uncertainty threshold of  0.2 0.2 0.2 0.2  allows to assign to each point a tag in  { Easy , Hard , Ambiguous } Easy Hard Ambiguous \\{\\textit{Easy},\\textit{Hard},\\textit{Ambiguous}\\} { Easy , Hard , Ambiguous } . Hard points are less than  3 % percent 3 3\\% 3 %  of training data and both hard and ambiguous points have a much higher proportion of defaulters than the full data ( cf.  Table  4(a) ).",
            "The  10 % percent 10 10\\% 10 %  hardest datapoints are chosen as training set for synthetic data generators, with the purpose of establishing whether augmenting only the hardest datapoints makes XGBoost more robust than non-targeted data augmentation. The choice of a hard threshold is imposed by the expensive computational cost of fine-tuning neural networks, and  10 % percent 10 10\\% 10 %  is justified as it is just before the elbow region where the decrease in validation performance when removing hard data slows down ( cf.  Figure  6 ). Both CTGAN and TVAE are implemented in Python using a heavily customized version of the  sdv  package  (Patki et al.,  2016 ) , with extensive details about the training given in Appendix  A.1 . The quality of the synthetic samples can be examined directly for the first feature: Figure  7(a)  (CTGAN) displays a good but not perfect overlap with real data, whereas from Figure  7(b)  (TVAE) we can see better overlap of synthetic and real data with respect to CTGAN for the first feature.",
            "To verify the robustness of the results, we run the same hardness characterization and data augmentation pipeline on simulated data. Specifically, we consider four bivariate normal distributions, assigning to two of them label  y = 0 y 0 y=0 italic_y = 0 , and  y = 1 y 1 y=1 italic_y = 1  to the remaining ones. We draw  n train = 5 000 subscript n train 5000 n_{\\mathrm{train}}=5\\,000 italic_n start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT = 5 000  training datapoints,  n valid = 2 500 subscript n valid 2500 n_{\\mathrm{valid}}=2\\,500 italic_n start_POSTSUBSCRIPT roman_valid end_POSTSUBSCRIPT = 2 500  observations for model validation and lastly  n test = 2 500 subscript n test 2500 n_{\\mathrm{test}}=2\\,500 italic_n start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT = 2 500  datapoints to calculate  5 5 5 5 NN Shapleys. The training dataset can be visualized in Figure  9(a)  on the left, while the  5 % percent 5 5\\% 5 %  hardest points according to  5 5 5 5 NN Shapleys are shown on the right: notice that they are concentrated around the decision boundary, with most of them falling on the wrong side, confirming the issue of  outcome heterogeneity  for hard data points discussed in Section  4.2.1 .",
            "We proceed by tuning TVAE using the GPEI algorithm (described in Appendix  A.1 ) both when augmenting the hardest  5 % percent 5 5\\% 5 %  by  100 % percent 100 100\\% 100 %  and when augmenting the entire dataset by  5 % percent 5 5\\% 5 % . Figures  9(c)  and  9(d)  display the scores after 10 attempts: the targeted approach results in a larger performance improvement, both for the best attempt and on average across different sets of hyperparameters. Information on the tuning process can be found in Appendix  A.3 . Finally, we augment by different amounts using the best attempts, with results displayed in Figure  9(b) : hard points augmentation consistently outperforms the non-targeted approach and improves as more data is added.",
            "Methodologically, this work aims at bridging the gap between learning-based hardness characterization and game theoretic data valuation. The benchmarks reported in Section  4.1  represent the first quantitative comparison between a Shapley-based evaluator and existing hardness characterizers. While research on the two topics has often dealt with similar problems, such as mislabeling or data summarization  (Paul et al.,  2023 ; Jia et al.,  2021 ) , the two paths have never crossed.",
            "The Amex dataset is publicly available on the public data repository  Kaggle . This work uses the denoised version by user  raddar , which is publicly available on Kaggle in  parquet  format . The code to reproduce the results in this work is available in the GitHub repository  tommaso-ferracci/Targeted_Augmentation_Amex . In particular, code to reproduce the results in Section  4  is available in the  analyses/  folder. The  outputs/  folder contains, in addition to figures and results, the model weights for the best synthesizers. Notice how every method from the  ctgan  and  sdv  packages uses the forked versions ( tommaso-ferracci/CTGAN   and  tommaso-ferracci/SDV ) with the custom early stopping. Detailed instructions on how to create a Python virtual environment and install these dependencies are available in the  Makefile . Details around hyperparameter tuning are also reported in Appendices  A.1 ,  A.2  and  A.3 .",
            "This section provides details about CTGAN and TVAE SDGs on the Amex dataset. A common issue when training GANs is the instability of both generator and discriminator losses. More specifically, the two typically move in opposite ways with oscillatory patterns, making it difficult to decide when to stop training. For this reason, we introduce a novel early stopping condition which tracks epoch after epoch a weighted average of the  Kolgomorov-Smirnov  (KS) statistic between real and synthetic data across all individual features. In particular, we choose to use as weights the feature importances, in order to focus more on the features relevant to XGBoost. The patience is set to  50 50 50 50  epochs with a maximum number of epochs of  500 500 500 500 . If the early stopping condition is triggered, the model is reverted to the best epoch. A large batch size of  10 000 10000 10\\,000 10 000  is chosen to limit the number of updates per epoch and guarantee a smoother training process. Both the generator and discriminator are trained using the Adam optimizer  (Kingma & Ba,  2017 ) , setting the learning rate to  2  10  4  2 superscript 10 4 2\\cdot 10^{-4} 2  10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , weight decay to  10  6 superscript 10 6 10^{-6} 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT , and the momentum parameters to   1 = 0.5 subscript  1 0.5 \\beta_{1}=0.5 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.5  and   2 = 0.9 subscript  2 0.9 \\beta_{2}=0.9 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.9 . Figure  10(a)  shows the losses and the statistic epoch by epoch: we can see that the losses move against each other and then more or less converge once the model cannot further improve. We notice oscillatory patterns in the tracked statistic, symptomatic of the training dynamics of the generator and discriminator pair, and the early stopping condition kicking in after around 100 epochs when the weighted KS statistic peaks at  0.83 0.83 0.83 0.83 .",
            "For TVAE trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the architectures of encoder and decoder. We fix the number of hidden layers to two for the encoder and the decoder, tuning the number of units in each hidden layer. Details on the grid search and the best attempt can be found in Table  1 , while a parallel coordinates plot is displayed in Figure  11 . For CTGAN trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the discriminator and generator architectures. We fix the number of hidden layers to two for the discriminator and the generator, tuning the number of units in each hidden layer. Further details can be found in Table  2  and Figure  12 . Similar summaries are reported for TVAE (Table  3 , Figure  13 ) and CTGAN (Table  4 , Figure  14 ) on the entire dataset. Notice once again the deterioration in performance with respect to the targeted case.",
            "The setup is analogous to Appendix  A.2 , with the difference that CTGAN has been discarded due to excessive instability during training, and smaller architectures are considered to account for a much simpler dataset. Specifically, Table  5  and Figure  15  report the results for TVAE trained on the hardest 5%, while Table  6  and Figure  16  present TVAE trained on the entire dataset. Once again, we notice worse performance in the non-targeted case in the parallel coordinates plots ."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "A1.EGx2",
        "footnotes": [],
        "references": [
            "The rest of the work is structured as follows: after a recap on the related literature in Section  2 , the proposed approach is discussed and empirically verified in Section  3 . Next we explore the use of KNN Shapleys as hardness characterizers via a benchmark study in Section  4.1 , followed by a discussion on how to utilize them for synthetic data generation on real (Section  4.2 ) and simulated data (Section  4.3 ). The results are extensively discussed in Section  5 .",
            "Any test data point  ( x test , y test )  R  { 0 , 1 } subscript x test subscript y test R 0 1 (x_{\\mathrm{test}},y_{\\mathrm{test}})\\in\\mathbb{R}\\times\\{0,1\\} ( italic_x start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT )  blackboard_R  { 0 , 1 }  induces a ranking of the training data based on the distances  | x test  x | , x  { x 1 , x 2 , x train } subscript x test x x subscript x 1 subscript x 2 subscript x train |x_{\\mathrm{test}}-x|,\\ x\\in\\{x_{1},x_{2},x_{\\mathrm{train}}\\} | italic_x start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT - italic_x | , italic_x  { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT } . For  x train = 0 subscript x train 0 x_{\\mathrm{train}}=0 italic_x start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT = 0 , the four possible regions with different rankings of the three training points are plotted in Figure  2(b) . The  1 1 1 1 NN Shapley value  s train subscript s train s_{\\mathrm{train}} italic_s start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT  for the point  ( x train , y train ) = ( 0 , 0 ) subscript x train subscript y train 0 0 (x_{\\mathrm{train}},y_{\\mathrm{train}})=(0,0) ( italic_x start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT ) = ( 0 , 0 )  can be calculated explicitly for all the possible values of  ( x test , y test ) subscript x test subscript y test (x_{\\mathrm{test}},y_{\\mathrm{test}}) ( italic_x start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT ) . The results are reported in Table  2(b) . Since the data distribution is known, the expected value  E  ( s train ) E subscript s train \\mathbb{E}(s_{\\mathrm{train}}) blackboard_E ( italic_s start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT )  of the  1 1 1 1 NN Shapley for  ( x train , y train ) = ( 0 , 0 ) subscript x train subscript y train 0 0 (x_{\\mathrm{train}},y_{\\mathrm{train}})=(0,0) ( italic_x start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT ) = ( 0 , 0 )  can be explicitly calculated:",
            "It must be remarked that the value of  s train subscript s train s_{\\mathrm{train}} italic_s start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT  depends on  ( x test , y test ) subscript x test subscript y test (x_{\\mathrm{test}},y_{\\mathrm{test}}) ( italic_x start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT ) , as detailed in Table  2(b)  and equation  1 . The same procedure as equation  3  can be repeated for any  x train  R subscript x train R x_{\\mathrm{train}}\\in\\mathbb{R} italic_x start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT  blackboard_R , with results displayed in Figure  2(c) . As expected, the  1 1 1 1 NN Shapley decreases in the direction of increasing hardness, suggesting an association between low KNN Shapleys and hard regions of feature space.",
            "In  Seedat et al. ( 2024 ) , the problem of hardness characterization is approached quantitatively by comparing existing methods on how confidently they can identify different kinds of hard datapoints on a variety of OpenML datasets  (Vanschoren et al.,  2013 ) . Their toolkit supports two tabular datasets ( diabetes  and  cover ) and it allows to perturb a chosen proportion  p p p italic_p  of datapoints according to one of three hardness types ( mislabeling ,  out-of-distribution , or  atypical ;  cf.  Section  2.1 ). For this study, we use  p  { 0.05 , 0.1 , 0.15 , 0.2 } p 0.05 0.1 0.15 0.2 p\\in\\{0.05,0.1,0.15,0.2\\} italic_p  { 0.05 , 0.1 , 0.15 , 0.2 }  and the performance metric is the  Area Under the Precision Recall Curve  (AUPRC) of an MLP. Results are averaged across three independent runs per hardness type and then across the different hardness types. The KNN Shapleys-based characterizer is implemented by adding a custom class to the original GitHub repository ( https://github.com/seedatnabeel/H-CAT ). The results are plotted in Figure  3 .",
            "On the diabetes dataset, our characterizer outperforms existing methods for all choices of  p p p italic_p , although based on the critical difference diagram in Figure  3(a) , it can be noticed that performance is not significantly different from Data-IQ, the other method to offer native XGBoost support. On the cover dataset (see Figure  3(b) ), our characterizer ranks 7th overall, although once again performance is statistically compatible with Data-IQ. Based on the mathematical intuition discussed in Section  3.1 , KNN Shapleys are expected to be lower when a datapoint lies in a region of feature space where its target is  atypical  ( cf.  Section  2.1 ). When  p p p italic_p  is too large relative to the datasets difficulty, this atypicality is compromised, causing a deterioration in the performance of our characterizer.",
            "Data-IQs confidence scores and aleatoric uncertainties are estimated for each training observation as per Section  2.1 . Figure  4(b)  displays the relationship between the two: in the top left a large amount of easy points, with high confidence and low aleatoric uncertainty; in the bottom left a few hard points, with low confidence and low aleatoric uncertainty; lastly, the ambiguous points are located around the elbow, with high aleatoric uncertainty. Looking at the marginal distributions in Figure  4(c) , it is noticed that XGBoost is generally very confident in its predictions, causing the number of hard points to be low. Setting a low confidence threshold of  0.25 0.25 0.25 0.25 , a high confidence threshold of  0.75 0.75 0.75 0.75  and a low aleatoric uncertainty threshold of  0.2 0.2 0.2 0.2  allows to assign to each point a tag in  { Easy , Hard , Ambiguous } Easy Hard Ambiguous \\{\\textit{Easy},\\textit{Hard},\\textit{Ambiguous}\\} { Easy , Hard , Ambiguous } . Hard points are less than  3 % percent 3 3\\% 3 %  of training data and both hard and ambiguous points have a much higher proportion of defaulters than the full data ( cf.  Table  4(a) ).",
            "Figures  8(a)  and  8(b)  display the scores after 20 attempts: the best scores are in both cases achieved by TVAE, and augmenting only the hardest points generally leads to a more significant improvement in validation performance, both for the best attempts and on average across different sets of hyperparameters. Information on the tuning grid and best hyperparameters are detailed in Appendix  A.2 .",
            "To verify the robustness of the results, we run the same hardness characterization and data augmentation pipeline on simulated data. Specifically, we consider four bivariate normal distributions, assigning to two of them label  y = 0 y 0 y=0 italic_y = 0 , and  y = 1 y 1 y=1 italic_y = 1  to the remaining ones. We draw  n train = 5 000 subscript n train 5000 n_{\\mathrm{train}}=5\\,000 italic_n start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT = 5 000  training datapoints,  n valid = 2 500 subscript n valid 2500 n_{\\mathrm{valid}}=2\\,500 italic_n start_POSTSUBSCRIPT roman_valid end_POSTSUBSCRIPT = 2 500  observations for model validation and lastly  n test = 2 500 subscript n test 2500 n_{\\mathrm{test}}=2\\,500 italic_n start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT = 2 500  datapoints to calculate  5 5 5 5 NN Shapleys. The training dataset can be visualized in Figure  9(a)  on the left, while the  5 % percent 5 5\\% 5 %  hardest points according to  5 5 5 5 NN Shapleys are shown on the right: notice that they are concentrated around the decision boundary, with most of them falling on the wrong side, confirming the issue of  outcome heterogeneity  for hard data points discussed in Section  4.2.1 .",
            "The Amex dataset is publicly available on the public data repository  Kaggle . This work uses the denoised version by user  raddar , which is publicly available on Kaggle in  parquet  format . The code to reproduce the results in this work is available in the GitHub repository  tommaso-ferracci/Targeted_Augmentation_Amex . In particular, code to reproduce the results in Section  4  is available in the  analyses/  folder. The  outputs/  folder contains, in addition to figures and results, the model weights for the best synthesizers. Notice how every method from the  ctgan  and  sdv  packages uses the forked versions ( tommaso-ferracci/CTGAN   and  tommaso-ferracci/SDV ) with the custom early stopping. Detailed instructions on how to create a Python virtual environment and install these dependencies are available in the  Makefile . Details around hyperparameter tuning are also reported in Appendices  A.1 ,  A.2  and  A.3 .",
            "For TVAE trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the architectures of encoder and decoder. We fix the number of hidden layers to two for the encoder and the decoder, tuning the number of units in each hidden layer. Details on the grid search and the best attempt can be found in Table  1 , while a parallel coordinates plot is displayed in Figure  11 . For CTGAN trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the discriminator and generator architectures. We fix the number of hidden layers to two for the discriminator and the generator, tuning the number of units in each hidden layer. Further details can be found in Table  2  and Figure  12 . Similar summaries are reported for TVAE (Table  3 , Figure  13 ) and CTGAN (Table  4 , Figure  14 ) on the entire dataset. Notice once again the deterioration in performance with respect to the targeted case.",
            "The setup is analogous to Appendix  A.2 , with the difference that CTGAN has been discarded due to excessive instability during training, and smaller architectures are considered to account for a much simpler dataset. Specifically, Table  5  and Figure  15  report the results for TVAE trained on the hardest 5%, while Table  6  and Figure  16  present TVAE trained on the entire dataset. Once again, we notice worse performance in the non-targeted case in the parallel coordinates plots ."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S3.F2.sf2.51.45",
        "footnotes": [],
        "references": [
            "The rest of the work is structured as follows: after a recap on the related literature in Section  2 , the proposed approach is discussed and empirically verified in Section  3 . Next we explore the use of KNN Shapleys as hardness characterizers via a benchmark study in Section  4.1 , followed by a discussion on how to utilize them for synthetic data generation on real (Section  4.2 ) and simulated data (Section  4.3 ). The results are extensively discussed in Section  5 .",
            "It must be remarked that the value of  s train subscript s train s_{\\mathrm{train}} italic_s start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT  depends on  ( x test , y test ) subscript x test subscript y test (x_{\\mathrm{test}},y_{\\mathrm{test}}) ( italic_x start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT ) , as detailed in Table  2(b)  and equation  1 . The same procedure as equation  3  can be repeated for any  x train  R subscript x train R x_{\\mathrm{train}}\\in\\mathbb{R} italic_x start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT  blackboard_R , with results displayed in Figure  2(c) . As expected, the  1 1 1 1 NN Shapley decreases in the direction of increasing hardness, suggesting an association between low KNN Shapleys and hard regions of feature space.",
            "In  Seedat et al. ( 2024 ) , the problem of hardness characterization is approached quantitatively by comparing existing methods on how confidently they can identify different kinds of hard datapoints on a variety of OpenML datasets  (Vanschoren et al.,  2013 ) . Their toolkit supports two tabular datasets ( diabetes  and  cover ) and it allows to perturb a chosen proportion  p p p italic_p  of datapoints according to one of three hardness types ( mislabeling ,  out-of-distribution , or  atypical ;  cf.  Section  2.1 ). For this study, we use  p  { 0.05 , 0.1 , 0.15 , 0.2 } p 0.05 0.1 0.15 0.2 p\\in\\{0.05,0.1,0.15,0.2\\} italic_p  { 0.05 , 0.1 , 0.15 , 0.2 }  and the performance metric is the  Area Under the Precision Recall Curve  (AUPRC) of an MLP. Results are averaged across three independent runs per hardness type and then across the different hardness types. The KNN Shapleys-based characterizer is implemented by adding a custom class to the original GitHub repository ( https://github.com/seedatnabeel/H-CAT ). The results are plotted in Figure  3 .",
            "On the diabetes dataset, our characterizer outperforms existing methods for all choices of  p p p italic_p , although based on the critical difference diagram in Figure  3(a) , it can be noticed that performance is not significantly different from Data-IQ, the other method to offer native XGBoost support. On the cover dataset (see Figure  3(b) ), our characterizer ranks 7th overall, although once again performance is statistically compatible with Data-IQ. Based on the mathematical intuition discussed in Section  3.1 , KNN Shapleys are expected to be lower when a datapoint lies in a region of feature space where its target is  atypical  ( cf.  Section  2.1 ). When  p p p italic_p  is too large relative to the datasets difficulty, this atypicality is compromised, causing a deterioration in the performance of our characterizer.",
            "We proceed by tuning TVAE using the GPEI algorithm (described in Appendix  A.1 ) both when augmenting the hardest  5 % percent 5 5\\% 5 %  by  100 % percent 100 100\\% 100 %  and when augmenting the entire dataset by  5 % percent 5 5\\% 5 % . Figures  9(c)  and  9(d)  display the scores after 10 attempts: the targeted approach results in a larger performance improvement, both for the best attempt and on average across different sets of hyperparameters. Information on the tuning process can be found in Appendix  A.3 . Finally, we augment by different amounts using the best attempts, with results displayed in Figure  9(b) : hard points augmentation consistently outperforms the non-targeted approach and improves as more data is added.",
            "The Amex dataset is publicly available on the public data repository  Kaggle . This work uses the denoised version by user  raddar , which is publicly available on Kaggle in  parquet  format . The code to reproduce the results in this work is available in the GitHub repository  tommaso-ferracci/Targeted_Augmentation_Amex . In particular, code to reproduce the results in Section  4  is available in the  analyses/  folder. The  outputs/  folder contains, in addition to figures and results, the model weights for the best synthesizers. Notice how every method from the  ctgan  and  sdv  packages uses the forked versions ( tommaso-ferracci/CTGAN   and  tommaso-ferracci/SDV ) with the custom early stopping. Detailed instructions on how to create a Python virtual environment and install these dependencies are available in the  Makefile . Details around hyperparameter tuning are also reported in Appendices  A.1 ,  A.2  and  A.3 .",
            "For TVAE trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the architectures of encoder and decoder. We fix the number of hidden layers to two for the encoder and the decoder, tuning the number of units in each hidden layer. Details on the grid search and the best attempt can be found in Table  1 , while a parallel coordinates plot is displayed in Figure  11 . For CTGAN trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the discriminator and generator architectures. We fix the number of hidden layers to two for the discriminator and the generator, tuning the number of units in each hidden layer. Further details can be found in Table  2  and Figure  12 . Similar summaries are reported for TVAE (Table  3 , Figure  13 ) and CTGAN (Table  4 , Figure  14 ) on the entire dataset. Notice once again the deterioration in performance with respect to the targeted case."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S4.F4.sf1.7.7",
        "footnotes": [],
        "references": [
            "The rest of the work is structured as follows: after a recap on the related literature in Section  2 , the proposed approach is discussed and empirically verified in Section  3 . Next we explore the use of KNN Shapleys as hardness characterizers via a benchmark study in Section  4.1 , followed by a discussion on how to utilize them for synthetic data generation on real (Section  4.2 ) and simulated data (Section  4.3 ). The results are extensively discussed in Section  5 .",
            "Data-IQs confidence scores and aleatoric uncertainties are estimated for each training observation as per Section  2.1 . Figure  4(b)  displays the relationship between the two: in the top left a large amount of easy points, with high confidence and low aleatoric uncertainty; in the bottom left a few hard points, with low confidence and low aleatoric uncertainty; lastly, the ambiguous points are located around the elbow, with high aleatoric uncertainty. Looking at the marginal distributions in Figure  4(c) , it is noticed that XGBoost is generally very confident in its predictions, causing the number of hard points to be low. Setting a low confidence threshold of  0.25 0.25 0.25 0.25 , a high confidence threshold of  0.75 0.75 0.75 0.75  and a low aleatoric uncertainty threshold of  0.2 0.2 0.2 0.2  allows to assign to each point a tag in  { Easy , Hard , Ambiguous } Easy Hard Ambiguous \\{\\textit{Easy},\\textit{Hard},\\textit{Ambiguous}\\} { Easy , Hard , Ambiguous } . Hard points are less than  3 % percent 3 3\\% 3 %  of training data and both hard and ambiguous points have a much higher proportion of defaulters than the full data ( cf.  Table  4(a) ).",
            "Beyond these initial statistics, it is interesting to know where these points lie in feature space. By looking at the total reduction in loss due to a feature in the nodes where said feature is used to split the data, we can calculate the feature importances of a trained XGBoost classifier. In the case of the Amex dataset, the first feature dominates these scores, with an importance comparable to the bottom 100 features combined. Figure  4(d)  displays the distribution of this feature separated by Data-IQ tag for both defaulters and non-defaulters: hard defaulters somewhat overlap with easy non-defaulters and vice-versa, while ambiguous defaulters are almost indistinguishable from ambiguous non-defaulters. This is a common issue with tabular data, referred to as  outcome heterogeneity  and is known to be captured by Data-IQ  (Seedat et al.,  2022 ) .",
            "To verify the robustness of the results, we run the same hardness characterization and data augmentation pipeline on simulated data. Specifically, we consider four bivariate normal distributions, assigning to two of them label  y = 0 y 0 y=0 italic_y = 0 , and  y = 1 y 1 y=1 italic_y = 1  to the remaining ones. We draw  n train = 5 000 subscript n train 5000 n_{\\mathrm{train}}=5\\,000 italic_n start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT = 5 000  training datapoints,  n valid = 2 500 subscript n valid 2500 n_{\\mathrm{valid}}=2\\,500 italic_n start_POSTSUBSCRIPT roman_valid end_POSTSUBSCRIPT = 2 500  observations for model validation and lastly  n test = 2 500 subscript n test 2500 n_{\\mathrm{test}}=2\\,500 italic_n start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT = 2 500  datapoints to calculate  5 5 5 5 NN Shapleys. The training dataset can be visualized in Figure  9(a)  on the left, while the  5 % percent 5 5\\% 5 %  hardest points according to  5 5 5 5 NN Shapleys are shown on the right: notice that they are concentrated around the decision boundary, with most of them falling on the wrong side, confirming the issue of  outcome heterogeneity  for hard data points discussed in Section  4.2.1 .",
            "Methodologically, this work aims at bridging the gap between learning-based hardness characterization and game theoretic data valuation. The benchmarks reported in Section  4.1  represent the first quantitative comparison between a Shapley-based evaluator and existing hardness characterizers. While research on the two topics has often dealt with similar problems, such as mislabeling or data summarization  (Paul et al.,  2023 ; Jia et al.,  2021 ) , the two paths have never crossed.",
            "The Amex dataset is publicly available on the public data repository  Kaggle . This work uses the denoised version by user  raddar , which is publicly available on Kaggle in  parquet  format . The code to reproduce the results in this work is available in the GitHub repository  tommaso-ferracci/Targeted_Augmentation_Amex . In particular, code to reproduce the results in Section  4  is available in the  analyses/  folder. The  outputs/  folder contains, in addition to figures and results, the model weights for the best synthesizers. Notice how every method from the  ctgan  and  sdv  packages uses the forked versions ( tommaso-ferracci/CTGAN   and  tommaso-ferracci/SDV ) with the custom early stopping. Detailed instructions on how to create a Python virtual environment and install these dependencies are available in the  Makefile . Details around hyperparameter tuning are also reported in Appendices  A.1 ,  A.2  and  A.3 .",
            "For TVAE trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the architectures of encoder and decoder. We fix the number of hidden layers to two for the encoder and the decoder, tuning the number of units in each hidden layer. Details on the grid search and the best attempt can be found in Table  1 , while a parallel coordinates plot is displayed in Figure  11 . For CTGAN trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the discriminator and generator architectures. We fix the number of hidden layers to two for the discriminator and the generator, tuning the number of units in each hidden layer. Further details can be found in Table  2  and Figure  12 . Similar summaries are reported for TVAE (Table  3 , Figure  13 ) and CTGAN (Table  4 , Figure  14 ) on the entire dataset. Notice once again the deterioration in performance with respect to the targeted case."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S4.F8.sf3.11.9",
        "footnotes": [],
        "references": [
            "The rest of the work is structured as follows: after a recap on the related literature in Section  2 , the proposed approach is discussed and empirically verified in Section  3 . Next we explore the use of KNN Shapleys as hardness characterizers via a benchmark study in Section  4.1 , followed by a discussion on how to utilize them for synthetic data generation on real (Section  4.2 ) and simulated data (Section  4.3 ). The results are extensively discussed in Section  5 .",
            "The setup is analogous to Appendix  A.2 , with the difference that CTGAN has been discarded due to excessive instability during training, and smaller architectures are considered to account for a much simpler dataset. Specifically, Table  5  and Figure  15  report the results for TVAE trained on the hardest 5%, while Table  6  and Figure  16  present TVAE trained on the entire dataset. Once again, we notice worse performance in the non-targeted case in the parallel coordinates plots ."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A1.F11.fig1.1.1",
        "footnotes": [],
        "references": [
            "KNN Shapleys are then computed for  K  { 1 , 5 , 100 } K 1 5 100 K\\in\\{1,5,100\\} italic_K  { 1 , 5 , 100 }  under the assumption that the  n test subscript n test n_{\\mathrm{test}} italic_n start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT  available out-of-sample data points are representative of the entire data distribution. Figure  6  displays the distribution of 100NN Shapleys separated by Data-IQs tag: hard points as per Data-IQ exhibit low 100NN Shapleys, with very good adherence for the hardest points. In addition, the left tail of the ambiguous points lies in the region of low 100NN Shapleys and would thus be recognized as hard by our novel characterizer.",
            "To choose the best  K K K italic_K , validation Gini is monitored as we re-train XGBoost after gradually removing the hardest datapoints. These points are expected to be the most valuable for XGBoost and thus the best hardness characterizer should be the one displaying the fastest drop in validation performance. 100NN Shapleys outperform both other choices of  K K K italic_K  and Data-IQ ( cf.  Figure  6 ). This comparison highlights an issue with Data-IQ: after a sharp drop when the hard points are removed, performance decreases at a rate comparable to random as the most ambiguous points are removed.  Seedat et al. ( 2022 )  argue that the removal of ambiguous points should actually make the model more robust, as aleatoric uncertainty is data-dependent and cannot be reduced unless additional features are collected. This is however not the case for the Amex dataset, and beyond the small amount of hard points identified, Data-IQ does not provide guidance on the next most valuable points.",
            "The  10 % percent 10 10\\% 10 %  hardest datapoints are chosen as training set for synthetic data generators, with the purpose of establishing whether augmenting only the hardest datapoints makes XGBoost more robust than non-targeted data augmentation. The choice of a hard threshold is imposed by the expensive computational cost of fine-tuning neural networks, and  10 % percent 10 10\\% 10 %  is justified as it is just before the elbow region where the decrease in validation performance when removing hard data slows down ( cf.  Figure  6 ). Both CTGAN and TVAE are implemented in Python using a heavily customized version of the  sdv  package  (Patki et al.,  2016 ) , with extensive details about the training given in Appendix  A.1 . The quality of the synthetic samples can be examined directly for the first feature: Figure  7(a)  (CTGAN) displays a good but not perfect overlap with real data, whereas from Figure  7(b)  (TVAE) we can see better overlap of synthetic and real data with respect to CTGAN for the first feature.",
            "The setup is analogous to Appendix  A.2 , with the difference that CTGAN has been discarded due to excessive instability during training, and smaller architectures are considered to account for a much simpler dataset. Specifically, Table  5  and Figure  15  report the results for TVAE trained on the hardest 5%, while Table  6  and Figure  16  present TVAE trained on the entire dataset. Once again, we notice worse performance in the non-targeted case in the parallel coordinates plots ."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.F12.fig1.1.1",
        "footnotes": [],
        "references": [
            "The  10 % percent 10 10\\% 10 %  hardest datapoints are chosen as training set for synthetic data generators, with the purpose of establishing whether augmenting only the hardest datapoints makes XGBoost more robust than non-targeted data augmentation. The choice of a hard threshold is imposed by the expensive computational cost of fine-tuning neural networks, and  10 % percent 10 10\\% 10 %  is justified as it is just before the elbow region where the decrease in validation performance when removing hard data slows down ( cf.  Figure  6 ). Both CTGAN and TVAE are implemented in Python using a heavily customized version of the  sdv  package  (Patki et al.,  2016 ) , with extensive details about the training given in Appendix  A.1 . The quality of the synthetic samples can be examined directly for the first feature: Figure  7(a)  (CTGAN) displays a good but not perfect overlap with real data, whereas from Figure  7(b)  (TVAE) we can see better overlap of synthetic and real data with respect to CTGAN for the first feature."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.F13.fig1.1.1",
        "footnotes": [],
        "references": [
            "Figures  8(a)  and  8(b)  display the scores after 20 attempts: the best scores are in both cases achieved by TVAE, and augmenting only the hardest points generally leads to a more significant improvement in validation performance, both for the best attempts and on average across different sets of hyperparameters. Information on the tuning grid and best hyperparameters are detailed in Appendix  A.2 .",
            "Table  8(c)  reports the performance variations for the best TVAE, CTGAN and SMOTE: the latter does not work for the hardest points and is outperformed by TVAE for general augmentation. The robustness of these results for the best attempts can be verified by augmenting by different amounts ( 5 % , 10 % , 15 % , 20 % percent 5 percent 10 percent 15 percent 20 5\\%,10\\%,15\\%,20\\% 5 % , 10 % , 15 % , 20 % ), with results displayed in Figure  8(d) : generating more hard points further increases performance with diminishing returns after  15 % percent 15 15\\% 15 % , while for non-targeted augmentation, the performance actually worsens as more data is added due to the extra noise. Finally, to quantify the magnitude of this improvement, adding back the  n test = 50 000 subscript n test 50000 n_{\\mathrm{test}}=50\\,000 italic_n start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT = 50 000  observations used to compute KNN Shapleys into training data improves validation performance by   0.000326 absent 0.000326 \\approx 0.000326  0.000326 , which is less than the improvement obtained from augmentation of the hard points via TVAE."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.F14.fig1.1.1",
        "footnotes": [],
        "references": [
            "To verify the robustness of the results, we run the same hardness characterization and data augmentation pipeline on simulated data. Specifically, we consider four bivariate normal distributions, assigning to two of them label  y = 0 y 0 y=0 italic_y = 0 , and  y = 1 y 1 y=1 italic_y = 1  to the remaining ones. We draw  n train = 5 000 subscript n train 5000 n_{\\mathrm{train}}=5\\,000 italic_n start_POSTSUBSCRIPT roman_train end_POSTSUBSCRIPT = 5 000  training datapoints,  n valid = 2 500 subscript n valid 2500 n_{\\mathrm{valid}}=2\\,500 italic_n start_POSTSUBSCRIPT roman_valid end_POSTSUBSCRIPT = 2 500  observations for model validation and lastly  n test = 2 500 subscript n test 2500 n_{\\mathrm{test}}=2\\,500 italic_n start_POSTSUBSCRIPT roman_test end_POSTSUBSCRIPT = 2 500  datapoints to calculate  5 5 5 5 NN Shapleys. The training dataset can be visualized in Figure  9(a)  on the left, while the  5 % percent 5 5\\% 5 %  hardest points according to  5 5 5 5 NN Shapleys are shown on the right: notice that they are concentrated around the decision boundary, with most of them falling on the wrong side, confirming the issue of  outcome heterogeneity  for hard data points discussed in Section  4.2.1 .",
            "We proceed by tuning TVAE using the GPEI algorithm (described in Appendix  A.1 ) both when augmenting the hardest  5 % percent 5 5\\% 5 %  by  100 % percent 100 100\\% 100 %  and when augmenting the entire dataset by  5 % percent 5 5\\% 5 % . Figures  9(c)  and  9(d)  display the scores after 10 attempts: the targeted approach results in a larger performance improvement, both for the best attempt and on average across different sets of hyperparameters. Information on the tuning process can be found in Appendix  A.3 . Finally, we augment by different amounts using the best attempts, with results displayed in Figure  9(b) : hard points augmentation consistently outperforms the non-targeted approach and improves as more data is added."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A1.F15.fig1.1.1",
        "footnotes": [],
        "references": [
            "This section provides details about CTGAN and TVAE SDGs on the Amex dataset. A common issue when training GANs is the instability of both generator and discriminator losses. More specifically, the two typically move in opposite ways with oscillatory patterns, making it difficult to decide when to stop training. For this reason, we introduce a novel early stopping condition which tracks epoch after epoch a weighted average of the  Kolgomorov-Smirnov  (KS) statistic between real and synthetic data across all individual features. In particular, we choose to use as weights the feature importances, in order to focus more on the features relevant to XGBoost. The patience is set to  50 50 50 50  epochs with a maximum number of epochs of  500 500 500 500 . If the early stopping condition is triggered, the model is reverted to the best epoch. A large batch size of  10 000 10000 10\\,000 10 000  is chosen to limit the number of updates per epoch and guarantee a smoother training process. Both the generator and discriminator are trained using the Adam optimizer  (Kingma & Ba,  2017 ) , setting the learning rate to  2  10  4  2 superscript 10 4 2\\cdot 10^{-4} 2  10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , weight decay to  10  6 superscript 10 6 10^{-6} 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT , and the momentum parameters to   1 = 0.5 subscript  1 0.5 \\beta_{1}=0.5 italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT = 0.5  and   2 = 0.9 subscript  2 0.9 \\beta_{2}=0.9 italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT = 0.9 . Figure  10(a)  shows the losses and the statistic epoch by epoch: we can see that the losses move against each other and then more or less converge once the model cannot further improve. We notice oscillatory patterns in the tracked statistic, symptomatic of the training dynamics of the generator and discriminator pair, and the early stopping condition kicking in after around 100 epochs when the weighted KS statistic peaks at  0.83 0.83 0.83 0.83 ."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A1.F16.fig1.1.1",
        "footnotes": [],
        "references": [
            "For TVAE trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the architectures of encoder and decoder. We fix the number of hidden layers to two for the encoder and the decoder, tuning the number of units in each hidden layer. Details on the grid search and the best attempt can be found in Table  1 , while a parallel coordinates plot is displayed in Figure  11 . For CTGAN trained on the hardest  10 % percent 10 10\\% 10 % , tuning is performed on the embedding dimension and the discriminator and generator architectures. We fix the number of hidden layers to two for the discriminator and the generator, tuning the number of units in each hidden layer. Further details can be found in Table  2  and Figure  12 . Similar summaries are reported for TVAE (Table  3 , Figure  13 ) and CTGAN (Table  4 , Figure  14 ) on the entire dataset. Notice once again the deterioration in performance with respect to the targeted case."
        ]
    }
}