{
    "id_table_1": {
        "caption": "Table 1:  Performance comparison of our model on 7B and 13B parameters using four methods across seven tasks. In POPE, (R), (P), and (A) stand for Random, Popular, and Adversarial subsets, respectively (applies to all tables below.).  Vizwiz V superscript Vizwiz V \\text{Vizwiz}^{V} Vizwiz start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT  and  Vizwiz C superscript Vizwiz C \\text{Vizwiz}^{C} Vizwiz start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT  represents VQA and captioning based on Vizwiz. The best performance in the table is highlighted in  bold .",
        "table": "S2.T1.2.2",
        "footnotes": [],
        "references": [
            "Typically, the retrieval process  P    ( i j  x )   P conditional subscript i j x \\bar{P}(i_{j}\\mid x) over  start_ARG italic_P end_ARG ( italic_i start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  italic_x )  or  P    ( c j  x )   P conditional subscript c j x \\bar{P}(c_{j}\\mid x) over  start_ARG italic_P end_ARG ( italic_c start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  italic_x )  is typically implemented by computing image-to-image or image-to-text similarity in CLIP embedding space  Ramos et al. ( 2023c ,  a ) . However, this retrieval process is not always reliable, leading to the inclusion of irrelevant or misleading references. For example in Figure  1 , the similarity scores returned the  Top-2  images most similar to the test image. Nevertheless, these two images contribute differently to the original question. The latter image misleads the model and causes incorrect responses.",
            "The algorithm of our method is shown in the Appendix Algorithm  1 .",
            "Table  1  presents the comparison of our model, trained with our method, against four other methods. On the VQA task, our model significantly outperforms previous methods, with a VQA accuracy improvement of approximately 3.7% for the 7B model compared to zero-shot and 2.3% compared to Filter-RAG, achieving state-of-the-art results. Furthermore, on the captioning task, the improvement of our model is even more pronounced (detailed results can be found in the Appendix).",
            "We present four examples comparing our method with zero-shot and vanilla-RAG, as shown in Figures  8 ,  9 ,  10 , and  11 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance of our 7B model compared to four ICL models on the three POPE subsets (VQA) and MS-COCO (captioning). The results of the ICL models are directly from the original paper.",
        "table": "S2.T2.1.1",
        "footnotes": [],
        "references": [
            "As shown in Figure  2 , when addressing tasks such as VQA, captioning, and classification, we can enhance the model performance by retrieving relevant images and their corresponding descriptions to provide a pattern mapping for the input  x x x italic_x . The collection of pattern state is denoted as  M = { M 0 , M 1 ,  , M n } M subscript M 0 subscript M 1  subscript M n \\mathbb{M}=\\{M_{0},M_{1},\\cdots,M_{n}\\} blackboard_M = { italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_M start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } , and   M i  ( [ I , T ]  M ) similar-to subscript M i I T M M_{i}\\sim([I,T]\\in\\mathbb{M}) italic_M start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  ( [ italic_I , italic_T ]  blackboard_M )   , where  I I I italic_I  and  T T T italic_T  denote the image and description, respectively. Next, our goal is to learn this mapping:",
            "We evaluated our model using seven datasets across three distinct tasks: VQA: POPE  Li et al. ( 2023c ) , MMStar  Chen et al. ( 2024 ) , Vizwiz-VQA  Chen et al. ( 2022 ) , Image Captioning: MS-COCO  Lin et al. ( 2014 ) , Vizwiz-Caption  Gurari et al. ( 2020 ) , Image Classification: CIFAR-10  Krizhevsky ( 2009 ) , EmoSet  Yang et al. ( 2023a ) . For more detailed information and metrics can be found in the Appendix  A.2 .",
            "We collected 60,000 initial incorrect responses from LVLMs and generated 10,000 samples with positive and negative sample pairs. After filtering, we refined this to 2,000 samples for the final training data. We use LLaVA-1.5 as the LVLM backbone of our model SURf-7B and SURf-13B and use CLIP (ViT-L with a resolution of 336*336)  Radford et al. ( 2021 )  as the vision encoder. Our 7B and 13B models are further trained from the instruction-finetuned LLaVA-1.5-7B and LLaVA-1.5-13B models following previous works  Lin et al. ( 2024 ,  2023a ); Li et al. ( 2023b ); Liu et al. ( 2023c )  since LLaVA is the most popular used LVLMs. We use 8 A100-80G to training 1 hour for 2 epochs. For the VQA and image captioning task, we use exact match and Bert-Score  Zhang et al. ( 2020 )  as the evaluation tool respectively, mentioned in Section  2.4.2 . We use ShareGPT4v-PT  Chen et al. ( 2023 )  as our database for RAG, which includes approximately 1,246k image-caption pairs with an average caption length of 826. For the retrieval system, we use FAISS  Johnson et al. ( 2021 )  with flat indexes to pre-index the computed embeddings of all images in the database.",
            "The experiments in Table  2  compare our model with various ICL models, as ICL models are very similar to ours at the input level. Despite having fewer parameters and exemplars (For the ICL models, more shots correspond to better performance, we used their 4-shot results for comparison since they only reported results for 4-shot or 32-shot scenarios.) in the prompts compared to the other models, our model achieves the best performance on both the POPE and MS-COCO datasets. Specifically, it improves the average accuracy by 3.4% and the F1 score by 3.8% on POPE compared to the second-best model. This demonstrates that our model can effectively utilize the retrieved content to enhance the performance of downstream tasks."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance comparison of our model and vanilla-RAG on three tasks when introducing irrelevant image-caption pairs. \"1k to 1,000k\" indicates the range of similarity between the introduced images and the test images, with larger values indicating less relevance.",
        "table": "S2.T3.1.1",
        "footnotes": [],
        "references": [
            "Tables  3  and  4  present the results of our robustness tests. In Table  3 , we maintain the image-caption pair with the highest CLIP similarity score among the retrieved content to ensure effective information. We also introduce image-caption pairs from the Top-K (from 1k to 1,000k) positions as forced irrelevant information. The results show that the performance of vanilla-RAG significantly declines on the three datasets as more irrelevant image-caption pairs are introduced. In contrast, our models performance remains very stable. Notably, the models performance when introducing 100k and 1000k irrelevant image-caption pairs is better than when introducing 1k pairs. This improvement is because, after training with hard negative samples, our model can easily distinguish content unrelated to the test image and question, thereby focusing more on other relevant information in the retrieval."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance comparison of our model and vanilla-RAG on three tasks in the random switching of retrieved content setting.",
        "table": "S2.T4.1.1",
        "footnotes": [],
        "references": [
            "To better understand the impact of RAG on model performance, we conducted experiments comparing direct inference with RAG-enhanced inference across three datasets. Figure  4  illustrates the performance differences. It can be observed that retrieving and incorporating additional multimodal information (both image and text) significantly improves the models performance in tasks across VQA, Image Captioning, and Classification.",
            "Figure  4  demonstrates the impact of irrelevant information on RAG. It can be seen that the performance of RAG is even worse than without introducing any additional information, which indicates the negative impact of irrelevant or disturbing information on current LVLMs. We believe that the RAG of current LVLMs still has significant potential. If we can teach the model to selectively utilize the retrieved information and ignore the irrelevant or misleading ones, the performance of RAG in LVLMs will be further improved, potentially approaching the results shown by the gray bars.",
            "We collected 60,000 initial incorrect responses from LVLMs and generated 10,000 samples with positive and negative sample pairs. After filtering, we refined this to 2,000 samples for the final training data. We use LLaVA-1.5 as the LVLM backbone of our model SURf-7B and SURf-13B and use CLIP (ViT-L with a resolution of 336*336)  Radford et al. ( 2021 )  as the vision encoder. Our 7B and 13B models are further trained from the instruction-finetuned LLaVA-1.5-7B and LLaVA-1.5-13B models following previous works  Lin et al. ( 2024 ,  2023a ); Li et al. ( 2023b ); Liu et al. ( 2023c )  since LLaVA is the most popular used LVLMs. We use 8 A100-80G to training 1 hour for 2 epochs. For the VQA and image captioning task, we use exact match and Bert-Score  Zhang et al. ( 2020 )  as the evaluation tool respectively, mentioned in Section  2.4.2 . We use ShareGPT4v-PT  Chen et al. ( 2023 )  as our database for RAG, which includes approximately 1,246k image-caption pairs with an average caption length of 826. For the retrieval system, we use FAISS  Johnson et al. ( 2021 )  with flat indexes to pre-index the computed embeddings of all images in the database.",
            "Tables  3  and  4  present the results of our robustness tests. In Table  3 , we maintain the image-caption pair with the highest CLIP similarity score among the retrieved content to ensure effective information. We also introduce image-caption pairs from the Top-K (from 1k to 1,000k) positions as forced irrelevant information. The results show that the performance of vanilla-RAG significantly declines on the three datasets as more irrelevant image-caption pairs are introduced. In contrast, our models performance remains very stable. Notably, the models performance when introducing 100k and 1000k irrelevant image-caption pairs is better than when introducing 1k pairs. This improvement is because, after training with hard negative samples, our model can easily distinguish content unrelated to the test image and question, thereby focusing more on other relevant information in the retrieval.",
            "Table  4  shows that our model remains robust even after randomly shuffling the examples, whereas vanilla-RAG exhibits a significant decline in performance. This demonstrates that training the model with our proposed framework enables it to selectively extract relevant information from the retrieved content, making it less sensitive to the order of the examples."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  The statistics of the datasets used in this paper. * denotes we randomly selected 800  samples from EmoSet to constitute the test set.",
        "table": "A1.T5.1.1",
        "footnotes": [],
        "references": [
            "In this section, we introduce the datasets used in our experiments. The statistics of these datasets are shown in Table  5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Number of exemplars.",
        "table": "A1.T6.1.1",
        "footnotes": [],
        "references": [
            "We compared the efficiency of our method with four other methods, as shown in Figure  6 . We calculate the average running time for 1,000 samples in image captioning tasks with the max token length set to 256. It can be observed that our method increases the time by approximately 1.3 seconds per sample compared to the zero-shot approach. This increase is primarily due to the time required to convert the image to an embedding, retrieval time, and the additional overhead introduced by the increased length of the prompt. However, this slight increase in time is acceptable considering the performance improvement.",
            "Table  6  shows the performance of our model with different numbers of examples. Due to the long captions of ShareGPT-4V, only three examples can fit within a 4096 context window. Our method demonstrates robustness with 1, 2, and 3 examples, indicating adaptability to various numbers of examples. However, the performance peaks with 2 examples and declines with 1 and 3 examples. The decline with 1 example may be due to insufficient information, while 3 examples may introduce excessive irrelevant information."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Effect of training data size.",
        "table": "A1.T7.1.1",
        "footnotes": [],
        "references": [
            "Table  7  shows the experiments on the amount of training data. Using only 2k data points, our model is already able to utilize RAG and achieve the best performance fully. Although the performance with 3k and 4k data points is slightly worse than with 2k, the results still surpass those of vanilla-RAG. This indicates that our framework can sufficiently leverage its capabilities using just 2k samples self-generated by the model."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Performance of Vanilla-RAG on downstream tasks with different retrieval content.",
        "table": "A1.T8.1.1",
        "footnotes": [],
        "references": [
            "In this section, we explore the performance differences when using image-caption pairs versus using only captions for retrieval across three tasks, as shown in Table  8 . For VQA and classification tasks, using both image and caption yields the best results, as the additional information from the image is beneficial for tasks that require a strong understanding of the image. However, for the captioning task, using only captions performs better since this task requires the model to generate a relevant response based solely on the retrieved descriptions.",
            "We present four examples comparing our method with zero-shot and vanilla-RAG, as shown in Figures  8 ,  9 ,  10 , and  11 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Full results of 7B and 13B Robust-LlaVA on MS-COCO and Vizwiz-Caption. The best performance in the table is highlighted in  bold .",
        "table": "A1.T9.1.1",
        "footnotes": [],
        "references": [
            "In the main table, the metrics for the captioning task are the sum of BLEU-4, METEOR, ROUGE-L, CIDEr, and SPICE. We present the detailed results of our 7B and 13B models on MS-COCO and Vizwiz-Caption in Table  9 .",
            "We present four examples comparing our method with zero-shot and vanilla-RAG, as shown in Figures  8 ,  9 ,  10 , and  11 ."
        ]
    },
    "global_footnotes": [
        "https://openai.com/research/gpt-4v-system-card",
        "We use the official COCO evaluation toolkit."
    ]
}