{
    "S3.T1": {
        "caption": "Table 1:  Evaluation Results on GQA. All numbers are in percentages. The lower the better for distribution.",
        "table": "",
        "footnotes": "",
        "references": []
    },
    "A2.T2": {
        "caption": "Table 2:  Scene Graphs Statistics of the GQA Dataset",
        "table": "",
        "footnotes": "",
        "references": [
            "We use the official split of the GQA dataset.\nWe use two files \"val_sceneGraphs.json\" and \"train_sceneGraphs.json\" directly obtained on the GQA website as our raw dataset.\nSince each image (graph) is independent, GQA splits the dataset by individual graphs with rough split percentages of  train/validation: 88%/12%.\nIn the table 2, we summarize the statistics that we collected from the dataset.\nWe did not report the statistics of the test set since the scene graphs in the test set is not publicly available."
        ]
    },
    "A2.T3": {
        "caption": "Table 3: Typical types of questions",
        "table": "",
        "footnotes": "",
        "references": []
    },
    "A2.T4": {
        "caption": "Table 4:  Ablation Study Results for 2 layer GraphVQA-GINE. All numbers are in percentages. The lower the better for distribution.",
        "table": "",
        "footnotes": "",
        "references": [
            "As mentioned in Section 3.3.1 and Section 3.3.2, a expressive function ΘΘ\\Theta is used in GINE layer. When ΘΘ\\Theta is just a single layer MLP, the corresponding GIN/GINE structure will be very similar to the GCN structure. Since in Section 4 we implemented ΘΘ\\Theta as a single layer MLP, the performance of GraphVQA-GCN and GraphVQA-GINE stays at very similar stage. As GIN and GINE are now very popular as basic components for large-scale graph neural network design, one may ask if using ΘΘ\\Theta with more powerful expression ability will help the performance. The short answer is no. We provide a simple ablation study on different choice of ΘΘ\\Theta, using a two layer MLP-style network with (FC, ReLU, FC, ReLU, BN) structure. Table 4 shows that the result of GraphVQA-GINE-2 degrades to the worst. One possible reason is that the scale for each scen graph is generally small, therefore the expression ability might already be enough for a single layer MLP, and use a more complex ΘΘ\\Theta may leads to harder optimization problems, and thus leads to a downgrade of the performance. Such guess could possibly be further investigated and evaluated in our future work.\nIn addition, the scene graph-based VQA as in this work might offer an opportunity for further accelerating the real world image-based applications Liang and Zou (2020). Exploring such deployment benefits is another direction of future work."
        ]
    }
}