{
    "PAPER'S NUMBER OF TABLES": 12,
    "S1.T1": {
        "caption": "Table 1: Summary of the per-group (i.e., per-client) and per-example (i.e., per-sequence) statistics of the new language modeling datasets we introduce using Dataset Grouper, compared to those of previous federated benchmark datasets supplied by TFF [11], LEAF [12], FedNLP [13, 14], and FedScale [15].\nErratum: The previous version displayed the maximum words per group instead of the 909090th percentile for the first 4 datasets.\n",
        "table": "",
        "footnotes": "\n\n\n\n\n\nSource\nDataset\nGroup by\nWords\nGroups\nWords per group\nExamples\nWords per example\n\n\n\n\n\n\n101010th perc.\nMedian\n909090th perc.\n\n101010th perc.\nMedian\n909090th perc.\n\nOurs\nFedC4\nDomain\n𝟏𝟑𝟐​𝐁132𝐁\\pagecolor{blue!7}{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}}\\bm{132\\mathrm{B}}\n15.6​𝐌15.6𝐌\\pagecolor{blue!7}{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}}\\bm{15.6\\mathrm{M}}\n828282\n815815815\n11​K11K11\\mathrm{K}\n0.36​𝐁0.36𝐁\\pagecolor{blue!7}{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}}\\bm{0.36\\mathrm{B}}\n494949\n191191191\n783783783\n\nFedWiki\nArticle\n3​B3B3\\mathrm{B}\n6.5​M6.5M6.5\\mathrm{M}\n393939\n198198198\n1​K1K1\\mathrm{K}\n6.5​M6.5M6.5\\mathrm{M}\n393939\n198198198\n1​K1K1\\mathrm{K}\n\nFedBookCO\nBook\n1.2​B1.2B1.2\\mathrm{B}\n18​K18K18\\mathrm{K}\n𝟐𝟒​𝐊24𝐊\\pagecolor{blue!7}{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}}\\bm{24\\mathrm{K}}\n𝟓𝟐​𝐊52𝐊\\pagecolor{blue!7}{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}}\\bm{52\\mathrm{K}}\n𝟏𝟏𝟏​𝐊111𝐊\\pagecolor{blue!7}{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}}\\bm{111\\mathrm{K}}\n18​K18K18\\mathrm{K}\n𝟐𝟒​𝐊24𝐊\\pagecolor{blue!7}{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}}\\bm{24\\mathrm{K}}\n𝟓𝟐​𝐊52𝐊\\pagecolor{blue!7}{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}}\\bm{52\\mathrm{K}}\n𝟏𝟏𝟏​𝐊111𝐊\\pagecolor{blue!7}{\\color[rgb]{0,0,0}\\definecolor[named]{pgfstrokecolor}{rgb}{0,0,0}}\\bm{111\\mathrm{K}}\n\nFedCCnews\nDomain\n0.3​B0.3B0.3\\mathrm{B}\n8.8​K8.8K8.8\\mathrm{K}\n303303303\n5​K5K5\\mathrm{K}\n63​K63K63\\mathrm{K}\n0.7​M0.7M0.7\\mathrm{M}\n787878\n316316316\n842842842\n\nExisting\n\n\n\nAmazon Reviews\n\nAccount\n4.3​B4.3B4.3\\mathrm{B}\n1.5​M1.5M1.5\\mathrm{M}\n278278278\n1.1​K1.1K1.1\\mathrm{K}\n5​K5K5\\mathrm{K}\n68​M68M68\\mathrm{M}\n333\n282828\n155155155\n\nStack Overflow\nAccount\n2​B2B2\\mathrm{B}\n0.3​M0.3M0.3\\mathrm{M}\n1.2​K1.2K1.2\\mathrm{K}\n2.7​K2.7K2.7\\mathrm{K}\n11​K11K11\\mathrm{K}\n0.1​B0.1B0.1\\mathrm{B}\n333\n131313\n292929\n\nReddit\nAccount\n1.2​B1.2B1.2\\mathrm{B}\n1.7​M1.7M1.7\\mathrm{M}\n585858\n257257257\n172017201720\n33​M33M33\\mathrm{M}\n777\n212121\n818181\n\nBlog Corpus\nAccount\n0.1​B0.1B0.1\\mathrm{B}\n17​K17K17\\mathrm{K}\n551551551\n2​K2K2\\mathrm{K}\n13​K13K13\\mathrm{K}\n0.5​M0.5M0.5\\mathrm{M}\n666\n105105105\n460460460\n\nShakespeare\nRole/play\n0.4​M0.4M0.4\\mathrm{M}\n715715715\n141414\n175175175\n1.6​K1.6K1.6\\mathrm{K}\n16​K16K16\\mathrm{K}\n444\n121212\n636363\n\nGigaword\nSynthetic\n0.3​M0.3M0.3\\mathrm{M}\n100100100\n3.0​K3.0K3.0\\mathrm{K}\n3.1​K3.1K3.1\\mathrm{K}\n3.2​K3.2K3.2\\mathrm{K}\n10​K10K10\\mathrm{K}\n212121\n313131\n414141\n\n\n",
        "references": [
            "Large-scale federated text datasets:\nWhile Dataset Grouper can be used for a wide array of modalities and tasks,\nwe illustrate its use by creating group-structured versions of four large language modeling datasets (see Table 1 and Figure 1), designed specifically for FL research. They are orders of magnitude larger than previous datasets in terms of one or more of the following: the number of groups, the amount of data, and the length of sequences. They are suitable for both pre-training and fine-tuning tasks, and exhibit long tails, as is common in large-scale text corpora."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Characteristics of group-structured dataset formats.",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<th id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">In-Memory</span></th>\n<th id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Hierarchical</span></th>\n<th id=\"S3.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Streaming</span></th>\n</tr>\n<tr id=\"S3.T2.1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Scalability</td>\n<td id=\"S3.T2.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#FFE0E0;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.2.2.2.1\" class=\"ltx_text\" style=\"background-color:#FFE0E0;\">Limited</span></td>\n<td id=\"S3.T2.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.2.2.3.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\">High</span></td>\n<td id=\"S3.T2.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.2.2.4.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\">High</span></td>\n</tr>\n<tr id=\"S3.T2.1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.3.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Group Access Time</td>\n<td id=\"S3.T2.1.3.3.2\" class=\"ltx_td ltx_align_center\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.3.3.2.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\">Very Fast</span></td>\n<td id=\"S3.T2.1.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"background-color:#FFE0E0;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.3.3.3.1\" class=\"ltx_text\" style=\"background-color:#FFE0E0;\">Slow</span></td>\n<td id=\"S3.T2.1.3.3.4\" class=\"ltx_td ltx_align_center\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.3.3.4.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\">Fast</span></td>\n</tr>\n<tr id=\"S3.T2.1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">Group Access Patterns</td>\n<td id=\"S3.T2.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.4.4.2.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\">Arbitrary</span></td>\n<td id=\"S3.T2.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.4.4.3.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\">Arbitrary</span></td>\n<td id=\"S3.T2.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#FFE0E0;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T2.1.4.4.4.1\" class=\"ltx_text\" style=\"background-color:#FFE0E0;\">Shuffle + Streaming</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Our primary goal is to enable research using large-scale group-structured datasets. In order to do so, we need a group-structured dataset format that balances the following characteristics:",
                "Scalability",
                ": Can the dataset format scale to large numbers of examples, groups, and bytes?",
                "Group access time",
                ": How long does it take to access the examples held by a single group?",
                "Group access patterns",
                ": What kinds of sampling patterns across groups are permitted? Can we access group datasets arbitrarily, and in any order?",
                "There is a trade-off between these characteristics. Dataset formats used in the FL community often optimize for either scalability or group sampling time, while enabling maximum flexibility in access patterns. Our core insight is that by limiting the access patterns possible, we can use a dataset format that is scalable and efficient simultaneously. We discuss three archetypes of group-structured dataset formats (in-memory, hierarchical, and streaming) and their resulting trade-offs briefly in ",
                "Table",
                " ",
                "2",
                ", and in more detail below. ",
                "Figure",
                " ",
                "2",
                " gives a graphical representation of the formats.",
                "In-memory formats.",
                " In-memory group-structured datasets are essentially key-value mappings held entirely in memory. Adopted by e.g. LEAF ",
                "[",
                "12",
                "]",
                " and FedNLP ",
                "[",
                "13",
                "]",
                ", this is suitable for small datasets such as EMNIST or CIFAR-100. Looking up a group’s dataset is fast (e.g., via a hash map), and groups can be accessed in an arbitrary order. Of course, this approach is limited by the cumulative size of the dataset and is therefore not scalable in full generality. As we see in ",
                "Table",
                " ",
                "3",
                ", this format does not even scale to FedBookCO on a single CPU; FedC4 and FedWiki are even larger.",
                "Hierarchical formats.",
                " Hierarchical dataset formats store examples across files in such a way that (a) the dataset need not be loaded entirely into memory, and (b) individual groups can be constructed in arbitrary orders. For example, TensorFlow Federated ",
                "[",
                "11",
                "]",
                " uses SQL databases to both store and access client datasets for FL simulations, facilitating the loading of the group index in-memory, then construction of a group’s dataset at a later time.\nFor larger datasets, constructing an arbitrary group’s dataset can be slow, as it is often bottlenecked by indexing and searching over a large number of (possibly distributed) files. ",
                "Table",
                " ",
                "3",
                " shows that the hierarchical format can be significantly slower than other formats when accessing groups in very large datasets.",
                "Streaming formats.",
                " Instead of allowing arbitrary group access, Dataset Grouper provides ways to iterate over all the groups in a stream-like fashion. The datasets for each group are backed by some number of files,",
                "1",
                "1",
                "1",
                "We use the TFRecord format ",
                "[",
                "70",
                "]",
                " for all datasets.",
                " which are interleaved to create a “group stream”. Concretely, this restricts the possible group access patterns, only allowing stream-level operations such as buffered shuffling, repeating, and batching. This essentially lifts the stream-of-examples format used large-scale centralized training pipelines to streams of groups for federated training — both formats allow dataset iterators with limited shuffling (e.g., with a fixed-size buffer), but not arbitrary access to the individual elements. This restriction allows us to use parallel reads, prefetching, and interleaving to speed up dataset iteration and generally enables the total iteration time of the dataset to scale linearly (as opposed to super-linearly) with the number of groups in the dataset.",
                "Each group’s dataset is further represented as a stream of examples so that no group’s data need to be fully loaded into memory. This is crucial in scaling to large datasets like FedC4, something that is memory-prohibitive for in-memory formats, and speed-prohibitive for hierarchical formats. To illustrate this further, we detail the time required to iterate fully over various group-structured datasets (accessing the groups’ datasets sequentially, in a random order) in different formats in ",
                "Table",
                " ",
                "3",
                ". For details on the amount of memory used by each format, see ",
                "Appendix",
                " ",
                "E",
                "."
            ]
        ]
    },
    "S3.T3": {
        "caption": "Table 3: The time (in seconds) to iterate over federated datasets. This is the time required to iterate over all examples in all group datasets, in serial, on a single CPU. We present the average and standard deviation over 5 trials, omitting trials that take more than 2 hours (>7200absent7200>7200 seconds), or that ran out of memory.\nWe compare a federated CIFAR-100 dataset (partitioned across 100 groups, each with 100 examples), FedCCnews (in which examples are split across users at a domain level), and FedBookCO (in which examples are split across users at a title level). See Section 4 for more details on the latter two datasets.",
        "table": "<table id=\"S3.T3.8\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T3.8.9.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.8.9.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.8.9.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset Format</span></th>\n<th id=\"S3.T3.8.9.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.8.9.1.2.1\" class=\"ltx_text ltx_font_bold\">In-Memory</span></th>\n<th id=\"S3.T3.8.9.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.8.9.1.3.1\" class=\"ltx_text ltx_font_bold\">Hierarchical</span></th>\n<th id=\"S3.T3.8.9.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.8.9.1.4.1\" class=\"ltx_text ltx_font_bold\">Streaming</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T3.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">CIFAR-100</td>\n<td id=\"S3.T3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.1.1.1.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S3.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"0.0783\\pm 0.0007\" display=\"inline\"><semantics id=\"S3.T3.1.1.1.1.m1.1a\"><mrow id=\"S3.T3.1.1.1.1.m1.1.1\" xref=\"S3.T3.1.1.1.1.m1.1.1.cmml\"><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.1.1.1.1.m1.1.1.2\" xref=\"S3.T3.1.1.1.1.m1.1.1.2.cmml\">0.0783</mn><mo mathbackground=\"#EDEDFF\" id=\"S3.T3.1.1.1.1.m1.1.1.1\" xref=\"S3.T3.1.1.1.1.m1.1.1.1.cmml\">±</mo><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.1.1.1.1.m1.1.1.3\" xref=\"S3.T3.1.1.1.1.m1.1.1.3.cmml\">0.0007</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.1.1.1.1.m1.1b\"><apply id=\"S3.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T3.1.1.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"S3.T3.1.1.1.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"S3.T3.1.1.1.1.m1.1.1.2\">0.0783</cn><cn type=\"float\" id=\"S3.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"S3.T3.1.1.1.1.m1.1.1.3\">0.0007</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.1.1.1.1.m1.1c\">0.0783\\pm 0.0007</annotation></semantics></math></span></td>\n<td id=\"S3.T3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.2.2.2.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S3.T3.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"25.11\\pm 0.81\" display=\"inline\"><semantics id=\"S3.T3.2.2.2.1.m1.1a\"><mrow id=\"S3.T3.2.2.2.1.m1.1.1\" xref=\"S3.T3.2.2.2.1.m1.1.1.cmml\"><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.2.2.2.1.m1.1.1.2\" xref=\"S3.T3.2.2.2.1.m1.1.1.2.cmml\">25.11</mn><mo mathbackground=\"#EDEDFF\" id=\"S3.T3.2.2.2.1.m1.1.1.1\" xref=\"S3.T3.2.2.2.1.m1.1.1.1.cmml\">±</mo><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.2.2.2.1.m1.1.1.3\" xref=\"S3.T3.2.2.2.1.m1.1.1.3.cmml\">0.81</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.2.2.2.1.m1.1b\"><apply id=\"S3.T3.2.2.2.1.m1.1.1.cmml\" xref=\"S3.T3.2.2.2.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T3.2.2.2.1.m1.1.1.1.cmml\" xref=\"S3.T3.2.2.2.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.T3.2.2.2.1.m1.1.1.2.cmml\" xref=\"S3.T3.2.2.2.1.m1.1.1.2\">25.11</cn><cn type=\"float\" id=\"S3.T3.2.2.2.1.m1.1.1.3.cmml\" xref=\"S3.T3.2.2.2.1.m1.1.1.3\">0.81</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.2.2.2.1.m1.1c\">25.11\\pm 0.81</annotation></semantics></math></span></td>\n<td id=\"S3.T3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.3.3.3.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S3.T3.3.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"9.88\\pm 0.075\" display=\"inline\"><semantics id=\"S3.T3.3.3.3.1.m1.1a\"><mrow id=\"S3.T3.3.3.3.1.m1.1.1\" xref=\"S3.T3.3.3.3.1.m1.1.1.cmml\"><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.3.3.3.1.m1.1.1.2\" xref=\"S3.T3.3.3.3.1.m1.1.1.2.cmml\">9.88</mn><mo mathbackground=\"#EDEDFF\" id=\"S3.T3.3.3.3.1.m1.1.1.1\" xref=\"S3.T3.3.3.3.1.m1.1.1.1.cmml\">±</mo><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.3.3.3.1.m1.1.1.3\" xref=\"S3.T3.3.3.3.1.m1.1.1.3.cmml\">0.075</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.3.3.3.1.m1.1b\"><apply id=\"S3.T3.3.3.3.1.m1.1.1.cmml\" xref=\"S3.T3.3.3.3.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T3.3.3.3.1.m1.1.1.1.cmml\" xref=\"S3.T3.3.3.3.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.T3.3.3.3.1.m1.1.1.2.cmml\" xref=\"S3.T3.3.3.3.1.m1.1.1.2\">9.88</cn><cn type=\"float\" id=\"S3.T3.3.3.3.1.m1.1.1.3.cmml\" xref=\"S3.T3.3.3.3.1.m1.1.1.3\">0.075</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.3.3.3.1.m1.1c\">9.88\\pm 0.075</annotation></semantics></math></span></td>\n</tr>\n<tr id=\"S3.T3.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T3.6.6.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedCCnews</td>\n<td id=\"S3.T3.4.4.1\" class=\"ltx_td ltx_align_center\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.4.4.1.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S3.T3.4.4.1.1.m1.1\" class=\"ltx_Math\" alttext=\"0.549\\pm 0.014\" display=\"inline\"><semantics id=\"S3.T3.4.4.1.1.m1.1a\"><mrow id=\"S3.T3.4.4.1.1.m1.1.1\" xref=\"S3.T3.4.4.1.1.m1.1.1.cmml\"><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.4.4.1.1.m1.1.1.2\" xref=\"S3.T3.4.4.1.1.m1.1.1.2.cmml\">0.549</mn><mo mathbackground=\"#EDEDFF\" id=\"S3.T3.4.4.1.1.m1.1.1.1\" xref=\"S3.T3.4.4.1.1.m1.1.1.1.cmml\">±</mo><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.4.4.1.1.m1.1.1.3\" xref=\"S3.T3.4.4.1.1.m1.1.1.3.cmml\">0.014</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.4.4.1.1.m1.1b\"><apply id=\"S3.T3.4.4.1.1.m1.1.1.cmml\" xref=\"S3.T3.4.4.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T3.4.4.1.1.m1.1.1.1.cmml\" xref=\"S3.T3.4.4.1.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.T3.4.4.1.1.m1.1.1.2.cmml\" xref=\"S3.T3.4.4.1.1.m1.1.1.2\">0.549</cn><cn type=\"float\" id=\"S3.T3.4.4.1.1.m1.1.1.3.cmml\" xref=\"S3.T3.4.4.1.1.m1.1.1.3\">0.014</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.4.4.1.1.m1.1c\">0.549\\pm 0.014</annotation></semantics></math></span></td>\n<td id=\"S3.T3.5.5.2\" class=\"ltx_td ltx_align_center\" style=\"background-color:#FFE0E0;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.5.5.2.1\" class=\"ltx_text\" style=\"background-color:#FFE0E0;\"><math id=\"S3.T3.5.5.2.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;7200\" display=\"inline\"><semantics id=\"S3.T3.5.5.2.1.m1.1a\"><mrow id=\"S3.T3.5.5.2.1.m1.1.1\" xref=\"S3.T3.5.5.2.1.m1.1.1.cmml\"><mi id=\"S3.T3.5.5.2.1.m1.1.1.2\" xref=\"S3.T3.5.5.2.1.m1.1.1.2.cmml\"></mi><mo mathbackground=\"#FFE0E0\" id=\"S3.T3.5.5.2.1.m1.1.1.1\" xref=\"S3.T3.5.5.2.1.m1.1.1.1.cmml\">&gt;</mo><mn mathbackground=\"#FFE0E0\" id=\"S3.T3.5.5.2.1.m1.1.1.3\" xref=\"S3.T3.5.5.2.1.m1.1.1.3.cmml\">7200</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.5.5.2.1.m1.1b\"><apply id=\"S3.T3.5.5.2.1.m1.1.1.cmml\" xref=\"S3.T3.5.5.2.1.m1.1.1\"><gt id=\"S3.T3.5.5.2.1.m1.1.1.1.cmml\" xref=\"S3.T3.5.5.2.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S3.T3.5.5.2.1.m1.1.1.2.cmml\" xref=\"S3.T3.5.5.2.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S3.T3.5.5.2.1.m1.1.1.3.cmml\" xref=\"S3.T3.5.5.2.1.m1.1.1.3\">7200</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.5.5.2.1.m1.1c\">&gt;7200</annotation></semantics></math></span></td>\n<td id=\"S3.T3.6.6.3\" class=\"ltx_td ltx_align_center\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.6.6.3.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S3.T3.6.6.3.1.m1.1\" class=\"ltx_Math\" alttext=\"248\\pm 17.5\" display=\"inline\"><semantics id=\"S3.T3.6.6.3.1.m1.1a\"><mrow id=\"S3.T3.6.6.3.1.m1.1.1\" xref=\"S3.T3.6.6.3.1.m1.1.1.cmml\"><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.6.6.3.1.m1.1.1.2\" xref=\"S3.T3.6.6.3.1.m1.1.1.2.cmml\">248</mn><mo mathbackground=\"#EDEDFF\" id=\"S3.T3.6.6.3.1.m1.1.1.1\" xref=\"S3.T3.6.6.3.1.m1.1.1.1.cmml\">±</mo><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.6.6.3.1.m1.1.1.3\" xref=\"S3.T3.6.6.3.1.m1.1.1.3.cmml\">17.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.6.6.3.1.m1.1b\"><apply id=\"S3.T3.6.6.3.1.m1.1.1.cmml\" xref=\"S3.T3.6.6.3.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T3.6.6.3.1.m1.1.1.1.cmml\" xref=\"S3.T3.6.6.3.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"integer\" id=\"S3.T3.6.6.3.1.m1.1.1.2.cmml\" xref=\"S3.T3.6.6.3.1.m1.1.1.2\">248</cn><cn type=\"float\" id=\"S3.T3.6.6.3.1.m1.1.1.3.cmml\" xref=\"S3.T3.6.6.3.1.m1.1.1.3\">17.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.6.6.3.1.m1.1c\">248\\pm 17.5</annotation></semantics></math></span></td>\n</tr>\n<tr id=\"S3.T3.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T3.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedBookCO</td>\n<td id=\"S3.T3.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#FFE0E0;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.8.8.4.1\" class=\"ltx_text\" style=\"background-color:#FFE0E0;\">Out of memory</span></td>\n<td id=\"S3.T3.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#FFE0E0;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.7.7.1.1\" class=\"ltx_text\" style=\"background-color:#FFE0E0;\"><math id=\"S3.T3.7.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"&gt;7200\" display=\"inline\"><semantics id=\"S3.T3.7.7.1.1.m1.1a\"><mrow id=\"S3.T3.7.7.1.1.m1.1.1\" xref=\"S3.T3.7.7.1.1.m1.1.1.cmml\"><mi id=\"S3.T3.7.7.1.1.m1.1.1.2\" xref=\"S3.T3.7.7.1.1.m1.1.1.2.cmml\"></mi><mo mathbackground=\"#FFE0E0\" id=\"S3.T3.7.7.1.1.m1.1.1.1\" xref=\"S3.T3.7.7.1.1.m1.1.1.1.cmml\">&gt;</mo><mn mathbackground=\"#FFE0E0\" id=\"S3.T3.7.7.1.1.m1.1.1.3\" xref=\"S3.T3.7.7.1.1.m1.1.1.3.cmml\">7200</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.7.7.1.1.m1.1b\"><apply id=\"S3.T3.7.7.1.1.m1.1.1.cmml\" xref=\"S3.T3.7.7.1.1.m1.1.1\"><gt id=\"S3.T3.7.7.1.1.m1.1.1.1.cmml\" xref=\"S3.T3.7.7.1.1.m1.1.1.1\"></gt><csymbol cd=\"latexml\" id=\"S3.T3.7.7.1.1.m1.1.1.2.cmml\" xref=\"S3.T3.7.7.1.1.m1.1.1.2\">absent</csymbol><cn type=\"integer\" id=\"S3.T3.7.7.1.1.m1.1.1.3.cmml\" xref=\"S3.T3.7.7.1.1.m1.1.1.3\">7200</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.7.7.1.1.m1.1c\">&gt;7200</annotation></semantics></math></span></td>\n<td id=\"S3.T3.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S3.T3.8.8.2.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S3.T3.8.8.2.1.m1.1\" class=\"ltx_Math\" alttext=\"192\\pm 9.07\" display=\"inline\"><semantics id=\"S3.T3.8.8.2.1.m1.1a\"><mrow id=\"S3.T3.8.8.2.1.m1.1.1\" xref=\"S3.T3.8.8.2.1.m1.1.1.cmml\"><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.8.8.2.1.m1.1.1.2\" xref=\"S3.T3.8.8.2.1.m1.1.1.2.cmml\">192</mn><mo mathbackground=\"#EDEDFF\" id=\"S3.T3.8.8.2.1.m1.1.1.1\" xref=\"S3.T3.8.8.2.1.m1.1.1.1.cmml\">±</mo><mn mathbackground=\"#EDEDFF\" id=\"S3.T3.8.8.2.1.m1.1.1.3\" xref=\"S3.T3.8.8.2.1.m1.1.1.3.cmml\">9.07</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.T3.8.8.2.1.m1.1b\"><apply id=\"S3.T3.8.8.2.1.m1.1.1.cmml\" xref=\"S3.T3.8.8.2.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.T3.8.8.2.1.m1.1.1.1.cmml\" xref=\"S3.T3.8.8.2.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"integer\" id=\"S3.T3.8.8.2.1.m1.1.1.2.cmml\" xref=\"S3.T3.8.8.2.1.m1.1.1.2\">192</cn><cn type=\"float\" id=\"S3.T3.8.8.2.1.m1.1.1.3.cmml\" xref=\"S3.T3.8.8.2.1.m1.1.1.3\">9.07</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T3.8.8.2.1.m1.1c\">192\\pm 9.07</annotation></semantics></math></span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In-memory formats. In-memory group-structured datasets are essentially key-value mappings held entirely in memory. Adopted by e.g. LEAF [12] and FedNLP [13], this is suitable for small datasets such as EMNIST or CIFAR-100. Looking up a group’s dataset is fast (e.g., via a hash map), and groups can be accessed in an arbitrary order. Of course, this approach is limited by the cumulative size of the dataset and is therefore not scalable in full generality. As we see in Table 3, this format does not even scale to FedBookCO on a single CPU; FedC4 and FedWiki are even larger."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Average time spent per round on iterating over data, including preprocessing, versus training. Results are computed for 100 rounds of training of FedAvg, with varying cohort sizes.",
        "table": "<table id=\"S5.T4.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.6.7.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.6.7.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T4.6.7.1.1.1\" class=\"ltx_text ltx_font_bold\">Cohort Size</span></th>\n<th id=\"S5.T4.6.7.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span id=\"S5.T4.6.7.1.2.1\" class=\"ltx_text ltx_font_bold\">Data Iteration Time</span> (s)</th>\n<th id=\"S5.T4.6.7.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span id=\"S5.T4.6.7.1.3.1\" class=\"ltx_text ltx_font_bold\">Training Time</span> (s)</th>\n<th id=\"S5.T4.6.7.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span id=\"S5.T4.6.7.1.4.1\" class=\"ltx_text ltx_font_bold\">Data Iteration Time</span> (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">8</td>\n<td id=\"S5.T4.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T4.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"0.26\\pm 0.48\" display=\"inline\"><semantics id=\"S5.T4.1.1.1.m1.1a\"><mrow id=\"S5.T4.1.1.1.m1.1.1\" xref=\"S5.T4.1.1.1.m1.1.1.cmml\"><mn id=\"S5.T4.1.1.1.m1.1.1.2\" xref=\"S5.T4.1.1.1.m1.1.1.2.cmml\">0.26</mn><mo id=\"S5.T4.1.1.1.m1.1.1.1\" xref=\"S5.T4.1.1.1.m1.1.1.1.cmml\">±</mo><mn id=\"S5.T4.1.1.1.m1.1.1.3\" xref=\"S5.T4.1.1.1.m1.1.1.3.cmml\">0.48</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.1.1.1.m1.1b\"><apply id=\"S5.T4.1.1.1.m1.1.1.cmml\" xref=\"S5.T4.1.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.T4.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T4.1.1.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S5.T4.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T4.1.1.1.m1.1.1.2\">0.26</cn><cn type=\"float\" id=\"S5.T4.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T4.1.1.1.m1.1.1.3\">0.48</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.1.1.1.m1.1c\">0.26\\pm 0.48</annotation></semantics></math></td>\n<td id=\"S5.T4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T4.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"3.03\\pm 2.58\" display=\"inline\"><semantics id=\"S5.T4.2.2.2.m1.1a\"><mrow id=\"S5.T4.2.2.2.m1.1.1\" xref=\"S5.T4.2.2.2.m1.1.1.cmml\"><mn id=\"S5.T4.2.2.2.m1.1.1.2\" xref=\"S5.T4.2.2.2.m1.1.1.2.cmml\">3.03</mn><mo id=\"S5.T4.2.2.2.m1.1.1.1\" xref=\"S5.T4.2.2.2.m1.1.1.1.cmml\">±</mo><mn id=\"S5.T4.2.2.2.m1.1.1.3\" xref=\"S5.T4.2.2.2.m1.1.1.3.cmml\">2.58</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.2.2.2.m1.1b\"><apply id=\"S5.T4.2.2.2.m1.1.1.cmml\" xref=\"S5.T4.2.2.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.T4.2.2.2.m1.1.1.1.cmml\" xref=\"S5.T4.2.2.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S5.T4.2.2.2.m1.1.1.2.cmml\" xref=\"S5.T4.2.2.2.m1.1.1.2\">3.03</cn><cn type=\"float\" id=\"S5.T4.2.2.2.m1.1.1.3.cmml\" xref=\"S5.T4.2.2.2.m1.1.1.3\">2.58</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.2.2.2.m1.1c\">3.03\\pm 2.58</annotation></semantics></math></td>\n<td id=\"S5.T4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">7.78</td>\n</tr>\n<tr id=\"S5.T4.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T4.4.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">16</td>\n<td id=\"S5.T4.3.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T4.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"0.66\\pm 0.85\" display=\"inline\"><semantics id=\"S5.T4.3.3.1.m1.1a\"><mrow id=\"S5.T4.3.3.1.m1.1.1\" xref=\"S5.T4.3.3.1.m1.1.1.cmml\"><mn id=\"S5.T4.3.3.1.m1.1.1.2\" xref=\"S5.T4.3.3.1.m1.1.1.2.cmml\">0.66</mn><mo id=\"S5.T4.3.3.1.m1.1.1.1\" xref=\"S5.T4.3.3.1.m1.1.1.1.cmml\">±</mo><mn id=\"S5.T4.3.3.1.m1.1.1.3\" xref=\"S5.T4.3.3.1.m1.1.1.3.cmml\">0.85</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.3.3.1.m1.1b\"><apply id=\"S5.T4.3.3.1.m1.1.1.cmml\" xref=\"S5.T4.3.3.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.T4.3.3.1.m1.1.1.1.cmml\" xref=\"S5.T4.3.3.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S5.T4.3.3.1.m1.1.1.2.cmml\" xref=\"S5.T4.3.3.1.m1.1.1.2\">0.66</cn><cn type=\"float\" id=\"S5.T4.3.3.1.m1.1.1.3.cmml\" xref=\"S5.T4.3.3.1.m1.1.1.3\">0.85</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.3.3.1.m1.1c\">0.66\\pm 0.85</annotation></semantics></math></td>\n<td id=\"S5.T4.4.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T4.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"5.70\\pm 2.61\" display=\"inline\"><semantics id=\"S5.T4.4.4.2.m1.1a\"><mrow id=\"S5.T4.4.4.2.m1.1.1\" xref=\"S5.T4.4.4.2.m1.1.1.cmml\"><mn id=\"S5.T4.4.4.2.m1.1.1.2\" xref=\"S5.T4.4.4.2.m1.1.1.2.cmml\">5.70</mn><mo id=\"S5.T4.4.4.2.m1.1.1.1\" xref=\"S5.T4.4.4.2.m1.1.1.1.cmml\">±</mo><mn id=\"S5.T4.4.4.2.m1.1.1.3\" xref=\"S5.T4.4.4.2.m1.1.1.3.cmml\">2.61</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.4.4.2.m1.1b\"><apply id=\"S5.T4.4.4.2.m1.1.1.cmml\" xref=\"S5.T4.4.4.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.T4.4.4.2.m1.1.1.1.cmml\" xref=\"S5.T4.4.4.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S5.T4.4.4.2.m1.1.1.2.cmml\" xref=\"S5.T4.4.4.2.m1.1.1.2\">5.70</cn><cn type=\"float\" id=\"S5.T4.4.4.2.m1.1.1.3.cmml\" xref=\"S5.T4.4.4.2.m1.1.1.3\">2.61</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.4.4.2.m1.1c\">5.70\\pm 2.61</annotation></semantics></math></td>\n<td id=\"S5.T4.4.4.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">10.43</td>\n</tr>\n<tr id=\"S5.T4.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T4.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">32</td>\n<td id=\"S5.T4.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T4.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"1.16\\pm 1.48\" display=\"inline\"><semantics id=\"S5.T4.5.5.1.m1.1a\"><mrow id=\"S5.T4.5.5.1.m1.1.1\" xref=\"S5.T4.5.5.1.m1.1.1.cmml\"><mn id=\"S5.T4.5.5.1.m1.1.1.2\" xref=\"S5.T4.5.5.1.m1.1.1.2.cmml\">1.16</mn><mo id=\"S5.T4.5.5.1.m1.1.1.1\" xref=\"S5.T4.5.5.1.m1.1.1.1.cmml\">±</mo><mn id=\"S5.T4.5.5.1.m1.1.1.3\" xref=\"S5.T4.5.5.1.m1.1.1.3.cmml\">1.48</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.5.5.1.m1.1b\"><apply id=\"S5.T4.5.5.1.m1.1.1.cmml\" xref=\"S5.T4.5.5.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.T4.5.5.1.m1.1.1.1.cmml\" xref=\"S5.T4.5.5.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S5.T4.5.5.1.m1.1.1.2.cmml\" xref=\"S5.T4.5.5.1.m1.1.1.2\">1.16</cn><cn type=\"float\" id=\"S5.T4.5.5.1.m1.1.1.3.cmml\" xref=\"S5.T4.5.5.1.m1.1.1.3\">1.48</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.5.5.1.m1.1c\">1.16\\pm 1.48</annotation></semantics></math></td>\n<td id=\"S5.T4.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T4.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"11.30\\pm 2.48\" display=\"inline\"><semantics id=\"S5.T4.6.6.2.m1.1a\"><mrow id=\"S5.T4.6.6.2.m1.1.1\" xref=\"S5.T4.6.6.2.m1.1.1.cmml\"><mn id=\"S5.T4.6.6.2.m1.1.1.2\" xref=\"S5.T4.6.6.2.m1.1.1.2.cmml\">11.30</mn><mo id=\"S5.T4.6.6.2.m1.1.1.1\" xref=\"S5.T4.6.6.2.m1.1.1.1.cmml\">±</mo><mn id=\"S5.T4.6.6.2.m1.1.1.3\" xref=\"S5.T4.6.6.2.m1.1.1.3.cmml\">2.48</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.6.6.2.m1.1b\"><apply id=\"S5.T4.6.6.2.m1.1.1.cmml\" xref=\"S5.T4.6.6.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S5.T4.6.6.2.m1.1.1.1.cmml\" xref=\"S5.T4.6.6.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S5.T4.6.6.2.m1.1.1.2.cmml\" xref=\"S5.T4.6.6.2.m1.1.1.2\">11.30</cn><cn type=\"float\" id=\"S5.T4.6.6.2.m1.1.1.3.cmml\" xref=\"S5.T4.6.6.2.m1.1.1.3\">2.48</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.6.6.2.m1.1c\">11.30\\pm 2.48</annotation></semantics></math></td>\n<td id=\"S5.T4.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">9.33</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Iteration efficiency.",
                " We test the efficiency of Dataset Grouper in practical large-scale simulations. Specifically, we measure the time it takes for each round of federated training and what portion of that time is spent iterating over data, including preprocessing. We perform 100 rounds of FedAvg for varying cohort sizes (the number of clients per round) and present the results in ",
                "Table",
                " ",
                "4",
                ". We see that dataset iteration takes under ",
                "10",
                "%",
                "percent",
                "10",
                "10\\%",
                " of the total runtime, even for larger cohort sizes. This is despite the fact that dataset iteration is done entirely on the host, while the training time is parallelized between multiple TPU slices.\nFurther improvements in the data pipeline can only lead to a marginal speedup, highlighting the efficiency and scalability of the streaming dataset format in ",
                "Section",
                " ",
                "3.1",
                ".",
                "Federated learning rate schedules.",
                " Large-scale training on non-partitioned data generally involves a variety of important techniques, such as learning rate scheduling, to attain good performance. In order to determine how best to scale federated training to larger-scale settings, we investigate the use of various learning rate schedules for FedAvg and FedSGD. In both cases, we apply the learning rate schedule at the ",
                "server",
                " (see ",
                "[",
                "30",
                "]",
                " for a discussion of client versus server optimizers). We use constant learning rates, warmup with exponential decay, and warmup with cosine decay. Whenever we use warmup, we warmup for 10% of the total number of training rounds, and decay for the remainder.",
                "We compare the resulting training loss for FedAvg and FedSGD in ",
                "Figure",
                " ",
                "4",
                ". Notably, we see that learning rate scheduling leads to significant improvements in the behavior of FedSGD, while FedAvg is robust to different choices. This reflects the fact that these learning rate schedules were developed in the context of SGD, which involves applying many unbiased gradient estimates. FedSGD operates similarly, computing an unbiased gradient estimate at each round. By contrast, FedAvg involves biased gradients, often called “pseudo-gradients” ",
                "[",
                "30",
                "]",
                ", which may not be the gradient of any loss function ",
                "[",
                "82",
                "]",
                ". Our results suggest that developing effective learning rate scheduling techniques for FedAvg is an open question, and may involve coordinating client and server learning rates.",
                "We also see that FedAvg appears to attain a significantly lower train loss than FedSGD. We stress that this is due to how the training loss is computed. For both algorithms, it is computed by averaging the loss of all batches seen by a client and then averaging that quantity across all clients. However, the client trains as it sees data batches in FedAvg. Therefore, the client’s local model adapts to its own distribution (leading to a lower loss), while in FedSGD the client does not adapt its local model. We explore this difference, which is connected to ",
                "meta-learning",
                ", below.",
                "Federated evaluation and personalization.",
                " Partitioned datasets enable group-structured learning, as well as group-level (or federated) evaluation, which may be particularly informative for measuring downstream performance across heterogeneous data splits. To demonstrate this, we use Dataset Grouper to generate an evaluation dataset from FedC4 by using its held-out validation split. We use the same partition structure as before, grouping examples according to their base domain. Because of this group structure, we can compute histograms of metrics across all groups, rather than just an average metric across all examples.",
                "We take the resulting models trained by FedAvg and FedSGD (with constant learning rates, though we see similar results for all learning rate schedules we considered above), and compute two separate metrics for each validation client. First, we compute the average loss of the model on all examples held by the client. We refer to this as the ",
                "pre-personalization loss",
                ". We then fine-tune the model for a single epoch on the client’s dataset (using a client optimizer of SGD with a tuned learning rate). After personalization, we compute the average loss again, resulting in the ",
                "post-personalization loss",
                ".",
                "We present quantiles of these metrics in ",
                "Table",
                " ",
                "5",
                ". Intriguingly, they show that the FedSGD-trained model works better for pre-personalization, but the FedAvg-trained model is much more effective at personalizing to the client’s data. To further illustrate this, we consider histograms of the two distributions (across all clients) in ",
                "Figure",
                " ",
                "5",
                ". This suggests a more dramatic shift. While the FedAvg- and FedSGD-trained models are close in pre-personalization performance (though FedSGD does better), the post-personalization distribution for FedAvg is extremely light-tailed.",
                "Task-specific personalization.",
                " Pre-trained foundation models are typically employed on a range of downstream tasks. In this spirit, we use the models trained on FedC4 to perform pre- and post-personalization evaluation on FedBookCO.\nThe results, ",
                "Figures",
                " ",
                "6",
                " and ",
                "7",
                ", are similar to but less drastic than those of ",
                "Figure",
                " ",
                "5",
                ". The pre-personalization loss of FedAvg is slightly larger than FedSGD (",
                "5.0",
                "5.0",
                "5.0",
                " vs. ",
                "4.3",
                "4.3",
                "4.3",
                " in the last checkpoint) while its post-personalization loss is smaller (",
                "2.9",
                "2.9",
                "2.9",
                " vs. ",
                "4.0",
                "4.0",
                "4.0",
                ").\nSimilar trends hold for FedCCnews and FedWiki datasets; cf. ",
                "Appendix",
                " ",
                "D",
                ".\nOverall, these results show that FedAvg’s superior personalization performance is ",
                "robust to shifts in the distribution over clients",
                ".",
                "This phenomenon highlights connections between federated learning and meta-learning previously noted in the literature ",
                "[",
                "83",
                ", ",
                "46",
                ", ",
                "84",
                ", ",
                "85",
                ", ",
                "86",
                ", ",
                "87",
                ", ",
                "88",
                "]",
                ".\nIn short, we see that FedAvg acts as a meta-learning algorithm (specifically, the Reptile algorithm ",
                "[",
                "89",
                "]",
                ") where it quickly minimizes the loss of a client after a few local gradient steps (i.e., after personalization). It does not behave like an algorithm designed to minimize the empirical risk. By contrast, FedSGD operates much like SGD in the centralized setting, attempting to minimize the average loss across all examples. To the best of our knowledge, ",
                "Figure",
                " ",
                "5",
                " constitutes some of the strongest empirical evidence of the connection between federated and meta-learning to date. The scale of the FedC4 dataset (enabled by Dataset Grouper) is critical here, as clients have sufficiently large amounts of data to exacerbate client drift ",
                "[",
                "28",
                "]",
                " and cause tension between loss minimization and meta-learning.",
                "Scaling to larger models.",
                "\nTo further demonstrate the scalability of Dataset Grouper, we train a transformer model with 1 billion parameters on the FedC4 dataset. In contrast to the results above, we train with 4 batches per client (rather than 64). Despite this, we see in ",
                "Figure",
                " ",
                "8",
                " that FedSGD still sees improved pre-personalization loss compared to FedAvg. Moreover, both algorithms see improved pre-personalization compared to ",
                "Table",
                " ",
                "5",
                ", highlighting the effect of increasing the larger size."
            ]
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Validation loss of FedAvg and FedSGD, before and after personalizing on a client’s dataset. Percentiles are computed across all clients in the FedC4 validation dataset.",
        "table": "<table id=\"S5.T5.16\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.16.17.1\" class=\"ltx_tr\">\n<td id=\"S5.T5.16.17.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T5.16.17.1.1.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></td>\n<td id=\"S5.T5.16.17.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\" colspan=\"3\"><span id=\"S5.T5.16.17.1.2.1\" class=\"ltx_text ltx_font_bold\">Pre-Personalization Loss</span></td>\n<td id=\"S5.T5.16.17.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\" colspan=\"3\"><span id=\"S5.T5.16.17.1.3.1\" class=\"ltx_text ltx_font_bold\">Post-Personalization Loss</span></td>\n</tr>\n<tr id=\"S5.T5.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T5.4.4.5\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<td id=\"S5.T5.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<math id=\"S5.T5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"10\" display=\"inline\"><semantics id=\"S5.T5.1.1.1.m1.1a\"><mn id=\"S5.T5.1.1.1.m1.1.1\" xref=\"S5.T5.1.1.1.m1.1.1.cmml\">10</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.1.1.1.m1.1b\"><cn type=\"integer\" id=\"S5.T5.1.1.1.m1.1.1.cmml\" xref=\"S5.T5.1.1.1.m1.1.1\">10</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.1.1.1.m1.1c\">10</annotation></semantics></math><sup id=\"S5.T5.1.1.1.1\" class=\"ltx_sup\">th</sup> perc.</td>\n<td id=\"S5.T5.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Median</td>\n<td id=\"S5.T5.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<math id=\"S5.T5.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"90\" display=\"inline\"><semantics id=\"S5.T5.2.2.2.m1.1a\"><mn id=\"S5.T5.2.2.2.m1.1.1\" xref=\"S5.T5.2.2.2.m1.1.1.cmml\">90</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.2.2.2.m1.1b\"><cn type=\"integer\" id=\"S5.T5.2.2.2.m1.1.1.cmml\" xref=\"S5.T5.2.2.2.m1.1.1\">90</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.2.2.2.m1.1c\">90</annotation></semantics></math><sup id=\"S5.T5.2.2.2.1\" class=\"ltx_sup\">th</sup> perc.</td>\n<td id=\"S5.T5.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<math id=\"S5.T5.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"10\" display=\"inline\"><semantics id=\"S5.T5.3.3.3.m1.1a\"><mn id=\"S5.T5.3.3.3.m1.1.1\" xref=\"S5.T5.3.3.3.m1.1.1.cmml\">10</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.3.3.3.m1.1b\"><cn type=\"integer\" id=\"S5.T5.3.3.3.m1.1.1.cmml\" xref=\"S5.T5.3.3.3.m1.1.1\">10</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.3.3.3.m1.1c\">10</annotation></semantics></math><sup id=\"S5.T5.3.3.3.1\" class=\"ltx_sup\">th</sup> perc.</td>\n<td id=\"S5.T5.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Median</td>\n<td id=\"S5.T5.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<math id=\"S5.T5.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"90\" display=\"inline\"><semantics id=\"S5.T5.4.4.4.m1.1a\"><mn id=\"S5.T5.4.4.4.m1.1.1\" xref=\"S5.T5.4.4.4.m1.1.1.cmml\">90</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.4.4.4.m1.1b\"><cn type=\"integer\" id=\"S5.T5.4.4.4.m1.1.1.cmml\" xref=\"S5.T5.4.4.4.m1.1.1\">90</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.4.4.4.m1.1c\">90</annotation></semantics></math><sup id=\"S5.T5.4.4.4.1\" class=\"ltx_sup\">th</sup> perc.</td>\n</tr>\n<tr id=\"S5.T5.10.10\" class=\"ltx_tr\">\n<td id=\"S5.T5.10.10.7\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedAvg</td>\n<td id=\"S5.T5.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T5.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"5.13\" display=\"inline\"><semantics id=\"S5.T5.5.5.1.m1.1a\"><mn id=\"S5.T5.5.5.1.m1.1.1\" xref=\"S5.T5.5.5.1.m1.1.1.cmml\">5.13</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.5.5.1.m1.1b\"><cn type=\"float\" id=\"S5.T5.5.5.1.m1.1.1.cmml\" xref=\"S5.T5.5.5.1.m1.1.1\">5.13</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.5.5.1.m1.1c\">5.13</annotation></semantics></math></td>\n<td id=\"S5.T5.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T5.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"5.64\" display=\"inline\"><semantics id=\"S5.T5.6.6.2.m1.1a\"><mn id=\"S5.T5.6.6.2.m1.1.1\" xref=\"S5.T5.6.6.2.m1.1.1.cmml\">5.64</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.6.6.2.m1.1b\"><cn type=\"float\" id=\"S5.T5.6.6.2.m1.1.1.cmml\" xref=\"S5.T5.6.6.2.m1.1.1\">5.64</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.6.6.2.m1.1c\">5.64</annotation></semantics></math></td>\n<td id=\"S5.T5.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T5.7.7.3.m1.1\" class=\"ltx_Math\" alttext=\"6.27\" display=\"inline\"><semantics id=\"S5.T5.7.7.3.m1.1a\"><mn id=\"S5.T5.7.7.3.m1.1.1\" xref=\"S5.T5.7.7.3.m1.1.1.cmml\">6.27</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.7.7.3.m1.1b\"><cn type=\"float\" id=\"S5.T5.7.7.3.m1.1.1.cmml\" xref=\"S5.T5.7.7.3.m1.1.1\">6.27</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.7.7.3.m1.1c\">6.27</annotation></semantics></math></td>\n<td id=\"S5.T5.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T5.8.8.4.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S5.T5.8.8.4.1.m1.1\" class=\"ltx_Math\" alttext=\"0.002\" display=\"inline\"><semantics id=\"S5.T5.8.8.4.1.m1.1a\"><mn mathbackground=\"#EDEDFF\" id=\"S5.T5.8.8.4.1.m1.1.1\" xref=\"S5.T5.8.8.4.1.m1.1.1.cmml\">0.002</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.8.8.4.1.m1.1b\"><cn type=\"float\" id=\"S5.T5.8.8.4.1.m1.1.1.cmml\" xref=\"S5.T5.8.8.4.1.m1.1.1\">0.002</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.8.8.4.1.m1.1c\">0.002</annotation></semantics></math></span></td>\n<td id=\"S5.T5.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T5.9.9.5.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S5.T5.9.9.5.1.m1.1\" class=\"ltx_Math\" alttext=\"0.012\" display=\"inline\"><semantics id=\"S5.T5.9.9.5.1.m1.1a\"><mn mathbackground=\"#EDEDFF\" id=\"S5.T5.9.9.5.1.m1.1.1\" xref=\"S5.T5.9.9.5.1.m1.1.1.cmml\">0.012</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.9.9.5.1.m1.1b\"><cn type=\"float\" id=\"S5.T5.9.9.5.1.m1.1.1.cmml\" xref=\"S5.T5.9.9.5.1.m1.1.1\">0.012</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.9.9.5.1.m1.1c\">0.012</annotation></semantics></math></span></td>\n<td id=\"S5.T5.10.10.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T5.10.10.6.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S5.T5.10.10.6.1.m1.1\" class=\"ltx_Math\" alttext=\"0.934\" display=\"inline\"><semantics id=\"S5.T5.10.10.6.1.m1.1a\"><mn mathbackground=\"#EDEDFF\" id=\"S5.T5.10.10.6.1.m1.1.1\" xref=\"S5.T5.10.10.6.1.m1.1.1.cmml\">0.934</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.10.10.6.1.m1.1b\"><cn type=\"float\" id=\"S5.T5.10.10.6.1.m1.1.1.cmml\" xref=\"S5.T5.10.10.6.1.m1.1.1\">0.934</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.10.10.6.1.m1.1c\">0.934</annotation></semantics></math></span></td>\n</tr>\n<tr id=\"S5.T5.16.16\" class=\"ltx_tr\">\n<td id=\"S5.T5.16.16.7\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedSGD</td>\n<td id=\"S5.T5.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T5.11.11.1.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S5.T5.11.11.1.1.m1.1\" class=\"ltx_Math\" alttext=\"4.38\" display=\"inline\"><semantics id=\"S5.T5.11.11.1.1.m1.1a\"><mn mathbackground=\"#EDEDFF\" id=\"S5.T5.11.11.1.1.m1.1.1\" xref=\"S5.T5.11.11.1.1.m1.1.1.cmml\">4.38</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.11.11.1.1.m1.1b\"><cn type=\"float\" id=\"S5.T5.11.11.1.1.m1.1.1.cmml\" xref=\"S5.T5.11.11.1.1.m1.1.1\">4.38</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.11.11.1.1.m1.1c\">4.38</annotation></semantics></math></span></td>\n<td id=\"S5.T5.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T5.12.12.2.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S5.T5.12.12.2.1.m1.1\" class=\"ltx_Math\" alttext=\"4.93\" display=\"inline\"><semantics id=\"S5.T5.12.12.2.1.m1.1a\"><mn mathbackground=\"#EDEDFF\" id=\"S5.T5.12.12.2.1.m1.1.1\" xref=\"S5.T5.12.12.2.1.m1.1.1.cmml\">4.93</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.12.12.2.1.m1.1b\"><cn type=\"float\" id=\"S5.T5.12.12.2.1.m1.1.1.cmml\" xref=\"S5.T5.12.12.2.1.m1.1.1\">4.93</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.12.12.2.1.m1.1c\">4.93</annotation></semantics></math></span></td>\n<td id=\"S5.T5.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"background-color:#EDEDFF;padding-top:1pt;padding-bottom:1pt;\"><span id=\"S5.T5.13.13.3.1\" class=\"ltx_text\" style=\"background-color:#EDEDFF;\"><math id=\"S5.T5.13.13.3.1.m1.1\" class=\"ltx_Math\" alttext=\"5.40\" display=\"inline\"><semantics id=\"S5.T5.13.13.3.1.m1.1a\"><mn mathbackground=\"#EDEDFF\" id=\"S5.T5.13.13.3.1.m1.1.1\" xref=\"S5.T5.13.13.3.1.m1.1.1.cmml\">5.40</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.13.13.3.1.m1.1b\"><cn type=\"float\" id=\"S5.T5.13.13.3.1.m1.1.1.cmml\" xref=\"S5.T5.13.13.3.1.m1.1.1\">5.40</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.13.13.3.1.m1.1c\">5.40</annotation></semantics></math></span></td>\n<td id=\"S5.T5.14.14.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T5.14.14.4.m1.1\" class=\"ltx_Math\" alttext=\"1.25\" display=\"inline\"><semantics id=\"S5.T5.14.14.4.m1.1a\"><mn id=\"S5.T5.14.14.4.m1.1.1\" xref=\"S5.T5.14.14.4.m1.1.1.cmml\">1.25</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.14.14.4.m1.1b\"><cn type=\"float\" id=\"S5.T5.14.14.4.m1.1.1.cmml\" xref=\"S5.T5.14.14.4.m1.1.1\">1.25</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.14.14.4.m1.1c\">1.25</annotation></semantics></math></td>\n<td id=\"S5.T5.15.15.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T5.15.15.5.m1.1\" class=\"ltx_Math\" alttext=\"3.38\" display=\"inline\"><semantics id=\"S5.T5.15.15.5.m1.1a\"><mn id=\"S5.T5.15.15.5.m1.1.1\" xref=\"S5.T5.15.15.5.m1.1.1.cmml\">3.38</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.15.15.5.m1.1b\"><cn type=\"float\" id=\"S5.T5.15.15.5.m1.1.1.cmml\" xref=\"S5.T5.15.15.5.m1.1.1\">3.38</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.15.15.5.m1.1c\">3.38</annotation></semantics></math></td>\n<td id=\"S5.T5.16.16.6\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"S5.T5.16.16.6.m1.1\" class=\"ltx_Math\" alttext=\"4.53\" display=\"inline\"><semantics id=\"S5.T5.16.16.6.m1.1a\"><mn id=\"S5.T5.16.16.6.m1.1.1\" xref=\"S5.T5.16.16.6.m1.1.1.cmml\">4.53</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.16.16.6.m1.1b\"><cn type=\"float\" id=\"S5.T5.16.16.6.m1.1.1.cmml\" xref=\"S5.T5.16.16.6.m1.1.1\">4.53</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.16.16.6.m1.1c\">4.53</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Iteration efficiency.",
                " We test the efficiency of Dataset Grouper in practical large-scale simulations. Specifically, we measure the time it takes for each round of federated training and what portion of that time is spent iterating over data, including preprocessing. We perform 100 rounds of FedAvg for varying cohort sizes (the number of clients per round) and present the results in ",
                "Table",
                " ",
                "4",
                ". We see that dataset iteration takes under ",
                "10",
                "%",
                "percent",
                "10",
                "10\\%",
                " of the total runtime, even for larger cohort sizes. This is despite the fact that dataset iteration is done entirely on the host, while the training time is parallelized between multiple TPU slices.\nFurther improvements in the data pipeline can only lead to a marginal speedup, highlighting the efficiency and scalability of the streaming dataset format in ",
                "Section",
                " ",
                "3.1",
                ".",
                "Federated learning rate schedules.",
                " Large-scale training on non-partitioned data generally involves a variety of important techniques, such as learning rate scheduling, to attain good performance. In order to determine how best to scale federated training to larger-scale settings, we investigate the use of various learning rate schedules for FedAvg and FedSGD. In both cases, we apply the learning rate schedule at the ",
                "server",
                " (see ",
                "[",
                "30",
                "]",
                " for a discussion of client versus server optimizers). We use constant learning rates, warmup with exponential decay, and warmup with cosine decay. Whenever we use warmup, we warmup for 10% of the total number of training rounds, and decay for the remainder.",
                "We compare the resulting training loss for FedAvg and FedSGD in ",
                "Figure",
                " ",
                "4",
                ". Notably, we see that learning rate scheduling leads to significant improvements in the behavior of FedSGD, while FedAvg is robust to different choices. This reflects the fact that these learning rate schedules were developed in the context of SGD, which involves applying many unbiased gradient estimates. FedSGD operates similarly, computing an unbiased gradient estimate at each round. By contrast, FedAvg involves biased gradients, often called “pseudo-gradients” ",
                "[",
                "30",
                "]",
                ", which may not be the gradient of any loss function ",
                "[",
                "82",
                "]",
                ". Our results suggest that developing effective learning rate scheduling techniques for FedAvg is an open question, and may involve coordinating client and server learning rates.",
                "We also see that FedAvg appears to attain a significantly lower train loss than FedSGD. We stress that this is due to how the training loss is computed. For both algorithms, it is computed by averaging the loss of all batches seen by a client and then averaging that quantity across all clients. However, the client trains as it sees data batches in FedAvg. Therefore, the client’s local model adapts to its own distribution (leading to a lower loss), while in FedSGD the client does not adapt its local model. We explore this difference, which is connected to ",
                "meta-learning",
                ", below.",
                "Federated evaluation and personalization.",
                " Partitioned datasets enable group-structured learning, as well as group-level (or federated) evaluation, which may be particularly informative for measuring downstream performance across heterogeneous data splits. To demonstrate this, we use Dataset Grouper to generate an evaluation dataset from FedC4 by using its held-out validation split. We use the same partition structure as before, grouping examples according to their base domain. Because of this group structure, we can compute histograms of metrics across all groups, rather than just an average metric across all examples.",
                "We take the resulting models trained by FedAvg and FedSGD (with constant learning rates, though we see similar results for all learning rate schedules we considered above), and compute two separate metrics for each validation client. First, we compute the average loss of the model on all examples held by the client. We refer to this as the ",
                "pre-personalization loss",
                ". We then fine-tune the model for a single epoch on the client’s dataset (using a client optimizer of SGD with a tuned learning rate). After personalization, we compute the average loss again, resulting in the ",
                "post-personalization loss",
                ".",
                "We present quantiles of these metrics in ",
                "Table",
                " ",
                "5",
                ". Intriguingly, they show that the FedSGD-trained model works better for pre-personalization, but the FedAvg-trained model is much more effective at personalizing to the client’s data. To further illustrate this, we consider histograms of the two distributions (across all clients) in ",
                "Figure",
                " ",
                "5",
                ". This suggests a more dramatic shift. While the FedAvg- and FedSGD-trained models are close in pre-personalization performance (though FedSGD does better), the post-personalization distribution for FedAvg is extremely light-tailed.",
                "Task-specific personalization.",
                " Pre-trained foundation models are typically employed on a range of downstream tasks. In this spirit, we use the models trained on FedC4 to perform pre- and post-personalization evaluation on FedBookCO.\nThe results, ",
                "Figures",
                " ",
                "6",
                " and ",
                "7",
                ", are similar to but less drastic than those of ",
                "Figure",
                " ",
                "5",
                ". The pre-personalization loss of FedAvg is slightly larger than FedSGD (",
                "5.0",
                "5.0",
                "5.0",
                " vs. ",
                "4.3",
                "4.3",
                "4.3",
                " in the last checkpoint) while its post-personalization loss is smaller (",
                "2.9",
                "2.9",
                "2.9",
                " vs. ",
                "4.0",
                "4.0",
                "4.0",
                ").\nSimilar trends hold for FedCCnews and FedWiki datasets; cf. ",
                "Appendix",
                " ",
                "D",
                ".\nOverall, these results show that FedAvg’s superior personalization performance is ",
                "robust to shifts in the distribution over clients",
                ".",
                "This phenomenon highlights connections between federated learning and meta-learning previously noted in the literature ",
                "[",
                "83",
                ", ",
                "46",
                ", ",
                "84",
                ", ",
                "85",
                ", ",
                "86",
                ", ",
                "87",
                ", ",
                "88",
                "]",
                ".\nIn short, we see that FedAvg acts as a meta-learning algorithm (specifically, the Reptile algorithm ",
                "[",
                "89",
                "]",
                ") where it quickly minimizes the loss of a client after a few local gradient steps (i.e., after personalization). It does not behave like an algorithm designed to minimize the empirical risk. By contrast, FedSGD operates much like SGD in the centralized setting, attempting to minimize the average loss across all examples. To the best of our knowledge, ",
                "Figure",
                " ",
                "5",
                " constitutes some of the strongest empirical evidence of the connection between federated and meta-learning to date. The scale of the FedC4 dataset (enabled by Dataset Grouper) is critical here, as clients have sufficiently large amounts of data to exacerbate client drift ",
                "[",
                "28",
                "]",
                " and cause tension between loss minimization and meta-learning.",
                "Scaling to larger models.",
                "\nTo further demonstrate the scalability of Dataset Grouper, we train a transformer model with 1 billion parameters on the FedC4 dataset. In contrast to the results above, we train with 4 batches per client (rather than 64). Despite this, we see in ",
                "Figure",
                " ",
                "8",
                " that FedSGD still sees improved pre-personalization loss compared to FedAvg. Moreover, both algorithms see improved pre-personalization compared to ",
                "Table",
                " ",
                "5",
                ", highlighting the effect of increasing the larger size."
            ]
        ]
    },
    "A2.T6": {
        "caption": "Table 6: Detailed version of Table 1:\nA summary of the per-client statistics of the new language modeling datasets we introduce using Dataset Grouper, and of previous benchmark datasets supplied by TFF, Leaf, FedNLP, and FedScale.\nErratum: The previous version incorrectly displayed the maximum number of words per client instead of the 909090th percentile for the first 4 datasets.\n",
        "table": "",
        "footnotes": "\n\n\n\n\n\nSource\nDataset\nPartition on\n#Clients\n#Words\n#Words per client\n\n\n\n\n\n\n101010th perc.\n252525th perc.\nMedian\n757575th perc.\n909090th perc.\n\nOurs\nFedC4\nDomain\n15.6​M15.6M15.6\\mathrm{M}\n132​B132B132\\mathrm{B}\n828282\n220220220\n815815815\n3.3​K3.3K3.3\\mathrm{K}\n11​K11K11\\mathrm{K}\n\nFedWiki\nArticle\n6.5​M6.5M6.5\\mathrm{M}\n3​B3B3\\mathrm{B}\n393939\n757575\n198198198\n486486486\n1​K1K1\\mathrm{K}\n\nFedBookCO\nBook\n18​K18K18\\mathrm{K}\n1.2​B1.2B1.2\\mathrm{B}\n24​K24K24\\mathrm{K}\n32​K32K32\\mathrm{K}\n52​K52K52\\mathrm{K}\n81​K81K81\\mathrm{K}\n111​K111K111\\mathrm{K}\n\nFedCCnews\nDomain\n8.8​K8.8K8.8\\mathrm{K}\n0.3​B0.3B0.3\\mathrm{B}\n303303303\n1.1​K1.1K1.1\\mathrm{K}\n5​K5K5\\mathrm{K}\n20​K20K20\\mathrm{K}\n64​K64K64\\mathrm{K}\n\nExisting\nAmazon Reviews\nAccount\n1.5​M1.5M1.5\\mathrm{M}\n4.3​B4.3B4.3\\mathrm{B}\n278278278\n565565565\n1.1​K1.1K1.1\\mathrm{K}\n2.3​K2.3K2.3\\mathrm{K}\n5​K5K5\\mathrm{K}\n\nStack Overflow\nAccount\n0.3​M0.3M0.3\\mathrm{M}\n2​B2B2\\mathrm{B}\n1.2​K1.2K1.2\\mathrm{K}\n1.7​K1.7K1.7\\mathrm{K}\n2.7​K2.7K2.7\\mathrm{K}\n5.1​K5.1K5.1\\mathrm{K}\n11​K11K11\\mathrm{K}\n\nReddit\nAccount\n1.7​M1.7M1.7\\mathrm{M}\n1.2​B1.2B1.2\\mathrm{B}\n585858\n111111111\n257257257\n675675675\n172017201720\n\nBlog Corpus\nAccount\n17​K17K17\\mathrm{K}\n0.1​B0.1B0.1\\mathrm{B}\n551551551\n908908908\n2​K2K2\\mathrm{K}\n5.3​K5.3K5.3\\mathrm{K}\n13​K13K13\\mathrm{K}\n\nShakespeare\nRole/play\n715715715\n0.4​M0.4M0.4\\mathrm{M}\n141414\n454545\n175175175\n0.6​K0.6K0.6\\mathrm{K}\n1.6​K1.6K1.6\\mathrm{K}\n\nGigaword\nSynthetic\n100100100\n0.3​M0.3M0.3\\mathrm{M}\n3.0​K3.0K3.0\\mathrm{K}\n3.1​K3.1K3.1\\mathrm{K}\n3.1​K3.1K3.1\\mathrm{K}\n3.2​K3.2K3.2\\mathrm{K}\n3.2​K3.2K3.2\\mathrm{K}\n\n\n",
        "references": [
            [
                "Here, we detail various statistics of the federated language-modeling datasets we propose and discuss in ",
                "Section",
                " ",
                "4",
                ". In ",
                "Table",
                " ",
                "6",
                ", we present the per-client statistics of the dataset, and compare these to per-client statistics of existing federated language modeling datasets supplied by TensorFlow Federated ",
                "[",
                "11",
                "]",
                ", Leaf ",
                "[",
                "12",
                "]",
                ", FedNLP ",
                "[",
                "13",
                "]",
                ", and FedScale ",
                "[",
                "15",
                "]",
                ". We see that at larger percentiles, FedC4, FedWiki, FedBookCO, and FedCCnews contain many more words per client. FedBookCO also contains dramatically more words per client at lower percentiles than previous datasets. In general, datasets like FedC4 and FedCCnews exhibit a dramatic variance in statistics across clients. This can make the datasets more challenging for federated algorithms, and potentially more representative of heavy-tailed settings in practice.",
                "In ",
                "Table",
                " ",
                "7",
                ", we compare the per-example (i.e. per-sequence) statistics of the same datasets. We note that this information is important for large-scale language modeling tasks, which may require long sequences of tokens for training. We see that for nearly every percentile, the datasets we introduce contain sequences of significantly larger lengths. This is especially true of larger percentiles, at which point FedBookCO and FedWiki contains examples with thousands if not millions of words.",
                "To better visualize and compare the distribution of words per client, and given the large number of clients in each dataset, we present a letter value plot ",
                "[",
                "92",
                "]",
                " of these distributions. The result is in ",
                "Figure",
                " ",
                "9",
                ". This plot gives various quantiles of the distribution of the number of words per client. We see that many of these datasets exhibit large amounts of variance between quantiles. FedC4 has an especially heavy tail, with some clients having fewer than 10 words, while others have tens or even hundreds of millions of words.",
                "Example application scenarios.",
                "\nFedBookCO (each client is a book) and FedWiki (each client is a Wikipedia article) map to applications where each client is an “expert” in a certain topic. In the context of modern LLM pipelines where each sequence is broken up into multiple examples of a fixed length, the fact that each client has a single sequence is much less important than the total number of words. This is especially true for FedBookCO where the ",
                "10",
                "10",
                "10",
                "th",
                " percentile data sequence length is ",
                "24",
                "​",
                "K",
                "24",
                "K",
                "24\\mathrm{K}",
                " words, while the maximum sequence length for LLMs today is ",
                "O",
                "​",
                "(",
                "1",
                "​",
                "K",
                ")",
                "𝑂",
                "1",
                "K",
                "O(1\\mathrm{K})",
                " words.",
                "FedC4 is typical of a group-structured pre-training (or second stage pre-training) dataset, which might be practically encountered in practice with documents (corporate, medical, legal, etc.) or emails. FedCCnews is a subset of FedC4 with similar long-tailed characteristics and is suitable for faster experimentation of second stage pre-training or fine-tuning. It can also be used to study the effects of pretraining data contamination."
            ]
        ]
    },
    "A2.T7": {
        "caption": "Table 7: Detailed version of Table 1:\nA summary of the per-example (i.e., per-sequence) statistics of the new datasets we introduce using Dataset Grouper, and of previous benchmark datasets supplied by TFF, Leaf, FedNLP, and FedScale.\nErratum: The previous version incorrectly displayed the maximum number of words per example instead of the 909090th percentile for FedWiki and FedBookCO.\n",
        "table": "",
        "footnotes": "\n\n\n\n\n\nSource\nDataset\nPartition on\n#Examples\n#Words\n#Words per Exmaple\n\n\n\n\n\n\n101010th perc.\n252525th perc.\nMedian\n757575th perc.\n909090th perc.\n\nOurs\nFedC4\nDomain\n0.36​B0.36B0.36\\mathrm{B}\n132​B132B132\\mathrm{B}\n494949\n888888\n191191191\n417417417\n783783783\n\nFedWiki\nArticle\n6.5​M6.5M6.5\\mathrm{M}\n3​B3B3\\mathrm{B}\n393939\n757575\n198198198\n486486486\n1​K1K1\\mathrm{K}\n\nFedBookCO\nBook\n18​K18K18\\mathrm{K}\n1.2​B1.2B1.2\\mathrm{B}\n24​K24K24\\mathrm{K}\n32​K32K32\\mathrm{K}\n52​K52K52\\mathrm{K}\n81​K81K81\\mathrm{K}\n111​K111K111\\mathrm{K}\n\nFedCCnews\nDomain\n0.7​M0.7M0.7\\mathrm{M}\n0.3​B0.3B0.3\\mathrm{B}\n787878\n151151151\n316316316\n548548548\n842842842\n\nExisting\nAmazon Reviews\nAccount\n68​M68M68\\mathrm{M}\n4.3​B4.3B4.3\\mathrm{B}\n333\n101010\n282828\n676767\n155155155\n\nStack Overflow\nAccount\n0.1​B0.1B0.1\\mathrm{B}\n2​B2B2\\mathrm{B}\n333\n777\n131313\n202020\n292929\n\nReddit\nAccount\n33​M33M33\\mathrm{M}\n1.2​B1.2B1.2\\mathrm{B}\n777\n111111\n212121\n424242\n818181\n\nBlog Corpus\nAccount\n0.5​M0.5M0.5\\mathrm{M}\n0.1​B0.1B0.1\\mathrm{B}\n666\n282828\n105105105\n248248248\n460460460\n\nShakespeare\nRole/play\n16​K16K16\\mathrm{K}\n0.4​M0.4M0.4\\mathrm{M}\n444\n888\n121212\n292929\n636363\n\nGigaword\nSynthetic\n10​K10K10\\mathrm{K}\n0.3​M0.3M0.3\\mathrm{M}\n212121\n262626\n313131\n363636\n414141\n\n\n",
        "references": [
            [
                "Here, we detail various statistics of the federated language-modeling datasets we propose and discuss in ",
                "Section",
                " ",
                "4",
                ". In ",
                "Table",
                " ",
                "6",
                ", we present the per-client statistics of the dataset, and compare these to per-client statistics of existing federated language modeling datasets supplied by TensorFlow Federated ",
                "[",
                "11",
                "]",
                ", Leaf ",
                "[",
                "12",
                "]",
                ", FedNLP ",
                "[",
                "13",
                "]",
                ", and FedScale ",
                "[",
                "15",
                "]",
                ". We see that at larger percentiles, FedC4, FedWiki, FedBookCO, and FedCCnews contain many more words per client. FedBookCO also contains dramatically more words per client at lower percentiles than previous datasets. In general, datasets like FedC4 and FedCCnews exhibit a dramatic variance in statistics across clients. This can make the datasets more challenging for federated algorithms, and potentially more representative of heavy-tailed settings in practice.",
                "In ",
                "Table",
                " ",
                "7",
                ", we compare the per-example (i.e. per-sequence) statistics of the same datasets. We note that this information is important for large-scale language modeling tasks, which may require long sequences of tokens for training. We see that for nearly every percentile, the datasets we introduce contain sequences of significantly larger lengths. This is especially true of larger percentiles, at which point FedBookCO and FedWiki contains examples with thousands if not millions of words.",
                "To better visualize and compare the distribution of words per client, and given the large number of clients in each dataset, we present a letter value plot ",
                "[",
                "92",
                "]",
                " of these distributions. The result is in ",
                "Figure",
                " ",
                "9",
                ". This plot gives various quantiles of the distribution of the number of words per client. We see that many of these datasets exhibit large amounts of variance between quantiles. FedC4 has an especially heavy tail, with some clients having fewer than 10 words, while others have tens or even hundreds of millions of words.",
                "Example application scenarios.",
                "\nFedBookCO (each client is a book) and FedWiki (each client is a Wikipedia article) map to applications where each client is an “expert” in a certain topic. In the context of modern LLM pipelines where each sequence is broken up into multiple examples of a fixed length, the fact that each client has a single sequence is much less important than the total number of words. This is especially true for FedBookCO where the ",
                "10",
                "10",
                "10",
                "th",
                " percentile data sequence length is ",
                "24",
                "​",
                "K",
                "24",
                "K",
                "24\\mathrm{K}",
                " words, while the maximum sequence length for LLMs today is ",
                "O",
                "​",
                "(",
                "1",
                "​",
                "K",
                ")",
                "𝑂",
                "1",
                "K",
                "O(1\\mathrm{K})",
                " words.",
                "FedC4 is typical of a group-structured pre-training (or second stage pre-training) dataset, which might be practically encountered in practice with documents (corporate, medical, legal, etc.) or emails. FedCCnews is a subset of FedC4 with similar long-tailed characteristics and is suitable for faster experimentation of second stage pre-training or fine-tuning. It can also be used to study the effects of pretraining data contamination."
            ]
        ]
    },
    "A3.T8": {
        "caption": "Table 8: Optimizer hyperparameters.",
        "table": "<table id=\"A3.T8.8\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A3.T8.8.9.1\" class=\"ltx_tr\">\n<th id=\"A3.T8.8.9.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A3.T8.8.9.1.1.1\" class=\"ltx_text ltx_font_bold\">Placement</span></th>\n<th id=\"A3.T8.8.9.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A3.T8.8.9.1.2.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></th>\n<th id=\"A3.T8.8.9.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A3.T8.8.9.1.3.1\" class=\"ltx_text ltx_font_bold\">Hyperparameter</span></th>\n<th id=\"A3.T8.8.9.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A3.T8.8.9.1.4.1\" class=\"ltx_text ltx_font_bold\">Value</span></th>\n<th id=\"A3.T8.8.9.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A3.T8.8.9.1.5.1\" class=\"ltx_text ltx_font_bold\">Tuning Range</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T8.2.2\" class=\"ltx_tr\">\n<td id=\"A3.T8.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"4\"><span id=\"A3.T8.2.2.3.1\" class=\"ltx_text\">Server</span></td>\n<td id=\"A3.T8.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"4\"><span id=\"A3.T8.2.2.4.1\" class=\"ltx_text\">Adam</span></td>\n<td id=\"A3.T8.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Learning Rate (<math id=\"A3.T8.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\eta_{s}\" display=\"inline\"><semantics id=\"A3.T8.1.1.1.m1.1a\"><msub id=\"A3.T8.1.1.1.m1.1.1\" xref=\"A3.T8.1.1.1.m1.1.1.cmml\"><mi id=\"A3.T8.1.1.1.m1.1.1.2\" xref=\"A3.T8.1.1.1.m1.1.1.2.cmml\">η</mi><mi id=\"A3.T8.1.1.1.m1.1.1.3\" xref=\"A3.T8.1.1.1.m1.1.1.3.cmml\">s</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.T8.1.1.1.m1.1b\"><apply id=\"A3.T8.1.1.1.m1.1.1.cmml\" xref=\"A3.T8.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T8.1.1.1.m1.1.1.1.cmml\" xref=\"A3.T8.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"A3.T8.1.1.1.m1.1.1.2.cmml\" xref=\"A3.T8.1.1.1.m1.1.1.2\">𝜂</ci><ci id=\"A3.T8.1.1.1.m1.1.1.3.cmml\" xref=\"A3.T8.1.1.1.m1.1.1.3\">𝑠</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T8.1.1.1.m1.1c\">\\eta_{s}</annotation></semantics></math>)</td>\n<td id=\"A3.T8.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">N/A</td>\n<td id=\"A3.T8.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T8.2.2.2.m1.4\" class=\"ltx_Math\" alttext=\"\\{10^{-4},10^{-3},\\dots,10^{0}\\}\" display=\"inline\"><semantics id=\"A3.T8.2.2.2.m1.4a\"><mrow id=\"A3.T8.2.2.2.m1.4.4.3\" xref=\"A3.T8.2.2.2.m1.4.4.4.cmml\"><mo stretchy=\"false\" id=\"A3.T8.2.2.2.m1.4.4.3.4\" xref=\"A3.T8.2.2.2.m1.4.4.4.cmml\">{</mo><msup id=\"A3.T8.2.2.2.m1.2.2.1.1\" xref=\"A3.T8.2.2.2.m1.2.2.1.1.cmml\"><mn id=\"A3.T8.2.2.2.m1.2.2.1.1.2\" xref=\"A3.T8.2.2.2.m1.2.2.1.1.2.cmml\">10</mn><mrow id=\"A3.T8.2.2.2.m1.2.2.1.1.3\" xref=\"A3.T8.2.2.2.m1.2.2.1.1.3.cmml\"><mo id=\"A3.T8.2.2.2.m1.2.2.1.1.3a\" xref=\"A3.T8.2.2.2.m1.2.2.1.1.3.cmml\">−</mo><mn id=\"A3.T8.2.2.2.m1.2.2.1.1.3.2\" xref=\"A3.T8.2.2.2.m1.2.2.1.1.3.2.cmml\">4</mn></mrow></msup><mo id=\"A3.T8.2.2.2.m1.4.4.3.5\" xref=\"A3.T8.2.2.2.m1.4.4.4.cmml\">,</mo><msup id=\"A3.T8.2.2.2.m1.3.3.2.2\" xref=\"A3.T8.2.2.2.m1.3.3.2.2.cmml\"><mn id=\"A3.T8.2.2.2.m1.3.3.2.2.2\" xref=\"A3.T8.2.2.2.m1.3.3.2.2.2.cmml\">10</mn><mrow id=\"A3.T8.2.2.2.m1.3.3.2.2.3\" xref=\"A3.T8.2.2.2.m1.3.3.2.2.3.cmml\"><mo id=\"A3.T8.2.2.2.m1.3.3.2.2.3a\" xref=\"A3.T8.2.2.2.m1.3.3.2.2.3.cmml\">−</mo><mn id=\"A3.T8.2.2.2.m1.3.3.2.2.3.2\" xref=\"A3.T8.2.2.2.m1.3.3.2.2.3.2.cmml\">3</mn></mrow></msup><mo id=\"A3.T8.2.2.2.m1.4.4.3.6\" xref=\"A3.T8.2.2.2.m1.4.4.4.cmml\">,</mo><mi mathvariant=\"normal\" id=\"A3.T8.2.2.2.m1.1.1\" xref=\"A3.T8.2.2.2.m1.1.1.cmml\">…</mi><mo id=\"A3.T8.2.2.2.m1.4.4.3.7\" xref=\"A3.T8.2.2.2.m1.4.4.4.cmml\">,</mo><msup id=\"A3.T8.2.2.2.m1.4.4.3.3\" xref=\"A3.T8.2.2.2.m1.4.4.3.3.cmml\"><mn id=\"A3.T8.2.2.2.m1.4.4.3.3.2\" xref=\"A3.T8.2.2.2.m1.4.4.3.3.2.cmml\">10</mn><mn id=\"A3.T8.2.2.2.m1.4.4.3.3.3\" xref=\"A3.T8.2.2.2.m1.4.4.3.3.3.cmml\">0</mn></msup><mo stretchy=\"false\" id=\"A3.T8.2.2.2.m1.4.4.3.8\" xref=\"A3.T8.2.2.2.m1.4.4.4.cmml\">}</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A3.T8.2.2.2.m1.4b\"><set id=\"A3.T8.2.2.2.m1.4.4.4.cmml\" xref=\"A3.T8.2.2.2.m1.4.4.3\"><apply id=\"A3.T8.2.2.2.m1.2.2.1.1.cmml\" xref=\"A3.T8.2.2.2.m1.2.2.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T8.2.2.2.m1.2.2.1.1.1.cmml\" xref=\"A3.T8.2.2.2.m1.2.2.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T8.2.2.2.m1.2.2.1.1.2.cmml\" xref=\"A3.T8.2.2.2.m1.2.2.1.1.2\">10</cn><apply id=\"A3.T8.2.2.2.m1.2.2.1.1.3.cmml\" xref=\"A3.T8.2.2.2.m1.2.2.1.1.3\"><minus id=\"A3.T8.2.2.2.m1.2.2.1.1.3.1.cmml\" xref=\"A3.T8.2.2.2.m1.2.2.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T8.2.2.2.m1.2.2.1.1.3.2.cmml\" xref=\"A3.T8.2.2.2.m1.2.2.1.1.3.2\">4</cn></apply></apply><apply id=\"A3.T8.2.2.2.m1.3.3.2.2.cmml\" xref=\"A3.T8.2.2.2.m1.3.3.2.2\"><csymbol cd=\"ambiguous\" id=\"A3.T8.2.2.2.m1.3.3.2.2.1.cmml\" xref=\"A3.T8.2.2.2.m1.3.3.2.2\">superscript</csymbol><cn type=\"integer\" id=\"A3.T8.2.2.2.m1.3.3.2.2.2.cmml\" xref=\"A3.T8.2.2.2.m1.3.3.2.2.2\">10</cn><apply id=\"A3.T8.2.2.2.m1.3.3.2.2.3.cmml\" xref=\"A3.T8.2.2.2.m1.3.3.2.2.3\"><minus id=\"A3.T8.2.2.2.m1.3.3.2.2.3.1.cmml\" xref=\"A3.T8.2.2.2.m1.3.3.2.2.3\"></minus><cn type=\"integer\" id=\"A3.T8.2.2.2.m1.3.3.2.2.3.2.cmml\" xref=\"A3.T8.2.2.2.m1.3.3.2.2.3.2\">3</cn></apply></apply><ci id=\"A3.T8.2.2.2.m1.1.1.cmml\" xref=\"A3.T8.2.2.2.m1.1.1\">…</ci><apply id=\"A3.T8.2.2.2.m1.4.4.3.3.cmml\" xref=\"A3.T8.2.2.2.m1.4.4.3.3\"><csymbol cd=\"ambiguous\" id=\"A3.T8.2.2.2.m1.4.4.3.3.1.cmml\" xref=\"A3.T8.2.2.2.m1.4.4.3.3\">superscript</csymbol><cn type=\"integer\" id=\"A3.T8.2.2.2.m1.4.4.3.3.2.cmml\" xref=\"A3.T8.2.2.2.m1.4.4.3.3.2\">10</cn><cn type=\"integer\" id=\"A3.T8.2.2.2.m1.4.4.3.3.3.cmml\" xref=\"A3.T8.2.2.2.m1.4.4.3.3.3\">0</cn></apply></set></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T8.2.2.2.m1.4c\">\\{10^{-4},10^{-3},\\dots,10^{0}\\}</annotation></semantics></math></td>\n</tr>\n<tr id=\"A3.T8.3.3\" class=\"ltx_tr\">\n<td id=\"A3.T8.3.3.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">First Moment Decay (<math id=\"A3.T8.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\beta_{1}\" display=\"inline\"><semantics id=\"A3.T8.3.3.1.m1.1a\"><msub id=\"A3.T8.3.3.1.m1.1.1\" xref=\"A3.T8.3.3.1.m1.1.1.cmml\"><mi id=\"A3.T8.3.3.1.m1.1.1.2\" xref=\"A3.T8.3.3.1.m1.1.1.2.cmml\">β</mi><mn id=\"A3.T8.3.3.1.m1.1.1.3\" xref=\"A3.T8.3.3.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.T8.3.3.1.m1.1b\"><apply id=\"A3.T8.3.3.1.m1.1.1.cmml\" xref=\"A3.T8.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T8.3.3.1.m1.1.1.1.cmml\" xref=\"A3.T8.3.3.1.m1.1.1\">subscript</csymbol><ci id=\"A3.T8.3.3.1.m1.1.1.2.cmml\" xref=\"A3.T8.3.3.1.m1.1.1.2\">𝛽</ci><cn type=\"integer\" id=\"A3.T8.3.3.1.m1.1.1.3.cmml\" xref=\"A3.T8.3.3.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T8.3.3.1.m1.1c\">\\beta_{1}</annotation></semantics></math>)</td>\n<td id=\"A3.T8.3.3.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.9</td>\n<td id=\"A3.T8.3.3.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">N/A</td>\n</tr>\n<tr id=\"A3.T8.4.4\" class=\"ltx_tr\">\n<td id=\"A3.T8.4.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Second Moment Decay (<math id=\"A3.T8.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\beta_{2}\" display=\"inline\"><semantics id=\"A3.T8.4.4.1.m1.1a\"><msub id=\"A3.T8.4.4.1.m1.1.1\" xref=\"A3.T8.4.4.1.m1.1.1.cmml\"><mi id=\"A3.T8.4.4.1.m1.1.1.2\" xref=\"A3.T8.4.4.1.m1.1.1.2.cmml\">β</mi><mn id=\"A3.T8.4.4.1.m1.1.1.3\" xref=\"A3.T8.4.4.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.T8.4.4.1.m1.1b\"><apply id=\"A3.T8.4.4.1.m1.1.1.cmml\" xref=\"A3.T8.4.4.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T8.4.4.1.m1.1.1.1.cmml\" xref=\"A3.T8.4.4.1.m1.1.1\">subscript</csymbol><ci id=\"A3.T8.4.4.1.m1.1.1.2.cmml\" xref=\"A3.T8.4.4.1.m1.1.1.2\">𝛽</ci><cn type=\"integer\" id=\"A3.T8.4.4.1.m1.1.1.3.cmml\" xref=\"A3.T8.4.4.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T8.4.4.1.m1.1c\">\\beta_{2}</annotation></semantics></math>)</td>\n<td id=\"A3.T8.4.4.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.999</td>\n<td id=\"A3.T8.4.4.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">N/A</td>\n</tr>\n<tr id=\"A3.T8.6.6\" class=\"ltx_tr\">\n<td id=\"A3.T8.5.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Numerical Stability Term (<math id=\"A3.T8.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\varepsilon\" display=\"inline\"><semantics id=\"A3.T8.5.5.1.m1.1a\"><mi id=\"A3.T8.5.5.1.m1.1.1\" xref=\"A3.T8.5.5.1.m1.1.1.cmml\">ε</mi><annotation-xml encoding=\"MathML-Content\" id=\"A3.T8.5.5.1.m1.1b\"><ci id=\"A3.T8.5.5.1.m1.1.1.cmml\" xref=\"A3.T8.5.5.1.m1.1.1\">𝜀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T8.5.5.1.m1.1c\">\\varepsilon</annotation></semantics></math>)</td>\n<td id=\"A3.T8.6.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T8.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"10^{-8}\" display=\"inline\"><semantics id=\"A3.T8.6.6.2.m1.1a\"><msup id=\"A3.T8.6.6.2.m1.1.1\" xref=\"A3.T8.6.6.2.m1.1.1.cmml\"><mn id=\"A3.T8.6.6.2.m1.1.1.2\" xref=\"A3.T8.6.6.2.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T8.6.6.2.m1.1.1.3\" xref=\"A3.T8.6.6.2.m1.1.1.3.cmml\"><mo id=\"A3.T8.6.6.2.m1.1.1.3a\" xref=\"A3.T8.6.6.2.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T8.6.6.2.m1.1.1.3.2\" xref=\"A3.T8.6.6.2.m1.1.1.3.2.cmml\">8</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T8.6.6.2.m1.1b\"><apply id=\"A3.T8.6.6.2.m1.1.1.cmml\" xref=\"A3.T8.6.6.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T8.6.6.2.m1.1.1.1.cmml\" xref=\"A3.T8.6.6.2.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T8.6.6.2.m1.1.1.2.cmml\" xref=\"A3.T8.6.6.2.m1.1.1.2\">10</cn><apply id=\"A3.T8.6.6.2.m1.1.1.3.cmml\" xref=\"A3.T8.6.6.2.m1.1.1.3\"><minus id=\"A3.T8.6.6.2.m1.1.1.3.1.cmml\" xref=\"A3.T8.6.6.2.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T8.6.6.2.m1.1.1.3.2.cmml\" xref=\"A3.T8.6.6.2.m1.1.1.3.2\">8</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T8.6.6.2.m1.1c\">10^{-8}</annotation></semantics></math></td>\n<td id=\"A3.T8.6.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">N/A</td>\n</tr>\n<tr id=\"A3.T8.8.8\" class=\"ltx_tr\">\n<td id=\"A3.T8.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Clients</td>\n<td id=\"A3.T8.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">SGD</td>\n<td id=\"A3.T8.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Learning Rate (<math id=\"A3.T8.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"\\eta_{c}\" display=\"inline\"><semantics id=\"A3.T8.7.7.1.m1.1a\"><msub id=\"A3.T8.7.7.1.m1.1.1\" xref=\"A3.T8.7.7.1.m1.1.1.cmml\"><mi id=\"A3.T8.7.7.1.m1.1.1.2\" xref=\"A3.T8.7.7.1.m1.1.1.2.cmml\">η</mi><mi id=\"A3.T8.7.7.1.m1.1.1.3\" xref=\"A3.T8.7.7.1.m1.1.1.3.cmml\">c</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.T8.7.7.1.m1.1b\"><apply id=\"A3.T8.7.7.1.m1.1.1.cmml\" xref=\"A3.T8.7.7.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T8.7.7.1.m1.1.1.1.cmml\" xref=\"A3.T8.7.7.1.m1.1.1\">subscript</csymbol><ci id=\"A3.T8.7.7.1.m1.1.1.2.cmml\" xref=\"A3.T8.7.7.1.m1.1.1.2\">𝜂</ci><ci id=\"A3.T8.7.7.1.m1.1.1.3.cmml\" xref=\"A3.T8.7.7.1.m1.1.1.3\">𝑐</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T8.7.7.1.m1.1c\">\\eta_{c}</annotation></semantics></math>)</td>\n<td id=\"A3.T8.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">N/A</td>\n<td id=\"A3.T8.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T8.8.8.2.m1.4\" class=\"ltx_Math\" alttext=\"\\{10^{-4},10^{-3},\\dots,10^{0}\\}\" display=\"inline\"><semantics id=\"A3.T8.8.8.2.m1.4a\"><mrow id=\"A3.T8.8.8.2.m1.4.4.3\" xref=\"A3.T8.8.8.2.m1.4.4.4.cmml\"><mo stretchy=\"false\" id=\"A3.T8.8.8.2.m1.4.4.3.4\" xref=\"A3.T8.8.8.2.m1.4.4.4.cmml\">{</mo><msup id=\"A3.T8.8.8.2.m1.2.2.1.1\" xref=\"A3.T8.8.8.2.m1.2.2.1.1.cmml\"><mn id=\"A3.T8.8.8.2.m1.2.2.1.1.2\" xref=\"A3.T8.8.8.2.m1.2.2.1.1.2.cmml\">10</mn><mrow id=\"A3.T8.8.8.2.m1.2.2.1.1.3\" xref=\"A3.T8.8.8.2.m1.2.2.1.1.3.cmml\"><mo id=\"A3.T8.8.8.2.m1.2.2.1.1.3a\" xref=\"A3.T8.8.8.2.m1.2.2.1.1.3.cmml\">−</mo><mn id=\"A3.T8.8.8.2.m1.2.2.1.1.3.2\" xref=\"A3.T8.8.8.2.m1.2.2.1.1.3.2.cmml\">4</mn></mrow></msup><mo id=\"A3.T8.8.8.2.m1.4.4.3.5\" xref=\"A3.T8.8.8.2.m1.4.4.4.cmml\">,</mo><msup id=\"A3.T8.8.8.2.m1.3.3.2.2\" xref=\"A3.T8.8.8.2.m1.3.3.2.2.cmml\"><mn id=\"A3.T8.8.8.2.m1.3.3.2.2.2\" xref=\"A3.T8.8.8.2.m1.3.3.2.2.2.cmml\">10</mn><mrow id=\"A3.T8.8.8.2.m1.3.3.2.2.3\" xref=\"A3.T8.8.8.2.m1.3.3.2.2.3.cmml\"><mo id=\"A3.T8.8.8.2.m1.3.3.2.2.3a\" xref=\"A3.T8.8.8.2.m1.3.3.2.2.3.cmml\">−</mo><mn id=\"A3.T8.8.8.2.m1.3.3.2.2.3.2\" xref=\"A3.T8.8.8.2.m1.3.3.2.2.3.2.cmml\">3</mn></mrow></msup><mo id=\"A3.T8.8.8.2.m1.4.4.3.6\" xref=\"A3.T8.8.8.2.m1.4.4.4.cmml\">,</mo><mi mathvariant=\"normal\" id=\"A3.T8.8.8.2.m1.1.1\" xref=\"A3.T8.8.8.2.m1.1.1.cmml\">…</mi><mo id=\"A3.T8.8.8.2.m1.4.4.3.7\" xref=\"A3.T8.8.8.2.m1.4.4.4.cmml\">,</mo><msup id=\"A3.T8.8.8.2.m1.4.4.3.3\" xref=\"A3.T8.8.8.2.m1.4.4.3.3.cmml\"><mn id=\"A3.T8.8.8.2.m1.4.4.3.3.2\" xref=\"A3.T8.8.8.2.m1.4.4.3.3.2.cmml\">10</mn><mn id=\"A3.T8.8.8.2.m1.4.4.3.3.3\" xref=\"A3.T8.8.8.2.m1.4.4.3.3.3.cmml\">0</mn></msup><mo stretchy=\"false\" id=\"A3.T8.8.8.2.m1.4.4.3.8\" xref=\"A3.T8.8.8.2.m1.4.4.4.cmml\">}</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A3.T8.8.8.2.m1.4b\"><set id=\"A3.T8.8.8.2.m1.4.4.4.cmml\" xref=\"A3.T8.8.8.2.m1.4.4.3\"><apply id=\"A3.T8.8.8.2.m1.2.2.1.1.cmml\" xref=\"A3.T8.8.8.2.m1.2.2.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T8.8.8.2.m1.2.2.1.1.1.cmml\" xref=\"A3.T8.8.8.2.m1.2.2.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T8.8.8.2.m1.2.2.1.1.2.cmml\" xref=\"A3.T8.8.8.2.m1.2.2.1.1.2\">10</cn><apply id=\"A3.T8.8.8.2.m1.2.2.1.1.3.cmml\" xref=\"A3.T8.8.8.2.m1.2.2.1.1.3\"><minus id=\"A3.T8.8.8.2.m1.2.2.1.1.3.1.cmml\" xref=\"A3.T8.8.8.2.m1.2.2.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T8.8.8.2.m1.2.2.1.1.3.2.cmml\" xref=\"A3.T8.8.8.2.m1.2.2.1.1.3.2\">4</cn></apply></apply><apply id=\"A3.T8.8.8.2.m1.3.3.2.2.cmml\" xref=\"A3.T8.8.8.2.m1.3.3.2.2\"><csymbol cd=\"ambiguous\" id=\"A3.T8.8.8.2.m1.3.3.2.2.1.cmml\" xref=\"A3.T8.8.8.2.m1.3.3.2.2\">superscript</csymbol><cn type=\"integer\" id=\"A3.T8.8.8.2.m1.3.3.2.2.2.cmml\" xref=\"A3.T8.8.8.2.m1.3.3.2.2.2\">10</cn><apply id=\"A3.T8.8.8.2.m1.3.3.2.2.3.cmml\" xref=\"A3.T8.8.8.2.m1.3.3.2.2.3\"><minus id=\"A3.T8.8.8.2.m1.3.3.2.2.3.1.cmml\" xref=\"A3.T8.8.8.2.m1.3.3.2.2.3\"></minus><cn type=\"integer\" id=\"A3.T8.8.8.2.m1.3.3.2.2.3.2.cmml\" xref=\"A3.T8.8.8.2.m1.3.3.2.2.3.2\">3</cn></apply></apply><ci id=\"A3.T8.8.8.2.m1.1.1.cmml\" xref=\"A3.T8.8.8.2.m1.1.1\">…</ci><apply id=\"A3.T8.8.8.2.m1.4.4.3.3.cmml\" xref=\"A3.T8.8.8.2.m1.4.4.3.3\"><csymbol cd=\"ambiguous\" id=\"A3.T8.8.8.2.m1.4.4.3.3.1.cmml\" xref=\"A3.T8.8.8.2.m1.4.4.3.3\">superscript</csymbol><cn type=\"integer\" id=\"A3.T8.8.8.2.m1.4.4.3.3.2.cmml\" xref=\"A3.T8.8.8.2.m1.4.4.3.3.2\">10</cn><cn type=\"integer\" id=\"A3.T8.8.8.2.m1.4.4.3.3.3.cmml\" xref=\"A3.T8.8.8.2.m1.4.4.3.3.3\">0</cn></apply></set></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T8.8.8.2.m1.4c\">\\{10^{-4},10^{-3},\\dots,10^{0}\\}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "As discussed in ",
                "Section",
                " ",
                "C.3",
                ", for both FedAvg and FedSGD we use a server optimizer of Adam. We tune only the learning rate of this optimizer (the ",
                "server learning rate",
                ", denoted ",
                "η",
                "s",
                "subscript",
                "𝜂",
                "𝑠",
                "\\eta_{s}",
                ") and fix the Adam hyperparameters of ",
                "β",
                "1",
                "=",
                "0.9",
                ",",
                "β",
                "2",
                "=",
                "0.999",
                "formulae-sequence",
                "subscript",
                "𝛽",
                "1",
                "0.9",
                "subscript",
                "𝛽",
                "2",
                "0.999",
                "\\beta_{1}=0.9,\\beta_{2}=0.999",
                " and ",
                "ε",
                "=",
                "10",
                "−",
                "8",
                "𝜀",
                "superscript",
                "10",
                "8",
                "\\varepsilon=10^{-8}",
                ". For FedAvg we also use a client optimizer of SGD. We tune the learning rate of this optimizer (the ",
                "client learning rate",
                ", denoted ",
                "η",
                "c",
                "subscript",
                "𝜂",
                "𝑐",
                "\\eta_{c}",
                "). We tune both learning rates ",
                "η",
                "s",
                ",",
                "η",
                "c",
                "subscript",
                "𝜂",
                "𝑠",
                "subscript",
                "𝜂",
                "𝑐",
                "\\eta_{s},\\eta_{c}",
                " over the range ",
                "{",
                "10",
                "−",
                "4",
                ",",
                "10",
                "−",
                "3",
                ",",
                "…",
                ",",
                "10",
                "0",
                "}",
                "superscript",
                "10",
                "4",
                "superscript",
                "10",
                "3",
                "…",
                "superscript",
                "10",
                "0",
                "\\{10^{-4},10^{-3},\\dots,10^{0}\\}",
                ". We select the learning rates that minimize average training loss across rounds. See ",
                "Table",
                " ",
                "8",
                " for a summary.",
                "In ",
                "Figure",
                " ",
                "4",
                " we also experiment with learning rate scheduling. Note that these are only applied to the server optimizer. The client learning rate (for FedAvg) is held constant throughout an experiment. The learning rate schedule is applied across the 3125 training rounds. We compare constant learning rates to learning rates with (1) exponential decay and (2) cosine decay. For both of these decay schedules, we perform linear warmup (starting at 0) for the first 312 rounds ( ",
                "10",
                "%",
                "percent",
                "10",
                "10\\%",
                " of the total number of rounds). We then decay for the remaining rounds, with a final server learning rate of 0. In such cases, the server learning rate parameter ",
                "η",
                "s",
                "subscript",
                "𝜂",
                "𝑠",
                "\\eta_{s}",
                " refers to the ",
                "maximum",
                " learning rate attained (i.e. at round 312) and is tuned just as above.",
                "The best performing (tuned) learning rates for each algorithm and schedule are given in ",
                "Table",
                " ",
                "9",
                ". Generally, we found that a client learning rate of ",
                "η",
                "c",
                "=",
                "10",
                "−",
                "1",
                "subscript",
                "𝜂",
                "𝑐",
                "superscript",
                "10",
                "1",
                "\\eta_{c}=10^{-1}",
                " worked well throughout. For FedAvg, a server learning rate of ",
                "η",
                "s",
                "=",
                "10",
                "−",
                "3",
                "subscript",
                "𝜂",
                "𝑠",
                "superscript",
                "10",
                "3",
                "\\eta_{s}=10^{-3}",
                " worked well, though we see little to no difference between server learning rate schedules ",
                "Figure",
                " ",
                "4(a)",
                ". For FedSGD, we find that we could only use ",
                "η",
                "s",
                "=",
                "10",
                "−",
                "4",
                "subscript",
                "𝜂",
                "𝑠",
                "superscript",
                "10",
                "4",
                "\\eta_{s}=10^{-4}",
                " for constant learning rates, but learning rate schedules allowed us to use ",
                "η",
                "s",
                "=",
                "10",
                "−",
                "3",
                "subscript",
                "𝜂",
                "𝑠",
                "superscript",
                "10",
                "3",
                "\\eta_{s}=10^{-3}",
                " and led to improved convergence ",
                "Figure",
                " ",
                "4(b)",
                ".",
                "When performing personalization evaluation (",
                "Table",
                " ",
                "5",
                " and ",
                "Figure",
                " ",
                "5",
                "), we use the same client optimizer of SGD, and use the client learning rate that led to the best training performance for FedAvg (i.e. ",
                "η",
                "c",
                "=",
                "10",
                "−",
                "1",
                "subscript",
                "𝜂",
                "𝑐",
                "superscript",
                "10",
                "1",
                "\\eta_{c}=10^{-1}",
                ", as in ",
                "Table",
                " ",
                "9",
                ")."
            ]
        ]
    },
    "A3.T9": {
        "caption": "Table 9: Tuned learning rates for FedAvg and FedSGD, with varying server learning rate schedules.",
        "table": "<table id=\"A3.T9.11\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A3.T9.2.2\" class=\"ltx_tr\">\n<th id=\"A3.T9.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A3.T9.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></th>\n<th id=\"A3.T9.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A3.T9.2.2.4.1\" class=\"ltx_text ltx_font_bold\">Server LR Schedule</span></th>\n<th id=\"A3.T9.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span id=\"A3.T9.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Server LR</span> (<math id=\"A3.T9.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\eta_{s}\" display=\"inline\"><semantics id=\"A3.T9.1.1.1.m1.1a\"><msub id=\"A3.T9.1.1.1.m1.1.1\" xref=\"A3.T9.1.1.1.m1.1.1.cmml\"><mi id=\"A3.T9.1.1.1.m1.1.1.2\" xref=\"A3.T9.1.1.1.m1.1.1.2.cmml\">η</mi><mi id=\"A3.T9.1.1.1.m1.1.1.3\" xref=\"A3.T9.1.1.1.m1.1.1.3.cmml\">s</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.1.1.1.m1.1b\"><apply id=\"A3.T9.1.1.1.m1.1.1.cmml\" xref=\"A3.T9.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.1.1.1.m1.1.1.1.cmml\" xref=\"A3.T9.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"A3.T9.1.1.1.m1.1.1.2.cmml\" xref=\"A3.T9.1.1.1.m1.1.1.2\">𝜂</ci><ci id=\"A3.T9.1.1.1.m1.1.1.3.cmml\" xref=\"A3.T9.1.1.1.m1.1.1.3\">𝑠</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.1.1.1.m1.1c\">\\eta_{s}</annotation></semantics></math>)</th>\n<th id=\"A3.T9.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\">\n<span id=\"A3.T9.2.2.2.1\" class=\"ltx_text ltx_font_bold\">Client LR</span> <math id=\"A3.T9.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"(\\eta_{c})\" display=\"inline\"><semantics id=\"A3.T9.2.2.2.m1.1a\"><mrow id=\"A3.T9.2.2.2.m1.1.1.1\" xref=\"A3.T9.2.2.2.m1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"A3.T9.2.2.2.m1.1.1.1.2\" xref=\"A3.T9.2.2.2.m1.1.1.1.1.cmml\">(</mo><msub id=\"A3.T9.2.2.2.m1.1.1.1.1\" xref=\"A3.T9.2.2.2.m1.1.1.1.1.cmml\"><mi id=\"A3.T9.2.2.2.m1.1.1.1.1.2\" xref=\"A3.T9.2.2.2.m1.1.1.1.1.2.cmml\">η</mi><mi id=\"A3.T9.2.2.2.m1.1.1.1.1.3\" xref=\"A3.T9.2.2.2.m1.1.1.1.1.3.cmml\">c</mi></msub><mo stretchy=\"false\" id=\"A3.T9.2.2.2.m1.1.1.1.3\" xref=\"A3.T9.2.2.2.m1.1.1.1.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.2.2.2.m1.1b\"><apply id=\"A3.T9.2.2.2.m1.1.1.1.1.cmml\" xref=\"A3.T9.2.2.2.m1.1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.2.2.2.m1.1.1.1.1.1.cmml\" xref=\"A3.T9.2.2.2.m1.1.1.1\">subscript</csymbol><ci id=\"A3.T9.2.2.2.m1.1.1.1.1.2.cmml\" xref=\"A3.T9.2.2.2.m1.1.1.1.1.2\">𝜂</ci><ci id=\"A3.T9.2.2.2.m1.1.1.1.1.3.cmml\" xref=\"A3.T9.2.2.2.m1.1.1.1.1.3\">𝑐</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.2.2.2.m1.1c\">(\\eta_{c})</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T9.4.4\" class=\"ltx_tr\">\n<td id=\"A3.T9.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"3\"><span id=\"A3.T9.4.4.3.1\" class=\"ltx_text\">FedAvg</span></td>\n<td id=\"A3.T9.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Constant</td>\n<td id=\"A3.T9.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T9.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"10^{-3}\" display=\"inline\"><semantics id=\"A3.T9.3.3.1.m1.1a\"><msup id=\"A3.T9.3.3.1.m1.1.1\" xref=\"A3.T9.3.3.1.m1.1.1.cmml\"><mn id=\"A3.T9.3.3.1.m1.1.1.2\" xref=\"A3.T9.3.3.1.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T9.3.3.1.m1.1.1.3\" xref=\"A3.T9.3.3.1.m1.1.1.3.cmml\"><mo id=\"A3.T9.3.3.1.m1.1.1.3a\" xref=\"A3.T9.3.3.1.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T9.3.3.1.m1.1.1.3.2\" xref=\"A3.T9.3.3.1.m1.1.1.3.2.cmml\">3</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.3.3.1.m1.1b\"><apply id=\"A3.T9.3.3.1.m1.1.1.cmml\" xref=\"A3.T9.3.3.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.3.3.1.m1.1.1.1.cmml\" xref=\"A3.T9.3.3.1.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T9.3.3.1.m1.1.1.2.cmml\" xref=\"A3.T9.3.3.1.m1.1.1.2\">10</cn><apply id=\"A3.T9.3.3.1.m1.1.1.3.cmml\" xref=\"A3.T9.3.3.1.m1.1.1.3\"><minus id=\"A3.T9.3.3.1.m1.1.1.3.1.cmml\" xref=\"A3.T9.3.3.1.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T9.3.3.1.m1.1.1.3.2.cmml\" xref=\"A3.T9.3.3.1.m1.1.1.3.2\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.3.3.1.m1.1c\">10^{-3}</annotation></semantics></math></td>\n<td id=\"A3.T9.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T9.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"10^{-1}\" display=\"inline\"><semantics id=\"A3.T9.4.4.2.m1.1a\"><msup id=\"A3.T9.4.4.2.m1.1.1\" xref=\"A3.T9.4.4.2.m1.1.1.cmml\"><mn id=\"A3.T9.4.4.2.m1.1.1.2\" xref=\"A3.T9.4.4.2.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T9.4.4.2.m1.1.1.3\" xref=\"A3.T9.4.4.2.m1.1.1.3.cmml\"><mo id=\"A3.T9.4.4.2.m1.1.1.3a\" xref=\"A3.T9.4.4.2.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T9.4.4.2.m1.1.1.3.2\" xref=\"A3.T9.4.4.2.m1.1.1.3.2.cmml\">1</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.4.4.2.m1.1b\"><apply id=\"A3.T9.4.4.2.m1.1.1.cmml\" xref=\"A3.T9.4.4.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.4.4.2.m1.1.1.1.cmml\" xref=\"A3.T9.4.4.2.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T9.4.4.2.m1.1.1.2.cmml\" xref=\"A3.T9.4.4.2.m1.1.1.2\">10</cn><apply id=\"A3.T9.4.4.2.m1.1.1.3.cmml\" xref=\"A3.T9.4.4.2.m1.1.1.3\"><minus id=\"A3.T9.4.4.2.m1.1.1.3.1.cmml\" xref=\"A3.T9.4.4.2.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T9.4.4.2.m1.1.1.3.2.cmml\" xref=\"A3.T9.4.4.2.m1.1.1.3.2\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.4.4.2.m1.1c\">10^{-1}</annotation></semantics></math></td>\n</tr>\n<tr id=\"A3.T9.6.6\" class=\"ltx_tr\">\n<td id=\"A3.T9.6.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Warmup + Exponential Decay</td>\n<td id=\"A3.T9.5.5.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T9.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"10^{-3}\" display=\"inline\"><semantics id=\"A3.T9.5.5.1.m1.1a\"><msup id=\"A3.T9.5.5.1.m1.1.1\" xref=\"A3.T9.5.5.1.m1.1.1.cmml\"><mn id=\"A3.T9.5.5.1.m1.1.1.2\" xref=\"A3.T9.5.5.1.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T9.5.5.1.m1.1.1.3\" xref=\"A3.T9.5.5.1.m1.1.1.3.cmml\"><mo id=\"A3.T9.5.5.1.m1.1.1.3a\" xref=\"A3.T9.5.5.1.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T9.5.5.1.m1.1.1.3.2\" xref=\"A3.T9.5.5.1.m1.1.1.3.2.cmml\">3</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.5.5.1.m1.1b\"><apply id=\"A3.T9.5.5.1.m1.1.1.cmml\" xref=\"A3.T9.5.5.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.5.5.1.m1.1.1.1.cmml\" xref=\"A3.T9.5.5.1.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T9.5.5.1.m1.1.1.2.cmml\" xref=\"A3.T9.5.5.1.m1.1.1.2\">10</cn><apply id=\"A3.T9.5.5.1.m1.1.1.3.cmml\" xref=\"A3.T9.5.5.1.m1.1.1.3\"><minus id=\"A3.T9.5.5.1.m1.1.1.3.1.cmml\" xref=\"A3.T9.5.5.1.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T9.5.5.1.m1.1.1.3.2.cmml\" xref=\"A3.T9.5.5.1.m1.1.1.3.2\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.5.5.1.m1.1c\">10^{-3}</annotation></semantics></math></td>\n<td id=\"A3.T9.6.6.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T9.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"10^{-1}\" display=\"inline\"><semantics id=\"A3.T9.6.6.2.m1.1a\"><msup id=\"A3.T9.6.6.2.m1.1.1\" xref=\"A3.T9.6.6.2.m1.1.1.cmml\"><mn id=\"A3.T9.6.6.2.m1.1.1.2\" xref=\"A3.T9.6.6.2.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T9.6.6.2.m1.1.1.3\" xref=\"A3.T9.6.6.2.m1.1.1.3.cmml\"><mo id=\"A3.T9.6.6.2.m1.1.1.3a\" xref=\"A3.T9.6.6.2.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T9.6.6.2.m1.1.1.3.2\" xref=\"A3.T9.6.6.2.m1.1.1.3.2.cmml\">1</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.6.6.2.m1.1b\"><apply id=\"A3.T9.6.6.2.m1.1.1.cmml\" xref=\"A3.T9.6.6.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.6.6.2.m1.1.1.1.cmml\" xref=\"A3.T9.6.6.2.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T9.6.6.2.m1.1.1.2.cmml\" xref=\"A3.T9.6.6.2.m1.1.1.2\">10</cn><apply id=\"A3.T9.6.6.2.m1.1.1.3.cmml\" xref=\"A3.T9.6.6.2.m1.1.1.3\"><minus id=\"A3.T9.6.6.2.m1.1.1.3.1.cmml\" xref=\"A3.T9.6.6.2.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T9.6.6.2.m1.1.1.3.2.cmml\" xref=\"A3.T9.6.6.2.m1.1.1.3.2\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.6.6.2.m1.1c\">10^{-1}</annotation></semantics></math></td>\n</tr>\n<tr id=\"A3.T9.8.8\" class=\"ltx_tr\">\n<td id=\"A3.T9.8.8.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Warmup + Cosine Decay</td>\n<td id=\"A3.T9.7.7.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T9.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"10^{-3}\" display=\"inline\"><semantics id=\"A3.T9.7.7.1.m1.1a\"><msup id=\"A3.T9.7.7.1.m1.1.1\" xref=\"A3.T9.7.7.1.m1.1.1.cmml\"><mn id=\"A3.T9.7.7.1.m1.1.1.2\" xref=\"A3.T9.7.7.1.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T9.7.7.1.m1.1.1.3\" xref=\"A3.T9.7.7.1.m1.1.1.3.cmml\"><mo id=\"A3.T9.7.7.1.m1.1.1.3a\" xref=\"A3.T9.7.7.1.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T9.7.7.1.m1.1.1.3.2\" xref=\"A3.T9.7.7.1.m1.1.1.3.2.cmml\">3</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.7.7.1.m1.1b\"><apply id=\"A3.T9.7.7.1.m1.1.1.cmml\" xref=\"A3.T9.7.7.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.7.7.1.m1.1.1.1.cmml\" xref=\"A3.T9.7.7.1.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T9.7.7.1.m1.1.1.2.cmml\" xref=\"A3.T9.7.7.1.m1.1.1.2\">10</cn><apply id=\"A3.T9.7.7.1.m1.1.1.3.cmml\" xref=\"A3.T9.7.7.1.m1.1.1.3\"><minus id=\"A3.T9.7.7.1.m1.1.1.3.1.cmml\" xref=\"A3.T9.7.7.1.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T9.7.7.1.m1.1.1.3.2.cmml\" xref=\"A3.T9.7.7.1.m1.1.1.3.2\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.7.7.1.m1.1c\">10^{-3}</annotation></semantics></math></td>\n<td id=\"A3.T9.8.8.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T9.8.8.2.m1.1\" class=\"ltx_Math\" alttext=\"10^{-1}\" display=\"inline\"><semantics id=\"A3.T9.8.8.2.m1.1a\"><msup id=\"A3.T9.8.8.2.m1.1.1\" xref=\"A3.T9.8.8.2.m1.1.1.cmml\"><mn id=\"A3.T9.8.8.2.m1.1.1.2\" xref=\"A3.T9.8.8.2.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T9.8.8.2.m1.1.1.3\" xref=\"A3.T9.8.8.2.m1.1.1.3.cmml\"><mo id=\"A3.T9.8.8.2.m1.1.1.3a\" xref=\"A3.T9.8.8.2.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T9.8.8.2.m1.1.1.3.2\" xref=\"A3.T9.8.8.2.m1.1.1.3.2.cmml\">1</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.8.8.2.m1.1b\"><apply id=\"A3.T9.8.8.2.m1.1.1.cmml\" xref=\"A3.T9.8.8.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.8.8.2.m1.1.1.1.cmml\" xref=\"A3.T9.8.8.2.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T9.8.8.2.m1.1.1.2.cmml\" xref=\"A3.T9.8.8.2.m1.1.1.2\">10</cn><apply id=\"A3.T9.8.8.2.m1.1.1.3.cmml\" xref=\"A3.T9.8.8.2.m1.1.1.3\"><minus id=\"A3.T9.8.8.2.m1.1.1.3.1.cmml\" xref=\"A3.T9.8.8.2.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T9.8.8.2.m1.1.1.3.2.cmml\" xref=\"A3.T9.8.8.2.m1.1.1.3.2\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.8.8.2.m1.1c\">10^{-1}</annotation></semantics></math></td>\n</tr>\n<tr id=\"A3.T9.9.9\" class=\"ltx_tr\">\n<td id=\"A3.T9.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"3\"><span id=\"A3.T9.9.9.2.1\" class=\"ltx_text\">FedSGD</span></td>\n<td id=\"A3.T9.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Constant</td>\n<td id=\"A3.T9.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T9.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"10^{-4}\" display=\"inline\"><semantics id=\"A3.T9.9.9.1.m1.1a\"><msup id=\"A3.T9.9.9.1.m1.1.1\" xref=\"A3.T9.9.9.1.m1.1.1.cmml\"><mn id=\"A3.T9.9.9.1.m1.1.1.2\" xref=\"A3.T9.9.9.1.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T9.9.9.1.m1.1.1.3\" xref=\"A3.T9.9.9.1.m1.1.1.3.cmml\"><mo id=\"A3.T9.9.9.1.m1.1.1.3a\" xref=\"A3.T9.9.9.1.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T9.9.9.1.m1.1.1.3.2\" xref=\"A3.T9.9.9.1.m1.1.1.3.2.cmml\">4</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.9.9.1.m1.1b\"><apply id=\"A3.T9.9.9.1.m1.1.1.cmml\" xref=\"A3.T9.9.9.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.9.9.1.m1.1.1.1.cmml\" xref=\"A3.T9.9.9.1.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T9.9.9.1.m1.1.1.2.cmml\" xref=\"A3.T9.9.9.1.m1.1.1.2\">10</cn><apply id=\"A3.T9.9.9.1.m1.1.1.3.cmml\" xref=\"A3.T9.9.9.1.m1.1.1.3\"><minus id=\"A3.T9.9.9.1.m1.1.1.3.1.cmml\" xref=\"A3.T9.9.9.1.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T9.9.9.1.m1.1.1.3.2.cmml\" xref=\"A3.T9.9.9.1.m1.1.1.3.2\">4</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.9.9.1.m1.1c\">10^{-4}</annotation></semantics></math></td>\n<td id=\"A3.T9.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">N/A</td>\n</tr>\n<tr id=\"A3.T9.10.10\" class=\"ltx_tr\">\n<td id=\"A3.T9.10.10.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Warmup + Exponential Decay</td>\n<td id=\"A3.T9.10.10.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T9.10.10.1.m1.1\" class=\"ltx_Math\" alttext=\"10^{-3}\" display=\"inline\"><semantics id=\"A3.T9.10.10.1.m1.1a\"><msup id=\"A3.T9.10.10.1.m1.1.1\" xref=\"A3.T9.10.10.1.m1.1.1.cmml\"><mn id=\"A3.T9.10.10.1.m1.1.1.2\" xref=\"A3.T9.10.10.1.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T9.10.10.1.m1.1.1.3\" xref=\"A3.T9.10.10.1.m1.1.1.3.cmml\"><mo id=\"A3.T9.10.10.1.m1.1.1.3a\" xref=\"A3.T9.10.10.1.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T9.10.10.1.m1.1.1.3.2\" xref=\"A3.T9.10.10.1.m1.1.1.3.2.cmml\">3</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.10.10.1.m1.1b\"><apply id=\"A3.T9.10.10.1.m1.1.1.cmml\" xref=\"A3.T9.10.10.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.10.10.1.m1.1.1.1.cmml\" xref=\"A3.T9.10.10.1.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T9.10.10.1.m1.1.1.2.cmml\" xref=\"A3.T9.10.10.1.m1.1.1.2\">10</cn><apply id=\"A3.T9.10.10.1.m1.1.1.3.cmml\" xref=\"A3.T9.10.10.1.m1.1.1.3\"><minus id=\"A3.T9.10.10.1.m1.1.1.3.1.cmml\" xref=\"A3.T9.10.10.1.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T9.10.10.1.m1.1.1.3.2.cmml\" xref=\"A3.T9.10.10.1.m1.1.1.3.2\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.10.10.1.m1.1c\">10^{-3}</annotation></semantics></math></td>\n<td id=\"A3.T9.10.10.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">N/A</td>\n</tr>\n<tr id=\"A3.T9.11.11\" class=\"ltx_tr\">\n<td id=\"A3.T9.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">Warmup + Cosine Decay</td>\n<td id=\"A3.T9.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A3.T9.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"10^{-3}\" display=\"inline\"><semantics id=\"A3.T9.11.11.1.m1.1a\"><msup id=\"A3.T9.11.11.1.m1.1.1\" xref=\"A3.T9.11.11.1.m1.1.1.cmml\"><mn id=\"A3.T9.11.11.1.m1.1.1.2\" xref=\"A3.T9.11.11.1.m1.1.1.2.cmml\">10</mn><mrow id=\"A3.T9.11.11.1.m1.1.1.3\" xref=\"A3.T9.11.11.1.m1.1.1.3.cmml\"><mo id=\"A3.T9.11.11.1.m1.1.1.3a\" xref=\"A3.T9.11.11.1.m1.1.1.3.cmml\">−</mo><mn id=\"A3.T9.11.11.1.m1.1.1.3.2\" xref=\"A3.T9.11.11.1.m1.1.1.3.2.cmml\">3</mn></mrow></msup><annotation-xml encoding=\"MathML-Content\" id=\"A3.T9.11.11.1.m1.1b\"><apply id=\"A3.T9.11.11.1.m1.1.1.cmml\" xref=\"A3.T9.11.11.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A3.T9.11.11.1.m1.1.1.1.cmml\" xref=\"A3.T9.11.11.1.m1.1.1\">superscript</csymbol><cn type=\"integer\" id=\"A3.T9.11.11.1.m1.1.1.2.cmml\" xref=\"A3.T9.11.11.1.m1.1.1.2\">10</cn><apply id=\"A3.T9.11.11.1.m1.1.1.3.cmml\" xref=\"A3.T9.11.11.1.m1.1.1.3\"><minus id=\"A3.T9.11.11.1.m1.1.1.3.1.cmml\" xref=\"A3.T9.11.11.1.m1.1.1.3\"></minus><cn type=\"integer\" id=\"A3.T9.11.11.1.m1.1.1.3.2.cmml\" xref=\"A3.T9.11.11.1.m1.1.1.3.2\">3</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A3.T9.11.11.1.m1.1c\">10^{-3}</annotation></semantics></math></td>\n<td id=\"A3.T9.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">N/A</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "As discussed in ",
                "Section",
                " ",
                "C.3",
                ", for both FedAvg and FedSGD we use a server optimizer of Adam. We tune only the learning rate of this optimizer (the ",
                "server learning rate",
                ", denoted ",
                "η",
                "s",
                "subscript",
                "𝜂",
                "𝑠",
                "\\eta_{s}",
                ") and fix the Adam hyperparameters of ",
                "β",
                "1",
                "=",
                "0.9",
                ",",
                "β",
                "2",
                "=",
                "0.999",
                "formulae-sequence",
                "subscript",
                "𝛽",
                "1",
                "0.9",
                "subscript",
                "𝛽",
                "2",
                "0.999",
                "\\beta_{1}=0.9,\\beta_{2}=0.999",
                " and ",
                "ε",
                "=",
                "10",
                "−",
                "8",
                "𝜀",
                "superscript",
                "10",
                "8",
                "\\varepsilon=10^{-8}",
                ". For FedAvg we also use a client optimizer of SGD. We tune the learning rate of this optimizer (the ",
                "client learning rate",
                ", denoted ",
                "η",
                "c",
                "subscript",
                "𝜂",
                "𝑐",
                "\\eta_{c}",
                "). We tune both learning rates ",
                "η",
                "s",
                ",",
                "η",
                "c",
                "subscript",
                "𝜂",
                "𝑠",
                "subscript",
                "𝜂",
                "𝑐",
                "\\eta_{s},\\eta_{c}",
                " over the range ",
                "{",
                "10",
                "−",
                "4",
                ",",
                "10",
                "−",
                "3",
                ",",
                "…",
                ",",
                "10",
                "0",
                "}",
                "superscript",
                "10",
                "4",
                "superscript",
                "10",
                "3",
                "…",
                "superscript",
                "10",
                "0",
                "\\{10^{-4},10^{-3},\\dots,10^{0}\\}",
                ". We select the learning rates that minimize average training loss across rounds. See ",
                "Table",
                " ",
                "8",
                " for a summary.",
                "In ",
                "Figure",
                " ",
                "4",
                " we also experiment with learning rate scheduling. Note that these are only applied to the server optimizer. The client learning rate (for FedAvg) is held constant throughout an experiment. The learning rate schedule is applied across the 3125 training rounds. We compare constant learning rates to learning rates with (1) exponential decay and (2) cosine decay. For both of these decay schedules, we perform linear warmup (starting at 0) for the first 312 rounds ( ",
                "10",
                "%",
                "percent",
                "10",
                "10\\%",
                " of the total number of rounds). We then decay for the remaining rounds, with a final server learning rate of 0. In such cases, the server learning rate parameter ",
                "η",
                "s",
                "subscript",
                "𝜂",
                "𝑠",
                "\\eta_{s}",
                " refers to the ",
                "maximum",
                " learning rate attained (i.e. at round 312) and is tuned just as above.",
                "The best performing (tuned) learning rates for each algorithm and schedule are given in ",
                "Table",
                " ",
                "9",
                ". Generally, we found that a client learning rate of ",
                "η",
                "c",
                "=",
                "10",
                "−",
                "1",
                "subscript",
                "𝜂",
                "𝑐",
                "superscript",
                "10",
                "1",
                "\\eta_{c}=10^{-1}",
                " worked well throughout. For FedAvg, a server learning rate of ",
                "η",
                "s",
                "=",
                "10",
                "−",
                "3",
                "subscript",
                "𝜂",
                "𝑠",
                "superscript",
                "10",
                "3",
                "\\eta_{s}=10^{-3}",
                " worked well, though we see little to no difference between server learning rate schedules ",
                "Figure",
                " ",
                "4(a)",
                ". For FedSGD, we find that we could only use ",
                "η",
                "s",
                "=",
                "10",
                "−",
                "4",
                "subscript",
                "𝜂",
                "𝑠",
                "superscript",
                "10",
                "4",
                "\\eta_{s}=10^{-4}",
                " for constant learning rates, but learning rate schedules allowed us to use ",
                "η",
                "s",
                "=",
                "10",
                "−",
                "3",
                "subscript",
                "𝜂",
                "𝑠",
                "superscript",
                "10",
                "3",
                "\\eta_{s}=10^{-3}",
                " and led to improved convergence ",
                "Figure",
                " ",
                "4(b)",
                ".",
                "When performing personalization evaluation (",
                "Table",
                " ",
                "5",
                " and ",
                "Figure",
                " ",
                "5",
                "), we use the same client optimizer of SGD, and use the client learning rate that led to the best training performance for FedAvg (i.e. ",
                "η",
                "c",
                "=",
                "10",
                "−",
                "1",
                "subscript",
                "𝜂",
                "𝑐",
                "superscript",
                "10",
                "1",
                "\\eta_{c}=10^{-1}",
                ", as in ",
                "Table",
                " ",
                "9",
                ")."
            ]
        ]
    },
    "A4.T10": {
        "caption": "Table 10: Median pre-personalization and post-personalization loss after training with FedAvg and FedSGD, with different numbers of batches per client, keeping the total number of communication rounds constant.\n",
        "table": "<table id=\"A4.T10.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A4.T10.1.1\" class=\"ltx_tr\">\n<th id=\"A4.T10.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A4.T10.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></th>\n<th id=\"A4.T10.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A4.T10.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Loss</span></th>\n<th id=\"A4.T10.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\" colspan=\"4\">\n<span id=\"A4.T10.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Batches per Client</span> <math id=\"A4.T10.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"(\\tau)\" display=\"inline\"><semantics id=\"A4.T10.1.1.1.m1.1a\"><mrow id=\"A4.T10.1.1.1.m1.1.2.2\"><mo stretchy=\"false\" id=\"A4.T10.1.1.1.m1.1.2.2.1\">(</mo><mi id=\"A4.T10.1.1.1.m1.1.1\" xref=\"A4.T10.1.1.1.m1.1.1.cmml\">τ</mi><mo stretchy=\"false\" id=\"A4.T10.1.1.1.m1.1.2.2.2\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A4.T10.1.1.1.m1.1b\"><ci id=\"A4.T10.1.1.1.m1.1.1.cmml\" xref=\"A4.T10.1.1.1.m1.1.1\">𝜏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T10.1.1.1.m1.1c\">(\\tau)</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A4.T10.5.5\" class=\"ltx_tr\">\n<td id=\"A4.T10.5.5.5\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<th id=\"A4.T10.5.5.6\" class=\"ltx_td ltx_th ltx_th_column\" style=\"padding-top:1pt;padding-bottom:1pt;\"></th>\n<th id=\"A4.T10.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A4.T10.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics id=\"A4.T10.2.2.1.m1.1a\"><mn id=\"A4.T10.2.2.1.m1.1.1\" xref=\"A4.T10.2.2.1.m1.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T10.2.2.1.m1.1b\"><cn type=\"integer\" id=\"A4.T10.2.2.1.m1.1.1.cmml\" xref=\"A4.T10.2.2.1.m1.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T10.2.2.1.m1.1c\">1</annotation></semantics></math></th>\n<th id=\"A4.T10.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A4.T10.3.3.2.m1.1\" class=\"ltx_Math\" alttext=\"4\" display=\"inline\"><semantics id=\"A4.T10.3.3.2.m1.1a\"><mn id=\"A4.T10.3.3.2.m1.1.1\" xref=\"A4.T10.3.3.2.m1.1.1.cmml\">4</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T10.3.3.2.m1.1b\"><cn type=\"integer\" id=\"A4.T10.3.3.2.m1.1.1.cmml\" xref=\"A4.T10.3.3.2.m1.1.1\">4</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T10.3.3.2.m1.1c\">4</annotation></semantics></math></th>\n<th id=\"A4.T10.4.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A4.T10.4.4.3.m1.1\" class=\"ltx_Math\" alttext=\"16\" display=\"inline\"><semantics id=\"A4.T10.4.4.3.m1.1a\"><mn id=\"A4.T10.4.4.3.m1.1.1\" xref=\"A4.T10.4.4.3.m1.1.1.cmml\">16</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T10.4.4.3.m1.1b\"><cn type=\"integer\" id=\"A4.T10.4.4.3.m1.1.1.cmml\" xref=\"A4.T10.4.4.3.m1.1.1\">16</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T10.4.4.3.m1.1c\">16</annotation></semantics></math></th>\n<th id=\"A4.T10.5.5.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A4.T10.5.5.4.m1.1\" class=\"ltx_Math\" alttext=\"64\" display=\"inline\"><semantics id=\"A4.T10.5.5.4.m1.1a\"><mn id=\"A4.T10.5.5.4.m1.1.1\" xref=\"A4.T10.5.5.4.m1.1.1.cmml\">64</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T10.5.5.4.m1.1b\"><cn type=\"integer\" id=\"A4.T10.5.5.4.m1.1.1.cmml\" xref=\"A4.T10.5.5.4.m1.1.1\">64</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T10.5.5.4.m1.1c\">64</annotation></semantics></math></th>\n</tr>\n<tr id=\"A4.T10.5.6.1\" class=\"ltx_tr\">\n<td id=\"A4.T10.5.6.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"2\"><span id=\"A4.T10.5.6.1.1.1\" class=\"ltx_text\">FedAvg</span></td>\n<td id=\"A4.T10.5.6.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Pre-Personalization</td>\n<td id=\"A4.T10.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.4</td>\n<td id=\"A4.T10.5.6.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.2</td>\n<td id=\"A4.T10.5.6.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.8</td>\n<td id=\"A4.T10.5.6.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">5.2</td>\n</tr>\n<tr id=\"A4.T10.5.7.2\" class=\"ltx_tr\">\n<td id=\"A4.T10.5.7.2.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Post-Personalization</td>\n<td id=\"A4.T10.5.7.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.5</td>\n<td id=\"A4.T10.5.7.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">1.9</td>\n<td id=\"A4.T10.5.7.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.009</td>\n<td id=\"A4.T10.5.7.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.008</td>\n</tr>\n<tr id=\"A4.T10.5.8.3\" class=\"ltx_tr\">\n<td id=\"A4.T10.5.8.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"2\"><span id=\"A4.T10.5.8.3.1.1\" class=\"ltx_text\">FedSGD</span></td>\n<td id=\"A4.T10.5.8.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Pre-Personalization</td>\n<td id=\"A4.T10.5.8.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.5</td>\n<td id=\"A4.T10.5.8.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.4</td>\n<td id=\"A4.T10.5.8.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.4</td>\n<td id=\"A4.T10.5.8.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.2</td>\n</tr>\n<tr id=\"A4.T10.5.9.4\" class=\"ltx_tr\">\n<td id=\"A4.T10.5.9.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">Post-Personalization</td>\n<td id=\"A4.T10.5.9.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.6</td>\n<td id=\"A4.T10.5.9.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.4</td>\n<td id=\"A4.T10.5.9.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.4</td>\n<td id=\"A4.T10.5.9.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.3</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To better explore the phenomena discussed in ",
                "Section",
                " ",
                "5",
                ", we perform a modified experiment where we repeat the training from ",
                "Section",
                " ",
                "5",
                ", but vary the number of batches ",
                "τ",
                "𝜏",
                "\\tau",
                " each training client yields. As above, we repeat and truncate clients’ datasets so that each client has exactly 1024 examples. For example, when ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                ", and we use a batch size of 16, this means that for FedAvg and FedSGD, each client computes ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                " mini-batch gradients. For FedAvg, this means that the client does ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                " steps of training, while for FedSGD, this means that the client sends the average of these ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                " batches back to the server. We vary the number of batches ",
                "τ",
                "𝜏",
                "\\tau",
                " over ",
                "{",
                "1",
                ",",
                "4",
                ",",
                "16",
                ",",
                "64",
                "}",
                "1",
                "4",
                "16",
                "64",
                "\\{1,4,16,64\\}",
                ". We do so by repeating and truncating clients’ data so that they have 16, 64, 256, and 1024 examples, respectively. We then perform different amounts of training, one in which we equalize the number of communication rounds, and one in which we equalize the total number of tokens seen across all clients.",
                "Note that fixing the number of examples per client at 1024 and simply changing the batch size would be a more elegant way to do this kind of ablation, as it would allow normalizing the number of communication rounds and tokens simultaneously. Unfortunately, the batch size is often dictated by compute constraints (e.g. what fits in the memory of a given hardware device), and cannot be increased in an unbounded fashion in realistic machine learning settings, especially on-device settings which are common in FL. Thus, we instead focus on varying ",
                "τ",
                "𝜏",
                "\\tau",
                ".",
                "Equalizing communication rounds.",
                " We do 5000 rounds of training for each number of batches per client ",
                "τ",
                "𝜏",
                "\\tau",
                ", using the same warmup and cosine decay schedule discussed above. Throughout training, we perform personalization evaluation on the models. We give the pre-personalization results in ",
                "Figure",
                " ",
                "14",
                ", and the post-personalization results in ",
                "Figure",
                " ",
                "15",
                ". For more detailed numerical results, see ",
                "Section",
                " ",
                "D.2",
                ".",
                "We see that for FedSGD, the pre- and post-personalization losses do not change much with the number of batches per client ",
                "τ",
                "𝜏",
                "\\tau",
                ". For FedAvg, on the other hand, lower values of ",
                "τ",
                "𝜏",
                "\\tau",
                " attain better pre-personalization loss, while higher values achieve better post-personalization loss. Note that when the number of batches per client is ",
                "τ",
                "=",
                "1",
                "𝜏",
                "1",
                "\\tau=1",
                ", FedAvg and FedSGD are effectively the same algorithm (up to differences in normalization), and perform only a single update step per client each round.\nNotably, ",
                "FedAvg attains a better trade-off between the pre- and post-personalization metrics",
                ".\nSpecifically, FedAvg with ",
                "τ",
                "=",
                "4",
                "𝜏",
                "4",
                "\\tau=4",
                " can match the best pre-personalization performance of FedSGD (FedAvg with ",
                "τ",
                "=",
                "4",
                "𝜏",
                "4",
                "\\tau=4",
                " and FedSGD with ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                " both attain a median pre-personalization loss of ",
                "4.2",
                "4.2",
                "4.2",
                "),\nwhile performing significantly better on the post-personalization metrics (a median loss of ",
                "1.9",
                "1.9",
                "1.9",
                " for FedAvg with ",
                "τ",
                "=",
                "4",
                "𝜏",
                "4",
                "\\tau=4",
                " versus ",
                "3.4",
                "3.4",
                "3.4",
                " for FedSGD with ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                ").\nIn short, these results seem to suggest that “client drift” ",
                "[",
                "28",
                "]",
                " is less of an impediment to federated learning, and more indicative of a trade-off between minimizing pre- and post-personalization loss functions.",
                "Equalizing tokens.",
                " Next, we perform an analogous experiment, but where each setting of ",
                "τ",
                "𝜏",
                "\\tau",
                " (the number of batches per client), is trained for a different number of rounds, so that in each setting the same number of tokens is processed (over all clients). For example, ",
                "τ",
                "=",
                "1",
                "𝜏",
                "1",
                "\\tau=1",
                " trains for ",
                "64",
                "×",
                "64\\times",
                " more communication rounds than for ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                ". Each training run processes roughly 10.5 billion tokens in total. We give the pre-personalization results in ",
                "Figure",
                " ",
                "16",
                ", and the post-personalization results in ",
                "Figure",
                " ",
                "17",
                ". For more detailed numerical results, see ",
                "Section",
                " ",
                "D.2",
                ". Notably, the post-personalization loss diverged at intermediate stages for ",
                "τ",
                "=",
                "1",
                "𝜏",
                "1",
                "\\tau=1",
                ", but recovered (albeit with high variance across clients) afterwards.",
                "Here, we see a notably different story than in Figures ",
                "14",
                " and ",
                "15",
                ". In particular, we see that for both FedAvg and FedSGD, lower values of ",
                "τ",
                "𝜏",
                "\\tau",
                " lead to lower pre-personalization loss. However, for both algorithms as long as ",
                "τ",
                "𝜏",
                "\\tau",
                " is sufficiently large (at least 4 in this case), the post-personalization loss essentially does not change with ",
                "τ",
                "𝜏",
                "\\tau",
                ". This suggests that when communication is not a bottleneck, we can attain good pre- and post-personalization loss by using FedAvg with a small (but not too small) value of ",
                "τ",
                "𝜏",
                "\\tau",
                ". In other words, by performing enough rounds of FedAvg with a moderate ",
                "τ",
                "𝜏",
                "\\tau",
                ", we can attain a model that does well before and after personalization."
            ]
        ]
    },
    "A4.T11": {
        "caption": "Table 11: Median pre-personalization and post-personalization loss after training with FedAvg and FedSGD, with different numbers of batches per client, keeping the total number of tokens processed constant.\n",
        "table": "<table id=\"A4.T11.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A4.T11.1.1\" class=\"ltx_tr\">\n<th id=\"A4.T11.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A4.T11.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Algorithm</span></th>\n<th id=\"A4.T11.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A4.T11.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Loss</span></th>\n<th id=\"A4.T11.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\" colspan=\"4\">\n<span id=\"A4.T11.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Batches per Client</span> <math id=\"A4.T11.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"(\\tau)\" display=\"inline\"><semantics id=\"A4.T11.1.1.1.m1.1a\"><mrow id=\"A4.T11.1.1.1.m1.1.2.2\"><mo stretchy=\"false\" id=\"A4.T11.1.1.1.m1.1.2.2.1\">(</mo><mi id=\"A4.T11.1.1.1.m1.1.1\" xref=\"A4.T11.1.1.1.m1.1.1.cmml\">τ</mi><mo stretchy=\"false\" id=\"A4.T11.1.1.1.m1.1.2.2.2\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"A4.T11.1.1.1.m1.1b\"><ci id=\"A4.T11.1.1.1.m1.1.1.cmml\" xref=\"A4.T11.1.1.1.m1.1.1\">𝜏</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T11.1.1.1.m1.1c\">(\\tau)</annotation></semantics></math>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A4.T11.5.5\" class=\"ltx_tr\">\n<td id=\"A4.T11.5.5.5\" class=\"ltx_td\" style=\"padding-top:1pt;padding-bottom:1pt;\"></td>\n<th id=\"A4.T11.5.5.6\" class=\"ltx_td ltx_th ltx_th_column\" style=\"padding-top:1pt;padding-bottom:1pt;\"></th>\n<th id=\"A4.T11.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A4.T11.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"1\" display=\"inline\"><semantics id=\"A4.T11.2.2.1.m1.1a\"><mn id=\"A4.T11.2.2.1.m1.1.1\" xref=\"A4.T11.2.2.1.m1.1.1.cmml\">1</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T11.2.2.1.m1.1b\"><cn type=\"integer\" id=\"A4.T11.2.2.1.m1.1.1.cmml\" xref=\"A4.T11.2.2.1.m1.1.1\">1</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T11.2.2.1.m1.1c\">1</annotation></semantics></math></th>\n<th id=\"A4.T11.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A4.T11.3.3.2.m1.1\" class=\"ltx_Math\" alttext=\"4\" display=\"inline\"><semantics id=\"A4.T11.3.3.2.m1.1a\"><mn id=\"A4.T11.3.3.2.m1.1.1\" xref=\"A4.T11.3.3.2.m1.1.1.cmml\">4</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T11.3.3.2.m1.1b\"><cn type=\"integer\" id=\"A4.T11.3.3.2.m1.1.1.cmml\" xref=\"A4.T11.3.3.2.m1.1.1\">4</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T11.3.3.2.m1.1c\">4</annotation></semantics></math></th>\n<th id=\"A4.T11.4.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A4.T11.4.4.3.m1.1\" class=\"ltx_Math\" alttext=\"16\" display=\"inline\"><semantics id=\"A4.T11.4.4.3.m1.1a\"><mn id=\"A4.T11.4.4.3.m1.1.1\" xref=\"A4.T11.4.4.3.m1.1.1.cmml\">16</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T11.4.4.3.m1.1b\"><cn type=\"integer\" id=\"A4.T11.4.4.3.m1.1.1.cmml\" xref=\"A4.T11.4.4.3.m1.1.1\">16</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T11.4.4.3.m1.1c\">16</annotation></semantics></math></th>\n<th id=\"A4.T11.5.5.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A4.T11.5.5.4.m1.1\" class=\"ltx_Math\" alttext=\"64\" display=\"inline\"><semantics id=\"A4.T11.5.5.4.m1.1a\"><mn id=\"A4.T11.5.5.4.m1.1.1\" xref=\"A4.T11.5.5.4.m1.1.1.cmml\">64</mn><annotation-xml encoding=\"MathML-Content\" id=\"A4.T11.5.5.4.m1.1b\"><cn type=\"integer\" id=\"A4.T11.5.5.4.m1.1.1.cmml\" xref=\"A4.T11.5.5.4.m1.1.1\">64</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A4.T11.5.5.4.m1.1c\">64</annotation></semantics></math></th>\n</tr>\n<tr id=\"A4.T11.5.6.1\" class=\"ltx_tr\">\n<td id=\"A4.T11.5.6.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"2\"><span id=\"A4.T11.5.6.1.1.1\" class=\"ltx_text\">FedAvg</span></td>\n<td id=\"A4.T11.5.6.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Pre-Personalization</td>\n<td id=\"A4.T11.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.6</td>\n<td id=\"A4.T11.5.6.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.8</td>\n<td id=\"A4.T11.5.6.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.3</td>\n<td id=\"A4.T11.5.6.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">5.2</td>\n</tr>\n<tr id=\"A4.T11.5.7.2\" class=\"ltx_tr\">\n<td id=\"A4.T11.5.7.2.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">Post-Personalization</td>\n<td id=\"A4.T11.5.7.2.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.8</td>\n<td id=\"A4.T11.5.7.2.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.006</td>\n<td id=\"A4.T11.5.7.2.4\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.007</td>\n<td id=\"A4.T11.5.7.2.5\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\">0.007</td>\n</tr>\n<tr id=\"A4.T11.5.8.3\" class=\"ltx_tr\">\n<td id=\"A4.T11.5.8.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\" rowspan=\"2\"><span id=\"A4.T11.5.8.3.1.1\" class=\"ltx_text\">FedSGD</span></td>\n<td id=\"A4.T11.5.8.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">Pre-Personalization</td>\n<td id=\"A4.T11.5.8.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.6</td>\n<td id=\"A4.T11.5.8.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.7</td>\n<td id=\"A4.T11.5.8.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.9</td>\n<td id=\"A4.T11.5.8.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">4.2</td>\n</tr>\n<tr id=\"A4.T11.5.9.4\" class=\"ltx_tr\">\n<td id=\"A4.T11.5.9.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">Post-Personalization</td>\n<td id=\"A4.T11.5.9.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.9</td>\n<td id=\"A4.T11.5.9.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.5</td>\n<td id=\"A4.T11.5.9.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.3</td>\n<td id=\"A4.T11.5.9.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">3.3</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To better explore the phenomena discussed in ",
                "Section",
                " ",
                "5",
                ", we perform a modified experiment where we repeat the training from ",
                "Section",
                " ",
                "5",
                ", but vary the number of batches ",
                "τ",
                "𝜏",
                "\\tau",
                " each training client yields. As above, we repeat and truncate clients’ datasets so that each client has exactly 1024 examples. For example, when ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                ", and we use a batch size of 16, this means that for FedAvg and FedSGD, each client computes ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                " mini-batch gradients. For FedAvg, this means that the client does ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                " steps of training, while for FedSGD, this means that the client sends the average of these ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                " batches back to the server. We vary the number of batches ",
                "τ",
                "𝜏",
                "\\tau",
                " over ",
                "{",
                "1",
                ",",
                "4",
                ",",
                "16",
                ",",
                "64",
                "}",
                "1",
                "4",
                "16",
                "64",
                "\\{1,4,16,64\\}",
                ". We do so by repeating and truncating clients’ data so that they have 16, 64, 256, and 1024 examples, respectively. We then perform different amounts of training, one in which we equalize the number of communication rounds, and one in which we equalize the total number of tokens seen across all clients.",
                "Note that fixing the number of examples per client at 1024 and simply changing the batch size would be a more elegant way to do this kind of ablation, as it would allow normalizing the number of communication rounds and tokens simultaneously. Unfortunately, the batch size is often dictated by compute constraints (e.g. what fits in the memory of a given hardware device), and cannot be increased in an unbounded fashion in realistic machine learning settings, especially on-device settings which are common in FL. Thus, we instead focus on varying ",
                "τ",
                "𝜏",
                "\\tau",
                ".",
                "Equalizing communication rounds.",
                " We do 5000 rounds of training for each number of batches per client ",
                "τ",
                "𝜏",
                "\\tau",
                ", using the same warmup and cosine decay schedule discussed above. Throughout training, we perform personalization evaluation on the models. We give the pre-personalization results in ",
                "Figure",
                " ",
                "14",
                ", and the post-personalization results in ",
                "Figure",
                " ",
                "15",
                ". For more detailed numerical results, see ",
                "Section",
                " ",
                "D.2",
                ".",
                "We see that for FedSGD, the pre- and post-personalization losses do not change much with the number of batches per client ",
                "τ",
                "𝜏",
                "\\tau",
                ". For FedAvg, on the other hand, lower values of ",
                "τ",
                "𝜏",
                "\\tau",
                " attain better pre-personalization loss, while higher values achieve better post-personalization loss. Note that when the number of batches per client is ",
                "τ",
                "=",
                "1",
                "𝜏",
                "1",
                "\\tau=1",
                ", FedAvg and FedSGD are effectively the same algorithm (up to differences in normalization), and perform only a single update step per client each round.\nNotably, ",
                "FedAvg attains a better trade-off between the pre- and post-personalization metrics",
                ".\nSpecifically, FedAvg with ",
                "τ",
                "=",
                "4",
                "𝜏",
                "4",
                "\\tau=4",
                " can match the best pre-personalization performance of FedSGD (FedAvg with ",
                "τ",
                "=",
                "4",
                "𝜏",
                "4",
                "\\tau=4",
                " and FedSGD with ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                " both attain a median pre-personalization loss of ",
                "4.2",
                "4.2",
                "4.2",
                "),\nwhile performing significantly better on the post-personalization metrics (a median loss of ",
                "1.9",
                "1.9",
                "1.9",
                " for FedAvg with ",
                "τ",
                "=",
                "4",
                "𝜏",
                "4",
                "\\tau=4",
                " versus ",
                "3.4",
                "3.4",
                "3.4",
                " for FedSGD with ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                ").\nIn short, these results seem to suggest that “client drift” ",
                "[",
                "28",
                "]",
                " is less of an impediment to federated learning, and more indicative of a trade-off between minimizing pre- and post-personalization loss functions.",
                "Equalizing tokens.",
                " Next, we perform an analogous experiment, but where each setting of ",
                "τ",
                "𝜏",
                "\\tau",
                " (the number of batches per client), is trained for a different number of rounds, so that in each setting the same number of tokens is processed (over all clients). For example, ",
                "τ",
                "=",
                "1",
                "𝜏",
                "1",
                "\\tau=1",
                " trains for ",
                "64",
                "×",
                "64\\times",
                " more communication rounds than for ",
                "τ",
                "=",
                "64",
                "𝜏",
                "64",
                "\\tau=64",
                ". Each training run processes roughly 10.5 billion tokens in total. We give the pre-personalization results in ",
                "Figure",
                " ",
                "16",
                ", and the post-personalization results in ",
                "Figure",
                " ",
                "17",
                ". For more detailed numerical results, see ",
                "Section",
                " ",
                "D.2",
                ". Notably, the post-personalization loss diverged at intermediate stages for ",
                "τ",
                "=",
                "1",
                "𝜏",
                "1",
                "\\tau=1",
                ", but recovered (albeit with high variance across clients) afterwards.",
                "Here, we see a notably different story than in Figures ",
                "14",
                " and ",
                "15",
                ". In particular, we see that for both FedAvg and FedSGD, lower values of ",
                "τ",
                "𝜏",
                "\\tau",
                " lead to lower pre-personalization loss. However, for both algorithms as long as ",
                "τ",
                "𝜏",
                "\\tau",
                " is sufficiently large (at least 4 in this case), the post-personalization loss essentially does not change with ",
                "τ",
                "𝜏",
                "\\tau",
                ". This suggests that when communication is not a bottleneck, we can attain good pre- and post-personalization loss by using FedAvg with a small (but not too small) value of ",
                "τ",
                "𝜏",
                "\\tau",
                ". In other words, by performing enough rounds of FedAvg with a moderate ",
                "τ",
                "𝜏",
                "\\tau",
                ", we can attain a model that does well before and after personalization."
            ]
        ]
    },
    "A5.T12": {
        "caption": "Table 12: Peak memory usage, in megabytes, when iterating over federated datasets. We iterate over all examples in all group datasets, in serial, on a single CPU. We compare a federated CIFAR-100 dataset (partitioned across 100 groups, each with 100 examples), FedCCnews (in which examples are split across users at a domain level), and FedBookCO (in which examples are split across users at a title level). See Section 4 for more details on the latter two datasets.",
        "table": "<table id=\"A5.T12.9\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T12.9.10.1\" class=\"ltx_tr\">\n<th id=\"A5.T12.9.10.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A5.T12.9.10.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset Format</span></th>\n<th id=\"A5.T12.9.10.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A5.T12.9.10.1.2.1\" class=\"ltx_text ltx_font_bold\">In-Memory</span></th>\n<th id=\"A5.T12.9.10.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A5.T12.9.10.1.3.1\" class=\"ltx_text ltx_font_bold\">Hierarchical</span></th>\n<th id=\"A5.T12.9.10.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" style=\"padding-top:1pt;padding-bottom:1pt;\"><span id=\"A5.T12.9.10.1.4.1\" class=\"ltx_text ltx_font_bold\">Streaming</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T12.3.3\" class=\"ltx_tr\">\n<th id=\"A5.T12.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\">CIFAR-100</th>\n<td id=\"A5.T12.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A5.T12.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"156\" display=\"inline\"><semantics id=\"A5.T12.1.1.1.m1.1a\"><mn id=\"A5.T12.1.1.1.m1.1.1\" xref=\"A5.T12.1.1.1.m1.1.1.cmml\">156</mn><annotation-xml encoding=\"MathML-Content\" id=\"A5.T12.1.1.1.m1.1b\"><cn type=\"integer\" id=\"A5.T12.1.1.1.m1.1.1.cmml\" xref=\"A5.T12.1.1.1.m1.1.1\">156</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T12.1.1.1.m1.1c\">156</annotation></semantics></math></td>\n<td id=\"A5.T12.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A5.T12.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"0.40\" display=\"inline\"><semantics id=\"A5.T12.2.2.2.m1.1a\"><mn id=\"A5.T12.2.2.2.m1.1.1\" xref=\"A5.T12.2.2.2.m1.1.1.cmml\">0.40</mn><annotation-xml encoding=\"MathML-Content\" id=\"A5.T12.2.2.2.m1.1b\"><cn type=\"float\" id=\"A5.T12.2.2.2.m1.1.1.cmml\" xref=\"A5.T12.2.2.2.m1.1.1\">0.40</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T12.2.2.2.m1.1c\">0.40</annotation></semantics></math></td>\n<td id=\"A5.T12.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A5.T12.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"0.74\" display=\"inline\"><semantics id=\"A5.T12.3.3.3.m1.1a\"><mn id=\"A5.T12.3.3.3.m1.1.1\" xref=\"A5.T12.3.3.3.m1.1.1.cmml\">0.74</mn><annotation-xml encoding=\"MathML-Content\" id=\"A5.T12.3.3.3.m1.1b\"><cn type=\"float\" id=\"A5.T12.3.3.3.m1.1.1.cmml\" xref=\"A5.T12.3.3.3.m1.1.1\">0.74</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T12.3.3.3.m1.1c\">0.74</annotation></semantics></math></td>\n</tr>\n<tr id=\"A5.T12.6.6\" class=\"ltx_tr\">\n<th id=\"A5.T12.6.6.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedCCnews</th>\n<td id=\"A5.T12.4.4.1\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A5.T12.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"1996\" display=\"inline\"><semantics id=\"A5.T12.4.4.1.m1.1a\"><mn id=\"A5.T12.4.4.1.m1.1.1\" xref=\"A5.T12.4.4.1.m1.1.1.cmml\">1996</mn><annotation-xml encoding=\"MathML-Content\" id=\"A5.T12.4.4.1.m1.1b\"><cn type=\"integer\" id=\"A5.T12.4.4.1.m1.1.1.cmml\" xref=\"A5.T12.4.4.1.m1.1.1\">1996</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T12.4.4.1.m1.1c\">1996</annotation></semantics></math></td>\n<td id=\"A5.T12.5.5.2\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A5.T12.5.5.2.m1.1\" class=\"ltx_Math\" alttext=\"0.08\" display=\"inline\"><semantics id=\"A5.T12.5.5.2.m1.1a\"><mn id=\"A5.T12.5.5.2.m1.1.1\" xref=\"A5.T12.5.5.2.m1.1.1.cmml\">0.08</mn><annotation-xml encoding=\"MathML-Content\" id=\"A5.T12.5.5.2.m1.1b\"><cn type=\"float\" id=\"A5.T12.5.5.2.m1.1.1.cmml\" xref=\"A5.T12.5.5.2.m1.1.1\">0.08</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T12.5.5.2.m1.1c\">0.08</annotation></semantics></math></td>\n<td id=\"A5.T12.6.6.3\" class=\"ltx_td ltx_align_center\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A5.T12.6.6.3.m1.1\" class=\"ltx_Math\" alttext=\"1.16\" display=\"inline\"><semantics id=\"A5.T12.6.6.3.m1.1a\"><mn id=\"A5.T12.6.6.3.m1.1.1\" xref=\"A5.T12.6.6.3.m1.1.1.cmml\">1.16</mn><annotation-xml encoding=\"MathML-Content\" id=\"A5.T12.6.6.3.m1.1b\"><cn type=\"float\" id=\"A5.T12.6.6.3.m1.1.1.cmml\" xref=\"A5.T12.6.6.3.m1.1.1\">1.16</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T12.6.6.3.m1.1c\">1.16</annotation></semantics></math></td>\n</tr>\n<tr id=\"A5.T12.9.9\" class=\"ltx_tr\">\n<th id=\"A5.T12.9.9.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\">FedBookCO</th>\n<td id=\"A5.T12.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A5.T12.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"6643\" display=\"inline\"><semantics id=\"A5.T12.7.7.1.m1.1a\"><mn id=\"A5.T12.7.7.1.m1.1.1\" xref=\"A5.T12.7.7.1.m1.1.1.cmml\">6643</mn><annotation-xml encoding=\"MathML-Content\" id=\"A5.T12.7.7.1.m1.1b\"><cn type=\"integer\" id=\"A5.T12.7.7.1.m1.1.1.cmml\" xref=\"A5.T12.7.7.1.m1.1.1\">6643</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T12.7.7.1.m1.1c\">6643</annotation></semantics></math></td>\n<td id=\"A5.T12.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A5.T12.8.8.2.m1.1\" class=\"ltx_Math\" alttext=\"0.001\" display=\"inline\"><semantics id=\"A5.T12.8.8.2.m1.1a\"><mn id=\"A5.T12.8.8.2.m1.1.1\" xref=\"A5.T12.8.8.2.m1.1.1.cmml\">0.001</mn><annotation-xml encoding=\"MathML-Content\" id=\"A5.T12.8.8.2.m1.1b\"><cn type=\"float\" id=\"A5.T12.8.8.2.m1.1.1.cmml\" xref=\"A5.T12.8.8.2.m1.1.1\">0.001</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T12.8.8.2.m1.1c\">0.001</annotation></semantics></math></td>\n<td id=\"A5.T12.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\" style=\"padding-top:1pt;padding-bottom:1pt;\"><math id=\"A5.T12.9.9.3.m1.1\" class=\"ltx_Math\" alttext=\"0.10\" display=\"inline\"><semantics id=\"A5.T12.9.9.3.m1.1a\"><mn id=\"A5.T12.9.9.3.m1.1.1\" xref=\"A5.T12.9.9.3.m1.1.1.cmml\">0.10</mn><annotation-xml encoding=\"MathML-Content\" id=\"A5.T12.9.9.3.m1.1b\"><cn type=\"float\" id=\"A5.T12.9.9.3.m1.1.1.cmml\" xref=\"A5.T12.9.9.3.m1.1.1\">0.10</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T12.9.9.3.m1.1c\">0.10</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we provide details of how much memory are used by the various dataset formats in ",
                "Section",
                " ",
                "3.1",
                ". Recall that we consider three formats: in-memory, hierarchical, and streaming. We compare the amount of time required to iterate over various federated datasets in these formats in ",
                "Table",
                " ",
                "3",
                ". Here, we instead detail the peak memory usage (in megabytes) when iterating over the same datasets. We note that these were collected on a single CPU, and do not include the time to do operations like shuffling or batching.",
                "The results are in ",
                "Table",
                " ",
                "12",
                ". We see that in-memory formats use much larger amounts of memory, as they load the entire dataset into memory. By contrast, hierarchical and streaming formats use significantly less peak memory. Moreover, their peak memory usage does not scale with the total size of the dataset, unlike in-memory formats. We note that streaming can use slightly more memory, though only at most 2 MB more in all experiments."
            ]
        ]
    }
}