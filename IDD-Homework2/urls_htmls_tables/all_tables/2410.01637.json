{
    "id_table_1": {
        "caption": "Table 1:  Statistics of the datasets. The average number of tokens is obtained using the GPT-4 tokenizer:  https://github.com/openai/tiktoken .",
        "table": "id10.10",
        "footnotes": [],
        "references": [
            "In this work, we present a set of modifications to overcome this limitation of the Unlimiformer architecture, adapting it to the decoder-only models (see Figure  1 ).  These modifications consist of 1) modifying the cross-attention formulation to include information fusion, 2) updating the index creation procedure, 3) addressing the index staleness problem, and 4) adapting the chunk encoding procedure to causal attention (see Section  3 ).  Moreover, we introduce a new evaluation setting and present our experiments on four long-document datasets across two tasks: summarization and free-form Q&A.  Our experiments show that our modifications improve summarization datasets, performing on par with a model with 2x context length.  We also discuss the limitations and future directions for free-form Q&A and instruction-tuned models.",
            "In decoder-only transformers, due to the absence of a natural encoding/decoder splitting layer, we can arbitrarily choose any layer to use cross-attention instead of self-attention.  This allows us to use various simple or complex patterns for the set of cross-attention layers,  L L \\mathcal{L} caligraphic_L  (e.g.,  L = { 16 } L 16 \\mathcal{L}=\\{16\\} caligraphic_L = { 16 }  in Figure  1 ).  Appendix  B  details how these patterns are tuned as hyperparameters of the model.",
            "One of the potential issues of a static index is staleness.  Specifically, since Unlimiformer approximates dense attention, without updating the index, we would lose the information from the newly generated tokens, which might lead to incoherent outputs.  For example, assume we have a model with a context length of  N N N italic_N .  In this scenario, at generation step  N + 2 N 2 N+2 italic_N + 2 , the input to the model would be the last  N N N italic_N  generated tokens, effectively discarding the first generated token as it is also absent from the index.  To fix this problem, at each generation step and each layer, we add  h CA (  1 ) superscript subscript h CA 1 h_{\\text{CA}}^{(-1)} italic_h start_POSTSUBSCRIPT CA end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT  to its respective index.  This ensures the addition of the most recent token and keeps the indices from going stale.  Appendix  6.1  presents an ablation study that showcases the effectiveness of this simple change.",
            "In contrast to encoder-decoder models, decoder-only transformers use causal (unidirectional) attention.  This difference means that a token has seen enough contextual information if a certain number of tokens are behind it.  As a result, instead of only storing the hidden states of the middle half tokens, we can keep all the non-overlapping ones.  This will allow us to be slightly more efficient when processing long documents.  Note that only the first instance of overlapping tokens is added to the index, as illustrated by orange tokens in Figure  1 .",
            "For our experiments, we use datasets from the two tasks of summarization and free-form Q&A, chosen due to the existence of long-document benchmarks  Shaham et al. ( 2023 ) .  Specifically, for summarization, we use GovReport (GAO)  Huang et al. ( 2021 )  and BookSum  Kryscinski et al. ( 2022 ) , and for free-form Q&A, we use NarrativeQA  Kocisky et al. ( 2018 )  and Qasper  Dasigi et al. ( 2021 ) .  Moreover, we tune the hyperparameters on the validation sets and report the results on the test sets.  All the experiments are carried out in a zero-shot setting.  Table  1  presents the statistics of these datasets.",
            "Although similar in using separate indices for each layer, we present additional modifications to the original methodology 2 2 2 https://github.com/abertsch72/unlimiformer .  Specifically, 1) we introduce an update procedure for indices to avoid staleness, 2) we use a slightly more efficient chunk encoding approach, and 3) we introduce information fusion into the architecture and reformulate the attention calculations to be more comprehensive (see Section  3.1 ).  Moreover, to the best of our knowledge, no evaluation of their proposed methodology has been presented for decoder-only models, a shortcoming that our work aims to address using a new evaluation setting and new datasets."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Effect of adding newly generated tokens to the index on the performance of the model.",
        "table": "S4.T1.1.1",
        "footnotes": [],
        "references": [
            "The original Unlimiformer paper presents two main experimental comparisons: 1)  BART base subscript BART base \\text{BART}_{\\text{base}} BART start_POSTSUBSCRIPT base end_POSTSUBSCRIPT  vs.  BART base + Unlimiformer subscript BART base Unlimiformer \\text{BART}_{\\text{base}}+\\text{Unlimiformer} BART start_POSTSUBSCRIPT base end_POSTSUBSCRIPT + Unlimiformer  and 2)  BART base + Unlimiformer subscript BART base Unlimiformer \\text{BART}_{\\text{base}}+\\text{Unlimiformer} BART start_POSTSUBSCRIPT base end_POSTSUBSCRIPT + Unlimiformer  vs. SLED  Ivgi et al. ( 2023 )  and Longformer  Beltagy et al. ( 2020 ) .  These comparisons showcase the effectiveness of the proposed model; however, one missing crucial evaluation setup is the comparison to the same base model with longer context lengths, e.g.,  GPT-Summ  [ 2048 ] GPT-Summ delimited-[] 2048 \\text{GPT-Summ}[2048] GPT-Summ [ 2048 ]  vs.  GPT-Summ  [ 1024 ] + Unlimiformer GPT-Summ delimited-[] 1024 Unlimiformer \\text{GPT-Summ}[1024]+\\text{Unlimiformer} GPT-Summ [ 1024 ] + Unlimiformer .  Such a setup provides more insight into how efficiently Unlimiformer uses the extra information provided through the kNN index.  In this work, we focus on this setting by restricting the context length of the base model to 1024, 2048, 4096, and 8192 tokens and then comparing them to variations equipped with Unlimiformer.  To ensure that our models and datasets showcase meaningful differences across context lengths in such a setup, we ran experiments on the base models using the validation sets, estimating the performance changes as contextual information increased.  Figure  3  illustrates the results of our experiments.  Moreover, Appendix  6.2  presents a case study to ensure the performance disparity in Figure  3  is an artifact of the datasets, not a deficiency in the models.",
            "For the free-form Q&A datasets and the instruction-finetuned model, we opted for a simple three-part (Article, Question, and Answer) template.  Figure  2  illustrates an example of the prompt structure.  We also investigated ZeroScrollss prompt template  Shaham et al. ( 2023 ) , but since we did not notice any significant difference in the performance, we continued with the more straightforward template.",
            "Figure  6  illustrates our experimental results using the GPT-Inst model on the NarrativeQA and Qasper datasets.  Although we can observe some improvements in both Qasper and (mostly) NarrativeQA, they are less significant than the summarization datasets results.  Given the prompt-based approach used for the free-from Q&A task (see Figure  2 ), the insignificant improvements could be an artifact of the instruction-tuned model being too biased toward the input, making it insensitive to the added information in the cross-attention layers.  These results present exciting opportunities to investigate such models in future studies.",
            "To study the effect of index staleness, we experiment with an internal summarization dataset consisting of samples of up to 7k tokens using a model with a context length of 2048 and a generation limit of 700 tokens.  Table  2  presents the result of our experiments.  Although the generation length is still way under the 2048 limit, we can see a slight positive improvement in the performance, showcasing the usefulness of this simple addition with almost no cost.  Moreover, we believe the performance boost will increase as the generation length increases.",
            "Since both Unlimiformer and our approach use a specific query vector to retrieve hidden states from the index, the retrieval process becomes biased on the query vector.  In Unlimiformer, this vector is  h d (  1 ) superscript subscript h d 1 h_{d}^{(-1)} italic_h start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT , which is calculated by attending to the generated tokens.  In our approach, this vector is  h CA (  1 ) superscript subscript h CA 1 h_{\\text{CA}}^{(-1)} italic_h start_POSTSUBSCRIPT CA end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( - 1 ) end_POSTSUPERSCRIPT , which is calculated by attending to the whole input, i.e., original context + generated tokens.  This dependence on the original context potentially reduces the expected performance gains when external indices are used.  Moreover, it could partially explain the lack of significant improvements in Section  5.2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Percentage of answers with minimum evidence position outside a given context length in Qasper.",
        "table": "S6.T2.1",
        "footnotes": [],
        "references": [
            "In recent years, large language models (LLMs) have become critical to many language-based technologies, such as conversational and search systems.  LLMs have shown state-of-the-art performance on sequence-to-sequence downstream tasks such as summarization and question-answering.  However, the performance of these models is bounded by the information that can fit in their context (see Figure  3  and Section  4.3 ).  Despite the efforts in the community  Choromanski et al. ( 2021 ); Beltagy et al. ( 2020 ); Ivgi et al. ( 2023 ) , most of the common open-source models, e.g., MPT  Team ( 2023 ) , Falcon  Penedo et al. ( 2023 ) , and LLama-2  Touvron et al. ( 2023 ) , have a context length of 4096 or less.  As such, efficiently overcoming this limitation would allow a broader and fairer adaptation of LLMs while increasing their performance across benchmarks.",
            "In this work, we present a set of modifications to overcome this limitation of the Unlimiformer architecture, adapting it to the decoder-only models (see Figure  1 ).  These modifications consist of 1) modifying the cross-attention formulation to include information fusion, 2) updating the index creation procedure, 3) addressing the index staleness problem, and 4) adapting the chunk encoding procedure to causal attention (see Section  3 ).  Moreover, we introduce a new evaluation setting and present our experiments on four long-document datasets across two tasks: summarization and free-form Q&A.  Our experiments show that our modifications improve summarization datasets, performing on par with a model with 2x context length.  We also discuss the limitations and future directions for free-form Q&A and instruction-tuned models.",
            "The original Unlimiformer paper presents two main experimental comparisons: 1)  BART base subscript BART base \\text{BART}_{\\text{base}} BART start_POSTSUBSCRIPT base end_POSTSUBSCRIPT  vs.  BART base + Unlimiformer subscript BART base Unlimiformer \\text{BART}_{\\text{base}}+\\text{Unlimiformer} BART start_POSTSUBSCRIPT base end_POSTSUBSCRIPT + Unlimiformer  and 2)  BART base + Unlimiformer subscript BART base Unlimiformer \\text{BART}_{\\text{base}}+\\text{Unlimiformer} BART start_POSTSUBSCRIPT base end_POSTSUBSCRIPT + Unlimiformer  vs. SLED  Ivgi et al. ( 2023 )  and Longformer  Beltagy et al. ( 2020 ) .  These comparisons showcase the effectiveness of the proposed model; however, one missing crucial evaluation setup is the comparison to the same base model with longer context lengths, e.g.,  GPT-Summ  [ 2048 ] GPT-Summ delimited-[] 2048 \\text{GPT-Summ}[2048] GPT-Summ [ 2048 ]  vs.  GPT-Summ  [ 1024 ] + Unlimiformer GPT-Summ delimited-[] 1024 Unlimiformer \\text{GPT-Summ}[1024]+\\text{Unlimiformer} GPT-Summ [ 1024 ] + Unlimiformer .  Such a setup provides more insight into how efficiently Unlimiformer uses the extra information provided through the kNN index.  In this work, we focus on this setting by restricting the context length of the base model to 1024, 2048, 4096, and 8192 tokens and then comparing them to variations equipped with Unlimiformer.  To ensure that our models and datasets showcase meaningful differences across context lengths in such a setup, we ran experiments on the base models using the validation sets, estimating the performance changes as contextual information increased.  Figure  3  illustrates the results of our experiments.  Moreover, Appendix  6.2  presents a case study to ensure the performance disparity in Figure  3  is an artifact of the datasets, not a deficiency in the models.",
            "In the Qasper dataset, we have access to a set of evidence for each answer.  Hence, we can calculate the percentage of answers that all of their evidence falls out of the range of a specific context length.  Table  3  presents these numbers for different context lengths.  These numbers are consistent with the improvements in Figure  3 , which showcase the validity of our models.",
            "Although similar in using separate indices for each layer, we present additional modifications to the original methodology 2 2 2 https://github.com/abertsch72/unlimiformer .  Specifically, 1) we introduce an update procedure for indices to avoid staleness, 2) we use a slightly more efficient chunk encoding approach, and 3) we introduce information fusion into the architecture and reformulate the attention calculations to be more comprehensive (see Section  3.1 ).  Moreover, to the best of our knowledge, no evaluation of their proposed methodology has been presented for decoder-only models, a shortcoming that our work aims to address using a new evaluation setting and new datasets."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S6.T3.1.1",
        "footnotes": [],
        "references": [
            "In recent years, large language models (LLMs) have become critical to many language-based technologies, such as conversational and search systems.  LLMs have shown state-of-the-art performance on sequence-to-sequence downstream tasks such as summarization and question-answering.  However, the performance of these models is bounded by the information that can fit in their context (see Figure  3  and Section  4.3 ).  Despite the efforts in the community  Choromanski et al. ( 2021 ); Beltagy et al. ( 2020 ); Ivgi et al. ( 2023 ) , most of the common open-source models, e.g., MPT  Team ( 2023 ) , Falcon  Penedo et al. ( 2023 ) , and LLama-2  Touvron et al. ( 2023 ) , have a context length of 4096 or less.  As such, efficiently overcoming this limitation would allow a broader and fairer adaptation of LLMs while increasing their performance across benchmarks.",
            "Figures  4  and  5  present our experimental results using the GPT-Summ model on the GovReport (GAO) and BookSum datasets, respectively.  As evident from these results, adding our modifications improves the models performance to a 2x level (e.g.,  GPT-Summ  [ 1024 ] + Unlimiformer  GPT-Summ  [ 2048 ] GPT-Summ delimited-[] 1024 Unlimiformer GPT-Summ delimited-[] 2048 \\text{GPT-Summ}[1024]+\\text{Unlimiformer}\\approx\\text{GPT-Summ}[2048] GPT-Summ [ 1024 ] + Unlimiformer  GPT-Summ [ 2048 ] ) on the GovReport (GAO) dataset.  Similarly, we observe significant improvements in the BookSum dataset.  These results showcase the effectiveness of our proposed modifications."
        ]
    },
    "global_footnotes": [
        "Concurrently,",
        "have released an implementation for decoder-only models. Appendix",
        "details the differences in our adaptation."
    ]
}