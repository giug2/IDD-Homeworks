{
    "id_table_1": {
        "caption": "Table 1:  Compared with previous works,  MRAG-Bench  focuses on  evaluating LVLMs in utilizing vision-centric retrieval-augmented multimodal knowledge. Diverse scenarios refers to whether a benchmark categorized different scenarios during evaluation. : Web,  : ImageNet  (Russakovsky et al.,  2015 ) ,  : Flowers102  (Nilsback & Zisserman,  2008 ) ,  : StanfordCars  (Krause et al.,  2013 ) .",
        "table": "S1.T1.4.4",
        "footnotes": [],
        "references": [
            "There are several existing benchmarks evaluating retrieval-augmented LVLMs. For example, OK-VQA  (Marino et al.,  2019 )  focused on scenarios where the image content alone is insufficient to answer the questions. A-OKVQA  (Schwenk et al.,  2022 )  further extended this dataset to incorporate additional types of world knowledge. More recent works  (Chang et al.,  2022 ; Chen et al.,  2023b ; Mensink et al.,  2023 )  further expanded and curated large-scale knowledge base data to evaluate pre-trained vision and language models in knowledge-intensive and information-seeking visual questions.  However, as shown in Table  1 , these benchmarks remain text-centric, as their questions can often be resolved with related external textual knowledge. In contrast, retrieving visual information is sometimes more beneficial than retrieving text, as humans often gain greater insights from it.  Specifically, we illustrate examples in Figure  1  where retrieving correct textual knowledge can be  hard  and retrieved textual knowledge can be  useless , while retrieving additional images is helpful.  For instance, when presented with a top-down view of a car, humans may struggle to accurately identify it; however, with a front-facing view, they can quickly recognize the vehicle and effectively leverage the visual information.",
            "Our benchmark is designed for systematic evaluation of LVLMs vision-centric multimodal RAG abilities.  To achieve this, we focus on evaluating the models understanding of image objects that are not commonly associated with its knowledge base, while the collected ground-truth images can help incentivize specific visual concepts within LVLMs memory.  Therefore, we divide our benchmark into two main aspects, as illustrated in the examples in Figure  1 :",
            "As the guidelines discussed in    \\S    2.1 , our benchmark collection involves a clean ground-truth image corpus that can resonate with models internal knowledge and a query question and image that challenge models memory according to our definition of 9 diverse scenarios. To collect a dataset for systematic evaluation of vision-centric multimodal RAG scenarios, we manually annotate all multiple-choice question answering (MCQA) data while sourcing images from either publicly available datasets or manually scraping them from the web.",
            "We chose to manually scrape images from the web based on the definitions of the  transformative  aspect. To construct the image corpus, we employed Bing Image Search for each of the image object keyword predefined by us, please refer to Appendix  A.1  for more details. We filtered out image objects that did not form a clear transformative pair between the query image and the ground-truth image, retaining approximately 74% of the keyword names in the process. For ground-truth image examples, we employed automatic scripts to download the top 15 images related to its keyword names and human filtered out the unqualified image. On average, this results to 5.9 images per question and the five ground-truth images used during our evaluation are manually selected same as in  perspective  aspect.",
            "After constructing the entire benchmark, we implemented two quality control procedures: an automatic check with predefined rules and a manual examination of each instance. The automatic check verifies the correct MCQA format, assesses image validity and filters out redundant images in the corpus, more details are presented in Appendix  A.1 . The manual examination is conducted by two experts in this field, who checked the correspondence between query images and ground-truth image examples, and filtered or revised ambiguous questions and uncorrelated query image and ground-truth images.",
            "In this section, we first introduce the experimental setup and evaluation metric (   3.1  3.1 \\S~{}\\ref{sec: experimental setup}  ). Then, we present a comprehensive evaluation of 14 recent LVLMs (   3.2  3.2 \\S~{}\\ref{sec: main results}  ). We demonstrate the importance of visual knowledge and discuss the critical findings revealed by the results from  MRAG-Bench .",
            "We follow standard MCQA evaluation setup and employ accuracy score as our metric. We adopt default generation hyper-parameters selected by each model. Following  Lu et al. ( 2024b ) , we employ GPT-3.5-turbo to extract the multiple choice answer in rare cases where our pre-defined automatic extraction rules failed. We refer the readers to Appendix  A.1  and  B  for more details on evaluation prompts for both without multimodal RAG and with multimodal RAG scenarios, answer extraction prompt and human performance evaluation protocol.",
            "In this section, we conduct quantitative analysis addressing three important questions: 1) To what extent can LVLMs benefit more from visual knowledge than from textual knowledge on  MRAG-Bench ? (   \\S    4.1 ) 2) How does the performance of LVLMs vary with examples retrieved from different retrievers? (   \\S    4.2 ) 3) How many ground-truth visual knowledge examples are required for LVLMs to continue benefiting? (   \\S    4.3 )",
            "We overview three lines of related work: 1) multimodal retrieval-augmented generation benchmarks (   \\S    5.1 ), 2) large vision language models (   \\S    5.2 ), and 3) retrieval-augmented mutlimodal large language models (   \\S    5.3 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  Accuracy scores on  MRAG-Bench . The highest scores for  open-source  models in each section and  proprietary  models are highlighted in blue and red, respectively. Both Retrieved RAG and GT RAG employ top-5 image examples (except for the incomplete scenario, where a single example is intuitively sufficient). The relative difference in performance compared to the score without RAG is shown in subscript, with  blue  indicating performance drops and  red  indicating improvements.",
        "table": "S2.F2.fig1.1",
        "footnotes": [],
        "references": [
            "MRAG-Bench  consists of 16,130 images and 1,353 multiple choice questions, with key statistics shown in Table  2 .   MRAG-Bench  adheres to the following design principles: (1) it focuses on real-world scenarios where visually augmented information is useful; (2) it incorporates 9 diverse multimodal RAG scenarios covering various types of image objects; (3) it features cleaned ground-truth images for each question that align with human knowledge; and (4) it provides robust evaluation settings for deterministic evaluations. Unlike previous works focus on retrieving textual knowledge, evaluation on  MRAG-Bench  focuses on retrieving vision-centric knowledge, which can be formulated as follows: Given a query tuple  Q Q \\mathbf{Q} bold_Q  composed of (query image, textual question), the multimodal retriever  R R \\mathcal{R} caligraphic_R  returns a set of relevant images  I I \\mathbf{I} bold_I  ( [ i 1 , i 2 , ... , i N ] subscript i 1 subscript i 2 ... subscript i N [\\mathbf{i}_{1},\\mathbf{i}_{2},...,\\mathbf{i}_{N}] [ bold_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_i start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , bold_i start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ] ), then the LVLM  M M \\mathcal{M} caligraphic_M  take the input ( Q Q \\mathbf{Q} bold_Q ,  I I \\mathbf{I} bold_I ) and output the final answer.",
            "MRAG-Bench  provides a systematic evaluation across 9 distinctive multimodal RAG scenarios, with four scenarios focused on the  perspective  understanding of visual entities, four on  transformative  understanding, and one categorized as others. As illustrated in Figure  2 , each scenario comprises 7.5% to 23.8% of the whole benchmark. The selected examples of each scenario is shown in Figure  3 . The details of each scenario are introduced as follows.",
            "As the guidelines discussed in    \\S    2.1 , our benchmark collection involves a clean ground-truth image corpus that can resonate with models internal knowledge and a query question and image that challenge models memory according to our definition of 9 diverse scenarios. To collect a dataset for systematic evaluation of vision-centric multimodal RAG scenarios, we manually annotate all multiple-choice question answering (MCQA) data while sourcing images from either publicly available datasets or manually scraping them from the web.",
            "In this section, we first introduce the experimental setup and evaluation metric (   3.1  3.1 \\S~{}\\ref{sec: experimental setup}  ). Then, we present a comprehensive evaluation of 14 recent LVLMs (   3.2  3.2 \\S~{}\\ref{sec: main results}  ). We demonstrate the importance of visual knowledge and discuss the critical findings revealed by the results from  MRAG-Bench .",
            "In this section, we conduct quantitative analysis addressing three important questions: 1) To what extent can LVLMs benefit more from visual knowledge than from textual knowledge on  MRAG-Bench ? (   \\S    4.1 ) 2) How does the performance of LVLMs vary with examples retrieved from different retrievers? (   \\S    4.2 ) 3) How many ground-truth visual knowledge examples are required for LVLMs to continue benefiting? (   \\S    4.3 )",
            "For simplicity, all our experiments used five retrieved or ground-truth image examples. However, it is worth exploring how many examples LVLMs can effectively leverage. As noted in    \\S    2.3 , the perspective aspect of our benchmark includes an average of 20.4 ground-truth examples. To investigate further, we perform an analysis focusing on the perspective and others aspects, covering a total of 892 questions. As shown in Figure  5 , we evaluated LLaVA-Next-Interleave using 1, 2, 3, 5, 10, 20 GT examples, averaging the results across three random seeds for sampling the GT examples. LLaVA-Next-Interleave saw the greatest improvement of 5.64% with just one GT example. Performance continued to increase steadily, reaching a peak at 10 GT examples, which was 0.29% higher than with 20 GT examples. One possible explanation could be LLaVA-Next-Interleave may not able to better leverage visually augmented knowledge in long context scenarios. Moreover, the complexity of questions affects the number of images needed too, one ground-truth example sometimes help the model the most on  MRAG-Bench . We encourage the research on adaptatively deciding the number of necessary images based on the complexity of questions.",
            "We overview three lines of related work: 1) multimodal retrieval-augmented generation benchmarks (   \\S    5.1 ), 2) large vision language models (   \\S    5.2 ), and 3) retrieval-augmented mutlimodal large language models (   \\S    5.3 )."
        ]
    },
    "id_table_3": {
        "caption": "Table 4:  LVLMs performance on  MRAG-Bench  with textual knowledge v.s visual knowledge. Both the open-source and proprietary model benefit more from image knowledge.",
        "table": "S2.T3.300.300",
        "footnotes": [],
        "references": [
            "MRAG-Bench  provides a systematic evaluation across 9 distinctive multimodal RAG scenarios, with four scenarios focused on the  perspective  understanding of visual entities, four on  transformative  understanding, and one categorized as others. As illustrated in Figure  2 , each scenario comprises 7.5% to 23.8% of the whole benchmark. The selected examples of each scenario is shown in Figure  3 . The details of each scenario are introduced as follows.",
            "To collect diverse image objects and knowledge that are not extensively represented in LVLMs memories  (Zhang et al.,  2024c ) ,  we considered three sources of data, ImageNet  (Russakovsky et al.,  2015 ) , Oxford Flowers102  (Nilsback & Zisserman,  2008 ) , and StanfordCars  (Krause et al.,  2013 ) . To construct a high quality image corpus, for each of the image class that we included in our benchmark, we examined the validation set and excluded the unqualified images which cant provide sufficient visual information for the recognition of this class. Among the selected corpus, we further humanly picked five representative examples covering the diverse aspects of each class object, as the five ground-truth examples in our experimental results (See    \\S  3 ). For constructing the query images, we adhered to our scenario definitions and manually selected qualified images for the  [Angle] ,  [Scope] , and  [Occlusion]  scenarios. For the  [Partial]  scenario, we randomly cropped images by 50% in both height and width. Then we performed another human inspection to ensure the quality of the cropped images, filtering out examples where the visual object did not occupy the dominant area of the image. We repeated the random cropping process until satisfactory images were obtained, filtering to 20.4 GT images per question on average.",
            "In this section, we first introduce the experimental setup and evaluation metric (   3.1  3.1 \\S~{}\\ref{sec: experimental setup}  ). Then, we present a comprehensive evaluation of 14 recent LVLMs (   3.2  3.2 \\S~{}\\ref{sec: main results}  ). We demonstrate the importance of visual knowledge and discuss the critical findings revealed by the results from  MRAG-Bench .",
            "As shown in Table  3 , the average performance of the most advanced LVLMs is not better than 68.68% without multimodal RAG knowlege, and 74.5% with ground-truth knowledge, which demonstrates  MRAG-Bench  to be a challenging benchmark. The mean accuracies of open-source LVLMs are between 26.83% and 53.29% without RAG knowledge and between 28.90% and 59.28% with ground-truth knowledge, which fall behind from advanced proprietary LVLMs. Notably,  MRAG-Bench  proves to be knowledge-intensive as average humans achieved 38.47% without RAG knowledge, while proprietary LVLMs generally perform well, suggesting that their extensive training data equips them with a broader knowledge base.  However, when provided with either retrieved or ground-truth knowledge, humans achieve the most significant improvements of 22.91% and 33.16%, respectively. This underscore the need of LVLMs to better utilize visually augmented information like humans.",
            "As illustrated in Table  3 , all models demonstrate improvement when ground-truth image RAG knowledge is provided. Among the open-source models, they achieve improvements ranging from 2.07% to 11.31% when using ground-truth RAG knowledge, whereas 5.64% to 9.69% improvements are observed from proprietary LVLMs. Interestingly, when images from the multimodal retriever is provided, almost all open-source LVLMs on average demonstrate a declined performance while proprietary models can still gain improvement. This indicate proprietary models possess emerging abilities to distinguish between good and bad image knowledge sources, which is a critical skill in the multimodal RAG domain. We further conducted a qualitative analysis to investigate the reasons behind this, as detailed in the following paragraphs.",
            "We also report fine-grained scores across 9 scenarios on  MRAG-Bench  in Table  3 . Remarkably, GPT-4o surpasses most other baselines in various categories, with exceptions in problems related to partial, incomplete and biological scenarios. Notably, GPT-4o outperforms human performance on all perspective aspect as well as on temporal and deformation scenarios within the transformative aspect. We conjecture that incomplete and biological scenarios are less likely to be included in the training knowledge. Interestingly, all models exhibit a decline in performance on incomplete scenarios, with only a few exceptions, while humans find this task relatively easy, achieving 58.82% and 83.33% scores with ground-truth knowledge. This further highlights the importance of leveraging retrieved visually augmented knowledge to address questions that do not directly incentivize knowledge stored in the models memories.",
            "In this section, we conduct quantitative analysis addressing three important questions: 1) To what extent can LVLMs benefit more from visual knowledge than from textual knowledge on  MRAG-Bench ? (   \\S    4.1 ) 2) How does the performance of LVLMs vary with examples retrieved from different retrievers? (   \\S    4.2 ) 3) How many ground-truth visual knowledge examples are required for LVLMs to continue benefiting? (   \\S    4.3 )",
            "For simplicity, all our experiments used five retrieved or ground-truth image examples. However, it is worth exploring how many examples LVLMs can effectively leverage. As noted in    \\S    2.3 , the perspective aspect of our benchmark includes an average of 20.4 ground-truth examples. To investigate further, we perform an analysis focusing on the perspective and others aspects, covering a total of 892 questions. As shown in Figure  5 , we evaluated LLaVA-Next-Interleave using 1, 2, 3, 5, 10, 20 GT examples, averaging the results across three random seeds for sampling the GT examples. LLaVA-Next-Interleave saw the greatest improvement of 5.64% with just one GT example. Performance continued to increase steadily, reaching a peak at 10 GT examples, which was 0.29% higher than with 20 GT examples. One possible explanation could be LLaVA-Next-Interleave may not able to better leverage visually augmented knowledge in long context scenarios. Moreover, the complexity of questions affects the number of images needed too, one ground-truth example sometimes help the model the most on  MRAG-Bench . We encourage the research on adaptatively deciding the number of necessary images based on the complexity of questions.",
            "We overview three lines of related work: 1) multimodal retrieval-augmented generation benchmarks (   \\S    5.1 ), 2) large vision language models (   \\S    5.2 ), and 3) retrieval-augmented mutlimodal large language models (   \\S    5.3 )."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:  Prompt template to extract multiple choice answer from models response.   {In-context examples}   are in-context examples.",
        "table": "S4.T4.100.100",
        "footnotes": [],
        "references": [
            "We conduct an error analysis on an open-source model (LLaVA-Next-Interleave) and a proprietary model (Gemini Pro). For a fair comparison, we filtered results where LLaVA-Next-Interleave answered correctly without or with GT knowledge but was misled to wrong answer with retrieved examples. One example is illustrated in Figure  4 , the retrieved images contain two correct examples and three false examples. While Gemini Pro is able to utilize all retrieved images, LLaVA-Next-Interleave leverages bad examples and makes wrong prediction. This example helps explain why do almost all open-source models have lower performance with retrieved knowledge.",
            "In this section, we conduct quantitative analysis addressing three important questions: 1) To what extent can LVLMs benefit more from visual knowledge than from textual knowledge on  MRAG-Bench ? (   \\S    4.1 ) 2) How does the performance of LVLMs vary with examples retrieved from different retrievers? (   \\S    4.2 ) 3) How many ground-truth visual knowledge examples are required for LVLMs to continue benefiting? (   \\S    4.3 )",
            "We used the Wikipedia corpus as of 2023/07/01 as our text knowledge corpus 1 1 1 https://www.kaggle.com/datasets/jjinho/wikipedia-20230701 . To ensure a fair comparison, we employed the same multimodal retriever (CLIP) for retrieving either text or image knowledge. The top-5 ranked documents or images are used for augmenting the input.  We selected one open-source (LLaVA-Next-Interleave) and one proprietary (GPT-4-Turbo) LVLM to examine their preference for textual knowledge versus image knowledge on  MRAG-Bench . As shown in Table  4 , when both models utilized retrieved knowledge, LLaVA-Next-Interleave demonstrated a 2.36% improvement with image knowledge over text knowledge, while GPT-4-Turbo showed a 2.34% improvement. When using GT knowledge, LLaVA-Next-Interleave exhibited an 11.09% improvement with image knowledge over text knowledge, compared to a 3.87% improvement for GPT-4-Turbo.  Interestingly, when both GT image and text knowledge are provided, LLaVA-Next-Interleave indicated less improvement than with GT image alone whereas GPT-4-Turbo further pushed its performance.  All these results demonstrate that retrieving visual knowledge is more helpful than retrieving text on  MRAG-Bench ."
        ]
    },
    "id_table_5": {
        "caption": "Table 6:  Recall@5 scores with 4 retriever models on  MRAG-Bench .",
        "table": "A2.T5.1.p1.pic1.1.1.1.1.1",
        "footnotes": [],
        "references": [
            "We picked four recent best-performing multimodal retrievers, including CLIP  (Radford et al.,  2021 ) , MagicLens  (Zhang et al.,  2024a ) , E5-V  (Jiang et al.,  2024b ) , VISTA  (Zhou et al.,  2024 )  and evaluated their performance (Recall@5). The detailed retriever performance can be found at Table  6  in Appendix  C . We selected LLaVA-Next-Interleave as the end model to assess its performance. As shown in Figure  5 , when retrievers achieve higher Recall@5 scores (i.e., better retrieved examples), the LVLMs accuracy tends to improve, demonstrating a strong 95% positive correlation. Interestingly, despite similar Recall@5 scores from CLIP and VISTA retrievers, LLaVA-Next-Interleave demonstrated a 2.07% gap in overall accuracy. We conjecture that the order of the correctly retrieved examples may also impact the models final performance. The sensitivity to the order of retrieved examples is a common issue that persists across various models.  Although this phenomenon, known as position bias, has been examined in text-based RAG  (Lu et al.,  2022b ; Wang et al.,  2023 ) , its impact on visual RAG remains unexplored, presenting a promising direction for future research.",
            "For simplicity, all our experiments used five retrieved or ground-truth image examples. However, it is worth exploring how many examples LVLMs can effectively leverage. As noted in    \\S    2.3 , the perspective aspect of our benchmark includes an average of 20.4 ground-truth examples. To investigate further, we perform an analysis focusing on the perspective and others aspects, covering a total of 892 questions. As shown in Figure  5 , we evaluated LLaVA-Next-Interleave using 1, 2, 3, 5, 10, 20 GT examples, averaging the results across three random seeds for sampling the GT examples. LLaVA-Next-Interleave saw the greatest improvement of 5.64% with just one GT example. Performance continued to increase steadily, reaching a peak at 10 GT examples, which was 0.29% higher than with 20 GT examples. One possible explanation could be LLaVA-Next-Interleave may not able to better leverage visually augmented knowledge in long context scenarios. Moreover, the complexity of questions affects the number of images needed too, one ground-truth example sometimes help the model the most on  MRAG-Bench . We encourage the research on adaptatively deciding the number of necessary images based on the complexity of questions.",
            "We overview three lines of related work: 1) multimodal retrieval-augmented generation benchmarks (   \\S    5.1 ), 2) large vision language models (   \\S    5.2 ), and 3) retrieval-augmented mutlimodal large language models (   \\S    5.3 ).",
            "Following  Lu et al. ( 2024b ) , we first use a rule-based automatic tool to extract the exact answer. First, the tool detects if a valid option index appears in the model output. If no direct answer is found, the tool matches the output to the content of each option. If there is still no match, we employ GPT-3.5-turbo to automatically extract the answer following our prompts in Table  5 . If GPT-3.5-turbo finds there is still no match, we will randomly select an option as the answer."
        ]
    },
    "id_table_6": {
        "caption": "Table 7:  LLaVA-Next-Interleave accuracy scores on  MRAG-Bench  with 4 different retrievers.",
        "table": "A3.T6.1.1",
        "footnotes": [],
        "references": [
            "We picked four recent best-performing multimodal retrievers, including CLIP  (Radford et al.,  2021 ) , MagicLens  (Zhang et al.,  2024a ) , E5-V  (Jiang et al.,  2024b ) , VISTA  (Zhou et al.,  2024 )  and evaluated their performance (Recall@5). The detailed retriever performance can be found at Table  6  in Appendix  C . We selected LLaVA-Next-Interleave as the end model to assess its performance. As shown in Figure  5 , when retrievers achieve higher Recall@5 scores (i.e., better retrieved examples), the LVLMs accuracy tends to improve, demonstrating a strong 95% positive correlation. Interestingly, despite similar Recall@5 scores from CLIP and VISTA retrievers, LLaVA-Next-Interleave demonstrated a 2.07% gap in overall accuracy. We conjecture that the order of the correctly retrieved examples may also impact the models final performance. The sensitivity to the order of retrieved examples is a common issue that persists across various models.  Although this phenomenon, known as position bias, has been examined in text-based RAG  (Lu et al.,  2022b ; Wang et al.,  2023 ) , its impact on visual RAG remains unexplored, presenting a promising direction for future research.",
            "Three human annotators in domain conducted the human evaluation. The interface for human evaluation without RAG knowledge and with RAG knowledge are shown in Figure  6  and Figure  7 .",
            "We present the Recall@5 scores per each scenarios on 4 multimodal retreivers as shown in Table  6  and LLaVA-Next-Interleaves accuracy score affected by these retrievers in Table  7 ."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A3.T7.1.1",
        "footnotes": [],
        "references": [
            "Three human annotators in domain conducted the human evaluation. The interface for human evaluation without RAG knowledge and with RAG knowledge are shown in Figure  6  and Figure  7 .",
            "We present the Recall@5 scores per each scenarios on 4 multimodal retreivers as shown in Table  6  and LLaVA-Next-Interleaves accuracy score affected by these retrievers in Table  7 ."
        ]
    },
    "global_footnotes": [
        "https://www.kaggle.com/datasets/jjinho/wikipedia-20230701"
    ]
}