{
    "S3.T1": {
        "caption": "Table 1: Accuracy (in %) of different models and baselines on the Local-QA test-dev split. The QIPBA column indicates the model input: Question, Image, Point, ground truth Bounding box, and/or the set of all correct Answers in the image. Modal-A is a baseline oracle that selects the mode answer among all answers that are correct in this image. Q-only relies only on dataset language priors; Full Img is the original VQA model, not using the point input; Top-score and Smallest take the features of only the highest-scoring or smallest region proposal containing the point respectively (thus restricting the attention to a single region). Our proposed strategy outperforms these alternatives and even performs close to when the ground truth box instead of the point is provided during training and testing (bottom row).",
        "table": "<table id=\"S3.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th id=\"S3.T1.2.1.1.2\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\">Strategy</th>\n<td id=\"S3.T1.2.1.1.3\" class=\"ltx_td ltx_align_left\">QIPBA</td>\n<td id=\"S3.T1.2.1.1.4\" class=\"ltx_td ltx_nopad_l ltx_align_center\">Overall</td>\n<td id=\"S3.T1.2.1.1.5\" class=\"ltx_td ltx_nopad_l ltx_align_center\">Color</td>\n<td id=\"S3.T1.2.1.1.6\" class=\"ltx_td ltx_nopad_l ltx_align_center\">Action</td>\n<td id=\"S3.T1.2.1.1.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">Shape</td>\n</tr>\n<tr id=\"S3.T1.2.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span id=\"S3.T1.2.2.2.1.1\" class=\"ltx_text\">Priors</span></th>\n<th id=\"S3.T1.2.2.2.2\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\">Q-only</th>\n<td id=\"S3.T1.2.2.2.3\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">+ - - - -</td>\n<td id=\"S3.T1.2.2.2.4\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">27.8</td>\n<td id=\"S3.T1.2.2.2.5\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">25.8</td>\n<td id=\"S3.T1.2.2.2.6\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">45.5</td>\n<td id=\"S3.T1.2.2.2.7\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">32.1</td>\n</tr>\n<tr id=\"S3.T1.2.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.3.3.1\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\">Modal-A</th>\n<td id=\"S3.T1.2.3.3.2\" class=\"ltx_td ltx_nopad_l ltx_align_center\">- - - - +</td>\n<td id=\"S3.T1.2.3.3.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">52.2</td>\n<td id=\"S3.T1.2.3.3.4\" class=\"ltx_td ltx_nopad_l ltx_align_center\">52.0</td>\n<td id=\"S3.T1.2.3.3.5\" class=\"ltx_td ltx_nopad_l ltx_align_center\">60.1</td>\n<td id=\"S3.T1.2.3.3.6\" class=\"ltx_td ltx_nopad_l ltx_align_center\">52.8</td>\n</tr>\n<tr id=\"S3.T1.2.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.4.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_t\"></th>\n<th id=\"S3.T1.2.4.4.2\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_t\">Full Img</th>\n<td id=\"S3.T1.2.4.4.3\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">+ + - - -</td>\n<td id=\"S3.T1.2.4.4.4\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">37.4</td>\n<td id=\"S3.T1.2.4.4.5\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">37.2</td>\n<td id=\"S3.T1.2.4.4.6\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">45.4</td>\n<td id=\"S3.T1.2.4.4.7\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_t\">35.9</td>\n</tr>\n<tr id=\"S3.T1.2.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.5.5.1\" class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th id=\"S3.T1.2.5.5.2\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\">Top-score</th>\n<td id=\"S3.T1.2.5.5.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">+ + + - -</td>\n<td id=\"S3.T1.2.5.5.4\" class=\"ltx_td ltx_nopad_l ltx_align_center\">44.8</td>\n<td id=\"S3.T1.2.5.5.5\" class=\"ltx_td ltx_nopad_l ltx_align_center\">44.4</td>\n<td id=\"S3.T1.2.5.5.6\" class=\"ltx_td ltx_nopad_l ltx_align_center\">62.2</td>\n<td id=\"S3.T1.2.5.5.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">43.4</td>\n</tr>\n<tr id=\"S3.T1.2.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Model</th>\n<th id=\"S3.T1.2.6.6.2\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\">Smallest</th>\n<td id=\"S3.T1.2.6.6.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">+ + + - -</td>\n<td id=\"S3.T1.2.6.6.4\" class=\"ltx_td ltx_nopad_l ltx_align_center\">69.2</td>\n<td id=\"S3.T1.2.6.6.5\" class=\"ltx_td ltx_nopad_l ltx_align_center\">69.6</td>\n<td id=\"S3.T1.2.6.6.6\" class=\"ltx_td ltx_nopad_l ltx_align_center\">58.0</td>\n<td id=\"S3.T1.2.6.6.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">56.6</td>\n</tr>\n<tr id=\"S3.T1.2.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.7.7.1\" class=\"ltx_td ltx_th ltx_th_row\"></th>\n<th id=\"S3.T1.2.7.7.2\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row\">Ours</th>\n<td id=\"S3.T1.2.7.7.3\" class=\"ltx_td ltx_nopad_l ltx_align_center\">+ + + - -</td>\n<td id=\"S3.T1.2.7.7.4\" class=\"ltx_td ltx_nopad_l ltx_align_center\"><span id=\"S3.T1.2.7.7.4.1\" class=\"ltx_text ltx_font_bold\">75.0</span></td>\n<td id=\"S3.T1.2.7.7.5\" class=\"ltx_td ltx_nopad_l ltx_align_center\">75.4</td>\n<td id=\"S3.T1.2.7.7.6\" class=\"ltx_td ltx_nopad_l ltx_align_center\">66.4</td>\n<td id=\"S3.T1.2.7.7.7\" class=\"ltx_td ltx_nopad_l ltx_align_center\">56.6</td>\n</tr>\n<tr id=\"S3.T1.2.8.8\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.8.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b\"></th>\n<th id=\"S3.T1.2.8.8.2\" class=\"ltx_td ltx_nopad_l ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\">GT box</th>\n<td id=\"S3.T1.2.8.8.3\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\">+ + - + -</td>\n<td id=\"S3.T1.2.8.8.4\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\">80.2</td>\n<td id=\"S3.T1.2.8.8.5\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\">80.7</td>\n<td id=\"S3.T1.2.8.8.6\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\">67.1</td>\n<td id=\"S3.T1.2.8.8.7\" class=\"ltx_td ltx_nopad_l ltx_align_center ltx_border_b ltx_border_t\">60.4</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Ablation studies. Table 1 reports results on test-dev where we experiment with several different methods for incorporating a point input. Our strategy of removing regions not containing the point and allowing the model to learn attention over the rest achieves at least 5.8% improvement over simpler strategies such as removing all but the smallest or the highest-scoring region proposal containing the point. Improvement is particularly significant on action questions, where other methods perform 12.6% worse."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Results of the Pythia-based global model (Sec. 4.2) on the PointQA-LookTwice test. Rows are the question asked; columns are the disambiguation provided. Prior is a language-only model.",
        "table": "<table id=\"S4.T2.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.2.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row\"></th>\n<th id=\"S4.T2.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\" colspan=\"3\">Spatial Disambiguation</th>\n<th id=\"S4.T2.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" rowspan=\"2\"><span id=\"S4.T2.2.1.1.3.1\" class=\"ltx_text\">Prior</span></th>\n</tr>\n<tr id=\"S4.T2.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.2.2.2.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row\"></th>\n<th id=\"S4.T2.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">None</th>\n<th id=\"S4.T2.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S4.T2.2.2.2.3.1\" class=\"ltx_text ltx_font_bold\">Point</span></th>\n<th id=\"S4.T2.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r\">Box</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">How many of <em id=\"S4.T2.2.3.1.1.1\" class=\"ltx_emph ltx_font_italic\">these</em>…</th>\n<td id=\"S4.T2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">46.1</td>\n<td id=\"S4.T2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">56.5</td>\n<td id=\"S4.T2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">60.2</td>\n<td id=\"S4.T2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">37.8</td>\n</tr>\n<tr id=\"S4.T2.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">\n<table id=\"S4.T2.2.4.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.2.4.2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.4.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">How many of <em id=\"S4.T2.2.4.2.1.1.1.1.1\" class=\"ltx_emph ltx_font_italic\">these</em>\n</td>\n</tr>\n<tr id=\"S4.T2.2.4.2.1.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.4.2.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">[supercategory]…</td>\n</tr>\n</table>\n</th>\n<td id=\"S4.T2.2.4.2.2\" class=\"ltx_td ltx_align_center\">53.1</td>\n<td id=\"S4.T2.2.4.2.3\" class=\"ltx_td ltx_align_center\">59.l</td>\n<td id=\"S4.T2.2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">59.8</td>\n<td id=\"S4.T2.2.4.2.5\" class=\"ltx_td ltx_align_center\">38.6</td>\n</tr>\n<tr id=\"S4.T2.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\">How many [object]…</th>\n<td id=\"S4.T2.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\">62.7</td>\n<td id=\"S4.T2.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">62.8</td>\n<td id=\"S4.T2.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">61.4</td>\n<td id=\"S4.T2.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">40.3</td>\n</tr>\n</tbody>\n</table>\n<table id=\"S4.T2.2.4.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S4.T2.2.4.2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.4.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">How many of <em id=\"S4.T2.2.4.2.1.1.1.1.1\" class=\"ltx_emph ltx_font_italic\">these</em>\n</td>\n</tr>\n<tr id=\"S4.T2.2.4.2.1.1.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.2.4.2.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">[supercategory]…</td>\n</tr>\n</table>\n",
        "footnotes": "",
        "references": [
            "Test accuracy. When trained to answer the question “How many of these are there?” along with a disambiguating point input, the model achieves an accuracy of 56.5%, significantly higher than an image-only version (without the point) at 46.1% and the modal answer (“1”) baseline at 35.4%. As expected it is somewhat behind having access to the object’s bounding box at 60.2%. In Table 2 we demonstrate how the accuracy changes with decreasing the ambiguity of the question: specifying the supercategory (being, vehicle or object) boosts the accuracy from 56.5% to 59.1%, and naming the object class further boosts it to 62.8% (in fact, as expected making the need for spatial supervision irrelevant, as the image-only model achieves 62.7% in this setting).",
            "Need for Visual Grounding. Despite the presence of the point to guide attention, the model must still ground the text in the image (a key challenge in VQA). To verify this, we removed the subject from all questions across the three datasets (e.g. “What color is this shirt?” becomes “What color is this?”), to test if the model indeed benefits from grounding the object word in the image, when given the point input. For PointQA-General, the accuracy (w/ Pythia) drops most significantly from 83.1% to 75.1%; the accuracy for PointQA-LookTwice drops from 62.8% to 56.5% (Table 2), and even for the simpler PointQA-Local setting accuracy drops from 75.0% to 73.5%. Moreover, questions in PointQA-General often refer to objects beyond the one directly pointed to, necessitating grounding of those objects in the image (e.g. for the question “Is this person the farthest from the yellow box?”, the ‘yellow box’ must be grounded in the image). In a small sample of 100 questions from PointQA-General more than half (55) have this characteristic. Thus visual grounding is a necessary feature of the PointQA task."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Accuracy on the PointQA-General dataset. We evaluate the following methods and baselines: (1) Three-Stream and Two-Stream as described in Sec 5.2, (2) Q-only relies only on the language of the question; (3) Image+Q provides the question and visual features {𝐯j}subscript𝐯𝑗\\{\\mathbf{v}_{j}\\} corresponding to the entire image, and (4) Point+Q provides the question and only the features {𝐯jp​t}superscriptsubscript𝐯𝑗𝑝𝑡\\{\\mathbf{v}_{j}^{pt}\\} for the region proposals containing the point (for Pythia this is the local-only model of Sec. 3). Note that all these baselines are outperformed by our proposed models which capture local and global contextual information, and that the highest accuracy is achieved with our three-stream approach on the MCAN model.",
        "table": "<table id=\"S5.T3.6\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.6.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.1.1.1\" class=\"ltx_td\"></td>\n<th id=\"S5.T3.6.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">Pythia</th>\n<th id=\"S5.T3.6.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">MCAN</th>\n<th id=\"S5.T3.6.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column\">LXMERT</th>\n</tr>\n<tr id=\"S5.T3.6.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Q-Only</td>\n<td id=\"S5.T3.6.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">50.00</td>\n<td id=\"S5.T3.6.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\">50.00</td>\n<td id=\"S5.T3.6.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_t\">50.00</td>\n</tr>\n<tr id=\"S5.T3.6.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.3.3.1\" class=\"ltx_td ltx_align_left\">Image+Q</td>\n<td id=\"S5.T3.6.3.3.2\" class=\"ltx_td ltx_align_left\">50.00</td>\n<td id=\"S5.T3.6.3.3.3\" class=\"ltx_td ltx_align_left\">50.00</td>\n<td id=\"S5.T3.6.3.3.4\" class=\"ltx_td ltx_align_left\">50.00</td>\n</tr>\n<tr id=\"S5.T3.6.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.4.4.1\" class=\"ltx_td ltx_align_left\">Point+Q</td>\n<td id=\"S5.T3.6.4.4.2\" class=\"ltx_td ltx_align_left\">81.81</td>\n<td id=\"S5.T3.6.4.4.3\" class=\"ltx_td ltx_align_left\">82.60</td>\n<td id=\"S5.T3.6.4.4.4\" class=\"ltx_td ltx_align_left\">81.33</td>\n</tr>\n<tr id=\"S5.T3.6.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.5.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Two-Stream</td>\n<td id=\"S5.T3.6.5.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\">77.84</td>\n<td id=\"S5.T3.6.5.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\">81.62</td>\n<td id=\"S5.T3.6.5.5.4\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T3.6.5.5.4.1\" class=\"ltx_text ltx_font_bold\">82.41</span></td>\n</tr>\n<tr id=\"S5.T3.6.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.6.6.6.1\" class=\"ltx_td ltx_align_left\">Three-Stream</td>\n<td id=\"S5.T3.6.6.6.2\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.6.6.6.2.1\" class=\"ltx_text ltx_font_bold\">83.12</span></td>\n<td id=\"S5.T3.6.6.6.3\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.6.6.6.3.1\" class=\"ltx_text ltx_font_bold\">83.21</span></td>\n<td id=\"S5.T3.6.6.6.4\" class=\"ltx_td ltx_align_left\">81.71</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Test Results. Results on the PointQA-General dataset are shown in Table 3. The single highest accuracy is achieved using our three-stream approach with the MCAN-based model. Across all models, the three-stream approach outperforms any of the ablations that use one (Q-Only) or two streams (Image+Q, Point+Q). The high overall accuracy of our three-stream approach and its improvement over Point+Q indicates the benefit of adding the point as a separate stream and modifying the model cross-attention to create a rich set of contextual interactions between the streams. The Image+Q and Q-Only models perform no better than random chance since for each image-question pair in Visual7W we generate two questions with opposite answers."
        ]
    }
}