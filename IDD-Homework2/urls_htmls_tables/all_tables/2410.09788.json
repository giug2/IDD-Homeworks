{
    "id_table_1": {
        "caption": "Table 2 :  Comparison with previous multi-instance matting dataset and ours.",
        "table": "S1.1.fig1.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "For MRN, we build a dual-branch network to effectively capture fine-grained local details with global instance-level consideration. As summarized in  Sec.   1 , our DFIMat distinguishes existing works in: (1) supporting multi-types of user inputs; (2) allowing any combination of different input types (including a single input) at each time; (3) with multi-round iteration ability. Those properties make it more user-friendly and with better effectiveness as verified by experiments.",
            "Our extensive experiments verify the superiority of DFIMat over representative methods. Notably, DFIMat outperforms previous SOTA by 3.48 SAD on the challenging SMPMat dataset with higher efficiency. We also provide a more lightweight version, DFIMat-S, with only 33% of the parameters of SOTA methods, while still achieving higher matting accuracy, as shown in  Fig.   1 . By utilizing DFIMat, we also investigate the roles of different input types, providing valuable principles for users on more effective interaction. Our main contributions are:",
            "Here we proposed a decoupled network DFIMat that is of better interpretability and performance. It also enables truly flexible inputs by encoding different types of inputs into a unified visual-semantic space, resulting in a more effective and user-friendly matting experience. We also consider the multi-round interaction characteristic and design a contrastive reasoning module to enhance cross-round refinement. A summary of different methods can be seen in  Sec.   1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 3 :  Quantitative comparison on SMPMat validation set and HIM2K natural set. The MSE metrics are scaled by  10 2 superscript 10 2 10^{2} 10 start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT .",
        "table": "S3.E3",
        "footnotes": [],
        "references": [
            "By reflecting on and summarizing the shortcomings of the existing works, we propose DFIMat, a novel decoupled framework for flexible interactive matting. It consists of two independent components: the interactive semantic capture network (ISCN) and the matting refinement network (MRN), as illustrated in  Fig.   2 . The ISCN is responsible for understanding the user intention from their various inputs, and localizing the interested instance in the image ( Sec.   3.2 ). The MRN takes the prediction result of ISCN (i.e., a coarse mask of the interested instance) as well as the original image, and performs refinement to produce the final alpha matte for the corresponding instance-level matting ( Sec.   3.3 ).",
            "Here we introduce the proposed ISCN, a model that understands scene semantics and multiple/flexible user inputs to localize the target instance, in an interactive manner. The ISCN design takes inspiration from the recent success of multi-modal learning methods   [ 9 ,  11 ,  41 ]  and encodes the different types of user inputs and image into a unified visual-semantic space. Then, strong interactions among them are introduced to better understand the user intentions and then predict the target instance mask for instance localization. Leveraging the characteristic of multi-round interaction, we further design a simple but effective contrastive reasoning module to evaluate the consistency between the model prediction and user intention while also explicitly identifying and reasoning the conflict areas during each rounds interaction, providing valuable auxiliary guidance for the model to further refine its outputs. The overall architecture is shown in  Fig.   2 , which will be introduced in detail.",
            "MRN aims to refine the mask prediction from ISCN to obtain accurate alpha matte predictions. As ISCN has already given a relatively good instance mask as a good beginning, the task difficulty for MRN is largely reduced. Here, we design a simple dual-branch network as our MRN, to capture fine-grained local details while simultaneously considering global instance-level semantics, as shown in  Fig.   2 . Our insight is that images for multi-instance matting tasks often contain complex human interaction and background, so a global encoder is needed to better capture the overall structure and background information, while a local encoder can focus more on details. Then, a subsequent progressive feature fusion should be built to fuse the features and decode them to final matte. Taking those things in mind, we design a simple network containing a global encoder, a local encoder, and a progressive feature fusion module, as shown in   Fig.   2 . Specifically, we choose a CNN as the local encoder, as it can effectively exploit local features, and we choose a transformer-based encoder as our global encoder, as the self-attention operation can build strong non-local interactions in the images to form a better global instance-level understanding. For progressive fusion, we start from the latent feature from the global branch, as it contains rich global instance-level representation. We then fuse it with the feature from local branch in a progressive manner (from low to high resolution), we also utilize PRM  [ 38 ]  to further refine the result.",
            "We propose a synthetic multi-person matting dataset called SMPMat to facilitate the research of instance matting task. To our knowledge, the existing multi-instance dataset from natural images  [ 30 ]  suffers from low data scale as well as diversity. Specifically, HIM2K  [ 30 ]  is the mainly used dataset that focuses on instance-level matting under multi-person scenarios. As can be observed in  Tab.   2 , the HIM2K dataset only contains a limited scale of data that is collected from natural scenes (i.e., only contains 320 natural images with 930 instances in total), making it only serve as a validation set.",
            "The SMPMat dataset.  We followed the above generation pipeline to generate a large-scale multi-person matting dataset SMPMat, in which we carefully select 40,000 high-quality multi-person scene images from our generated samples to form the dataset. We generated diverse text descriptions for the people in each image through GPT-4  [ 1 ] , as the additional text annotations to broaden the usage of the proposed dataset. Compared with existing multi-person matting datasets, the SMPMat dataset shows superiority in both data diversity (see  Tab.   2 ) and image quality (see  Fig.   3 ).",
            "Qualitative comparison.  We follow the same protocol in  Sec.   5.2  to conduct the experiment, as in  Fig.   7 . The red points in (a) indicate the target instances. For our DFIMat, we only give the result from our full model (i.e., mix trained & inference) in (h) due to the space limitation. More comparisons with other variants of DFIMat refer to our supplementary material. From  Fig.   7 , it can be seen that DFIMat can accurately localize the target instance. More importantly, it shows superiority at perceiving fine-grained details: (1) Hair regions across all examples. (2) Other hollow body areas like the fingers or arm in row 2-5."
        ]
    },
    "id_table_3": {
        "caption": "Table 4 :  Analysis of the decoupled design.",
        "table": "S3.E5",
        "footnotes": [],
        "references": [
            "By reflecting on and summarizing the shortcomings of the existing works, we propose DFIMat, a novel decoupled framework for flexible interactive matting. It consists of two independent components: the interactive semantic capture network (ISCN) and the matting refinement network (MRN), as illustrated in  Fig.   2 . The ISCN is responsible for understanding the user intention from their various inputs, and localizing the interested instance in the image ( Sec.   3.2 ). The MRN takes the prediction result of ISCN (i.e., a coarse mask of the interested instance) as well as the original image, and performs refinement to produce the final alpha matte for the corresponding instance-level matting ( Sec.   3.3 ).",
            "In order to obtain a large amount of matting data that contains multi-instance scenes, previous methods  [ 30 ,  17 ]  adopt a simple synthesis strategy to iteratively add portrait foregrounds to no-portrait background. Due to the randomness of the adding positions and the lack of instance-scene prior consideration, there is often a large gap between the synthetic images and natural images, as shown in  Fig.   3 . To fill this gap, we design a new synthetic data generation pipeline that can generate much more diverse and realistic samples, and build a new large-scale dataset SMPMat, which consists of 40,000 realistic multi-instance images with high-quality matte GT.",
            "where   = 0.1  0.1 \\lambda=0.1 italic_ = 0.1 ,  L ^ , A ^ ^ L ^ A \\hat{L},\\hat{A} over^ start_ARG italic_L end_ARG , over^ start_ARG italic_A end_ARG  means the ground truth category and alpha matte respectively. To enable the interpreter training, we collected 400 real images and labeled them with matte ground truth as well as a text description. Once the training finishes, we use the Stable Diffusion to generate realistic images, and use the trained interpreter to obtain the matte ground truth at the same time. To let the Stable Diffusion generate diverse images, we design a prompt to instruct GPT-4  [ 1 ]  to generate an infinite amount of diverse and semantic-rich text descriptions (see supplementary for details), and send them to the Stable Diffusion for text-to-image generation. We give a visual comparison in  Fig.   3 . It can be observed that the image synthesized by existing data synthetic algorithm  [ 30 ]  often lacks of realistic instance lay-out with scene-instance prior consideration, while our new data synthetic pipeline enables a much more realistic data generation. Please also refer to  Sec.   5.4  for quantitative evaluation.",
            "The SMPMat dataset.  We followed the above generation pipeline to generate a large-scale multi-person matting dataset SMPMat, in which we carefully select 40,000 high-quality multi-person scene images from our generated samples to form the dataset. We generated diverse text descriptions for the people in each image through GPT-4  [ 1 ] , as the additional text annotations to broaden the usage of the proposed dataset. Compared with existing multi-person matting datasets, the SMPMat dataset shows superiority in both data diversity (see  Tab.   2 ) and image quality (see  Fig.   3 ).",
            "Single-type user input.  We conducted comparisons of various models on the SMPMat validation set and the HIM2k natural subset, with results listed in  Tab.   3 . Experimental outcomes demonstrate that under single-type input settings, our approach consistently outperforms all state-of-the-art methods. To investigate how different models perform during multiple rounds of interaction, we present the SAD variation curves of various methods during the interactive process in  Fig.   5 . Notably, Our DFIMat also achieves more accurate prediction output with fewer interactions needed.",
            "Multi-type user input.  From the test results in  Tab.   3 , the following conclusions can be drawn: (1) Unlike previous methods that only support one type of input, our model enables mixed types of inputs. Experiments show that when trained under mixed user inputs, the performance of our model can be further enhanced, even with the same single-type input inference. This verifies the benefit of multi-type user input for model training, as they can give more complementary information. (2) As in the bottom line of  Tab.   3 , it can be seen that when applying mixed type inputs during inference, the performance of our model can be further improved. This further verifies the benefit of multi-type user inference, and it also makes user interaction more flexible.",
            "Here we give some quantitative evaluation of our proposed data synthesis pipeline. We compare it with the existing methods  [ 30 ] . We separately apply it and our method to synthesize a same amount of data (i.e., 45k instances) from model training. Then, we apply 3 different methods (i.e., MG  [ 38 ] , MatteFormer  [ 25 ] , and MRN) to train on the synthesized data, and evaluate their testing performance on the HIM2K natural dataset. The result is listed in  Sec.   5.3 , it can be observed that when trained on the data synthesized by our method, the performance of different models is significantly and consistently better than trained on the data generated by existing method  [ 30 ] , which further verifies the superiority of our data synthesis pipeline.",
            "Design choices of the matting refinement network (MRN).   Sec.   5.3  shows that our dual-stream design can enhance the performance."
        ]
    },
    "id_table_4": {
        "caption": "Table 5 :  Effectiveness of CRM.",
        "table": "S4.T2.4",
        "footnotes": [],
        "references": [
            "Specifically, our method is illustrated in  Fig.   4 . During training, given a real image  I I I italic_I  and the paired text description  H H H italic_H , we feed them into a pre-trained text-to-image diffusion model (i.e., Stable Diffusion  [ 27 ]  in our implementation) and acquire the multi-scale latent feature map  F F \\mathcal{F} caligraphic_F  as well as the text-visual cross-attention map  M M \\mathcal{M} caligraphic_M  in it (i.e., the denoising U-Net). We concat those intermediate representations  F ^ = C  o  n  c  a  t  ( [ F , M ] ) ^ F C o n c a t F M \\hat{\\mathcal{F}}=Concat(\\left[\\mathcal{F},\\mathcal{M}\\right]) over^ start_ARG caligraphic_F end_ARG = italic_C italic_o italic_n italic_c italic_a italic_t ( [ caligraphic_F , caligraphic_M ] )  and send  F ^ ^ F \\hat{\\mathcal{F}} over^ start_ARG caligraphic_F end_ARG  to our interpreter and interpret them into GT matte. For the detailed architecture of the interpreter, any decoder for dense prediction task can be used, and here we adopt Mask2Former  [ 4 ] , which contains a transformer decoder and a pixel decoder. Given  F ^ ^ F \\hat{\\mathcal{F}} over^ start_ARG caligraphic_F end_ARG , and  N N N italic_N  learnable queues  { Q 0 , Q 1  ...  Q T } subscript Q 0 subscript Q 1 ... subscript Q T \\{Q_{0},Q_{1}...Q_{T}\\} { italic_Q start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_Q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ... italic_Q start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT }  as input, it outputs  N N N italic_N  foreground alpha matte  A A A italic_A  and their corresponding categories  L L L italic_L  (is a human instance or not). We train the interpreter with binary cross-entropy loss and alpha loss:",
            "where   = 0.1  0.1 \\lambda=0.1 italic_ = 0.1 ,  L ^ , A ^ ^ L ^ A \\hat{L},\\hat{A} over^ start_ARG italic_L end_ARG , over^ start_ARG italic_A end_ARG  means the ground truth category and alpha matte respectively. To enable the interpreter training, we collected 400 real images and labeled them with matte ground truth as well as a text description. Once the training finishes, we use the Stable Diffusion to generate realistic images, and use the trained interpreter to obtain the matte ground truth at the same time. To let the Stable Diffusion generate diverse images, we design a prompt to instruct GPT-4  [ 1 ]  to generate an infinite amount of diverse and semantic-rich text descriptions (see supplementary for details), and send them to the Stable Diffusion for text-to-image generation. We give a visual comparison in  Fig.   3 . It can be observed that the image synthesized by existing data synthetic algorithm  [ 30 ]  often lacks of realistic instance lay-out with scene-instance prior consideration, while our new data synthetic pipeline enables a much more realistic data generation. Please also refer to  Sec.   5.4  for quantitative evaluation."
        ]
    },
    "id_table_5": {
        "caption": "Table 6 :  Ablation study on   the encoder setting of MRN.",
        "table": "S5.T3.5.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "where   = 0.1  0.1 \\lambda=0.1 italic_ = 0.1 ,  L ^ , A ^ ^ L ^ A \\hat{L},\\hat{A} over^ start_ARG italic_L end_ARG , over^ start_ARG italic_A end_ARG  means the ground truth category and alpha matte respectively. To enable the interpreter training, we collected 400 real images and labeled them with matte ground truth as well as a text description. Once the training finishes, we use the Stable Diffusion to generate realistic images, and use the trained interpreter to obtain the matte ground truth at the same time. To let the Stable Diffusion generate diverse images, we design a prompt to instruct GPT-4  [ 1 ]  to generate an infinite amount of diverse and semantic-rich text descriptions (see supplementary for details), and send them to the Stable Diffusion for text-to-image generation. We give a visual comparison in  Fig.   3 . It can be observed that the image synthesized by existing data synthetic algorithm  [ 30 ]  often lacks of realistic instance lay-out with scene-instance prior consideration, while our new data synthetic pipeline enables a much more realistic data generation. Please also refer to  Sec.   5.4  for quantitative evaluation.",
            "Single-type user input.  We conducted comparisons of various models on the SMPMat validation set and the HIM2k natural subset, with results listed in  Tab.   3 . Experimental outcomes demonstrate that under single-type input settings, our approach consistently outperforms all state-of-the-art methods. To investigate how different models perform during multiple rounds of interaction, we present the SAD variation curves of various methods during the interactive process in  Fig.   5 . Notably, Our DFIMat also achieves more accurate prediction output with fewer interactions needed.",
            "Qualitative comparison.  We follow the same protocol in  Sec.   5.2  to conduct the experiment, as in  Fig.   7 . The red points in (a) indicate the target instances. For our DFIMat, we only give the result from our full model (i.e., mix trained & inference) in (h) due to the space limitation. More comparisons with other variants of DFIMat refer to our supplementary material. From  Fig.   7 , it can be seen that DFIMat can accurately localize the target instance. More importantly, it shows superiority at perceiving fine-grained details: (1) Hair regions across all examples. (2) Other hollow body areas like the fingers or arm in row 2-5.",
            "Here we give some quantitative evaluation of our proposed data synthesis pipeline. We compare it with the existing methods  [ 30 ] . We separately apply it and our method to synthesize a same amount of data (i.e., 45k instances) from model training. Then, we apply 3 different methods (i.e., MG  [ 38 ] , MatteFormer  [ 25 ] , and MRN) to train on the synthesized data, and evaluate their testing performance on the HIM2K natural dataset. The result is listed in  Sec.   5.3 , it can be observed that when trained on the data synthesized by our method, the performance of different models is significantly and consistently better than trained on the data generated by existing method  [ 30 ] , which further verifies the superiority of our data synthesis pipeline.",
            "Effect of the decoupled design.  Here we evaluate the following settings:  a) Coupled Network  (ISCN with matting head);  b) Coupled Training  (ISCN + MRN but trained jointly);  c) DFIMat  (Decoupled in both network and training). Tab.  5  shows that both decoupled network and decoupled training have obvious performance gains (in both semantic capture and matting that align with our insight). Besides, the extra complexity from our decoupled design is neglectable (2% in GFLOPS), which validates the effectiveness of our design. We think the reason is that it can make the 2 independent tasks more focused and easier to be optimized, thus leading to a clear performance gain.",
            "Effect of the contrastive reasoning module (CRM).  From  Tab.   5 , it can be seen that with CRM, both semantic capture and matting performance improve by a noticeable margin, which shows the advantages of our design.",
            "Design choices of the matting refinement network (MRN).   Sec.   5.3  shows that our dual-stream design can enhance the performance."
        ]
    },
    "id_table_6": {
        "caption": "Table 7 :  Matting performance comparison under datasets using different data generation schemes.",
        "table": "S5.T5.fig1.3.1",
        "footnotes": [],
        "references": [
            "Analysis on user input choice.  Here we investigate the roles of different input types, aiming to provide valuable principles for users on more effective interaction. We assume only one input per interaction. We use IoU to measure the coarse-grained instance capture accuracy, and SAD to measure the fine-grained matting accuracy. As in Fig.  6 , box input can give a good start as its effectiveness for instance localization, scribble (we observe a similar role but slightly lower performance on click) is useful to refine local details, text is usually not-efficient. As a result, box at round 1 and scribble at the remaining iteration is an optimal choice for efficient user interaction."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S5.T5.fig2.3.1",
        "footnotes": [],
        "references": [
            "Qualitative comparison.  We follow the same protocol in  Sec.   5.2  to conduct the experiment, as in  Fig.   7 . The red points in (a) indicate the target instances. For our DFIMat, we only give the result from our full model (i.e., mix trained & inference) in (h) due to the space limitation. More comparisons with other variants of DFIMat refer to our supplementary material. From  Fig.   7 , it can be seen that DFIMat can accurately localize the target instance. More importantly, it shows superiority at perceiving fine-grained details: (1) Hair regions across all examples. (2) Other hollow body areas like the fingers or arm in row 2-5."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S5.SS3.1.fig1.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "S5.SS3.2.fig2.1.1",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": []
    }
}