{
    "id_table_1": {
        "caption": "Table 1.    Retrieval performance on classical datasets NQ, SQuAD and ELI5. Higher is better, the best one is bolded. Hyperparameters including QAEncoder variants and weight terms   ,    \\alpha,\\;\\beta italic_ , italic_  are optimized simultaneously for six classical datasets. - denotes default or null values.",
        "table": "S4.T1.10.8",
        "footnotes": [],
        "references": [
            "As demonstrated in Fig.  1 , our method initially generates diversified queries (e.g. 5W1H) and then estimates the cluster center using Monte Carlo methods, i.e. mean pooling. The similarity matrix reveals that, compared to both document-query and query-query similarities, the mean-query similarity is significantly higher than both document-query and query-query similarities. Hence, we advocate using the cluster center as a surrogate for the document embedding in QA systems, which bridges the document-query gap robustly without extra index size and retrieval latency.",
            "As shown in Table  1 , for sparse retrievers, Doc2Query exhibits some improvements over the standard BM25 method. However, the improvements are not as significant as DeepImpact. Conversely, the DeepImpact method, which builds on Doc2Query and incorporates BERT for term weight assignment, achieves better performance across all metrics and datasets. This not only suggests that non-neural retrievers have limited alignment capability, but also highlights the importance of weight adjustment of document and query information.",
            "The theorem provides a robust theoretical foundation for the QAEncoders capabilities in computing the similarity between documents and queries.  These concentration inequalities in equation  7  and equation  8  show that cosine similarities between  q q \\mathbf{q} bold_q ,  d d \\mathbf{d} bold_d , and    \\mu italic_  are concentrated around their expected values.  Inequality  9  indicates that the similarity between the  q q \\mathbf{q} bold_q  and    \\mu italic_  is always greater than between  q q \\mathbf{q} bold_q  and  d d \\mathbf{d} bold_d . This confirms that using the mean vector as a projection in QAEncoder better captures the semantic relationship between queries and documents. This theoretical result aligns with the experimental findings  1 , further validating QAEncoders effectiveness ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.    Retrieval performance on the latest datasets FIGNEWS and CRUD-RAG. Higher is better, the best one is bolded. Hyperparameters including QAEncoder variants and weight terms   ,    \\alpha,\\;\\beta italic_ , italic_  are optimized simultaneously for all lastest datasets. - denotes default or null values.",
        "table": "S4.T2.10.8",
        "footnotes": [],
        "references": [
            "As illustrated in Table  2 , for the latest datasets, QAEncoder significantly enhances the alignment of user queries with relevant documents across both state-of-the-art embedding models and other popular open-source models. For instance, the gte-multilingual-base models MRR metric on the FIGNEWS(English) dataset increases from 65.3 to 75.3. Similarly, the mContriever models MRR on the FIGNEWS(English) dataset improves from 32.8 to 61.2. Besides, the text2vec-base-multilingual models MRR on the CRUD-RAG dataset rose from 12.1 to 55.3. These results confirm QAEncoders remarkably generalized alignment capability across various embedding models and multilingual datasets.",
            "Since  q q \\mathbf{q} bold_q  is Gaussian, based on Lemma  2 ,the inner product  q   d superscript q top d \\mathbf{q}^{\\top}\\mathbf{d} bold_q start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_d  is a linear transformation of  q q \\mathbf{q} bold_q  and hence is a normal distribution with mean     d superscript  top d \\mu^{\\top}\\mathbf{d} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_d  and variance  d     d superscript d top  d \\mathbf{d}^{\\top}\\Sigma\\mathbf{d} bold_d start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT roman_ bold_d . Given that    \\mu italic_  and  d d \\mathbf{d} bold_d  are unit vectors and the angle between    \\mu italic_  and  d d \\mathbf{d} bold_d  is    \\theta italic_ , we have:",
            "Given that  q  N  (  ,  ) similar-to q N   \\mathbf{q}\\sim\\mathcal{N}(\\mu,\\Sigma) bold_q  caligraphic_N ( italic_ , roman_ ) , based on Lemma  2 , the inner product  q    superscript q top  \\mathbf{q}^{\\top}\\mu bold_q start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT italic_ ,  representing the cosine of the angle between  q q \\mathbf{q} bold_q  and    \\mu italic_   is a linear transformation of a Gaussian random vector with mean:"
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Performance comparison of QAEncoder variants on the latest datasets FIGNEWS and CRUD-RAG. Higher is better, the best one is bolded. Hyperparameters are optimized simultaneously across all the latest datasets.  n n n italic_n  indicates the number of predicted queries in QA naive .",
        "table": "S4.T3.10.8",
        "footnotes": [],
        "references": [
            "We present the performance comparison of QAEncoder variants on two state-of-the-art multilingual embedding models in Table  3 .  Generally, QAE hyb  and QAE emb  outperform the QAE txt  and QAE naive  approaches.  For instance, for the bge-m3 model, QAE hyb  consistently outperforms other variants. Conversely, the multilingual-e5-large model performs best with QAE emb . However, the best performance differences between QAE emb , QAE txt , and QAE hyb  are not substantial, demonstrating the robustness of our approach to hyperparameter variations.  Regarding QAE naive , it evidently underperforms other ablations, despite storing 10 times the number of embedding vectors. This leads to unacceptable storage management overhead and recall latency in large-scale production systems.  We also provide some more granular ablation experiments in Figure  4  and the appendix.",
            "Single-cluster sub-hypothesis verification.   As illustrated in Fig.  3 (a), we validate the single-cluster sub-hypothesis by visualizing the embedding space using t-SNE dimensionality reduction techniques. This visualization displays that the predicted queries for each document form distinct and cohesive clusters (different colored). And these clusters are notably distant from the clusters of other documents, thereby supporting the single-cluster sub-hypothesis.",
            "Perpendicular sub-hypothesis verification.   To further assess the perpendicular sub-hypothesis, let  v d = E  ( d )  E  [ E  ( Q  ( d ) ) ] subscript v d E d E delimited-[] E Q d v_{d}=\\mathcal{E}(d)-\\mathbb{E}[\\mathcal{E}(\\mathcal{Q}(d))] italic_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT = caligraphic_E ( italic_d ) - blackboard_E [ caligraphic_E ( caligraphic_Q ( italic_d ) ) ]  and  v q i = E  ( q i )  E  [ E  ( Q  ( d ) ) ] subscript v subscript q i E subscript q i E delimited-[] E Q d v_{q_{i}}=\\mathcal{E}(q_{i})-\\mathbb{E}[\\mathcal{E}(\\mathcal{Q}(d))] italic_v start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT = caligraphic_E ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - blackboard_E [ caligraphic_E ( caligraphic_Q ( italic_d ) ) ]  be the vectors from the cluster center to the document embedding and the individual query embedding, respectively. As illustrated in Fig.  3 (b), the degree distribution between vector  v d subscript v d v_{d} italic_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  and vector  v q i subscript v subscript q i v_{q_{i}} italic_v start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT  exhibits a bell-shaped curve. The mean value is slightly less than 90 degrees, and the primary range of distribution lies between 75 and 100 degrees, which confirms that  v d subscript v d v_{d} italic_v start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT  is approximately orthogonal to each  v q i subscript v subscript q i v_{q_{i}} italic_v start_POSTSUBSCRIPT italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT  and can be regarded as the normal vector to some hyperplane  H H \\mathcal{H} caligraphic_H .",
            "Conical distribution in unit sphere demonstration.   Finally, we illustrate the highly simplified conical distribution hypothesis within the unit sphere embedding space, as most embedding models utilize normalized embedding vectors. As depicted in Fig.  3 (c), the embeddings of potential queries form a cluster on the surface of the unit sphere, with each point color-coded. The center of the cluster is indicated by a star, while the document embedding is represented by a black point positioned above the cluster. It is evident that these elements form a distorted cone, aligning with the above hypothesis and the degree distribution experiment.",
            "In this subsection, we further substantiate the original hypothesis that the potential queries adhere to a Gaussian distribution:  For any document  d d d italic_d , the potential queries in the embedding space approximately follow a Gaussian distribution, characterized by a mean    \\mu italic_  and covariance matrix    \\Sigma roman_ . Refer to  A.6.3  for detailed validation.",
            "We employ the Chi-Squared Q-Q Plot to assess whether the squared Mahalanobis distances  3  conform to the chi-squared distribution. By leveraging Lemma  4 , We compare the observed  D 2 superscript D 2 D^{2} italic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  values with the theoretical quantiles of the chi-squared distribution to assess the conformity of the data to a high-dimensional Gaussian model.  We conclude that close alignment of the sample points along the 45-degree reference line indicates support for the original hypothesis."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Performance comparison of QAEncoder against training-based and document-centric methods on the latest datasets FIGNEWS and CRUD-RAG. Higher is better, the best one is bolded. Hyperparameters   ,    \\alpha,\\;\\beta italic_ , italic_  are optimized simultaneously across the two latest datasets.  n n n italic_n  denotes the number of pseudo-documents in HyDE. - indicates default or null values.",
        "table": "S4.T4.8.4",
        "footnotes": [],
        "references": [
            "Hybrid fingerprint -   QAE hyb subscript QAE hyb \\text{{QAE}}_{\\text{{hyb}}} QAE start_POSTSUBSCRIPT hyb end_POSTSUBSCRIPT .  The hybrid approach, QAE hyb , seeks to combine the benefits of both the embedding and textual fingerprints for more sophisticated and nuanced document representation.  Although QAE emb  combines the document embedding  E  ( d ) E d \\mathcal{E}(d) caligraphic_E ( italic_d )  and the cluster center  E  [ E  ( Q  ( d ) ) ] E delimited-[] E Q d \\mathbb{E}[\\mathcal{E}(\\mathcal{Q}(d))] blackboard_E [ caligraphic_E ( caligraphic_Q ( italic_d ) ) ]  through linear interpolation, inherent differences between these embeddings suggest that a simple linear interpolation should be suboptimal. Therefore, we explore the potential of substituting the document embedding  E  ( d ) E d \\mathcal{E}(d) caligraphic_E ( italic_d )  in Equation  4  with QAE txt , which fuses the semantics of both documents and queries.",
            "Note that a more straightforward alternative can be  QAE hyb  ( d ) = ( 1   )  E  ( d ) +   QAE txt  ( d ) subscript QAE hyb d  1  E d   subscript QAE txt d \\text{QAE}_{\\text{hyb'}}(d)=(1-\\alpha)\\cdot\\mathcal{E}(d)+\\alpha\\cdot\\text{QAE%  }_{\\text{txt}}(d) QAE start_POSTSUBSCRIPT hyb end_POSTSUBSCRIPT ( italic_d ) = ( 1 - italic_ )  caligraphic_E ( italic_d ) + italic_  QAE start_POSTSUBSCRIPT txt end_POSTSUBSCRIPT ( italic_d ) , which replaces  QAE base subscript QAE base \\text{QAE}_{\\text{base}} QAE start_POSTSUBSCRIPT base end_POSTSUBSCRIPT  in Equation  4  with  QAE txt subscript QAE txt \\text{QAE}_{\\text{txt}} QAE start_POSTSUBSCRIPT txt end_POSTSUBSCRIPT  that integrates more document information. However, this substitution faces insufficient query semantics and limited improvements.",
            "We present the performance comparison of QAEncoder variants on two state-of-the-art multilingual embedding models in Table  3 .  Generally, QAE hyb  and QAE emb  outperform the QAE txt  and QAE naive  approaches.  For instance, for the bge-m3 model, QAE hyb  consistently outperforms other variants. Conversely, the multilingual-e5-large model performs best with QAE emb . However, the best performance differences between QAE emb , QAE txt , and QAE hyb  are not substantial, demonstrating the robustness of our approach to hyperparameter variations.  Regarding QAE naive , it evidently underperforms other ablations, despite storing 10 times the number of embedding vectors. This leads to unacceptable storage management overhead and recall latency in large-scale production systems.  We also provide some more granular ablation experiments in Figure  4  and the appendix.",
            "Training-based and query-centric methods operate at training time and indexing time, respectively. Therefore, integrating these approaches cloud lead to more improvements.  As illustrated in Table  4 , both types of fine-tuned models significantly benefit from the QAEncoder. For instance, the mContriever-msmarco model improves MRR from 70.1 to 77.2 on FIGNEWS(Arabic); the multilingual-e5-large-instruct models MRR increases 8.8 and 9.1 MRR points on the FIGNEWS(English) and CRUD-RAG(Chinese) datasets, respectively.",
            "For document-centric methods, we investigate the widely-reported hallucination phenomenon on latest datasets  (Wang et al . ,  2023a ; Gao et al . ,  2023 ; Kim and Min,  2024 ) . Table  4  shows the recall performance heavily decreases for all the latest datasets and both models, attributed to the hallucination of pseudo-documents. Besides, the LLM invocation for pseudo-documents is both time-consuming and costly. In our case, the time for single LLM invocation is greater than 2000ms while the time for vector search is less than 10ms. These highlight the irreplaceable importance and practicality of QAEncoder method.",
            "We employ the Chi-Squared Q-Q Plot to assess whether the squared Mahalanobis distances  3  conform to the chi-squared distribution. By leveraging Lemma  4 , We compare the observed  D 2 superscript D 2 D^{2} italic_D start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  values with the theoretical quantiles of the chi-squared distribution to assess the conformity of the data to a high-dimensional Gaussian model.  We conclude that close alignment of the sample points along the 45-degree reference line indicates support for the original hypothesis."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Complete retrieval performance across six classical datasets: NQ, SQuAD, ELI5, HotPotQA, MSMARCO, and TriviaQA. Higher is better, the best one is bolded. Hyperparameters including QAEncoder variants and weight terms   ,    \\alpha,\\;\\beta italic_ , italic_  are optimized simultaneously for six classical datasets. - denotes default or null values.",
        "table": "A1.T5.10.8",
        "footnotes": [],
        "references": [
            "In this subsection, we formally define the highly simplified conical distribution hypothesis and validates its reasonableness with empirical analysis detailed in the appendix  A.5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  Comprehensive retrieval performance on the latest datasets FIGNEWS and CRUD-RAG. Higher is better, the best one is bolded. Hyperparameters including QAEncoder variants and weight terms   ,    \\alpha,\\;\\beta italic_ , italic_  are optimized simultaneously for six latest datasets. - denotes default or null values.",
        "table": "A1.T6.10.8",
        "footnotes": [],
        "references": [
            "In this subsection, we further substantiate the original hypothesis that the potential queries adhere to a Gaussian distribution:  For any document  d d d italic_d , the potential queries in the embedding space approximately follow a Gaussian distribution, characterized by a mean    \\mu italic_  and covariance matrix    \\Sigma roman_ . Refer to  A.6.3  for detailed validation."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Complete performance comparison of QAEncoder variants on latest datasets FIGNEWS and CRUD-RAG. Higher is better, the best one is bolded. Hyperparameters are optimized simultaneously across the six latest datasets.  n n n italic_n  indicates the number of predicted queries in QA naive .",
        "table": "A1.T7.10.8",
        "footnotes": [],
        "references": [
            "The theorem provides a robust theoretical foundation for the QAEncoders capabilities in computing the similarity between documents and queries.  These concentration inequalities in equation  7  and equation  8  show that cosine similarities between  q q \\mathbf{q} bold_q ,  d d \\mathbf{d} bold_d , and    \\mu italic_  are concentrated around their expected values.  Inequality  9  indicates that the similarity between the  q q \\mathbf{q} bold_q  and    \\mu italic_  is always greater than between  q q \\mathbf{q} bold_q  and  d d \\mathbf{d} bold_d . This confirms that using the mean vector as a projection in QAEncoder better captures the semantic relationship between queries and documents. This theoretical result aligns with the experimental findings  1 , further validating QAEncoders effectiveness ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8.    The table illustrates a comprehensive performance comparison of QAEncoder against training-based and document-centric methods on the latest datasets: FIGNEWS and CRUD-RAG.  Higher is better, the best one is bolded. Hyperparameters   ,    \\alpha,\\;\\beta italic_ , italic_  are optimized simultaneously across the six latest datasets.  n n n italic_n  denotes the number of pseudo-documents in HyDE. - indicates default or null values.",
        "table": "A1.T8.8.4",
        "footnotes": [],
        "references": [
            "The theorem provides a robust theoretical foundation for the QAEncoders capabilities in computing the similarity between documents and queries.  These concentration inequalities in equation  7  and equation  8  show that cosine similarities between  q q \\mathbf{q} bold_q ,  d d \\mathbf{d} bold_d , and    \\mu italic_  are concentrated around their expected values.  Inequality  9  indicates that the similarity between the  q q \\mathbf{q} bold_q  and    \\mu italic_  is always greater than between  q q \\mathbf{q} bold_q  and  d d \\mathbf{d} bold_d . This confirms that using the mean vector as a projection in QAEncoder better captures the semantic relationship between queries and documents. This theoretical result aligns with the experimental findings  1 , further validating QAEncoders effectiveness ."
        ]
    },
    "global_footnotes": [
        "The first two authors have equal contributions.",
        "Corresponding Author.",
        "The first author completed this work during an internship at IAAR.",
        "LlamaIndex adopts the same common practice and provides templated workflow, i.e. generating queries for recall and rerank evaluation. See",
        "."
    ]
}