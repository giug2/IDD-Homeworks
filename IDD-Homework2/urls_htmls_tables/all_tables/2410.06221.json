{
    "id_table_1": {
        "caption": "TABLE I:  Datasets proposed in the state-of-the-art for the microphone model identification task applied to smartphones.",
        "table": "S2.T1.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "We structured the recording sessions in two batches, each involving  10 10 10 10  models, to guarantee that the acquisition conditions were the same for all smartphones and to eliminate differences due their positioning relative to the loudspeaker. During the sessions, the phones were positioned approximately  3   m times 3 meter 3\\text{\\,}\\mathrm{m} start_ARG 3 end_ARG start_ARG times end_ARG start_ARG roman_m end_ARG  away from the loudspeaker on a suspended structure to mitigate unwanted vibrations and ensure far-field recording conditions.  Figure   1  shows a picture of the considered recording setup."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  List of the recorded smartphone models.",
        "table": "S2.T1.1.1.1.8.1",
        "footnotes": [],
        "references": [
            "First, we focus our analysis on the recordings of sine sweep signals. We do so since these recordings, although brief, include highly informative content regarding the microphone characteristics of the considered smartphones.  Figure   2  shows the waveforms of the recorded chirps, while  Figure   3  shows the spectra of the  IRs  extracted from them."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  List of the considered speakers from the VCTK corpus.",
        "table": "S3.T2.1",
        "footnotes": [],
        "references": [
            "First, we focus our analysis on the recordings of sine sweep signals. We do so since these recordings, although brief, include highly informative content regarding the microphone characteristics of the considered smartphones.  Figure   2  shows the waveforms of the recorded chirps, while  Figure   3  shows the spectra of the  IRs  extracted from them."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S3.T3.1.1",
        "footnotes": [],
        "references": [
            "During the analysis, we noticed a particular behavior exhibited by two Xiaomi phones, namely the Xiaomi 12 and Xiaomi 12 Pro. Despite their capability to capture audio data at  44.1   kHz times 44.1 kilohertz 44.1\\text{\\,}\\mathrm{kHz} start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG , these models clip all the content above  8   kHz times 8 kilohertz 8\\text{\\,}\\mathrm{kHz} start_ARG 8 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG , as if acquiring at  f s = subscript f s absent f_{\\text{s}}= italic_f start_POSTSUBSCRIPT s end_POSTSUBSCRIPT =   16   kHz times 16 kilohertz 16\\text{\\,}\\mathrm{kHz} start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG . This characteristic is even more evident by looking at  Figure   4 , which shows the Mel-spectrogram representations of the noisy speech signals recorded by all the considered smartphones. In the case of these two models, a black horizontal band shows the lack of content above  8   kHz times 8 kilohertz 8\\text{\\,}\\mathrm{kHz} start_ARG 8 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG . Interestingly, this particular behavior is not present in the recordings of Xiaomi 12X, which is produced by the same manufacturer.",
            "Another notable aspect observed in  Figure   4  belongs to Moto G9 Power. As the recorded signal is injected with  AWGN , we expect to observe content across the entire frequency range at each time instant. However, it seems like this model includes a  Voice Activity Detector (VAD)  mechanism and puts to zero all the time windows where no speech is detected. This behavior may prove advantageous or disadvantageous, depending on the purpose of the recording. While it discards content of un-voiced segments, enhancing clarity in acquired speech, it also discards potentially valuable audio information. As in the previous case, this behavior belongs only to this specific model and is not extended to other smartphones from the same manufacturer (i.e., Motorola Edge 20 and Motorola Edge 30). We recall that we considered the same recording pipeline for all the models, meaning that these behaviors are likely due to hardware filtering or post-processing operations specific to each model."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S3.T3.2.1.1",
        "footnotes": [],
        "references": [
            "Figure   5  shows the results of this analysis considering balanced accuracy as a metric. We observe a significant change in performance trends when considering different sampling frequencies. The classifier trained and tested on  44.1   kHz times 44.1 kilohertz 44.1\\text{\\,}\\mathrm{kHz} start_ARG 44.1 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG  data demonstrates nearly perfect accuracy, reaching almost 100% in all scenarios except when trained on only 10% of the training set, where performance drops by  5 5 5 5 %. Conversely, the classifier that considers  16   kHz times 16 kilohertz 16\\text{\\,}\\mathrm{kHz} start_ARG 16 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG  data achieves a maximum accuracy of  92 92 92 92 % and experiences a substantial drop in performance as the training set size decreases. This indicates that the critical audio content for recording smartphone model identification is likely found in the frequency bands above  8   kHz times 8 kilohertz 8\\text{\\,}\\mathrm{kHz} start_ARG 8 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG , enabling the first classifier to perform exceptionally well even with limited training samples. In contrast, the second classifiers task is more challenging, necessitating more training data to enhance its classification performance."
        ]
    }
}