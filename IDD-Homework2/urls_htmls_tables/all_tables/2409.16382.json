{
    "id_table_1": {
        "caption": "Table 1:  Adult Pain Recognition Databases.    \\star   denotes no longer available datasets.    \\dagger   denotes datasets that are not specifically focused on pain. UNBC-McMaster SPEA  (Lucey et al.,  2011 )  BioVid Heat   (Walter et al.,  2013b )  BP4D-Spontaneous  (Zhang et al.,  2014 )  BP4D+  (Zhang et al.,  2016b )  MIntPAIN  (Tobon et al.,  2020 )",
        "table": "S2.T1.7",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "These factors contribute to the scarcity of publicly available pain datasets, limiting the diversity and volume of data necessary for training deep learning models   (Zhang et al.,  2013 ; Walter et al.,  2013b ; Zhang et al.,  2016a ) . Consequently, data scarcity can hinder model performance, making them fragile in scenarios involving underrepresented demographics. We display currently existing datasets for pain recognition in Table 1 . Here we can notice the general limitation in number of participants."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Pain recognition results on the  BioVid Heat Pain dataset . We consider models trained on only real data (Real to Real), only synthetic data with 10 different textures per patient (Synth to Real), and a mix of both (Mixed to Real).",
        "table": "S5.T2.3",
        "footnotes": [],
        "references": [
            "Our pipeline for generating synthetic head videos builds on existing datasets such as the BioVid Heat Pain Database  Walter et al. ( 2013a ) . Our approach integrates advanced methods in 3D mesh generation, texture mapping, and rendering to produce a diverse set of synthetic head videos for pain recognition research. The outline of this process is displayed in  2 .",
            "This section presents the results of our experiments, evaluating the performance of pain recognition models trained on different datasets: real, synthetic, and a combination of both. Subsequently, we provide an analysis of design decisions regarding the generation of synthetic data. The evaluation metrics used are Area Under the Receiver Operating Characteristic Curve (AUROC), F1-Score, and Accuracy. These metrics provide a comprehensive overview of the models capabilities in handling the BioVid Heat Pain dataset, as detailed in Table  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Training result with  synthetic data as validation set  on the epoch with the highest AUROC-Score",
        "table": "S5.T3.3",
        "footnotes": [],
        "references": [
            "Texture mapping is performed via the FFHQ-UV repository  Bai et al. ( 2023 ) , which provides a robust framework for generating high-resolution facial textures. The process begins with the detection of facial landmarks to ensure accurate texture application. A synthetic face is initially created from these landmarks, and high-resolution textures are then applied to this base mesh. To address common issues such as blank eye textures, we incorporate additional mapping refinements that enhance the realism of the synthetic heads. These refinements are essential for creating diverse and visually compelling head textures that reflect variations in age, gender, and ethnicity, as sourced from CelebV-HQ  Zhu et al. ( 2022 ) . We display how final translations might look in Fig.  3 .",
            "Table  3  evaluates model performance on synthetic datasets with varying texture complexity and patient representations. The  10 Textures per Patient  configuration achieved the highest AUROC (0.655) and F1-Score (0.799), illustrating the positive impact of extensive texture diversity on pain recognition. The baseline  Only Mesh  setup showed that models relying solely on mesh geometry achieved a lower AUROC of 0.624, highlighting the limitations of texture-free models. As texture diversity increased, there was a notable improvement in both AUROC and accuracy metrics, demonstrating that richer texture information enhances the models ability to capture nuanced pain indicators. Notably, the  10 Textures per Patient  configuration resulted in the best overall performance, underscoring synthetic data capability to complement traditional pain recognition methods and improve generalization across varied scenarios."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Training result with  synthetic data as validation set  on the epoch with the highest AUROC-Score",
        "table": "S5.T4.3",
        "footnotes": [],
        "references": [
            "Table  4  summarizes the performance of models trained on synthetic data across different view configurations. Both the Front View and Side View models yielded identical metrics: AUROC of 0.641, F1-Score of 0.786, and Accuracy of 0.658, indicating that each view independently offers a similar level of information for pain recognition, and thus are both helpful in their own way. The Multiple Views configuration using both views achieved the highest performance with an AUROC of 0.653, F1-Score of 0.799, and Accuracy of 0.666. This demonstrates the enhanced generalization capability afforded by combining diverse perspectives, suggesting that multi-view synthetic data significantly improves the models ability to capture pain-related expressions."
        ]
    }
}