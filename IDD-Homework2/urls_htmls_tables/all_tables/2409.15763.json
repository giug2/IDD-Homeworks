{
    "id_table_1": {
        "caption": "Table 1:   IRSC Benchmark Results of S-Arctic Series, BGE Series, GTE Series, M3E Series, and MiniLM Series in All Languages for All Tasks.  Metrics:  r@10 - Recall at 10, m@10 - MRR(Mean Reciprocal Rank) at 10, n@10 - nDCG(Normalized Discounted Cumulative Gain) at 10",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "Figure  1  provides an overview of tasks and datasets available in IRSC. The benchmark consists of the following five task types:",
            "Based on the results in Table  1 , we observe that the BGE-M3 model consistently outperforms other models across all metrics and categories, indicating its robustness and effectiveness in both Chinese and English retrieval tasks. Specifically, BGE-M3 achieves the highest recall at 10 (r@10), mean average precision at 10 (m@10), and normalized discounted cumulative gain at 10 (n@10) in the Keywords, Title, Query, Part, and Summary categories. For instance, in the Keywords category, BGE-M3 has an impressive r@10 of 0.8668, m@10 of 0.8205, and n@10 of 0.8320.",
            "The diverse performance across different models highlights the importance of selecting the appropriate model based on the specific retrieval task and the language requirements. Table  1  presents the results for Mixed-Language task. The results for the Chinese and English tasks will publish in our Github repository."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:   IRSC Benchmark Results of the S-Arctic Series, BGE Series, GTE Series, M3E Series, and MiniLM Series in Cross Languages.  Metrics:  recall@10",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "To more intuitively showcase the performance of different models, we created radar charts 2  where the values for each capability are derived from the average of  r  @  10 r @ 10 r@10 italic_r @ 10 ,  m  @  10 m @ 10 m@10 italic_m @ 10 , and  n  @  10 n @ 10 n@10 italic_n @ 10 . These charts provide a clearer view of the comprehensive performance of each model across various tasks.",
            "In Table  2  , we also conducted experiments on the cross-lingual retrieval capabilities of different models using five IRSC tasks, with 1,000 randomly selected queries for each task. We obtained 5,000 data entries in both English and Chinese languages. Queries originally in English were translated into the target language (Chinese) and then searched within an entirely English database to obtain the Chinese to English (C2E) results in Table  2 . The scores are the averages of r@10, m@10, and n@10."
        ]
    },
    "global_footnotes": []
}