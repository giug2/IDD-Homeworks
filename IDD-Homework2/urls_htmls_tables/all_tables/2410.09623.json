{
    "id_table_1": {
        "caption": "Table 1:  Aggregate statistics of our French corpora and similar English insurance QA corpora introduced in  Section 2 . Avg stands for average, LW for lexical words.",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "Table 1  presents some key statistics of our French corpora and similar English insurance QA corpora introduced in  Section 2 .  For the English insurance QA corpora, we have used their latest official version 5 5 5 https://github.com/shuzi/insuranceQA ,  https://huggingface.co/datasets/umarbutler/open-australian-legal-qa .  All statistics were computed using SpaCys latest language-specific tokenizer  Honnibal et al. ( 2020 ) .  They exclude new lines ( \\n ), whitespaces, punctuations and some special characters ( < ,  > ,  |  and  $ ).  Moreover, to evaluate the reading complexity level of the contracts, we compute readability scores using the frequently used Flesch-Kincaid formula  Flesch ( 1948 ) . It computes a score using a scale from 0 (hardest) to 100 (easier) to assess the readability level.  We will first analyze our reference corpus and then compare our QA corpus with similar corpora using  Table 1 .",
            "In  Table 1  (left side), we see that all four sources share relatively similar statistics.  Indeed, the average number of lexical words (LW), average sentence lengths (both), and average number of sentences are relatively similar.  Moreover, since legal documents are known to be complex and lengthy and to use specialized vocabulary  Katz et al. ( 2023 ) , we can see that the average number of tokens, lexical richness and average Flesch-Kincaid score are lower than the two other types of documents.",
            "We can see in  Table 1  (right side) that our QA corpus shares similar patterns to the other corpora.  Indeed, for all corpora, the questions use less than half the vocabulary size as the answers and are half as long in terms of tokens, LW, number of sentences, and average sentence length as the answers.  They are also easier to read than the answers based on the Flesch-Kincaid score.  However, ours is significantly smaller compared to other similar corpora due to its nature.  Indeed, the other two similar corpora focus on the broader insurance domain.  For example, Insurance QA includes questions about all types of insurance (property, life, and health) throughout the USA.  In contrast, our corpus focuses on a single insurance product for a single province in Canada.",
            "This section details our methodology for leveraging a large language model (LLM) to answer insurance questions.  Our choice of architecture is similar to  Louis et al. ( 2024 ) ,  Ajmi ( 2024 ) , and  Wiratunga et al. ( 2024 ) . We use a RAG architecture to inject domain expertise into an LLM generation for QA.  Like the previous authors, our RAG architecture is inspired by the concept of advanced RAG  Gao et al. ( 2023 ) , an architecture that adds a pre- and post-retrieval steps to the traditional processing. Our architecture was built using  LangChain   Chase ( 2022 ) , a Python framework that consolidates the various components of the RAG architecture.  As illustrated in  Figure 1 , first, a retriever selects a small subset of insurance documents from our reference corpus ( red ), some relevant to the question and some not.  Then, a generator conditions its answer on the subset of articles returned by the retriever ( blue ). We describe these two components in details in the following subsections."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Our evaluation grading scale to evaluates a machine-generated answer using a set of criteria.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "This paper is outlined as follows: first, we study the relevant questions-answering legal RAG research and its related corpora in  Section 2 .  Then, we propose our corpora in  Section 3 , and in  Section 4 ,  Section 5  and  Section 6  we present a set of experiments  aimed at evaluating the performances of GPT-4o at answering Quebec automobile insurance questions.  Finally, in  Section 7 , we conclude and discuss our future work.",
            "Table 1  presents some key statistics of our French corpora and similar English insurance QA corpora introduced in  Section 2 .  For the English insurance QA corpora, we have used their latest official version 5 5 5 https://github.com/shuzi/insuranceQA ,  https://huggingface.co/datasets/umarbutler/open-australian-legal-qa .  All statistics were computed using SpaCys latest language-specific tokenizer  Honnibal et al. ( 2020 ) .  They exclude new lines ( \\n ), whitespaces, punctuations and some special characters ( < ,  > ,  |  and  $ ).  Moreover, to evaluate the reading complexity level of the contracts, we compute readability scores using the frequently used Flesch-Kincaid formula  Flesch ( 1948 ) . It computes a score using a scale from 0 (hardest) to 100 (easier) to assess the readability level.  We will first analyze our reference corpus and then compare our QA corpus with similar corpora using  Table 1 .",
            "As shown in  Figure 2  and  Figure 3 , we have used two prompts for our experiment. The first ( Figure 2 ) is a zero-shot prompt where the LLM is simply asked to answer the question. The second ( Figure 3  is a domain-specific prompt that gives additional information to support the LLM.  In each prompt,  {input}  corresponds to the question, and  {context}  to the retrieved references.",
            "To discern the strengths and shortcomings of our generator with or without using an RAG architecture, we conduct a detailed manual analysis of all question-answer pairs.  Inspired by  Chartier et al. ( 2024 )  and  Baray et al. ( 2024 ) , we, in partnership with our insurance partner, have developed an evaluation guide with an exam-like setup to evaluate each pair.Based on the expert answers, we defined a set of criteria, or key elements that a machine-generated answer must include.  To evaluate each criterion, we developed a grading scale inspired by the one used by  Chartier et al. ( 2024 )  and  Baray et al. ( 2024 ) ; this scale is presented in  Table 2 . In case of a false answer to a criterion, we penalize the score with a negative point since an erroneous answer could mislead the customer or hinder their understanding of an insurance product. On the other hand, a complete answer to a criterion results in the maximum score of 2 points.  In total, 288 criteria have been extracted from the human answers.  On average, each question has 3.51 criteria with a standard deviation of 1.75. The maximum grade a system can receive is  288  2 = 576 288 2 576 288\\times 2=576 288  2 = 576  points, and the lowest is   288 288 -288 - 288  points when a system always gives a false answer."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Automatic metrics (left) average and one standard deviation over the five restarts on our questions-answering corpus and manual (right) evaluation using our evaluation guide. The best score is  bolded .",
        "table": "S6.T3.54.54",
        "footnotes": [],
        "references": [
            "This paper is outlined as follows: first, we study the relevant questions-answering legal RAG research and its related corpora in  Section 2 .  Then, we propose our corpora in  Section 3 , and in  Section 4 ,  Section 5  and  Section 6  we present a set of experiments  aimed at evaluating the performances of GPT-4o at answering Quebec automobile insurance questions.  Finally, in  Section 7 , we conclude and discuss our future work.",
            "As shown in  Figure 2  and  Figure 3 , we have used two prompts for our experiment. The first ( Figure 2 ) is a zero-shot prompt where the LLM is simply asked to answer the question. The second ( Figure 3  is a domain-specific prompt that gives additional information to support the LLM.  In each prompt,  {input}  corresponds to the question, and  {context}  to the retrieved references.",
            "The left-hand side of  Table 3  presents the results of the automatic metrics averaged over the five random restarts, with  bolded  value indicating the best score.  First, we observe that, for all automatic metrics, on average, the  All references  approach outperforms other methods.  Moreover, this methods BLEU, ROUGE, and METEOR scores indicate that it gives answers using a vocabulary similar to that of humans in the ground truth.  These scores are 40% to 300% higher than the zero-shot baseline approach.  It shows that using all our references greatly improves the LLMs ability to answer questions properly.  However, surprisingly, the second best approach is the  No reference  approach, which outperforms approaches using the same prompt along with a subset of our references.  We hypothesize that using Laws and other juridical documents confuses the LLM and generates longer text that are penalized by automatic  N N N italic_N -grams metrics.  We will explore and discuss this in the following section.",
            "The right-hand side of  Table 3  also presents the manual grading obtained using our evaluation guide, with  bolded  value indicating the best score.  Once again, we observe that  All references  approach outperforms other methods, achieving a score nearly double that of the baseline method.",
            "First, despite our efforts to make our systems more factually grounded using Quebec insurance references, our proposed framework remains at risk of generating hallucinations in its answers, as shown in  Table 3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Z-test significance test of our two bests approaches ( bold  value are rejected null hypothesis with   = 0.05  0.05 \\alpha=0.05 italic_ = 0.05 ).",
        "table": "S6.T4.3.1",
        "footnotes": [],
        "references": [
            "This paper is outlined as follows: first, we study the relevant questions-answering legal RAG research and its related corpora in  Section 2 .  Then, we propose our corpora in  Section 3 , and in  Section 4 ,  Section 5  and  Section 6  we present a set of experiments  aimed at evaluating the performances of GPT-4o at answering Quebec automobile insurance questions.  Finally, in  Section 7 , we conclude and discuss our future work.",
            "For our other five experiments, we use our RAG architecture described in  Section 4  and the domain-specific prompt, with an increasing number of reference sources.  Namely, we start with an approach that uses no references. The difference between this approach and the baseline is only prompt engineering.  Then, we incrementally add in reference sources.  The next approach only uses the  Laws  source, the following adds the  F.P.Q. 1 , then we add the AMF reference, and finally we add the educative resources to use all four references. We label these five approaches,  No references ,  Laws ,  Laws, F.P.Q. 1 ,  Laws, F.P.Q. 1, AMF  and  All references  respectively.",
            "Finally, to further assess our approaches performance, we report the two best approaches z-test significance test in  Table 4 . Our null hypothesis is that the pair of approaches have equal performances, meaning that values smaller or greater than  | 1.96 | 1.96 |1.96| | 1.96 |  allow us to reject the null hypothesis with   = 0.05  0.05 \\alpha=0.05 italic_ = 0.05 . A positive value means that the  No references  model (left) performs significantly better than the  All references  (right), and a negative value means the opposite.  We can see that for most metrics,  All references  has a significantly better performance compared to  No references ; we can conclude that  All references  is better than  No references .",
            "Figure 4  presents the evaluation interface used for our evaluation (in French). It is a custom adaptation of the Prodigy annotation tool  Montani and Honnibal ( 2018 ) ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Automatic metrics average and one standard deviation over the five restarts on our questions-answering corpus of our ablation study.",
        "table": "A2.T5.36.36",
        "footnotes": [],
        "references": [
            "This paper is outlined as follows: first, we study the relevant questions-answering legal RAG research and its related corpora in  Section 2 .  Then, we propose our corpora in  Section 3 , and in  Section 4 ,  Section 5  and  Section 6  we present a set of experiments  aimed at evaluating the performances of GPT-4o at answering Quebec automobile insurance questions.  Finally, in  Section 7 , we conclude and discuss our future work.",
            "Table 5  presents the ablation study based on the references used for the RAG, namely using only one source reference instead of the cumulative approach. Our results show that using the cumulative approach yields better results than using only one.  We did not conduct the manual evaluation of our ablation study."
        ]
    },
    "global_footnotes": [
        "We also discuss the risk of data leakage in our Limitations section.",
        ","
    ]
}