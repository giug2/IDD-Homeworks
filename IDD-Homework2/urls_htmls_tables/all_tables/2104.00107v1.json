{
    "S3.T1": {
        "caption": "Table 1: Dataset specifications",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T1.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Dataset</span></th>\n<th id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T1.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">Train Set</span></th>\n<th id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T1.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">Test Set</span></th>\n<th id=\"S3.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T1.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">Unique Answers</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\"><span id=\"S3.T1.1.2.1.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Indoor - Gibson</span></td>\n<td id=\"S3.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S3.T1.1.2.1.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">69,207</span></td>\n<td id=\"S3.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S3.T1.1.2.1.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">22,272</span></td>\n<td id=\"S3.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.1.2.1.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">961</span></td>\n</tr>\n<tr id=\"S3.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S3.T1.1.3.2.1.1\" class=\"ltx_text\" style=\"font-size:70%;\">Outdoor - nuScenes</span></td>\n<td id=\"S3.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T1.1.3.2.2.1\" class=\"ltx_text\" style=\"font-size:70%;\">33,973</span></td>\n<td id=\"S3.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T1.1.3.2.3.1\" class=\"ltx_text\" style=\"font-size:70%;\">15,644</span></td>\n<td id=\"S3.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T1.1.3.2.4.1\" class=\"ltx_text\" style=\"font-size:70%;\">650</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We propose to use the dataset provided by ISVQA. The dataset comprises of 2 modalities - image and language. The authors of ISVQA use existing image set datasets of Gibson (Xia et al., 2018) and nuScenes (Caesar et al., 2020) as sources to build an ISVQA dataset. Gibson dataset provides 3D indoor scans of buildings, rooms and offices. The scans are of 2 types 1) Gibson Building and 2) Gibson Room. nuScenes contain outdoor scenes generated by a 360∘superscript360360^{\\circ} camera mounted on a car in city streets. The details for the dataset are presented in Table 1."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Baseline model results",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Model Type</th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Train Accuracy (%)</th>\n<th id=\"S4.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Test Accuracy (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">LXMERT</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Baseline</td>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">89.95</td>\n<td id=\"S4.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">64.55</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">LXMERT without pretraining</td>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Baseline</td>\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">79.37</td>\n<td id=\"S4.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">57.05</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Visual BERT</td>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Baseline</td>\n<td id=\"S4.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">74.3</td>\n<td id=\"S4.T2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">55.55</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">HME - Video QA</td>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Baseline</td>\n<td id=\"S4.T2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">97.1</td>\n<td id=\"S4.T2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">49.9</td>\n</tr>\n<tr id=\"S4.T2.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">LXMERT with enhanced pre-training</td>\n<td id=\"S4.T2.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">Ours</td>\n<td id=\"S4.T2.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">88.74</td>\n<td id=\"S4.T2.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T2.1.6.5.4.1\" class=\"ltx_text ltx_font_bold\">64.93</span></td>\n</tr>\n<tr id=\"S4.T2.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">LXMERT with Count-based VQA</td>\n<td id=\"S4.T2.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Ours</td>\n<td id=\"S4.T2.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">79.56</td>\n<td id=\"S4.T2.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">57.50</td>\n</tr>\n<tr id=\"S4.T2.1.8.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">LXMERT with Regression Loss</td>\n<td id=\"S4.T2.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Ours</td>\n<td id=\"S4.T2.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">92.6</td>\n<td id=\"S4.T2.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">64.38</td>\n</tr>\n<tr id=\"S4.T2.1.9.8\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Adversarial Regularization</td>\n<td id=\"S4.T2.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Ours</td>\n<td id=\"S4.T2.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">82.62</td>\n<td id=\"S4.T2.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">63.24</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table 2 tabulates the results obtained for the baseline and the advanced models proposed by us on the ISVQA dataset. We further assess the performance of each research enhancement in the following sections.",
            "Table 2 shows that our LXMERT model with enhanced pre-training performs slightly better than the baseline LXMERT model. This shows the efficacy of pre-training on our artificially created data. Further, from Table 5, we see that our performance on the color-based questions is significantly better than the baseline LXMERT model. We do not report results on position-specific questions as the aggregator for such questions is not provided in the dataset. These results re-iterate the need to provide better color representations to our model as they can significantly enhance the performance. More importantly, these results provide motivation on developing and spending computational resources on training better pre-training objectives that incorporate position and color as they can help improve accuracy on ISVQA even further.",
            "The results for the Count based ISVQA is presented in table 2. The results are obtained on an LXMERT model trained from scratch rather than adopting a pretrained one. Thus the comparison will be done on an LXMERT model without pretraining using base RCNN features. This is done because the input features for the Count based model are changed and thus we cannot adapt a pretrained model as is (and pre-training our model is computationally unfeasible). A slight improvement over the baseline is observed on the overall results whereas Table 6 compares the model performance specifically on count based questions. This shows a clear improvement over the baseline confirming that ’count-aware’ features perform better. The claims made by (Zhang et al., 2018) that the soft attention based models generally do not perform well on count based question is proven. This also shows that a graph based modelling of the regions of interest in an image is a good approach to resolve disambiguities and solve the double counting problem especially in datasets such ISVQA where such anomalies are present in abundance.",
            "The results for this experiment in Table 2 and 7 show similar results to the LXMERT baseline and a marginal 0.2% improvement in Count-only question accuracy was observed. The results suggest that introducing an additional regression loss is enough to improve accuracy on count-based questions. And additional modifications such as changes in the features extraction process, better attention mechanisms to improve alignment and overall enhancements to the model architecture are required to improve accuracy on count based questions."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Results for Adversarial Regularization. Here L corresponds to Language and V corresponds to visual modality",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_t\"></th>\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Base LXMERT</th>\n<th id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">w/ CE Reg.</th>\n<th id=\"S5.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">w/ BCE Reg.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">L + V</td>\n<td id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">64.55%</td>\n<td id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">63.24%</td>\n<td id=\"S5.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">63.05%</td>\n</tr>\n<tr id=\"S5.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">L</td>\n<td id=\"S5.T3.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">42.87%</td>\n<td id=\"S5.T3.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">5.66%</td>\n<td id=\"S5.T3.1.3.2.4\" class=\"ltx_td ltx_align_center\">12.8%</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The idea to incorporate adversarial regularization stems from the observations made on the Question only Analysis. A model which captures strong language biases limits its ability to generalize to data which is not observed during training and hence introducing a regularizing component can prove beneficial to improve the performance of the ISVQA baselines discussed in section 3.5. A similar observation to the one made in the Question only BERT baseline is observed in the LXMERT model. Here the LXMERT model which achieves the best result for ISVQA is able to accurately answer questions even when the information from the visual modality is completely removed by zeroing out object features extracted from the RCNN module. The Table 3 shows the result from our experiment on adversarial regularization.",
            "To mitigate the presence of such biases we introduce a new adversarial regularization formulation. Here we experiment with training the LXMERT model using 2 different regularization loss terms which penalizes overconfident predictions on adversarial examples and to compare the effect of adversarial regularization we utilize the baseline LXMERT model which was trained on both Language and Visual Modalities. Our results presented in Table 3 indicate that when visual information is scrubbed (zeroed out) during inference time (but is trained with both Visual and Language modality present) the LXMERT model trained without adversarial regularization is able to obtain considerably high accuracy on the test dataset showing its over reliance on language modality and the presence of bias in the test dataset. However, when the model is trained with adversarial regularization, performance on test set with srubbed out visual information is reduced drastically  37% while little impact is seen when both the visual and text data are provided which indicates that regularized model does not over rely on language biases. We also observe that using the label-wise Binary Cross-Entropy loss is less effective in removing bias when compared to using the Cross-Entropy adversarial loss formulation."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Question-only BERT result",
        "table": "<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S5.T4.1.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.1.1.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\">Train Accuracy (%)</span>\n</span>\n</th>\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S5.T4.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\">Test Accuracy (%)</span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.1.2.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_tt\">\n<span id=\"S5.T4.1.2.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.2.1.1.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\">72.55</span>\n</span>\n</td>\n<td id=\"S5.T4.1.2.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_tt\">\n<span id=\"S5.T4.1.2.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T4.1.2.1.2.1.1\" class=\"ltx_p\" style=\"width:85.4pt;\">52.69</span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "To better understand the bias present in the text dataset and its implications on the model performance, we analyze the distribution of answer categories across each questions and find that colour-based and count-based questions are often dominated by a smaller number of answer categories, as shown in Figure 4. We can see that for count-based questions like how many people/cars/pedestrian etc, same 2 answer categories occurred about 75% times (denoted by blue and green stacks), while all the remaining classes accounted for about 25% of the occurrences, thus denoting a high answer bias for count-based questions. Meanwhile, object-based questions like what does the/what are the show a high diversity of answer classes, and thus low text-induced bias. This indicates the presence of bias in the ISVQA dataset and can lead the models learned on this data to not generalize well and under-utilize visual information when answering questions as the information needed to answer the questions can be learnt from just the language modality features. This is further highlighted from Table 4 which shows the results for a BERT-based question-only model. We observe that this unimodal model is able to achieve over 52% accuracy."
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Model performance on Color-based Questions only",
        "table": "<table id=\"S5.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S5.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Color-Only Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">LXMERT</td>\n<td id=\"S5.T5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">67.72</td>\n</tr>\n<tr id=\"S5.T5.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T5.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">LXMERT w/ enhanced PT</td>\n<td id=\"S5.T5.1.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\">70.09</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table 2 shows that our LXMERT model with enhanced pre-training performs slightly better than the baseline LXMERT model. This shows the efficacy of pre-training on our artificially created data. Further, from Table 5, we see that our performance on the color-based questions is significantly better than the baseline LXMERT model. We do not report results on position-specific questions as the aggregator for such questions is not provided in the dataset. These results re-iterate the need to provide better color representations to our model as they can significantly enhance the performance. More importantly, these results provide motivation on developing and spending computational resources on training better pre-training objectives that incorporate position and color as they can help improve accuracy on ISVQA even further."
        ]
    },
    "S5.T6": {
        "caption": "Table 6: Model performance on Count-based Questions only",
        "table": "<table id=\"S5.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S5.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Count-Only Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">LXMERT w/o pre-training</td>\n<td id=\"S5.T6.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">42.7%</td>\n</tr>\n<tr id=\"S5.T6.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T6.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">LXMERT w/ Count Based VQA</td>\n<td id=\"S5.T6.1.3.2.2\" class=\"ltx_td ltx_align_center\">46.5%</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The results for the Count based ISVQA is presented in table 2. The results are obtained on an LXMERT model trained from scratch rather than adopting a pretrained one. Thus the comparison will be done on an LXMERT model without pretraining using base RCNN features. This is done because the input features for the Count based model are changed and thus we cannot adapt a pretrained model as is (and pre-training our model is computationally unfeasible). A slight improvement over the baseline is observed on the overall results whereas Table 6 compares the model performance specifically on count based questions. This shows a clear improvement over the baseline confirming that ’count-aware’ features perform better. The claims made by (Zhang et al., 2018) that the soft attention based models generally do not perform well on count based question is proven. This also shows that a graph based modelling of the regions of interest in an image is a good approach to resolve disambiguities and solve the double counting problem especially in datasets such ISVQA where such anomalies are present in abundance."
        ]
    },
    "S5.T7": {
        "caption": "Table 7: Model performance on Count-based Questions only",
        "table": "<table id=\"S5.T7.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T7.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S5.T7.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Count-Only Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T7.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T7.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">LXMERT</td>\n<td id=\"S5.T7.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">49.84%</td>\n</tr>\n<tr id=\"S5.T7.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T7.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">LXMERT w/ Regression Loss</td>\n<td id=\"S5.T7.1.3.2.2\" class=\"ltx_td ltx_align_center\">50.02%</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The results for this experiment in Table 2 and 7 show similar results to the LXMERT baseline and a marginal 0.2% improvement in Count-only question accuracy was observed. The results suggest that introducing an additional regression loss is enough to improve accuracy on count-based questions. And additional modifications such as changes in the features extraction process, better attention mechanisms to improve alignment and overall enhancements to the model architecture are required to improve accuracy on count based questions."
        ]
    }
}