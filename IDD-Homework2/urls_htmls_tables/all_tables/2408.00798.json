{
    "id_table_1": {
        "caption": "Table 1:  Question answering experiment results. We use quizzes from six different domains of the new-hire training documents for engineers as test questions. All questions are multiple choice questions. Average scores across five trials are shown. The best scores are in  bold .",
        "table": "S3.T1.3",
        "footnotes": [],
        "references": [
            "Vanilla RAG  Lewis et al. ( 2020 ) , for instance, struggles with accurately interpreting domain-specific jargons. When asked,  \"What is the PUC architecture of Samsung or Hynix NAND chip?\" , the system incorrectly interprets  \"PUC\"  as  \"Process-Unit-Controller\"  instead of the correct  \"Peripheral Under Cell\" . This misinterpretation highlights the problem of hallucination, where the model generates incorrect or nonsensical information based on ambiguous input. This issue is further illustrated in Figure  1 , which shows that both Corrective RAG  Yan et al. ( 2024 )  and Self-RAG  Asai et al. ( 2023 )  attempt to modify the response after the document retrieval step. However, if the initial retrieval is flawed due to misinterpreted jargons or lack of context, these post-processing techniques cannot fully rectify the inaccuracies.",
            "Moreover, Corrective-RAG and Self-RAG focus on refining the generated responses after retrieval, which is inherently limited if the retrieved documents themselves are not relevant. As depicted in Figure  1 , these methods fail to address the root cause: the ambiguity in the users question and the initial retrieval process. A related approach by  Kochedykov et al. ( 2023 )  aims to address vague questions by deconstructing them into an AST and synthesizing SQL queries accordingly. While this method improves query fidelity, it is limited to SQL queries and does not generalize to broader question-answering scenarios. Figure  1  illustrates this limitation, showing that while the method can disambiguate and structure queries more effectively, it is not applicable to general retrieval tasks where context and jargon interpretation are crucial.",
            "Golden-Retriever consists of offline and online parts. The offline part is a data pre-processing step that occurs before the deployment of the knowledge base chatbot, described in Section  3.1 . The online part is an interactive process that takes place every time a user asks a question, detailed in Sections  3.2  through  3.6 .",
            "We list the scores of each method and LLM backbone in Table  1 . Compared with Vanilla LLM and RAG, Golden-Retriever improves the total score of Meta-Llama-3-70B by 79.2% and 40.7%, respectively. Across all three LLMs tested, Golden-Retriever improves the scores by an average of 57.3% over Vanilla LLM and 35.0% over RAG. This demonstrates that Golden-Retriever significantly enhances question-answering accuracy across multiple LLM backbones.",
            "To test if LLMs can robustly identify unknown abbreviations (Section  3.2 ), we generated random abbreviations and inserted them into question templates to create a synthetic dataset. For abbreviation generation, we computed the probability distribution of each letter being the first letter in all words in an English dictionary, then sequentially sampled the letters by that distribution to form abbreviations. We manually prepared question templates. The question templates and generated abbreviations are shown in the random abbreviation generation code in Appendix  C.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Abbreviation identification accuracy.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "Golden-Retriever consists of offline and online parts. The offline part is a data pre-processing step that occurs before the deployment of the knowledge base chatbot, described in Section  3.1 . The online part is an interactive process that takes place every time a user asks a question, detailed in Sections  3.2  through  3.6 .",
            "We choose to use the LLM for this task because traditional string-exact-match methods are inadequate. These methods may fail to detect jargons that are mistyped or not yet included in the dictionary, which could lead to misinterpretation in the following process. The LLMs ability to adapt to new terms provides a more robust solution. This step is represented as a two-way branching node in the workflow, shown in Figure  2 . If the resulting list is empty, the main program proceeds along the \"No\" path; otherwise, it follows the \"Yes\" path. The structured response containing the identified terms is saved in a temporary file, which is then accessed by the main program to determine the next steps in the workflow.",
            "This process involves querying a SQL database with the list of jargon terms identified in Section  3.2 . The jargon list is inserted into a SQL query template, which is then processed to retrieve the relevant information from the jargon dictionary. The retrieved information includes extended names, detailed descriptions, and any pertinent notes about the jargon. We choose not to use the LLM to generate SQL queries directly, as described in  Qin et al. ( 2023 )  and  Li et al. ( 2024 ) . Generating SQL queries with LLMs can introduce uncertainties regarding query quality and safety, and can also increase inference costs. Instead, by using a code-based approach to synthesize the SQL query, we ensure that the queries are verifiably safe and reliable.",
            "To test if LLMs can robustly identify unknown abbreviations (Section  3.2 ), we generated random abbreviations and inserted them into question templates to create a synthetic dataset. For abbreviation generation, we computed the probability distribution of each letter being the first letter in all words in an English dictionary, then sequentially sampled the letters by that distribution to form abbreviations. We manually prepared question templates. The question templates and generated abbreviations are shown in the random abbreviation generation code in Appendix  C.1 .",
            "The synthetic questions are integrated with the prompt template, as shown in the \"Identify Jargon\" step in Figure  2 . We prompt the LLM, record the responses, and check if they contain all abbreviations used in the questions. This experiment is conducted on the three aforementioned LLMs.",
            "We list the accuracy of each LLM in identifying all abbreviations in questions with varying numbers of abbreviations in Table  2 . The experiment shows that state-of-the-art models such as Llama3 and Mistral have high accuracy in identifying unknown abbreviations. We also observe different failure modes across the three LLMs, with detailed fail cases shown in Appendix  C.2 ."
        ]
    },
    "global_footnotes": [
        "Work conducted while interning at Western Digital Corporation."
    ]
}