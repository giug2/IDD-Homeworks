{
    "id_table_1": {
        "caption": "Table 1 .  Data statistics of the VeJudge dataset. The dataset comprises 12,681 statement-citation pairs. Each pair has been annotated by human assessors based on three categories: full, partial, and no support.",
        "table": "S4.T1.4",
        "footnotes": [],
        "references": [
            "Prior studies  (Gao et al . ,  2023a ,  b )  have primarily limited the application of faithfulness metrics in automated citation evaluation to a binary classification scenario.  In this scenario, faithfulness metrics are solely tasked with determining whether a citation supports the associated statement.  This binary approach fails to capture the fine-grained citation support encountered in real-world applications.  For instance, consider a  partial support  scenario illustrated in  Figure 1 . Given a user query  Where is the most humid place in Australia? , a retrieval-augmented LLM generates a response along with multiple citations.  A human assessor categorizes the first citation as partial support since it only supports the initial segment of the statement:  the most humid place in Australia is Macquarie Island . However, it does not provide evidence for the latter part of the statement:  which is located in the Southern Ocean off the coast of Tasmania . The complexity of this partial support scenario leads to noticeable inconsistencies across three distinct faithfulness metrics.  However, the effectiveness of faithfulness metrics in accurately distinguishing citations in such fine-grained citation support scenarios remains largely under-explored.",
            "In the experiments, we employ the dataset of verifiability judgments  (Liu et al . ,  2023 )  as our evaluation benchmark, referred to as VeJudge.  This dataset comprises a total of  12 , 681 12 681 12,681 12 , 681  statement-citation pairs. For each pair, human assessors categorize the citation into one of three categories of support levels: full, partial, or no support.  These categories indicate whether a citation provides full, partial, or no support to the associated statement.  The data statistics are illustrated in  Table 1 .  Notably, for citations classified under the full or partial support categories, human assessors additionally extract explicit evidence from the citation that substantiates the associated statement."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 .  Partial correlation coefficients between human-annotated levels of support and faithfulness metric scores on the VeJudge dataset. The best correlations are marked in bold.",
        "table": "S5.T3.4.1",
        "footnotes": [],
        "references": [
            "In this section, we introduce the proposed comparative evaluation framework. We begin by formalizing the task of automated citation evaluation. Subsequently, we detail three distinct evaluation protocols within this framework, ensuring a comprehensive assessment in alignment between faithfulness metrics and human judgments. Our framework is demonstrated in  Figure 2 .",
            "The correlation analysis results are demonstrated in  Table 2 . The following observations can be made:        1)   the best-performing metrics reveal moderate correlations when analyzed using the Pearson coefficient. Specifically,  AutoAIS  achieves the highest Pearson coefficient, recording a value of  0.604 0.604 0.604 0.604 , marginally surpassing the second-best  BARTScore , which posts a coefficient of  0.593 0.593 0.593 0.593 ;      2)   there is a noticeable variation in correlation trends among high-performing metrics. Notably,  AutoAIS  shows a more substantial Pearson correlation, whereas  SummaC Conv  outperforms in Spearman and Kendall correlations. This divergence might be attributed to the Pearson coefficient assuming linear relationships between two variables. Such an assumption is often invalid in automated citation evaluation, rendering the Pearson coefficient less suitable for capturing the true relationships between metric scores and human judgments; and      3)   Generally, most metrics display relatively low Spearman and Kendall correlations compared to their Pearson correlations. For instance,  SummaC Conv  achieves the highest Spearman and Kendall correlations, with values of  0.445 0.445 0.445 0.445  and  0.297 0.297 0.297 0.297  respectively, which are considerably lower than its Pearson correlation of  0.565 0.565 0.565 0.565 . This disparity indicates that the metric scores of the best-performing metrics do not correlate well with human judgments, highlighting the limitations of existing metrics in scenarios involving fine-grained levels of support."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 .  Classification performance of faithfulness metrics regarding ROC-AUC score (%) on the VeJudge dataset. The overall performance is the macro-averaged performance of three binary classification settings. The best scores are marked in bold.",
        "table": "S5.T4.9.1",
        "footnotes": [],
        "references": [
            "Table 3  presents the results of the classification evaluation. The observations can be summarized as follows:        1)   among all three binary classification task settings, most faithfulness metrics demonstrate superior performance in the  FS-vs-NS  setting. Notably, entailment-based  AutoAIS , with the highest ROC-AUC score of  92.65 92.65 92.65 92.65 , exemplifies significant discriminability between full support and no support instances. This performance can be attributed to its extensive parameters, comprising 11 billion parameters, in contrast to the hundreds of millions of other metrics;      2)   a pronounced decline in classification performance is observed across the other two settings. For instance, when comparing the  FS-vs-NS  and  PS-vs-NS  settings, the ROC-AUC score of  AutoAIS  diminishes from  92.65 92.65 92.65 92.65  to  74.21 74.21 74.21 74.21 . This decline indicates that even the best-performing metric struggles with granular sensitivity to varying levels of support; and      3)   while entailment-based  AutoAIS  generally surpasses other metrics in overall performance, it is outperformed by similarity-based  BERTScore  in the  PS-vs-NS  setting. Interestingly, while most metrics exhibit their lowest performance in this particular setting,  BERTScore  shows its least effectiveness in another setting,  FS-vs-PS . This underscores the unique prediction behaviors displayed by different types of metrics across the binary classification settings."
        ]
    },
    "global_footnotes": []
}