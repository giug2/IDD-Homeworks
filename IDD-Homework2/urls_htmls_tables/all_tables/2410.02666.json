{
    "id_table_1": {
        "caption": "Table 1:  Comparison of model accuracies on the integration task.",
        "table": "S7.T1.3",
        "footnotes": [],
        "references": [
            "In each step, the symbolic engine takes in a mathematical expression  f f f italic_f , a subexpression  g g g italic_g , an action  a a a italic_a , and action parameters  p 1 , ... , p n subscript p 1 ... subscript p n p_{1},\\ldots,p_{n} italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , if applicable. It returns an expression that results from applying action  a  ( p 1 , ... , p n ) a subscript p 1 ... subscript p n a(p_{1},\\ldots,p_{n}) italic_a ( italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ... , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT )  to subexpression  g g g italic_g , along with a boolean that specifies whether the expression was modified or not. The expression is not modified if  g g g italic_g  is not a valid subexpression of  f f f italic_f , or if the action is not valid on this subexpression. Figure  1  shows an example of a successfully executed action. A full list of actions can be found in Table  LABEL:actiontable  in Appendix  A.1 .",
            "To parse outputs of the model, we need a function that can unambiguously map from sequences to trees. We achieve this using Algorithm  1 , discussed in Appendix  B.1 . Note that this function returns a tree as well as the remaining part of the sequence. We assert that  r  e  m  a  i  n  i  n  g r e m a i n i n g remaining italic_r italic_e italic_m italic_a italic_i italic_n italic_i italic_n italic_g  has to be an emtpy list for the sequential representation to be valid.",
            "To create a large-scale dataset of mathematical expressions, we adopt an algorithm from  Lample and Charton ( 2019 ) , which samples random unary-binary trees and fills the nodes with operators and operands. We generate   similar-to \\sim  5M unique expressions with this algorithm described in Appendix  C.1 .",
            "We hold out a test set of 10k expressions with integration steps, unseen during training. We compare our model, SymPy, and GPT-4o-mini. We run SymPy and our own model with timeouts of  120 120 120 120  and  10 10 10 10  seconds, respectively. We use  N = 5 N 5 N=5 italic_N = 5  for beam search decoding. We observe that this beam search results in correct steps in the proposed actions for  > 99 % absent percent 99 >99\\% > 99 %  of the test set, on a step-by-step level. More precisely, this means that if we predict a single step, there is almost always an exact match of the step in the test set within one of the five recommendations resulting from beam search. We prompt GPT-4o-mini with zero-shot CoT and we only check correctness of the result, ignoring any wrong intermediate steps, for a random subset of  1000 1000 1000 1000  expressions. We present accuracy results in Table  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Robustness results with simple primitives (top) and validation problems (bottom). Coefficients are sampled from [1, 50].",
        "table": "S7.T2.9",
        "footnotes": [],
        "references": [
            "The rest of the paper is organized as follows. In Sections  2  and  3 , we introduce the symbolic engine and our representation of mathematical expressions. In Sections  4  and  5 , we explain how to generate synthetic data for integration and how we train the model. Finally, we explain how we run and evaluate the model in Sections  6  and  7 .",
            "We represent mathematical expressions as trees. Leaf nodes are number constants or variables, such as  2 2 2 2 ,    \\pi italic_ , or  x x x italic_x . Internal nodes are operators and functions, such as  + + +  or  cosh \\cosh roman_cosh . We show representations of the expressions  1 x + 3 + 2  cosh 2  ( x ) 1 x 3 2 superscript 2 x \\frac{1}{x+3}+2\\cosh^{2}(x) divide start_ARG 1 end_ARG start_ARG italic_x + 3 end_ARG + 2 roman_cosh start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ( italic_x )  and   x 2  e x  d x superscript x 2 superscript e x differential-d x \\int x^{2}e^{x}dx  italic_x start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_e start_POSTSUPERSCRIPT italic_x end_POSTSUPERSCRIPT italic_d italic_x  in Figure  2 .",
            "We test both models by perturbing the set of functions that includes  sin \\sin roman_sin ,  cos \\cos roman_cos ,  exp \\exp roman_exp ,  tan \\tan roman_tan , by multiplying their arguments and return values by random integers. We report the results in Table  2 . The experiments show that AlphaIntegrator is significantly less brittle in almost all cases."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Bugs/Failure Modes in Sympy",
        "table": "S7.T3.6",
        "footnotes": [],
        "references": [
            "The rest of the paper is organized as follows. In Sections  2  and  3 , we introduce the symbolic engine and our representation of mathematical expressions. In Sections  4  and  5 , we explain how to generate synthetic data for integration and how we train the model. Finally, we explain how we run and evaluate the model in Sections  6  and  7 .",
            "Given an expression  f f f italic_f , we tokenize it and feed it to the transformer. We decode the transformer using beam search with  N N N italic_N  candidates until we reach the  END  token, i.e., we heuristically search for the sequence with maximum log-probability  (Freitag and Al-Onaizan,  2017 ) . Then, we find all valid generated actions (with parameters). We independently run them on the current expression using the symbolic engine. We obtain new expressions, ordered by decreasing log-probability of the proposed action that generated them. We greedily explore the tree resulting from feeding the expressions back into the transformer, until the integral sign disappears. This loop is illustrated in Figure  3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  List of Actions in the Symbolic Engine",
        "table": "A1.T4.67",
        "footnotes": [],
        "references": [
            "The rest of the paper is organized as follows. In Sections  2  and  3 , we introduce the symbolic engine and our representation of mathematical expressions. In Sections  4  and  5 , we explain how to generate synthetic data for integration and how we train the model. Finally, we explain how we run and evaluate the model in Sections  6  and  7 .",
            "Once we have generated a dataset of random mathematical expressions, we pass them through the  manualintegrate  module of SymPy  (Meurer et al.,  2017 ) , in order to get a step-by-step solution. Then, we map the solution into a sequence of actions and parameters in our symbolic engine. This results in a sequence of tuples of expression, subexpression, action, and action parameter for each expression. An example of a full solution in this format, generated by our model, is given in Figure  4 . There are, of course, many expressions SymPy cannot integrate as it enumeratively tries heuristic methods. Our hypothesis is that transformers are able to generalize to cases not covered by SymPy.",
            "We explore a number of different directions to evaluate our model. We aim to demonstrate that our method not only significantly outperforms existing step-by-step benchmarks, but is also more efficient and generalizes well, unlike existing learning-based approaches. To illustrate the capability of our model, we present an example solution that it generated, in Figure  4 . For this example, SymPy failed to figure out that we can apply the substitution  u = sin  ( 2  x ) u 2 x u=\\sin(2x) italic_u = roman_sin ( 2 italic_x ) . We show further examples in Appendix  D .",
            "Our model significantly outperforms both SymPy and GPT-4o-mini, a state-of-the-art language model. Of particular interest is that our model generalizes beyond its data generator SymPy. We observe that GPT-4o-mini generally fails on long chains of computations, or when expressions are not sufficiently similar to what is typical. During manual inspection, we observed that the model usually has the right methods in mind, but then cannot execute operations accurately. Typical mistakes include sign errors while doing integration-by-parts or simple errors in arithmetic. For SymPy, it is more often the case that the system is unable to find solutions rather than making mistakes, as some cases are missed by the software. We explore this further in Section  7.4 , where we show some of the bugs and limitations we found in SymPy."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Dataset Statistics",
        "table": "A3.T5.1",
        "footnotes": [],
        "references": [
            "The rest of the paper is organized as follows. In Sections  2  and  3 , we introduce the symbolic engine and our representation of mathematical expressions. In Sections  4  and  5 , we explain how to generate synthetic data for integration and how we train the model. Finally, we explain how we run and evaluate the model in Sections  6  and  7 .",
            "Then, if we know a step-by-step integration of    ( x )    ( x ) italic- x  x \\phi(x)\\Psi(x) italic_ ( italic_x ) roman_ ( italic_x ) , we can find the steps for    ( x )    ( x )  x  x \\Phi(x)\\psi(x) roman_ ( italic_x ) italic_ ( italic_x )  by applying integration by parts with the right parameters and applying the steps of    ( x )    ( x ) italic- x  x \\phi(x)\\Psi(x) italic_ ( italic_x ) roman_ ( italic_x )  to the relevant subexpression of the resulting expression. We augment our dataset with this technique by searching for such instances in the previous dataset. Our final dataset consists of 42.5M integration steps and we report further statistics in Table  5 .",
            "To understand how well the transformer model guides us in the tree search, we measure the number of tree nodes explored during integration for both SymPy and our model, on the test set. We find that on average, our model explores  N t = 12.9 subscript N t 12.9 N_{t}=12.9 italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 12.9  nodes for each successful integration whereas SymPy explores  N s = 25.6 subscript N s 25.6 N_{s}=25.6 italic_N start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT = 25.6 . This demonstrates that our model is not only more powerful but also more efficient, as it explores roughly  50 % percent 50 50\\% 50 %  fewer nodes to find solutions. We present the full distribution in Figure  5 .",
            "We report statistics for the final dataset in Table  5"
        ]
    }
}