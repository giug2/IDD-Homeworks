{
    "S6.T1": {
        "caption": "Table 1: Comparison of standard and meta-learning algorithms in terms of test RMSE in 5 meta-learning environments for regression. Reported are mean and standard deviation across 5 seeds. Our proposed method PACOH achieves the best performance across all tasks.",
        "table": null,
        "footnotes": [],
        "references": [
            "PACOH improves the predictive accuracy.\nUsing the meta-learning environments and baseline methods that we introduced in Sec. 6.1, we perform a comprehensive benchmark study. Table 1 reports the results on the regression environments in terms of the root mean squared error (RMSE) on unseen test tasks. Among the approaches, PACOH-NN and PACOH-GP consistently perform best or are among the best methods. Similarly, PACOH-NN achieves the highest accuracy in the Omniglot classification environment (cf. Table 3). Overall, this demonstrates that the introduced meta-learning framework is not only sound, but also yields state-of-the-art empirical performance in practice."
        ]
    },
    "S6.T2": {
        "caption": "Table 2: Comparison of standard and meta-learning methods in terms of the test calibration error in 5 regression environments. We report the mean and standard deviation across 5 random seeds. PACOH yields the best uncertainty calibration in the majority of environments.",
        "table": null,
        "footnotes": [],
        "references": [
            "PACOH improves the predictive uncertainty.\nWe hypothesize that by acquiring the prior in a principled data-driven manner (e.g., with PACOH), we can improve the quality of the GP’s and BNN’s uncertainty estimates. To investigate the effect of meta-learned priors on the uncertainty estimates of the base learners, we compute the probabilistic predictors’ calibration errors, reported in Table 2 and 3. The calibration error measures the discrepancy between the predicted confidence regions and the actual frequencies of test data in the respective areas (Kuleshov et al., 2018). Note that, since MAML only produces point predictions, the concept of calibration does not apply to it. We observe that meta-learning priors with PACOH-NN consistently improves the Vanilla BNN’s uncertainty estimates. Similarly, PACOH-GP yields a lower calibration error than the Vanilla GP in the majority of the envionments. For meta-learning environments where the task similarity is high, like SwissFEL and Berkeley-Sensor, the improvement is substantial.",
            "If the classifier is calibrated, we expect that the confidence of the classifier reflects it’s accuracy on unseen test data, that is, acc(Bh)=conf(Bh)∀h=1,….,H\\text{acc}(B_{h})=\\text{conf}(B_{h})~{}\\forall h=1,....,H. As proposed of Guo et al. (2017), we use the expected calibration error (ECE) to quantify how much the classifier deviates from this criterion: More precisely, in Table 2, we report the ECE with the following definition:"
        ]
    },
    "S6.T3": {
        "caption": "Table 3: Comparison of meta-learning algorithms in terms of test accuracy and calibration error on the Omniglot environment. Among the methods, PACOH-NN makes the most accurate and best-calibrated class predictions.",
        "table": null,
        "footnotes": [],
        "references": [
            "PACOH improves the predictive accuracy.\nUsing the meta-learning environments and baseline methods that we introduced in Sec. 6.1, we perform a comprehensive benchmark study. Table 1 reports the results on the regression environments in terms of the root mean squared error (RMSE) on unseen test tasks. Among the approaches, PACOH-NN and PACOH-GP consistently perform best or are among the best methods. Similarly, PACOH-NN achieves the highest accuracy in the Omniglot classification environment (cf. Table 3). Overall, this demonstrates that the introduced meta-learning framework is not only sound, but also yields state-of-the-art empirical performance in practice.",
            "PACOH improves the predictive uncertainty.\nWe hypothesize that by acquiring the prior in a principled data-driven manner (e.g., with PACOH), we can improve the quality of the GP’s and BNN’s uncertainty estimates. To investigate the effect of meta-learned priors on the uncertainty estimates of the base learners, we compute the probabilistic predictors’ calibration errors, reported in Table 2 and 3. The calibration error measures the discrepancy between the predicted confidence regions and the actual frequencies of test data in the respective areas (Kuleshov et al., 2018). Note that, since MAML only produces point predictions, the concept of calibration does not apply to it. We observe that meta-learning priors with PACOH-NN consistently improves the Vanilla BNN’s uncertainty estimates. Similarly, PACOH-GP yields a lower calibration error than the Vanilla GP in the majority of the envionments. For meta-learning environments where the task similarity is high, like SwissFEL and Berkeley-Sensor, the improvement is substantial.",
            "PACOH combats meta-overfitting.\nAs Qin et al. (2018) and Yin et al. (2020) point out, many popular meta-learners (e.g., Finn et al., 2017; Garnelo et al., 2018) require a large number of meta-training tasks to generalize well. When presented with only a limited number of tasks, such algorithms suffer from severe meta-overfitting, adversely impacting their performance on unseen tasks from 𝒯𝒯\\mathcal{T}. This can even lead to negative transfer, such that meta-learning actually hurts the performance when compared to standard learning. In our experiments, we also observe such failure cases: For instance, in the classification environment (cf. Table 3), MAML fails to improve upon the Vanilla BNN. Similarly, in the regression environments (cf. Table 3) we find that NPs, BMAML and MLL-GP often yield worse-calibrated predictive distributions than the Vanilla BNN and GP respectively.\nIn contrast, thanks to its theoretically principled construction, PACOH-NN is able to achieve positive transfer even when the tasks are diverse and small in number. In particular, the hyper-prior acts as meta-level regularizer by penalizing complex priors that are unlikely to convey useful inductive bias for unseen learning tasks."
        ]
    },
    "A5.T1": {
        "caption": "Table S1: Number of tasks n𝑛n and samples per task misubscript𝑚𝑖m_{i} for the different meta-learning environments.",
        "table": "<table id=\"A5.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T1.2.3.1\" class=\"ltx_tr\">\n<th id=\"A5.T1.2.3.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\"/>\n<th id=\"A5.T1.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Sinusoid</th>\n<th id=\"A5.T1.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Cauchy</th>\n<th id=\"A5.T1.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">SwissFEL</th>\n<th id=\"A5.T1.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Physionet</th>\n<th id=\"A5.T1.2.3.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Berkeley</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T1.1.1\" class=\"ltx_tr\">\n<th id=\"A5.T1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><math id=\"A5.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"n\" display=\"inline\"><semantics id=\"A5.T1.1.1.1.m1.1a\"><mi id=\"A5.T1.1.1.1.m1.1.1\" xref=\"A5.T1.1.1.1.m1.1.1.cmml\">n</mi><annotation-xml encoding=\"MathML-Content\" id=\"A5.T1.1.1.1.m1.1b\"><ci id=\"A5.T1.1.1.1.m1.1.1.cmml\" xref=\"A5.T1.1.1.1.m1.1.1\">𝑛</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T1.1.1.1.m1.1c\">n</annotation></semantics></math></th>\n<td id=\"A5.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n<td id=\"A5.T1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">20</td>\n<td id=\"A5.T1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">5</td>\n<td id=\"A5.T1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td id=\"A5.T1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">36</td>\n</tr>\n<tr id=\"A5.T1.2.2\" class=\"ltx_tr\">\n<th id=\"A5.T1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><math id=\"A5.T1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"m_{i}\" display=\"inline\"><semantics id=\"A5.T1.2.2.1.m1.1a\"><msub id=\"A5.T1.2.2.1.m1.1.1\" xref=\"A5.T1.2.2.1.m1.1.1.cmml\"><mi id=\"A5.T1.2.2.1.m1.1.1.2\" xref=\"A5.T1.2.2.1.m1.1.1.2.cmml\">m</mi><mi id=\"A5.T1.2.2.1.m1.1.1.3\" xref=\"A5.T1.2.2.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A5.T1.2.2.1.m1.1b\"><apply id=\"A5.T1.2.2.1.m1.1.1.cmml\" xref=\"A5.T1.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A5.T1.2.2.1.m1.1.1.1.cmml\" xref=\"A5.T1.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"A5.T1.2.2.1.m1.1.1.2.cmml\" xref=\"A5.T1.2.2.1.m1.1.1.2\">𝑚</ci><ci id=\"A5.T1.2.2.1.m1.1.1.3.cmml\" xref=\"A5.T1.2.2.1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T1.2.2.1.m1.1c\">m_{i}</annotation></semantics></math></th>\n<td id=\"A5.T1.2.2.2\" class=\"ltx_td ltx_align_center\">5</td>\n<td id=\"A5.T1.2.2.3\" class=\"ltx_td ltx_align_center\">20</td>\n<td id=\"A5.T1.2.2.4\" class=\"ltx_td ltx_align_center\">200</td>\n<td id=\"A5.T1.2.2.5\" class=\"ltx_td ltx_align_center\">4 - 24</td>\n<td id=\"A5.T1.2.2.6\" class=\"ltx_td ltx_align_center\">288</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "In this section, we provide further details on the meta-learning environments used in Section 5.3.\nInformation about the numbers of tasks and samples in the respective environments can be found in Table S1."
        ]
    },
    "A5.T2": {
        "caption": "Table S2: MHC-I alleles used for meta-training and their corresponding number of meta-training samples misubscript𝑚𝑖m_{i}.",
        "table": "<table id=\"A5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A5.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"A5.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r\">Allele</th>\n<th id=\"A5.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-0202</th>\n<th id=\"A5.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-0203</th>\n<th id=\"A5.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-0201</th>\n<th id=\"A5.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-2301</th>\n<th id=\"A5.T2.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">A-2402</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A5.T2.1.1\" class=\"ltx_tr\">\n<th id=\"A5.T2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><math id=\"A5.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"m_{i}\" display=\"inline\"><semantics id=\"A5.T2.1.1.1.m1.1a\"><msub id=\"A5.T2.1.1.1.m1.1.1\" xref=\"A5.T2.1.1.1.m1.1.1.cmml\"><mi id=\"A5.T2.1.1.1.m1.1.1.2\" xref=\"A5.T2.1.1.1.m1.1.1.2.cmml\">m</mi><mi id=\"A5.T2.1.1.1.m1.1.1.3\" xref=\"A5.T2.1.1.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"A5.T2.1.1.1.m1.1b\"><apply id=\"A5.T2.1.1.1.m1.1.1.cmml\" xref=\"A5.T2.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A5.T2.1.1.1.m1.1.1.1.cmml\" xref=\"A5.T2.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"A5.T2.1.1.1.m1.1.1.2.cmml\" xref=\"A5.T2.1.1.1.m1.1.1.2\">𝑚</ci><ci id=\"A5.T2.1.1.1.m1.1.1.3.cmml\" xref=\"A5.T2.1.1.1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A5.T2.1.1.1.m1.1c\">m_{i}</annotation></semantics></math></th>\n<td id=\"A5.T2.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">1446</td>\n<td id=\"A5.T2.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">1442</td>\n<td id=\"A5.T2.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">3088</td>\n<td id=\"A5.T2.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">103</td>\n<td id=\"A5.T2.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">196</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We use 5 alleles to meta-learn a BNN prior. The alleles and the corresponding number of data points, available for meta-training, are listed in Table S2. The most genetically dissimilar allele (A-6901) is used for our bandit task. In each iteration, the experimenter (i.e. bandit algorithm) chooses to test one peptide among the pool of 813 candidates and receives r𝑟r as a reward feedback. Hence, we are concerned with a 813-arm bandit wherein the action at∈{1,…,813}=𝒜subscript𝑎𝑡1…813𝒜a_{t}\\ \\in\\{1,...,813\\}=\\mathcal{A} in iteration t𝑡t corresponds to testing atsubscript𝑎𝑡a_{t}-th peptide candidate. In response, the algorithm receives the respective negative log-IC50subscriptIC50\\text{IC}_{50} as reward r​(at)𝑟subscript𝑎𝑡r(a_{t})."
        ]
    }
}