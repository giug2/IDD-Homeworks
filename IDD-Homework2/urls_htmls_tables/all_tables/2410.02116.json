{
    "id_table_1": {
        "caption": "Table 1:  Pre-training on CIFAR10 (2% of Full Data)",
        "table": "A5.EGx1",
        "footnotes": [],
        "references": [
            "High Variance Gradients Prevent Effective Trajectory Matching for SSL  SSL losses rely on interaction between all examples in a batch and consequently have high variance over choices of random batches (c.f., the Barlow-Twins loss in equation  1 ). As a result, the contribution of examples to the loss and hence their gradients varies significantly based on the rest of examples in the batch  [ 30 ] , unlike SL where each examples contribution to the loss is independent of other examples. The high variance in gradient over mini-batches in each iteration results in high variance of the trajectories of SSL.",
            "Theoretical Evidence for Higher Variance Gradients in SSL.  We now present, in Theorem  4.1 , theoretical evidence, in a simplified setting, demonstrating that the variance of the gradient of SSL over mini-batches is indeed greater than that of SL, i.e.,  Var  (  W L S  S  L  ( B ) ) > Var  (  W L S  L  ( B ) ) Var subscript  W subscript L S S L B Var subscript  W subscript L S L B \\mathrm{Var}\\big{(}\\nabla_{W}L_{SSL}(B)\\big{)}>\\mathrm{Var}\\big{(}\\nabla_{W}L_% {SL}(B)\\big{)} roman_Var (  start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_S italic_S italic_L end_POSTSUBSCRIPT ( italic_B ) ) > roman_Var (  start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_S italic_L end_POSTSUBSCRIPT ( italic_B ) ) . Proof appears in Appendix  C . Appendix  D  presents a more general version of this analysis, when optimizing with synhronous parallel SGD.",
            "Empirical Evidence for Challenges of Matching SSL Trajectories  Due to the high variance in the gradient of SSL objectives, the naive application of MTT to SSL does not succeed. Firstly, the slower convergence caused by high variance gradients necessitates much longer trajectories for both training on real and synthetic data. Secondly, the higher variance of gradients results in greater variance in the weights at the end of trajectories starting from the same initialization (henceforth referred to as  variance of trajectories ), as confirmed theoretically above. Attempting to match SSLs longer, higher variance trajectories is extremely challenging, as matching such trajectories results in chaotic updates to the synthetic images. Thus, the synthetic images cannot move away from their initialization meaningfully. Fig.  1(a)  shows empirically that the variance of SSL trajectories is larger than that of SL trajectories, across different trajectory lengths. Additionally, the variance of trajectories grows faster, with length of trajectory, for SSL than for SL, exacerbating the problem for longer trajectory matching. Fig.  1(b)  compares a simplified distillation using MTT with a single expert trajectory for SSL and SL. Despite extensive hyper-parameter tuning, matching even a single expert trajectory is challenging for SSL, confirmed by the slow decrease of distillation loss. This indicates that the training trajectory on the distilled set is unable to match the training trajectory on the real data for SSL. Fig.  1(c)  shows that the difficulty in aligning trajectories is due to the chaotic updates of the synthetic image, as evidenced by the synthetic images being unable to move away from their initialization. To further confirm that the inability to distill effectively is indeed due to the variance of trajectories, we also include a comparison to MTT SSL with 4x larger batch size, which leads to slightly lower variance. Fig.  1(a)  confirms that indeed the larger batch size reduces the variance of the trajectories slightly. However, Fig.  1(b)  and  1(c)  show that reducing the variance of SSL trajectories via larger batch size is insufficient to help distillation since an infeasibly large batch size will likely be required to achieve the necessary low variance trajectories.",
            "where  f S subscript f S f_{S} italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and  f T subscript f T f_{T} italic_f start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  represent the student and teacher encoders respectively.  [ 41 ]  trains student models with the aforementioned KD objective and the original SSL Loss. However, we only minimize the MSE between student and teacher representations to avoid the issues with matching SSL training trajectories (discussed in Sec.  4.1 ).",
            "Converting SSL to SL trajectories via KD  We use the objective presented in equation  5  i.e. minimizing the MSE between the  representations  of a student and a teacher model trained with SSL. In doing so, we train the student model to match the performance of the teacher trained with SSL. Note that training the student model by minimizing the MSE loss in equation  5  is a  supervised  objective. Therefore, while the trained student model will produce similar representations to that of the teacher, training with MSE loss is much faster than SSL, as its gradients have a much smaller variance ( c.f.  Fig.  1(a) ). Thus, we can get shorter and lower variance  expert  trajectories from the  student  models trained with KD using the MSE loss, instead of the teacher model trained with SSL. Then, we can generate synthetic examples by matching these shorter and lower variance trajectories, without relying on labels.",
            "In our experiments,  L SSL subscript L SSL \\mathcal{L}_{\\mathsf{SSL}} caligraphic_L start_POSTSUBSCRIPT sansserif_SSL end_POSTSUBSCRIPT  is the BarlowTwins loss function shown in equation  1 , but our method is agnostic to the choice of SSL algorithm. Since KD with larger models leads to better downstream performance  [ 13 ] , we use a teacher model that is much larger than the student encoder used for creating the expert trajectories for distillation. For example, in our experiments we use a ResNet-18 as the teacher encoder and a 3 or 4-layer ConvNet as the student encoder.",
            "Now, we compute our distillation loss  L D  D  ( D syn ) subscript L D D subscript D syn \\mathcal{L}_{DD}(\\mathcal{D}_{\\mathsf{syn}}) caligraphic_L start_POSTSUBSCRIPT italic_D italic_D end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT )  (shown in equation  4 ) using the parameters of the encoder trained on the synthetic data and the encoder trained on the full data, and update the synthetic data and learning rate,  D syn subscript D syn \\mathcal{D}_{\\mathsf{syn}} caligraphic_D start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT  and   syn subscript  syn \\alpha_{\\mathsf{syn}} italic_ start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT , respectively, to minimize this. Note that  Z syn subscript Z syn \\mathcal{Z}_{\\mathsf{syn}} caligraphic_Z start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT  remains unchanged. We repeat this distillation for  S S S italic_S  iterations. Pseudo-code of MKDT is provided in Alg.  1 .",
            "Downstream Generalization Performance  Table  1  demonstrates that pre-training on CIFAR10 using MKDT with a 2% distilled set improves performance by 8% on CIFAR10 and 5% on downstream tasks over the KRR-ST baseline. Gains on CIFAR10 are consistent across 1% and 5% labeled data, but improvements on downstream tasks are more pronounced with 5%, indicating MKDT scales well with more labeled data. On CIFAR100, MKDT 2% distilled set improves performance by 6% and 8% on CIFAR100 and downstream tasks, respectively. Additionally, MKDT shows up to 3% improvement on downstream tasks for larger, higher-resolution datasets like TinyImageNet (200K examples, 64x64 resolution), highlighting MKDTs scalability. KRR-ST consistently fails to outperform SSL pre-training on random subsets across all settings. In Appendix  B , we verify that this holds for fine-tuning experiments from KRR-ST  [ 21 ] , affirming MKDT as the only effective DD method for SSL pre-training. Table  4  shows that pre-training with larger distilled sets (5% of full data) further enhances performance by up to 13%, confirming MKDT scales effectively with distilled set size as well. Table  7  shows that MKDT outperforms the strongest baseline (random subsets) by 5% on pre-training and 7% on downstream tasks when using 10% and 50% labeled data.",
            "Using SimCLR for Obtaining Teacher Representation.  We conducted an ablation study using a teacher model trained with SimCLR  [ 2 ]  instead of Barlow Twins  [ 43 ]  for CIFAR 10 and CIFAR 100. The experiment steps are similar to the ones in  A.1 . During the \"Training the Teacher Model Using SSL\" step , we used the Adam optimizer with batch size 512, learning rate 0.001, and weight decay  10  6 superscript 10 6 10^{-6} 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT  to train a ResNet18 along with a 2-layer linear projection head for 400 epochs. The projection head included Batch Normalization and ReLU after the first layer, and Batch Normalization after the second layer, projecting to 128 dimensions. Then, we used the pre-projection head representation of the trained model for getting the teacher representation of size 512. The other steps are the same as the one in  A.1 . Table  6  shows that MKDT consistently outperforms the random subset across all downstream datasets and various sizes of labeled data, highlighting the methods generalizability to other contrastive learning methods.",
            "Scaling the Method to Larger Distillation Sets.  In addition to distilling 2% subsets, we also conducted experiments distilling 5% subsets of CIFAR10, CIFAR100, and TinyImageNet to evaluate the generalizability of the method to larger distillation sets. Table  4  shows the scalability of the method to larger distilled set sizes. The experiments procedure are the same as the ones in  A.1  except that we use different distillation hyperparameters for the 5% distilled set. The hyperparameters are summarized in Table  9 .",
            "Scaling the Method to Larger Downstream Labeled Dataset Sizes.  We evaluated the performance for CIFAR 10, CIFAR 100, and TinyImageNet on larger downstream labeled data sizes, specifically 10% and 50% labeled data sizes, using the 2% distilled set obtained with the method illustrated in  A.1 . As shown in Table  7 , MKDT continues to outperform random subset across all downstream datasets and data sizes, demonstrating its scalability with larger labeled data sizes.",
            "For the experiment in Figure  1(a) , we train 5 trajectories of each of MTT SSL and MTT SL for CIFAR 100 using the same random initialization of the network, respectively. For MTT SSL, we train the models with the Adam optimizer with batch size 1024, learning rate 0.001, and weight decay  10  6 superscript 10 6 10^{-6} 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT . For MTT SL, we train the model with SGD with batch size 256, learning rate 0.01, momentum 0, and no weight decay.",
            "For both of the experiments in Figure  1(b)  and  1(c) , we distill the dataset using MTT SL with image learning rate 1000, max start epoch 0, synthetic steps 20, and expert epochs 4. We distill using MTT SLL with image learning rate 1000, max start epoch 0, synthetic steps 10, and expert epochs 2. We distilled them for 100 iterations and record the change in the loss function and the average absolute change in pixels.",
            "From Eq.  51 , it is easy to see that the expectation of the square of element  ( 0 , 0 ) 0 0 (0,0) ( 0 , 0 )  is far larger due to far more   1 subscript italic- 1 \\epsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,   2 subscript italic- 2 \\epsilon_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,   1 2 superscript subscript italic- 1 2 \\epsilon_{1}^{2} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  and   2 2 superscript subscript italic- 2 2 \\epsilon_{2}^{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  terms."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Pre-training on CIFAR100 (2% of Full Data)",
        "table": "S5.T1.84.84",
        "footnotes": [],
        "references": [
            "As discussed in Sec.  2 , distribution matching and gradient matching methods cannot work without labels, and meta-model matching cannot effectively update the encoder. Therefore, in our work, we focus on application of MTT to SSL distillation. First, we discuss the challenges of applying MTT in the SSL setting and show that its naive application does not work. Then, we present our method, MKDT, that relies on recent results in knowledge distillation (KD) to enable trajectory matching for SSL.",
            "Ablations  We perform ablations over two factors: 1) initialization and 2) SSL algorithm. Table  2  presents results for pre-training with MKDT using random subset initialization, as well as results for pre-training directly on the high-loss subset initialization used by MKDT. Interestingly, the KD objective for SSL pre-training benefits slightly more from the high-loss subset than from random subsets. Consequently, MKDT initialized from the high-loss subset performs better than when initialized from a random subset. Table  6  shows results for MKDT using a teacher model trained with SimCLR  [ 2 ]  instead of BarlowTwins. Specifically, we train a ResNet-18 for 400 epochs using SimCLR. Here too, MKDT achieves approximately 6% higher performance compared to random subsets across downstream datasets. This confirms that MKDT generalizes across different SSL training algorithms."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Pre-training on TinyImageNet (2% of Full Data)",
        "table": "S5.T2.112.112",
        "footnotes": [],
        "references": [
            "We then evaluate the encoder  f  syn subscript f subscript  syn f_{\\theta_{\\mathsf{syn}}} italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT end_POSTSUBSCRIPT  using  Err f  syn  ( D d ) subscript Err subscript f subscript  syn subscript D d \\text{Err}_{f_{\\theta_{\\mathsf{syn}}}}(\\mathcal{D}_{d}) Err start_POSTSUBSCRIPT italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ) , defined in equation  3 , i.e. the generalization error of linear classifier  g D d subscript g subscript D d g_{\\mathcal{D}_{d}} italic_g start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT end_POSTSUBSCRIPT  trained on the representations obtained from encoder  f  syn subscript f subscript  syn f_{\\theta_{\\mathsf{syn}}} italic_f start_POSTSUBSCRIPT italic_ start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT end_POSTSUBSCRIPT  and corresponding labels of downstream task  D d subscript D d \\mathcal{D}_{d} caligraphic_D start_POSTSUBSCRIPT italic_d end_POSTSUBSCRIPT ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Pre-training with Larger Distilled Set Size (5% of Full Data)",
        "table": "S5.T3.70.70",
        "footnotes": [],
        "references": [
            "In equation  4 ,   t  subscript superscript  t \\theta^{*}_{t} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  denotes the model parameters after training on real data up to step  t t t italic_t . The term   ^ t + N subscript ^  t N \\hat{\\theta}_{t+N} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_t + italic_N end_POSTSUBSCRIPT  represents the model parameters after training on the synthetic dataset for  N N N italic_N  steps, starting from   t  subscript superscript  t \\theta^{*}_{t} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . Similarly,   t + M  subscript superscript  t M \\theta^{*}_{t+M} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_t + italic_M end_POSTSUBSCRIPT  refers to the model parameters after  M M M italic_M  steps of training on the real dataset. The primary goal of MTT is to ensure that the encoders weights after training on the synthetic dataset for  N N N italic_N  steps closely match the encoders weights after training on real data for a significantly larger number of steps  M M M italic_M , usually with  N  M much-less-than N M N\\ll M italic_N  italic_M . MTT is agnostic to the training algorithm and doesnt rely on labels; thus, can be applied to dataset distillation for SSL. However, naive application of MTT cannot effectively distill synthetic data for SSL pre-training, as we will discuss next.",
            "Theoretical Evidence for Higher Variance Gradients in SSL.  We now present, in Theorem  4.1 , theoretical evidence, in a simplified setting, demonstrating that the variance of the gradient of SSL over mini-batches is indeed greater than that of SL, i.e.,  Var  (  W L S  S  L  ( B ) ) > Var  (  W L S  L  ( B ) ) Var subscript  W subscript L S S L B Var subscript  W subscript L S L B \\mathrm{Var}\\big{(}\\nabla_{W}L_{SSL}(B)\\big{)}>\\mathrm{Var}\\big{(}\\nabla_{W}L_% {SL}(B)\\big{)} roman_Var (  start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_S italic_S italic_L end_POSTSUBSCRIPT ( italic_B ) ) > roman_Var (  start_POSTSUBSCRIPT italic_W end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT italic_S italic_L end_POSTSUBSCRIPT ( italic_B ) ) . Proof appears in Appendix  C . Appendix  D  presents a more general version of this analysis, when optimizing with synhronous parallel SGD.",
            "where  f S subscript f S f_{S} italic_f start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  and  f T subscript f T f_{T} italic_f start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  represent the student and teacher encoders respectively.  [ 41 ]  trains student models with the aforementioned KD objective and the original SSL Loss. However, we only minimize the MSE between student and teacher representations to avoid the issues with matching SSL training trajectories (discussed in Sec.  4.1 ).",
            "Now, we compute our distillation loss  L D  D  ( D syn ) subscript L D D subscript D syn \\mathcal{L}_{DD}(\\mathcal{D}_{\\mathsf{syn}}) caligraphic_L start_POSTSUBSCRIPT italic_D italic_D end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT )  (shown in equation  4 ) using the parameters of the encoder trained on the synthetic data and the encoder trained on the full data, and update the synthetic data and learning rate,  D syn subscript D syn \\mathcal{D}_{\\mathsf{syn}} caligraphic_D start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT  and   syn subscript  syn \\alpha_{\\mathsf{syn}} italic_ start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT , respectively, to minimize this. Note that  Z syn subscript Z syn \\mathcal{Z}_{\\mathsf{syn}} caligraphic_Z start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT  remains unchanged. We repeat this distillation for  S S S italic_S  iterations. Pseudo-code of MKDT is provided in Alg.  1 .",
            "Downstream Generalization Performance  Table  1  demonstrates that pre-training on CIFAR10 using MKDT with a 2% distilled set improves performance by 8% on CIFAR10 and 5% on downstream tasks over the KRR-ST baseline. Gains on CIFAR10 are consistent across 1% and 5% labeled data, but improvements on downstream tasks are more pronounced with 5%, indicating MKDT scales well with more labeled data. On CIFAR100, MKDT 2% distilled set improves performance by 6% and 8% on CIFAR100 and downstream tasks, respectively. Additionally, MKDT shows up to 3% improvement on downstream tasks for larger, higher-resolution datasets like TinyImageNet (200K examples, 64x64 resolution), highlighting MKDTs scalability. KRR-ST consistently fails to outperform SSL pre-training on random subsets across all settings. In Appendix  B , we verify that this holds for fine-tuning experiments from KRR-ST  [ 21 ] , affirming MKDT as the only effective DD method for SSL pre-training. Table  4  shows that pre-training with larger distilled sets (5% of full data) further enhances performance by up to 13%, confirming MKDT scales effectively with distilled set size as well. Table  7  shows that MKDT outperforms the strongest baseline (random subsets) by 5% on pre-training and 7% on downstream tasks when using 10% and 50% labeled data.",
            "Scaling the Method to Larger Distillation Sets.  In addition to distilling 2% subsets, we also conducted experiments distilling 5% subsets of CIFAR10, CIFAR100, and TinyImageNet to evaluate the generalizability of the method to larger distillation sets. Table  4  shows the scalability of the method to larger distilled set sizes. The experiments procedure are the same as the ones in  A.1  except that we use different distillation hyperparameters for the 5% distilled set. The hyperparameters are summarized in Table  9 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Transfer to Larger Architectures (Pre-Training on CIFAR100 5%, 5% Downsteam Labels)",
        "table": "S5.T4.84.84",
        "footnotes": [],
        "references": [
            "Converting SSL to SL trajectories via KD  We use the objective presented in equation  5  i.e. minimizing the MSE between the  representations  of a student and a teacher model trained with SSL. In doing so, we train the student model to match the performance of the teacher trained with SSL. Note that training the student model by minimizing the MSE loss in equation  5  is a  supervised  objective. Therefore, while the trained student model will produce similar representations to that of the teacher, training with MSE loss is much faster than SSL, as its gradients have a much smaller variance ( c.f.  Fig.  1(a) ). Thus, we can get shorter and lower variance  expert  trajectories from the  student  models trained with KD using the MSE loss, instead of the teacher model trained with SSL. Then, we can generate synthetic examples by matching these shorter and lower variance trajectories, without relying on labels.",
            "Initializing Synthetic Data  Empirically, we find that initializing  D syn subscript D syn \\mathcal{D}_{\\mathsf{syn}} caligraphic_D start_POSTSUBSCRIPT sansserif_syn end_POSTSUBSCRIPT  from the subset of examples from  D real subscript D real \\mathcal{D}_{\\mathsf{real}} caligraphic_D start_POSTSUBSCRIPT sansserif_real end_POSTSUBSCRIPT  that have  high loss  across the expert trajectories, leads to better downstream performance than initializing with random subsets. In particular, for all expert trajectories, we use the encoders after 1 epoch of training and use it to compute the MSE loss for all examples  x i  D real subscript x i subscript D real x_{i}\\in\\mathcal{D}_{\\mathsf{real}} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  caligraphic_D start_POSTSUBSCRIPT sansserif_real end_POSTSUBSCRIPT  i.e.  L MSE  ( f  1   ( x i ) , [ Z T ] i ) subscript L MSE subscript f subscript superscript  1 subscript x i subscript delimited-[] subscript Z T i \\mathcal{L}_{\\mathsf{MSE}}(f_{\\theta^{*}_{1}}(x_{i}),{[\\mathcal{Z}_{T}]}_{i}) caligraphic_L start_POSTSUBSCRIPT sansserif_MSE end_POSTSUBSCRIPT ( italic_f start_POSTSUBSCRIPT italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) , [ caligraphic_Z start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT ] start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) . We then average the loss for examples across encoders from all expert trajectories and choose the examples with highest loss to initialize our synthetic data. Sec.  5  compares initializing MKDT with random subsets and such  high loss  subsets.",
            "Generalization to Larger Architectures  Table  5  compares CIFAR100 5% size MKDT distilled set to 5% size random subsets, using the larger ResNet-10 and ResNet-18 architectures. Across all downstream tasks, we confirm that MKDT distilled sets outperform baselines even when using larger architectures. Surprisingly, the larger ResNet-18 slightly under-performs the smaller ResNet-10. This trend is observed across all baselines, including no-pretraining. We conjecture this is due to larger models requiring a lot more data to be able to use their extra capacity to surpass their smaller counterparts.",
            "Distillation Hyperparameters  We distilled 2% of CIFAR 10, CIFAR 100, and Tiny ImageNet. We used SGD for optimizing the synthetic images with batch size 256, momentum 0.5. We distilled CIFAR 10 and CIFAR 100 with depth 3 ConvNet and Tiny ImageNet with depth 4 ConvNet. We initialize the synthetic learning rate as 0.1 and used SGD with learning rate  10  4 superscript 10 4 10^{-4} 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT  and momentum 0.5 to update it. We distilled the datasets for 5000 iterations and evaluated their performance for all the experiments except those in Table  5 , where we use the distilled dataset after 1000 iterations. The other hyper-parameters are recorded in Table  8 .",
            "From Eq.  51 , it is easy to see that the expectation of the square of element  ( 0 , 0 ) 0 0 (0,0) ( 0 , 0 )  is far larger due to far more   1 subscript italic- 1 \\epsilon_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,   2 subscript italic- 2 \\epsilon_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,   1 2 superscript subscript italic- 1 2 \\epsilon_{1}^{2} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  and   2 2 superscript subscript italic- 2 2 \\epsilon_{2}^{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  terms."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Ablation over SSL Algorithm (SimCLR), Distilled Set Size 2%",
        "table": "S5.T5.42.42",
        "footnotes": [],
        "references": [
            "Ablations  We perform ablations over two factors: 1) initialization and 2) SSL algorithm. Table  2  presents results for pre-training with MKDT using random subset initialization, as well as results for pre-training directly on the high-loss subset initialization used by MKDT. Interestingly, the KD objective for SSL pre-training benefits slightly more from the high-loss subset than from random subsets. Consequently, MKDT initialized from the high-loss subset performs better than when initialized from a random subset. Table  6  shows results for MKDT using a teacher model trained with SimCLR  [ 2 ]  instead of BarlowTwins. Specifically, we train a ResNet-18 for 400 epochs using SimCLR. Here too, MKDT achieves approximately 6% higher performance compared to random subsets across downstream datasets. This confirms that MKDT generalizes across different SSL training algorithms.",
            "Using SimCLR for Obtaining Teacher Representation.  We conducted an ablation study using a teacher model trained with SimCLR  [ 2 ]  instead of Barlow Twins  [ 43 ]  for CIFAR 10 and CIFAR 100. The experiment steps are similar to the ones in  A.1 . During the \"Training the Teacher Model Using SSL\" step , we used the Adam optimizer with batch size 512, learning rate 0.001, and weight decay  10  6 superscript 10 6 10^{-6} 10 start_POSTSUPERSCRIPT - 6 end_POSTSUPERSCRIPT  to train a ResNet18 along with a 2-layer linear projection head for 400 epochs. The projection head included Batch Normalization and ReLU after the first layer, and Batch Normalization after the second layer, projecting to 128 dimensions. Then, we used the pre-projection head representation of the trained model for getting the teacher representation of size 512. The other steps are the same as the one in  A.1 . Table  6  shows that MKDT consistently outperforms the random subset across all downstream datasets and various sizes of labeled data, highlighting the methods generalizability to other contrastive learning methods."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Larger Labeled Data Fractions (10%, 50%), Distilled Set Size 2%",
        "table": "S5.T6.56.56",
        "footnotes": [],
        "references": [
            "Downstream Generalization Performance  Table  1  demonstrates that pre-training on CIFAR10 using MKDT with a 2% distilled set improves performance by 8% on CIFAR10 and 5% on downstream tasks over the KRR-ST baseline. Gains on CIFAR10 are consistent across 1% and 5% labeled data, but improvements on downstream tasks are more pronounced with 5%, indicating MKDT scales well with more labeled data. On CIFAR100, MKDT 2% distilled set improves performance by 6% and 8% on CIFAR100 and downstream tasks, respectively. Additionally, MKDT shows up to 3% improvement on downstream tasks for larger, higher-resolution datasets like TinyImageNet (200K examples, 64x64 resolution), highlighting MKDTs scalability. KRR-ST consistently fails to outperform SSL pre-training on random subsets across all settings. In Appendix  B , we verify that this holds for fine-tuning experiments from KRR-ST  [ 21 ] , affirming MKDT as the only effective DD method for SSL pre-training. Table  4  shows that pre-training with larger distilled sets (5% of full data) further enhances performance by up to 13%, confirming MKDT scales effectively with distilled set size as well. Table  7  shows that MKDT outperforms the strongest baseline (random subsets) by 5% on pre-training and 7% on downstream tasks when using 10% and 50% labeled data.",
            "Scaling the Method to Larger Downstream Labeled Dataset Sizes.  We evaluated the performance for CIFAR 10, CIFAR 100, and TinyImageNet on larger downstream labeled data sizes, specifically 10% and 50% labeled data sizes, using the 2% distilled set obtained with the method illustrated in  A.1 . As shown in Table  7 , MKDT continues to outperform random subset across all downstream datasets and data sizes, demonstrating its scalability with larger labeled data sizes."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  MKDT Hyperparameters on 2% Distilled Set",
        "table": "S5.T7.84.84",
        "footnotes": [],
        "references": [
            "Distillation Hyperparameters  We distilled 2% of CIFAR 10, CIFAR 100, and Tiny ImageNet. We used SGD for optimizing the synthetic images with batch size 256, momentum 0.5. We distilled CIFAR 10 and CIFAR 100 with depth 3 ConvNet and Tiny ImageNet with depth 4 ConvNet. We initialize the synthetic learning rate as 0.1 and used SGD with learning rate  10  4 superscript 10 4 10^{-4} 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT  and momentum 0.5 to update it. We distilled the datasets for 5000 iterations and evaluated their performance for all the experiments except those in Table  5 , where we use the distilled dataset after 1000 iterations. The other hyper-parameters are recorded in Table  8 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  MKDT Hyperparameters on 5% Distilled Set",
        "table": "A1.T8.6.6",
        "footnotes": [],
        "references": [
            "Scaling the Method to Larger Distillation Sets.  In addition to distilling 2% subsets, we also conducted experiments distilling 5% subsets of CIFAR10, CIFAR100, and TinyImageNet to evaluate the generalizability of the method to larger distillation sets. Table  4  shows the scalability of the method to larger distilled set sizes. The experiments procedure are the same as the ones in  A.1  except that we use different distillation hyperparameters for the 5% distilled set. The hyperparameters are summarized in Table  9 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Pre-training on CIFAR100",
        "table": "A1.T9.6.6",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "A2.T10.56.56",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A5.EGx2",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A5.EGx3",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A5.EGx4",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A5.EGx5",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A5.EGx6",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A5.EGx7",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A5.EGx8",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A5.EGx9",
        "footnotes": [],
        "references": []
    },
    "id_table_20": {
        "caption": "",
        "table": "A5.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_21": {
        "caption": "",
        "table": "A5.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_22": {
        "caption": "",
        "table": "A5.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_23": {
        "caption": "",
        "table": "A5.EGx13",
        "footnotes": [],
        "references": []
    },
    "id_table_24": {
        "caption": "",
        "table": "A5.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_25": {
        "caption": "",
        "table": "A5.EGx15",
        "footnotes": [],
        "references": []
    },
    "id_table_26": {
        "caption": "",
        "table": "A5.EGx16",
        "footnotes": [],
        "references": []
    },
    "id_table_27": {
        "caption": "",
        "table": "A5.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_28": {
        "caption": "",
        "table": "A5.EGx18",
        "footnotes": [],
        "references": []
    },
    "id_table_29": {
        "caption": "",
        "table": "A5.EGx19",
        "footnotes": [],
        "references": []
    },
    "id_table_30": {
        "caption": "",
        "table": "A5.EGx20",
        "footnotes": [],
        "references": []
    },
    "id_table_31": {
        "caption": "",
        "table": "A5.EGx21",
        "footnotes": [],
        "references": []
    },
    "id_table_32": {
        "caption": "",
        "table": "A5.EGx22",
        "footnotes": [],
        "references": []
    },
    "id_table_33": {
        "caption": "",
        "table": "A5.EGx23",
        "footnotes": [],
        "references": []
    },
    "id_table_34": {
        "caption": "",
        "table": "A5.EGx24",
        "footnotes": [],
        "references": []
    },
    "id_table_35": {
        "caption": "",
        "table": "A5.EGx25",
        "footnotes": [],
        "references": []
    },
    "id_table_36": {
        "caption": "",
        "table": "A5.EGx26",
        "footnotes": [],
        "references": []
    },
    "id_table_37": {
        "caption": "",
        "table": "A5.EGx27",
        "footnotes": [],
        "references": []
    }
}