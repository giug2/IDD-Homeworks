{
    "id_table_1": {
        "caption": "Table 1.  DQA  dataset statistics",
        "table": "S3.T1.3",
        "footnotes": [],
        "references": [
            "Database maintenance takes more than 20% in total database cost  ( Zhou et al . ,  2024a ) , and it heavily relies on human DBAs to maintain databases. However, it is tedious to configure, tune, optimize and upgrade databases, especially for millions of database instances on the cloud. Therefore, there is a need to design an intelligent database Q & A bot as a copilot of DBAs. As illustrated in Figure  1  (a) 1 1 1 The distribution is calculated based on  68 , 801 68 801 68,801 68 , 801  questions collected from StackExchange. , database Q&As can be generally categorized into (1)  General DB queries , which involve fundamental concepts in the database domain, e.g., SQL grammar, the definition of the E-R data model. (2)  Product-specific queries , which relate to the use of particular database products, e.g., set up a local PostgreSQL database, operational instruction for Snowflake. (3)  Instance-specific queries , which are centered on a particular database instance, e.g., fault diagnosis and data analysis for a banks database running on Postgres v16 with Intel i7 CPU.",
            "Traditionally, seeking solutions to these questions is challenging, often requiring users to consult a vast array of textbooks, product documents and search engines. Users also need to repeat manual operations to obtain contextual information and engage in multi-round dialogues. For example, as shown in Figure  1  (b), users have great trouble getting the final answer by repeatedly  ( i ) i (i) ( italic_i )  searching diagnosis knowledge and then  ( i  i ) i i (ii) ( italic_i italic_i )  verifying in their database environment (e.g., examine error messages or resource usage).",
            "The emergence of Large Language Models (LLMs)  ( OpenAI ,  2022 )  has revolutionized Q & A in various domains, such as medicine  ( Zhang et al . ,  2023 ;  Yuan et al . ,  2023 ) , finance  ( Lee et al . ,  2024 ) , earth science  ( Bi et al . ,  2023 ) , law  ( Colombo et al . ,  2024 ) , etc.  Our vision for the LLM-based DB Q & A bot is that, as shown in Figure  1  (c) 2 2 2 Showcases are abbreviated from answers generated by Baichuan2-cpt-sft 13B, which is Baichuan2 continued-pre-trained, fine-tuned, and enhanced by various modules in our proposed testbed. , the user only needs to input a query, and the bot will autonomously utilize various external sources such as DBMS manual documents and database instance information to provide a customized and accurate answer.",
            "However, whether the current LLMs can fully realize our vision is unclear. For example, as shown in Figure  1  (d) 3 3 3 Showcases are abbreviated from answers generated by GPT-4. , even the most popular and powerful LLM (GPT-4) cannot always give correct and targeted answers. There  lacks a comprehensive evaluation of LLMs on DB-specific Q & A tasks  that can enable us to understand the challenges and opportunities of database QA in the era of LLMs. Although numerous research efforts have evaluated LLMs from different aspects, they have not been dedicated to database Q & A tasks, and face the following challenges.",
            "As shown in Figure  1  (a), the DB queries are divided into three subsets, corresponding to three key skill sets of an LLM-based DB Q & A bot.  General DB queries  can evaluate whether the bot grasps DB-related concepts and knowledge.  Product-specific queries  can evaluate whether the bot applies knowledge and adapts to real-life DB circumstances.  Instance-specific queries  can evaluate the proficiency of the bot in DB troubleshooting.",
            "Data Statistics . Accordingly, we propose methods to construct the dataset for each category separately. As shown in Table  1 , we construct a dataset with bi-lingual Q & A pairs on the three categories, translating English pairs into Chinese and vice-versa, leading to a total of over 120,000 Q & A pairs in each language.",
            "(3) Standard Tool Result . For instance-specific queries, we formulate the Thought-Action-Action_Input-Observation chain using the Algorithm  1 . Given the model  M M M italic_M  to be evaluated, the tool pool  P P P italic_P , the question  q q q italic_q , the prompt template  p 0 subscript p 0 p_{0} italic_p start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , and the ground truth answer and tools invoked in the benchmark (line 1), the LLMs answer is initiated as empty (line 2). We first construct the valid tool set {{T}} in the prompt to include all tools mentioned in the ground truth answer, along with four randomly selected tools from the tool pool  P P P italic_P  (line 3). For each tool invocation on the ground truth answer (line 4), a prompt is constructed using the template and the output of the last invoked tool (line 5), and this prompt is used to obtain an answer from the LLM to extract the next action (line 6). If the answer from the evaluated model  M M M italic_M  uses the same tool as the benchmark (line 7), the Observation part of the benchmark answer is appended to the answer from model  M M M italic_M  (line 8). If a wrong tool is used, it outputs Tool Invocation Failure (line 11), and the process is terminated (line 12).",
            "The above LLMs are implemented on the proposed testbed with QCR, RAG and TIG. Limited by training time and cost, we only implement Baichuan2-13B with pre-training and fine-tuning on the testbed, leading to two additional LLM variants.  (8) Baichuan2-13B-sft , which is a Baichuan2-13B variant fine-tuned as shown in Section  4.1 .  (9) Baichuan2-13B-cpt-sft , which is a Baichuan2-13B variant pre-trained and fine-tuned as shown in Section  4.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Example of general DB Q & A",
        "table": "S3.T2.3",
        "footnotes": [],
        "references": [
            "Step 2: Rewriting . The answers in the Q & A data collected are insufficient as evaluation ground truth. For example, the exam questions are only associated with letter options, and the LLMs generation may be too random when only generating a letter option, making the answer unstable in assessing the professional levels of LLMs. Therefore, for each exam query, we extend the answer by instructing GPT-4 to provide detailed explanations for the answer choices. Meanwhile, online responses are also often overly concise, emotional and subjective. As shown in Table  2 , while the replies explicitly advise the inquirer to learn basic Linux knowledge, they do not specify a learning path or key points, and the tone is not user-friendly. For each online query, we reform the accepted response by instructing GPT-4 to convert it to a detailed, professional and friendly writing style. Table  2  shows the prompts, and the rewritten results are more specific and friendly.",
            "Constructing product-specific Q & A pairs from online sources can be problematic. Because most DBMS products are constantly being updated, online queries can be outdated, and the accepted answers can be unreliable and inaccurate. Thus, it is important to ground the LLM on an external knowledge source, e.g., a certain product manual. We construct the product-specific Q & A pairs via the workflow illustrated in Figure  2 .",
            "Step 2: Q & A Generating . To reduce manual efforts, we use LLM to generate several Q & A pairs on each document segment. The challenge is, directly instructing the LLM to generate Q & A often results in low-quality outcomes. Specifically, the generated questions can overly focus on minor details of the text while neglecting the main points of the given document segment, the answers can be overly concise, and the QA pairs can be repetitive, lacking diverse coverage. Therefore, we propose a novel prompt chain to generate Q & A. As shown in Figure  2 , the prompt chain first requires LLM to summarize the document segments key points. Then, the prompt chain demands LLM to generate a question for each key point that can be answered based on the document segment. Next, the prompt chain asks LLM to produce a detailed, user-friendly answer for each question.",
            "Step 1: Constructing DB tool pool .  (1) We first survey the DB tools commonly used in real-production systems. As shown in Table  3.2 , we identify six types of  common DB tools  that are frequently used for data modeling, database monitoring and performance optimization. The implementation of each tool type, including the exact tool name and the format of input and output, may vary for different DBMS products. 7 7 7 Implementation Details on PostgreSQL and OpenGauss are shown in  [link]  (2) The common tools can not cover all DB tools, especially new ones developed to meet the demands of a real-production system. We expand the common tools to a set of  generalization tools  to evaluate the Q&A bots generalization ability to utilize different DB tools properly. We include scenarios such as Operations Diagnostics, Business Intelligence, Performance Monitoring and Tuning, Data Analysis, System Utilization Analysis, Deployment Issues, Operations Management, Query Optimization, Backup and Recovery, Permission Management, Index Management, and Database Tuning. We require GPT-4 to generate imaginary DB tools that can benefit the above DB scenarios.",
            "As shown in Figure  4 , the LLM initially outputs the COT containing the first tool it wants to invoke and the input for the tool. The tool trigger reads the output and interrupts the LLM from continuing its output. Simultaneously, the TIG module matches the tools name (content after Action:) in the tool pool, which comprises the common tools listed in Table  3.2 . If it finds the appropriate tool, it invokes the corresponding tool interface based on the content after Action_Input:.",
            "QCR Evaluation.    Accuracy is the most critical metric when evaluating the QCR module. If the questions are not accurately categorized, the LLM can not provide customized responses. Given the ground truth class labels mentioned in Section  4.2 , the accuracy is defined as  A  c  c =  i m i , i /  i  j m i , j A c c subscript i subscript m i i subscript i subscript j subscript m i j Acc=\\sum_{i}m_{i,i}/\\sum_{i}\\sum_{j}m_{i,j} italic_A italic_c italic_c =  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i , italic_i end_POSTSUBSCRIPT /  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT , where  m i , j subscript m i j m_{i,j} italic_m start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  is the number of queries that belong to ground truth class  i i i italic_i  that are classified as class  j j j italic_j . We will also report the F1 result of each category  i i i italic_i  in the experiment as  F  1  ( i ) = 2  ( p  r  e  c  i  s  i  o  n  ( i )  r  e  c  a  l  l  ( i ) ) / ( p  r  e  c  i  s  i  o  n  ( i ) + r  e  c  a  l  l  ( i ) ) F 1 i 2  p r e c i s i o n i r e c a l l i p r e c i s i o n i r e c a l l i F1(i)=2(precision(i)\\cdot recall(i))/(precision(i)+recall(i)) italic_F 1 ( italic_i ) = 2 ( italic_p italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n ( italic_i )  italic_r italic_e italic_c italic_a italic_l italic_l ( italic_i ) ) / ( italic_p italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n ( italic_i ) + italic_r italic_e italic_c italic_a italic_l italic_l ( italic_i ) ) . where  p  r  e  c  i  s  i  o  n  ( i ) = m i , i /  j m i , j p r e c i s i o n i subscript m i i subscript j subscript m i j precision(i)=m_{i,i}/\\sum_{j}m_{i,j} italic_p italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n ( italic_i ) = italic_m start_POSTSUBSCRIPT italic_i , italic_i end_POSTSUBSCRIPT /  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  and  r  e  c  a  l  l  ( i ) = m i , i /  j m j , i r e c a l l i subscript m i i subscript j subscript m j i recall(i)=m_{i,i}/\\sum_{j}m_{j,i} italic_r italic_e italic_c italic_a italic_l italic_l ( italic_i ) = italic_m start_POSTSUBSCRIPT italic_i , italic_i end_POSTSUBSCRIPT /  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_j , italic_i end_POSTSUBSCRIPT .",
            "To answer RQ1, we comprehensively evaluate seven popular LLMs by  DQA  in Section  6.2  and construct fine-grained analysis in Section  6.3 . To answer RQ2, we validate pre-training and fine-tuning in Section  6.2 , QCR evaluation in Section  6.4 , RAG ablation in Section  6.5  and TIG evaluation in Section  6.6 .",
            "To further investigate the impact of the RAG module on DB Q&A, we conduct an ablation study on the RAG module. Specifically, based on the experiments described in Section  6.2 , we conduct experiments for the product-specific sub-dataset without the external knowledge provided by the RAG module. The WinRate results are shown in Figure  6 .",
            "From the experimental results, we can observe the following: (1) The RAG module significantly enhances the performance of general-purpose LLMs of any model size, consistent with the analysis in Section  6.2 . (2) The RAG module can boost the performance of small-sized models several times (e.g., 2.4x for Llama2), demonstrating its substantial value for edge deployment models. (3) The RAG module offers smaller performance improvements for models like Baichuan2-sft fine-tuned with domain knowledge in databases. This indicates a high degree of overlap between the improvements from model fine-tuning and those from the RAG module, suggesting that deploying either one is sufficient within a limited budget.",
            "In the experimental setup described in Section  6.2 , we further evaluated LLMs on the instance-specific sub-dataset for Tool Selection Accuracy (TSA) and Tool Format Accuracy (TFA). The experimental results are shown in Figure  7 . From the experimental results, we can observe the following: (1) For general-purpose LLMs, the ability to select and correctly format tools is strongly correlated with their overall capabilities. (2) Specific instruction-following fine-tuning is crucial. Llama-2 and Yuan-2, which have not undergone any instruction-following fine-tuning, lack any tool-invocation abilities. In contrast, the Baichuan2-sft model, fine-tuned with specific tool instruction following, achieves excellent performance. (3) The tool accuracy of Baichuan-2-sft even surpasses that of GPT-4. This suggests that in scenarios with limited tools (strong generalization in tool invocation is not required), specific tool instruction-following fine-tuning can significantly adapt LLMs to the task. (4) Further analysis reveals that in WinRate metrics, the expert evaluator GPT-4 consistently favored responses with successful tool calls as winners. This indicates a strong preference for customized answers utilizing tools, not only among humans but also in LLMs."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Supported types of common DB tools",
        "table": "S3.SS2.tab1.1",
        "footnotes": [],
        "references": [
            "(1) We propose the first benchmark dataset  DQA  in the database Q & A domain. To simulate real-world DB scenarios and cover a wide spectrum of questions, including questions regarding DB general knowledge and complex questions that demand assistance from external manuals and DB tools. The dataset is large-scale with 240,000 Q & A pairs, larger than existing instruction datasets in the IT domain  ( Liu et al . ,  2024 ) . (Section  3 )",
            "There are numerous database  tools  provided in DBMS that support database monitoring, optimizing and analyzing for DB developers, administrators and analysts. The LLMs proficiency in answering instance-specific questions relies on whether LLMs can accurately invoke different tools to obtain the instances contextual information. Thus, as shown in Figure  3 , our dataset construction workflow starts with building a DB tool pool.",
            "Step 1: Constructing DB tool pool .  (1) We first survey the DB tools commonly used in real-production systems. As shown in Table  3.2 , we identify six types of  common DB tools  that are frequently used for data modeling, database monitoring and performance optimization. The implementation of each tool type, including the exact tool name and the format of input and output, may vary for different DBMS products. 7 7 7 Implementation Details on PostgreSQL and OpenGauss are shown in  [link]  (2) The common tools can not cover all DB tools, especially new ones developed to meet the demands of a real-production system. We expand the common tools to a set of  generalization tools  to evaluate the Q&A bots generalization ability to utilize different DB tools properly. We include scenarios such as Operations Diagnostics, Business Intelligence, Performance Monitoring and Tuning, Data Analysis, System Utilization Analysis, Deployment Issues, Operations Management, Query Optimization, Backup and Recovery, Permission Management, Index Management, and Database Tuning. We require GPT-4 to generate imaginary DB tools that can benefit the above DB scenarios.",
            "Step 2: Generating questions .  For each tool, including common tools and generalization tools, we require GPT-4 to generate questions that can be solved by the target tool using the prompt in Figure  3 .  Moreover, we want to effectively evaluate the Q&A bots  planning  ability, which involves adequately combining several DB tools and organizing tools with the right action order. Thus, we manually construct 3-4 questions for each common tool and scenario that demand a chain of multiple tool invocations, and we use these questions as few-shot examples to encourage GPT-4 to generate complex questions. We post-process the resulting questions to ensure that more than  50 % percent 50 50\\% 50 %  of the generated questions are solved by invoking at least two DB tools.",
            "Fine-tuning . To improve the LLMs ability to follow instructions and solve DB-specific tasks, we propose a sequential fine-tuning strategy, including three stages. We prioritize the fine-tuning sequence based on the crucial abilities in DB problem-solving. For instance, the first fine-tuning stage focuses on enhancing the LLMs NL2SQL and table understanding ability using NL2SQL data like Spider  ( Yu et al . ,  2018 )  because it is fundamental in DB tasks. In the second fine-tuning stage, a mixture of different fine-tuning data is adopted. The fine-tuning data includes (1) general conversational datasets like Stanford Alpaca  ( Taori et al . ,  2023 )  to mitigate the LLMs forgetting of general dialogue skills, and (2) reformulated questions from  DQA  using corresponding prompts in the PTE module to enhance the LLMs understanding of the prompt template. The last fine-tuning stage focuses on enhancing the alignment of LLMs final response with DB experts in terms of quality and format, by using answer cases written by DB experts in Section  3 . The specific settings will be detailed in Section  6 .",
            "As shown in Figure  4 , the LLM initially outputs the COT containing the first tool it wants to invoke and the input for the tool. The tool trigger reads the output and interrupts the LLM from continuing its output. Simultaneously, the TIG module matches the tools name (content after Action:) in the tool pool, which comprises the common tools listed in Table  3.2 . If it finds the appropriate tool, it invokes the corresponding tool interface based on the content after Action_Input:.",
            "RAG Evaluation.    Firstly, the performance of RAG can be measured by how precise the retrieval is, i.e., whether the retrieval text is relevant to the question. Thus, we can use the P@n metric. In particular, given that the relevance of external knowledge is crucial to the answer quality, we consider only the top three retrieval results. The precision of the RAG module is defined as  P  @  3 =  i  3 I  { r i , j } / ( 3   j ) P @ 3 subscript i 3 I subscript r i j 3 subscript j P@3=\\sum_{i\\leq 3}I\\{r_{i,j}\\}/(3\\sum_{j}) italic_P @ 3 =  start_POSTSUBSCRIPT italic_i  3 end_POSTSUBSCRIPT italic_I { italic_r start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT } / ( 3  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , where  r i , j subscript r i j r_{i,j} italic_r start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  is the  i  limit-from i i- italic_i - th ranked text block in the top-3 retrieval results of query  j j j italic_j , and  I  { } I I\\{\\} italic_I { }  is an indicator function that returns whether the text block is relevant as labeled in the retrieval annotation in Section  3 ,   j subscript j \\sum_{j}  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  means the RAG modules performance is averaged over all queries.",
            "where  t i , j subscript t i j t_{i,j} italic_t start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  is  i  limit-from i i- italic_i - th tool in COT for the query  j j j italic_j ,  I  { } I I\\{\\} italic_I { }  is an indicator function that returns whether the tool (the name after Action) is labeled in the tool annotation in Section  3 , and  k j subscript k j k_{j} italic_k start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  means the number of LLM tool invocations.",
            "To answer RQ1, we comprehensively evaluate seven popular LLMs by  DQA  in Section  6.2  and construct fine-grained analysis in Section  6.3 . To answer RQ2, we validate pre-training and fine-tuning in Section  6.2 , QCR evaluation in Section  6.4 , RAG ablation in Section  6.5  and TIG evaluation in Section  6.6 ."
        ]
    },
    "id_table_4": {
        "caption": "Figure 3.  Specific Database Instance Dataset Generation",
        "table": "S4.T4.14",
        "footnotes": [],
        "references": [
            "(2) We propose a plug-and-play testbed to experiment with different LLM application strategies. The testbed encapsulates all components potentially involved in the database Q & A, such as Question-Categorization Routing (QCR), Prompt-Template Engineering (PTE), Retriever-Augmented Generation (RAG) and Tool-Invocation Generation (TIG). (Section  4 )",
            "When adopting a general-purpose LLM for DB Q&A, various auxiliary modules are indispensable to leverage and adapt the LLMs general knowledge of linguistic patterns and common senses into the DB environment. Currently, there is no complete database Q&A testbed that incorporates LLM and various auxiliary modules. Table  4  compares the completeness of the proposed testbed with recent LLM adaptation frameworks in the open domain and DB domain. Existing works overlook some important modules. Specifically, without domain continual pre-training and fine-tuning, the LLMs are not able to evolve as new domain-specific topics emerge. Without question routing, the LLMs can become trapped in incorrect logical reasoning paths, e.g., attempting to answer a product-specific problem on its own leads to hallucination while there are product manuals to provide factual groundings. Limited coverage of the DB tools can harm the LLMs ability as a DB agent. On the contrary, our proposed testbed supports a full chain of auxiliary modules for LLMs domain adaptation.",
            "The workflow of the proposed testbed is shown in Figure  4 .",
            "Here, the keyword using tools can trigger the TIG module. The content after Action: and Action_Input: will be used to invocate the appropriate tools correctly, and the tools output will be extracted, summarized and appended after Observation:. More details are described in Section  4.5",
            "As shown in Figure  4 , the module first segments texts from the knowledge base into independent text blocks. Each text block is then processed through an embedding model to be transformed into a dense vector and stored in a vector database, establishing mappings between texts and vectors. Similarly, when a user submits a query, it is also transformed into a vector using the same embedding model, which is then matched against the vectors of the text blocks in the database based on similarity computation. The system obtains the most relevant text block vector, appends it to the prompt and feeds the prompt to the core LLM module. The LLM then generates precise and relevant responses to the users query based on the knowledge.",
            "As shown in Figure  4 , the LLM initially outputs the COT containing the first tool it wants to invoke and the input for the tool. The tool trigger reads the output and interrupts the LLM from continuing its output. Simultaneously, the TIG module matches the tools name (content after Action:) in the tool pool, which comprises the common tools listed in Table  3.2 . If it finds the appropriate tool, it invokes the corresponding tool interface based on the content after Action_Input:.",
            "QCR Evaluation.    Accuracy is the most critical metric when evaluating the QCR module. If the questions are not accurately categorized, the LLM can not provide customized responses. Given the ground truth class labels mentioned in Section  4.2 , the accuracy is defined as  A  c  c =  i m i , i /  i  j m i , j A c c subscript i subscript m i i subscript i subscript j subscript m i j Acc=\\sum_{i}m_{i,i}/\\sum_{i}\\sum_{j}m_{i,j} italic_A italic_c italic_c =  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i , italic_i end_POSTSUBSCRIPT /  start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT , where  m i , j subscript m i j m_{i,j} italic_m start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  is the number of queries that belong to ground truth class  i i i italic_i  that are classified as class  j j j italic_j . We will also report the F1 result of each category  i i i italic_i  in the experiment as  F  1  ( i ) = 2  ( p  r  e  c  i  s  i  o  n  ( i )  r  e  c  a  l  l  ( i ) ) / ( p  r  e  c  i  s  i  o  n  ( i ) + r  e  c  a  l  l  ( i ) ) F 1 i 2  p r e c i s i o n i r e c a l l i p r e c i s i o n i r e c a l l i F1(i)=2(precision(i)\\cdot recall(i))/(precision(i)+recall(i)) italic_F 1 ( italic_i ) = 2 ( italic_p italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n ( italic_i )  italic_r italic_e italic_c italic_a italic_l italic_l ( italic_i ) ) / ( italic_p italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n ( italic_i ) + italic_r italic_e italic_c italic_a italic_l italic_l ( italic_i ) ) . where  p  r  e  c  i  s  i  o  n  ( i ) = m i , i /  j m i , j p r e c i s i o n i subscript m i i subscript j subscript m i j precision(i)=m_{i,i}/\\sum_{j}m_{i,j} italic_p italic_r italic_e italic_c italic_i italic_s italic_i italic_o italic_n ( italic_i ) = italic_m start_POSTSUBSCRIPT italic_i , italic_i end_POSTSUBSCRIPT /  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT  and  r  e  c  a  l  l  ( i ) = m i , i /  j m j , i r e c a l l i subscript m i i subscript j subscript m j i recall(i)=m_{i,i}/\\sum_{j}m_{j,i} italic_r italic_e italic_c italic_a italic_l italic_l ( italic_i ) = italic_m start_POSTSUBSCRIPT italic_i , italic_i end_POSTSUBSCRIPT /  start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT italic_m start_POSTSUBSCRIPT italic_j , italic_i end_POSTSUBSCRIPT .",
            "LLMs are sensitive to prompt engineering. The choice of wording and structure in prompts can influence the quality of LLMs responses. In our practice, the proposed testbed workflow in Section  4  can improve the credibility, relevance and fluency of all tested LLMs. Thus, instead of asking a simple query, we use a standard prompt pipeline to obtain the best output of various LLMs.",
            "To answer RQ1, we comprehensively evaluate seven popular LLMs by  DQA  in Section  6.2  and construct fine-grained analysis in Section  6.3 . To answer RQ2, we validate pre-training and fine-tuning in Section  6.2 , QCR evaluation in Section  6.4 , RAG ablation in Section  6.5  and TIG evaluation in Section  6.6 .",
            "The above LLMs are implemented on the proposed testbed with QCR, RAG and TIG. Limited by training time and cost, we only implement Baichuan2-13B with pre-training and fine-tuning on the testbed, leading to two additional LLM variants.  (8) Baichuan2-13B-sft , which is a Baichuan2-13B variant fine-tuned as shown in Section  4.1 .  (9) Baichuan2-13B-cpt-sft , which is a Baichuan2-13B variant pre-trained and fine-tuned as shown in Section  4.1 ."
        ]
    },
    "id_table_5": {
        "caption": "Figure 4.  LLM-based Database Q&A Testbed Overview",
        "table": "S6.T5.13.13",
        "footnotes": [],
        "references": [
            "(3) We propose a standardized evaluation pipeline that includes modularized validation of the multi-dimensional abilities of LLMs in DB Q & A scenarios. Additionally, we conduct controlled end-to-end evaluations using the  DQA  dataset and the proposed testbed to ensure accuracy and fairness in our assessments. (Section  5 )",
            "Here, the keyword using tools can trigger the TIG module. The content after Action: and Action_Input: will be used to invocate the appropriate tools correctly, and the tools output will be extracted, summarized and appended after Observation:. More details are described in Section  4.5",
            "To answer RQ1, we comprehensively evaluate seven popular LLMs by  DQA  in Section  6.2  and construct fine-grained analysis in Section  6.3 . To answer RQ2, we validate pre-training and fine-tuning in Section  6.2 , QCR evaluation in Section  6.4 , RAG ablation in Section  6.5  and TIG evaluation in Section  6.6 .",
            "We compute the WinRate as described in Section  5  of each LLM versus two competitors.  (1) GPT-3.5-Turbo (Vanilla) : we input the users question to GPT-3.5-Turbo without employing any prompts strategy. This result focuses on demonstrating the difference between a dedicated database Q&A system (the testbed) and typical LLM applications.  (2) GPT-3.5-Turbo (Testbed) : we also equip GPT-3.5-Turbo on the testbed. This result emphasizes the inherent capability difference of each LLM. We have the following insights from Table  5 .",
            "I3: RAG and TIG on the testbed can enhance LLM performance in DB Q&A.  Comparing the upper half with the bottom half of Table  5 , we observe that the WinRate has generally decreased because GPT-3.5-Turbo is more competitive after being equipped with the testbed. It shows that, even for a sophisticated large-scale LLM, the RAG and TIG on the testbed can be beneficial. We will discuss the impact of the RAG and TIG modules in more detail in Section  6.5  and  6.6 .",
            "The experimental results, illustrated in Figure  5  (a)s radar chart, lead to the following conclusions: (1) Numerically, the response capabilities of different models in each sub-field correlate positively with their model size and overall ability, with no model being highly specialized in any particular field. (2) In terms of shape proportion, the radar charts of GPT-3.5 and GPT-4 are similar, showing balanced capabilities across the eight aspects. In contrast, Llama2, Llama3, Baichuan2 and other models based on the llama architecture display a similar pattern, excelling in Performance Monitoring and Tuning but weaker in SQL Programming. This indicates that GPT series models are better at generating accurate SQL Programming instructions, while llama-based models excel in comprehensive subjective analysis.",
            "Next, we report the Multiple-Choice Accuracy, which is measured on the objective questions. As shown in Figure  5  (b), MCA aligns closely with the WinRate on subjective questions. This validates that questions in  DQA  are set with an appropriate difficulty level and require a good understanding of DB knowledge to answer."
        ]
    },
    "id_table_6": {
        "caption": "Table 4.  Comparison of LLM-based Database QA Solutions.",
        "table": "S6.T6.3",
        "footnotes": [],
        "references": [
            "(4) We implement various LLMs and auxiliary modules on the proposed testbed and comprehensively evaluate them by  DQA . We reveal the strengths and weaknesses of different LLMs. For instance, extensively large LLMs are significantly superior to mid-sized open-accessible LLMs in terms of answering instance-specific questions, while their differences in answering DB general questions are less significant. For example, GPT-4 is 1.4  \\times   more accurate than Llama3 8B in DB general questions and 1.9  \\times   more accurate in instance-specific questions. We also show that the modules in the proposed testbed can substantially improve the DB Q & A performance of a mid-sized open-accessible LLM. For example, Baichuan2-13B accommodated in the testbed can be comparable to GPT-4 on  DQA , after targeted pre-training and fine-tuning. We believe that these insights can strongly guide the future development of LLMs for database Q & A tasks. (Section  6 )",
            "Fine-tuning . To improve the LLMs ability to follow instructions and solve DB-specific tasks, we propose a sequential fine-tuning strategy, including three stages. We prioritize the fine-tuning sequence based on the crucial abilities in DB problem-solving. For instance, the first fine-tuning stage focuses on enhancing the LLMs NL2SQL and table understanding ability using NL2SQL data like Spider  ( Yu et al . ,  2018 )  because it is fundamental in DB tasks. In the second fine-tuning stage, a mixture of different fine-tuning data is adopted. The fine-tuning data includes (1) general conversational datasets like Stanford Alpaca  ( Taori et al . ,  2023 )  to mitigate the LLMs forgetting of general dialogue skills, and (2) reformulated questions from  DQA  using corresponding prompts in the PTE module to enhance the LLMs understanding of the prompt template. The last fine-tuning stage focuses on enhancing the alignment of LLMs final response with DB experts in terms of quality and format, by using answer cases written by DB experts in Section  3 . The specific settings will be detailed in Section  6 .",
            "(2) Classifier.  We train an XLNet-based  ( Yang et al . ,  2019 )  classifier. We construct the training data 9 9 9 The sources and statistics of the dataset for these classifiers are detailed in  [link] .  where each question is labeled as unsafe, safe but irrelevant, DB general, product-specific, or instance-specific. The positive samples for the unsafe category are collected from Safety-Prompts  ( Sun et al . ,  2023b )  and BeaverTails-Evaluation  ( Ji et al . ,  2023 ) . The safe but irrelevant samples are collected from Alpaca  ( Taori et al . ,  2023 )  and Longbench  ( Bai et al . ,  2023b ) . The rest three categories are from the training set of  DQA  (which does not overlap with the test set used in Section  6 ).",
            "To answer RQ1, we comprehensively evaluate seven popular LLMs by  DQA  in Section  6.2  and construct fine-grained analysis in Section  6.3 . To answer RQ2, we validate pre-training and fine-tuning in Section  6.2 , QCR evaluation in Section  6.4 , RAG ablation in Section  6.5  and TIG evaluation in Section  6.6 .",
            "I3: RAG and TIG on the testbed can enhance LLM performance in DB Q&A.  Comparing the upper half with the bottom half of Table  5 , we observe that the WinRate has generally decreased because GPT-3.5-Turbo is more competitive after being equipped with the testbed. It shows that, even for a sophisticated large-scale LLM, the RAG and TIG on the testbed can be beneficial. We will discuss the impact of the RAG and TIG modules in more detail in Section  6.5  and  6.6 .",
            "I4: Small-size general LLM can hardly invoke DB tools . Among all tested models, Llama2 and Yuan2 lack pre-aligned instruction following, resulting in no tool usage capabilities. Although models like Baichuan2-13B claim to have instruction-following alignment, experimental results show that their model size still significantly limits their tool usage capabilities. This indicates that instruction enhancement for specific tool usage is necessary when deploying small to mid sized models. We will further discuss the accuracy of the TIG module in Section  6.6 .",
            "We use the testing data include DB-related questions from  DQA , safe but DB-irrelevant questions from Alpaca  ( Taori et al . ,  2023 )  and Longbench  ( Bai et al . ,  2023b )  and unsafe labeled questions from Safety-Prompts  ( Sun et al . ,  2023b )  and BeaverTails Evaluation  ( Ji et al . ,  2023 )   13 13 13 The sources and statistics of the dataset are detailed in  [link] . . The F1 score for each category, average accuracy and response latency deployed on a workstation with a RTX-3090 GPU card are shown in Table  6 .",
            "From the experimental results, we have the following conclusions: (1) Training a dedicated model for safety filtering and classification is essential. Even with the most advanced LLM, GPT-4, the accuracy and latency in classification are inferior to those of a well-trained small model. (2) There is a trade-off between latency and performance across different small models. As shown in Table  6 , the hierarchical classifier can achieve approximately a 0.05 performance improvement at the cost of 300ms.",
            "To further investigate the impact of the RAG module on DB Q&A, we conduct an ablation study on the RAG module. Specifically, based on the experiments described in Section  6.2 , we conduct experiments for the product-specific sub-dataset without the external knowledge provided by the RAG module. The WinRate results are shown in Figure  6 .",
            "From the experimental results, we can observe the following: (1) The RAG module significantly enhances the performance of general-purpose LLMs of any model size, consistent with the analysis in Section  6.2 . (2) The RAG module can boost the performance of small-sized models several times (e.g., 2.4x for Llama2), demonstrating its substantial value for edge deployment models. (3) The RAG module offers smaller performance improvements for models like Baichuan2-sft fine-tuned with domain knowledge in databases. This indicates a high degree of overlap between the improvements from model fine-tuning and those from the RAG module, suggesting that deploying either one is sufficient within a limited budget.",
            "In the experimental setup described in Section  6.2 , we further evaluated LLMs on the instance-specific sub-dataset for Tool Selection Accuracy (TSA) and Tool Format Accuracy (TFA). The experimental results are shown in Figure  7 . From the experimental results, we can observe the following: (1) For general-purpose LLMs, the ability to select and correctly format tools is strongly correlated with their overall capabilities. (2) Specific instruction-following fine-tuning is crucial. Llama-2 and Yuan-2, which have not undergone any instruction-following fine-tuning, lack any tool-invocation abilities. In contrast, the Baichuan2-sft model, fine-tuned with specific tool instruction following, achieves excellent performance. (3) The tool accuracy of Baichuan-2-sft even surpasses that of GPT-4. This suggests that in scenarios with limited tools (strong generalization in tool invocation is not required), specific tool instruction-following fine-tuning can significantly adapt LLMs to the task. (4) Further analysis reveals that in WinRate metrics, the expert evaluator GPT-4 consistently favored responses with successful tool calls as winners. This indicates a strong preference for customized answers utilizing tools, not only among humans but also in LLMs."
        ]
    },
    "global_footnotes": [
        "The distribution is calculated based on",
        "questions collected from StackExchange.",
        "Showcases are abbreviated from answers generated by Baichuan2-cpt-sft 13B, which is Baichuan2 continued-pre-trained, fine-tuned, and enhanced by various modules in our proposed testbed.",
        "Showcases are abbreviated from answers generated by GPT-4.",
        "Implementation Details on PostgreSQL and OpenGauss are shown in",
        "The query prompt can be found on",
        "The sources and statistics of the dataset for these classifiers are detailed in",
        ".",
        "Detailed prompts for WinRate, can be found at",
        ".",
        "The details of the classification prompt can be found at",
        "The sources and statistics of the dataset are detailed in",
        "."
    ]
}