{
    "S4.T1": {
        "caption": "Table 1: Results of training the model on different amounts of images sampled from the fully random dataset.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Image Count</th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">AP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\">1755</th>\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">69.42</td>\n</tr>\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">4387</th>\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">72.23</td>\n</tr>\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">8775</th>\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">72.58</td>\n</tr>\n<tr id=\"S4.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">17550</th>\n<td id=\"S4.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T1.1.5.4.2.1\" class=\"ltx_text ltx_font_bold\">73.01</span></td>\n</tr>\n<tr id=\"S4.T1.1.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">35100</th>\n<td id=\"S4.T1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">72.01</td>\n</tr>\n<tr id=\"S4.T1.1.7.6\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">70200</th>\n<td id=\"S4.T1.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">71.52</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "To further analyse the impact of data augmentation and transfer learning, we compute the AP on the real test set for every two epochs. This allows us to investigate the evolution of the training process. Figure\u00a04 shows the training process for the model trained without any techniques and for the models trained with either data augmentation or transfer learning. When using no techniques we see that learning flattens of very quickly, especially for the datasets with no variation in lighting. When using data augmentation the model is able to learn for longer and at a better rate. This has a way larger impact on the datasets with no variation in lighting. When using transfer learning, the models for all of the datasets show good performance after one epoch, the learning flattens off very quickly however.\nFrom these results we conclude that data augmentation and especially transfer learning help overcome the difference in low level features between real and synthetic data. Data augmentation does so by introducing more variation in these features, leading to a more robust model. Transfer learning achieves this by initialising the model with weights that are already capable of detecting low level features from real world images. Thus when using these techniques, it is beneficial to accurately simulate the poses and lighting conditions of the target domain in the synthetic dataset. When it is not possible to model the target domain, one should try to maximise variation in lighting conditions to achieve the best generalisation. Additionally, it is not necessary to use a large amount of images, even when using a randomised dataset. To confirm this, we trained models on a number of subsequently smaller subsets of the full random synthetic dataset. The results, shown in Table\u00a01, indicate that adding more images only helps until a certain amount as we see a peak at 20k images. The differences in AP are not big, showing that only a few thousand images can already produce a decent model."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Performance in AP on the real test set of models trained with transfer learning, but with different layers retrained.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Layers Retrained</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">AP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\">All</th>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S4.T2.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">81.26</span></td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Stage 3+</th>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">76.71</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Stage 4+</th>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">80.77</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">Stage 5+</th>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">77.13</td>\n</tr>\n<tr id=\"S4.T2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">Heads</th>\n<td id=\"S4.T2.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">71.52</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "The results for this experiment are shown in Table\u00a02. The best performing model is the model where all layers were retrained. The model where only the detection heads were retrained has the worst performance, falling at least five AP points below the other models. This shows us that while it is useful to initialise a network with transfer learning, it is important to let the network learn new features from the synthetic dataset as well."
        ]
    }
}