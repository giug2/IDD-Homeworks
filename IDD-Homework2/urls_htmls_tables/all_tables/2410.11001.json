{
    "id_table_1": {
        "caption": "Table 1:  Experimental results on QMSum, AcademicEval, WCEP, and BookSum datasets over long-context global summarization tasks w.r.t. Rouge-L (R-L), Rouge-1 (R-1), and Rouge-2 (R-2) . Note that the average LLM input token length of GoR and retriever-based baselines is  6  256 6 256 6\\times 256 6  256 , which is about 1.5K. ( BOLD  indicates the best score)",
        "table": "S3.T1.7",
        "footnotes": [],
        "references": [
            "Large Language Models (LLMs) have recently achieved remarkable performance across sorts of language modeling tasks  Achiam et al. ( 2023 ); AI@Meta ( 2024 ) .  Among them, the long-context global summarization task is of great importance, which requires ultra-long context understanding capabilities of LLMs  Li et al. ( 2024a ); Liu et al. ( 2024b ) .  Current attempts to accomplish this task mainly include long-context LLMs  Touvron et al. ( 2023 ); GLM et al. ( 2024 ); Li* et al. ( 2023 ); Tworkowski et al. ( 2023 )  and retrieval-augmented generation (RAG)  Ram et al. ( 2023 ); Yu et al. ( 2023 ); Trivedi et al. ( 2022 ); Jiang et al. ( 2023b ); Asai et al. ( 2023 ) .  In comparison with long-context LLMs that expand their context window to accommodate long-context inputs, RAG performs a cost-effective  retrieve-then-generate  paradigm and provides a few retrieved short text chunks from a long document to LLMs.  In a running RAG system (Figure  1 ), there are usually a large number of historical user queries and LLM-generated responses for a long document.  Nevertheless, these historical responses, which contain informative task-related content, are mostly neglected without sufficient utilization by current RAG approaches.",
            "In this section, we first present some necessary backgrounds in Section  2.1 .  Then, we describe our proposed method sequentially through two sections,  i.e. , Graph Construction (Section  2.2 ) and BERTScore-based Objective for Self-supervised Training (Section  2.3 ).",
            "We conduct comprehensive experiments on QMSum, AcademicEval, WCEP, and BookSum datasets compared with dozens of baselines to evaluate the long-context global summarization capabilities of our proposed method. The results are shown in Table  1 .",
            "GoR consistently outperforms retriever-based methods .  From Table  1 , our proposed GoR beats sparse retrievers, dense retrievers, and hybrid retrievers in every aspect.  Thanks to the constructed graph, which integrates text chunks from long documents and LLM historical responses into a whole, node embeddings can better reflect the complicated correlations with given queries, thus significantly improving the retrieval performance of GoR.  Moreover, the informative content of historical responses can also potentially enhance the summarization task.",
            "In the experimental setup phase  3.1 , we clearly describe the open-source LLMs used in this paper (Mixtral-8x7B-Instruct-v0.1  Jiang et al. ( 2024 )  and LLaMA-2-7b-chat  Touvron et al. ( 2023 ) ), the data preprocessing details (including RAG-related chunk size settings and dataset descriptions, etc.), and the training hyper-parameters.  Moreover, we provide all the prompts used in this work in Appendix  B .  More detailed information can be found in the Appendix.  The above description ensures the reproducibility of this work."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation study on QMSum, AcademicEval, WCEP, and BookSum datasets w.r.t. R-L, R-1, and R-2 . ( BOLD  indicates the best score)",
        "table": "S3.T2.2.2",
        "footnotes": [],
        "references": [
            "In this section, we first present some necessary backgrounds in Section  2.1 .  Then, we describe our proposed method sequentially through two sections,  i.e. , Graph Construction (Section  2.2 ) and BERTScore-based Objective for Self-supervised Training (Section  2.3 ).",
            "Query Simulation.  User queries play a very critical role in the design of GoR since LLM historical responses generated by lots of repetitive, nonsense, or meaningless questions are inherently not beneficial for summarization.  One solution is to use doc2query  Nogueira et al. ( 2019 )  to simulate queries for a long document, but the generated results inevitably suffer from simplicity and rigidity due to the limited text generation capabilities of T5  Raffel et al. ( 2020 ) .  To this end, we directly turn to LLMs for query simulation with temperature sampling instead of greedy decoding for generating meaningful, insightful, and diverse questions.  Specifically, we split a long document into several text chunks  C C \\mathbf{C} bold_C  following the standard procedure of RAG and prompt LLMs to generate a query  q s superscript q s \\mathbf{q}^{\\mathbf{s}} bold_q start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT  based on a randomly selected text chunk  c s superscript c s \\mathbf{c}^{\\mathbf{s}} bold_c start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT .  We repeat the above process until a certain number of non-duplicate queries are generated, which are gathered in pairs with the corresponding text chunks to form a corpus  T = { ( q i s , c i s ) } i = 1 | T | T superscript subscript subscript superscript q s i subscript superscript c s i i 1 T \\mathbf{T}=\\{(\\mathbf{q}^{\\mathbf{s}}_{i},\\mathbf{c}^{\\mathbf{s}}_{i})\\}_{i=1}%  ^{|\\mathbf{T}|} bold_T = { ( bold_q start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_c start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_T | end_POSTSUPERSCRIPT  for further model training (Section  2.3 ).  The prompt for query simulation can be found in Appendix  B .",
            "To investigate how each component of GoR contributes to its performance, we conduct an ablation experiment, and the results are shown in Table  2 .",
            "From Table  2 , we can draw several conclusions.  (1) Directly using the text embeddings from the retriever without training leads to degraded performance ( i.e. , w/o train), highlighting the effectiveness of the learned node embeddings.  (2) Both the contrastive loss  L CL subscript L CL \\mathcal{L}_{\\mathrm{CL}} caligraphic_L start_POSTSUBSCRIPT roman_CL end_POSTSUBSCRIPT  and pair-wise ranking loss  L RANK subscript L RANK \\mathcal{L}_{\\mathrm{RANK}} caligraphic_L start_POSTSUBSCRIPT roman_RANK end_POSTSUBSCRIPT  significantly improve performance. The pair-wise ranking loss imposes stricter ranking constraints on node embeddings, making effective use of the indirect supervision signal from the self-supervised reference summaries.  (3) In-batch negatives are crucial to the performance of contrastive learning. Removing in-batch negatives ( i.e. , w/o in-b neg) leads to a significant drop in results, especially on the WCEP and BookSum datasets.  (4) Compared with self-supervised training, we utilize global reference summaries as labels to conduct supervised training ( i.e. , w/ sup), and the results are significantly worse than the self-supervised setting. We will further discuss it in Section  3.4 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Dataset statistics",
        "table": "A1.T3.1",
        "footnotes": [],
        "references": [
            "In this section, we first present some necessary backgrounds in Section  2.1 .  Then, we describe our proposed method sequentially through two sections,  i.e. , Graph Construction (Section  2.2 ) and BERTScore-based Objective for Self-supervised Training (Section  2.3 ).",
            "Query Simulation.  User queries play a very critical role in the design of GoR since LLM historical responses generated by lots of repetitive, nonsense, or meaningless questions are inherently not beneficial for summarization.  One solution is to use doc2query  Nogueira et al. ( 2019 )  to simulate queries for a long document, but the generated results inevitably suffer from simplicity and rigidity due to the limited text generation capabilities of T5  Raffel et al. ( 2020 ) .  To this end, we directly turn to LLMs for query simulation with temperature sampling instead of greedy decoding for generating meaningful, insightful, and diverse questions.  Specifically, we split a long document into several text chunks  C C \\mathbf{C} bold_C  following the standard procedure of RAG and prompt LLMs to generate a query  q s superscript q s \\mathbf{q}^{\\mathbf{s}} bold_q start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT  based on a randomly selected text chunk  c s superscript c s \\mathbf{c}^{\\mathbf{s}} bold_c start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT .  We repeat the above process until a certain number of non-duplicate queries are generated, which are gathered in pairs with the corresponding text chunks to form a corpus  T = { ( q i s , c i s ) } i = 1 | T | T superscript subscript subscript superscript q s i subscript superscript c s i i 1 T \\mathbf{T}=\\{(\\mathbf{q}^{\\mathbf{s}}_{i},\\mathbf{c}^{\\mathbf{s}}_{i})\\}_{i=1}%  ^{|\\mathbf{T}|} bold_T = { ( bold_q start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_c start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT | bold_T | end_POSTSUPERSCRIPT  for further model training (Section  2.3 ).  The prompt for query simulation can be found in Appendix  B .",
            "Therefore, inspired by BERTScore  Zhang et al. ( 2019 ) , which measures the semantic similarity between the reference and the generated text, we propose to use it to rank all nodes based on the similarity with reference summaries.  By this means, BERTScore fills the gap in the backpropagation so that node embeddings can benefit the indirect supervision signal from the reference summaries.  Nevertheless, global reference summaries contain broad information about long documents, making them highly semantically relevant to many nodes, which will confuse the model optimization direction and degrade the performance (we will discuss it in Section  3.4 ).",
            "From Table  2 , we can draw several conclusions.  (1) Directly using the text embeddings from the retriever without training leads to degraded performance ( i.e. , w/o train), highlighting the effectiveness of the learned node embeddings.  (2) Both the contrastive loss  L CL subscript L CL \\mathcal{L}_{\\mathrm{CL}} caligraphic_L start_POSTSUBSCRIPT roman_CL end_POSTSUBSCRIPT  and pair-wise ranking loss  L RANK subscript L RANK \\mathcal{L}_{\\mathrm{RANK}} caligraphic_L start_POSTSUBSCRIPT roman_RANK end_POSTSUBSCRIPT  significantly improve performance. The pair-wise ranking loss imposes stricter ranking constraints on node embeddings, making effective use of the indirect supervision signal from the self-supervised reference summaries.  (3) In-batch negatives are crucial to the performance of contrastive learning. Removing in-batch negatives ( i.e. , w/o in-b neg) leads to a significant drop in results, especially on the WCEP and BookSum datasets.  (4) Compared with self-supervised training, we utilize global reference summaries as labels to conduct supervised training ( i.e. , w/ sup), and the results are significantly worse than the self-supervised setting. We will further discuss it in Section  3.4 .",
            "Impact of GNN Architectures .  GNNs play a vital role in learning node embeddings. we explore various GNN architectures to study their impact on learning node embeddings, including GCN  Kipf & Welling ( 2016 ) , SGC  Wu et al. ( 2019 ) , GIN  Xu et al. ( 2019 ) , and GraphSAGE  Hamilton et al. ( 2017 ) . Our findings, illustrated in Figure  3 , show that GAT outperforms the other architectures. This is because GAT considers the significance of neighboring nodes when updating node embeddings, allowing the model to effectively capture essential information from the nodes.  Among the other architectures, GraphSAGE performs poorly due to its unstable neighbor sampling mechanism.",
            "In the experimental setup phase  3.1 , we clearly describe the open-source LLMs used in this paper (Mixtral-8x7B-Instruct-v0.1  Jiang et al. ( 2024 )  and LLaMA-2-7b-chat  Touvron et al. ( 2023 ) ), the data preprocessing details (including RAG-related chunk size settings and dataset descriptions, etc.), and the training hyper-parameters.  Moreover, we provide all the prompts used in this work in Appendix  B .  More detailed information can be found in the Appendix.  The above description ensures the reproducibility of this work.",
            "We present dataset statistics in Table  3 .  Due to the limited budget, we randomly select training and test samples for the training and test set and calculate the average input and output token lengths using the LLaMA-2 tokenizer  Touvron et al. ( 2023 )  (samples with short input lengths are filtered out)."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Hyper-parameters",
        "table": "A1.T4.1.1",
        "footnotes": [],
        "references": [
            "Therefore, inspired by BERTScore  Zhang et al. ( 2019 ) , which measures the semantic similarity between the reference and the generated text, we propose to use it to rank all nodes based on the similarity with reference summaries.  By this means, BERTScore fills the gap in the backpropagation so that node embeddings can benefit the indirect supervision signal from the reference summaries.  Nevertheless, global reference summaries contain broad information about long documents, making them highly semantically relevant to many nodes, which will confuse the model optimization direction and degrade the performance (we will discuss it in Section  3.4 ).",
            "From Table  2 , we can draw several conclusions.  (1) Directly using the text embeddings from the retriever without training leads to degraded performance ( i.e. , w/o train), highlighting the effectiveness of the learned node embeddings.  (2) Both the contrastive loss  L CL subscript L CL \\mathcal{L}_{\\mathrm{CL}} caligraphic_L start_POSTSUBSCRIPT roman_CL end_POSTSUBSCRIPT  and pair-wise ranking loss  L RANK subscript L RANK \\mathcal{L}_{\\mathrm{RANK}} caligraphic_L start_POSTSUBSCRIPT roman_RANK end_POSTSUBSCRIPT  significantly improve performance. The pair-wise ranking loss imposes stricter ranking constraints on node embeddings, making effective use of the indirect supervision signal from the self-supervised reference summaries.  (3) In-batch negatives are crucial to the performance of contrastive learning. Removing in-batch negatives ( i.e. , w/o in-b neg) leads to a significant drop in results, especially on the WCEP and BookSum datasets.  (4) Compared with self-supervised training, we utilize global reference summaries as labels to conduct supervised training ( i.e. , w/ sup), and the results are significantly worse than the self-supervised setting. We will further discuss it in Section  3.4 .",
            "Impact of the Number of Simulated Queries During Training .  Query Simulation is a crucial stage in our method design, and we will discuss the impact of the number of simulated queries used during training on learning performance. In particular, we explore this effect by gradually increasing the number of simulated queries used in training.  We present the results in Figure  4 . From a holistic perspective, R-L shows an upward trend as the number of simulated queries increases. Nevertheless, since fewer queries cover less relevant content from long documents, the curves of each dataset have some fluctuations, indicating the occurrence of underfitting.",
            "In the stage of graph construction, due to the number and randomness of the simulated queries, there may be some isolated nodes, and we just keep them in the graph with self-loop edges. During model optimization, BERTScore is pre-computed for efficient training. We present hyper-parameters on QMSum, AcademicEval, WCEP, and BookSum datasets in Table  4 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Experimental results on GovReport and SQuALITY datasets over long-context global summarization tasks w.r.t. Rouge-L (R-L), Rouge-1 (R-1), and Rouge-2 (R-2) . Note that the average LLM input token length of GoR and retriever-based baselines is  6  256 6 256 6\\times 256 6  256 , which is about 1.5K. ( BOLD  indicates the best score)",
        "table": "A1.T5.7",
        "footnotes": [],
        "references": [
            "where    \\tau italic_  is the temperature coefficient.  Note that in the optimization pipeline of GoR, we conduct mini-batch training on the graph level, and each graph is associated with an independent self-supervised training dataset  T T \\mathbf{T} bold_T .  We also leverage in-batch negatives from other graphs since the nodes in them are completely irrelevant content from other long documents (it is not shown in Formula  5  for brevity).",
            "Supervised Training on Global Summarization Queries .  To dive deeper into the differences between self-supervised and supervised training, we carry out additional experiments using global reference summaries.  Specifically, we utilize global summarization queries and reference summaries to serve as a training corpus under the supervised setting.  As there is only one global summarization query for each long document, we replicate it multiple times to match the quantity of self-supervised training data, thus eliminating the impact of the quantity difference.  We present the results on the WCEP and BookSum datasets in Figure  5 , and the  Entropy  denotes the entropy of the similarity distribution between queries and node embeddings.",
            "From Figure  5 , it is evident that in the self-supervised setting, the loss is consistently lower than in the supervised setting.  This suggests that the global reference summaries are highly correlated with many nodes, causing most nodes to exhibit a high semantic similarity with the global query.  As a result, this confuses the models optimization direction.  Additionally, the entropy curve shows that the entropy in the supervised setting is consistently higher than in the self-supervised setting, indicating that the model struggles to select the most similar node.  In contrast, the self-supervised label, derived from a specific part of a long document, contains more focused content and can more effectively guide the models optimization direction.",
            "We conduct extensive experiments on GovReport  Huang et al. ( 2021 )  and SQuALITY  Wang et al. ( 2022a )  datasets, and the results are shown in Table  5 , which demonstrate our proposed GoR is still competitive among baselines on these two datasets."
        ]
    },
    "global_footnotes": [
        "https://www.langchain.com/",
        "https://pytorch.org/",
        "https://www.dgl.ai/",
        "https://www.together.ai/",
        "We use all-MiniLM-L6-v2 as the backbone.",
        "If the input length exceeds the context window limit, we randomly sample continuous text spans of maximum length multiple times to feed into LLMs and calculate the average result."
    ]
}