{
    "S3.T1": {
        "caption": "Table 1: The benchmark datasets used in our experiments, and their statistics.",
        "table": "<table id=\"S3.T1.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.2.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Dataset</th>\n<th id=\"S3.T1.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Attribute/Dim</th>\n<th id=\"S3.T1.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">#Image</th>\n<th id=\"S3.T1.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">Seen/Unseen Class</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.2.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">AwA2<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite>\n</td>\n<td id=\"S3.T1.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">A/85</td>\n<td id=\"S3.T1.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">37322</td>\n<td id=\"S3.T1.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">40/10</td>\n</tr>\n<tr id=\"S3.T1.2.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">CUB<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite>\n</td>\n<td id=\"S3.T1.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">CR/1024</td>\n<td id=\"S3.T1.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">11788</td>\n<td id=\"S3.T1.2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">150/50</td>\n</tr>\n<tr id=\"S3.T1.2.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">SUN<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">54</a>]</cite>\n</td>\n<td id=\"S3.T1.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">A/102</td>\n<td id=\"S3.T1.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">14340</td>\n<td id=\"S3.T1.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">645/72</td>\n</tr>\n<tr id=\"S3.T1.2.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">aPY<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>\n</td>\n<td id=\"S3.T1.2.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">A/64</td>\n<td id=\"S3.T1.2.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">15339</td>\n<td id=\"S3.T1.2.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" style=\"padding-left:4.5pt;padding-right:4.5pt;\">20/12</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "S4.T2": {
        "caption": "Table 2: Zero-Shot learning (ZSL) results only using five and ten example per seen classes to train the model",
        "table": null,
        "footnotes": [],
        "references": [
            "For the ZSL setting, we compare our Meta-VGAN with the state-of-the-art methods on all the datasets in Table 2. The proposed approach shows a significant improvement over the baseline methods for all four datasets. We use a small training set (5 and 10 samples per seen class) to train the model in all of these experiments. We observe that our Meta-VGAN model can train the model well using only a few examples per seen class and significantly better results than the baselines. The proposed approach achieves the substantial absolute performance gains by more than 11.1%percent11.111.1\\%, 5.4%percent5.45.4\\%, 7.2%percent7.27.2\\%, and 6.4%percent6.46.4\\%, in comparison with the baseline methods, on CUB, AWA2, aPY, and SUN datasets, respectively for five examples per seen class."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: GZSL results when only five and ten examples per seen class are used to train the model. We randomly selected 5/10 samples per class, for examples in the CUB dataset for 150 training classes we have new dataset size 150*(5/10), rest samples are not used.",
        "table": null,
        "footnotes": [],
        "references": [
            "In the GZSL settings, the test input is classified into the joint class space of the seen and unseen classes, i.e., Y=YS∪YU𝑌superscript𝑌𝑆superscript𝑌𝑈Y=Y^{S}\\cup Y^{U}. The GZSL setting is more practical as it removes the assumption that test input only comes from unseen classes. For the evaluation metric, we compute the harmonic mean [51] of seen and unseen class accuracy’s: H=2∗S∗U/(S+U)𝐻2𝑆𝑈𝑆𝑈H=2*S*U/(S+U), where S𝑆S and U𝑈U denote the accuracy of seen classes and unseen classes respectively. We evaluate our method on three standard datasets and show the performance comparison with the baseline approach in Table 3."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: The comparison of our ZSL and GZSL result with the recent state-of-the-art generative model when using all samples. All the approach follow the same setting proposed by [51]",
        "table": null,
        "footnotes": [],
        "references": [
            "We train our model and all the baselines using five and ten examples per seen class. For this experiment, we compare our model with CVAE-ZSL, GF-ZSL, f-CLSWGAN, and cycle-UWGAN. We consider both the ZSL (test examples only from unseen classes) as well as generalized ZSL (test examples from seen as well as unseen classes) settings. To prove our proposed Meta-VGAN model’s efficacy, we also perform the experiments using all the examples from seen classes and compare against recent methods (refer Table-4).",
            "We conduct experiments on both standard ZSL and GZSL setting for CUB and AWA2 datasets, and the results are shown in Table 4. Using all the examples from seen classes to train the model, we observe that our model improves the result with a significant margin compared to all baseline approaches in both the settings. For the ZSL setting, our model achieves 9.4%percent9.49.4\\% and 2.1%percent2.12.1\\% improvement as compared to the state-of-the-art result on CUB and AWA2 datasets, respectively. Similarly, for the GZSL setting, the model achieves consistent performance gain harmonic mean (a more meaningful metric) on CUB and AWA2 datasets. We observe that all existing baseline methods show a significant difference in performance between the two regimes, i.e., using all samples and using only a few samples. In contrast, our proposed model shows competitive results in both cases. It shows that the existing baseline approaches are not suitable when the number of seen class examples is very small."
        ]
    },
    "S5.T5": {
        "caption": "Table 5: ZSL results using vanilla CVAE as generative model trained with meta learner over the five standard dataset.",
        "table": "<table id=\"S5.T5.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.2.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        <span id=\"S5.T5.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Datasets</span></th>\n<th id=\"S5.T5.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\" colspan=\"2\">        <span id=\"S5.T5.2.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.2.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T5.2.2.1.1\" class=\"ltx_td\" style=\"padding-left:24.0pt;padding-right:24.0pt;\"/>\n<th id=\"S5.T5.2.2.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        <span id=\"S5.T5.2.2.1.2.1\" class=\"ltx_text ltx_font_bold\">5</span></th>\n<th id=\"S5.T5.2.2.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        <span id=\"S5.T5.2.2.1.3.1\" class=\"ltx_text ltx_font_bold\">10</span></th>\n</tr>\n<tr id=\"S5.T5.2.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T5.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        SUN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">54</a>]</cite></td>\n<td id=\"S5.T5.2.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        54.95</td>\n<td id=\"S5.T5.2.3.2.3\" class=\"ltx_td ltx_align_left ltx_border_tt\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        56.54</td>\n</tr>\n<tr id=\"S5.T5.2.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T5.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        CUB<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite></td>\n<td id=\"S5.T5.2.4.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        63.70</td>\n<td id=\"S5.T5.2.4.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        65.63</td>\n</tr>\n<tr id=\"S5.T5.2.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T5.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        AWA1 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite></td>\n<td id=\"S5.T5.2.5.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        64.20</td>\n<td id=\"S5.T5.2.5.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        64.32</td>\n</tr>\n<tr id=\"S5.T5.2.6.5\" class=\"ltx_tr\">\n<td id=\"S5.T5.2.6.5.1\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        AwA2<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite></td>\n<td id=\"S5.T5.2.6.5.2\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        64.22</td>\n<td id=\"S5.T5.2.6.5.3\" class=\"ltx_td ltx_align_left ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        64.85</td>\n</tr>\n<tr id=\"S5.T5.2.7.6\" class=\"ltx_tr\">\n<td id=\"S5.T5.2.7.6.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        aPY</td>\n<td id=\"S5.T5.2.7.6.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        42.15</td>\n<td id=\"S5.T5.2.7.6.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" style=\"padding-left:24.0pt;padding-right:24.0pt;\">        42.99</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We also compare with vanilla CVAE trained with meta-learning (a simple generative model) with the Meta-VGAN. The results are shown in Table 5, we observe that a complex model (CVAE+CGAN) synthesizes better quality features for unseen classes as compared to a simple generative model. The proposed model outperforms over CVAE based model for all datasets by a significant margin. Therefore the joint model has better generalization ability for the feature generation of the unseen classes."
        ]
    },
    "A2.T6": {
        "caption": "Table 6: The benchmark datasets used in our experiments, and their statistics.",
        "table": "<table id=\"A2.T6.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T6.2.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T6.2.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Dataset</th>\n<th id=\"A2.T6.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Attribute/Dim</th>\n<th id=\"A2.T6.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">#Image</th>\n<th id=\"A2.T6.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Seen/Unseen Class</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T6.2.2.1\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">AwA2<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>\n</td>\n<td id=\"A2.T6.2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">A/85</td>\n<td id=\"A2.T6.2.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">37322</td>\n<td id=\"A2.T6.2.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">40/10</td>\n</tr>\n<tr id=\"A2.T6.2.3.2\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">CUB<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite>\n</td>\n<td id=\"A2.T6.2.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">CR/1024</td>\n<td id=\"A2.T6.2.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">11788</td>\n<td id=\"A2.T6.2.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">150/50</td>\n</tr>\n<tr id=\"A2.T6.2.4.3\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">SUN<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">54</a>]</cite>\n</td>\n<td id=\"A2.T6.2.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">A/102</td>\n<td id=\"A2.T6.2.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">14340</td>\n<td id=\"A2.T6.2.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">645/72</td>\n</tr>\n<tr id=\"A2.T6.2.5.4\" class=\"ltx_tr\">\n<td id=\"A2.T6.2.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">aPY<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>\n</td>\n<td id=\"A2.T6.2.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">A/64</td>\n<td id=\"A2.T6.2.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">15339</td>\n<td id=\"A2.T6.2.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">20/12</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We conduct experiments on four widely used benchmark datasets and compare our approach with several state-of-the-art methods. Our datasets consist of Animals with Attributes (AwA) [51], aPascal and aYahoo (aPY) [7] , Caltech-UCSD Birds-200-2011 (CUB-\n200) [49] and SUN Attribute (SUN-A) [54]. The Table 6 shows description about the datsets.The details description of the datasets are provided in the supplementary material. We report our results on ZSL evaluation metrics proposed by [51]. In particular, for GZSL, the harmonic mean of the seen and unseen class accuracies is reported, while for ZSL, the mean of the per-class accuracy is reported.\nThese evaluation metrics helps to evaluate the model in an unbiased way. ResNet-101 features are used for all the datasets. All the baselines models also use the same features and evaluation metrics.\nDue to space limitations, further details of the implementation and experimental setting are provided in the supplementary material.",
            "To evaluate our proposed model in comparison with several state-of-the-art ZSL and generalized ZSL methods, we applied our approach to the following benchmark ZSL datasets: SUN[54], CUB[49], AwA2[52], and aPY [7]. Table 6 shows the summary of the datasets used and their statistics.\nSUN Scene Recognition: SUN is a fine-grained dataset with 717 scene categories and 14,340 images. We use the widely used split of the dataset for the ZSL setting, 645 seen classes, and 72 unseen classes. The dataset has image-level attributes. For training, we use class-level attributes obtained by combining the attributes of all the images in a class.\nAnimals with Attributes: AwA2 is a coarse-grained dataset with 50 classes and 37,322 images. We follow a standard zero-shot split of 40 seen (train) classes and ten unseen (test) classes. The dataset has 85-dimensional human-annotated class-attributes. \nCaltech UCSD Birds 200: CUB is a fine-grained dataset with 11,788 images from 200 different types of birds, annotated with 312 attributes. We use a zero-shot split of 150 unseen and 50 seen classes. The dataset has image-level attributes like the SUN dataset. We average these image-level attributes of all the classes to obtain class attributes for training.\nAttribute Pascal and Yahoo (aPY): aPY is a coarse-grained dataset with 64 attributes. The dataset has 32 classes. For Zero-Shot learning, we follow a split of 20 Pascal classes for training and 12 Yahoo classes for testing."
        ]
    }
}