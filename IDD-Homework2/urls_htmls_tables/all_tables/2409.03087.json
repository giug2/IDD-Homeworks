{
    "id_table_1": {
        "caption": "Table 1 :  Mean DSC and IoU result for, from Left to Right, UNet_1, UNet_19, SAM, and MedSAM",
        "table": "S4.T1.2.1",
        "footnotes": [],
        "references": [
            "In short, we established a  versatile  framework that can expand limited medical image dataset in  quantity  and also with similar label  quality  as the domain experts. Such dataset will be called  enhanced dataset  (quality + quantity) for the rest of this work. An overview of our framework can be seen in Figure  1 .",
            "Segmentation AI assistance has been proven to be an effective approach to further resolve the complexity of labelling [ 11 ] . However, the use of segmentation AI aiding in the labelling process needs to be versatile for a wide range of tasks regarding medical images, and intuitive for the users to operate. Label Studio [ 15 ]  also supports AI integration during the labelling process. It operates by using another server as a Machine Learning (ML) backend, where the AI model is hosted. We chose MedSAM [ 22 ]  for our AI integration (see Section  4.1.1 ). MedSAM is a zero-shot segmentation model based on Metas Segment Anything model (SAM) [ 18 ]  that has been trained on 1 million medical images. For integration with the labelling platform, we only allow the bounding box functionality to appear when a toggle switch is activated. A user would select the rectangular label from the available selection and draw a box on the image (see Supplementary Section 3). Then, Label Studio will send the necessary information (bounding box coordinates and current image) to MedSAM. MedSAM will consider the spatial relationship and contextual information inherent in medical images when processing the information for segmentation [ 22 ] . Finally, it will send its predicted labels of the specified region back to Label Studio. This would allow for faster and more accurate labelling created by the users.",
            "To evaluate the quality of the segmentations results, the Srensen-Dice index (DSC) [ 31 ]  and the Jaccard Index (IoU) [ 31 ]  are commonly used. These metrics, defined in Equation  1  and  2  respectively, are selected due to their widespread usage and ease of comparison with other publications and methods.",
            "The experiment section is comprised of 3 sub-sections. Section  4.1  contains our initial findings on the framework and evaluates the performance of crowd labellers in achieving expert-level labelling. Section  4.2  evaluates the performance of an enlarged dataset using synthetic images generated by pix2pixGAN. Lastly, Section  4.3  evaluates the effectiveness of training DL segmentation models by combining crowd-labelled images and synthetic images into one enhanced dataset, which is the overall outcome of the framework.",
            "Figure  3  shows example prediction masks generated by UNet_1 and UNet_19 models on MMWHS-CT slice 110. Notably, both predicted masks are characterized by un-smooth and irregular contours with small, scattered regions due to overlapping labels. These graphical observations are confirmed by the metrics in Table  1 , where UNet_1 outperforms UNet_19 with a higher overall metric score. Both models achieve relatively high metric scores above 0.65 for ventricle labels and relatively low scores below 0.53 for atrium labels. Results from Figure  3  and Table  1  suggest that the UNet models are unsuitable for platform AI assistance due to their poor versatility across different datasets. Each label task in the platform requires a new UNet model specifically trained for the corresponding dataset, and even ideally, the sub-datasets, despite the same modality and similar morphological structure. This is validated by the superior performance of UNet_1 over UNet_19, which was achieved even with less training data and reduced training time.",
            "SAM and MedSAM are tested as large-scale models without specific training on any MMWHS datasets. Figure  3  also illustrates the prediction masks of the same testing slice generated by SAM and MedSAM models, characterized by smooth contours and significantly fewer overlapping regions. These observations are corroborated by metric scores detailed in Table  1 . In contrast to the UNet models, which demonstrated higher performance of ventricle labels over atrium labels, both SAM and MedSAM exhibit consistent performance across all labels, which demonstrate their high versatility. Specifically, SAM achieves an average DSC of 0.7344 (IoU of 0.6183), while MedSAM reaches an average DSC of 0.8123 (IoU of 0.7197). MedSAM outperforms SAM and the UNet models in graphical representation and metric evaluations and hence, the chosen segmentation AI for crowd-labelling.",
            "It is evident in Table  1  that the crowd segmentation results from Task 3 are statistically more accurate than those from Task 2 with 95% CI ( p < 0.05 p 0.05 p<0.05 italic_p < 0.05 ), demonstrating that the instructions with ground truth exemplars are crucial to the accuracy of crowd labelling outcomes. To account for the variance between annotators, we merged the crowd labels using pixel-wise majority voting approach with threshold of 4 (as discussed in Section  3.6 ). Table  3  shows the metrics after merging. Notably, LA has a relatively low DSC of 0.3839 (IoU of 0.3627), indicating the difficulty in labelling this ROI. Hence, it is demonstrated that crowd annotators tend to perform better with simple anatomical structures that have less variance between slices. This observation suggests that crowdsourcing should be limited to datasets with relatively simple structures. Nonetheless, this indicates the importance of providing clear guidance and ground truth exemplars to the crowd annotators by the researchers when setting up segmentation tasks."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Mean DSC and IoU with 95% CI for results collected from Task 1 (Hand-drawn), Task 2 (with AI assistance), and Task 3 (with AI assistance and instructions)",
        "table": "S4.T2.2.1",
        "footnotes": [],
        "references": [
            "Image labelling tasks for crowds, especially with medical images, are often complex and result in a lack of accuracy and willingness to participate. We first needed to implement an online platform for the ease of communication and labelling operation from the researchers to a wide audience with various types of device. Label Studio was chosen as the main data labelling platform as it is open source and contains a simple, friendly user interface (UI) (See Figure  2 ) [ 15 ] . An easily navigable UI is key in this study as the labelling process needs to be straightforward to account for various computer literacy in the public. We designed the platform to allow the use of a few tools which includes labelling brush, eraser, zooming, etc. Furthermore, we provided all users with an instructional PDF file containing ground truth label exemplars and a short video to guide them on using the platform (see Supplementary Section 3 and 4). Label Studio was also chosen because it is easily deployed on online servers [ 15 ] , a feature well supported by its extensive developer community. Our current implementation of Label Studio is hosted on the Hugging Face Spaces platform [ 4 ] . This hosting platform was selected for its capability to support application deployment using Docker containers.",
            "To evaluate the quality of the segmentations results, the Srensen-Dice index (DSC) [ 31 ]  and the Jaccard Index (IoU) [ 31 ]  are commonly used. These metrics, defined in Equation  1  and  2  respectively, are selected due to their widespread usage and ease of comparison with other publications and methods.",
            "The experiment section is comprised of 3 sub-sections. Section  4.1  contains our initial findings on the framework and evaluates the performance of crowd labellers in achieving expert-level labelling. Section  4.2  evaluates the performance of an enlarged dataset using synthetic images generated by pix2pixGAN. Lastly, Section  4.3  evaluates the effectiveness of training DL segmentation models by combining crowd-labelled images and synthetic images into one enhanced dataset, which is the overall outcome of the framework.",
            "From Figure  4 , it is demonstrated that the distribution of metrics scores does not vary significantly between Task 1 and Task 2. A noticeable amount of data points clustered around 0 is observed. Quantitatively in Table  2 , it is statistically evident that, for all compartments of the heart, crowd segmentation accuracy from Task 1 and Task 2 are not significantly different ( p > 0.05 p 0.05 p>0.05 italic_p > 0.05 ). This indicates that with only segmentation AI assistance provided, the accuracy of the crowd result would not be improved. It is hypothesised that most of the volunteers have no prior knowledge of heart anatomy, leading to almost random annotations that do not fit with the ground truth. Furthermore, some users also reported being confused with the orientation of the images. It should be noted that once participants became accustomed to the AI tool, the majority of users reported an easier labelling process and reduction in labelling time by simply making quick refinements on the AI-generated regions. This result highlights the success of making the segmentation process easier and less tedium with the deployment of the MedSAM segmentation facilitated tool. To seek for actual improvement in accuracy, we hypothesised that an instruction, in addition to AI assistance, would provide fundamental knowledge to users that will in turn increase labelling accuracy."
        ]
    },
    "id_table_3": {
        "caption": "Table 9 :  Mean DSC and IoU with 95% CI for UNet specifically trained for liver and aorta from control, enlarged, and enhanced FLARE22 dataset",
        "table": "S4.T3.2.1",
        "footnotes": [],
        "references": [
            "The experiment section is comprised of 3 sub-sections. Section  4.1  contains our initial findings on the framework and evaluates the performance of crowd labellers in achieving expert-level labelling. Section  4.2  evaluates the performance of an enlarged dataset using synthetic images generated by pix2pixGAN. Lastly, Section  4.3  evaluates the effectiveness of training DL segmentation models by combining crowd-labelled images and synthetic images into one enhanced dataset, which is the overall outcome of the framework.",
            "Figure  3  shows example prediction masks generated by UNet_1 and UNet_19 models on MMWHS-CT slice 110. Notably, both predicted masks are characterized by un-smooth and irregular contours with small, scattered regions due to overlapping labels. These graphical observations are confirmed by the metrics in Table  1 , where UNet_1 outperforms UNet_19 with a higher overall metric score. Both models achieve relatively high metric scores above 0.65 for ventricle labels and relatively low scores below 0.53 for atrium labels. Results from Figure  3  and Table  1  suggest that the UNet models are unsuitable for platform AI assistance due to their poor versatility across different datasets. Each label task in the platform requires a new UNet model specifically trained for the corresponding dataset, and even ideally, the sub-datasets, despite the same modality and similar morphological structure. This is validated by the superior performance of UNet_1 over UNet_19, which was achieved even with less training data and reduced training time.",
            "SAM and MedSAM are tested as large-scale models without specific training on any MMWHS datasets. Figure  3  also illustrates the prediction masks of the same testing slice generated by SAM and MedSAM models, characterized by smooth contours and significantly fewer overlapping regions. These observations are corroborated by metric scores detailed in Table  1 . In contrast to the UNet models, which demonstrated higher performance of ventricle labels over atrium labels, both SAM and MedSAM exhibit consistent performance across all labels, which demonstrate their high versatility. Specifically, SAM achieves an average DSC of 0.7344 (IoU of 0.6183), while MedSAM reaches an average DSC of 0.8123 (IoU of 0.7197). MedSAM outperforms SAM and the UNet models in graphical representation and metric evaluations and hence, the chosen segmentation AI for crowd-labelling.",
            "It is evident in Table  1  that the crowd segmentation results from Task 3 are statistically more accurate than those from Task 2 with 95% CI ( p < 0.05 p 0.05 p<0.05 italic_p < 0.05 ), demonstrating that the instructions with ground truth exemplars are crucial to the accuracy of crowd labelling outcomes. To account for the variance between annotators, we merged the crowd labels using pixel-wise majority voting approach with threshold of 4 (as discussed in Section  3.6 ). Table  3  shows the metrics after merging. Notably, LA has a relatively low DSC of 0.3839 (IoU of 0.3627), indicating the difficulty in labelling this ROI. Hence, it is demonstrated that crowd annotators tend to perform better with simple anatomical structures that have less variance between slices. This observation suggests that crowdsourcing should be limited to datasets with relatively simple structures. Nonetheless, this indicates the importance of providing clear guidance and ground truth exemplars to the crowd annotators by the researchers when setting up segmentation tasks."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S4.T4.2.1",
        "footnotes": [],
        "references": [
            "Segmentation AI assistance has been proven to be an effective approach to further resolve the complexity of labelling [ 11 ] . However, the use of segmentation AI aiding in the labelling process needs to be versatile for a wide range of tasks regarding medical images, and intuitive for the users to operate. Label Studio [ 15 ]  also supports AI integration during the labelling process. It operates by using another server as a Machine Learning (ML) backend, where the AI model is hosted. We chose MedSAM [ 22 ]  for our AI integration (see Section  4.1.1 ). MedSAM is a zero-shot segmentation model based on Metas Segment Anything model (SAM) [ 18 ]  that has been trained on 1 million medical images. For integration with the labelling platform, we only allow the bounding box functionality to appear when a toggle switch is activated. A user would select the rectangular label from the available selection and draw a box on the image (see Supplementary Section 3). Then, Label Studio will send the necessary information (bounding box coordinates and current image) to MedSAM. MedSAM will consider the spatial relationship and contextual information inherent in medical images when processing the information for segmentation [ 22 ] . Finally, it will send its predicted labels of the specified region back to Label Studio. This would allow for faster and more accurate labelling created by the users.",
            "The experiment section is comprised of 3 sub-sections. Section  4.1  contains our initial findings on the framework and evaluates the performance of crowd labellers in achieving expert-level labelling. Section  4.2  evaluates the performance of an enlarged dataset using synthetic images generated by pix2pixGAN. Lastly, Section  4.3  evaluates the effectiveness of training DL segmentation models by combining crowd-labelled images and synthetic images into one enhanced dataset, which is the overall outcome of the framework.",
            "From Figure  4 , it is demonstrated that the distribution of metrics scores does not vary significantly between Task 1 and Task 2. A noticeable amount of data points clustered around 0 is observed. Quantitatively in Table  2 , it is statistically evident that, for all compartments of the heart, crowd segmentation accuracy from Task 1 and Task 2 are not significantly different ( p > 0.05 p 0.05 p>0.05 italic_p > 0.05 ). This indicates that with only segmentation AI assistance provided, the accuracy of the crowd result would not be improved. It is hypothesised that most of the volunteers have no prior knowledge of heart anatomy, leading to almost random annotations that do not fit with the ground truth. Furthermore, some users also reported being confused with the orientation of the images. It should be noted that once participants became accustomed to the AI tool, the majority of users reported an easier labelling process and reduction in labelling time by simply making quick refinements on the AI-generated regions. This result highlights the success of making the segmentation process easier and less tedium with the deployment of the MedSAM segmentation facilitated tool. To seek for actual improvement in accuracy, we hypothesised that an instruction, in addition to AI assistance, would provide fundamental knowledge to users that will in turn increase labelling accuracy.",
            "It is notable that in Figure  6 , visually the merged crowd segmentation is close to the ground truth, with the edges of the organ identified with high definition. Quantitatively, Table  4  shows that the labelling accuracy is very high in liver and kidney segmentation. Specifically, the DSC is approximately 0.96 for both the liver and aorta (IoU of about 0.93) and about 0.75 for the kidney (IoU of about 0.65). According to Table  5 , despite the complexity of MRI images, the DSC and IoU metrics are acceptable, yielding a DSC of about 0.7 (IoU of about 0.6) for all. These results illustrate that our platform is versatile to ensure the accuracy of labelling tasks across different modalities of images. This endorses the customizability of the crowdsourcing platform by ensuring that different datasets can all be segmented efficiently by the merged crowd labels.",
            "Finally, we combined the high quality merged crowd labels with GAN enlarged dataset as an enhanced dataset to evaluate the potential to improve model training in limited data scenarios further. To ensure the training datasets quality, only 5 FLARE22 Liver and Aorta merged crowd labels are used for the enhanced dataset due to their high similarity to the ground truth, with DSC above 0.95 and IoU above 0.92 as shown in Table  4 . Therefore, as a preliminary evaluation, we trained three segmentation UNet models for the Aorta and Liver using three versions of the FLARE22 dataset: a control dataset, an enlarged dataset, and an enhanced dataset. The control dataset included 10 real images; the enlarged dataset consisted of 10 real images and 10 synthetic images; and the enhanced dataset comprised 10 real images, 10 synthetic images, and 5 merged crowd labels."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S4.T5.2.1",
        "footnotes": [],
        "references": [
            "It is notable that in Figure  6 , visually the merged crowd segmentation is close to the ground truth, with the edges of the organ identified with high definition. Quantitatively, Table  4  shows that the labelling accuracy is very high in liver and kidney segmentation. Specifically, the DSC is approximately 0.96 for both the liver and aorta (IoU of about 0.93) and about 0.75 for the kidney (IoU of about 0.65). According to Table  5 , despite the complexity of MRI images, the DSC and IoU metrics are acceptable, yielding a DSC of about 0.7 (IoU of about 0.6) for all. These results illustrate that our platform is versatile to ensure the accuracy of labelling tasks across different modalities of images. This endorses the customizability of the crowdsourcing platform by ensuring that different datasets can all be segmented efficiently by the merged crowd labels."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "S4.T6.2.1",
        "footnotes": [],
        "references": [
            "It is evident in Table  1  that the crowd segmentation results from Task 3 are statistically more accurate than those from Task 2 with 95% CI ( p < 0.05 p 0.05 p<0.05 italic_p < 0.05 ), demonstrating that the instructions with ground truth exemplars are crucial to the accuracy of crowd labelling outcomes. To account for the variance between annotators, we merged the crowd labels using pixel-wise majority voting approach with threshold of 4 (as discussed in Section  3.6 ). Table  3  shows the metrics after merging. Notably, LA has a relatively low DSC of 0.3839 (IoU of 0.3627), indicating the difficulty in labelling this ROI. Hence, it is demonstrated that crowd annotators tend to perform better with simple anatomical structures that have less variance between slices. This observation suggests that crowdsourcing should be limited to datasets with relatively simple structures. Nonetheless, this indicates the importance of providing clear guidance and ground truth exemplars to the crowd annotators by the researchers when setting up segmentation tasks.",
            "It is notable that in Figure  6 , visually the merged crowd segmentation is close to the ground truth, with the edges of the organ identified with high definition. Quantitatively, Table  4  shows that the labelling accuracy is very high in liver and kidney segmentation. Specifically, the DSC is approximately 0.96 for both the liver and aorta (IoU of about 0.93) and about 0.75 for the kidney (IoU of about 0.65). According to Table  5 , despite the complexity of MRI images, the DSC and IoU metrics are acceptable, yielding a DSC of about 0.7 (IoU of about 0.6) for all. These results illustrate that our platform is versatile to ensure the accuracy of labelling tasks across different modalities of images. This endorses the customizability of the crowdsourcing platform by ensuring that different datasets can all be segmented efficiently by the merged crowd labels.",
            "Results from Table  6 ,  7 , and  8  suggest all modalities have shown notable improvements in training scores with the enlarged dataset, apart from few IoUs fluctuated at a slightly lower score. Notably, Table  7  shows an average of 15.9% increase for DSC and 11.1% increase for IoU. Aorta, as the hardest segmented ROI in FLARE22, has improved from a DSC value of 0.1490 (IoU of 0.6580) to a DSC value of 0.3392 (IoU of 0.7802). Overall, this result validates the effectiveness of incorporating synthetic data to improve model training outcomes in data-limited scenarios."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S4.T7.2.1",
        "footnotes": [],
        "references": [
            "To evaluate the versatility of the pix2pixGAN model, we trained it on datasets comprising different diverse segmentation pairs from medical imaging modalities and organs. The results demonstrated the models ability to generate synthetic images across different modalities, including MMWHS-CT, MMWHS-MRI, and FLARE22. Furthermore, the models versatility was evidenced by its capacity to generate clearly identifiable organs and compartment tissues, such as the heart in MMWHS, and the liver and kidneys in FLARE22, as we can see in Figure  7 . The generated synthetic images exhibited distinct edges and good contrast, particularly when multiple organs are present within a single image, with characterisable and identifiable morphology. These findings demonstrated pix2pixGANs high versatility in generating synthetic images across various modalities and anatomical structures in medical imaging. However, it is noted that the synthetic image organs are often found at wrong vertebral levels, which indicates a lack of realism. A potential improvement is suggested where landmarks apart from ROIs could be included during synthesis to refine anatomical accuracy.",
            "Results from Table  6 ,  7 , and  8  suggest all modalities have shown notable improvements in training scores with the enlarged dataset, apart from few IoUs fluctuated at a slightly lower score. Notably, Table  7  shows an average of 15.9% increase for DSC and 11.1% increase for IoU. Aorta, as the hardest segmented ROI in FLARE22, has improved from a DSC value of 0.1490 (IoU of 0.6580) to a DSC value of 0.3392 (IoU of 0.7802). Overall, this result validates the effectiveness of incorporating synthetic data to improve model training outcomes in data-limited scenarios."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S4.T8.2.1",
        "footnotes": [],
        "references": [
            "Results from Table  6 ,  7 , and  8  suggest all modalities have shown notable improvements in training scores with the enlarged dataset, apart from few IoUs fluctuated at a slightly lower score. Notably, Table  7  shows an average of 15.9% increase for DSC and 11.1% increase for IoU. Aorta, as the hardest segmented ROI in FLARE22, has improved from a DSC value of 0.1490 (IoU of 0.6580) to a DSC value of 0.3392 (IoU of 0.7802). Overall, this result validates the effectiveness of incorporating synthetic data to improve model training outcomes in data-limited scenarios."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S4.T9.2.1",
        "footnotes": [],
        "references": [
            "The metrics present in Table  9  indicate a significant improvement ( p < 0.001 p 0.001 p<0.001 italic_p < 0.001  for both liver and aorta DSC using unpaired t-test) in segmentation accuracy from the control dataset to the enlarged dataset, with further enhancement ( p < 0.001 p 0.001 p<0.001 italic_p < 0.001  for both liver and aorta DSC using unpaired t-test) when utilising the enhanced dataset compared to the enlarged dataset. Overall, the enhance dataset performance has significant improvement when compared with the control dataset ( p < 0.0001 p 0.0001 p<0.0001 italic_p < 0.0001  for both liver and aorta DSC using unpaired t-test). Notably, the enhanced dataset achieves a 12.3% increase in DSC for liver segmentation compared to the control dataset. Furthermore, the DSC for the Aorta increase substantially, from 0.1467 to 0.5045, and IoU improve from 0.4932 to 0.7291, highlighting enhanced feature extraction for challenging segmented ROIs."
        ]
    }
}