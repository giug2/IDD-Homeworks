{
    "id_table_1": {
        "caption": "Table 1:  Feature approximation results on synthetic datasets. We compare the function approximation ability of different pre-training methods given the same architecture and pre-training pipeline. All presented numbers are averaged across three runs and scaled by 100 for better readability. Lower numbers are better.",
        "table": "S3.E2",
        "footnotes": [],
        "references": [
            "Many works in time series analysis mimic the transformer-based modeling approaches in language by building sequences from samples through segmenting time series into periods of time points (Figure  1 (A)  (Nie et al.,  2022 ; Liu et al.,  2022 ; Zhang & Yan,  2023 ) . An AR transformer on top of it would predict the next time period based on the existing ones (Figure  1 (B)  (Garza & Mergenthaler-Canseco,  2023 ) . However, this modeling approach has two issues: (1) slicing time series into periods  breaks nonlocal functional properties  like trend or periodicity, and often requires special remedies to compensate for the issues  (Zhou et al.,  2022 ) ; (2) the predicted time periods  lack generalizability , as the prediction is sensitive to the length of chunks, the position of where the slicing happens, and the characteristics of datasets. To compensate for the issues of patching and build generalizability into the transformer, recent works rely on the usage of operators like Fourier neural operators or Koopman operators, but they either require specially engineered coding blocks  (Liu et al.,  2023a ) , or a specific set of predetermined bases that may vary across datasets  (Liu et al.,  2024b ) .",
            "Inspired by  Tian et al. ( 2024 )  that replaces next-patch prediction with a next-resolution prediction task in computer vision, in this work, we re-think alternative approaches to build a coarse-to-fine sequence of time series by considering them as functions of time. Instead of slicing time series into periods, we consider time series samples  S S {\\mathbf{S}} bold_S  as a sampled version of an underlying function  g  ( t ) g t g(t) italic_g ( italic_t )  that can be structurally simplified in its functional form (Figure  1 (A)). Instead of mapping the sample onto fixed sets of basis like Taylor or Fourier series, we isolate functional components in a data-dependent way by building degradation operators  d k  (  ) subscript d k  d_{k}(\\cdot) italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT (  )  of different intensity levels  k k k italic_k  and progressively applying them on the signals. By doing so, we generate an alternative sequence of samples consisting of augmented variants of the signal with increasing amount of information, offering an interconnected yet simplified representation of the original signal. We train an autoregressive transformer to learn the connection of the different set of functionals, building a knowledge map of different functional components  1 1 1 For example, in brain decoding tasks, signals often contain cross-frequency coupling where low-frequency components drive the high-frequency components  (Klimesch,  2018 ; Donoghue et al.,  2020 ) . . Analogous to the next word prediction task that learns narrative in languages by completing sentences, we denote our method as the Narratives of Time Series (NoTS) because it learns the functional narrative of temporal signals (Figure  1 (C)).",
            "Alternative to modeling time series as sequences of fragmented time periods, our framework is built on the idea to model time series as sequences of constructed temporal functions with transformers. We begin by introducing the high-level objective in Section  3.1 , and then introduce the pre-training method NoTS in Section  3.2  as well as how to adapt it in real-world tasks in Section  3.3 .",
            "We estimate different pre-training methods capability of approximating functions with the feature regression task. The ground truth of features are built based on common signal processing analysis methods Slope Sign Change (SSC, 32D) and Willison Amplitude (WAMP, 32D), and we also include the Hurst index ( H H \\mathcal{H} caligraphic_H -index, 1D) for the fBM dataset and the band power (b. power, 96D) for the sinusoids (Appendix  B.1 ). Note that SSC and WAMP are both implemented with global thresholding, making them discontinuous sequence-to-sequence functions. Following Section  3.3 , we train a VQVAE  (Van Den Oord et al.,  2017 ) , masked autoencoder (MAE)  (Dong et al.,  2024 ) , frequency-aware MAE (FAMAE)  (Liu et al.,  2023a ) , next-period prediction transformer  (Garza & Mergenthaler-Canseco,  2023 ) , and  NoTS-lw  on the synthetic datasets by appending them with a regression task adaptor to validate the performance of our proposed method.",
            "As shown in Table  1 , across the board,  NoTS-lw  significantly outperforms all other pre-training methods given the same architecture and training pipeline. The relative improvement is especially pronounced in the fBm dataset, where data has complicated covariance architecture that was found relevant in many real-world applications, where we have  26 % percent 26 26\\% 26 %  improvements across the features.",
            "which shows that the attention map converges given deterministic  X X {\\mathbf{X}} bold_X  and  W = ( W K i )   W Q i W superscript superscript subscript W K i top superscript subscript W Q i {\\mathbf{W}}=({\\mathbf{W}}_{K}^{i})^{\\top}{\\mathbf{W}}_{Q}^{i} bold_W = ( bold_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT . Thus, Eq.  10  holds by considering the self attention operator  Attn  ( X ) Attn X \\operatorname{Attn}({\\mathbf{X}}) roman_Attn ( bold_X )  as a convex combination of attention heads given deterministic  X X {\\mathbf{X}} bold_X ,  W O i  R d  m superscript subscript W O i superscript R d m \\bm{W}_{O}^{i}\\in\\mathbb{R}^{d\\times m} bold_italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d  italic_m end_POSTSUPERSCRIPT ,  W V i  R m  d superscript subscript W V i superscript R m d \\bm{W}_{V}^{i}\\in\\mathbb{R}^{m\\times d} bold_italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_m  italic_d end_POSTSUPERSCRIPT .",
            "We show the complete classification results in Table  8  and Table  9  and the complete imputation results in Table  10 . The averaged results are presented in Table  2 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparisons between NoTS and other pre-training methods on real-world datasets. We categorize the results based on (a) if adaptors are used, and (b) if the weights of the pre-trained models are frozen. We compute an average error rate (   \\downarrow  ) to compare the final performance of different methods in each condition.",
        "table": "S5.T1.44.44.44",
        "footnotes": [],
        "references": [
            "Alternative to modeling time series as sequences of fragmented time periods, our framework is built on the idea to model time series as sequences of constructed temporal functions with transformers. We begin by introducing the high-level objective in Section  3.1 , and then introduce the pre-training method NoTS in Section  3.2  as well as how to adapt it in real-world tasks in Section  3.3 .",
            "and   k subscript  k \\Omega_{k} roman_ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  represents the set of sequence positions corresponding to  R k subscript R k {\\mathbf{R}}_{k} bold_R start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT . While  E E \\mathcal{E} caligraphic_E  and  D D \\mathcal{D} caligraphic_D  can be any encoder/decoder architectures, we implement a lightweight model NoTS-lw with a simple channel-independent 1D-ResNet encoder/decoder block to maintain fidelity in the token space. We also report results with different encoder/decoder architectures in Table  2 .",
            "Proof.  We construct a negative example with  d = 1 d 1 d=1 italic_d = 1 . Consider a set of input functions  g M  ( t ) = sin  ( M  t ) / M subscript g M t M t M g_{M}(t)=\\sin(Mt)/M italic_g start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ( italic_t ) = roman_sin ( italic_M italic_t ) / italic_M , the target functions under the differential operator are  h M  ( t ) = cos  ( M  t ) subscript h M t M t h_{M}(t)=\\cos(Mt) italic_h start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ( italic_t ) = roman_cos ( italic_M italic_t ) . As  M M M italic_M  increases, the input function converges uniformly to a constant zero function, which gives a sampled input matrix  X  0  D  X 0 D \\bm{X}\\rightarrow\\mathbf{0}\\in\\mathcal{D} bold_italic_X  bold_0  caligraphic_D . At limit, the studied transformer network  f P  ( X ) subscript f P X f_{P}(\\bm{X}) italic_f start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT ( bold_italic_X )  converges to a fixed matrix  B B {\\mathbf{B}} bold_B  (see Appendix  A.2 ). Thus, given a sampling plan of interval  t i + 1  t i =  / M subscript t i 1 subscript t i  M t_{i+1}-t_{i}=\\pi/M italic_t start_POSTSUBSCRIPT italic_i + 1 end_POSTSUBSCRIPT - italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_ / italic_M  and two initial starting points  t 1 ( 1 ) = 0 superscript subscript t 1 1 0 t_{1}^{(1)}=0 italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT = 0  and  t 1 ( 2 ) =  superscript subscript t 1 2  t_{1}^{(2)}=\\pi italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT = italic_ , we form  X 1 subscript X 1 \\bm{X}_{1} bold_italic_X start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  X 2 subscript X 2 \\bm{X}_{2} bold_italic_X start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  that give:",
            "To examine the performance of NoTS in real-world applications, we perform multi-task validation following the setups in  Wu et al. ( 2022 ) . Specifically, we perform the classification task on the UCR subset  (Dau et al.,  2019 )  and UEA subset  (Bagnall et al.,  2018 ) ; the imputation task on the ETDataset  (Zhou et al.,  2021 ) , and the anomaly detection task on MSL  (Hundman et al.,  2018 ) , PSM  (Abdulaal et al.,  2021 ) , SWaT  (Mathur & Tippenhauer,  2016 ) , and SMD  (Su et al.,  2019 )  datasets. We follow  Wu et al. ( 2022 )  for standard data pre-processing and task deployment pipeline, except for the imputation task where we tested a more challenging variant of channel-wise imputation (see Appendix  B.2.2  and  B.2.3  for details and original imputation results).",
            "As shown in Table  2 , with or without parameters frozen, NoTS-lw significantly outperforms all other pre-training methods. Specifically, given the same pre-training pipeline and architecture, NoTS-lw outperforms other method across all tasks by up to  6 % percent 6 6\\% 6 %  average. Interestingly, we note that NoTS-lw show comparable performance on imputation tasks, where MAE-like architectures are trained to perform the task. Additionally, when attaching NoTS on existing architectures PatchTST  (Nie et al.,  2022 )  and iTransformer  (Liu et al.,  2023b ) , NoTS improves their performance without specific backbone or adaptors, showing the versatility of the pre-training method.",
            "Interestingly, we should like to emphasize on the context-aware generalization ability of NoTS. With the architecture frozen (first 4 rows of Table  2 ), we only train < 1 % percent 1 1\\% 1 %  of the parameters, yet it performs  82 % percent 82 82\\% 82 %  performance, potentially demonstrating the context-aware generalization.",
            "We show the complete classification results in Table  8  and Table  9  and the complete imputation results in Table  10 . The averaged results are presented in Table  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 4:  Detailed information about the selected datasets from the UCR archive.",
        "table": "S5.T2.26.26.24",
        "footnotes": [],
        "references": [
            "Alternative to modeling time series as sequences of fragmented time periods, our framework is built on the idea to model time series as sequences of constructed temporal functions with transformers. We begin by introducing the high-level objective in Section  3.1 , and then introduce the pre-training method NoTS in Section  3.2  as well as how to adapt it in real-world tasks in Section  3.3 .",
            "Proof.  See Appendix  A.3 .",
            "We estimate different pre-training methods capability of approximating functions with the feature regression task. The ground truth of features are built based on common signal processing analysis methods Slope Sign Change (SSC, 32D) and Willison Amplitude (WAMP, 32D), and we also include the Hurst index ( H H \\mathcal{H} caligraphic_H -index, 1D) for the fBM dataset and the band power (b. power, 96D) for the sinusoids (Appendix  B.1 ). Note that SSC and WAMP are both implemented with global thresholding, making them discontinuous sequence-to-sequence functions. Following Section  3.3 , we train a VQVAE  (Van Den Oord et al.,  2017 ) , masked autoencoder (MAE)  (Dong et al.,  2024 ) , frequency-aware MAE (FAMAE)  (Liu et al.,  2023a ) , next-period prediction transformer  (Garza & Mergenthaler-Canseco,  2023 ) , and  NoTS-lw  on the synthetic datasets by appending them with a regression task adaptor to validate the performance of our proposed method.",
            "We present the data and latent visualizations in Figure  3 . In Figure  3 (A), we show the original data sequence  { S i } i = 1 K superscript subscript subscript S i i 1 K \\{{\\mathbf{S}}_{i}\\}_{i=1}^{K} { bold_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT  and the reconstructed data sequence  { S i  } i = 2 K superscript subscript superscript subscript S i  i 2 K \\{{\\mathbf{S}}_{i}^{\\prime}\\}_{i=2}^{K} { bold_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT . Note that the original signal  S K subscript S K {\\mathbf{S}}_{K} bold_S start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT  was not passed into the transformer. We can see that the predicted sequence has information that is not presented in previous signals, showing the function prediction capacity of the transformer. In Figure  3 (B), we plot the token space before or after the AR transformer using the PCA reduction on  { R i } i = 1 K superscript subscript subscript R i i 1 K \\{{\\mathbf{R}}_{i}\\}_{i=1}^{K} { bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT  and  { R i  } i = 2 K superscript subscript superscript subscript R i  i 2 K \\{{\\mathbf{R}}_{i}^{\\prime}\\}_{i=2}^{K} { bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , respectively. When coloring the tokens differently based on their degradation parameter  i i i italic_i , we observe that: (1) In original token space  { R i } i = 1 K superscript subscript subscript R i i 1 K \\{{\\mathbf{R}}_{i}\\}_{i=1}^{K} { bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , severely degraded signals generates more clustered tokens, and the tokens would gradually disperse as signals become more realistic; (2) The predicted tokens  { R i  } i = 2 K superscript subscript superscript subscript R i  i 2 K \\{{\\mathbf{R}}_{i}^{\\prime}\\}_{i=2}^{K} { bold_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_i = 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT  would generate a token space with similar behaviour without seeing the original set of tokens  R K subscript R K {\\mathbf{R}}_{K} bold_R start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT . This behaviour demonstrates the autoregressive capacity of the transformer.",
            "To examine the performance of NoTS in real-world applications, we perform multi-task validation following the setups in  Wu et al. ( 2022 ) . Specifically, we perform the classification task on the UCR subset  (Dau et al.,  2019 )  and UEA subset  (Bagnall et al.,  2018 ) ; the imputation task on the ETDataset  (Zhou et al.,  2021 ) , and the anomaly detection task on MSL  (Hundman et al.,  2018 ) , PSM  (Abdulaal et al.,  2021 ) , SWaT  (Mathur & Tippenhauer,  2016 ) , and SMD  (Su et al.,  2019 )  datasets. We follow  Wu et al. ( 2022 )  for standard data pre-processing and task deployment pipeline, except for the imputation task where we tested a more challenging variant of channel-wise imputation (see Appendix  B.2.2  and  B.2.3  for details and original imputation results).",
            "In Table  3 , we perform ablations of NoTS by isolating the effective components of NoTS-lw in the feature regression task (the  H H \\mathcal{H} caligraphic_H -index). Specifically, we train three variants of NoTS-lw by (1) removing the latent consistency term in training loss, (2) removing the autoregressive masking within transformer, creating a transformer that merely bridges tokens of the augmented samples, (3) removing the connections among constructed augmentations, and using degradation operator only as augmentations. As expected, removing the latent consistency term would cause distributional shift as the model never sees raw data, and would result in severe performance degradation. Interestingly, training a transformer that connects augmented samples can also provide improved performance, as observed in other works  (Hou et al.,  2022 ; Liu et al.,  2024a ) .",
            "One might relate our work with diffusion models  (Ho et al.,  2020 )  by using a stochastic additive Gaussian noise of varying degrees as a degradation operator. We attempted this as model variant (4) in Table  3 , yet the performance is inferior to the convolution-based degradation operators. One hypothesis is that time series data is inherently noisy, and adding Gaussian noise instead of performing smoothing or filtering can be less effective, as observed in audio signals  (Dieleman,  2024 ) . Building connections between NoTS and cold diffusion models with deterministic degradation operators  (Bansal et al.,  2024 ) , or more recent diffusion models  (Chen et al.,  2023 )  can be an exciting future research direction.",
            "While this work aims only to provide an initial experimental exploration of the proposed pre-training methodology NoTS, we attempted a pilot study to increase the size of NoTS-lw to demonstrate its potential given more parameters. We trained four models with 127k, 243k (used in all previous experiments), 641k, 2.1M parameters to observe their performance. As shown in Figure  3 (C), when fixing the amount of training data, training the models to convergence with increased parameters leads to increased performance, potentially following a power law curve of AR frameworks in language and computer vision  (Kaplan et al.,  2020 ; El-Nouby et al.,  2024 ) ."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:  Detailed information about the selected datasets from the UEA archive.",
        "table": "S5.T3.4.2.2",
        "footnotes": [],
        "references": [
            "We first justify the construction of the alternative sequence using an intuitive function approximation analysis in Section  4 . When learning time series with transformers, under the universal approximation framework  (Yun et al.,  2019 ) , we show that learning time series as sequences of periods of time points would cause approximation issues, as performing sampling operation on commonly encountered time series signal processing operators (e.g., differentiation) creates discontinuous sequence-to-sequence functions. Instead, such limitations can be bypassed by forming and learning from the alternative function sequences, as long as either (1) the constructed sequence is expressive, or (2) an expressive tokenizer  (Ismailov,  2023 )  is used before learning with transformers. The analytical result is validated experimentally through a feature regression task on synthetic datasets. We show that NoTS significantly outperforms other pre-training methods when approximating features with real-world characteristics, showing its superior expressiveness both theoretically and experimentally.",
            "where  f ( A , M ) subscript f A M f_{(A,M)} italic_f start_POSTSUBSCRIPT ( italic_A , italic_M ) end_POSTSUBSCRIPT  denotes the function formed from  { A  [ g M  ( t i ) ] } i = 1 T superscript subscript A delimited-[] subscript g M subscript t i i 1 T \\{A[g_{M}(t_{i})]\\}_{i=1}^{T} { italic_A [ italic_g start_POSTSUBSCRIPT italic_M end_POSTSUBSCRIPT ( italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ] } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT , which leads to Eq.  4 .",
            "We selected 9 univariate datasets from the UCR archive  (Dau et al.,  2019 ) , filtering out all datasets with less than  140 140 140 140  series length or less than  350 350 350 350  training samples. The dataset selection is performed to ensure each dataset has both sufficient samples and dynamics. The detailed information about the selected datasets is provided in Table  4 .",
            "We present additional data and token space visualizations in Figure  4  and Figure  5 , respectively."
        ]
    },
    "id_table_5": {
        "caption": "Table 6:  ETDataset for imputation tasks.",
        "table": "A1.E6",
        "footnotes": [],
        "references": [
            "We also selected 5 multivariate datasets from the UEA archive  (Bagnall et al.,  2018 ) , excluding those with a series length below  100 100 100 100  and the training sample size below  200 200 200 200 . The detailed information about the selected datasets is provided in Table  5 .",
            "We present additional data and token space visualizations in Figure  4  and Figure  5 , respectively."
        ]
    },
    "id_table_6": {
        "caption": "Table 7:  Detailed information about the selected datasets for the anomaly detection tasks.",
        "table": "A1.E11",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "where  t h , m , r superscript t h m r t^{h,m,r} italic_t start_POSTSUPERSCRIPT italic_h , italic_m , italic_r end_POSTSUPERSCRIPT  consists of one self-attention layer of  h h h italic_h  heads of size  m m m italic_m  and one feed-forward layer with  r r r italic_r  hidden dimensions (see definitions in Appendix Eq.  6 ). To remove the restriction of permutation equivariance, we consider adding absolute positional embedding 2 2 2 Refer to  Luo et al. ( 2022 )  for a case study of relative positional embedding under the UA framework.  to the transformer that creates  T P h , m , r := { f P  ( X ) = f  ( X + E ) } assign superscript subscript T P h m r subscript f P X f X E \\mathcal{T}_{\\mathrm{P}}^{h,m,r}:=\\{f_{\\mathrm{P}}(\\bm{X})=f(\\bm{X}+\\bm{E})\\} caligraphic_T start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_h , italic_m , italic_r end_POSTSUPERSCRIPT := { italic_f start_POSTSUBSCRIPT roman_P end_POSTSUBSCRIPT ( bold_italic_X ) = italic_f ( bold_italic_X + bold_italic_E ) }  where  f  T h , m , r f superscript T h m r f\\in\\mathcal{T}^{h,m,r} italic_f  caligraphic_T start_POSTSUPERSCRIPT italic_h , italic_m , italic_r end_POSTSUPERSCRIPT  and  X , E  R d  n X E superscript R d n \\bm{X},\\bm{E}\\in\\mathbb{R}^{d\\times n} bold_italic_X , bold_italic_E  blackboard_R start_POSTSUPERSCRIPT italic_d  italic_n end_POSTSUPERSCRIPT . Detailed in Appendix  A , our analysis is an extension of previous results in  Yun et al. ( 2019 ); Ismailov ( 2023 ) .",
            "Proof.  First, we re-write the activation component    [ ( W K i  X )   W Q i  X ]  delimited-[] superscript superscript subscript W K i X top superscript subscript W Q i X \\sigma\\left[\\left(\\bm{W}_{K}^{i}\\bm{X}\\right)^{\\top}\\bm{W}_{Q}^{i}\\bm{X}\\right] italic_ [ ( bold_italic_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT bold_italic_X ) start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_italic_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT bold_italic_X ]  in Eq.  6  as column-wise softmax operation  softmax  ( X   Wx j ) softmax superscript X top subscript Wx j \\operatorname{softmax}({\\mathbf{X}}^{\\top}{\\mathbf{W}}{\\mathbf{x}}_{j}) roman_softmax ( bold_X start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_Wx start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) , where  x j subscript x j {\\mathbf{x}}_{j} bold_x start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT  is a random column of  X X {\\mathbf{X}} bold_X . We have:",
            "For the imputation tasks, we use the ETDataset  (Zhou et al.,  2021 ) , where ETTm1 and ETTm2 are sampled at minute intervals, and ETTh1 and ETTh2 are sampled at hourly intervals. The detailed information about the selected datasets is provided in Table  6 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 8:  Complete classification results on the UCR datasets.",
        "table": "A2.T4.1",
        "footnotes": [],
        "references": [
            "The detailed information about the selected datasets is provided in Table  7 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 9:  Complete classification results on the UEA datasets.",
        "table": "A2.T5.1",
        "footnotes": [],
        "references": [
            "We show the complete classification results in Table  8  and Table  9  and the complete imputation results in Table  10 . The averaged results are presented in Table  2 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 10:  Complete imputation results with masking ratio  12.5 % percent 12.5 12.5\\% 12.5 %  and  25 % percent 25 25\\% 25 % .",
        "table": "A2.T6.1",
        "footnotes": [],
        "references": [
            "We show the complete classification results in Table  8  and Table  9  and the complete imputation results in Table  10 . The averaged results are presented in Table  2 ."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A2.T7.1",
        "footnotes": [],
        "references": [
            "which shows that the attention map converges given deterministic  X X {\\mathbf{X}} bold_X  and  W = ( W K i )   W Q i W superscript superscript subscript W K i top superscript subscript W Q i {\\mathbf{W}}=({\\mathbf{W}}_{K}^{i})^{\\top}{\\mathbf{W}}_{Q}^{i} bold_W = ( bold_W start_POSTSUBSCRIPT italic_K end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT ) start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT bold_W start_POSTSUBSCRIPT italic_Q end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT . Thus, Eq.  10  holds by considering the self attention operator  Attn  ( X ) Attn X \\operatorname{Attn}({\\mathbf{X}}) roman_Attn ( bold_X )  as a convex combination of attention heads given deterministic  X X {\\mathbf{X}} bold_X ,  W O i  R d  m superscript subscript W O i superscript R d m \\bm{W}_{O}^{i}\\in\\mathbb{R}^{d\\times m} bold_italic_W start_POSTSUBSCRIPT italic_O end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d  italic_m end_POSTSUPERSCRIPT ,  W V i  R m  d superscript subscript W V i superscript R m d \\bm{W}_{V}^{i}\\in\\mathbb{R}^{m\\times d} bold_italic_W start_POSTSUBSCRIPT italic_V end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_m  italic_d end_POSTSUPERSCRIPT .",
            "We show the complete classification results in Table  8  and Table  9  and the complete imputation results in Table  10 . The averaged results are presented in Table  2 ."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A2.T8.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A2.T8.2",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A2.T9.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A2.T9.2",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A2.T10.5",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A2.T10.6",
        "footnotes": [],
        "references": []
    }
}