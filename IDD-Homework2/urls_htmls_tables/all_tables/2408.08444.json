{
    "id_table_1": {
        "caption": "Table 1 .  OpenQA Performance. Top 1 passage is inserted into the OpenQA prompt, limit number of generated tokens to 20.   : Performance improvements are statistically significant with a ttest of  p < 0.05 p 0.05 p<0.05 italic_p < 0.05 . DPR ReCon s baseline is ReContriever, ColBERT base s baseline is ColBERT init",
        "table": "S4.T1.5",
        "footnotes": [],
        "references": [
            "Concerning the generation of weakly labeled ranking data using LLMs as passage rankers, existing work can be categorized into three categories. The first involves a zero-shot listwise ranking strategy, where a prompt containing the question, instructions, and multiple passages are given to the LLM, and the LLM generates a permutation of ranked indices  (Baldelli et al . ,  2024 ; Ma et al . ,  2023 ; Pradeep et al . ,  2023 ; Shen et al . ,  2023 ) . The second category prompts the LLM with passages and directly instructs it to rate their relevance  (Zhuang et al . ,  2024 ) , or perform pairwise comparisons  (Qin et al . ,  2023 ) . The third approach calculates the question generation likelihood when a passage is included in the prompt  (Cho et al . ,  2023 ; Sachan et al . ,  2022b ; Zhuang et al . ,  2023 ) . Our method for scoring passages aligns with this third approach; however, as mentioned in Section  1 , we rank the passages by the likelihood of the LLM generating the questions ground-truth answer based on the question-passage pair.",
            "As illustrated in Figure  1 , W-RAG introduces a novel approach for training the retrieval component within the RAG pipeline from scratch. Our method leverages a weakly-supervised training paradigm that requires only a set of question-answer pairs and an evidence corpus. The W-RAG process unfolds in three stages: first, it retrieves relevant passages from the evidence corpus; second, it employs a LLM to generate weak labels from them by reranking the retrieved passages by the likelihood of generating the ground-truth answer; finally, the dense retiever is trained using these weak labels. The resulting dense retriever is then capable of retrieving evidence, thereby enhancing the performance of LLMs across a wide array of tasks.",
            "As depicted in Figure  1  Step 2, we construct weak label generation prompts with candidate passage  s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , question  q q q italic_q , some instructions  I I I italic_I , and ground-truth answer  a a a italic_a . The downstream signal is captured through the conditional probability of the ground-truth answer, denoted as  p  ( a | s i , q , I ) p conditional a subscript s i q I p(a|s_{i},q,I) italic_p ( italic_a | italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_q , italic_I ) :",
            "The goal of W-RAG is to enhance LLM generation quality by providing relevant evidence passages. Tabel  1  presents the main OpenQA results using different retrievers in the RAG pipeline, evaluated on 2,000 test questions for each dataset. For these experiments, only the top 1 retrieved passage is added to the prompt, and the LLM is limited to generating 20 tokens. The Naive and Groundtruth baselines represent the lower and upper bounds, respectively. The Naive baseline excludes any supplementary passage, while Groundtruth includes the best passage. Any retriever that retrieves relevant passages should outperform the Naive baseline, while surpassing the Groundtruth baseline would be very unexpected. Results show consistent improvements with W-RAG trained retrievers across all datasets, with most of them being statistically significant. The relatively small gap between W-RAG trained and ground-truth trained retrievers suggests that W-RAG data approaches the quality of human-labeled data."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 .  Retrieval Performance after training with W-RAG or ground-truth data.   : Performance improvements are statistically significant with a ttest of  p < 0.05 p 0.05 p<0.05 italic_p < 0.05 .",
        "table": "S4.T2.7",
        "footnotes": [],
        "references": [
            "Once the dense retriever is trained, we integrate it into the generic RAG pipeline, where it is used to retrieve the top- K K K italic_K  evidence passages for a given user question. The retrieved passages are directly inserted into the prompt, as shown in Figure  2  alongside the question and instructions asking the LLM to generate an answer based on the retrieved passages or, if the passages are not useful, to answer using its internal knowledge. For our main experiments, we restrict Llama3-8B-Instruct to generate a maximum of 20 tokens and only use top 1 retrieved passage to supplement the prompt. We also explored the impact of using different numbers of supplemental evidence passages on both OpenQA performance and latency, which we discuss in the ablation studies ( 5.4 ). We did not experiment with different LLMs for the OpenQA task, as the LLM component of RAG is not the primary focus of our study.",
            "The prompts used to generate weak labels are shown in Figure  2 . We use the term DOCUMENT instead of PASSAGE because our testing experiments showed that DOCUMENT consistently yields better reranking performance. We suspect this is because DOCUMENT is a more common term and likely appears more frequently in Llama3s training data, leading it to a deeper understanding of the word DOCUMENT. Additionally, we tested different orderings of the passage, question, and instructions, finding that the Passage-Question-Instruction format consistently outperforms Passage-Instruction-Question and Instruction-Passage-Question.",
            "The retrieval results of different trained retrievers are presented in Table  2 . These results are based on 2,000 test questions for each trained or baseline retriever. On the MSMARCO QnA dataset, all retrievers trained on W-RAG data outperformed all baselines, while DPR ReCon  consistently outperformed all unsupervised baselines, except for SQuAD where BM25 performed best. As expected, DPR trained on ReContriever showed better retrieval performance than DPR trained from scratch. Similar to the OpenQA task, W-RAG trained retrievers slightly underperform compared to those trained on ground-truth data.",
            "Using Llama3-8B-Instruct, we evaluated reranking and OpenQA performance, as presented in Table  4 . UPR  (Sachan et al . ,  2022a ) , which ranks passages based on question likelihood, serve as an unsupervised approach for generating training labels. With weak supervision of ground-truth answers, W-RAGs zero-shot, one-shot, and two-shot prompts significantly outperforms BM25 and UPR in both reranking and OpenQA metrics. However, the differences between the various shot prompts are not very consistent and are not significant. We hypothesize that this inconsistency may have stemmed from the LLM misinterpreting example passages as the relevant passage, or if the question types are drastically different between the example and the given question, thus introducing noise and reducing result reliability. Due to this, we conducted all subsequent experiments using zero-shot prompts, shown in Figure  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 .  BM25 retrieves the top 100 passages, which are then reranked by Llama3-8B. The reranked lists are used as weak labels for training a dense retriever.",
        "table": "S4.T3.2.1",
        "footnotes": [],
        "references": [
            "After generating all of W-RAG data, we examine its retrieval performance and the quality of the weakly labeled rank lists. In Table  3 , a significant gap between BM25 and Llama3s reranking performance at Recall@1 is evident, which justifies our choise of only using the top 1 passage as the positive when training different retrievers. We also observe that Llama3s performance declines as the number of passages increases. This is likely due to the fact that starting from around the 50th position, the difference in answer likelihood between adjacent passages diminishes to negligible values, such as  exp  (  3.24815 )  exp  (  3.24899 )  3  10  5 3.24815 3.24899 3 superscript 10 5 \\exp(-3.24815)-\\exp(-3.24899)\\approx 3\\times 10^{-5} roman_exp ( - 3.24815 ) - roman_exp ( - 3.24899 )  3  10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT .",
            "We assessed the quality of W-RAG data generated by different LLMs, specifically their ability to rerank passages based on answer likelihood. This is illustrated in Figure  3 , showing the reranking performance of various LLMs on 500 questions from the validation set of MSMARCO QnA. Although the LLMs notably differs at Recall@5, they exhibit consistent trends with each other. We notice Llama3-8B-Instruct is weak when compared to other examined LLMs. However, since their recall distributions are similar, any of these LLMs can serve as the reranker. We chose to use Llama3-8B-Instruct for all our experiments."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 .  Llama3-8B-Instructs reranking performance using different prompts on 500 validation questions from the MSMARCO QnA dataset. Answers are generated using only the top 1 reranked passage.",
        "table": "S5.T4.2",
        "footnotes": [],
        "references": [
            "For our main experiments, we use Llama3-8B-Instruct  (Dubey and .etl,  2024 )  to serve as the reranker. The 2,000 question-answer pairs in the training set are used to generate weak labels. For each question, weak labels are generated by scoring the top 100 passages retrieved by BM25, based on the LLMs likelihood of generating the answer. This process requires the LLM to perform inference 100 times to produce a ranked list. The choice of 100 passages is a trade-off between accuracy and latency. We found that BM25s Recall@100 reaches approximately 80% across all four datasets, and retrieving additional passages yields diminishing returns as the LLMs inference time increases linearly. We evaluate the reranking performance using different prompts and LLMs, which will be elaborated on later in the ablation studies ( 5.4 ).",
            "Once the dense retriever is trained, we integrate it into the generic RAG pipeline, where it is used to retrieve the top- K K K italic_K  evidence passages for a given user question. The retrieved passages are directly inserted into the prompt, as shown in Figure  2  alongside the question and instructions asking the LLM to generate an answer based on the retrieved passages or, if the passages are not useful, to answer using its internal knowledge. For our main experiments, we restrict Llama3-8B-Instruct to generate a maximum of 20 tokens and only use top 1 retrieved passage to supplement the prompt. We also explored the impact of using different numbers of supplemental evidence passages on both OpenQA performance and latency, which we discuss in the ablation studies ( 5.4 ). We did not experiment with different LLMs for the OpenQA task, as the LLM component of RAG is not the primary focus of our study.",
            "Using Llama3-8B-Instruct, we evaluated reranking and OpenQA performance, as presented in Table  4 . UPR  (Sachan et al . ,  2022a ) , which ranks passages based on question likelihood, serve as an unsupervised approach for generating training labels. With weak supervision of ground-truth answers, W-RAGs zero-shot, one-shot, and two-shot prompts significantly outperforms BM25 and UPR in both reranking and OpenQA metrics. However, the differences between the various shot prompts are not very consistent and are not significant. We hypothesize that this inconsistency may have stemmed from the LLM misinterpreting example passages as the relevant passage, or if the question types are drastically different between the example and the given question, thus introducing noise and reducing result reliability. Due to this, we conducted all subsequent experiments using zero-shot prompts, shown in Figure  2 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 .  Adding different # of evidence passages to the prompt when doing OpenQA using DPR ReCon  on 2,000 test questions from MSMARCO QnA.",
        "table": "S5.T5.4",
        "footnotes": [],
        "references": [
            "For our main experiments, we use Llama3-8B-Instruct  (Dubey and .etl,  2024 )  to serve as the reranker. The 2,000 question-answer pairs in the training set are used to generate weak labels. For each question, weak labels are generated by scoring the top 100 passages retrieved by BM25, based on the LLMs likelihood of generating the answer. This process requires the LLM to perform inference 100 times to produce a ranked list. The choice of 100 passages is a trade-off between accuracy and latency. We found that BM25s Recall@100 reaches approximately 80% across all four datasets, and retrieving additional passages yields diminishing returns as the LLMs inference time increases linearly. We evaluate the reranking performance using different prompts and LLMs, which will be elaborated on later in the ablation studies ( 5.4 ).",
            "Once the dense retriever is trained, we integrate it into the generic RAG pipeline, where it is used to retrieve the top- K K K italic_K  evidence passages for a given user question. The retrieved passages are directly inserted into the prompt, as shown in Figure  2  alongside the question and instructions asking the LLM to generate an answer based on the retrieved passages or, if the passages are not useful, to answer using its internal knowledge. For our main experiments, we restrict Llama3-8B-Instruct to generate a maximum of 20 tokens and only use top 1 retrieved passage to supplement the prompt. We also explored the impact of using different numbers of supplemental evidence passages on both OpenQA performance and latency, which we discuss in the ablation studies ( 5.4 ). We did not experiment with different LLMs for the OpenQA task, as the LLM component of RAG is not the primary focus of our study.",
            "During the final OpenQA, we also study the impact of giving more evidence passages, shown in Table  5 . We observe a consistent and steady increase in all OpenQA metrics when more evidence passages are inserted into the prompt. This behavior is expected because the more passages supplied, the better chance a good passage is within them. However, latency also grows as we increase the number of passages."
        ]
    },
    "global_footnotes": []
}