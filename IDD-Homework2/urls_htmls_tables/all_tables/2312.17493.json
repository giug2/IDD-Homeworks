{
    "PAPER'S NUMBER OF TABLES": 24,
    "S5.T1": {
        "caption": "Table 1. Statistics of the training dataset",
        "table": "<table id=\"S5.T1.st1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.st1.4.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Metric</td>\n<td id=\"S5.T1.st1.4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Value</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"># dialogues</td>\n<td id=\"S5.T1.st1.4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">257,332</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\"># utterances</td>\n<td id=\"S5.T1.st1.4.1.3.3.2\" class=\"ltx_td ltx_align_center\">514,664</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\"># tokens</td>\n<td id=\"S5.T1.st1.4.1.4.4.2\" class=\"ltx_td ltx_align_center\">44,527,872</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Avg. # of utterances in a dialogue</td>\n<td id=\"S5.T1.st1.4.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Max # of utterances in a dialogue</td>\n<td id=\"S5.T1.st1.4.1.6.6.2\" class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Min # of utterances in a dialogue</td>\n<td id=\"S5.T1.st1.4.1.7.7.2\" class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Avg. # of tokens in an utterance</td>\n<td id=\"S5.T1.st1.4.1.8.8.2\" class=\"ltx_td ltx_align_center\">86.5</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Max # of tokens in an utterance</td>\n<td id=\"S5.T1.st1.4.1.9.9.2\" class=\"ltx_td ltx_align_center\">3,672</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.10.10\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">Min # tokens in an utterance</td>\n<td id=\"S5.T1.st1.4.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_b\">1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "GPT-2 (Table 19) showed a performance drop in BoolQ from 61.9 to 33.5, PIQA from 56.9 to 46.3, and WinoGrande from 48.9 to 39.4 as the rank reduced from origin to 32, alongside an increase in compression ratio from 45.5 to 79.5. BERT (Table 20) exhibited a similar performance decline, for example, in WinoGrande from 60.2 to 37.8, with a compression improvement from 42.0 to 75.5. Llama-7B (Table 21), a larger model, maintained a relatively stable performance (e.g., BoolQ at 76.3 to 39.1, WinoGrande 70.0 to 67.8 ) despite significant parameter reduction (compression ratio from 7.31 to 36.27). In contrast, ChatGLM-6B (Table 22) showed a more pronounced performance degradation (BoolQ dropping from 69.5 to 50.1, WinoGrande from 71.3 to 46.3) with a notable decrease in parameters (compression ratio moving from 7.6 to 47.4). These results highlight the variability in how different models respond to parameter reduction and compression, underscoring the challenge of balancing communication efficiency with performance in federated learning environments."
        ]
    },
    "S5.T1.st1": {
        "caption": "(a) Statistics of the Medical Dialogue dataset. ",
        "table": "<table id=\"S5.T1.st1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.st1.4.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Metric</td>\n<td id=\"S5.T1.st1.4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Value</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"># dialogues</td>\n<td id=\"S5.T1.st1.4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">257,332</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\"># utterances</td>\n<td id=\"S5.T1.st1.4.1.3.3.2\" class=\"ltx_td ltx_align_center\">514,664</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r\"># tokens</td>\n<td id=\"S5.T1.st1.4.1.4.4.2\" class=\"ltx_td ltx_align_center\">44,527,872</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Avg. # of utterances in a dialogue</td>\n<td id=\"S5.T1.st1.4.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">2</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Max # of utterances in a dialogue</td>\n<td id=\"S5.T1.st1.4.1.6.6.2\" class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Min # of utterances in a dialogue</td>\n<td id=\"S5.T1.st1.4.1.7.7.2\" class=\"ltx_td ltx_align_center\">2</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Avg. # of tokens in an utterance</td>\n<td id=\"S5.T1.st1.4.1.8.8.2\" class=\"ltx_td ltx_align_center\">86.5</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Max # of tokens in an utterance</td>\n<td id=\"S5.T1.st1.4.1.9.9.2\" class=\"ltx_td ltx_align_center\">3,672</td>\n</tr>\n<tr id=\"S5.T1.st1.4.1.10.10\" class=\"ltx_tr\">\n<td id=\"S5.T1.st1.4.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">Min # tokens in an utterance</td>\n<td id=\"S5.T1.st1.4.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_b\">1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The surge in interest and application of large language models (LLMs) has sparked a drive to fine-tune these models to suit specific applications, such as finance and medical science. However, concerns regarding data privacy have emerged, especially when multiple stakeholders aim to collaboratively enhance LLMs using sensitive data. In this scenario, federated learning becomes a natural choice, allowing decentralized fine-tuning without exposing raw data to central servers. Motivated by this, we investigate how data privacy can be ensured in LLM fine-tuning through practical federated learning approaches, enabling secure contributions from multiple parties to enhance LLMs. Yet, challenges arise: 1) despite avoiding raw data exposure, there is a risk of inferring sensitive information from model outputs, and 2) federated learning for LLMs incurs notable communication overhead. To address these challenges, this article introduces DP-LoRA, a novel federated learning algorithm tailored for LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism that adds noise in weight updates, maintaining individual data privacy while facilitating collaborative model training. Moreover, DP-LoRA optimizes communication efficiency via low-rank adaptation, minimizing the transmission of updated weights during distributed training. The experimental results across medical, financial, and general datasets using various LLMs demonstrate that DP-LoRA effectively ensures strict privacy constraints while minimizing communication overhead.",
            "However, accomplishing the aforementioned objective presents non-trivial challenges. Firstly, despite federated learning ensuring no direct exposure to raw data, there remains a risk for attackers to potentially deduce training data from model outputs. This vulnerability arises because LLMs can accidentally disclose sensitive information through their responsesÂ (Carlini etÂ al., 2021; Kim etÂ al., 2023; Li etÂ al., 2023a). For instance, reports indicate that malicious adversaries could exploit the New Bing to link victimsâ€™ personally identifiable information using partial informationÂ (Li etÂ al., 2023b). Secondly, naively applying federated learning to fine-tune LLMs can trigger significant communication overhead. This is because model updates from diverse decentralized sources need iterative aggregation, and frequent transmission of these updates for a complex model like LLM is extremely expensive. Despite strategies to mitigate communication challenges in federated learningÂ (KoneÄná»³ etÂ al., 2016; Hamer etÂ al., 2020; Lan etÂ al., 2023), a notable trade-off persists: while these methods alleviate communication bottlenecks, they cannot ensure concrete guarantees in protecting the privacy of the training data.",
            "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower Ïµitalic-Ïµ\\epsilon and higher Î´ğ›¿\\delta values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when Ïµitalic-Ïµ\\epsilon is reduced to 2 and Î´ğ›¿\\delta to 1â€‹eâˆ’051ğ‘’051e-05, respectively (Tables 3 and 7). This trend indicates a trade-off between privacy and utilities.",
            "In contrast, some models like Llama-7B show a more resilient performance under varying Ïµitalic-Ïµ\\epsilon values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when Ïµitalic-Ïµ\\epsilon is increased from the original to 10 (Table 6). This suggests that certain models might be better suited for privacy-sensitive applications.",
            "Additionally, the impact of changing Î´ğ›¿\\delta values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when Î´ğ›¿\\delta is reduced to 1â€‹eâˆ’061ğ‘’061e-06 (Table 8), highlighting a potentially higher sensitivity to Î´ğ›¿\\delta adjustments.",
            "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower Ïµitalic-Ïµ\\epsilon and Î´ğ›¿\\delta values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when Ïµitalic-Ïµ\\epsilon is reduced to 2 and Î´ğ›¿\\delta to 1â€‹eâˆ’051ğ‘’051e-05, respectively (Tables 3 and 7). This trend indicates a trade-off between privacy and effectiveness.",
            "In contrast, some models like Llama-7B show a more resilient performance under varying Ïµitalic-Ïµ\\epsilon values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when Ïµitalic-Ïµ\\epsilon is increased from the original to 10 (Table 6). This suggests that certain models might be better suited for privacy-sensitive applications.",
            "Additionally, the impact of changing Î´ğ›¿\\delta values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when Î´ğ›¿\\delta is reduced to 1â€‹eâˆ’061ğ‘’061e-06 (Table 8), highlighting a potentially higher sensitivity to Î´ğ›¿\\delta adjustments.",
            "GPT-2 (Table 19) showed a performance drop in BoolQ from 61.9 to 33.5, PIQA from 56.9 to 46.3, and WinoGrande from 48.9 to 39.4 as the rank reduced from origin to 32, alongside an increase in compression ratio from 45.5 to 79.5. BERT (Table 20) exhibited a similar performance decline, for example, in WinoGrande from 60.2 to 37.8, with a compression improvement from 42.0 to 75.5. Llama-7B (Table 21), a larger model, maintained a relatively stable performance (e.g., BoolQ at 76.3 to 39.1, WinoGrande 70.0 to 67.8 ) despite significant parameter reduction (compression ratio from 7.31 to 36.27). In contrast, ChatGLM-6B (Table 22) showed a more pronounced performance degradation (BoolQ dropping from 69.5 to 50.1, WinoGrande from 71.3 to 46.3) with a notable decrease in parameters (compression ratio moving from 7.6 to 47.4). These results highlight the variability in how different models respond to parameter reduction and compression, underscoring the challenge of balancing communication efficiency with performance in federated learning environments."
        ]
    },
    "S5.T1.st2": {
        "caption": "(b) Statistics of the Slimpajama. ",
        "table": "<table id=\"S5.T1.st2.4.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.st2.4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.st2.4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Data source</th>\n<th id=\"S5.T1.st2.4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SlimPajama</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.st2.4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">Commoncrawl</td>\n<td id=\"S5.T1.st2.4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">52.2%</td>\n</tr>\n<tr id=\"S5.T1.st2.4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.1.3.2.1\" class=\"ltx_td ltx_align_center\">C4</td>\n<td id=\"S5.T1.st2.4.1.3.2.2\" class=\"ltx_td ltx_align_center\">26.7%</td>\n</tr>\n<tr id=\"S5.T1.st2.4.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.1.4.3.1\" class=\"ltx_td ltx_align_center\">GitHub</td>\n<td id=\"S5.T1.st2.4.1.4.3.2\" class=\"ltx_td ltx_align_center\">5.2%</td>\n</tr>\n<tr id=\"S5.T1.st2.4.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.1.5.4.1\" class=\"ltx_td ltx_align_center\">Books</td>\n<td id=\"S5.T1.st2.4.1.5.4.2\" class=\"ltx_td ltx_align_center\">4.2%</td>\n</tr>\n<tr id=\"S5.T1.st2.4.1.6.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.1.6.5.1\" class=\"ltx_td ltx_align_center\">ArXiv</td>\n<td id=\"S5.T1.st2.4.1.6.5.2\" class=\"ltx_td ltx_align_center\">4.6%</td>\n</tr>\n<tr id=\"S5.T1.st2.4.1.7.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.1.7.6.1\" class=\"ltx_td ltx_align_center\">Wikipedia</td>\n<td id=\"S5.T1.st2.4.1.7.6.2\" class=\"ltx_td ltx_align_center\">3.8%</td>\n</tr>\n<tr id=\"S5.T1.st2.4.1.8.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.st2.4.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">StackExchange</td>\n<td id=\"S5.T1.st2.4.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">3.3%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The surge in interest and application of large language models (LLMs) has sparked a drive to fine-tune these models to suit specific applications, such as finance and medical science. However, concerns regarding data privacy have emerged, especially when multiple stakeholders aim to collaboratively enhance LLMs using sensitive data. In this scenario, federated learning becomes a natural choice, allowing decentralized fine-tuning without exposing raw data to central servers. Motivated by this, we investigate how data privacy can be ensured in LLM fine-tuning through practical federated learning approaches, enabling secure contributions from multiple parties to enhance LLMs. Yet, challenges arise: 1) despite avoiding raw data exposure, there is a risk of inferring sensitive information from model outputs, and 2) federated learning for LLMs incurs notable communication overhead. To address these challenges, this article introduces DP-LoRA, a novel federated learning algorithm tailored for LLMs. DP-LoRA preserves data privacy by employing a Gaussian mechanism that adds noise in weight updates, maintaining individual data privacy while facilitating collaborative model training. Moreover, DP-LoRA optimizes communication efficiency via low-rank adaptation, minimizing the transmission of updated weights during distributed training. The experimental results across medical, financial, and general datasets using various LLMs demonstrate that DP-LoRA effectively ensures strict privacy constraints while minimizing communication overhead.",
            "However, accomplishing the aforementioned objective presents non-trivial challenges. Firstly, despite federated learning ensuring no direct exposure to raw data, there remains a risk for attackers to potentially deduce training data from model outputs. This vulnerability arises because LLMs can accidentally disclose sensitive information through their responsesÂ (Carlini etÂ al., 2021; Kim etÂ al., 2023; Li etÂ al., 2023a). For instance, reports indicate that malicious adversaries could exploit the New Bing to link victimsâ€™ personally identifiable information using partial informationÂ (Li etÂ al., 2023b). Secondly, naively applying federated learning to fine-tune LLMs can trigger significant communication overhead. This is because model updates from diverse decentralized sources need iterative aggregation, and frequent transmission of these updates for a complex model like LLM is extremely expensive. Despite strategies to mitigate communication challenges in federated learningÂ (KoneÄná»³ etÂ al., 2016; Hamer etÂ al., 2020; Lan etÂ al., 2023), a notable trade-off persists: while these methods alleviate communication bottlenecks, they cannot ensure concrete guarantees in protecting the privacy of the training data.",
            "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower Ïµitalic-Ïµ\\epsilon and higher Î´ğ›¿\\delta values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when Ïµitalic-Ïµ\\epsilon is reduced to 2 and Î´ğ›¿\\delta to 1â€‹eâˆ’051ğ‘’051e-05, respectively (Tables 3 and 7). This trend indicates a trade-off between privacy and utilities.",
            "In contrast, some models like Llama-7B show a more resilient performance under varying Ïµitalic-Ïµ\\epsilon values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when Ïµitalic-Ïµ\\epsilon is increased from the original to 10 (Table 6). This suggests that certain models might be better suited for privacy-sensitive applications.",
            "Additionally, the impact of changing Î´ğ›¿\\delta values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when Î´ğ›¿\\delta is reduced to 1â€‹eâˆ’061ğ‘’061e-06 (Table 8), highlighting a potentially higher sensitivity to Î´ğ›¿\\delta adjustments.",
            "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower Ïµitalic-Ïµ\\epsilon and Î´ğ›¿\\delta values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when Ïµitalic-Ïµ\\epsilon is reduced to 2 and Î´ğ›¿\\delta to 1â€‹eâˆ’051ğ‘’051e-05, respectively (Tables 3 and 7). This trend indicates a trade-off between privacy and effectiveness.",
            "In contrast, some models like Llama-7B show a more resilient performance under varying Ïµitalic-Ïµ\\epsilon values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when Ïµitalic-Ïµ\\epsilon is increased from the original to 10 (Table 6). This suggests that certain models might be better suited for privacy-sensitive applications.",
            "Additionally, the impact of changing Î´ğ›¿\\delta values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when Î´ğ›¿\\delta is reduced to 1â€‹eâˆ’061ğ‘’061e-06 (Table 8), highlighting a potentially higher sensitivity to Î´ğ›¿\\delta adjustments.",
            "GPT-2 (Table 19) showed a performance drop in BoolQ from 61.9 to 33.5, PIQA from 56.9 to 46.3, and WinoGrande from 48.9 to 39.4 as the rank reduced from origin to 32, alongside an increase in compression ratio from 45.5 to 79.5. BERT (Table 20) exhibited a similar performance decline, for example, in WinoGrande from 60.2 to 37.8, with a compression improvement from 42.0 to 75.5. Llama-7B (Table 21), a larger model, maintained a relatively stable performance (e.g., BoolQ at 76.3 to 39.1, WinoGrande 70.0 to 67.8 ) despite significant parameter reduction (compression ratio from 7.31 to 36.27). In contrast, ChatGLM-6B (Table 22) showed a more pronounced performance degradation (BoolQ dropping from 69.5 to 50.1, WinoGrande from 71.3 to 46.3) with a notable decrease in parameters (compression ratio moving from 7.6 to 47.4). These results highlight the variability in how different models respond to parameter reduction and compression, underscoring the challenge of balancing communication efficiency with performance in federated learning environments."
        ]
    },
    "S5.T2": {
        "caption": "Table 2. hyper-parameter settings for training",
        "table": "<table id=\"S5.T2.8\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.8.9.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.8.9.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">hyper-parameter</th>\n<th id=\"S5.T2.8.9.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Value</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">batch size <math id=\"S5.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"B\" display=\"inline\"><semantics id=\"S5.T2.1.1.1.m1.1a\"><mi id=\"S5.T2.1.1.1.m1.1.1\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\">B</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.1b\"><ci id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\">ğµ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.1c\">B</annotation></semantics></math>\n</td>\n<td id=\"S5.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S5.T2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"8\" display=\"inline\"><semantics id=\"S5.T2.2.2.2.m1.1a\"><mn id=\"S5.T2.2.2.2.m1.1.1\" xref=\"S5.T2.2.2.2.m1.1.1.cmml\">8</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.2.2.2.m1.1b\"><cn type=\"integer\" id=\"S5.T2.2.2.2.m1.1.1.cmml\" xref=\"S5.T2.2.2.2.m1.1.1\">8</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.2.2.2.m1.1c\">8</annotation></semantics></math></td>\n</tr>\n<tr id=\"S5.T2.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.3.3.1\" class=\"ltx_td ltx_align_center\">noise scale <math id=\"S5.T2.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sigma\" display=\"inline\"><semantics id=\"S5.T2.3.3.1.m1.1a\"><mi id=\"S5.T2.3.3.1.m1.1.1\" xref=\"S5.T2.3.3.1.m1.1.1.cmml\">Ïƒ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.3.3.1.m1.1b\"><ci id=\"S5.T2.3.3.1.m1.1.1.cmml\" xref=\"S5.T2.3.3.1.m1.1.1\">ğœ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.3.3.1.m1.1c\">\\sigma</annotation></semantics></math>\n</td>\n<td id=\"S5.T2.4.4.2\" class=\"ltx_td ltx_align_center\"><math id=\"S5.T2.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"2\" display=\"inline\"><semantics id=\"S5.T2.4.4.2.m1.1a\"><mn id=\"S5.T2.4.4.2.m1.1.1\" xref=\"S5.T2.4.4.2.m1.1.1.cmml\">2</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.4.4.2.m1.1b\"><cn type=\"integer\" id=\"S5.T2.4.4.2.m1.1.1.cmml\" xref=\"S5.T2.4.4.2.m1.1.1\">2</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.4.4.2.m1.1c\">2</annotation></semantics></math></td>\n</tr>\n<tr id=\"S5.T2.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T2.5.5.1\" class=\"ltx_td ltx_align_center\">learning rate <math id=\"S5.T2.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\gamma\" display=\"inline\"><semantics id=\"S5.T2.5.5.1.m1.1a\"><mi id=\"S5.T2.5.5.1.m1.1.1\" xref=\"S5.T2.5.5.1.m1.1.1.cmml\">Î³</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.5.5.1.m1.1b\"><ci id=\"S5.T2.5.5.1.m1.1.1.cmml\" xref=\"S5.T2.5.5.1.m1.1.1\">ğ›¾</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.5.5.1.m1.1c\">\\gamma</annotation></semantics></math>\n</td>\n<td id=\"S5.T2.6.6.2\" class=\"ltx_td ltx_align_center\"><math id=\"S5.T2.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"5e^{-4}\" display=\"inline\"><semantics id=\"S5.T2.6.6.2.m1.1a\"><mrow id=\"S5.T2.6.6.2.m1.1.1\" xref=\"S5.T2.6.6.2.m1.1.1.cmml\"><mn id=\"S5.T2.6.6.2.m1.1.1.2\" xref=\"S5.T2.6.6.2.m1.1.1.2.cmml\">5</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T2.6.6.2.m1.1.1.1\" xref=\"S5.T2.6.6.2.m1.1.1.1.cmml\">â€‹</mo><msup id=\"S5.T2.6.6.2.m1.1.1.3\" xref=\"S5.T2.6.6.2.m1.1.1.3.cmml\"><mi id=\"S5.T2.6.6.2.m1.1.1.3.2\" xref=\"S5.T2.6.6.2.m1.1.1.3.2.cmml\">e</mi><mrow id=\"S5.T2.6.6.2.m1.1.1.3.3\" xref=\"S5.T2.6.6.2.m1.1.1.3.3.cmml\"><mo id=\"S5.T2.6.6.2.m1.1.1.3.3a\" xref=\"S5.T2.6.6.2.m1.1.1.3.3.cmml\">âˆ’</mo><mn id=\"S5.T2.6.6.2.m1.1.1.3.3.2\" xref=\"S5.T2.6.6.2.m1.1.1.3.3.2.cmml\">4</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.6.6.2.m1.1b\"><apply id=\"S5.T2.6.6.2.m1.1.1.cmml\" xref=\"S5.T2.6.6.2.m1.1.1\"><times id=\"S5.T2.6.6.2.m1.1.1.1.cmml\" xref=\"S5.T2.6.6.2.m1.1.1.1\"></times><cn type=\"integer\" id=\"S5.T2.6.6.2.m1.1.1.2.cmml\" xref=\"S5.T2.6.6.2.m1.1.1.2\">5</cn><apply id=\"S5.T2.6.6.2.m1.1.1.3.cmml\" xref=\"S5.T2.6.6.2.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S5.T2.6.6.2.m1.1.1.3.1.cmml\" xref=\"S5.T2.6.6.2.m1.1.1.3\">superscript</csymbol><ci id=\"S5.T2.6.6.2.m1.1.1.3.2.cmml\" xref=\"S5.T2.6.6.2.m1.1.1.3.2\">ğ‘’</ci><apply id=\"S5.T2.6.6.2.m1.1.1.3.3.cmml\" xref=\"S5.T2.6.6.2.m1.1.1.3.3\"><minus id=\"S5.T2.6.6.2.m1.1.1.3.3.1.cmml\" xref=\"S5.T2.6.6.2.m1.1.1.3.3\"></minus><cn type=\"integer\" id=\"S5.T2.6.6.2.m1.1.1.3.3.2.cmml\" xref=\"S5.T2.6.6.2.m1.1.1.3.3.2\">4</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.6.6.2.m1.1c\">5e^{-4}</annotation></semantics></math></td>\n</tr>\n<tr id=\"S5.T2.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T2.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">clipping bound <math id=\"S5.T2.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"C\" display=\"inline\"><semantics id=\"S5.T2.7.7.1.m1.1a\"><mi id=\"S5.T2.7.7.1.m1.1.1\" xref=\"S5.T2.7.7.1.m1.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.7.7.1.m1.1b\"><ci id=\"S5.T2.7.7.1.m1.1.1.cmml\" xref=\"S5.T2.7.7.1.m1.1.1\">ğ¶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.7.7.1.m1.1c\">C</annotation></semantics></math>\n</td>\n<td id=\"S5.T2.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><math id=\"S5.T2.8.8.2.m1.1\" class=\"ltx_Math\" alttext=\"10\" display=\"inline\"><semantics id=\"S5.T2.8.8.2.m1.1a\"><mn id=\"S5.T2.8.8.2.m1.1.1\" xref=\"S5.T2.8.8.2.m1.1.1.cmml\">10</mn><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.8.8.2.m1.1b\"><cn type=\"integer\" id=\"S5.T2.8.8.2.m1.1.1.cmml\" xref=\"S5.T2.8.8.2.m1.1.1\">10</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.8.8.2.m1.1c\">10</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "GPT-2 (Table 19) showed a performance drop in BoolQ from 61.9 to 33.5, PIQA from 56.9 to 46.3, and WinoGrande from 48.9 to 39.4 as the rank reduced from origin to 32, alongside an increase in compression ratio from 45.5 to 79.5. BERT (Table 20) exhibited a similar performance decline, for example, in WinoGrande from 60.2 to 37.8, with a compression improvement from 42.0 to 75.5. Llama-7B (Table 21), a larger model, maintained a relatively stable performance (e.g., BoolQ at 76.3 to 39.1, WinoGrande 70.0 to 67.8 ) despite significant parameter reduction (compression ratio from 7.31 to 36.27). In contrast, ChatGLM-6B (Table 22) showed a more pronounced performance degradation (BoolQ dropping from 69.5 to 50.1, WinoGrande from 71.3 to 46.3) with a notable decrease in parameters (compression ratio moving from 7.6 to 47.4). These results highlight the variability in how different models respond to parameter reduction and compression, underscoring the challenge of balancing communication efficiency with performance in federated learning environments."
        ]
    },
    "S5.T3": {
        "caption": "Table 3. The performance of the global model under different Ïµitalic-Ïµ\\epsilon for GPT-2 in general and Medical tasks. ",
        "table": "<table id=\"S5.T3.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T3.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T3.3.1.1.m1.1a\"><mi id=\"S5.T3.3.1.1.m1.1.1\" xref=\"S5.T3.3.1.1.m1.1.1.cmml\">Ïµ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.3.1.1.m1.1b\"><ci id=\"S5.T3.3.1.1.m1.1.1.cmml\" xref=\"S5.T3.3.1.1.m1.1.1\">italic-Ïµ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.3.1.1.m1.1c\">\\epsilon</annotation></semantics></math></th>\n<th id=\"S5.T3.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T3.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T3.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T3.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MedQuAD</th>\n<th id=\"S5.T3.3.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LiveQA</th>\n<th id=\"S5.T3.3.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MEDIQA-Ans</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T3.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.9</td>\n<td id=\"S5.T3.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">53.8</td>\n<td id=\"S5.T3.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">48.3</td>\n<td id=\"S5.T3.3.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">69.2</td>\n<td id=\"S5.T3.3.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">39.5</td>\n<td id=\"S5.T3.3.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">55.1</td>\n</tr>\n<tr id=\"S5.T3.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td id=\"S5.T3.3.3.2.2\" class=\"ltx_td ltx_align_center\">51.5</td>\n<td id=\"S5.T3.3.3.2.3\" class=\"ltx_td ltx_align_center\">46.3</td>\n<td id=\"S5.T3.3.3.2.4\" class=\"ltx_td ltx_align_center\">46.1</td>\n<td id=\"S5.T3.3.3.2.5\" class=\"ltx_td ltx_align_center\">55.3</td>\n<td id=\"S5.T3.3.3.2.6\" class=\"ltx_td ltx_align_center\">27.0</td>\n<td id=\"S5.T3.3.3.2.7\" class=\"ltx_td ltx_align_center\">49.1</td>\n</tr>\n<tr id=\"S5.T3.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td id=\"S5.T3.3.4.3.2\" class=\"ltx_td ltx_align_center\">49.0</td>\n<td id=\"S5.T3.3.4.3.3\" class=\"ltx_td ltx_align_center\">45.4</td>\n<td id=\"S5.T3.3.4.3.4\" class=\"ltx_td ltx_align_center\">46.7</td>\n<td id=\"S5.T3.3.4.3.5\" class=\"ltx_td ltx_align_center\">57.3</td>\n<td id=\"S5.T3.3.4.3.6\" class=\"ltx_td ltx_align_center\">37.1</td>\n<td id=\"S5.T3.3.4.3.7\" class=\"ltx_td ltx_align_center\">47.4</td>\n</tr>\n<tr id=\"S5.T3.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">6</th>\n<td id=\"S5.T3.3.5.4.2\" class=\"ltx_td ltx_align_center\">58.4</td>\n<td id=\"S5.T3.3.5.4.3\" class=\"ltx_td ltx_align_center\">41.5</td>\n<td id=\"S5.T3.3.5.4.4\" class=\"ltx_td ltx_align_center\">45.6</td>\n<td id=\"S5.T3.3.5.4.5\" class=\"ltx_td ltx_align_center\">63.5</td>\n<td id=\"S5.T3.3.5.4.6\" class=\"ltx_td ltx_align_center\">38.9</td>\n<td id=\"S5.T3.3.5.4.7\" class=\"ltx_td ltx_align_center\">52.9</td>\n</tr>\n<tr id=\"S5.T3.3.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8</th>\n<td id=\"S5.T3.3.6.5.2\" class=\"ltx_td ltx_align_center\">52.9</td>\n<td id=\"S5.T3.3.6.5.3\" class=\"ltx_td ltx_align_center\">45.1</td>\n<td id=\"S5.T3.3.6.5.4\" class=\"ltx_td ltx_align_center\">42.7</td>\n<td id=\"S5.T3.3.6.5.5\" class=\"ltx_td ltx_align_center\">66.6</td>\n<td id=\"S5.T3.3.6.5.6\" class=\"ltx_td ltx_align_center\">39.1</td>\n<td id=\"S5.T3.3.6.5.7\" class=\"ltx_td ltx_align_center\">47.9</td>\n</tr>\n<tr id=\"S5.T3.3.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T3.3.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T3.3.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">58.9</td>\n<td id=\"S5.T3.3.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">53.1</td>\n<td id=\"S5.T3.3.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">44.7</td>\n<td id=\"S5.T3.3.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">64.1</td>\n<td id=\"S5.T3.3.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">35.9</td>\n<td id=\"S5.T3.3.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">51.8</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our experiments, we evaluated the performance of several models in the context of differential privacy for medical tasks. We investigated the impact of varying the parameters ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " for the language model fine-tuning on the medical datasets. In all experiments, the compression rank was fixed at ",
                "512",
                "512",
                "512",
                " and the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is set to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we varying the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is settd to ",
                "2",
                "2",
                "2",
                ".",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and higher ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and utilities.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments.",
                "We can observe a significant trade-off between privacy and utility in medical tasks using models like GPT-2, Bert, ChatGLM-6B, and Llama-7B. Our data clearly shows that stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) generally correlate with reduced performance across tasks."
            ]
        ]
    },
    "S5.T4": {
        "caption": "Table 4. The performance of the global model under different Ïµitalic-Ïµ\\epsilon for Bert in general and Medical tasks.",
        "table": "<table id=\"S5.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T4.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T4.3.1.1.m1.1a\"><mi id=\"S5.T4.3.1.1.m1.1.1\" xref=\"S5.T4.3.1.1.m1.1.1.cmml\">Ïµ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.3.1.1.m1.1b\"><ci id=\"S5.T4.3.1.1.m1.1.1.cmml\" xref=\"S5.T4.3.1.1.m1.1.1\">italic-Ïµ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.3.1.1.m1.1c\">\\epsilon</annotation></semantics></math></th>\n<th id=\"S5.T4.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T4.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T4.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T4.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MedQuAD</th>\n<th id=\"S5.T4.3.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LiveQA</th>\n<th id=\"S5.T4.3.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MEDIQA-Ans</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T4.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">45.8</td>\n<td id=\"S5.T4.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">51.3</td>\n<td id=\"S5.T4.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">41.2</td>\n<td id=\"S5.T4.3.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">69.1</td>\n<td id=\"S5.T4.3.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">56.4</td>\n<td id=\"S5.T4.3.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">73.3</td>\n</tr>\n<tr id=\"S5.T4.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td id=\"S5.T4.3.3.2.2\" class=\"ltx_td ltx_align_center\">36.2</td>\n<td id=\"S5.T4.3.3.2.3\" class=\"ltx_td ltx_align_center\">41.0</td>\n<td id=\"S5.T4.3.3.2.4\" class=\"ltx_td ltx_align_center\">29.8</td>\n<td id=\"S5.T4.3.3.2.5\" class=\"ltx_td ltx_align_center\">53.2</td>\n<td id=\"S5.T4.3.3.2.6\" class=\"ltx_td ltx_align_center\">46.8</td>\n<td id=\"S5.T4.3.3.2.7\" class=\"ltx_td ltx_align_center\">66.5</td>\n</tr>\n<tr id=\"S5.T4.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td id=\"S5.T4.3.4.3.2\" class=\"ltx_td ltx_align_center\">33.1</td>\n<td id=\"S5.T4.3.4.3.3\" class=\"ltx_td ltx_align_center\">37.2</td>\n<td id=\"S5.T4.3.4.3.4\" class=\"ltx_td ltx_align_center\">38.0</td>\n<td id=\"S5.T4.3.4.3.5\" class=\"ltx_td ltx_align_center\">63.6</td>\n<td id=\"S5.T4.3.4.3.6\" class=\"ltx_td ltx_align_center\">46.3</td>\n<td id=\"S5.T4.3.4.3.7\" class=\"ltx_td ltx_align_center\">61.0</td>\n</tr>\n<tr id=\"S5.T4.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">6</th>\n<td id=\"S5.T4.3.5.4.2\" class=\"ltx_td ltx_align_center\">40.1</td>\n<td id=\"S5.T4.3.5.4.3\" class=\"ltx_td ltx_align_center\">40.2</td>\n<td id=\"S5.T4.3.5.4.4\" class=\"ltx_td ltx_align_center\">33.1</td>\n<td id=\"S5.T4.3.5.4.5\" class=\"ltx_td ltx_align_center\">68.8</td>\n<td id=\"S5.T4.3.5.4.6\" class=\"ltx_td ltx_align_center\">47.6</td>\n<td id=\"S5.T4.3.5.4.7\" class=\"ltx_td ltx_align_center\">60.9</td>\n</tr>\n<tr id=\"S5.T4.3.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8</th>\n<td id=\"S5.T4.3.6.5.2\" class=\"ltx_td ltx_align_center\">40.1</td>\n<td id=\"S5.T4.3.6.5.3\" class=\"ltx_td ltx_align_center\">42.2</td>\n<td id=\"S5.T4.3.6.5.4\" class=\"ltx_td ltx_align_center\">38.9</td>\n<td id=\"S5.T4.3.6.5.5\" class=\"ltx_td ltx_align_center\">60.3</td>\n<td id=\"S5.T4.3.6.5.6\" class=\"ltx_td ltx_align_center\">51.5</td>\n<td id=\"S5.T4.3.6.5.7\" class=\"ltx_td ltx_align_center\">70.2</td>\n</tr>\n<tr id=\"S5.T4.3.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T4.3.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">42.2</td>\n<td id=\"S5.T4.3.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">45.9</td>\n<td id=\"S5.T4.3.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">37.9</td>\n<td id=\"S5.T4.3.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">68.9</td>\n<td id=\"S5.T4.3.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">51.6</td>\n<td id=\"S5.T4.3.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">69.2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our experiments, we evaluated the performance of several models in the context of differential privacy for medical tasks. We investigated the impact of varying the parameters ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " for the language model fine-tuning on the medical datasets. In all experiments, the compression rank was fixed at ",
                "512",
                "512",
                "512",
                " and the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is set to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we varying the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is settd to ",
                "2",
                "2",
                "2",
                ".",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and higher ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and utilities.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments.",
                "We can observe a significant trade-off between privacy and utility in medical tasks using models like GPT-2, Bert, ChatGLM-6B, and Llama-7B. Our data clearly shows that stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) generally correlate with reduced performance across tasks."
            ]
        ]
    },
    "S5.T5": {
        "caption": "Table 5. The performance of the global model under different Ïµitalic-Ïµ\\epsilon for ChatGLM-6B in general and Medical tasks.",
        "table": "<table id=\"S5.T5.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T5.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T5.3.1.1.m1.1a\"><mi id=\"S5.T5.3.1.1.m1.1.1\" xref=\"S5.T5.3.1.1.m1.1.1.cmml\">Ïµ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T5.3.1.1.m1.1b\"><ci id=\"S5.T5.3.1.1.m1.1.1.cmml\" xref=\"S5.T5.3.1.1.m1.1.1\">italic-Ïµ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T5.3.1.1.m1.1c\">\\epsilon</annotation></semantics></math></th>\n<th id=\"S5.T5.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T5.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T5.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T5.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MedQuAD</th>\n<th id=\"S5.T5.3.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LiveQA</th>\n<th id=\"S5.T5.3.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MEDIQA-Ans</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T5.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">44.2</td>\n<td id=\"S5.T5.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">51.2</td>\n<td id=\"S5.T5.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">50.1</td>\n<td id=\"S5.T5.3.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">66.6</td>\n<td id=\"S5.T5.3.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">71.9</td>\n<td id=\"S5.T5.3.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">69.3</td>\n</tr>\n<tr id=\"S5.T5.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td id=\"S5.T5.3.3.2.2\" class=\"ltx_td ltx_align_center\">34.0</td>\n<td id=\"S5.T5.3.3.2.3\" class=\"ltx_td ltx_align_center\">47.9</td>\n<td id=\"S5.T5.3.3.2.4\" class=\"ltx_td ltx_align_center\">34.9</td>\n<td id=\"S5.T5.3.3.2.5\" class=\"ltx_td ltx_align_center\">66.4</td>\n<td id=\"S5.T5.3.3.2.6\" class=\"ltx_td ltx_align_center\">67.3</td>\n<td id=\"S5.T5.3.3.2.7\" class=\"ltx_td ltx_align_center\">57.8</td>\n</tr>\n<tr id=\"S5.T5.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td id=\"S5.T5.3.4.3.2\" class=\"ltx_td ltx_align_center\">32.9</td>\n<td id=\"S5.T5.3.4.3.3\" class=\"ltx_td ltx_align_center\">45.4</td>\n<td id=\"S5.T5.3.4.3.4\" class=\"ltx_td ltx_align_center\">46.0</td>\n<td id=\"S5.T5.3.4.3.5\" class=\"ltx_td ltx_align_center\">57.1</td>\n<td id=\"S5.T5.3.4.3.6\" class=\"ltx_td ltx_align_center\">57.5</td>\n<td id=\"S5.T5.3.4.3.7\" class=\"ltx_td ltx_align_center\">62.2</td>\n</tr>\n<tr id=\"S5.T5.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">6</th>\n<td id=\"S5.T5.3.5.4.2\" class=\"ltx_td ltx_align_center\">37.8</td>\n<td id=\"S5.T5.3.5.4.3\" class=\"ltx_td ltx_align_center\">42.2</td>\n<td id=\"S5.T5.3.5.4.4\" class=\"ltx_td ltx_align_center\">47.5</td>\n<td id=\"S5.T5.3.5.4.5\" class=\"ltx_td ltx_align_center\">60.6</td>\n<td id=\"S5.T5.3.5.4.6\" class=\"ltx_td ltx_align_center\">58.9</td>\n<td id=\"S5.T5.3.5.4.7\" class=\"ltx_td ltx_align_center\">65.8</td>\n</tr>\n<tr id=\"S5.T5.3.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8</th>\n<td id=\"S5.T5.3.6.5.2\" class=\"ltx_td ltx_align_center\">37.4</td>\n<td id=\"S5.T5.3.6.5.3\" class=\"ltx_td ltx_align_center\">47.2</td>\n<td id=\"S5.T5.3.6.5.4\" class=\"ltx_td ltx_align_center\">48.7</td>\n<td id=\"S5.T5.3.6.5.5\" class=\"ltx_td ltx_align_center\">59.8</td>\n<td id=\"S5.T5.3.6.5.6\" class=\"ltx_td ltx_align_center\">69.5</td>\n<td id=\"S5.T5.3.6.5.7\" class=\"ltx_td ltx_align_center\">66.9</td>\n</tr>\n<tr id=\"S5.T5.3.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T5.3.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T5.3.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">38.8</td>\n<td id=\"S5.T5.3.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">47.9</td>\n<td id=\"S5.T5.3.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">43.8</td>\n<td id=\"S5.T5.3.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">65.9</td>\n<td id=\"S5.T5.3.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">69.8</td>\n<td id=\"S5.T5.3.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">67.7</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our experiments, we evaluated the performance of several models in the context of differential privacy for medical tasks. We investigated the impact of varying the parameters ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " for the language model fine-tuning on the medical datasets. In all experiments, the compression rank was fixed at ",
                "512",
                "512",
                "512",
                " and the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is set to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we varying the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is settd to ",
                "2",
                "2",
                "2",
                ".",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and higher ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and utilities.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments.",
                "We can observe a significant trade-off between privacy and utility in medical tasks using models like GPT-2, Bert, ChatGLM-6B, and Llama-7B. Our data clearly shows that stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) generally correlate with reduced performance across tasks."
            ]
        ]
    },
    "S5.T6": {
        "caption": "Table 6. The performance of the global model under different Ïµitalic-Ïµ\\epsilon for Llama-7B in general and Medical tasks.",
        "table": "<table id=\"S5.T6.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T6.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T6.3.1.1.m1.1a\"><mi id=\"S5.T6.3.1.1.m1.1.1\" xref=\"S5.T6.3.1.1.m1.1.1.cmml\">Ïµ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.3.1.1.m1.1b\"><ci id=\"S5.T6.3.1.1.m1.1.1.cmml\" xref=\"S5.T6.3.1.1.m1.1.1\">italic-Ïµ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.3.1.1.m1.1c\">\\epsilon</annotation></semantics></math></th>\n<th id=\"S5.T6.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T6.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T6.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T6.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MedQuAD</th>\n<th id=\"S5.T6.3.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LiveQA</th>\n<th id=\"S5.T6.3.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MEDIQA-Ans</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T6.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">71.3</td>\n<td id=\"S5.T6.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">66.2</td>\n<td id=\"S5.T6.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">65.5</td>\n<td id=\"S5.T6.3.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">73.1</td>\n<td id=\"S5.T6.3.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">69.4</td>\n<td id=\"S5.T6.3.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">65.5</td>\n</tr>\n<tr id=\"S5.T6.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T6.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td id=\"S5.T6.3.3.2.2\" class=\"ltx_td ltx_align_center\">54.9</td>\n<td id=\"S5.T6.3.3.2.3\" class=\"ltx_td ltx_align_center\">57.1</td>\n<td id=\"S5.T6.3.3.2.4\" class=\"ltx_td ltx_align_center\">53.4</td>\n<td id=\"S5.T6.3.3.2.5\" class=\"ltx_td ltx_align_center\">66.5</td>\n<td id=\"S5.T6.3.3.2.6\" class=\"ltx_td ltx_align_center\">55.9</td>\n<td id=\"S5.T6.3.3.2.7\" class=\"ltx_td ltx_align_center\">52.8</td>\n</tr>\n<tr id=\"S5.T6.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T6.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td id=\"S5.T6.3.4.3.2\" class=\"ltx_td ltx_align_center\">60.6</td>\n<td id=\"S5.T6.3.4.3.3\" class=\"ltx_td ltx_align_center\">59.5</td>\n<td id=\"S5.T6.3.4.3.4\" class=\"ltx_td ltx_align_center\">55.5</td>\n<td id=\"S5.T6.3.4.3.5\" class=\"ltx_td ltx_align_center\">66.6</td>\n<td id=\"S5.T6.3.4.3.6\" class=\"ltx_td ltx_align_center\">67.3</td>\n<td id=\"S5.T6.3.4.3.7\" class=\"ltx_td ltx_align_center\">53.2</td>\n</tr>\n<tr id=\"S5.T6.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T6.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">6</th>\n<td id=\"S5.T6.3.5.4.2\" class=\"ltx_td ltx_align_center\">60.8</td>\n<td id=\"S5.T6.3.5.4.3\" class=\"ltx_td ltx_align_center\">64.9</td>\n<td id=\"S5.T6.3.5.4.4\" class=\"ltx_td ltx_align_center\">51.7</td>\n<td id=\"S5.T6.3.5.4.5\" class=\"ltx_td ltx_align_center\">67.3</td>\n<td id=\"S5.T6.3.5.4.6\" class=\"ltx_td ltx_align_center\">68.1</td>\n<td id=\"S5.T6.3.5.4.7\" class=\"ltx_td ltx_align_center\">65.3</td>\n</tr>\n<tr id=\"S5.T6.3.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T6.3.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8</th>\n<td id=\"S5.T6.3.6.5.2\" class=\"ltx_td ltx_align_center\">66.3</td>\n<td id=\"S5.T6.3.6.5.3\" class=\"ltx_td ltx_align_center\">64.1</td>\n<td id=\"S5.T6.3.6.5.4\" class=\"ltx_td ltx_align_center\">60.4</td>\n<td id=\"S5.T6.3.6.5.5\" class=\"ltx_td ltx_align_center\">66.2</td>\n<td id=\"S5.T6.3.6.5.6\" class=\"ltx_td ltx_align_center\">60.5</td>\n<td id=\"S5.T6.3.6.5.7\" class=\"ltx_td ltx_align_center\">62.1</td>\n</tr>\n<tr id=\"S5.T6.3.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T6.3.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T6.3.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">66.6</td>\n<td id=\"S5.T6.3.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">62.2</td>\n<td id=\"S5.T6.3.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">65.2</td>\n<td id=\"S5.T6.3.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">69.7</td>\n<td id=\"S5.T6.3.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">66.1</td>\n<td id=\"S5.T6.3.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">58.6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In contrast, some models like Llama-7B show a more resilient performance under varying Ïµitalic-Ïµ\\epsilon values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when Ïµitalic-Ïµ\\epsilon is increased from the original to 10 (Table 6). This suggests that certain models might be better suited for privacy-sensitive applications.",
            "In contrast, some models like Llama-7B show a more resilient performance under varying Ïµitalic-Ïµ\\epsilon values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when Ïµitalic-Ïµ\\epsilon is increased from the original to 10 (Table 6). This suggests that certain models might be better suited for privacy-sensitive applications."
        ]
    },
    "S5.T7": {
        "caption": "Table 7. The performance of the global model under different Î´ğ›¿\\delta for GPT-2 in general and Medical tasks. ",
        "table": "<table id=\"S5.T7.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T7.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T7.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T7.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\delta\" display=\"inline\"><semantics id=\"S5.T7.3.1.1.m1.1a\"><mi id=\"S5.T7.3.1.1.m1.1.1\" xref=\"S5.T7.3.1.1.m1.1.1.cmml\">Î´</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T7.3.1.1.m1.1b\"><ci id=\"S5.T7.3.1.1.m1.1.1.cmml\" xref=\"S5.T7.3.1.1.m1.1.1\">ğ›¿</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T7.3.1.1.m1.1c\">\\delta</annotation></semantics></math></th>\n<th id=\"S5.T7.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T7.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T7.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T7.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MedQuAD</th>\n<th id=\"S5.T7.3.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LiveQA</th>\n<th id=\"S5.T7.3.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MEDIQA-Ans</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T7.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T7.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T7.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.9</td>\n<td id=\"S5.T7.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">53.8</td>\n<td id=\"S5.T7.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">48.3</td>\n<td id=\"S5.T7.3.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">69.2</td>\n<td id=\"S5.T7.3.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">39.5</td>\n<td id=\"S5.T7.3.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">55.1</td>\n</tr>\n<tr id=\"S5.T7.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T7.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-02</th>\n<td id=\"S5.T7.3.3.2.2\" class=\"ltx_td ltx_align_center\">60.2</td>\n<td id=\"S5.T7.3.3.2.3\" class=\"ltx_td ltx_align_center\">50.7</td>\n<td id=\"S5.T7.3.3.2.4\" class=\"ltx_td ltx_align_center\">46.6</td>\n<td id=\"S5.T7.3.3.2.5\" class=\"ltx_td ltx_align_center\">69.1</td>\n<td id=\"S5.T7.3.3.2.6\" class=\"ltx_td ltx_align_center\">32.5</td>\n<td id=\"S5.T7.3.3.2.7\" class=\"ltx_td ltx_align_center\">51.1</td>\n</tr>\n<tr id=\"S5.T7.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T7.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-03</th>\n<td id=\"S5.T7.3.4.3.2\" class=\"ltx_td ltx_align_center\">54.7</td>\n<td id=\"S5.T7.3.4.3.3\" class=\"ltx_td ltx_align_center\">49.5</td>\n<td id=\"S5.T7.3.4.3.4\" class=\"ltx_td ltx_align_center\">38.7</td>\n<td id=\"S5.T7.3.4.3.5\" class=\"ltx_td ltx_align_center\">59.8</td>\n<td id=\"S5.T7.3.4.3.6\" class=\"ltx_td ltx_align_center\">36.1</td>\n<td id=\"S5.T7.3.4.3.7\" class=\"ltx_td ltx_align_center\">46.1</td>\n</tr>\n<tr id=\"S5.T7.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T7.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-04</th>\n<td id=\"S5.T7.3.5.4.2\" class=\"ltx_td ltx_align_center\">61.6</td>\n<td id=\"S5.T7.3.5.4.3\" class=\"ltx_td ltx_align_center\">47.6</td>\n<td id=\"S5.T7.3.5.4.4\" class=\"ltx_td ltx_align_center\">35.0</td>\n<td id=\"S5.T7.3.5.4.5\" class=\"ltx_td ltx_align_center\">68.5</td>\n<td id=\"S5.T7.3.5.4.6\" class=\"ltx_td ltx_align_center\">30.9</td>\n<td id=\"S5.T7.3.5.4.7\" class=\"ltx_td ltx_align_center\">50.3</td>\n</tr>\n<tr id=\"S5.T7.3.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T7.3.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-05</th>\n<td id=\"S5.T7.3.6.5.2\" class=\"ltx_td ltx_align_center\">51.5</td>\n<td id=\"S5.T7.3.6.5.3\" class=\"ltx_td ltx_align_center\">46.3</td>\n<td id=\"S5.T7.3.6.5.4\" class=\"ltx_td ltx_align_center\">46.1</td>\n<td id=\"S5.T7.3.6.5.5\" class=\"ltx_td ltx_align_center\">55.3</td>\n<td id=\"S5.T7.3.6.5.6\" class=\"ltx_td ltx_align_center\">27.0</td>\n<td id=\"S5.T7.3.6.5.7\" class=\"ltx_td ltx_align_center\">49.1</td>\n</tr>\n<tr id=\"S5.T7.3.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T7.3.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">1e-06</th>\n<td id=\"S5.T7.3.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">53.4</td>\n<td id=\"S5.T7.3.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">42.9</td>\n<td id=\"S5.T7.3.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">40.6</td>\n<td id=\"S5.T7.3.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">69.1</td>\n<td id=\"S5.T7.3.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">30.5</td>\n<td id=\"S5.T7.3.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">41.4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our experiments, we evaluated the performance of several models in the context of differential privacy for medical tasks. We investigated the impact of varying the parameters ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " for the language model fine-tuning on the medical datasets. In all experiments, the compression rank was fixed at ",
                "512",
                "512",
                "512",
                " and the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is set to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we varying the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is settd to ",
                "2",
                "2",
                "2",
                ".",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and higher ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and utilities.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments.",
                "We can observe a significant trade-off between privacy and utility in medical tasks using models like GPT-2, Bert, ChatGLM-6B, and Llama-7B. Our data clearly shows that stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) generally correlate with reduced performance across tasks."
            ]
        ]
    },
    "S5.T8": {
        "caption": "Table 8. The performance of the global model under different Î´ğ›¿\\delta for Bert in general and Medical tasks.",
        "table": "<table id=\"S5.T8.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T8.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T8.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T8.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\delta\" display=\"inline\"><semantics id=\"S5.T8.3.1.1.m1.1a\"><mi id=\"S5.T8.3.1.1.m1.1.1\" xref=\"S5.T8.3.1.1.m1.1.1.cmml\">Î´</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T8.3.1.1.m1.1b\"><ci id=\"S5.T8.3.1.1.m1.1.1.cmml\" xref=\"S5.T8.3.1.1.m1.1.1\">ğ›¿</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T8.3.1.1.m1.1c\">\\delta</annotation></semantics></math></th>\n<th id=\"S5.T8.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T8.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T8.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T8.3.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MedQuAD</th>\n<th id=\"S5.T8.3.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LiveQA</th>\n<th id=\"S5.T8.3.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MEDIQA-Ans</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T8.3.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T8.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T8.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">45.8</td>\n<td id=\"S5.T8.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">51.3</td>\n<td id=\"S5.T8.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">41.2</td>\n<td id=\"S5.T8.3.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">69.1</td>\n<td id=\"S5.T8.3.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">56.4</td>\n<td id=\"S5.T8.3.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">73.3</td>\n</tr>\n<tr id=\"S5.T8.3.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T8.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-02</th>\n<td id=\"S5.T8.3.3.2.2\" class=\"ltx_td ltx_align_center\">37.7</td>\n<td id=\"S5.T8.3.3.2.3\" class=\"ltx_td ltx_align_center\">44.3</td>\n<td id=\"S5.T8.3.3.2.4\" class=\"ltx_td ltx_align_center\">37.8</td>\n<td id=\"S5.T8.3.3.2.5\" class=\"ltx_td ltx_align_center\">65.3</td>\n<td id=\"S5.T8.3.3.2.6\" class=\"ltx_td ltx_align_center\">50.9</td>\n<td id=\"S5.T8.3.3.2.7\" class=\"ltx_td ltx_align_center\">73.3</td>\n</tr>\n<tr id=\"S5.T8.3.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T8.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-03</th>\n<td id=\"S5.T8.3.4.3.2\" class=\"ltx_td ltx_align_center\">37.3</td>\n<td id=\"S5.T8.3.4.3.3\" class=\"ltx_td ltx_align_center\">44.1</td>\n<td id=\"S5.T8.3.4.3.4\" class=\"ltx_td ltx_align_center\">38.6</td>\n<td id=\"S5.T8.3.4.3.5\" class=\"ltx_td ltx_align_center\">65.4</td>\n<td id=\"S5.T8.3.4.3.6\" class=\"ltx_td ltx_align_center\">48.2</td>\n<td id=\"S5.T8.3.4.3.7\" class=\"ltx_td ltx_align_center\">67.5</td>\n</tr>\n<tr id=\"S5.T8.3.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T8.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-04</th>\n<td id=\"S5.T8.3.5.4.2\" class=\"ltx_td ltx_align_center\">35.1</td>\n<td id=\"S5.T8.3.5.4.3\" class=\"ltx_td ltx_align_center\">40.1</td>\n<td id=\"S5.T8.3.5.4.4\" class=\"ltx_td ltx_align_center\">33.6</td>\n<td id=\"S5.T8.3.5.4.5\" class=\"ltx_td ltx_align_center\">64.3</td>\n<td id=\"S5.T8.3.5.4.6\" class=\"ltx_td ltx_align_center\">52.5</td>\n<td id=\"S5.T8.3.5.4.7\" class=\"ltx_td ltx_align_center\">59.1</td>\n</tr>\n<tr id=\"S5.T8.3.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T8.3.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-05</th>\n<td id=\"S5.T8.3.6.5.2\" class=\"ltx_td ltx_align_center\">36.2</td>\n<td id=\"S5.T8.3.6.5.3\" class=\"ltx_td ltx_align_center\">41.0</td>\n<td id=\"S5.T8.3.6.5.4\" class=\"ltx_td ltx_align_center\">29.8</td>\n<td id=\"S5.T8.3.6.5.5\" class=\"ltx_td ltx_align_center\">53.2</td>\n<td id=\"S5.T8.3.6.5.6\" class=\"ltx_td ltx_align_center\">46.8</td>\n<td id=\"S5.T8.3.6.5.7\" class=\"ltx_td ltx_align_center\">66.5</td>\n</tr>\n<tr id=\"S5.T8.3.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T8.3.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">1e-06</th>\n<td id=\"S5.T8.3.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">31.3</td>\n<td id=\"S5.T8.3.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">43.2</td>\n<td id=\"S5.T8.3.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">36.4</td>\n<td id=\"S5.T8.3.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">62.9</td>\n<td id=\"S5.T8.3.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">40.6</td>\n<td id=\"S5.T8.3.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">57.4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Additionally, the impact of changing Î´ğ›¿\\delta values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when Î´ğ›¿\\delta is reduced to 1â€‹eâˆ’061ğ‘’061e-06 (Table 8), highlighting a potentially higher sensitivity to Î´ğ›¿\\delta adjustments.",
            "Additionally, the impact of changing Î´ğ›¿\\delta values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when Î´ğ›¿\\delta is reduced to 1â€‹eâˆ’061ğ‘’061e-06 (Table 8), highlighting a potentially higher sensitivity to Î´ğ›¿\\delta adjustments."
        ]
    },
    "S5.T9": {
        "caption": "Table 9. The performance of the global model under different Î´ğ›¿\\delta for ChatGLM-6B in general and Medical tasks. The left column denotes the maximum value of Î´ğ›¿\\delta. ",
        "table": "<table id=\"S5.T9.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T9.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T9.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T9.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\delta\" display=\"inline\"><semantics id=\"S5.T9.5.1.1.m1.1a\"><mi id=\"S5.T9.5.1.1.m1.1.1\" xref=\"S5.T9.5.1.1.m1.1.1.cmml\">Î´</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T9.5.1.1.m1.1b\"><ci id=\"S5.T9.5.1.1.m1.1.1.cmml\" xref=\"S5.T9.5.1.1.m1.1.1\">ğ›¿</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T9.5.1.1.m1.1c\">\\delta</annotation></semantics></math></th>\n<th id=\"S5.T9.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T9.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T9.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T9.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MedQuAD</th>\n<th id=\"S5.T9.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LiveQA</th>\n<th id=\"S5.T9.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MEDIQA-Ans</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T9.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T9.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T9.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">44.2</td>\n<td id=\"S5.T9.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">51.2</td>\n<td id=\"S5.T9.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">50.1</td>\n<td id=\"S5.T9.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">66.6</td>\n<td id=\"S5.T9.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">71.9</td>\n<td id=\"S5.T9.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">69.3</td>\n</tr>\n<tr id=\"S5.T9.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T9.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-02</th>\n<td id=\"S5.T9.5.3.2.2\" class=\"ltx_td ltx_align_center\">37.7</td>\n<td id=\"S5.T9.5.3.2.3\" class=\"ltx_td ltx_align_center\">44.3</td>\n<td id=\"S5.T9.5.3.2.4\" class=\"ltx_td ltx_align_center\">37.8</td>\n<td id=\"S5.T9.5.3.2.5\" class=\"ltx_td ltx_align_center\">65.3</td>\n<td id=\"S5.T9.5.3.2.6\" class=\"ltx_td ltx_align_center\">50.9</td>\n<td id=\"S5.T9.5.3.2.7\" class=\"ltx_td ltx_align_center\">73.3</td>\n</tr>\n<tr id=\"S5.T9.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T9.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-03</th>\n<td id=\"S5.T9.5.4.3.2\" class=\"ltx_td ltx_align_center\">43.3</td>\n<td id=\"S5.T9.5.4.3.3\" class=\"ltx_td ltx_align_center\">44.1</td>\n<td id=\"S5.T9.5.4.3.4\" class=\"ltx_td ltx_align_center\">38.6</td>\n<td id=\"S5.T9.5.4.3.5\" class=\"ltx_td ltx_align_center\">65.4</td>\n<td id=\"S5.T9.5.4.3.6\" class=\"ltx_td ltx_align_center\">48.2</td>\n<td id=\"S5.T9.5.4.3.7\" class=\"ltx_td ltx_align_center\">67.5</td>\n</tr>\n<tr id=\"S5.T9.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T9.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-04</th>\n<td id=\"S5.T9.5.5.4.2\" class=\"ltx_td ltx_align_center\">39.1</td>\n<td id=\"S5.T9.5.5.4.3\" class=\"ltx_td ltx_align_center\">40.1</td>\n<td id=\"S5.T9.5.5.4.4\" class=\"ltx_td ltx_align_center\">33.6</td>\n<td id=\"S5.T9.5.5.4.5\" class=\"ltx_td ltx_align_center\">64.3</td>\n<td id=\"S5.T9.5.5.4.6\" class=\"ltx_td ltx_align_center\">55.5</td>\n<td id=\"S5.T9.5.5.4.7\" class=\"ltx_td ltx_align_center\">59.1</td>\n</tr>\n<tr id=\"S5.T9.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T9.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-05</th>\n<td id=\"S5.T9.5.6.5.2\" class=\"ltx_td ltx_align_center\">34.0</td>\n<td id=\"S5.T9.5.6.5.3\" class=\"ltx_td ltx_align_center\">47.9</td>\n<td id=\"S5.T9.5.6.5.4\" class=\"ltx_td ltx_align_center\">34.9</td>\n<td id=\"S5.T9.5.6.5.5\" class=\"ltx_td ltx_align_center\">66.4</td>\n<td id=\"S5.T9.5.6.5.6\" class=\"ltx_td ltx_align_center\">67.3</td>\n<td id=\"S5.T9.5.6.5.7\" class=\"ltx_td ltx_align_center\">57.8</td>\n</tr>\n<tr id=\"S5.T9.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T9.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">1e-06</th>\n<td id=\"S5.T9.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">31.3</td>\n<td id=\"S5.T9.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">43.2</td>\n<td id=\"S5.T9.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">36.4</td>\n<td id=\"S5.T9.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">62.9</td>\n<td id=\"S5.T9.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">40.6</td>\n<td id=\"S5.T9.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">57.4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our experiments, we evaluated the performance of several models in the context of differential privacy for medical tasks. We investigated the impact of varying the parameters ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " for the language model fine-tuning on the medical datasets. In all experiments, the compression rank was fixed at ",
                "512",
                "512",
                "512",
                " and the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is set to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we varying the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is settd to ",
                "2",
                "2",
                "2",
                ".",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and higher ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and utilities.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments.",
                "We can observe a significant trade-off between privacy and utility in medical tasks using models like GPT-2, Bert, ChatGLM-6B, and Llama-7B. Our data clearly shows that stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) generally correlate with reduced performance across tasks."
            ]
        ]
    },
    "S5.T10": {
        "caption": "Table 10. The performance of the global model under different Î´ğ›¿\\delta for Llama-7B in general and Medical tasks. The left column denotes the maximum value of Î´ğ›¿\\delta. ",
        "table": "<table id=\"S5.T10.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T10.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T10.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T10.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\delta\" display=\"inline\"><semantics id=\"S5.T10.5.1.1.m1.1a\"><mi id=\"S5.T10.5.1.1.m1.1.1\" xref=\"S5.T10.5.1.1.m1.1.1.cmml\">Î´</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T10.5.1.1.m1.1b\"><ci id=\"S5.T10.5.1.1.m1.1.1.cmml\" xref=\"S5.T10.5.1.1.m1.1.1\">ğ›¿</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T10.5.1.1.m1.1c\">\\delta</annotation></semantics></math></th>\n<th id=\"S5.T10.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T10.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T10.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T10.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MedQuAD</th>\n<th id=\"S5.T10.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">LiveQA</th>\n<th id=\"S5.T10.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">MEDIQA-Ans</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T10.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T10.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T10.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">71.3</td>\n<td id=\"S5.T10.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">66.2</td>\n<td id=\"S5.T10.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">65.5</td>\n<td id=\"S5.T10.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">73.1</td>\n<td id=\"S5.T10.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">69.4</td>\n<td id=\"S5.T10.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">65.5</td>\n</tr>\n<tr id=\"S5.T10.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T10.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-02</th>\n<td id=\"S5.T10.5.3.2.2\" class=\"ltx_td ltx_align_center\">68.7</td>\n<td id=\"S5.T10.5.3.2.3\" class=\"ltx_td ltx_align_center\">60.3</td>\n<td id=\"S5.T10.5.3.2.4\" class=\"ltx_td ltx_align_center\">63.7</td>\n<td id=\"S5.T10.5.3.2.5\" class=\"ltx_td ltx_align_center\">70.6</td>\n<td id=\"S5.T10.5.3.2.6\" class=\"ltx_td ltx_align_center\">68.7</td>\n<td id=\"S5.T10.5.3.2.7\" class=\"ltx_td ltx_align_center\">60.2</td>\n</tr>\n<tr id=\"S5.T10.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T10.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-03</th>\n<td id=\"S5.T10.5.4.3.2\" class=\"ltx_td ltx_align_center\">67.1</td>\n<td id=\"S5.T10.5.4.3.3\" class=\"ltx_td ltx_align_center\">62.6</td>\n<td id=\"S5.T10.5.4.3.4\" class=\"ltx_td ltx_align_center\">65.0</td>\n<td id=\"S5.T10.5.4.3.5\" class=\"ltx_td ltx_align_center\">64.1</td>\n<td id=\"S5.T10.5.4.3.6\" class=\"ltx_td ltx_align_center\">63.8</td>\n<td id=\"S5.T10.5.4.3.7\" class=\"ltx_td ltx_align_center\">62.1</td>\n</tr>\n<tr id=\"S5.T10.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T10.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-04</th>\n<td id=\"S5.T10.5.5.4.2\" class=\"ltx_td ltx_align_center\">59.9</td>\n<td id=\"S5.T10.5.5.4.3\" class=\"ltx_td ltx_align_center\">57.9</td>\n<td id=\"S5.T10.5.5.4.4\" class=\"ltx_td ltx_align_center\">56.6</td>\n<td id=\"S5.T10.5.5.4.5\" class=\"ltx_td ltx_align_center\">70.3</td>\n<td id=\"S5.T10.5.5.4.6\" class=\"ltx_td ltx_align_center\">60.7</td>\n<td id=\"S5.T10.5.5.4.7\" class=\"ltx_td ltx_align_center\">58.0</td>\n</tr>\n<tr id=\"S5.T10.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T10.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-05</th>\n<td id=\"S5.T10.5.6.5.2\" class=\"ltx_td ltx_align_center\">54.9</td>\n<td id=\"S5.T10.5.6.5.3\" class=\"ltx_td ltx_align_center\">57.1</td>\n<td id=\"S5.T10.5.6.5.4\" class=\"ltx_td ltx_align_center\">53.4</td>\n<td id=\"S5.T10.5.6.5.5\" class=\"ltx_td ltx_align_center\">66.5</td>\n<td id=\"S5.T10.5.6.5.6\" class=\"ltx_td ltx_align_center\">55.9</td>\n<td id=\"S5.T10.5.6.5.7\" class=\"ltx_td ltx_align_center\">52.8</td>\n</tr>\n<tr id=\"S5.T10.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T10.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">1e-06</th>\n<td id=\"S5.T10.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">49.7</td>\n<td id=\"S5.T10.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">48.4</td>\n<td id=\"S5.T10.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">59.1</td>\n<td id=\"S5.T10.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">59.9</td>\n<td id=\"S5.T10.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">49.3</td>\n<td id=\"S5.T10.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">44.3</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our experiments, we evaluated the performance of several models in the context of differential privacy for medical tasks. We investigated the impact of varying the parameters ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " for the language model fine-tuning on the medical datasets. In all experiments, the compression rank was fixed at ",
                "512",
                "512",
                "512",
                " and the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is set to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we varying the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is settd to ",
                "2",
                "2",
                "2",
                ".",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and higher ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and utilities.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments.",
                "We can observe a significant trade-off between privacy and utility in medical tasks using models like GPT-2, Bert, ChatGLM-6B, and Llama-7B. Our data clearly shows that stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) generally correlate with reduced performance across tasks."
            ]
        ]
    },
    "S5.T11": {
        "caption": "Table 11. The performance of the global model under different Î´ğ›¿\\delta for GPT-2 in general and financial tasks. The left column denotes the maximum value of Î´ğ›¿\\delta.",
        "table": "<table id=\"S5.T11.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T11.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T11.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T11.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\delta\" display=\"inline\"><semantics id=\"S5.T11.5.1.1.m1.1a\"><mi id=\"S5.T11.5.1.1.m1.1.1\" xref=\"S5.T11.5.1.1.m1.1.1.cmml\">Î´</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T11.5.1.1.m1.1b\"><ci id=\"S5.T11.5.1.1.m1.1.1.cmml\" xref=\"S5.T11.5.1.1.m1.1.1\">ğ›¿</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T11.5.1.1.m1.1c\">\\delta</annotation></semantics></math></th>\n<th id=\"S5.T11.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T11.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T11.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T11.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FPB</th>\n<th id=\"S5.T11.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FiQA SA</th>\n<th id=\"S5.T11.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TFNS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T11.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T11.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T11.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">41.1</td>\n<td id=\"S5.T11.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">39.9</td>\n<td id=\"S5.T11.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">37.5</td>\n<td id=\"S5.T11.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">59.8</td>\n<td id=\"S5.T11.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">73.8</td>\n<td id=\"S5.T11.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">66,0</td>\n</tr>\n<tr id=\"S5.T11.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T11.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-02</th>\n<td id=\"S5.T11.5.3.2.2\" class=\"ltx_td ltx_align_center\">36.4</td>\n<td id=\"S5.T11.5.3.2.3\" class=\"ltx_td ltx_align_center\">39.6</td>\n<td id=\"S5.T11.5.3.2.4\" class=\"ltx_td ltx_align_center\">35.7</td>\n<td id=\"S5.T11.5.3.2.5\" class=\"ltx_td ltx_align_center\">57.1</td>\n<td id=\"S5.T11.5.3.2.6\" class=\"ltx_td ltx_align_center\">71.4</td>\n<td id=\"S5.T11.5.3.2.7\" class=\"ltx_td ltx_align_center\">57.0</td>\n</tr>\n<tr id=\"S5.T11.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T11.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-03</th>\n<td id=\"S5.T11.5.4.3.2\" class=\"ltx_td ltx_align_center\">34.7</td>\n<td id=\"S5.T11.5.4.3.3\" class=\"ltx_td ltx_align_center\">35.4</td>\n<td id=\"S5.T11.5.4.3.4\" class=\"ltx_td ltx_align_center\">29.4</td>\n<td id=\"S5.T11.5.4.3.5\" class=\"ltx_td ltx_align_center\">51.1</td>\n<td id=\"S5.T11.5.4.3.6\" class=\"ltx_td ltx_align_center\">63.3</td>\n<td id=\"S5.T11.5.4.3.7\" class=\"ltx_td ltx_align_center\">52.2</td>\n</tr>\n<tr id=\"S5.T11.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T11.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-04</th>\n<td id=\"S5.T11.5.5.4.2\" class=\"ltx_td ltx_align_center\">33.1</td>\n<td id=\"S5.T11.5.5.4.3\" class=\"ltx_td ltx_align_center\">28.6</td>\n<td id=\"S5.T11.5.5.4.4\" class=\"ltx_td ltx_align_center\">27.8</td>\n<td id=\"S5.T11.5.5.4.5\" class=\"ltx_td ltx_align_center\">42.1</td>\n<td id=\"S5.T11.5.5.4.6\" class=\"ltx_td ltx_align_center\">57.5</td>\n<td id=\"S5.T11.5.5.4.7\" class=\"ltx_td ltx_align_center\">45.1</td>\n</tr>\n<tr id=\"S5.T11.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T11.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-05</th>\n<td id=\"S5.T11.5.6.5.2\" class=\"ltx_td ltx_align_center\">25.5</td>\n<td id=\"S5.T11.5.6.5.3\" class=\"ltx_td ltx_align_center\">20.5</td>\n<td id=\"S5.T11.5.6.5.4\" class=\"ltx_td ltx_align_center\">22.2</td>\n<td id=\"S5.T11.5.6.5.5\" class=\"ltx_td ltx_align_center\">39.1</td>\n<td id=\"S5.T11.5.6.5.6\" class=\"ltx_td ltx_align_center\">42.9</td>\n<td id=\"S5.T11.5.6.5.7\" class=\"ltx_td ltx_align_center\">34.3</td>\n</tr>\n<tr id=\"S5.T11.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T11.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">1e-06</th>\n<td id=\"S5.T11.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">23.2</td>\n<td id=\"S5.T11.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">19.0</td>\n<td id=\"S5.T11.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">20.3</td>\n<td id=\"S5.T11.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">28.1</td>\n<td id=\"S5.T11.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">52.2</td>\n<td id=\"S5.T11.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">34.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we compare the financial tasks performance of the global model which is fine-tuned on the financial datasets. In our experiments, We assume that there are ",
                "5",
                "5",
                "5",
                " clients in participating the whole training process. We use the above-mentioned models as our initial models. All the local models and global models follow the same size and use the original model weights in the initial. We trained each local model under the same set of parameters, such as learning rate, decay strategy, and training epochs. We set the compression rank ",
                "r",
                "ğ‘Ÿ",
                "r",
                " to 512 and ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " value as 2 when we vary the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " as ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we vary the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                ". The origin row means that we train our model without differential private adaptations.",
                "The experimental data consistently demonstrate a clear trade-off between the privacy level (as adjusted by ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) and the model performance across various tasks. While all models show a decrease in performance with stricter privacy settings, the extent of this decrease varies, suggesting differences in how each model adapts to privacy constraints.",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and effectiveness.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments."
            ]
        ]
    },
    "S5.T12": {
        "caption": "Table 12. The performance of the global model under different Î´ğ›¿\\delta for Bert in general and financial tasks. The left column denotes the maximum value of Î´ğ›¿\\delta.",
        "table": "<table id=\"S5.T12.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T12.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T12.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T12.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\delta\" display=\"inline\"><semantics id=\"S5.T12.5.1.1.m1.1a\"><mi id=\"S5.T12.5.1.1.m1.1.1\" xref=\"S5.T12.5.1.1.m1.1.1.cmml\">Î´</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T12.5.1.1.m1.1b\"><ci id=\"S5.T12.5.1.1.m1.1.1.cmml\" xref=\"S5.T12.5.1.1.m1.1.1\">ğ›¿</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T12.5.1.1.m1.1c\">\\delta</annotation></semantics></math></th>\n<th id=\"S5.T12.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T12.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T12.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T12.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FPB</th>\n<th id=\"S5.T12.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FiQA SA</th>\n<th id=\"S5.T12.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TFNS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T12.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T12.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T12.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">45.3</td>\n<td id=\"S5.T12.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">47.5</td>\n<td id=\"S5.T12.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">51.3</td>\n<td id=\"S5.T12.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">65.5</td>\n<td id=\"S5.T12.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">60.1</td>\n<td id=\"S5.T12.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">57.3</td>\n</tr>\n<tr id=\"S5.T12.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T12.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-02</th>\n<td id=\"S5.T12.5.3.2.2\" class=\"ltx_td ltx_align_center\">41.8</td>\n<td id=\"S5.T12.5.3.2.3\" class=\"ltx_td ltx_align_center\">41.7</td>\n<td id=\"S5.T12.5.3.2.4\" class=\"ltx_td ltx_align_center\">46.7</td>\n<td id=\"S5.T12.5.3.2.5\" class=\"ltx_td ltx_align_center\">59.6</td>\n<td id=\"S5.T12.5.3.2.6\" class=\"ltx_td ltx_align_center\">59.1</td>\n<td id=\"S5.T12.5.3.2.7\" class=\"ltx_td ltx_align_center\">55.6</td>\n</tr>\n<tr id=\"S5.T12.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T12.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-03</th>\n<td id=\"S5.T12.5.4.3.2\" class=\"ltx_td ltx_align_center\">37.9</td>\n<td id=\"S5.T12.5.4.3.3\" class=\"ltx_td ltx_align_center\">39.7</td>\n<td id=\"S5.T12.5.4.3.4\" class=\"ltx_td ltx_align_center\">44.9</td>\n<td id=\"S5.T12.5.4.3.5\" class=\"ltx_td ltx_align_center\">50.6</td>\n<td id=\"S5.T12.5.4.3.6\" class=\"ltx_td ltx_align_center\">55.6</td>\n<td id=\"S5.T12.5.4.3.7\" class=\"ltx_td ltx_align_center\">50.9</td>\n</tr>\n<tr id=\"S5.T12.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T12.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-04</th>\n<td id=\"S5.T12.5.5.4.2\" class=\"ltx_td ltx_align_center\">37.7</td>\n<td id=\"S5.T12.5.5.4.3\" class=\"ltx_td ltx_align_center\">36.8</td>\n<td id=\"S5.T12.5.5.4.4\" class=\"ltx_td ltx_align_center\">39.2</td>\n<td id=\"S5.T12.5.5.4.5\" class=\"ltx_td ltx_align_center\">48.4</td>\n<td id=\"S5.T12.5.5.4.6\" class=\"ltx_td ltx_align_center\">51.4</td>\n<td id=\"S5.T12.5.5.4.7\" class=\"ltx_td ltx_align_center\">47.9</td>\n</tr>\n<tr id=\"S5.T12.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T12.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-05</th>\n<td id=\"S5.T12.5.6.5.2\" class=\"ltx_td ltx_align_center\">21.7</td>\n<td id=\"S5.T12.5.6.5.3\" class=\"ltx_td ltx_align_center\">29.9</td>\n<td id=\"S5.T12.5.6.5.4\" class=\"ltx_td ltx_align_center\">38.8</td>\n<td id=\"S5.T12.5.6.5.5\" class=\"ltx_td ltx_align_center\">43.7</td>\n<td id=\"S5.T12.5.6.5.6\" class=\"ltx_td ltx_align_center\">47.5</td>\n<td id=\"S5.T12.5.6.5.7\" class=\"ltx_td ltx_align_center\">44.1</td>\n</tr>\n<tr id=\"S5.T12.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T12.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">1e-06</th>\n<td id=\"S5.T12.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">20.3</td>\n<td id=\"S5.T12.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">28.7</td>\n<td id=\"S5.T12.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">30.2</td>\n<td id=\"S5.T12.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">37.3</td>\n<td id=\"S5.T12.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">45.3</td>\n<td id=\"S5.T12.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">40.3</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we compare the financial tasks performance of the global model which is fine-tuned on the financial datasets. In our experiments, We assume that there are ",
                "5",
                "5",
                "5",
                " clients in participating the whole training process. We use the above-mentioned models as our initial models. All the local models and global models follow the same size and use the original model weights in the initial. We trained each local model under the same set of parameters, such as learning rate, decay strategy, and training epochs. We set the compression rank ",
                "r",
                "ğ‘Ÿ",
                "r",
                " to 512 and ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " value as 2 when we vary the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " as ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we vary the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                ". The origin row means that we train our model without differential private adaptations.",
                "The experimental data consistently demonstrate a clear trade-off between the privacy level (as adjusted by ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) and the model performance across various tasks. While all models show a decrease in performance with stricter privacy settings, the extent of this decrease varies, suggesting differences in how each model adapts to privacy constraints.",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and effectiveness.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments."
            ]
        ]
    },
    "S5.T13": {
        "caption": "Table 13. The performance of the global model under different Î´ğ›¿\\delta for ChatGLM-6B in general and financial tasks. The left column denotes the maximum value of Î´ğ›¿\\delta. ",
        "table": "<table id=\"S5.T13.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T13.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T13.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T13.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\delta\" display=\"inline\"><semantics id=\"S5.T13.5.1.1.m1.1a\"><mi id=\"S5.T13.5.1.1.m1.1.1\" xref=\"S5.T13.5.1.1.m1.1.1.cmml\">Î´</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T13.5.1.1.m1.1b\"><ci id=\"S5.T13.5.1.1.m1.1.1.cmml\" xref=\"S5.T13.5.1.1.m1.1.1\">ğ›¿</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T13.5.1.1.m1.1c\">\\delta</annotation></semantics></math></th>\n<th id=\"S5.T13.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T13.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T13.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T13.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FPB</th>\n<th id=\"S5.T13.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FiQA SA</th>\n<th id=\"S5.T13.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TFNS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T13.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T13.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T13.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.5</td>\n<td id=\"S5.T13.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">69.3</td>\n<td id=\"S5.T13.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">63.8</td>\n<td id=\"S5.T13.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">55.5</td>\n<td id=\"S5.T13.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">74.3</td>\n<td id=\"S5.T13.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">70.1</td>\n</tr>\n<tr id=\"S5.T13.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T13.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-02</th>\n<td id=\"S5.T13.5.3.2.2\" class=\"ltx_td ltx_align_center\">55.5</td>\n<td id=\"S5.T13.5.3.2.3\" class=\"ltx_td ltx_align_center\">68.7</td>\n<td id=\"S5.T13.5.3.2.4\" class=\"ltx_td ltx_align_center\">61.1</td>\n<td id=\"S5.T13.5.3.2.5\" class=\"ltx_td ltx_align_center\">50.7</td>\n<td id=\"S5.T13.5.3.2.6\" class=\"ltx_td ltx_align_center\">69.4</td>\n<td id=\"S5.T13.5.3.2.7\" class=\"ltx_td ltx_align_center\">62.2</td>\n</tr>\n<tr id=\"S5.T13.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T13.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-03</th>\n<td id=\"S5.T13.5.4.3.2\" class=\"ltx_td ltx_align_center\">53.6</td>\n<td id=\"S5.T13.5.4.3.3\" class=\"ltx_td ltx_align_center\">65.3</td>\n<td id=\"S5.T13.5.4.3.4\" class=\"ltx_td ltx_align_center\">54.9</td>\n<td id=\"S5.T13.5.4.3.5\" class=\"ltx_td ltx_align_center\">49.1</td>\n<td id=\"S5.T13.5.4.3.6\" class=\"ltx_td ltx_align_center\">64.5</td>\n<td id=\"S5.T13.5.4.3.7\" class=\"ltx_td ltx_align_center\">56.2</td>\n</tr>\n<tr id=\"S5.T13.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T13.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-04</th>\n<td id=\"S5.T13.5.5.4.2\" class=\"ltx_td ltx_align_center\">50.7</td>\n<td id=\"S5.T13.5.5.4.3\" class=\"ltx_td ltx_align_center\">62.3</td>\n<td id=\"S5.T13.5.5.4.4\" class=\"ltx_td ltx_align_center\">51.7</td>\n<td id=\"S5.T13.5.5.4.5\" class=\"ltx_td ltx_align_center\">47.4</td>\n<td id=\"S5.T13.5.5.4.6\" class=\"ltx_td ltx_align_center\">58.9</td>\n<td id=\"S5.T13.5.5.4.7\" class=\"ltx_td ltx_align_center\">47.6</td>\n</tr>\n<tr id=\"S5.T13.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T13.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-05</th>\n<td id=\"S5.T13.5.6.5.2\" class=\"ltx_td ltx_align_center\">44.9</td>\n<td id=\"S5.T13.5.6.5.3\" class=\"ltx_td ltx_align_center\">58.3</td>\n<td id=\"S5.T13.5.6.5.4\" class=\"ltx_td ltx_align_center\">48.6</td>\n<td id=\"S5.T13.5.6.5.5\" class=\"ltx_td ltx_align_center\">29.9</td>\n<td id=\"S5.T13.5.6.5.6\" class=\"ltx_td ltx_align_center\">48.1</td>\n<td id=\"S5.T13.5.6.5.7\" class=\"ltx_td ltx_align_center\">42.5</td>\n</tr>\n<tr id=\"S5.T13.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T13.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">1e-06</th>\n<td id=\"S5.T13.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">38.5</td>\n<td id=\"S5.T13.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">55.2</td>\n<td id=\"S5.T13.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">47.4</td>\n<td id=\"S5.T13.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">30.6</td>\n<td id=\"S5.T13.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">51.5</td>\n<td id=\"S5.T13.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">38.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we compare the financial tasks performance of the global model which is fine-tuned on the financial datasets. In our experiments, We assume that there are ",
                "5",
                "5",
                "5",
                " clients in participating the whole training process. We use the above-mentioned models as our initial models. All the local models and global models follow the same size and use the original model weights in the initial. We trained each local model under the same set of parameters, such as learning rate, decay strategy, and training epochs. We set the compression rank ",
                "r",
                "ğ‘Ÿ",
                "r",
                " to 512 and ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " value as 2 when we vary the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " as ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we vary the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                ". The origin row means that we train our model without differential private adaptations.",
                "The experimental data consistently demonstrate a clear trade-off between the privacy level (as adjusted by ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) and the model performance across various tasks. While all models show a decrease in performance with stricter privacy settings, the extent of this decrease varies, suggesting differences in how each model adapts to privacy constraints.",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and effectiveness.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments."
            ]
        ]
    },
    "S5.T14": {
        "caption": "Table 14. The performance of the global model under different Î´ğ›¿\\delta for Llama-7B in general and financial tasks. The left column denotes the maximum value of Î´ğ›¿\\delta. ",
        "table": "<table id=\"S5.T14.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T14.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T14.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T14.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\delta\" display=\"inline\"><semantics id=\"S5.T14.5.1.1.m1.1a\"><mi id=\"S5.T14.5.1.1.m1.1.1\" xref=\"S5.T14.5.1.1.m1.1.1.cmml\">Î´</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T14.5.1.1.m1.1b\"><ci id=\"S5.T14.5.1.1.m1.1.1.cmml\" xref=\"S5.T14.5.1.1.m1.1.1\">ğ›¿</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T14.5.1.1.m1.1c\">\\delta</annotation></semantics></math></th>\n<th id=\"S5.T14.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T14.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T14.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T14.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FPB</th>\n<th id=\"S5.T14.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FiQA SA</th>\n<th id=\"S5.T14.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TFNS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T14.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T14.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T14.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">76.3</td>\n<td id=\"S5.T14.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">79.7</td>\n<td id=\"S5.T14.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">70.0</td>\n<td id=\"S5.T14.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">59.1</td>\n<td id=\"S5.T14.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">80.0</td>\n<td id=\"S5.T14.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">69.6</td>\n</tr>\n<tr id=\"S5.T14.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T14.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-02</th>\n<td id=\"S5.T14.5.3.2.2\" class=\"ltx_td ltx_align_center\">88.1</td>\n<td id=\"S5.T14.5.3.2.3\" class=\"ltx_td ltx_align_center\">80.6</td>\n<td id=\"S5.T14.5.3.2.4\" class=\"ltx_td ltx_align_center\">69.3</td>\n<td id=\"S5.T14.5.3.2.5\" class=\"ltx_td ltx_align_center\">80.3</td>\n<td id=\"S5.T14.5.3.2.6\" class=\"ltx_td ltx_align_center\">81.9</td>\n<td id=\"S5.T14.5.3.2.7\" class=\"ltx_td ltx_align_center\">67.3</td>\n</tr>\n<tr id=\"S5.T14.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T14.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-03</th>\n<td id=\"S5.T14.5.4.3.2\" class=\"ltx_td ltx_align_center\">81.8</td>\n<td id=\"S5.T14.5.4.3.3\" class=\"ltx_td ltx_align_center\">78.2</td>\n<td id=\"S5.T14.5.4.3.4\" class=\"ltx_td ltx_align_center\">57.6</td>\n<td id=\"S5.T14.5.4.3.5\" class=\"ltx_td ltx_align_center\">64.1</td>\n<td id=\"S5.T14.5.4.3.6\" class=\"ltx_td ltx_align_center\">75.0</td>\n<td id=\"S5.T14.5.4.3.7\" class=\"ltx_td ltx_align_center\">65.4</td>\n</tr>\n<tr id=\"S5.T14.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T14.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-04</th>\n<td id=\"S5.T14.5.5.4.2\" class=\"ltx_td ltx_align_center\">75.4</td>\n<td id=\"S5.T14.5.5.4.3\" class=\"ltx_td ltx_align_center\">69.3</td>\n<td id=\"S5.T14.5.5.4.4\" class=\"ltx_td ltx_align_center\">59.1</td>\n<td id=\"S5.T14.5.5.4.5\" class=\"ltx_td ltx_align_center\">49.1</td>\n<td id=\"S5.T14.5.5.4.6\" class=\"ltx_td ltx_align_center\">76.1</td>\n<td id=\"S5.T14.5.5.4.7\" class=\"ltx_td ltx_align_center\">70.3</td>\n</tr>\n<tr id=\"S5.T14.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T14.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1e-05</th>\n<td id=\"S5.T14.5.6.5.2\" class=\"ltx_td ltx_align_center\">59.3</td>\n<td id=\"S5.T14.5.6.5.3\" class=\"ltx_td ltx_align_center\">71.5</td>\n<td id=\"S5.T14.5.6.5.4\" class=\"ltx_td ltx_align_center\">61.6</td>\n<td id=\"S5.T14.5.6.5.5\" class=\"ltx_td ltx_align_center\">38.5</td>\n<td id=\"S5.T14.5.6.5.6\" class=\"ltx_td ltx_align_center\">48.3</td>\n<td id=\"S5.T14.5.6.5.7\" class=\"ltx_td ltx_align_center\">43.2</td>\n</tr>\n<tr id=\"S5.T14.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T14.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">1e-06</th>\n<td id=\"S5.T14.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">69.5</td>\n<td id=\"S5.T14.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">56.6</td>\n<td id=\"S5.T14.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">63.5</td>\n<td id=\"S5.T14.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">35.2</td>\n<td id=\"S5.T14.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">51.5</td>\n<td id=\"S5.T14.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">28.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we compare the financial tasks performance of the global model which is fine-tuned on the financial datasets. In our experiments, We assume that there are ",
                "5",
                "5",
                "5",
                " clients in participating the whole training process. We use the above-mentioned models as our initial models. All the local models and global models follow the same size and use the original model weights in the initial. We trained each local model under the same set of parameters, such as learning rate, decay strategy, and training epochs. We set the compression rank ",
                "r",
                "ğ‘Ÿ",
                "r",
                " to 512 and ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " value as 2 when we vary the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " as ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we vary the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                ". The origin row means that we train our model without differential private adaptations.",
                "The experimental data consistently demonstrate a clear trade-off between the privacy level (as adjusted by ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) and the model performance across various tasks. While all models show a decrease in performance with stricter privacy settings, the extent of this decrease varies, suggesting differences in how each model adapts to privacy constraints.",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and effectiveness.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments."
            ]
        ]
    },
    "S5.T15": {
        "caption": "Table 15. The performance of the global model under different Ïµitalic-Ïµ\\epsilon for GPT-2 in general and financial tasks. The left column denotes the maximum value of Ïµitalic-Ïµ\\epsilon.",
        "table": "<table id=\"S5.T15.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T15.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T15.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T15.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T15.5.1.1.m1.1a\"><mi id=\"S5.T15.5.1.1.m1.1.1\" xref=\"S5.T15.5.1.1.m1.1.1.cmml\">Ïµ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T15.5.1.1.m1.1b\"><ci id=\"S5.T15.5.1.1.m1.1.1.cmml\" xref=\"S5.T15.5.1.1.m1.1.1\">italic-Ïµ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T15.5.1.1.m1.1c\">\\epsilon</annotation></semantics></math></th>\n<th id=\"S5.T15.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T15.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T15.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T15.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FPB</th>\n<th id=\"S5.T15.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FiQA SA</th>\n<th id=\"S5.T15.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TFNS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T15.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T15.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T15.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">41.1</td>\n<td id=\"S5.T15.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">39.9</td>\n<td id=\"S5.T15.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">37.5</td>\n<td id=\"S5.T15.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">59.8</td>\n<td id=\"S5.T15.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">73.8</td>\n<td id=\"S5.T15.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">66</td>\n</tr>\n<tr id=\"S5.T15.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T15.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td id=\"S5.T15.5.3.2.2\" class=\"ltx_td ltx_align_center\">25.5</td>\n<td id=\"S5.T15.5.3.2.3\" class=\"ltx_td ltx_align_center\">20.5</td>\n<td id=\"S5.T15.5.3.2.4\" class=\"ltx_td ltx_align_center\">22.2</td>\n<td id=\"S5.T15.5.3.2.5\" class=\"ltx_td ltx_align_center\">39.1</td>\n<td id=\"S5.T15.5.3.2.6\" class=\"ltx_td ltx_align_center\">42.9</td>\n<td id=\"S5.T15.5.3.2.7\" class=\"ltx_td ltx_align_center\">34.3</td>\n</tr>\n<tr id=\"S5.T15.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T15.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td id=\"S5.T15.5.4.3.2\" class=\"ltx_td ltx_align_center\">26.8</td>\n<td id=\"S5.T15.5.4.3.3\" class=\"ltx_td ltx_align_center\">19.5</td>\n<td id=\"S5.T15.5.4.3.4\" class=\"ltx_td ltx_align_center\">29.3</td>\n<td id=\"S5.T15.5.4.3.5\" class=\"ltx_td ltx_align_center\">40.8</td>\n<td id=\"S5.T15.5.4.3.6\" class=\"ltx_td ltx_align_center\">53.9</td>\n<td id=\"S5.T15.5.4.3.7\" class=\"ltx_td ltx_align_center\">35.2</td>\n</tr>\n<tr id=\"S5.T15.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T15.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">6</th>\n<td id=\"S5.T15.5.5.4.2\" class=\"ltx_td ltx_align_center\">37.3</td>\n<td id=\"S5.T15.5.5.4.3\" class=\"ltx_td ltx_align_center\">26.5</td>\n<td id=\"S5.T15.5.5.4.4\" class=\"ltx_td ltx_align_center\">28.2</td>\n<td id=\"S5.T15.5.5.4.5\" class=\"ltx_td ltx_align_center\">49.0</td>\n<td id=\"S5.T15.5.5.4.6\" class=\"ltx_td ltx_align_center\">55.5</td>\n<td id=\"S5.T15.5.5.4.7\" class=\"ltx_td ltx_align_center\">49.6</td>\n</tr>\n<tr id=\"S5.T15.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T15.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8</th>\n<td id=\"S5.T15.5.6.5.2\" class=\"ltx_td ltx_align_center\">33.9</td>\n<td id=\"S5.T15.5.6.5.3\" class=\"ltx_td ltx_align_center\">27.5</td>\n<td id=\"S5.T15.5.6.5.4\" class=\"ltx_td ltx_align_center\">31.1</td>\n<td id=\"S5.T15.5.6.5.5\" class=\"ltx_td ltx_align_center\">53.2</td>\n<td id=\"S5.T15.5.6.5.6\" class=\"ltx_td ltx_align_center\">56.9</td>\n<td id=\"S5.T15.5.6.5.7\" class=\"ltx_td ltx_align_center\">54.7</td>\n</tr>\n<tr id=\"S5.T15.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T15.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T15.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">39.5</td>\n<td id=\"S5.T15.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">31.2</td>\n<td id=\"S5.T15.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">31.2</td>\n<td id=\"S5.T15.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">53.9</td>\n<td id=\"S5.T15.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">57.1</td>\n<td id=\"S5.T15.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">54.8</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we compare the financial tasks performance of the global model which is fine-tuned on the financial datasets. In our experiments, We assume that there are ",
                "5",
                "5",
                "5",
                " clients in participating the whole training process. We use the above-mentioned models as our initial models. All the local models and global models follow the same size and use the original model weights in the initial. We trained each local model under the same set of parameters, such as learning rate, decay strategy, and training epochs. We set the compression rank ",
                "r",
                "ğ‘Ÿ",
                "r",
                " to 512 and ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " value as 2 when we vary the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " as ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we vary the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                ". The origin row means that we train our model without differential private adaptations.",
                "The experimental data consistently demonstrate a clear trade-off between the privacy level (as adjusted by ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) and the model performance across various tasks. While all models show a decrease in performance with stricter privacy settings, the extent of this decrease varies, suggesting differences in how each model adapts to privacy constraints.",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and effectiveness.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments."
            ]
        ]
    },
    "S5.T16": {
        "caption": "Table 16. The performance of the global model under different Ïµitalic-Ïµ\\epsilon for Bert in general and financial tasks. The left column denotes the maximum value of Ïµitalic-Ïµ\\epsilon.",
        "table": "<table id=\"S5.T16.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T16.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T16.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T16.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T16.5.1.1.m1.1a\"><mi id=\"S5.T16.5.1.1.m1.1.1\" xref=\"S5.T16.5.1.1.m1.1.1.cmml\">Ïµ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T16.5.1.1.m1.1b\"><ci id=\"S5.T16.5.1.1.m1.1.1.cmml\" xref=\"S5.T16.5.1.1.m1.1.1\">italic-Ïµ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T16.5.1.1.m1.1c\">\\epsilon</annotation></semantics></math></th>\n<th id=\"S5.T16.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T16.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T16.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T16.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FPB</th>\n<th id=\"S5.T16.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FiQA SA</th>\n<th id=\"S5.T16.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TFNS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T16.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T16.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T16.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">45.3</td>\n<td id=\"S5.T16.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">47.5</td>\n<td id=\"S5.T16.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">51.3</td>\n<td id=\"S5.T16.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">65.5</td>\n<td id=\"S5.T16.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">60.1</td>\n<td id=\"S5.T16.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">57.3</td>\n</tr>\n<tr id=\"S5.T16.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T16.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td id=\"S5.T16.5.3.2.2\" class=\"ltx_td ltx_align_center\">21.7</td>\n<td id=\"S5.T16.5.3.2.3\" class=\"ltx_td ltx_align_center\">29.9</td>\n<td id=\"S5.T16.5.3.2.4\" class=\"ltx_td ltx_align_center\">38.8</td>\n<td id=\"S5.T16.5.3.2.5\" class=\"ltx_td ltx_align_center\">43.7</td>\n<td id=\"S5.T16.5.3.2.6\" class=\"ltx_td ltx_align_center\">47.5</td>\n<td id=\"S5.T16.5.3.2.7\" class=\"ltx_td ltx_align_center\">44.1</td>\n</tr>\n<tr id=\"S5.T16.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T16.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td id=\"S5.T16.5.4.3.2\" class=\"ltx_td ltx_align_center\">24.5</td>\n<td id=\"S5.T16.5.4.3.3\" class=\"ltx_td ltx_align_center\">33.5</td>\n<td id=\"S5.T16.5.4.3.4\" class=\"ltx_td ltx_align_center\">39.9</td>\n<td id=\"S5.T16.5.4.3.5\" class=\"ltx_td ltx_align_center\">45.2</td>\n<td id=\"S5.T16.5.4.3.6\" class=\"ltx_td ltx_align_center\">47.3</td>\n<td id=\"S5.T16.5.4.3.7\" class=\"ltx_td ltx_align_center\">44.5</td>\n</tr>\n<tr id=\"S5.T16.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T16.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">6</th>\n<td id=\"S5.T16.5.5.4.2\" class=\"ltx_td ltx_align_center\">27.5</td>\n<td id=\"S5.T16.5.5.4.3\" class=\"ltx_td ltx_align_center\">34.6</td>\n<td id=\"S5.T16.5.5.4.4\" class=\"ltx_td ltx_align_center\">45.0</td>\n<td id=\"S5.T16.5.5.4.5\" class=\"ltx_td ltx_align_center\">49.9</td>\n<td id=\"S5.T16.5.5.4.6\" class=\"ltx_td ltx_align_center\">53.8</td>\n<td id=\"S5.T16.5.5.4.7\" class=\"ltx_td ltx_align_center\">45.7</td>\n</tr>\n<tr id=\"S5.T16.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T16.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8</th>\n<td id=\"S5.T16.5.6.5.2\" class=\"ltx_td ltx_align_center\">31.0</td>\n<td id=\"S5.T16.5.6.5.3\" class=\"ltx_td ltx_align_center\">36.2</td>\n<td id=\"S5.T16.5.6.5.4\" class=\"ltx_td ltx_align_center\">46.7</td>\n<td id=\"S5.T16.5.6.5.5\" class=\"ltx_td ltx_align_center\">50.4</td>\n<td id=\"S5.T16.5.6.5.6\" class=\"ltx_td ltx_align_center\">54.9</td>\n<td id=\"S5.T16.5.6.5.7\" class=\"ltx_td ltx_align_center\">51.9</td>\n</tr>\n<tr id=\"S5.T16.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T16.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T16.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">29.1</td>\n<td id=\"S5.T16.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">37.6</td>\n<td id=\"S5.T16.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">47.5</td>\n<td id=\"S5.T16.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">52.3</td>\n<td id=\"S5.T16.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">59.9</td>\n<td id=\"S5.T16.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">49.2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we compare the financial tasks performance of the global model which is fine-tuned on the financial datasets. In our experiments, We assume that there are ",
                "5",
                "5",
                "5",
                " clients in participating the whole training process. We use the above-mentioned models as our initial models. All the local models and global models follow the same size and use the original model weights in the initial. We trained each local model under the same set of parameters, such as learning rate, decay strategy, and training epochs. We set the compression rank ",
                "r",
                "ğ‘Ÿ",
                "r",
                " to 512 and ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " value as 2 when we vary the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " as ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we vary the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                ". The origin row means that we train our model without differential private adaptations.",
                "The experimental data consistently demonstrate a clear trade-off between the privacy level (as adjusted by ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) and the model performance across various tasks. While all models show a decrease in performance with stricter privacy settings, the extent of this decrease varies, suggesting differences in how each model adapts to privacy constraints.",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and effectiveness.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments."
            ]
        ]
    },
    "S5.T17": {
        "caption": "Table 17. The performance of the global model under different Ïµitalic-Ïµ\\epsilon for ChatGLM-6B in general and financial tasks.. The left column denotes the maximum value of Ïµitalic-Ïµ\\epsilon.",
        "table": "<table id=\"S5.T17.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T17.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T17.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T17.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T17.5.1.1.m1.1a\"><mi id=\"S5.T17.5.1.1.m1.1.1\" xref=\"S5.T17.5.1.1.m1.1.1.cmml\">Ïµ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T17.5.1.1.m1.1b\"><ci id=\"S5.T17.5.1.1.m1.1.1.cmml\" xref=\"S5.T17.5.1.1.m1.1.1\">italic-Ïµ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T17.5.1.1.m1.1c\">\\epsilon</annotation></semantics></math></th>\n<th id=\"S5.T17.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T17.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T17.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T17.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FPB</th>\n<th id=\"S5.T17.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FiQA SA</th>\n<th id=\"S5.T17.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TFNS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T17.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T17.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T17.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.5</td>\n<td id=\"S5.T17.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">69.3</td>\n<td id=\"S5.T17.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">63.8</td>\n<td id=\"S5.T17.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">55.5</td>\n<td id=\"S5.T17.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">74.3</td>\n<td id=\"S5.T17.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">70.1</td>\n</tr>\n<tr id=\"S5.T17.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T17.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td id=\"S5.T17.5.3.2.2\" class=\"ltx_td ltx_align_center\">44.9</td>\n<td id=\"S5.T17.5.3.2.3\" class=\"ltx_td ltx_align_center\">58.3</td>\n<td id=\"S5.T17.5.3.2.4\" class=\"ltx_td ltx_align_center\">48.6</td>\n<td id=\"S5.T17.5.3.2.5\" class=\"ltx_td ltx_align_center\">29.9</td>\n<td id=\"S5.T17.5.3.2.6\" class=\"ltx_td ltx_align_center\">48.1</td>\n<td id=\"S5.T17.5.3.2.7\" class=\"ltx_td ltx_align_center\">42.5</td>\n</tr>\n<tr id=\"S5.T17.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T17.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td id=\"S5.T17.5.4.3.2\" class=\"ltx_td ltx_align_center\">43.2</td>\n<td id=\"S5.T17.5.4.3.3\" class=\"ltx_td ltx_align_center\">58.3</td>\n<td id=\"S5.T17.5.4.3.4\" class=\"ltx_td ltx_align_center\">48.3</td>\n<td id=\"S5.T17.5.4.3.5\" class=\"ltx_td ltx_align_center\">40.2</td>\n<td id=\"S5.T17.5.4.3.6\" class=\"ltx_td ltx_align_center\">57.9</td>\n<td id=\"S5.T17.5.4.3.7\" class=\"ltx_td ltx_align_center\">54.8</td>\n</tr>\n<tr id=\"S5.T17.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T17.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">6</th>\n<td id=\"S5.T17.5.5.4.2\" class=\"ltx_td ltx_align_center\">48.4</td>\n<td id=\"S5.T17.5.5.4.3\" class=\"ltx_td ltx_align_center\">59.1</td>\n<td id=\"S5.T17.5.5.4.4\" class=\"ltx_td ltx_align_center\">51.4</td>\n<td id=\"S5.T17.5.5.4.5\" class=\"ltx_td ltx_align_center\">43.5</td>\n<td id=\"S5.T17.5.5.4.6\" class=\"ltx_td ltx_align_center\">62.2</td>\n<td id=\"S5.T17.5.5.4.7\" class=\"ltx_td ltx_align_center\">56.3</td>\n</tr>\n<tr id=\"S5.T17.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T17.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8</th>\n<td id=\"S5.T17.5.6.5.2\" class=\"ltx_td ltx_align_center\">52.5</td>\n<td id=\"S5.T17.5.6.5.3\" class=\"ltx_td ltx_align_center\">60.9</td>\n<td id=\"S5.T17.5.6.5.4\" class=\"ltx_td ltx_align_center\">58.0</td>\n<td id=\"S5.T17.5.6.5.5\" class=\"ltx_td ltx_align_center\">45.6</td>\n<td id=\"S5.T17.5.6.5.6\" class=\"ltx_td ltx_align_center\">68.1</td>\n<td id=\"S5.T17.5.6.5.7\" class=\"ltx_td ltx_align_center\">63.2</td>\n</tr>\n<tr id=\"S5.T17.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T17.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T17.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">55.3</td>\n<td id=\"S5.T17.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">67.0</td>\n<td id=\"S5.T17.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">62.2</td>\n<td id=\"S5.T17.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">53.2</td>\n<td id=\"S5.T17.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">72.6</td>\n<td id=\"S5.T17.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">64.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we compare the financial tasks performance of the global model which is fine-tuned on the financial datasets. In our experiments, We assume that there are ",
                "5",
                "5",
                "5",
                " clients in participating the whole training process. We use the above-mentioned models as our initial models. All the local models and global models follow the same size and use the original model weights in the initial. We trained each local model under the same set of parameters, such as learning rate, decay strategy, and training epochs. We set the compression rank ",
                "r",
                "ğ‘Ÿ",
                "r",
                " to 512 and ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " value as 2 when we vary the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " as ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we vary the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                ". The origin row means that we train our model without differential private adaptations.",
                "The experimental data consistently demonstrate a clear trade-off between the privacy level (as adjusted by ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) and the model performance across various tasks. While all models show a decrease in performance with stricter privacy settings, the extent of this decrease varies, suggesting differences in how each model adapts to privacy constraints.",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and effectiveness.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments."
            ]
        ]
    },
    "S5.T18": {
        "caption": "Table 18. The performance of the global model under different Ïµitalic-Ïµ\\epsilon for Llama-7B in general and financial tasks. The left column denotes the maximum value of Ïµitalic-Ïµ\\epsilon.",
        "table": "<table id=\"S5.T18.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T18.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T18.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S5.T18.5.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\epsilon\" display=\"inline\"><semantics id=\"S5.T18.5.1.1.m1.1a\"><mi id=\"S5.T18.5.1.1.m1.1.1\" xref=\"S5.T18.5.1.1.m1.1.1.cmml\">Ïµ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T18.5.1.1.m1.1b\"><ci id=\"S5.T18.5.1.1.m1.1.1.cmml\" xref=\"S5.T18.5.1.1.m1.1.1\">italic-Ïµ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T18.5.1.1.m1.1c\">\\epsilon</annotation></semantics></math></th>\n<th id=\"S5.T18.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T18.5.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T18.5.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T18.5.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FPB</th>\n<th id=\"S5.T18.5.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FiQA SA</th>\n<th id=\"S5.T18.5.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">TFNS</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T18.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T18.5.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T18.5.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">76.3</td>\n<td id=\"S5.T18.5.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">79.7</td>\n<td id=\"S5.T18.5.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">70.0</td>\n<td id=\"S5.T18.5.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">59.1</td>\n<td id=\"S5.T18.5.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">80.0</td>\n<td id=\"S5.T18.5.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">69.6</td>\n</tr>\n<tr id=\"S5.T18.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T18.5.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">2</th>\n<td id=\"S5.T18.5.3.2.2\" class=\"ltx_td ltx_align_center\">59.6</td>\n<td id=\"S5.T18.5.3.2.3\" class=\"ltx_td ltx_align_center\">71.5</td>\n<td id=\"S5.T18.5.3.2.4\" class=\"ltx_td ltx_align_center\">61.6</td>\n<td id=\"S5.T18.5.3.2.5\" class=\"ltx_td ltx_align_center\">38.5</td>\n<td id=\"S5.T18.5.3.2.6\" class=\"ltx_td ltx_align_center\">48.3</td>\n<td id=\"S5.T18.5.3.2.7\" class=\"ltx_td ltx_align_center\">43.2</td>\n</tr>\n<tr id=\"S5.T18.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T18.5.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">4</th>\n<td id=\"S5.T18.5.4.3.2\" class=\"ltx_td ltx_align_center\">63.4</td>\n<td id=\"S5.T18.5.4.3.3\" class=\"ltx_td ltx_align_center\">69.3</td>\n<td id=\"S5.T18.5.4.3.4\" class=\"ltx_td ltx_align_center\">61.9</td>\n<td id=\"S5.T18.5.4.3.5\" class=\"ltx_td ltx_align_center\">43.1</td>\n<td id=\"S5.T18.5.4.3.6\" class=\"ltx_td ltx_align_center\">65.0</td>\n<td id=\"S5.T18.5.4.3.7\" class=\"ltx_td ltx_align_center\">65.4</td>\n</tr>\n<tr id=\"S5.T18.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T18.5.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">6</th>\n<td id=\"S5.T18.5.5.4.2\" class=\"ltx_td ltx_align_center\">71.9</td>\n<td id=\"S5.T18.5.5.4.3\" class=\"ltx_td ltx_align_center\">70.3</td>\n<td id=\"S5.T18.5.5.4.4\" class=\"ltx_td ltx_align_center\">59.1</td>\n<td id=\"S5.T18.5.5.4.5\" class=\"ltx_td ltx_align_center\">49.1</td>\n<td id=\"S5.T18.5.5.4.6\" class=\"ltx_td ltx_align_center\">76.1</td>\n<td id=\"S5.T18.5.5.4.7\" class=\"ltx_td ltx_align_center\">65.3</td>\n</tr>\n<tr id=\"S5.T18.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T18.5.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">8</th>\n<td id=\"S5.T18.5.6.5.2\" class=\"ltx_td ltx_align_center\">76.4</td>\n<td id=\"S5.T18.5.6.5.3\" class=\"ltx_td ltx_align_center\">80.4</td>\n<td id=\"S5.T18.5.6.5.4\" class=\"ltx_td ltx_align_center\">69.9</td>\n<td id=\"S5.T18.5.6.5.5\" class=\"ltx_td ltx_align_center\">68.5</td>\n<td id=\"S5.T18.5.6.5.6\" class=\"ltx_td ltx_align_center\">73.1</td>\n<td id=\"S5.T18.5.6.5.7\" class=\"ltx_td ltx_align_center\">77.2</td>\n</tr>\n<tr id=\"S5.T18.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T18.5.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">10</th>\n<td id=\"S5.T18.5.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">87.9</td>\n<td id=\"S5.T18.5.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">80.1</td>\n<td id=\"S5.T18.5.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">71.1</td>\n<td id=\"S5.T18.5.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">69.9</td>\n<td id=\"S5.T18.5.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">81.5</td>\n<td id=\"S5.T18.5.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_bb\">82.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we compare the financial tasks performance of the global model which is fine-tuned on the financial datasets. In our experiments, We assume that there are ",
                "5",
                "5",
                "5",
                " clients in participating the whole training process. We use the above-mentioned models as our initial models. All the local models and global models follow the same size and use the original model weights in the initial. We trained each local model under the same set of parameters, such as learning rate, decay strategy, and training epochs. We set the compression rank ",
                "r",
                "ğ‘Ÿ",
                "r",
                " to 512 and ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " value as 2 when we vary the ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " as ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "5",
                "1",
                "ğ‘’",
                "5",
                "1e-5",
                " when we vary the ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                ". The origin row means that we train our model without differential private adaptations.",
                "The experimental data consistently demonstrate a clear trade-off between the privacy level (as adjusted by ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values) and the model performance across various tasks. While all models show a decrease in performance with stricter privacy settings, the extent of this decrease varies, suggesting differences in how each model adapts to privacy constraints.",
                "One notable trend is the general decrease in performance across all models with stricter privacy settings (lower ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values). For instance, GPT-2â€™s performance on MedQuAD drops from 69.2 at the original setting to 55.3 and 49.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is reduced to 2 and ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "05",
                "1",
                "ğ‘’",
                "05",
                "1e-05",
                ", respectively (Tables ",
                "3",
                " and ",
                "7",
                "). This trend indicates a trade-off between privacy and effectiveness.",
                "In contrast, some models like Llama-7B show a more resilient performance under varying ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " values. For example, its performance on LiveQA only marginally decreases from 69.4 to 66.1 when ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                " is increased from the original to 10 (Table ",
                "6",
                "). This suggests that certain models might be better suited for privacy-sensitive applications.",
                "Additionally, the impact of changing ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " values appears to be more model-specific. Bertâ€™s performance on MEDIQA-Ans decreases significantly from 73.3 in the original setting to 57.4 when ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " is reduced to ",
                "1",
                "â€‹",
                "e",
                "âˆ’",
                "06",
                "1",
                "ğ‘’",
                "06",
                "1e-06",
                " (Table ",
                "8",
                "), highlighting a potentially higher sensitivity to ",
                "Î´",
                "ğ›¿",
                "\\delta",
                " adjustments."
            ]
        ]
    },
    "S5.T19": {
        "caption": "Table 19. We show the communication overhead of GPT-2 in federated learning. We use the model parameter amount to denote the communication efficiency and compression ratio as an improvement indicator for different sets of rank rğ‘Ÿr. The origin row means the results without our algorithm. For all the experiments, we train our local model under privacy parameter Ïµ=8italic-Ïµ8\\epsilon=8 and Î´=10â€‹eâˆ’5ğ›¿10superscriptğ‘’5\\delta=10e^{-5}. We also include the model performance on general tasks.",
        "table": "<table id=\"S5.T19.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T19.7.1\" class=\"ltx_tr\">\n<th id=\"S5.T19.7.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">rank <math id=\"S5.T19.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"r\" display=\"inline\"><semantics id=\"S5.T19.7.1.1.m1.1a\"><mi id=\"S5.T19.7.1.1.m1.1.1\" xref=\"S5.T19.7.1.1.m1.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T19.7.1.1.m1.1b\"><ci id=\"S5.T19.7.1.1.m1.1.1.cmml\" xref=\"S5.T19.7.1.1.m1.1.1\">ğ‘Ÿ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T19.7.1.1.m1.1c\">r</annotation></semantics></math>\n</th>\n<th id=\"S5.T19.7.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T19.7.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T19.7.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T19.7.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Communication overhead</th>\n<th id=\"S5.T19.7.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Reduction ratio</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T19.7.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T19.7.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T19.7.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">61.9</td>\n<td id=\"S5.T19.7.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">56.9</td>\n<td id=\"S5.T19.7.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">48.9</td>\n<td id=\"S5.T19.7.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">127.4M</td>\n<td id=\"S5.T19.7.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T19.7.2.1.6.1\" class=\"ltx_text ltx_font_bold\">-</span></td>\n</tr>\n<tr id=\"S5.T19.7.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T19.7.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">512</th>\n<td id=\"S5.T19.7.3.2.2\" class=\"ltx_td ltx_align_center\">39.4</td>\n<td id=\"S5.T19.7.3.2.3\" class=\"ltx_td ltx_align_center\">41.7</td>\n<td id=\"S5.T19.7.3.2.4\" class=\"ltx_td ltx_align_center\">45.5</td>\n<td id=\"S5.T19.7.3.2.5\" class=\"ltx_td ltx_align_center\">101.3M</td>\n<td id=\"S5.T19.7.3.2.6\" class=\"ltx_td ltx_align_center\">79.5%</td>\n</tr>\n<tr id=\"S5.T19.7.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T19.7.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">256</th>\n<td id=\"S5.T19.7.4.3.2\" class=\"ltx_td ltx_align_center\">38.2</td>\n<td id=\"S5.T19.7.4.3.3\" class=\"ltx_td ltx_align_center\">44.3</td>\n<td id=\"S5.T19.7.4.3.4\" class=\"ltx_td ltx_align_center\">43.2</td>\n<td id=\"S5.T19.7.4.3.5\" class=\"ltx_td ltx_align_center\">82.1M</td>\n<td id=\"S5.T19.7.4.3.6\" class=\"ltx_td ltx_align_center\">64.4%</td>\n</tr>\n<tr id=\"S5.T19.7.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T19.7.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">128</th>\n<td id=\"S5.T19.7.5.4.2\" class=\"ltx_td ltx_align_center\">32.1</td>\n<td id=\"S5.T19.7.5.4.3\" class=\"ltx_td ltx_align_center\">50.3</td>\n<td id=\"S5.T19.7.5.4.4\" class=\"ltx_td ltx_align_center\">45.9</td>\n<td id=\"S5.T19.7.5.4.5\" class=\"ltx_td ltx_align_center\">69.5M</td>\n<td id=\"S5.T19.7.5.4.6\" class=\"ltx_td ltx_align_center\">54.6%</td>\n</tr>\n<tr id=\"S5.T19.7.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T19.7.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">64</th>\n<td id=\"S5.T19.7.6.5.2\" class=\"ltx_td ltx_align_center\">32.4</td>\n<td id=\"S5.T19.7.6.5.3\" class=\"ltx_td ltx_align_center\">49.1</td>\n<td id=\"S5.T19.7.6.5.4\" class=\"ltx_td ltx_align_center\">39.9</td>\n<td id=\"S5.T19.7.6.5.5\" class=\"ltx_td ltx_align_center\">58.6M</td>\n<td id=\"S5.T19.7.6.5.6\" class=\"ltx_td ltx_align_center\">46.0%</td>\n</tr>\n<tr id=\"S5.T19.7.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T19.7.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">32</th>\n<td id=\"S5.T19.7.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">33.5</td>\n<td id=\"S5.T19.7.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">46.3</td>\n<td id=\"S5.T19.7.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">39.4</td>\n<td id=\"S5.T19.7.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">58.0M</td>\n<td id=\"S5.T19.7.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">45.5%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "GPT-2 (Table 19) showed a performance drop in BoolQ from 61.9 to 33.5, PIQA from 56.9 to 46.3, and WinoGrande from 48.9 to 39.4 as the rank reduced from origin to 32, alongside an increase in compression ratio from 45.5 to 79.5. BERT (Table 20) exhibited a similar performance decline, for example, in WinoGrande from 60.2 to 37.8, with a compression improvement from 42.0 to 75.5. Llama-7B (Table 21), a larger model, maintained a relatively stable performance (e.g., BoolQ at 76.3 to 39.1, WinoGrande 70.0 to 67.8 ) despite significant parameter reduction (compression ratio from 7.31 to 36.27). In contrast, ChatGLM-6B (Table 22) showed a more pronounced performance degradation (BoolQ dropping from 69.5 to 50.1, WinoGrande from 71.3 to 46.3) with a notable decrease in parameters (compression ratio moving from 7.6 to 47.4). These results highlight the variability in how different models respond to parameter reduction and compression, underscoring the challenge of balancing communication efficiency with performance in federated learning environments."
        ]
    },
    "S5.T20": {
        "caption": "Table 20. We show the communication overhead of Bert in federated learning. We use the model parameter amount to denote the communication efficiency and compression ratio as an improvement indicator for different sets of rank rğ‘Ÿr. The origin row means the results without our algorithm. For all the experiments, we train our local model under privacy parameter Ïµ=8italic-Ïµ8\\epsilon=8 and Î´=10â€‹eâˆ’5ğ›¿10superscriptğ‘’5\\delta=10e^{-5}. We also include the model performance on general tasks.",
        "table": "<table id=\"S5.T20.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T20.7.1\" class=\"ltx_tr\">\n<th id=\"S5.T20.7.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">rank <math id=\"S5.T20.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"r\" display=\"inline\"><semantics id=\"S5.T20.7.1.1.m1.1a\"><mi id=\"S5.T20.7.1.1.m1.1.1\" xref=\"S5.T20.7.1.1.m1.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T20.7.1.1.m1.1b\"><ci id=\"S5.T20.7.1.1.m1.1.1.cmml\" xref=\"S5.T20.7.1.1.m1.1.1\">ğ‘Ÿ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T20.7.1.1.m1.1c\">r</annotation></semantics></math>\n</th>\n<th id=\"S5.T20.7.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T20.7.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T20.7.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T20.7.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Communication overhead</th>\n<th id=\"S5.T20.7.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Reduction ratio</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T20.7.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T20.7.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T20.7.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">49.8</td>\n<td id=\"S5.T20.7.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">55.5</td>\n<td id=\"S5.T20.7.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">60.2</td>\n<td id=\"S5.T20.7.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">110.8M</td>\n<td id=\"S5.T20.7.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T20.7.2.1.6.1\" class=\"ltx_text ltx_font_bold\">-</span></td>\n</tr>\n<tr id=\"S5.T20.7.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T20.7.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">512</th>\n<td id=\"S5.T20.7.3.2.2\" class=\"ltx_td ltx_align_center\">35.5</td>\n<td id=\"S5.T20.7.3.2.3\" class=\"ltx_td ltx_align_center\">44.7</td>\n<td id=\"S5.T20.7.3.2.4\" class=\"ltx_td ltx_align_center\">41.3</td>\n<td id=\"S5.T20.7.3.2.5\" class=\"ltx_td ltx_align_center\">83.7M</td>\n<td id=\"S5.T20.7.3.2.6\" class=\"ltx_td ltx_align_center\">75.5%</td>\n</tr>\n<tr id=\"S5.T20.7.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T20.7.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">256</th>\n<td id=\"S5.T20.7.4.3.2\" class=\"ltx_td ltx_align_center\">34.2</td>\n<td id=\"S5.T20.7.4.3.3\" class=\"ltx_td ltx_align_center\">47.3</td>\n<td id=\"S5.T20.7.4.3.4\" class=\"ltx_td ltx_align_center\">40.3</td>\n<td id=\"S5.T20.7.4.3.5\" class=\"ltx_td ltx_align_center\">69.9M</td>\n<td id=\"S5.T20.7.4.3.6\" class=\"ltx_td ltx_align_center\">63.1%</td>\n</tr>\n<tr id=\"S5.T20.7.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T20.7.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">128</th>\n<td id=\"S5.T20.7.5.4.2\" class=\"ltx_td ltx_align_center\">35.1</td>\n<td id=\"S5.T20.7.5.4.3\" class=\"ltx_td ltx_align_center\">46.2</td>\n<td id=\"S5.T20.7.5.4.4\" class=\"ltx_td ltx_align_center\">43.8</td>\n<td id=\"S5.T20.7.5.4.5\" class=\"ltx_td ltx_align_center\">54.5M</td>\n<td id=\"S5.T20.7.5.4.6\" class=\"ltx_td ltx_align_center\">49.2%</td>\n</tr>\n<tr id=\"S5.T20.7.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T20.7.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">64</th>\n<td id=\"S5.T20.7.6.5.2\" class=\"ltx_td ltx_align_center\">33.2</td>\n<td id=\"S5.T20.7.6.5.3\" class=\"ltx_td ltx_align_center\">39.1</td>\n<td id=\"S5.T20.7.6.5.4\" class=\"ltx_td ltx_align_center\">38.9</td>\n<td id=\"S5.T20.7.6.5.5\" class=\"ltx_td ltx_align_center\">48.1M</td>\n<td id=\"S5.T20.7.6.5.6\" class=\"ltx_td ltx_align_center\">43.4%</td>\n</tr>\n<tr id=\"S5.T20.7.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T20.7.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">32</th>\n<td id=\"S5.T20.7.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">33.7</td>\n<td id=\"S5.T20.7.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">37.0</td>\n<td id=\"S5.T20.7.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">37.8</td>\n<td id=\"S5.T20.7.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">46.6M</td>\n<td id=\"S5.T20.7.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">42.0%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "GPT-2 (Table 19) showed a performance drop in BoolQ from 61.9 to 33.5, PIQA from 56.9 to 46.3, and WinoGrande from 48.9 to 39.4 as the rank reduced from origin to 32, alongside an increase in compression ratio from 45.5 to 79.5. BERT (Table 20) exhibited a similar performance decline, for example, in WinoGrande from 60.2 to 37.8, with a compression improvement from 42.0 to 75.5. Llama-7B (Table 21), a larger model, maintained a relatively stable performance (e.g., BoolQ at 76.3 to 39.1, WinoGrande 70.0 to 67.8 ) despite significant parameter reduction (compression ratio from 7.31 to 36.27). In contrast, ChatGLM-6B (Table 22) showed a more pronounced performance degradation (BoolQ dropping from 69.5 to 50.1, WinoGrande from 71.3 to 46.3) with a notable decrease in parameters (compression ratio moving from 7.6 to 47.4). These results highlight the variability in how different models respond to parameter reduction and compression, underscoring the challenge of balancing communication efficiency with performance in federated learning environments."
        ]
    },
    "S5.T21": {
        "caption": "Table 21. We show the communication overhead of Llama-7B in federated learning. We use the model parameter amount to denote the communication efficiency and compression ratio as an improvement indicator for different sets of rank rğ‘Ÿr. The origin row means the results without our algorithm. For all the experiments, we train our local model under privacy parameter Ïµ=8italic-Ïµ8\\epsilon=8 and Î´=10â€‹eâˆ’5ğ›¿10superscriptğ‘’5\\delta=10e^{-5}. We also include the model performance on general tasks.",
        "table": "<table id=\"S5.T21.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T21.7.1\" class=\"ltx_tr\">\n<th id=\"S5.T21.7.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">rank <math id=\"S5.T21.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"r\" display=\"inline\"><semantics id=\"S5.T21.7.1.1.m1.1a\"><mi id=\"S5.T21.7.1.1.m1.1.1\" xref=\"S5.T21.7.1.1.m1.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T21.7.1.1.m1.1b\"><ci id=\"S5.T21.7.1.1.m1.1.1.cmml\" xref=\"S5.T21.7.1.1.m1.1.1\">ğ‘Ÿ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T21.7.1.1.m1.1c\">r</annotation></semantics></math>\n</th>\n<th id=\"S5.T21.7.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T21.7.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T21.7.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T21.7.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Communication overhead</th>\n<th id=\"S5.T21.7.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Reduction ratio</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T21.7.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T21.7.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T21.7.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">76.3</td>\n<td id=\"S5.T21.7.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">79.7</td>\n<td id=\"S5.T21.7.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">70.0</td>\n<td id=\"S5.T21.7.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">6.7B</td>\n<td id=\"S5.T21.7.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T21.7.2.1.6.1\" class=\"ltx_text ltx_font_bold\">-</span></td>\n</tr>\n<tr id=\"S5.T21.7.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T21.7.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1024</th>\n<td id=\"S5.T21.7.3.2.2\" class=\"ltx_td ltx_align_center\">78.8</td>\n<td id=\"S5.T21.7.3.2.3\" class=\"ltx_td ltx_align_center\">83.1</td>\n<td id=\"S5.T21.7.3.2.4\" class=\"ltx_td ltx_align_center\">71.1</td>\n<td id=\"S5.T21.7.3.2.5\" class=\"ltx_td ltx_align_center\">2.43B</td>\n<td id=\"S5.T21.7.3.2.6\" class=\"ltx_td ltx_align_center\">36.27%</td>\n</tr>\n<tr id=\"S5.T21.7.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T21.7.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">512</th>\n<td id=\"S5.T21.7.4.3.2\" class=\"ltx_td ltx_align_center\">76.4</td>\n<td id=\"S5.T21.7.4.3.3\" class=\"ltx_td ltx_align_center\">80.4</td>\n<td id=\"S5.T21.7.4.3.4\" class=\"ltx_td ltx_align_center\">69.9</td>\n<td id=\"S5.T21.7.4.3.5\" class=\"ltx_td ltx_align_center\">1.35B</td>\n<td id=\"S5.T21.7.4.3.6\" class=\"ltx_td ltx_align_center\">20.15%</td>\n</tr>\n<tr id=\"S5.T21.7.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T21.7.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">256</th>\n<td id=\"S5.T21.7.5.4.2\" class=\"ltx_td ltx_align_center\">70.6</td>\n<td id=\"S5.T21.7.5.4.3\" class=\"ltx_td ltx_align_center\">74.1</td>\n<td id=\"S5.T21.7.5.4.4\" class=\"ltx_td ltx_align_center\">68.3</td>\n<td id=\"S5.T21.7.5.4.5\" class=\"ltx_td ltx_align_center\">0.93B</td>\n<td id=\"S5.T21.7.5.4.6\" class=\"ltx_td ltx_align_center\">13.88%</td>\n</tr>\n<tr id=\"S5.T21.7.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T21.7.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">128</th>\n<td id=\"S5.T21.7.6.5.2\" class=\"ltx_td ltx_align_center\">50.3</td>\n<td id=\"S5.T21.7.6.5.3\" class=\"ltx_td ltx_align_center\">78.5</td>\n<td id=\"S5.T21.7.6.5.4\" class=\"ltx_td ltx_align_center\">69.7</td>\n<td id=\"S5.T21.7.6.5.5\" class=\"ltx_td ltx_align_center\">0.65B</td>\n<td id=\"S5.T21.7.6.5.6\" class=\"ltx_td ltx_align_center\">9.70%</td>\n</tr>\n<tr id=\"S5.T21.7.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T21.7.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">64</th>\n<td id=\"S5.T21.7.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">39.1</td>\n<td id=\"S5.T21.7.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">68.5</td>\n<td id=\"S5.T21.7.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">67.8</td>\n<td id=\"S5.T21.7.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.49B</td>\n<td id=\"S5.T21.7.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">7.31%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "GPT-2 (Table 19) showed a performance drop in BoolQ from 61.9 to 33.5, PIQA from 56.9 to 46.3, and WinoGrande from 48.9 to 39.4 as the rank reduced from origin to 32, alongside an increase in compression ratio from 45.5 to 79.5. BERT (Table 20) exhibited a similar performance decline, for example, in WinoGrande from 60.2 to 37.8, with a compression improvement from 42.0 to 75.5. Llama-7B (Table 21), a larger model, maintained a relatively stable performance (e.g., BoolQ at 76.3 to 39.1, WinoGrande 70.0 to 67.8 ) despite significant parameter reduction (compression ratio from 7.31 to 36.27). In contrast, ChatGLM-6B (Table 22) showed a more pronounced performance degradation (BoolQ dropping from 69.5 to 50.1, WinoGrande from 71.3 to 46.3) with a notable decrease in parameters (compression ratio moving from 7.6 to 47.4). These results highlight the variability in how different models respond to parameter reduction and compression, underscoring the challenge of balancing communication efficiency with performance in federated learning environments."
        ]
    },
    "S5.T22": {
        "caption": "Table 22. We show the communication overhead of ChatGLM-6B in federated learning. We use the model parameter amount to denote the communication efficiency and compression ratio as an improvement indicator for different sets of rank rğ‘Ÿr. The origin row means the results without our algorithm. For all the experiments, we train our local model under privacy parameter Ïµ=8italic-Ïµ8\\epsilon=8 and Î´=10â€‹eâˆ’5ğ›¿10superscriptğ‘’5\\delta=10e^{-5}. We also include the model performance on general tasks.",
        "table": "<table id=\"S5.T22.7\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T22.7.1\" class=\"ltx_tr\">\n<th id=\"S5.T22.7.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">rank <math id=\"S5.T22.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"r\" display=\"inline\"><semantics id=\"S5.T22.7.1.1.m1.1a\"><mi id=\"S5.T22.7.1.1.m1.1.1\" xref=\"S5.T22.7.1.1.m1.1.1.cmml\">r</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T22.7.1.1.m1.1b\"><ci id=\"S5.T22.7.1.1.m1.1.1.cmml\" xref=\"S5.T22.7.1.1.m1.1.1\">ğ‘Ÿ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T22.7.1.1.m1.1c\">r</annotation></semantics></math>\n</th>\n<th id=\"S5.T22.7.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">BoolQ</th>\n<th id=\"S5.T22.7.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">PIQA</th>\n<th id=\"S5.T22.7.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">WinoGrande</th>\n<th id=\"S5.T22.7.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Communication overhead</th>\n<th id=\"S5.T22.7.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Reduction ratio</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T22.7.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T22.7.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">origin</th>\n<td id=\"S5.T22.7.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">69.5</td>\n<td id=\"S5.T22.7.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">76.3</td>\n<td id=\"S5.T22.7.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">71.3</td>\n<td id=\"S5.T22.7.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">6.2B</td>\n<td id=\"S5.T22.7.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T22.7.2.1.6.1\" class=\"ltx_text ltx_font_bold\">-</span></td>\n</tr>\n<tr id=\"S5.T22.7.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T22.7.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">1024</th>\n<td id=\"S5.T22.7.3.2.2\" class=\"ltx_td ltx_align_center\">57.3</td>\n<td id=\"S5.T22.7.3.2.3\" class=\"ltx_td ltx_align_center\">65.3</td>\n<td id=\"S5.T22.7.3.2.4\" class=\"ltx_td ltx_align_center\">55.5</td>\n<td id=\"S5.T22.7.3.2.5\" class=\"ltx_td ltx_align_center\">2.94B</td>\n<td id=\"S5.T22.7.3.2.6\" class=\"ltx_td ltx_align_center\">47.4%</td>\n</tr>\n<tr id=\"S5.T22.7.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T22.7.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">512</th>\n<td id=\"S5.T22.7.4.3.2\" class=\"ltx_td ltx_align_center\">55.7</td>\n<td id=\"S5.T22.7.4.3.3\" class=\"ltx_td ltx_align_center\">59.7</td>\n<td id=\"S5.T22.7.4.3.4\" class=\"ltx_td ltx_align_center\">56.5</td>\n<td id=\"S5.T22.7.4.3.5\" class=\"ltx_td ltx_align_center\">1.76B</td>\n<td id=\"S5.T22.7.4.3.6\" class=\"ltx_td ltx_align_center\">28.4%</td>\n</tr>\n<tr id=\"S5.T22.7.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T22.7.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">256</th>\n<td id=\"S5.T22.7.5.4.2\" class=\"ltx_td ltx_align_center\">47.6</td>\n<td id=\"S5.T22.7.5.4.3\" class=\"ltx_td ltx_align_center\">55.4</td>\n<td id=\"S5.T22.7.5.4.4\" class=\"ltx_td ltx_align_center\">48.8</td>\n<td id=\"S5.T22.7.5.4.5\" class=\"ltx_td ltx_align_center\">0.83B</td>\n<td id=\"S5.T22.7.5.4.6\" class=\"ltx_td ltx_align_center\">13.4%</td>\n</tr>\n<tr id=\"S5.T22.7.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T22.7.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">128</th>\n<td id=\"S5.T22.7.6.5.2\" class=\"ltx_td ltx_align_center\">51.3</td>\n<td id=\"S5.T22.7.6.5.3\" class=\"ltx_td ltx_align_center\">56.7</td>\n<td id=\"S5.T22.7.6.5.4\" class=\"ltx_td ltx_align_center\">44.1</td>\n<td id=\"S5.T22.7.6.5.5\" class=\"ltx_td ltx_align_center\">0.52B</td>\n<td id=\"S5.T22.7.6.5.6\" class=\"ltx_td ltx_align_center\">9.3%</td>\n</tr>\n<tr id=\"S5.T22.7.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T22.7.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">64</th>\n<td id=\"S5.T22.7.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">50.1</td>\n<td id=\"S5.T22.7.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">49.9</td>\n<td id=\"S5.T22.7.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">46.3</td>\n<td id=\"S5.T22.7.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">0.47B</td>\n<td id=\"S5.T22.7.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">7.6%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "GPT-2 (Table 19) showed a performance drop in BoolQ from 61.9 to 33.5, PIQA from 56.9 to 46.3, and WinoGrande from 48.9 to 39.4 as the rank reduced from origin to 32, alongside an increase in compression ratio from 45.5 to 79.5. BERT (Table 20) exhibited a similar performance decline, for example, in WinoGrande from 60.2 to 37.8, with a compression improvement from 42.0 to 75.5. Llama-7B (Table 21), a larger model, maintained a relatively stable performance (e.g., BoolQ at 76.3 to 39.1, WinoGrande 70.0 to 67.8 ) despite significant parameter reduction (compression ratio from 7.31 to 36.27). In contrast, ChatGLM-6B (Table 22) showed a more pronounced performance degradation (BoolQ dropping from 69.5 to 50.1, WinoGrande from 71.3 to 46.3) with a notable decrease in parameters (compression ratio moving from 7.6 to 47.4). These results highlight the variability in how different models respond to parameter reduction and compression, underscoring the challenge of balancing communication efficiency with performance in federated learning environments."
        ]
    }
}