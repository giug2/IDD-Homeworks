{
    "PAPER'S NUMBER OF TABLES": 5,
    "S4.T1": {
        "caption": "Table 1: Quality degradation with non-IID training.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nID\nExp.\nWER\n\nTest\nTestOther\nDev\nDevOther\n\nE0\nBaseline\n4.8\n12.1\n5.1\n12.1\n\nE1\nNon-IID\n6.8\n17.2\n7.0\n17.3\n\nE0 v. E1 % Rel. WER\n+42%\n+42%\n+37%\n+43%\n\n\n",
        "references": [
            "We conducted a series of experiments to recover quality degradation due to non-IID training, and compared them to a centrally trained IID Baseline. The Baseline configuration was trained with a linear ramp-up learning rate schedule, SpecAugment [21], and Variational Noise [22]. Baseline results (E0) can be found in Table 1.",
            "Table 1 shows the Baseline performance and the initial non-IID config, with a substantial WER degradation across all evaluation sets."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Impact of data-limiting on non-IID training.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nID\nData Limit\nWER\n\nTest\nTestOther\nDev\nDevOther\n\nE1\nNone\n6.8\n17.2\n7.0\n17.3\n\nE2\n32\n6.2\n14.8\n6.5\n15.3\n\nE3\n64\n6.5\n15.1\n6.8\n15.5\n\nE4\n128\n7.1\n16.4\n7.1\n16.5\n\nE0 v. E2 % Rel. WER\n+29%\n+22%\n+27%\n+26%\n\n\n",
        "references": [
            "Table 2 illustrates the quality improvement due to data-limiting, bringing WER degradation from over 40% to less than 30% relative across all sets.",
            "Results in Table 2 show that model quality degrades as the per-client data volume is increased. Client models drift in varying directions, causing server updates to be sub-optimal. However, experiments E7 and E8 in Table 4 show that, with the addition of FVN, there is minimal change in model quality even without per-client data limits. This adds evidence to our claim that FVN prevents client drift."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Impact of FVN on non-IID training.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nID\nFVN Std Dev\nWER\n\nTest\nTestOther\nDev\nDevOther\n\nE2\n-\n6.2\n14.8\n6.5\n15.3\n\n\n\nE5\n0.01\n5.1\n12.6\n5.5\n12.4\n\nE6\n0.02\n5.0\n12.2\n5.2\n12.4\n\nE7\nRamp to 0.03\n4.6\n11.9\n5.0\n11.9\n\nE0 v. E7 % Rel. WER\n-4%\n-2%\n-2%\n-2%\n\n\n",
        "references": [
            "The Baseline used Variational Noise [22] (VN), applied by adding Gaussian noise to model parameters during each optimization step. A modification had to be made to VN in order to accommodate the two-step optimization that exists in FL: allowing each client to add its own random noise tensors during local optimization. We called this Federated Variational Noise (FVN), and found it was critical to recuperating the non-IID quality degradation. Table 3 shows experiments E5 and E6, which introduced FVN in a similar manner to the Baseline. We exceeded Baseline model quality by further improving the application of FVN in E7, wherein we increased the standard deviation of Gaussian noise linearly during training."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Impact of data-limiting on FVN experiments.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nID\nData Limit\nWER\n\nTest\nTestOther\nDev\nDevOther\n\nE7\n32\n4.6\n11.9\n5.0\n11.9\n\nE8\n-\n4.6\n11.9\n5.1\n11.8\n\n\n",
        "references": [
            "Results in Table 2 show that model quality degrades as the per-client data volume is increased. Client models drift in varying directions, causing server updates to be sub-optimal. However, experiments E7 and E8 in Table 4 show that, with the addition of FVN, there is minimal change in model quality even without per-client data limits. This adds evidence to our claim that FVN prevents client drift."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Exceeding Baseline quality with lower CFMQ.",
        "table": "",
        "footnotes": "\n\n\n\n\n\nID\nCFMQ [TB]\nWER\n\nTest\nTestOther\nDev\nDevOther\n\nE0\n3077\n4.8\n12.1\n5.1\n12.1\n\nE9\n2779\n4.8\n11.4\n4.6\n11.5\n\nE10\n2945\n4.8\n11.4\n4.6\n11.4\n\n\n",
        "references": [
            "Experiments thus far have recuperated the quality loss due to non-IID training data, but incurred an increase in cost. New experiments, aimed at reducing cost, were conducted by varying the number of local epochs, server learning rate schedule, and amount of SpecAugment. Table 5 shows the two most promising experiments, E9 and E10, which had a lower CFMQ and better quality in comparison to the Baseline. They both modified the learning rate schedule to have a shorter ramp-up and introduced an exponential decay. E10 increased the amount of SpecAugment during the training procedure and yielded slightly better quality. Therefore, we were able to recuperate the quality degradation from non-IID training data at a lower computation cost than the IID Baseline in this study. We must note that in order to limit scope we did not re-visit and refine the Baseline in this work."
        ]
    }
}