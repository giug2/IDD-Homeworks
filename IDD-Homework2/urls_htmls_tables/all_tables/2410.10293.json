{
    "id_table_1": {
        "caption": "Table 1:  Key feature comparison between the prevailing flat retrieval and the proposed progressive retrieval.",
        "table": "S1.T1.3",
        "footnotes": [],
        "references": [
            "Retrieval-Augmented Generation (RAG) has been shown highly effective in enhancing Large Language Models (LLMs)  Gao et al. ( 2023 ); Shi et al. ( 2023 )  and has been widely adopted in the industry, such as Microsofts GraphRAG  Edge et al. ( 2024 ) , Googles REALM  Guu et al. ( 2020 ) , and Metas RA-DIT  Lin et al. ( 2024 ) . Its effectiveness mainly comes from retrieving external non-parametric knowledge into LLMs to remedy their incomplete, incorrect, or outdated internal parametric knowledge  Karpukhin et al. ( 2020 ); Min et al. ( 2019 ) .  The de facto RAG framework usually segments documents into short retrieval units, such as 100-word passages  Jiang et al. ( 2024 ) , resulting in a massive corpus with tens of millions of candidate units. Then, the retriever is tasked to find the needle ( i.e.,  the golden retrieval units) from the haystack ( i.e.,  the enormous candidate corpus)  Lee et al. ( 2024 ); Kamradt ( 2023 ) . Finally, the retrieved units serve as the input context to the generator to facilitate generation.  Its working flow is shown in Figure  1 (a). Wikipedia dump is used as the non-parametric knowledge source  Lewis et al. ( 2020 ) , where each document is segmented into disjoint 100-word chunks, resulting in a total of 21M short passages. Then, the retriever seeks through a vast number of 21M candidates to get several potentially valuable passages, such as four.",
            "Constant Granularity.  Some RAG frameworks draw inspiration from recommender systems  Wang et al. ( 2020 ); Covington et al. ( 2016 ); Grbovic and Cheng ( 2018 ) , following a multi-stage cascade architecture, where candidates are extracted through  retrieval  and  reranking  as shown in Figure  1 (a). However, these practices directly introduce recommender systems experience into RAG and neglect the characteristics of RAG. Each entry in recommender systems is usually inseparable, while entries in RAG are usually separable. Unfortunately, existing RAG methods commonly treat entries as constant-grained retrieval units, ultimately restricting  performance.",
            "The above issues inflict a heavy burden on one retriever and neglect the subdivisible characteristic of entries in RAG. In this work, we propose a progressive retrieval paradigm with coarse-to-fine granularity for RAG, termed  FunnelRAG .  Specifically, it progressively reduces the scale of candidate entries, refines the granularity of retrieval units, and increases the level of retrievers capacity, whose working flow is shown in Figure  1 (b). Particularly, there are two important designs in  FunnelRAG :",
            "These two novel designs jointly contribute to the considerable improvement of retrieval accuracy and efficiency in open-domain question answering (QA), such as Natural Question (NQ)  Kwiatkowski et al. ( 2019 )  and Trivia QA (TQA)  Joshi et al. ( 2017 ) .  Table  1  shows the key differences between the progressive retrieval and the flat one.   FunnelRAG  features  (1)  Coarse-to-fine granularity which balances load and accuracy;  (2)  Later chunking which perceives the contextual information of retrieval units well 2 2 2 Flat retrieval reranks passages that are not contextual integrity. However, our progressive retrieval perceives contextual information well in the post-ranking stage, since these passages are derived from documents with contextual integrity. ;   (3)  Compressed corpus (30x smaller from 21M to 600K) which reduces the burden on the retrieval stage; and  (4)  Long retrieval unit which improves answer recall to the full extent. More details can be found in  3 .",
            "As stated in Section  1 , we emphasize the significance of (i) reducing the scale of candidates, (ii) refining the granularity of units, and (iii) increasing the level of retrievers capacity, throughout the retrieval paradigm. Here, we conduct empirical studies to verify claims (ii) and (iii), as it is obvious that claim (i) will lead to better retrieval accuracy 4 4 4 Assuming a fixed number of golden units that exists in candidates, increasing the scale of candidates reduces the signal-to-noise ratio, inevitable degrading retrieval accuracy.   Sawarkar et al. ( 2024 ) . We aim to evaluate whether these two claims can contribute to better retrieval performance, so as to validate that the progressive retrieval paradigm can balance effectiveness and efficiency well.  To empirically verify these claims, we construct two synthetic datasets for NQ and TQA. Specifically, for each query in NQ and TQA, we retrieve the top-10 relevant clustered documents from the coarse-grained corpus 5 5 5 Referring to Section  3.1.1  and Algorithm  1  for more technical details about how the clustered  documents  formulated.  using BM25  Robertson and Zaragoza ( 2009 ) , where each one has approximate 4K tokens. Then, we segment each clustered document into document-level units with about 1K tokens, striking the coarse-fine contrast. On the other hand, we adopt bge-reranker-v2-m3  Chen et al. ( 2024 )  and BM25 to make a contrast between high- and low-capacity retrievers:",
            "The overall framework of  FunnelRAG  is shown in Figure  4 . Then, we introduce three progressive retrieval stages from coarse to fine ( 3.1 ) and then describe how to distill aggregated signals from the succeeding retriever  to the  preceding  one  ( 3.2 ).",
            "The clustering algorithm is presented in Appendix  D  in Algorithm  1 , where we draw inspiration from  Jiang et al. ( 2024 )  but make several major modifications.  Specially, we sort documents by their local cluster coefficient from high to low in Line 1, so that documents with highly relevant will be clustered first.  Furthermore, we measure the closeness centrality between document  d d d italic_d  and its related clusters  R R \\mathcal{R} caligraphic_R  and sort them from high to low in Line 10, so that the most related cluster will be merged first. Each cluster  c c c italic_c  in  C C \\mathcal{C} caligraphic_C  is a list of documents related to each other.  We set the max cluster size  S S S italic_S  as 4K tokens because the experimental results ( 4.4 ) demonstrate that overlength retrieval units do not bring much benefits to answer  recall.",
            "where  H q , p f = Enc  ( q  p f ) subscript H q superscript p f Enc direct-sum q superscript p f \\mathbf{H}_{q,p^{f}}=\\text{Enc}(q\\oplus p^{f}) bold_H start_POSTSUBSCRIPT italic_q , italic_p start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT end_POSTSUBSCRIPT = Enc ( italic_q  italic_p start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT ) ;   direct-sum \\oplus   and    \\|   are the concatenation operator. By independently processing each ( q q q italic_q ,  p f superscript p f p^{f} italic_p start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT ) pair, the encoder computes self-attention within one passage at a time.  As such, the computational cost scales linearly with the number of passages, making the post-ranking stage highly efficient.  Furthermore, the decoders cross-attention scores have shown to be remarkably effective in estimating the relative importance of the retrieval-augmented passage from the language models preferences   Izacard and Grave ( 2021a ); Yu et al. ( 2023 ); Izacard et al. ( 2023 ); Shi et al. ( 2024 ) .  Given that, we average FiD cross-attention scores corresponding to each ( q q q italic_q ,  p f superscript p f p^{f} italic_p start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT ) pair from the first decoder token 7 7 7 Here, we take the score of the first token, as it leads to satisfactory performance in general.  More analysis about attention aggregation schema can be found in Appendix  C.1 .  over all the layers, all the heads, as well as all the representative tokens within  q  p f direct-sum q superscript p f q\\oplus p^{f} italic_q  italic_p start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT :",
            "where  l  n l n {ln} italic_l italic_n ,  l  h l h {lh} italic_l italic_h , and  l  r l r {lr} italic_l italic_r  denote the number of the layers, heads, and representative tokens, respectively;   0 , i , j , r  [ k ] subscript  0 i j r delimited-[] k \\alpha_{0,i,j,\\mathbf{r}[k]} italic_ start_POSTSUBSCRIPT 0 , italic_i , italic_j , bold_r [ italic_k ] end_POSTSUBSCRIPT  represents the score of the first decoder token in terms of the  i i i italic_i -th layer,  j j j italic_j -th head, and  k k k italic_k -th representative token;   r  [ k ] r delimited-[] k \\mathbf{r}[k] bold_r [ italic_k ]  lookups the index of  k k k italic_k -th representative token. We select  l  r l r {lr} italic_l italic_r  tokens with the highest scores to serve as representative ones 8 8 8 Note that we do not consider query tokens in  q q q italic_q  when selecting representative tokens, refer to the ablation study  in Appendix  C.2  for more details. More technical details about representative token selection can be found in Appendix  A.2 . .  The cross-attention mechanism is provided in Appendix  A.1 . Before estimating the relative importance score, we train FiD to predict the answer with retrieval-augmented passages, so that FiD can learn to look for cues. After that, we obtain the post-ranking score and treat these passages with scores ranked in the top- H H H italic_H  as oracle passages  P f = { p 1 f , p 2 f , ... , p H f } subscript P f superscript subscript p 1 f superscript subscript p 2 f ... superscript subscript p H f \\mathcal{P}_{f}=\\{p_{1}^{f},p_{2}^{f},...,p_{H}^{f}\\} caligraphic_P start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT , ... , italic_p start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT } . The oracle passages  P f subscript P f \\mathcal{P}_{f} caligraphic_P start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  will be fed into generators to  facilitate  generation.",
            "Datasets.    We experiment on two QA datasets:  i.e.,  Natural Question (NQ)  Kwiatkowski et al. ( 2019 )  and Trivia QA (TQA)  Joshi et al. ( 2017 ) . Appendix  B.1  provides the statistics of the datasets.",
            "Table  2 ,  10 ,  11 , and  12  shows the retrieval results on NQ and TQA datasets.  As BM25s retrieval speed is breakneck, we mark its time cost as 0.00. From the results, we mainly have the following observations:   (1)  By adjusting the collaboration between large-to-small quantities, coarse-to-fine granularity, and simple-to-complex retrievers, progressive retrieval can find a gain-cost balance point.  Specifically, the time cost of progressive retrieval is considerably lower than that of the flat one (reduced by about 40%), while the AR is comparable to (or even better than) the flat one.  The main reason for this results is that progressive retrieval combines the advantage of different retrievers but circumvents their disadvantage 11 11 11 Progressive retrieval combines simple retrievers fast speed with complex ones high precision, while it avoids simple retrievers low precision and  complex  ones slow  speed. .   (2)  By retrieving in a funnel manner ( e.g.,  600K  Retrieval Retrieval  \\xrightarrow[]{\\text{Retrieval}} start_ARROW overRetrieval  end_ARROW 80c  Pre-ranking Pre-ranking  \\xrightarrow[]{\\text{Pre-ranking}} start_ARROW overPre-ranking  end_ARROW 8d  Post-ranking Post-ranking  \\xrightarrow[]{\\text{Post-ranking}} start_ARROW overPost-ranking  end_ARROW 4p),  progressive retrieval enables load balancing. It assigns simple but computationally demanding tasks to low-capacity retrievers,  e.g.,  retrieving 80 clustered documents from 600K candidates.  With almost no loss of golden units, the number of candidates dropped 7500x, largely increasing the signal-to-noise ratio.  On the other hand, it assigns hard but computationally undemanding tasks to high-capacity retrievers,  e.g.,  retrieving 4p clustered documents from 8d candidates.  More retrieval performance comparisons can be seen in  appendix  C.4 .",
            "Software Configurations.    We run retrieval and generation experiments on the above hardware. We conduct experiments with the FlagEmbedding 14 14 14 https://github.com/FlagOpen/FlagEmbedding  and huggingface transformers toolkit  Wolf et al. ( 2020 ) .  For dense retrieval, we leverage the open-source retrieval toolkit Tevatron  Gao et al. ( 2022 ) . On the other hand, we adopt BM25S  Lu ( 2024 )  for sparse retrieval. Besides, we adopt T5-large 15 15 15 https://huggingface.co/google-t5/t5-large  as the backbone of FiD ( 3.1.3 ). The detailed setting of the software environment is provided in Table  6 .",
            "Generally, a higher contextual integrity is in favor of most downstream tasks. To measure contextual integrity, we compute information entropy  Shannon ( 2001 )  on passages source in terms of Top-4 passages for each query, where we use the title of passages as the identifier. Table  9  shows the measurement results  w.r.t.  flat and progressive retrieval.  From the results, we find that progressive retrieval achieves lower information entropy ( i.e.,  higher contextual integrity) than flat one. Specifically, the improvement of progressive retrieval over flat one is 5.34% and 15.11% in terms of NQ and TQA datasets, respectively. The results fully verify that the proposed  FunnelRAG  can largely enhance the contextual integrity just as we discussed in  1 .",
            "Table  10 - 12  shows the retrieval performance on NQ and TQA datasets in terms of the cutoff position of 1, 2, and 3. The results show similar trends as Table  2 . Specifically, progressive retrieval usually achieves better performance while using less time cost.  For example, on the NQ dataset, progressive retrieval uses the time cost of 2.97s and obtains the  AR  of 63.73, in terms of Top-1, while flat one takes 5.25s and only achieves the  AR  of 56.09. The results verify the superiority of progressive retrieval in balancing effectiveness and efficiency.",
            "Table  13  shows a data-flow example of  FunnelRAG  from the NQ dataset. It consists of three stages: Retrieval, Pre-ranking, and Post-ranking. From the retrieval to the post-ranking stage, the granularity evolves from coarse-grained clustered documents to fine-grained passages. Besides, the number of candidates decreases step by step ( e.g.,  600K  Retrieval Retrieval  \\xrightarrow[]{\\text{Retrieval}} start_ARROW overRetrieval  end_ARROW 80c  Pre-ranking Pre-ranking  \\xrightarrow[]{\\text{Pre-ranking}} start_ARROW overPre-ranking  end_ARROW 8d  Post-ranking Post-ranking  \\xrightarrow[]{\\text{Post-ranking}} start_ARROW overPost-ranking  end_ARROW 4p). More importantly, we find that the proportion of candidates that contains the answer string gradually increases, which indicates that the signal-to-noise ratio is greatly improved through progressive retrieval.  Furthermore, with more advanced models, the pre-ranking and the post-ranking stage can perceive clues in the question,  i.e.,   the fifth season . Though the model becomes more complex, the number of candidates is limited, so the computational cost is small. As a result,  Americas Got Talent (season 5)  has been ranked first. The above analyses fully verify the effectiveness and reasonability of the proposed retrieval paradigm for RAG,  i.e.,   FunnelRAG , whose main design ideas are large-to-small quantities, coarse-to-fine granularity, as well as simple-to-complex  models.",
            "Algorithm  1  shows the document clustering algorithm used in the retrieval stage ( 3.1.1 ). Simply put, documents that are highly relevant will be clustered together, where each cluster is a list of documents related to each other. And, the clustered documents serve as the coarse-grained retrieval units."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Retrieval performance comparison  w.r.t.  time cost and answer recall on NQ and TQA datasets, where c, d, and p denote clustered documents, documents, and passages, respectively. The value on the arrow (   \\rightarrow  ) indicates the number of fine-grained candidates after segmenting coarse-grained ones. The number of time cost is in seconds. We provide detailed hardware and software configurations for experiments on time cost in Appendix  B.4 .",
        "table": "S4.T2.18",
        "footnotes": [],
        "references": [
            "Coarse-Fine Contrast.  To simulate this scenario, we fix the retriever as bge-reranker-v2-m3 and use it to rerank coarse-grained ( i.e.,  clustered documents) and fine-grained units ( i.e.,  documents), respectively. We report the answer recall (AR) performance on the NQ and TQA 6 6 6 Answer Recall (AR) measures the recall of the answer string in all the retrieved units. We treat it as retrieval metric. , which are presented in Figures  2(a)  and  2(b) , respectively.  From the results, the answer recall with fine-grained retrieval substantially outperforms that with coarse-grained one. Additionally, the performance degradation with fine-grained retrieval is significantly slower than that with coarse-grained one. For example, on the NQ dataset, fine-grained retrieval only drops 2.25% of its original performance, while coarse-grained retrieval drops 37.93%, when the cutoff position is top 20%. Given the above, it is necessary and valuable to segment coarse-grained units into fine-grained units along progressive retrieval stages in order for  better  retrieval performance.",
            "The overall framework of  FunnelRAG  is shown in Figure  4 . Then, we introduce three progressive retrieval stages from coarse to fine ( 3.1 ) and then describe how to distill aggregated signals from the succeeding retriever  to the  preceding  one  ( 3.2 ).",
            "Given pre-ranking scores, we retrieve top- N N N italic_N  documents closest to the query  D m = { d 1 m , d 2 m , ... , d N m } subscript D m superscript subscript d 1 m superscript subscript d 2 m ... superscript subscript d N m \\mathcal{D}_{m}=\\{d_{1}^{m},d_{2}^{m},...,d_{N}^{m}\\} caligraphic_D start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT = { italic_d start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , italic_d start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , ... , italic_d start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT } .  Figure  2  shows the performance comparison between fine-grained and coarse-grained units. The results indicate that the finer retrieval units can significantly reduce the loss of retrieval performance.",
            "where  l  n l n {ln} italic_l italic_n ,  l  h l h {lh} italic_l italic_h , and  l  r l r {lr} italic_l italic_r  denote the number of the layers, heads, and representative tokens, respectively;   0 , i , j , r  [ k ] subscript  0 i j r delimited-[] k \\alpha_{0,i,j,\\mathbf{r}[k]} italic_ start_POSTSUBSCRIPT 0 , italic_i , italic_j , bold_r [ italic_k ] end_POSTSUBSCRIPT  represents the score of the first decoder token in terms of the  i i i italic_i -th layer,  j j j italic_j -th head, and  k k k italic_k -th representative token;   r  [ k ] r delimited-[] k \\mathbf{r}[k] bold_r [ italic_k ]  lookups the index of  k k k italic_k -th representative token. We select  l  r l r {lr} italic_l italic_r  tokens with the highest scores to serve as representative ones 8 8 8 Note that we do not consider query tokens in  q q q italic_q  when selecting representative tokens, refer to the ablation study  in Appendix  C.2  for more details. More technical details about representative token selection can be found in Appendix  A.2 . .  The cross-attention mechanism is provided in Appendix  A.1 . Before estimating the relative importance score, we train FiD to predict the answer with retrieval-augmented passages, so that FiD can learn to look for cues. After that, we obtain the post-ranking score and treat these passages with scores ranked in the top- H H H italic_H  as oracle passages  P f = { p 1 f , p 2 f , ... , p H f } subscript P f superscript subscript p 1 f superscript subscript p 2 f ... superscript subscript p H f \\mathcal{P}_{f}=\\{p_{1}^{f},p_{2}^{f},...,p_{H}^{f}\\} caligraphic_P start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = { italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT , ... , italic_p start_POSTSUBSCRIPT italic_H end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT } . The oracle passages  P f subscript P f \\mathcal{P}_{f} caligraphic_P start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT  will be fed into generators to  facilitate  generation.",
            "Table  2 ,  10 ,  11 , and  12  shows the retrieval results on NQ and TQA datasets.  As BM25s retrieval speed is breakneck, we mark its time cost as 0.00. From the results, we mainly have the following observations:   (1)  By adjusting the collaboration between large-to-small quantities, coarse-to-fine granularity, and simple-to-complex retrievers, progressive retrieval can find a gain-cost balance point.  Specifically, the time cost of progressive retrieval is considerably lower than that of the flat one (reduced by about 40%), while the AR is comparable to (or even better than) the flat one.  The main reason for this results is that progressive retrieval combines the advantage of different retrievers but circumvents their disadvantage 11 11 11 Progressive retrieval combines simple retrievers fast speed with complex ones high precision, while it avoids simple retrievers low precision and  complex  ones slow  speed. .   (2)  By retrieving in a funnel manner ( e.g.,  600K  Retrieval Retrieval  \\xrightarrow[]{\\text{Retrieval}} start_ARROW overRetrieval  end_ARROW 80c  Pre-ranking Pre-ranking  \\xrightarrow[]{\\text{Pre-ranking}} start_ARROW overPre-ranking  end_ARROW 8d  Post-ranking Post-ranking  \\xrightarrow[]{\\text{Post-ranking}} start_ARROW overPost-ranking  end_ARROW 4p),  progressive retrieval enables load balancing. It assigns simple but computationally demanding tasks to low-capacity retrievers,  e.g.,  retrieving 80 clustered documents from 600K candidates.  With almost no loss of golden units, the number of candidates dropped 7500x, largely increasing the signal-to-noise ratio.  On the other hand, it assigns hard but computationally undemanding tasks to high-capacity retrievers,  e.g.,  retrieving 4p clustered documents from 8d candidates.  More retrieval performance comparisons can be seen in  appendix  C.4 .",
            "(2) Impact of representative tokens:  As shown in Figure  5(b)  and  6 , we experiment to investigate #rep tokens  l  r l r {lr} italic_l italic_r , where we search  l  r l r {lr} italic_l italic_r  in the range of {2, 4, 8, 16, 24, 32}. We also aggregate FiD cross-attention scores across all tokens with/without query ones. We observe that the performance of aggregating rep tokens significantly outperforms that of aggregating all ones. On the other hand, it gets a peak value when selecting a few rep tokens, such as 4 in most cases. The results fully validate the necessity of selecting rep tokens.  More related details can be seen in Appendix  C.2 .",
            "Our model is built upon FiD  Izacard and Grave ( 2021b ) , whose architecture is a sequence-to-sequence model that consists of an encoder and a decoder. The encoder encodes each query-passage pair separately. The output representations of the encoder are concatenated (Equ ( 2 )) along the sequence dimension to form a global representation  H =  m = 1 M H q , p m f \\mathbf{H}=\\|_{m=1}^{M}\\mathbf{H}_{q,p_{m}^{f}} bold_H =  start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT bold_H start_POSTSUBSCRIPT italic_q , italic_p start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT end_POSTSUBSCRIPT .  Then, the decoder autoregressively attends this representation, which alternates self-attention, cross-attention, and feed-forward modules. In the decoder, only the cross-attention module explicitly takes the global representation  H H \\mathbf{H} bold_H  of the encoder as the input. Let  X  denote the output of the previous self-attention layer of the decoder, the cross-attention operation consists of the following steps. First, three linear transformations are applied to produce queries  Q Q \\mathbf{Q} bold_Q , keys  K K \\mathbf{K} bold_K , and values  V , which can be formulated as follows:",
            "Table  10 - 12  shows the retrieval performance on NQ and TQA datasets in terms of the cutoff position of 1, 2, and 3. The results show similar trends as Table  2 . Specifically, progressive retrieval usually achieves better performance while using less time cost.  For example, on the NQ dataset, progressive retrieval uses the time cost of 2.97s and obtains the  AR  of 63.73, in terms of Top-1, while flat one takes 5.25s and only achieves the  AR  of 56.09. The results verify the superiority of progressive retrieval in balancing effectiveness and efficiency."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Generation performance in terms of different retrieval paradigms. The  bold  indicates the best results.",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "These two novel designs jointly contribute to the considerable improvement of retrieval accuracy and efficiency in open-domain question answering (QA), such as Natural Question (NQ)  Kwiatkowski et al. ( 2019 )  and Trivia QA (TQA)  Joshi et al. ( 2017 ) .  Table  1  shows the key differences between the progressive retrieval and the flat one.   FunnelRAG  features  (1)  Coarse-to-fine granularity which balances load and accuracy;  (2)  Later chunking which perceives the contextual information of retrieval units well 2 2 2 Flat retrieval reranks passages that are not contextual integrity. However, our progressive retrieval perceives contextual information well in the post-ranking stage, since these passages are derived from documents with contextual integrity. ;   (3)  Compressed corpus (30x smaller from 21M to 600K) which reduces the burden on the retrieval stage; and  (4)  Long retrieval unit which improves answer recall to the full extent. More details can be found in  3 .",
            "As stated in Section  1 , we emphasize the significance of (i) reducing the scale of candidates, (ii) refining the granularity of units, and (iii) increasing the level of retrievers capacity, throughout the retrieval paradigm. Here, we conduct empirical studies to verify claims (ii) and (iii), as it is obvious that claim (i) will lead to better retrieval accuracy 4 4 4 Assuming a fixed number of golden units that exists in candidates, increasing the scale of candidates reduces the signal-to-noise ratio, inevitable degrading retrieval accuracy.   Sawarkar et al. ( 2024 ) . We aim to evaluate whether these two claims can contribute to better retrieval performance, so as to validate that the progressive retrieval paradigm can balance effectiveness and efficiency well.  To empirically verify these claims, we construct two synthetic datasets for NQ and TQA. Specifically, for each query in NQ and TQA, we retrieve the top-10 relevant clustered documents from the coarse-grained corpus 5 5 5 Referring to Section  3.1.1  and Algorithm  1  for more technical details about how the clustered  documents  formulated.  using BM25  Robertson and Zaragoza ( 2009 ) , where each one has approximate 4K tokens. Then, we segment each clustered document into document-level units with about 1K tokens, striking the coarse-fine contrast. On the other hand, we adopt bge-reranker-v2-m3  Chen et al. ( 2024 )  and BM25 to make a contrast between high- and low-capacity retrievers:",
            "High-Low Contrast.  To simulate this scenario, we fix the granularity of retrieval units as fine-grained ones and employ bge-reranker-v2-m3 and BM25 to rerank them, respectively. The experimental results are presented in Figure  3(a)  and  3(b) , respectively. From the results, we observe that the answer recall with the high-capacity retriever is consistently higher than that with the low-capacity one. In particular, the gain brought by the high-capacity retriever generally increases as the cutoff position decreases.  For example, on the NQ dataset, the retrieval performance improvement of High over Low is 5.45% and 0.60% in terms of cutoff@10% and cutoff@20%. These observations indicate that using high-capacity retrievers in later stages can retain the golden retrieval units to a large extent.",
            "The overall framework of  FunnelRAG  is shown in Figure  4 . Then, we introduce three progressive retrieval stages from coarse to fine ( 3.1 ) and then describe how to distill aggregated signals from the succeeding retriever  to the  preceding  one  ( 3.2 ).",
            "Table  3  shows the generation performance  w.r.t.  flat and progressive retrieval.  From the results, we observe performance with progressive retrieval outperforms that with flat one in 13 out of 18 cases, indicating the superiority of supplementing LLMs with  FunnelRAG . Specifically, when the cutoff position is small, progressive retrieval considerably outperforms flat one, such as the improvement of 16.87% and 9.35% on NQ dataset in terms of @1. When the cutoff position is high, progressive retrieval still outperforms flat one in half of cases, even if the AR of progressive one is slightly lower than flat one.  The reason is that passages retrieved by  FunnelRAG  have higher contextual integrity, refer to Appendix  C.3  for more details. Besides, we find Llama3-8Bs performance may deteriorate when provided with more passages, whereas Qwen2-7B does not. The main reason is the difference in the ability of them to handle long contexts.",
            "Enlightened by the token redundancy in hidden states  Dai et al. ( 2020 ); Goyal et al. ( 2020 ) , we select several representative tokens per layer and head, to estimate the relative importance of each passage more accurately. Specifically, we select  l  r l r {lr} italic_l italic_r  tokens that have the highest cross-attention scores from the passage to represent it.  For the  i i i italic_i -th layer and  j j j italic_j -th head, we select the representative tokens according to their cross-attention scores and obtain an index list  r r \\mathbf{r} bold_r  that contains the index of  l  r l r {lr} italic_l italic_r  representative tokens. Then, we can average FiD cross-attention scores over representative tokens by looking  up  the  index  list   r r \\mathbf{r} bold_r , referring  to  Equ ( 3 ).",
            "We provide the prompt that is used for retrieval-augmented question answering  ( 4.3 ) in  Table  5 .",
            "Software Configurations.    We run retrieval and generation experiments on the above hardware. We conduct experiments with the FlagEmbedding 14 14 14 https://github.com/FlagOpen/FlagEmbedding  and huggingface transformers toolkit  Wolf et al. ( 2020 ) .  For dense retrieval, we leverage the open-source retrieval toolkit Tevatron  Gao et al. ( 2022 ) . On the other hand, we adopt BM25S  Lu ( 2024 )  for sparse retrieval. Besides, we adopt T5-large 15 15 15 https://huggingface.co/google-t5/t5-large  as the backbone of FiD ( 3.1.3 ). The detailed setting of the software environment is provided in Table  6 .",
            "Table  13  shows a data-flow example of  FunnelRAG  from the NQ dataset. It consists of three stages: Retrieval, Pre-ranking, and Post-ranking. From the retrieval to the post-ranking stage, the granularity evolves from coarse-grained clustered documents to fine-grained passages. Besides, the number of candidates decreases step by step ( e.g.,  600K  Retrieval Retrieval  \\xrightarrow[]{\\text{Retrieval}} start_ARROW overRetrieval  end_ARROW 80c  Pre-ranking Pre-ranking  \\xrightarrow[]{\\text{Pre-ranking}} start_ARROW overPre-ranking  end_ARROW 8d  Post-ranking Post-ranking  \\xrightarrow[]{\\text{Post-ranking}} start_ARROW overPost-ranking  end_ARROW 4p). More importantly, we find that the proportion of candidates that contains the answer string gradually increases, which indicates that the signal-to-noise ratio is greatly improved through progressive retrieval.  Furthermore, with more advanced models, the pre-ranking and the post-ranking stage can perceive clues in the question,  i.e.,   the fifth season . Though the model becomes more complex, the number of candidates is limited, so the computational cost is small. As a result,  Americas Got Talent (season 5)  has been ranked first. The above analyses fully verify the effectiveness and reasonability of the proposed retrieval paradigm for RAG,  i.e.,   FunnelRAG , whose main design ideas are large-to-small quantities, coarse-to-fine granularity, as well as simple-to-complex  models.",
            "Algorithm  1  shows the document clustering algorithm used in the retrieval stage ( 3.1.1 ). Simply put, documents that are highly relevant will be clustered together, where each cluster is a list of documents related to each other. And, the clustered documents serve as the coarse-grained retrieval units."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Statistics of the datasets.",
        "table": "A2.T4.1",
        "footnotes": [],
        "references": [
            "The overall framework of  FunnelRAG  is shown in Figure  4 . Then, we introduce three progressive retrieval stages from coarse to fine ( 3.1 ) and then describe how to distill aggregated signals from the succeeding retriever  to the  preceding  one  ( 3.2 ).",
            "The clustering algorithm is presented in Appendix  D  in Algorithm  1 , where we draw inspiration from  Jiang et al. ( 2024 )  but make several major modifications.  Specially, we sort documents by their local cluster coefficient from high to low in Line 1, so that documents with highly relevant will be clustered first.  Furthermore, we measure the closeness centrality between document  d d d italic_d  and its related clusters  R R \\mathcal{R} caligraphic_R  and sort them from high to low in Line 10, so that the most related cluster will be merged first. Each cluster  c c c italic_c  in  C C \\mathcal{C} caligraphic_C  is a list of documents related to each other.  We set the max cluster size  S S S italic_S  as 4K tokens because the experimental results ( 4.4 ) demonstrate that overlength retrieval units do not bring much benefits to answer  recall.",
            "Table  2 ,  10 ,  11 , and  12  shows the retrieval results on NQ and TQA datasets.  As BM25s retrieval speed is breakneck, we mark its time cost as 0.00. From the results, we mainly have the following observations:   (1)  By adjusting the collaboration between large-to-small quantities, coarse-to-fine granularity, and simple-to-complex retrievers, progressive retrieval can find a gain-cost balance point.  Specifically, the time cost of progressive retrieval is considerably lower than that of the flat one (reduced by about 40%), while the AR is comparable to (or even better than) the flat one.  The main reason for this results is that progressive retrieval combines the advantage of different retrievers but circumvents their disadvantage 11 11 11 Progressive retrieval combines simple retrievers fast speed with complex ones high precision, while it avoids simple retrievers low precision and  complex  ones slow  speed. .   (2)  By retrieving in a funnel manner ( e.g.,  600K  Retrieval Retrieval  \\xrightarrow[]{\\text{Retrieval}} start_ARROW overRetrieval  end_ARROW 80c  Pre-ranking Pre-ranking  \\xrightarrow[]{\\text{Pre-ranking}} start_ARROW overPre-ranking  end_ARROW 8d  Post-ranking Post-ranking  \\xrightarrow[]{\\text{Post-ranking}} start_ARROW overPost-ranking  end_ARROW 4p),  progressive retrieval enables load balancing. It assigns simple but computationally demanding tasks to low-capacity retrievers,  e.g.,  retrieving 80 clustered documents from 600K candidates.  With almost no loss of golden units, the number of candidates dropped 7500x, largely increasing the signal-to-noise ratio.  On the other hand, it assigns hard but computationally undemanding tasks to high-capacity retrievers,  e.g.,  retrieving 4p clustered documents from 8d candidates.  More retrieval performance comparisons can be seen in  appendix  C.4 .",
            "We experiment on NQ  Kwiatkowski et al. ( 2019 )  and TQA  Joshi et al. ( 2017 )  datasets. Following  Lee et al. ( 2019 ) , we discard answers with more than 5 tokens, as answers with many tokens often resemble extractive snippets instead of canonical ones. Table  4  shows the  statistics  of  the  datasets.",
            "We provide the prompt that is used for retrieval-augmented question answering  ( 4.3 ) in  Table  5 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  The prompt for retrieval-augmented QA. For intuition, we select a real example from the NQ dataset.",
        "table": "A2.T6.1",
        "footnotes": [],
        "references": [
            "To better balance effectiveness and efficiency, we propose a progressive retrieval paradigm, which enables load balancing and improves retrieval accuracy by collaborating mixed-capacity retrievers, refining retrieval granularity, and reducing candidate scale.  It can be formulated into three stages: (1) Retrieval, which usually uses relatively simple bi-encoder models to retrieve coarse-grained units; (2) Pre-ranking, which adopts cross-encoder models to rank the previously retrieved units; and (3) Post-ranking, which employs relatively complex list-wise models to rank these fine-grained units. We provide an intuitive example in Appendix  C.5 .",
            "(1) AR  w.r.t.  different granularity:  As shown in Figure  5(a) , we experiment to evaluate the answer recall  w.r.t.  different granularity of clustered documents, where we tune the max cluster size  S S S italic_S  within the range of {1K, 2K, 4K, 8K}. It can be observed that setting  S S S italic_S  as 8K does not bring much performance benefits compared to 4K. In contrast, setting  S S S italic_S  as a small value ( e.g.,  1K) considerably degrades the performance. In view of this, we set the max cluster  size  S S S italic_S  as 4K  tokens  in the  work.",
            "(2) Impact of representative tokens:  As shown in Figure  5(b)  and  6 , we experiment to investigate #rep tokens  l  r l r {lr} italic_l italic_r , where we search  l  r l r {lr} italic_l italic_r  in the range of {2, 4, 8, 16, 24, 32}. We also aggregate FiD cross-attention scores across all tokens with/without query ones. We observe that the performance of aggregating rep tokens significantly outperforms that of aggregating all ones. On the other hand, it gets a peak value when selecting a few rep tokens, such as 4 in most cases. The results fully validate the necessity of selecting rep tokens.  More related details can be seen in Appendix  C.2 .",
            "(3) Effect of local-to-global distillation:  As shown in Figure  5(c) , we experiment to explore the effect of local-to-global distillation, we tune    \\alpha italic_  in the range of {0.25, 0.5, 0.75}. From the results, we observe that a higher value    \\alpha italic_  ( e.g.,  0.75) usually leads to better performance and relatively high improvement on the top-ranked position ( e.g.,  Top-1). The improvement is relatively trivial when    \\alpha italic_  is small ( e.g.,  0.25). When the cutoff position is large ( e.g.,  Top-4), different values of    \\alpha italic_  bring similar performance improvements. In conclusion, we suggest tuning    \\alpha italic_  within the range of [0.5, 1.0).",
            "We provide the prompt that is used for retrieval-augmented question answering  ( 4.3 ) in  Table  5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Detailed settings of the software environment.",
        "table": "A2.T7.5",
        "footnotes": [],
        "references": [
            "(2) Impact of representative tokens:  As shown in Figure  5(b)  and  6 , we experiment to investigate #rep tokens  l  r l r {lr} italic_l italic_r , where we search  l  r l r {lr} italic_l italic_r  in the range of {2, 4, 8, 16, 24, 32}. We also aggregate FiD cross-attention scores across all tokens with/without query ones. We observe that the performance of aggregating rep tokens significantly outperforms that of aggregating all ones. On the other hand, it gets a peak value when selecting a few rep tokens, such as 4 in most cases. The results fully validate the necessity of selecting rep tokens.  More related details can be seen in Appendix  C.2 .",
            "Software Configurations.    We run retrieval and generation experiments on the above hardware. We conduct experiments with the FlagEmbedding 14 14 14 https://github.com/FlagOpen/FlagEmbedding  and huggingface transformers toolkit  Wolf et al. ( 2020 ) .  For dense retrieval, we leverage the open-source retrieval toolkit Tevatron  Gao et al. ( 2022 ) . On the other hand, we adopt BM25S  Lu ( 2024 )  for sparse retrieval. Besides, we adopt T5-large 15 15 15 https://huggingface.co/google-t5/t5-large  as the backbone of FiD ( 3.1.3 ). The detailed setting of the software environment is provided in Table  6 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Performance comparison of attention aggregation schemes on NQ and TQA datasets, where the index  i i i italic_i  corresponds to layers of the decoder,  j j j italic_j  corresponds to heads, and  k k k italic_k  corresponds to input tokens of a given passage.",
        "table": "A3.T8.1",
        "footnotes": [],
        "references": [
            "Table  7  shows the results with different aggregation schemes. Particularly, we consider  (1)  taking the average over all the layers, all the heads, and all the representative tokens;  (2)  taking the max over the layers instead of the average;  (3)  taking the max over the heads instead of the average;  (4)  taking the max over input tokens instead of the average;  (5)  taking the mean over the last 6 layers instead of all the layers.  In all of the above variants, we do not consider query tokens when aggregating. We observe that: (1) Aggregation schemes with representative tokens selection ( i.e.,  (i), (iv), and (v)) outperform those without representative tokens selection by a large margin, which indicates the necessity of selecting representative tokens. (2) Comparing (i) and (iv), we find that setting #rep tokens to a small value ( e.g.,  1) will hurt the retrieval performance, which suggests setting #rep tokens to moderate values ( e.g.,  4). (3) Comparing (i) and (v), only considering part of the decoder layers may bring a little performance degradation. The main reason is that certain clues captured by the other layers have been neglected. (4) Comparing (ii) and (iii), taking the max over the heads generally performs better than that over the layers.  The main reason is taking the max over the heads may find the influential head that plays a key role in identifying clues  Deng et al. ( 2024 ) , while taking the max over the layers will miss some key clues captured by the other layers. In conclusion, we suggest setting #rep tokens to moderate values, using all the layers, and trying to identify the influential heads."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  AR comparison with and without query tokens.",
        "table": "A3.T9.1",
        "footnotes": [],
        "references": [
            "In Table  8 , we conduct experiments to verify the effectiveness of ablating query tokens when estimating the relative importance of each passage. Averaging the FiD cross-attention score of all tokens in  q  p f direct-sum q superscript p f q\\oplus p^{f} italic_q  italic_p start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT  mentions  w/ query  and averaging that of all tokens in  p f superscript p f p^{f} italic_p start_POSTSUPERSCRIPT italic_f end_POSTSUPERSCRIPT  mentions  w/o query . Here, we do not consider selecting representative tokens so that we can verify the effectiveness of ablating query tokens straightforwardly. The results show that ablating query tokens consistently improves answer recall across different cutoff positions. Specifically, We find that the marginal benefit appears as the cutoff position increases.  For example, on the TQA dataset,  w/o query  achieves 5.17% improvement over  w/ query  at the Top-1 position, while it is 0.61% at the Top-4 position. It can be concluded that ablating query tokens can significantly improve answer recall in the top-ranked positions, however, the gain decreases as the number of passages retrieved increases. The main reason is that as the number of passages retrieved increases, it becomes more essential than ablating query tokens."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Contextual integrity measurement on passages. The lower the value, the better the  contextual  integrity.",
        "table": "A3.T10.18",
        "footnotes": [],
        "references": [
            "Generally, a higher contextual integrity is in favor of most downstream tasks. To measure contextual integrity, we compute information entropy  Shannon ( 2001 )  on passages source in terms of Top-4 passages for each query, where we use the title of passages as the identifier. Table  9  shows the measurement results  w.r.t.  flat and progressive retrieval.  From the results, we find that progressive retrieval achieves lower information entropy ( i.e.,  higher contextual integrity) than flat one. Specifically, the improvement of progressive retrieval over flat one is 5.34% and 15.11% in terms of NQ and TQA datasets, respectively. The results fully verify that the proposed  FunnelRAG  can largely enhance the contextual integrity just as we discussed in  1 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Retrieval performance comparison (in Top-1)  w.r.t.  time cost and answer recall on NQ and TQA datasets.",
        "table": "A3.T11.18",
        "footnotes": [],
        "references": [
            "Table  2 ,  10 ,  11 , and  12  shows the retrieval results on NQ and TQA datasets.  As BM25s retrieval speed is breakneck, we mark its time cost as 0.00. From the results, we mainly have the following observations:   (1)  By adjusting the collaboration between large-to-small quantities, coarse-to-fine granularity, and simple-to-complex retrievers, progressive retrieval can find a gain-cost balance point.  Specifically, the time cost of progressive retrieval is considerably lower than that of the flat one (reduced by about 40%), while the AR is comparable to (or even better than) the flat one.  The main reason for this results is that progressive retrieval combines the advantage of different retrievers but circumvents their disadvantage 11 11 11 Progressive retrieval combines simple retrievers fast speed with complex ones high precision, while it avoids simple retrievers low precision and  complex  ones slow  speed. .   (2)  By retrieving in a funnel manner ( e.g.,  600K  Retrieval Retrieval  \\xrightarrow[]{\\text{Retrieval}} start_ARROW overRetrieval  end_ARROW 80c  Pre-ranking Pre-ranking  \\xrightarrow[]{\\text{Pre-ranking}} start_ARROW overPre-ranking  end_ARROW 8d  Post-ranking Post-ranking  \\xrightarrow[]{\\text{Post-ranking}} start_ARROW overPost-ranking  end_ARROW 4p),  progressive retrieval enables load balancing. It assigns simple but computationally demanding tasks to low-capacity retrievers,  e.g.,  retrieving 80 clustered documents from 600K candidates.  With almost no loss of golden units, the number of candidates dropped 7500x, largely increasing the signal-to-noise ratio.  On the other hand, it assigns hard but computationally undemanding tasks to high-capacity retrievers,  e.g.,  retrieving 4p clustered documents from 8d candidates.  More retrieval performance comparisons can be seen in  appendix  C.4 .",
            "Table  10 - 12  shows the retrieval performance on NQ and TQA datasets in terms of the cutoff position of 1, 2, and 3. The results show similar trends as Table  2 . Specifically, progressive retrieval usually achieves better performance while using less time cost.  For example, on the NQ dataset, progressive retrieval uses the time cost of 2.97s and obtains the  AR  of 63.73, in terms of Top-1, while flat one takes 5.25s and only achieves the  AR  of 56.09. The results verify the superiority of progressive retrieval in balancing effectiveness and efficiency."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Retrieval performance comparison (in Top-2)  w.r.t.  time cost and answer recall on NQ and TQA datasets.",
        "table": "A3.T12.18",
        "footnotes": [],
        "references": [
            "Table  2 ,  10 ,  11 , and  12  shows the retrieval results on NQ and TQA datasets.  As BM25s retrieval speed is breakneck, we mark its time cost as 0.00. From the results, we mainly have the following observations:   (1)  By adjusting the collaboration between large-to-small quantities, coarse-to-fine granularity, and simple-to-complex retrievers, progressive retrieval can find a gain-cost balance point.  Specifically, the time cost of progressive retrieval is considerably lower than that of the flat one (reduced by about 40%), while the AR is comparable to (or even better than) the flat one.  The main reason for this results is that progressive retrieval combines the advantage of different retrievers but circumvents their disadvantage 11 11 11 Progressive retrieval combines simple retrievers fast speed with complex ones high precision, while it avoids simple retrievers low precision and  complex  ones slow  speed. .   (2)  By retrieving in a funnel manner ( e.g.,  600K  Retrieval Retrieval  \\xrightarrow[]{\\text{Retrieval}} start_ARROW overRetrieval  end_ARROW 80c  Pre-ranking Pre-ranking  \\xrightarrow[]{\\text{Pre-ranking}} start_ARROW overPre-ranking  end_ARROW 8d  Post-ranking Post-ranking  \\xrightarrow[]{\\text{Post-ranking}} start_ARROW overPost-ranking  end_ARROW 4p),  progressive retrieval enables load balancing. It assigns simple but computationally demanding tasks to low-capacity retrievers,  e.g.,  retrieving 80 clustered documents from 600K candidates.  With almost no loss of golden units, the number of candidates dropped 7500x, largely increasing the signal-to-noise ratio.  On the other hand, it assigns hard but computationally undemanding tasks to high-capacity retrievers,  e.g.,  retrieving 4p clustered documents from 8d candidates.  More retrieval performance comparisons can be seen in  appendix  C.4 ."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  Retrieval performance comparison (in Top-3)  w.r.t.  time cost and answer recall on NQ and TQA datasets.",
        "table": "A3.T13.1",
        "footnotes": [],
        "references": [
            "Table  2 ,  10 ,  11 , and  12  shows the retrieval results on NQ and TQA datasets.  As BM25s retrieval speed is breakneck, we mark its time cost as 0.00. From the results, we mainly have the following observations:   (1)  By adjusting the collaboration between large-to-small quantities, coarse-to-fine granularity, and simple-to-complex retrievers, progressive retrieval can find a gain-cost balance point.  Specifically, the time cost of progressive retrieval is considerably lower than that of the flat one (reduced by about 40%), while the AR is comparable to (or even better than) the flat one.  The main reason for this results is that progressive retrieval combines the advantage of different retrievers but circumvents their disadvantage 11 11 11 Progressive retrieval combines simple retrievers fast speed with complex ones high precision, while it avoids simple retrievers low precision and  complex  ones slow  speed. .   (2)  By retrieving in a funnel manner ( e.g.,  600K  Retrieval Retrieval  \\xrightarrow[]{\\text{Retrieval}} start_ARROW overRetrieval  end_ARROW 80c  Pre-ranking Pre-ranking  \\xrightarrow[]{\\text{Pre-ranking}} start_ARROW overPre-ranking  end_ARROW 8d  Post-ranking Post-ranking  \\xrightarrow[]{\\text{Post-ranking}} start_ARROW overPost-ranking  end_ARROW 4p),  progressive retrieval enables load balancing. It assigns simple but computationally demanding tasks to low-capacity retrievers,  e.g.,  retrieving 80 clustered documents from 600K candidates.  With almost no loss of golden units, the number of candidates dropped 7500x, largely increasing the signal-to-noise ratio.  On the other hand, it assigns hard but computationally undemanding tasks to high-capacity retrievers,  e.g.,  retrieving 4p clustered documents from 8d candidates.  More retrieval performance comparisons can be seen in  appendix  C.4 .",
            "Table  10 - 12  shows the retrieval performance on NQ and TQA datasets in terms of the cutoff position of 1, 2, and 3. The results show similar trends as Table  2 . Specifically, progressive retrieval usually achieves better performance while using less time cost.  For example, on the NQ dataset, progressive retrieval uses the time cost of 2.97s and obtains the  AR  of 63.73, in terms of Top-1, while flat one takes 5.25s and only achieves the  AR  of 56.09. The results verify the superiority of progressive retrieval in balancing effectiveness and efficiency."
        ]
    },
    "global_footnotes": [
        "Code will be released upon publication.",
        "Corresponding author.",
        "Flat retrieval reranks passages that are not contextual integrity. However, our progressive retrieval perceives contextual information well in the post-ranking stage, since these passages are derived from documents with contextual integrity.",
        "Note that the embedding encoder",
        "used for encoding queries and passages may not be the same, refer to",
        ". For  simplicity, we  do make no  distinction  here.",
        "Assuming a fixed number of golden units that exists in candidates, increasing the scale of candidates reduces the signal-to-noise ratio, inevitable degrading retrieval accuracy.",
        "Referring to Section",
        "and Algorithm",
        "for more technical details about how the clustered  documents  formulated.",
        "Answer Recall (AR) measures the recall of the answer string in all the retrieved units. We treat it as retrieval metric.",
        "Here, we take the score of the first token, as it leads to satisfactory performance in general.  More analysis about attention aggregation schema can be found in Appendix",
        ".",
        "Note that we do not consider query tokens in",
        "when selecting representative tokens, refer to the ablation study  in Appendix",
        "for more details. More technical details about representative token selection can be found in Appendix",
        ".",
        "In this work, we only distill retrieval knowledge from the post-ranking to pre-ranking stage, as we use sparse retrieval (BM25",
        ") in the retrieval stage.",
        "We use Llama3-8B and Qwen2-7B to represent Llama3-8B-Instruct and Qwen2-7B-Instruct, respectively, for brevity.",
        "Progressive retrieval combines simple retrievers fast speed with complex ones high precision, while it avoids simple retrievers low precision and  complex  ones slow  speed."
    ]
}