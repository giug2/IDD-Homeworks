{
    "A3.T5": {
        "caption": "Table 5: Small Object Ablation Study: We perform an ablation study of different perception procedures and modalities on the small objects dataset. We evaluate each model on real, annotated images on the OBB prediction task. We find that monocular networks perform very poorly, while depth and RGB-D perform much better. However, SimNet consistently performs the well on this dataset of optically easy scenarios.",
        "table": "<table id=\"A3.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A3.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"A3.T5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Method</th>\n<th id=\"A3.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">3D mAP</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T5.1.2.1\" class=\"ltx_tr\">\n<td id=\"A3.T5.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Mono</td>\n<td id=\"A3.T5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.164</td>\n</tr>\n<tr id=\"A3.T5.1.3.2\" class=\"ltx_tr\">\n<td id=\"A3.T5.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Mono-Aux</td>\n<td id=\"A3.T5.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.169</td>\n</tr>\n<tr id=\"A3.T5.1.4.3\" class=\"ltx_tr\">\n<td id=\"A3.T5.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Depth</td>\n<td id=\"A3.T5.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.831</td>\n</tr>\n<tr id=\"A3.T5.1.5.4\" class=\"ltx_tr\">\n<td id=\"A3.T5.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Depth-Aux</td>\n<td id=\"A3.T5.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.838</td>\n</tr>\n<tr id=\"A3.T5.1.6.5\" class=\"ltx_tr\">\n<td id=\"A3.T5.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">RGB-D</td>\n<td id=\"A3.T5.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.855</td>\n</tr>\n<tr id=\"A3.T5.1.7.6\" class=\"ltx_tr\">\n<td id=\"A3.T5.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">RGB-D-Aux</td>\n<td id=\"A3.T5.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.864</td>\n</tr>\n<tr id=\"A3.T5.1.8.7\" class=\"ltx_tr\">\n<td id=\"A3.T5.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">RGB-D-Stack</td>\n<td id=\"A3.T5.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.856</td>\n</tr>\n<tr id=\"A3.T5.1.9.8\" class=\"ltx_tr\">\n<td id=\"A3.T5.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">RGB-D-Seq</td>\n<td id=\"A3.T5.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0.774</td>\n</tr>\n<tr id=\"A3.T5.1.10.9\" class=\"ltx_tr\">\n<td id=\"A3.T5.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">SimNet</td>\n<td id=\"A3.T5.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"A3.T5.1.10.9.2.1\" class=\"ltx_text ltx_font_bold\">0.921</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "S4.T1": {
        "caption": "Table 1: SimNet Perception Results: We present an ablation of different sensing modalities on the 3D OBB prediction task. We evaluate 3D mAP on a dataset of real, human-annotated images of optically easy objects. We find that monocular sensing performs poorly on this task, likely due to the task being 3D in nature, while depth and RGB-D perform much better. SimNet outperforms these methods on the real images. Baseline implementation details can be found in Sec.\u00a0B.1. We present full ablation results in supplemental Sec.\u00a0C.",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Method</td>\n<td id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Mono</td>\n<td id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Depth</td>\n<td id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">RGB-D</td>\n<td id=\"S4.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">SimNet</td>\n</tr>\n<tr id=\"S4.T1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">3D mAP</td>\n<td id=\"S4.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.164</td>\n<td id=\"S4.T1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.831</td>\n<td id=\"S4.T1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.855</td>\n<td id=\"S4.T1.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.2.2.5.1\" class=\"ltx_text ltx_font_bold\">0.921</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "We compare to the following baselines: mono, which uses the left stereo image, depth, which only uses depth inputs, and RGB-D, which uses both RGB and depth inputs. To make the RGB-D baseline competitive, we swept over four different algorithms to fuse color and depth information on a held out set of 500500500 non-optically challenging scenes (i.e. matte objects with minimal natural light). We measured 3D mAP@0.25, a common metric in pose prediction\u00a0[46], of annotated bounding boxes of each object on the tabletop of interest. We report the best results of each method in Table 1.\nAdditional sensing ablations can be found in Sec.\u00a0C."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: SimNet Grasping Results: Grasp success scores between the best RGB-D method and SimNet across homes. On optically easy objects (i.e. matte and non-reflective) RGBD-D and SimNet perform similar on average. However, when presented with challenging objects such as glassware, SimNet outperforms RGB-D.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Method (Object Class)</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Home 1</th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Home 2</th>\n<th id=\"S4.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Home 3</th>\n<th id=\"S4.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Home 4</th>\n<th id=\"S4.T2.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Overall</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">RGB-D (O. Easy)</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4/5</td>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.1.2.1.3.1\" class=\"ltx_text ltx_font_bold\">5/5</span></td>\n<td id=\"S4.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4/5</td>\n<td id=\"S4.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.1.2.1.5.1\" class=\"ltx_text ltx_font_bold\">5/5</span></td>\n<td id=\"S4.T2.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.1.2.1.6.1\" class=\"ltx_text ltx_font_bold\">18/20</span></td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">SimNet (O. Easy)</td>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.1.3.2.2.1\" class=\"ltx_text ltx_font_bold\">5/5</span></td>\n<td id=\"S4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">4/5</td>\n<td id=\"S4.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.1.3.2.4.1\" class=\"ltx_text ltx_font_bold\">5/5</span></td>\n<td id=\"S4.T2.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">4/5</td>\n<td id=\"S4.T2.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.1.3.2.6.1\" class=\"ltx_text ltx_font_bold\">18/20</span></td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">RGB-D (O. Hard)</td>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">0/5</td>\n<td id=\"S4.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">1/5</td>\n<td id=\"S4.T2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">1/5</td>\n<td id=\"S4.T2.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T2.1.4.3.5.1\" class=\"ltx_text ltx_font_bold\">5/5</span></td>\n<td id=\"S4.T2.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_r\">7/20</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">SimNet (O. Hard)</td>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.1.5.4.2.1\" class=\"ltx_text ltx_font_bold\">5/5</span></td>\n<td id=\"S4.T2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.1.5.4.3.1\" class=\"ltx_text ltx_font_bold\">5/5</span></td>\n<td id=\"S4.T2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.1.5.4.4.1\" class=\"ltx_text ltx_font_bold\">5/5</span></td>\n<td id=\"S4.T2.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">4/5</td>\n<td id=\"S4.T2.1.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\"><span id=\"S4.T2.1.5.4.6.1\" class=\"ltx_text ltx_font_bold\">19/20</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "For each of the four homes we test five easy objects and five hard objects and compare SimNet against the best RGB-D baseline found in Sec. 4.1.2.\nWe report quantitative results in Table 2 and qualitative results of the predictions in Fig. 4. SimNet outperforms RGB-D, 92.5% vs. 62.5% in grasp success."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: T-Shirt Folding Results: A comparison of keypoint mAP shows that active depth-based models transfer poorly on this task, due to interference from natural lighting and low depth profile of the shirts. The monocular network transfers well to the real shirt images, and SimNet slightly outperforms it. Unlike the models that use active depth sensing, SimNet is robust to lighting variation and the low depth variation of the shirts.",
        "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Method</td>\n<td id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Mono</td>\n<td id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Depth</td>\n<td id=\"S4.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">RGB-D</td>\n<td id=\"S4.T3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">SimNet</td>\n</tr>\n<tr id=\"S4.T3.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">mAP</td>\n<td id=\"S4.T3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.893</td>\n<td id=\"S4.T3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.282</td>\n<td id=\"S4.T3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.631</td>\n<td id=\"S4.T3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S4.T3.1.2.2.5.1\" class=\"ltx_text ltx_font_bold\">0.917</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "The robot is also evaluated on a t-shirt folding task, where it must execute a sequence of four folds on unseen, real t-shirts. This task is challenging to perform using depth sensing, because the depth resolution of most commercial depth sensors cannot capture the subtle variations in depth due to the thickness of a t-shirt. Keypoints are a popular representation for manipulating deformable objects\u00a0[17, 18]. We parameterize a robot shirt folding policy using keypoint predictions for the shirt\u2019s neck, sleeves, and bottom corners (Sec.\u00a0B.4.2). To compute quantitative results on sim2real transfer, we collect a validation dataset of 32 real images from 12 t-shirts in 3 homes of stages of t-shirt folding (Fig.\u00a05) and report keypoint prediction mAP (Table\u00a03). We additionally present videos of folding using the robot on the project website for qualitative evaluation. We find that SimNet significantly outperforms RGB-D and depth, and slightly outperforms mono. A large part of our dataset was collected in a room with direct natural light, which increase noise in the active depth sensor, due to large amount of infrared light."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: KITTI Results: Results of different sim-to-real techniques for 2D car detection on the KITTI Benchmark. We find that the simply stacking the left and right images in the stacked input network significantly outperforms the monocular network. SimNet, however, transfers much better than naive stereo concatenation and is close to performance using real data.",
        "table": "<table id=\"S4.T4.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Method</td>\n<td id=\"S4.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Mono</td>\n<td id=\"S4.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Stacked</td>\n<td id=\"S4.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_t\">SimNet</td>\n<td id=\"S4.T4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">SimNet-real</td>\n</tr>\n<tr id=\"S4.T4.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">mAP</td>\n<td id=\"S4.T4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.565</td>\n<td id=\"S4.T4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.710</td>\n<td id=\"S4.T4.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t\"><span id=\"S4.T4.1.2.2.4.1\" class=\"ltx_text ltx_font_bold\">0.826</span></td>\n<td id=\"S4.T4.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.861</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    }
}