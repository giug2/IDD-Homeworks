{
    "id_table_1": {
        "caption": "TABLE I :  Mapping of Symbols to Values.",
        "table": "S3.T1.4",
        "footnotes": [],
        "references": [
            "As shown in Figure  1 ,  MoRSE  consists of two main components: a graphical user interface (GUI) and the MoRSE core. The GUI enables interaction with the user by allowing the input of queries and displaying the answers in a structured way 11 11 11 https://github.com/Mixture-of-RAGs-Security-Experts/MoRSE .",
            "The  Generation  part, in which the Large Language Model (LLM) uses the context provided in the  Prompt  to generate responses. After the retrieval phase, the collected information (info  1 1 1 1  to info  N N N italic_N ) is merged into a  Context , which is  Wrapped , along with the user query, in a  Prompt  used by LLM to generate the  Answer . The logic of the architecture is formalized in the Algorithm  1 .",
            "Answer Relevance , shown in Equation  1 , measures how pertinent the generated answer is to the given prompt. It is calculated by generating related questions from the models answer and comparing their embeddings to the original question using cosine similarity:"
        ]
    },
    "id_table_2": {
        "caption": "TABLE II :  Comparative Assessment of RAG Models on 156 General and 150 Multi-Hop Cybersecurity Questions",
        "table": "S3.T2.12",
        "footnotes": [],
        "references": [
            "Figure  2  shows the first stage of MoRSE Core process, starting with the  Query Handling  model. This module converts the original query  x x x italic_x  into an optimized version  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  (see subsection  III-C ). First,  x  superscript x x^{*} italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is forwarded to the  Structured RAG  module for processing. The structured RAG path, denoted as  S S \\mathcal{S} caligraphic_S , begins with the  Structured Retrievers , focused on high accuracy and fast responses to efficiently process most queries. Their primary function,  S  ( x  ) S superscript x \\mathcal{S}(x^{*}) caligraphic_S ( italic_x start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) , is to identify and retrieve information pertinent to the query.",
            "The complete workflow is outlined in Algorithm  2 , demonstrating the mechanism of Query Handling within the MoRSE system.",
            "Answer Similarity , shown in Equation  2 , evaluates semantic congruence between model-generated responses and predefined correct answers, calculated as:"
        ]
    },
    "id_table_3": {
        "caption": "TABLE III :  Performance comparison of models on 300 CVE Queries.",
        "table": "S4.T3.4",
        "footnotes": [],
        "references": [
            "The RAG architecture of the MoRSE system, which is used in both the Structured ( III-D ) and Unstructured RAG ( III-E ), follows the same underlying logic shown in Figure  3 . This architecture is divided into two parts:",
            "Answer Correctness , shown in Equation  3  and   4 , evaluates the factual accuracy of generated answers against ground truth. It combines semantic similarities and factual correctness:"
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV :  Fraction of Wins (No Ties) of Each Combination of Models for  General Cybersecurity Questions  and  Multi-Hop Cybersecurity Questions .",
        "table": "S4.T4.6",
        "footnotes": [],
        "references": [
            "As illustrated in Figure  4 , the Structured RAG module works post- Query Handling  by forwarding refined queries to seven  Parallel Retrievers , called  Structured Retrievers , each of which specializes in specific cybersecurity topics. Given a query, the information contained in the knowledge base of a Retriever is inserted into the  Context  if its similarity to the query is above a predefined threshold. In order to establish the threshold for each retriever, we conducted an analysis on the scores of top 50 results from a series of test queries. Thresholds were then determined by assessing the distribution of scores  13 13 13 https://github.com/Mixture-of-RAGs-Security-Experts/MoRSE/tree/main/Retriever-Threshold-Scores .  In particular, we used the median value of the test distributions as threshold for the  MITRE Retriever  and the  Malware Retriever , as these typically retrieve shorter texts. For  Question Retrieval System ,  CWE Retriever ,  Metasploit Retriever  and  Entity Retriever , we chose the third quartile (Q3) of the test distributions as threshold, as they generally retrieve longer texts. The  ExploitDB Retriever  works without a threshold and uses the TF-IDF algorithm  [ 52 ] .  To mitigate embedding biases  [ 53 ] , we used two different embeddings for the retrievers, (   \\alpha italic_ ) and (   \\beta italic_ ). The following paragraphs delineate each retrievers functionalities.",
            "Answer Correctness , shown in Equation  3  and   4 , evaluates the factual accuracy of generated answers against ground truth. It combines semantic similarities and factual correctness:",
            "The aim is to estimate the probability of the  Structured Retrievers  not finding relevant text data in response to a user query, so that a switch to the Unstructured RAG is necessary to continue the search for relevant information. To this end, we have focused on looking only at the  Contextual Information  part of the final  Context  (see Figure  4 ). We have individually multiplied the retrievers failure rates listed in Table   VII , determining a collective probability of failure at approximately 0.2569%, equal to an empirical rate ( p ^ ^ p \\hat{p} over^ start_ARG italic_p end_ARG ) of 0.0026. When assessing reliability, we used a 95% confidence interval  [ 62 ] , the empirical rate, a z-score of 1.96, and the Failure Rates (Fail.Rate) from Table   VII . This analysis shows that the maximum failure rate under the tested conditions does not exceed 0.46%, which confirms the robustness of the system when processing text-based queries."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V :  Elo Ratings, MLE Ratings, and Bootstrap Ratings for each competing Models.",
        "table": "S4.T5.4",
        "footnotes": [],
        "references": [
            "The unstructured RAG, shown in Figure  5 , plays a crucial role in the MoRSE system by handling cybersecurity queries that the structured RAG cannot solve. The module utilizes retrievers, called  buffers , to store documents in chunks of 2000 characters while maintaining the integrity of the original information. All buffers work as hybrid retrievers that use both semantic search and keyword search with the BM25 algorithm  [ 56 ] . Unlike other configurations, these retrievers do not have a fixed threshold for semantic search; instead, they are configured to return the top five documents regardless of similarity scores. This decision enables further  Context Transformation  process to apply semantic thresholds, ensuring the flexibility and comprehensiveness of the retrieval process.",
            "Elo Ratings:  it quantifies the comparative skill levels across entities in competitive scenarios, making it apt for evaluating models based on their head-to-head outcomes. This approach involves initial computation using a linear update algorithm, opting for a conservative  K K K italic_K -factor to ensure stability in ratings, minimizing bias from recent encounters. Equation  5  shows the Elo rating formula used in our context:"
        ]
    },
    "id_table_6": {
        "caption": "TABLE VI :  Scores Across All Cybersecurity Questions types.",
        "table": "S4.T6.4",
        "footnotes": [],
        "references": [
            "To calculate the impact of each retriever on 600 questions, we applied a systematic methodology. First, we collected all contexts generated for 150 general questions, 150 multi-hop questions and 300 CVE questions. We then analyzed the frequency with which each retriever was able to successfully retrieve relevant information within these contexts. The frequency of successful retrievals for each retriever was then calculated as a percentage of the total questions in each category. In this way, we were able to quantify the performance and impact of each retriever in both general and multi-hop question scenarios. Figures  6(a) ,  6(b)  and   6(c)  show the impact of the different retrievers for each question category. For general questions, the Question Retrieval System has the highest impact at 56.3%, followed by the Entity Retriever with 21.7%. Other retrievers, such as MITRE, CWE, ExploitDB, Metasploit and Malware Retriever, have an impact of between 6.1% and 9.0%. For multi-hop questions, the Question Retrieval System significantly influences results with a 35.4% contribution, and the Entity Retriever also plays an important role with a 28.3% contribution. The Metasploit Retriever has an increased influence of 12%. The other retrievers (Malware, CWE, ExploitDB and MITRE) have an influence of between 5% and 7%. Figure  6(c)  shows the impact on CVE questions, where the ExploitDB and Metasploit Retrievers have the highest impact at 18% and 31% respectively. The other retrievers (Malware, CWE, MITRE, Entity, and Question Ret. Sys.) have an impact ranging from 1% to 14%.",
            "where  R n  e  w subscript R n e w R_{new} italic_R start_POSTSUBSCRIPT italic_n italic_e italic_w end_POSTSUBSCRIPT  and  R o  l  d subscript R o l d R_{old} italic_R start_POSTSUBSCRIPT italic_o italic_l italic_d end_POSTSUBSCRIPT  represent the new and old Elo ratings, respectively. The constant  K K K italic_K , set equal to 4, controls the volatility of the rating,  S S S italic_S  denotes the actual match outcome (1 for a win, 0.5 for a tie, and 0 for a loss), and  E E E italic_E  is the expected outcome, as calculated in Equation  6 :"
        ]
    },
    "id_table_7": {
        "caption": "TABLE VII :  Performance summary of Structured and Unstructured RAG components tested on CPU and GPU (NVIDIA A100 80GB).  Fail. Qs.  states for Failed Test Queries, while  Tot. Qs.  states for Total Test Queries",
        "table": "S4.T7.22",
        "footnotes": [],
        "references": [
            "Maximum Likelihood Estimation:  Utilizing logistic regression for MLE, we further analyzed the pairwise comparisons, deducing each models probability of outperforming another, thereby enriching our evaluative scope with probabilistic insights.  The logistic regression model used for MLE can be formalized in Equation  7 :"
        ]
    },
    "global_footnotes": [
        "https://attack.mitre.org/",
        "https://cve.mitre.org/",
        "https://www.metasploit.com/",
        "https://www.exploit-db.com/",
        "https://chat.openai.com/",
        "https://gemini.google.com/app",
        "https://chat.hackerai.co/",
        "https://www.langchain.com/",
        "https://haystack.deepset.ai/",
        "https://github.com/Mixture-of-RAGs-Security-Experts/MoRSE/tree/main/Retriever-Threshold-Scores",
        "https://attack.mitre.org/software/"
    ]
}