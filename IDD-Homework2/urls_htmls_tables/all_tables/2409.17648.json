{
    "id_table_1": {
        "caption": "Table 1:  F1 scores on HotPotQA, NewsQA, NarrativeQA, PubMedQA, and WebGLM-QA comparing CRAFT and the baseline models: (i) an idealized RAG (Golden) and (ii) a realistic RAG. Bold numbers denote the best score in each comparison, where higher is better.",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "Using the above datasets and baselines, we evaluate our proposed method and demonstrate the effectiveness of CRAFT in Table  1 .  Overall, the Llama3.1-8B-instruct model performs well due to its pre-training and instruction-tuned answering style.  We see that CRAFT consistently outperforms the baselines. Compared to the Llama3.1-8B-instruct model, CRAFT does much better in terms of extracting information, yielding an average of 13.41% gains over the evaluated datasets."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of a) RAFT and b) CRAFT using generated RAFT data from HotPotQA, NewsQA, and NarrativeQA datasets. We also report perplexity over selected datasets. Lower perplexity is better.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "conduct an analysis to illustrate the resource efficiencies achieved by using LoRA compared to SFT for fine tuning on the same generated dataset. As shown in Table  2 , using CRAFT reduces the number of trainable parameters to just 2% of those required by SFT. This results in a nearly 35% decrease in GPU memory usage during training and an average of speedup of 7.5x. In terms of perplexity, for HotPotQA and NewsQA, SFT offered marginally lower perplexity (by 4 and 0.045, respectively), where LoRA achieved a lower perplexity for NarrativeQA (by 0.08). Overall, the average perplexity between the two methods is nearly identical, indicating very similar model performance."
        ]
    },
    "global_footnotes": [
        "Equal contributions.",
        "https://huggingface.co/collections/phatvo/raft-66c44a18ac74db25de87a4dc"
    ]
}