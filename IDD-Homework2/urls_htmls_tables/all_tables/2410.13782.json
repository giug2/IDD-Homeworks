{
    "id_table_1": {
        "caption": "Table 2:  Benchmarking comparison of unconditional protein generation, in terms of structure-sequence co-generation, backbone-only generation, and sequence-only generation.  For each method, we generate  100 100 100 100  samples for lengths in  [ 100 , 200 , 300 , 400 , 500 ] 100 200 300 400 500 [100,200,300,400,500] [ 100 , 200 , 300 , 400 , 500 ] . * denotes Multiflow variants retrained by us using different dataset  native PDB data without ProteinMPNN distillation and the same training data as  DPLM-2  ( i.e . , PDB+SwissProt), respectively.",
        "table": "S2.T1.5.5",
        "footnotes": [],
        "references": [
            "In this paper, we address the aforementioned questions by introducing DPLM-2, a multimodal protein foundation model that advances the state-of-the-art discrete diffusion-based protein language model ( i.e . , DPLM) to accommodate both sequences and structures. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We present several key recipes to facilitate multimodal learning in DPLM-2: (1) the core difficulty lies in enabling the language model to learn structural information, which is challenging and remains elusive, for which we develop a lookup-free quantization  (LFQ, Yu et al.,  2023 )  structure tokenizer to convert 3D coordinates to discrete tokens and vice versa (Fig.  1 A,  3.3 ); (2) we implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based DPLM (Fig.  1 B,  3.2 ); and (3) we also address the exposure bias problem in discrete diffusion for sequence learning  (Ranzato et al.,  2016 ; Bengio et al.,  2015 )  by a self-mixup training strategy that leads to enhanced generation quality and diversity.",
            "As a mulitmodal generative model,  DPLM-2  enables unconditional co-generation of designable and diverse proteins that guarantees consistency between structure and sequence (Fig.  1 C(1)). Our empirical evaluation shows that  DPLM-2  attains competitive co-generation performance compared to structure-based generative approaches, while the proteins generated by  DPLM-2  have a better alignment with the characteristics of natural proteins in secondary structure statistics ( 4.1 ).",
            "In addition,  DPLM-2  supports various conditional generation tasks by its multimodal nature, ranging from (sequence-conditioned) folding (Fig.  1 C(3),  4.2 ), (structure-conditioned) inverse-folding (Fig.  1 C(4),  4.3 ), to more successful motif-scaffolding given multimodal motif conditioning (Fig.  1 C(5),  4.4 ).",
            "Last but not least, we demonstrate that the structure-aware protein representation learned by  DPLM-2  brings additional benefit for a range of protein predictive tasks (Fig.  1 C(2),  4.5 ).",
            "As a result, most of protein tasks can be viewed as specifying their input conditioning and output between these two modalities (Tab.  1 ), including (1) sequence-conditioned structure prediction  (folding, Jumper et al.,  2021 ; Lin et al.,  2022 ; Huguet et al.,  2024 ) , (2) structure-conditioned sequence generation  (inverse folding or fixed-backbone design, Dauparas et al.,  2022 ; Hsu et al.,  2022 ; Zheng et al.,  2023b ) , (3) sequence learning or generation   (Rives et al.,  2019 ; Nijkamp et al.,  2022 ; Alamdari et al.,  2023 ; Wang et al.,  2024 ) , (4) structure generation  (Yim et al.,  2023 ; Watson et al.,  2023 ; Ingraham et al.,  2023 ) , and (5) sequence-structure co-generation  (Jin et al.,  2021 ; Shi et al.,  2022 ; Campbell et al.,  2024 ) . These further enable various conditional applications by allowing single or mixed-modal conditioning for partial generation,  e.g . , motif-scaffolding and antibody design.",
            "Fig.  1  illustrates  DPLM-2 s overall architecture.  DPLM-2  is built on the state-of-the-art sequence-based generative protein LM,  i.e . , DPLM  (Wang et al.,  2024 ) , using a discrete diffusion probabilistic framework to concurrently model both protein sequences and their corresponding structures. To facilitate structure learning in language models, we introduce a token-based representation for protein structure via a tokenizer that converts  x  R L  N backb  3 x superscript R L subscript N backb 3 \\mathbf{x}\\in\\mathbb{R}^{L\\times N_{\\text{backb}}\\times 3} bold_x  blackboard_R start_POSTSUPERSCRIPT italic_L  italic_N start_POSTSUBSCRIPT backb end_POSTSUBSCRIPT  3 end_POSTSUPERSCRIPT , the 3D coordinates of the protein backbone into a discrete structure token sequence, denoted as  z = ( z 1 , z 2 , ... , z L )  { 0 , 1 } L  | Z | z subscript z 1 subscript z 2 ... subscript z L superscript 0 1 L Z \\mathbf{z}=(z_{1},z_{2},\\dots,z_{L})\\in\\{0,1\\}^{L\\times|\\mathcal{Z}|} bold_z = ( italic_z start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_z start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_z start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT )  { 0 , 1 } start_POSTSUPERSCRIPT italic_L  | caligraphic_Z | end_POSTSUPERSCRIPT , where each token  z i subscript z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  represents a local structural element of the  i i i italic_i -th residue. Given tokenized structure,  DPLM-2  processes mulitmodal input by concatenating the structure token sequence  z z \\mathbf{z} bold_z  with the corresponding amino acid sequence  s s \\mathbf{s} bold_s  for the same protein. Notably, there exists a position-by-position correspondence between  z z \\mathbf{z} bold_z  and  s s \\mathbf{s} bold_s , where  z i subscript z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  refer to the two modalities of the  i i i italic_i -th residue, respectively. To reinforce this correspondence, we assign identical position encodings to both  z i subscript z i z_{i} italic_z start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  s i subscript s i s_{i} italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , thereby ensuring that structural and sequence information is aligned at the residue level.",
            "To train  DPLM-2 , we leverage a high-quality dataset comprising 20K clustered experimental structures from the Protein Data Bank (PDB)  (Berman et al.,  2000 )  and 200K predicted structures from the AFDB SwissProt split  (Varadi et al.,  2022 ) , with length  < 512 absent 512 <512 < 512 . During training,  DPLM-2  is tasked with denoising the input sequence across a spectrum of noise levels, ranging from fully noisy to completely clean. The multimodal training objective of  DPLM-2  is derived from Eq. ( 1 ) as,",
            "To further enhance  DPLM-2 s ability to differentiate between structure and sequence, noising level for each modality is subjected to distinct scheduler, denoted as  t z subscript t z t_{\\mathbf{z}} italic_t start_POSTSUBSCRIPT bold_z end_POSTSUBSCRIPT  and  t s subscript t s t_{\\mathbf{s}} italic_t start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT , respectively. This facilitates a more comprehensive understanding of the relationships between protein sequences and their corresponding structures. This design also allows us to explore arbitrary combinations of  ( t z , t s ) subscript t z subscript t s (t_{\\mathbf{z}},t_{\\mathbf{s}}) ( italic_t start_POSTSUBSCRIPT bold_z end_POSTSUBSCRIPT , italic_t start_POSTSUBSCRIPT bold_s end_POSTSUBSCRIPT ) , thus providing flexible sampling options, including sampling from the marginals of each modality and conditionals between them for various applications (Fig.  1 C). Furthermore, we also identify the exposure bias issue in discrete diffusion for sequence learning  (Ranzato et al.,  2016 ; Bengio et al.,  2015 ) , and mitigate this by proposing a self-mixup strategy inspired by scheduled sampling, which improves both generation quality and diversity (see  A.1 ).",
            "In this section, we evaluate  DPLM-2  on various generative and understanding scenarios, including unconditional protein generation (structure, sequence, and structure-sequence co-generation,  4.1 ), and a variety of conditional tasks, such as folding ( 4.2 ), inverse folding ( 4.3 ) and motif-scaffolding ( 4.4 ), and a series of protein predictive tasks ( 4.5 ).",
            "The goal of folding is to predict the 3D structure for the given amino acid sequence  (Jumper et al.,  2021 ) . As a mulitmodal generative model,  DPLM-2  spontaneously enables protein structure prediction task (see Fig.  1 C-3) given sequence as conditioning. We assess  DPLM-2  on CAMEO 2022 and a PDB data split used by Multiflow  (Campbell et al.,  2024 ) . We utilize  RMSD  and  TMscore  between predicted and ground truth structure for evaluation, while  DPLM-2  adopts  argmax  decoding for 100 sampling iterations.",
            "As shown in Fig.  1 A, the structure tokenizer in this paper consists of a structure encoder, quantizer, and structure decoder. The encoder is based on a pre-trained GVP-Transformer  (Hsu et al.,  2022 ) , with its parameters frozen during training. It transforms backbone structures into geometric features, which are projected onto a latent embedding using an MLP layer. For the quantizer, we adopt a lookup-free quantizer from a state-of-the-art video tokenizer  (Yu et al.,  2023 ) , where the latent dimension is set to  log 2  | Z | subscript 2 Z \\log_{2}|\\mathcal{Z}| roman_log start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT | caligraphic_Z | , with  | Z | Z |\\mathcal{Z}| | caligraphic_Z |  as the codebook size. The structure decoder follows the IPA-based modules from AlphaFold2  (Jumper et al.,  2021 ) , using 4 EvoFormer layers without MSA row attention, following ESMFold  (Lin et al.,  2022 ) , to generate atomic positions from the structure tokens."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  Ablation study on the sequence pre-training and training data augmentation.",
        "table": "A4.EGx1",
        "footnotes": [],
        "references": [
            "In this paper, we address the aforementioned questions by introducing DPLM-2, a multimodal protein foundation model that advances the state-of-the-art discrete diffusion-based protein language model ( i.e . , DPLM) to accommodate both sequences and structures. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We present several key recipes to facilitate multimodal learning in DPLM-2: (1) the core difficulty lies in enabling the language model to learn structural information, which is challenging and remains elusive, for which we develop a lookup-free quantization  (LFQ, Yu et al.,  2023 )  structure tokenizer to convert 3D coordinates to discrete tokens and vice versa (Fig.  1 A,  3.3 ); (2) we implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based DPLM (Fig.  1 B,  3.2 ); and (3) we also address the exposure bias problem in discrete diffusion for sequence learning  (Ranzato et al.,  2016 ; Bengio et al.,  2015 )  by a self-mixup training strategy that leads to enhanced generation quality and diversity.",
            "In addition,  DPLM-2  supports various conditional generation tasks by its multimodal nature, ranging from (sequence-conditioned) folding (Fig.  1 C(3),  4.2 ), (structure-conditioned) inverse-folding (Fig.  1 C(4),  4.3 ), to more successful motif-scaffolding given multimodal motif conditioning (Fig.  1 C(5),  4.4 ).",
            "As shown in Fig.  2 A, LFQ significantly outperforms VQ-VAE regarding reconstruction accuracy while training of LFQ is much faster than VQ-VAE (2  vs .  15 days on 8 A100s). Increasing codebook size leads to improved reconstruction while a codebook size of 8192 achieves the best compression-reconstruction trade-off. Meanwhile in Fig.  2 B, we observe a strong correlation between structure tokens and secondary structures. For instance, a lot of structure tokens concentrated at the alpha helix and beta sheet vertices, while some tokens lie between regions. This suggests that structure tokens the fine-grained structural elements in backbone local environment.",
            "In this section, we evaluate  DPLM-2  on various generative and understanding scenarios, including unconditional protein generation (structure, sequence, and structure-sequence co-generation,  4.1 ), and a variety of conditional tasks, such as folding ( 4.2 ), inverse folding ( 4.3 ) and motif-scaffolding ( 4.4 ), and a series of protein predictive tasks ( 4.5 ).",
            "Tab.  2  and Fig.  3  present the results of  DPLM-2  for unconditional protein generation. We highlight our key findings in the following aspects:",
            "(1)  DPLM-2  can generate diverse and highly-plausible protein with simultaneous structure-sequence co-generation.  We sampled 100 proteins for each length in 100, 200, 300, 400, and 500. Fig.  3 A/B demonstrates that  DPLM-2  can sample sequence and structures with high designability across various lengths, with most  sc-TM  values exceeding 0.9, with diverse structure clusters. Fig.  3 D shows that the novelty of sampled proteins, measured by  pdb-TM , generally increases with longer protein lengths. In addition,  DPLM-2  can generate with both modalities simultaneously or a modality-by-modality. As shown in Tab.  2 , the co-generation performance exhibit highest  scTM , suggesting that co-modeling indeed benefits protein generation.",
            "(2)  DPLM-2  can attains competitive performance with strong baselines on co-generation, as well as backbone-only and sequence-only generation, respectively.  As shown in Tab.  2 ,  DPLM-2  achieves the strong  sc-TM  compared to strong baselines, approaching the quality of native structures from PDB. We notice that ESM3-Open  (Hayes et al.,  2024 ) , which runs in a sequence-then-structure order, fails short of unconditional generation. Compared to MultiFlow  (Campbell et al.,  2024 ) ,  DPLM-2  achieves comparable co-generation quality. Notably, as also reported in  Campbell et al. ( 2024 ) , Multiflow falls short of sequence generation when directly trained from structures with native sequences, resulting in greatly degraded co-generation performance without data distillation from external inverse folding models (ProteinMPNN). For reference, we also provide the result of Multiflow retrained using our training data, where its co-generation performance remains unsatisfying and lags behind  DPLM-2 , which suggests that  DPLM-2  has advantages of directly and effectively learning from complex structure-sequence joint distribution. Moreover,  DPLM-2  can also only produce single modality if needed, where it matches the best competitive models in these settings respectively. These results demonstrate  DPLM-2 s effectiveness as a mulitmodal generative model.",
            "As seen in Fig.  4 A, structure-based models like RFDiffusion and MultiFlow generate proteins with more helices and fewer sheets and loops than natural proteins in PDB. Protein language models like ESM3 and  DPLM-2  show no strong bias towards alpha helices, but ESM3 tends to generate more loops. Among the methods,  DPLM-2  produces the most natural-like secondary structure proportions, closely matching PDB proteins. In Fig.  4 C, proteins generated by MultiFlow contain many helices and become more globular as length increases, exhibiting idealized secondary structures. In contrast, proteins generated from  DPLM-2  resembles natural ones have more balanced structures, with fewer helices and more beta sheets and loops. On the other hands, simplex plots in Fig.  4 C shows that while MultiFlows proteins are clustered in helix-rich regions,  DPLM-2 s proteins span a wider area similar to natural proteins, while it rarely samples proteins composed mostly of sheets and loops, which do occur in nature. Additionally, Fig.  4 B shows that the loop ratio has a significant impact on designability, where a higher proportion of loops will increase  scRMSD , as loops are highly flexible. Thus, proteins with long loops, which  DPLM-2  often generates, tend to have relatively high  scRMSD , aligning with the results in Tab.  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 6:  Performance on various protein predictive downstream tasks.     \\dagger  : benchmarked results are quoted from  Su et al. ( 2023 ) .",
        "table": "A4.EGx2",
        "footnotes": [
            ""
        ],
        "references": [
            "In this paper, we address the aforementioned questions by introducing DPLM-2, a multimodal protein foundation model that advances the state-of-the-art discrete diffusion-based protein language model ( i.e . , DPLM) to accommodate both sequences and structures. By training on both experimental and high-quality synthetic structures, DPLM-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We present several key recipes to facilitate multimodal learning in DPLM-2: (1) the core difficulty lies in enabling the language model to learn structural information, which is challenging and remains elusive, for which we develop a lookup-free quantization  (LFQ, Yu et al.,  2023 )  structure tokenizer to convert 3D coordinates to discrete tokens and vice versa (Fig.  1 A,  3.3 ); (2) we implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based DPLM (Fig.  1 B,  3.2 ); and (3) we also address the exposure bias problem in discrete diffusion for sequence learning  (Ranzato et al.,  2016 ; Bengio et al.,  2015 )  by a self-mixup training strategy that leads to enhanced generation quality and diversity.",
            "In addition,  DPLM-2  supports various conditional generation tasks by its multimodal nature, ranging from (sequence-conditioned) folding (Fig.  1 C(3),  4.2 ), (structure-conditioned) inverse-folding (Fig.  1 C(4),  4.3 ), to more successful motif-scaffolding given multimodal motif conditioning (Fig.  1 C(5),  4.4 ).",
            "In this section, we evaluate  DPLM-2  on various generative and understanding scenarios, including unconditional protein generation (structure, sequence, and structure-sequence co-generation,  4.1 ), and a variety of conditional tasks, such as folding ( 4.2 ), inverse folding ( 4.3 ) and motif-scaffolding ( 4.4 ), and a series of protein predictive tasks ( 4.5 ).",
            "Tab.  2  and Fig.  3  present the results of  DPLM-2  for unconditional protein generation. We highlight our key findings in the following aspects:",
            "(1)  DPLM-2  can generate diverse and highly-plausible protein with simultaneous structure-sequence co-generation.  We sampled 100 proteins for each length in 100, 200, 300, 400, and 500. Fig.  3 A/B demonstrates that  DPLM-2  can sample sequence and structures with high designability across various lengths, with most  sc-TM  values exceeding 0.9, with diverse structure clusters. Fig.  3 D shows that the novelty of sampled proteins, measured by  pdb-TM , generally increases with longer protein lengths. In addition,  DPLM-2  can generate with both modalities simultaneously or a modality-by-modality. As shown in Tab.  2 , the co-generation performance exhibit highest  scTM , suggesting that co-modeling indeed benefits protein generation.",
            "(3)  DPLM-2  generates longer proteins beyond training data.  As  DPLM-2  is trained with a  512 512 512 512  length cutoff, we are curious about its length extrapolation, and evaluate sampled proteins at lengths of  [ 600 , 700 , 800 , 900 , 1000 ] 600 700 800 900 1000 [600,700,800,900,1000] [ 600 , 700 , 800 , 900 , 1000 ] . As shown in Fig.  3 F, notably, for proteins exceeding the maximum training length of 512, the  pLDDT  scores of sequences sampled by  DPLM-2  are close to those of DPLM. This suggests that  DPLM-2  largely retains its sequence generation capability inherited from sequence pre-training in DPLM, leading to its capability of length extrapolation.",
            "(4) Case study.  Fig.  3 H shows some generated samples of  DPLM-2  up to 700 residues, while in Fig.  3 I we showcase that we can manipulate  DPLM-2  to design symmetric oligomers by forcing to duplicate the predicted tokens with repetitive structure and sequence patterns.",
            "Tab.  3  demonstrates that  sequence pre-training and data augmentation can significantly improve the designability and diversity , especially in generating long proteins (length  > 300 absent 300 >300 > 300 ). We hypothesize that the limited number of long proteins in PDB leads to insufficient training. In contrast, sequence pretraining, which includes evolutionary data, is essential and can be transferred to improve protein structure modeling and generation quality. Additionally, this evolutionary information boosts sampling diversity. While increasing the amount of training data improves designability, it is less effective in enhancing diversity compared to sequence pretraining. By combining both strategies, we achieve the best overall performance, which forms the core of our training strategy."
        ]
    },
    "id_table_4": {
        "caption": "Table 8:  Ablation study on the  self-mixup  training strategy.",
        "table": "A4.EGx3",
        "footnotes": [],
        "references": [
            "As a mulitmodal generative model,  DPLM-2  enables unconditional co-generation of designable and diverse proteins that guarantees consistency between structure and sequence (Fig.  1 C(1)). Our empirical evaluation shows that  DPLM-2  attains competitive co-generation performance compared to structure-based generative approaches, while the proteins generated by  DPLM-2  have a better alignment with the characteristics of natural proteins in secondary structure statistics ( 4.1 ).",
            "In addition,  DPLM-2  supports various conditional generation tasks by its multimodal nature, ranging from (sequence-conditioned) folding (Fig.  1 C(3),  4.2 ), (structure-conditioned) inverse-folding (Fig.  1 C(4),  4.3 ), to more successful motif-scaffolding given multimodal motif conditioning (Fig.  1 C(5),  4.4 ).",
            "Last but not least, we demonstrate that the structure-aware protein representation learned by  DPLM-2  brings additional benefit for a range of protein predictive tasks (Fig.  1 C(2),  4.5 ).",
            "In this section, we evaluate  DPLM-2  on various generative and understanding scenarios, including unconditional protein generation (structure, sequence, and structure-sequence co-generation,  4.1 ), and a variety of conditional tasks, such as folding ( 4.2 ), inverse folding ( 4.3 ) and motif-scaffolding ( 4.4 ), and a series of protein predictive tasks ( 4.5 ).",
            "As seen in Fig.  4 A, structure-based models like RFDiffusion and MultiFlow generate proteins with more helices and fewer sheets and loops than natural proteins in PDB. Protein language models like ESM3 and  DPLM-2  show no strong bias towards alpha helices, but ESM3 tends to generate more loops. Among the methods,  DPLM-2  produces the most natural-like secondary structure proportions, closely matching PDB proteins. In Fig.  4 C, proteins generated by MultiFlow contain many helices and become more globular as length increases, exhibiting idealized secondary structures. In contrast, proteins generated from  DPLM-2  resembles natural ones have more balanced structures, with fewer helices and more beta sheets and loops. On the other hands, simplex plots in Fig.  4 C shows that while MultiFlows proteins are clustered in helix-rich regions,  DPLM-2 s proteins span a wider area similar to natural proteins, while it rarely samples proteins composed mostly of sheets and loops, which do occur in nature. Additionally, Fig.  4 B shows that the loop ratio has a significant impact on designability, where a higher proportion of loops will increase  scRMSD , as loops are highly flexible. Thus, proteins with long loops, which  DPLM-2  often generates, tend to have relatively high  scRMSD , aligning with the results in Tab.  2 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 9:  Motif-scaffolding results of each problem. * means best result from 8 samples.",
        "table": "A4.EGx4",
        "footnotes": [],
        "references": [
            "Last but not least, we demonstrate that the structure-aware protein representation learned by  DPLM-2  brings additional benefit for a range of protein predictive tasks (Fig.  1 C(2),  4.5 ).",
            "In this section, we evaluate  DPLM-2  on various generative and understanding scenarios, including unconditional protein generation (structure, sequence, and structure-sequence co-generation,  4.1 ), and a variety of conditional tasks, such as folding ( 4.2 ), inverse folding ( 4.3 ) and motif-scaffolding ( 4.4 ), and a series of protein predictive tasks ( 4.5 ).",
            "Tab.  5  presents that  DPLM-2  can outperform or be on par with other co-generation models (MultiFlow, ESM3). As the model size increases, the performance in terms of sequence recovery ( AAR ) and structural consistency ( scTM ) improves, revealing the same scaling law observed in the folding task. We suggest that multimodal training effectively aligns the structure and sequence into the same space, such that  DPLM-2  can yield the corresponding sequence without additional training."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A4.EGx5",
        "footnotes": [],
        "references": [
            "Tab.  6  presents that  DPLM-2  shows further improvement compared to sequence-only methods (ESM2, DPLM) on some tasks, indicating that  DPLM-2  can leverage protein structures to generate better representations containing multimodal information for downstream tasks. However, we find that  DPLM-2  falls behind the state-of-the-art structure-aware protein LM,  i.e . , SaProt, in most tasks and even lags behind DPLM in certain tasks. We hypothesize this is because the strutcure training data of DPLM-2, consisting of PDB and SwissProt, is smaller and differs from UniRef50, which DPLM is pretrained on, potentially causing catastrophic forgetting and suboptimal representation. To test this, we conducted an experiment on the DeepLoc subcellular task, where  DPLM-2  underperforms compared to DPLM. As shown in Tab.  7 , without large-scale sequence pretraining,  DPLM-2  outperforms DPLM significantly, suggesting that: (1) Incorporating structure information enhances performance over sequence-only models. (2) Smaller datasets can lead to catastrophic forgetting, diminishing the benefits of large-scale pretraining. As result, to further improve the predictive performance, one deserving direction is to exploit larger-scale predicted structures in our future work.",
            "We evaluate  DPLM-2  in sequence-based, structure-based and co-generation ways. The overall illustration is shown in Fig.  6 ."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S4.T2.64.60",
        "footnotes": [],
        "references": [
            "Tab.  6  presents that  DPLM-2  shows further improvement compared to sequence-only methods (ESM2, DPLM) on some tasks, indicating that  DPLM-2  can leverage protein structures to generate better representations containing multimodal information for downstream tasks. However, we find that  DPLM-2  falls behind the state-of-the-art structure-aware protein LM,  i.e . , SaProt, in most tasks and even lags behind DPLM in certain tasks. We hypothesize this is because the strutcure training data of DPLM-2, consisting of PDB and SwissProt, is smaller and differs from UniRef50, which DPLM is pretrained on, potentially causing catastrophic forgetting and suboptimal representation. To test this, we conducted an experiment on the DeepLoc subcellular task, where  DPLM-2  underperforms compared to DPLM. As shown in Tab.  7 , without large-scale sequence pretraining,  DPLM-2  outperforms DPLM significantly, suggesting that: (1) Incorporating structure information enhances performance over sequence-only models. (2) Smaller datasets can lead to catastrophic forgetting, diminishing the benefits of large-scale pretraining. As result, to further improve the predictive performance, one deserving direction is to exploit larger-scale predicted structures in our future work."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": [
            "Tab.  8  shows that the  self-mixup  training strategy effectively enhances the diversity of samples. We attribute this to the model producing more accurate logits during inference, leading to more diverse reasonable sampling paths instead of converging on the sampling paths with the highest probability, which results in more diverse proteins."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S4.T4.3.1",
        "footnotes": [],
        "references": [
            "Tab.  9  presents the result of each motif-scaffolding problem.  DPLM-2  achieves the best average success rate in each evaluation. Compared with ESM3,  DPLM-2  shows better results in 12 problems in co-generation evaluation and 10 problems in sequence-based evaluation. Meanwhile,  DPLM-2  outperforms RFDiffusion in 14 problems in structure-based evaluation. This demonstrates that  DPLM-2  can achieve strong performance under various evaluation methods."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "S4.T5.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S4.T6.9.7",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "S4.T7.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.T8.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A3.T9.1.1",
        "footnotes": [],
        "references": []
    }
}