{
    "S3.T1": {
        "caption": "Table 1: Accuracies obtained by using SkipRes and VBFusion with different number l𝑙l of layers (RSVQA-LR dataset).",
        "table": "<table id=\"S3.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.1.2\" class=\"ltx_td ltx_align_left ltx_align_bottom ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T1.3.1.2.1\" class=\"ltx_text\">Architecture</span></th>\n<th id=\"S3.T1.3.1.1\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T1.3.1.1.1\" class=\"ltx_text\"><math id=\"S3.T1.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"l\" display=\"inline\"><semantics id=\"S3.T1.3.1.1.1.m1.1a\"><mi id=\"S3.T1.3.1.1.1.m1.1.1\" xref=\"S3.T1.3.1.1.1.m1.1.1.cmml\">l</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.3.1.1.1.m1.1b\"><ci id=\"S3.T1.3.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.3.1.1.1.m1.1.1\">𝑙</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.3.1.1.1.m1.1c\">l</annotation></semantics></math></span></th>\n<th id=\"S3.T1.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\">Question Type</th>\n<th id=\"S3.T1.3.1.4\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T1.3.1.4.1\" class=\"ltx_text\">AA</span></th>\n<th id=\"S3.T1.3.1.5\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T1.3.1.5.1\" class=\"ltx_text\">OA</span></th>\n</tr>\n<tr id=\"S3.T1.3.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Count</th>\n<th id=\"S3.T1.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Presence</th>\n<th id=\"S3.T1.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Comparison</th>\n<th id=\"S3.T1.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Rural/Urban</th>\n</tr>\n<tr id=\"S3.T1.3.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">SkipRes [<cite class=\"ltx_cite ltx_citemacro_citet\"><a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a></cite>]</th>\n<th id=\"S3.T1.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\">–</th>\n<th id=\"S3.T1.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">67.01</th>\n<th id=\"S3.T1.3.3.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">87.46</th>\n<th id=\"S3.T1.3.3.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">81.50</th>\n<th id=\"S3.T1.3.3.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T1.3.3.2.6.1\" class=\"ltx_text ltx_font_bold\">90.00</span></th>\n<th id=\"S3.T1.3.3.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">81.49</th>\n<th id=\"S3.T1.3.3.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">79.08</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.3.4.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row ltx_border_bb\" rowspan=\"5\"><span id=\"S3.T1.3.4.1.1.1\" class=\"ltx_text\">VBFusion</span></th>\n<th id=\"S3.T1.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">4</th>\n<td id=\"S3.T1.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">68.17</td>\n<td id=\"S3.T1.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">88.02</td>\n<td id=\"S3.T1.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">88.51</td>\n<td id=\"S3.T1.3.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">87.00</td>\n<td id=\"S3.T1.3.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">82.92</td>\n<td id=\"S3.T1.3.4.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">82.36</td>\n</tr>\n<tr id=\"S3.T1.3.5.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">6</th>\n<td id=\"S3.T1.3.5.2.2\" class=\"ltx_td ltx_align_center\">68.17</td>\n<td id=\"S3.T1.3.5.2.3\" class=\"ltx_td ltx_align_center\">88.87</td>\n<td id=\"S3.T1.3.5.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.3.5.2.4.1\" class=\"ltx_text ltx_font_bold\">88.83</span></td>\n<td id=\"S3.T1.3.5.2.5\" class=\"ltx_td ltx_align_center\">84.00</td>\n<td id=\"S3.T1.3.5.2.6\" class=\"ltx_td ltx_align_center\">82.47</td>\n<td id=\"S3.T1.3.5.2.7\" class=\"ltx_td ltx_align_center\">82.71</td>\n</tr>\n<tr id=\"S3.T1.3.6.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.6.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">8</th>\n<td id=\"S3.T1.3.6.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.3.6.3.2.1\" class=\"ltx_text ltx_font_bold\">69.36</span></td>\n<td id=\"S3.T1.3.6.3.3\" class=\"ltx_td ltx_align_center\">89.00</td>\n<td id=\"S3.T1.3.6.3.4\" class=\"ltx_td ltx_align_center\">83.46</td>\n<td id=\"S3.T1.3.6.3.5\" class=\"ltx_td ltx_align_center\">88.00</td>\n<td id=\"S3.T1.3.6.3.6\" class=\"ltx_td ltx_align_center\">82.45</td>\n<td id=\"S3.T1.3.6.3.7\" class=\"ltx_td ltx_align_center\">80.99</td>\n</tr>\n<tr id=\"S3.T1.3.7.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.7.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S3.T1.3.7.4.2\" class=\"ltx_td ltx_align_center\">67.73</td>\n<td id=\"S3.T1.3.7.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.3.7.4.3.1\" class=\"ltx_text ltx_font_bold\">89.48</span></td>\n<td id=\"S3.T1.3.7.4.4\" class=\"ltx_td ltx_align_center\">86.68</td>\n<td id=\"S3.T1.3.7.4.5\" class=\"ltx_td ltx_align_center\">88.00</td>\n<td id=\"S3.T1.3.7.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.3.7.4.6.1\" class=\"ltx_text ltx_font_bold\">82.97</span></td>\n<td id=\"S3.T1.3.7.4.7\" class=\"ltx_td ltx_align_center\">81.94</td>\n</tr>\n<tr id=\"S3.T1.3.8.5\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.8.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">12</th>\n<td id=\"S3.T1.3.8.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">68.14</td>\n<td id=\"S3.T1.3.8.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">89.27</td>\n<td id=\"S3.T1.3.8.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">88.71</td>\n<td id=\"S3.T1.3.8.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">85.00</td>\n<td id=\"S3.T1.3.8.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">82.78</td>\n<td id=\"S3.T1.3.8.5.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T1.3.8.5.7.1\" class=\"ltx_text ltx_font_bold\">82.78</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table 1 shows the results obtained on the RSVQA-LR dataset. By analyzing the table, one can see that our proposed VBFusion architecture outperforms the SkipRes model in all metrics except “Rural/Urban”, independently of the number of layers.\nThe most significant difference occurs in the “Comparison” accuracy, which represents the most challenging question type.\nHere, a model is required to count the number of occurrences of a selected object with a specific positional relationship to a reference object.\nFor this type of question, our 6-layer VBFusion architecture significantly outperforms SkipRes by more than 7 %times7percent7\\text{\\,}\\mathrm{\\char 37\\relax}.\nFor the “Count” questions the differences are smaller, e.g., VBFusion with 10 layers outperforms the SkipRes model by only 0.7 %times0.7percent0.7\\text{\\,}\\mathrm{\\char 37\\relax}.\nIn the summarizing metrics “AA”, and “OA”, all VBFusion models, independently of the number of layers, significantly outperform SkipRes.\nFurthermore, no significant improvement is observed when increasing the number l𝑙l of VisualBert layers.\nAlthough complexity increases, when using more layers, performance changes only slightly.\nThis observation aligns with the findings of [22] that large transformers struggle on small datasets.\nTo benefit from the larger transformer configurations, more samples are probably required.\nIn the case of the “Rural/Urban” questions, the underperformance can be explained with the same argument; there are too few questions of this type.\nThis is because the “Rural/Urban” question type makes up the smallest proportion of the dataset (1 %times1percent1\\text{\\,}\\mathrm{\\char 37\\relax}).\nBy analyzing the results, one can conclude that the proposed VBFusion architecture is able to improve the performance compared to the SkipRes architecture. It is worth emphasizing that the datasets used in the experiments are benchmarks, whereas in many real applications the VQA is expected to be applied to much larger archives. Due to the nature of the large transformer-based architecture, we expect that the gain achieved by the VBFusion architecture will be increased for large-scale RS VQA problems and also for the cases of when much more complex question types are present."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Accuracies obtained by using SkipRes and VBFusion with different number l𝑙l of layers (RSVQAxBEN dataset).",
        "table": "<table id=\"S4.T2.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.1.2\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.2.1\" class=\"ltx_text\">Architecture</span></th>\n<th id=\"S4.T2.3.1.1\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.1.1\" class=\"ltx_text\"><math id=\"S4.T2.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"l\" display=\"inline\"><semantics id=\"S4.T2.3.1.1.1.m1.1a\"><mi id=\"S4.T2.3.1.1.1.m1.1.1\" xref=\"S4.T2.3.1.1.1.m1.1.1.cmml\">l</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.3.1.1.1.m1.1b\"><ci id=\"S4.T2.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.3.1.1.1.m1.1.1\">𝑙</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.3.1.1.1.m1.1c\">l</annotation></semantics></math></span></th>\n<th id=\"S4.T2.3.1.3\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.3.1\" class=\"ltx_text\">Number of Bands</span></th>\n<td id=\"S4.T2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Question Type</td>\n<td id=\"S4.T2.3.1.5\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.5.1\" class=\"ltx_text\">AA</span></td>\n<td id=\"S4.T2.3.1.6\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.6.1\" class=\"ltx_text\">OA</span></td>\n</tr>\n<tr id=\"S4.T2.3.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">LULC</td>\n<td id=\"S4.T2.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Yes/No</td>\n</tr>\n<tr id=\"S4.T2.3.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">SkipRes [<cite class=\"ltx_cite ltx_citemacro_citet\"><a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a></cite>]</th>\n<th id=\"S4.T2.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">–</th>\n<th id=\"S4.T2.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">20.68</td>\n<td id=\"S4.T2.3.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">80.02</td>\n<td id=\"S4.T2.3.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">50.35</td>\n<td id=\"S4.T2.3.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">69.92</td>\n</tr>\n<tr id=\"S4.T2.3.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" rowspan=\"10\"><span id=\"S4.T2.3.4.3.1.1\" class=\"ltx_text\">VBFusion</span></th>\n<th id=\"S4.T2.3.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.3.4.3.2.1\" class=\"ltx_text\">4</span></th>\n<th id=\"S4.T2.3.4.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">20.40</td>\n<td id=\"S4.T2.3.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">83.86</td>\n<td id=\"S4.T2.3.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">52.13</td>\n<td id=\"S4.T2.3.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">73.06</td>\n</tr>\n<tr id=\"S4.T2.3.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S4.T2.3.5.4.2\" class=\"ltx_td ltx_align_center\">25.72</td>\n<td id=\"S4.T2.3.5.4.3\" class=\"ltx_td ltx_align_center\">85.41</td>\n<td id=\"S4.T2.3.5.4.4\" class=\"ltx_td ltx_align_center\">55.56</td>\n<td id=\"S4.T2.3.5.4.5\" class=\"ltx_td ltx_align_center\">75.26</td>\n</tr>\n<tr id=\"S4.T2.3.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span id=\"S4.T2.3.6.5.1.1\" class=\"ltx_text\">6</span></th>\n<th id=\"S4.T2.3.6.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">21.66</td>\n<td id=\"S4.T2.3.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">84.58</td>\n<td id=\"S4.T2.3.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">53.12</td>\n<td id=\"S4.T2.3.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\">73.88</td>\n</tr>\n<tr id=\"S4.T2.3.7.6\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S4.T2.3.7.6.2\" class=\"ltx_td ltx_align_center\">25.88</td>\n<td id=\"S4.T2.3.7.6.3\" class=\"ltx_td ltx_align_center\">85.48</td>\n<td id=\"S4.T2.3.7.6.4\" class=\"ltx_td ltx_align_center\">55.68</td>\n<td id=\"S4.T2.3.7.6.5\" class=\"ltx_td ltx_align_center\">75.34</td>\n</tr>\n<tr id=\"S4.T2.3.8.7\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span id=\"S4.T2.3.8.7.1.1\" class=\"ltx_text\">8</span></th>\n<th id=\"S4.T2.3.8.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">20.07</td>\n<td id=\"S4.T2.3.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">84.37</td>\n<td id=\"S4.T2.3.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\">52.22</td>\n<td id=\"S4.T2.3.8.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\">73.43</td>\n</tr>\n<tr id=\"S4.T2.3.9.8\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S4.T2.3.9.8.2\" class=\"ltx_td ltx_align_center\">25.04</td>\n<td id=\"S4.T2.3.9.8.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.3.9.8.3.1\" class=\"ltx_text ltx_font_bold\">86.56</span></td>\n<td id=\"S4.T2.3.9.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.3.9.8.4.1\" class=\"ltx_text ltx_font_bold\">55.80</span></td>\n<td id=\"S4.T2.3.9.8.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.3.9.8.5.1\" class=\"ltx_text ltx_font_bold\">76.10</span></td>\n</tr>\n<tr id=\"S4.T2.3.10.9\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span id=\"S4.T2.3.10.9.1.1\" class=\"ltx_text\">10</span></th>\n<th id=\"S4.T2.3.10.9.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\">20.82</td>\n<td id=\"S4.T2.3.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\">85.13</td>\n<td id=\"S4.T2.3.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\">52.97</td>\n<td id=\"S4.T2.3.10.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\">74.19</td>\n</tr>\n<tr id=\"S4.T2.3.11.10\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S4.T2.3.11.10.2\" class=\"ltx_td ltx_align_center\">25.19</td>\n<td id=\"S4.T2.3.11.10.3\" class=\"ltx_td ltx_align_center\">85.95</td>\n<td id=\"S4.T2.3.11.10.4\" class=\"ltx_td ltx_align_center\">55.57</td>\n<td id=\"S4.T2.3.11.10.5\" class=\"ltx_td ltx_align_center\">75.61</td>\n</tr>\n<tr id=\"S4.T2.3.12.11\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.12.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" rowspan=\"2\"><span id=\"S4.T2.3.12.11.1.1\" class=\"ltx_text\">12</span></th>\n<th id=\"S4.T2.3.12.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.12.11.3\" class=\"ltx_td ltx_align_center ltx_border_t\">24.33</td>\n<td id=\"S4.T2.3.12.11.4\" class=\"ltx_td ltx_align_center ltx_border_t\">85.47</td>\n<td id=\"S4.T2.3.12.11.5\" class=\"ltx_td ltx_align_center ltx_border_t\">54.90</td>\n<td id=\"S4.T2.3.12.11.6\" class=\"ltx_td ltx_align_center ltx_border_t\">75.07</td>\n</tr>\n<tr id=\"S4.T2.3.13.12\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.13.12.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">10</th>\n<td id=\"S4.T2.3.13.12.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.3.13.12.2.1\" class=\"ltx_text ltx_font_bold\">26.26</span></td>\n<td id=\"S4.T2.3.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">85.34</td>\n<td id=\"S4.T2.3.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.3.13.12.4.1\" class=\"ltx_text ltx_font_bold\">55.80</span></td>\n<td id=\"S4.T2.3.13.12.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">75.29</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table 2 reports the accuracies for the three- and ten-band RSVQAxBEN dataset variants.\nBy analyzing the tables one can observe that the proposed architecture VBFusion outperforms the SkipRes architecture, irrespective of the layer configuration, in both summarizing metrics “AA” and “OA”.\nEven the smallest configuration with 4 layers trained on three bands improves the “AA” by almost 2 %times2percent2\\text{\\,}\\mathrm{\\char 37\\relax} and the “OA” by more than 3 %times3percent3\\text{\\,}\\mathrm{\\char 37\\relax}.\nWhen we compare the performance of the proposed architecture trained on the three-band variant, most of the improvements are attributed to the “Yes/No” related questions.\nWe can also see that in the 3-band scenario, deeper and larger VBFusion models lead to higher “OA” scores.\nThis pattern is in contrast to the generally similar performance in the “LULC” question category among the layer configurations up to l=10𝑙10l=10 trained on three bands.\nInterestingly, a performance jump can be observed for the “LULC” questions with the largest layer configuration (l=12𝑙12l=12).\nA possible reason is that with 12 layers, all pre-trained VisualBERT layers are utilized and none are rejected.\nTherefore, the initial weights can produce more meaningful high-level embeddings than the pruned models.\nThis seems to be especially important for the complex “LULC” questions, where the model is required to deeply understand the contents of intricate multispectral images and connect them to the associated question.\nThe performance improvement exists for the three- and ten-band variants, although it is more dominant in the three-band configuration.\nWith all 12 layers, the “LULC” accuracy increases by 3.5 %times3.5percent3.5\\text{\\,}\\mathrm{\\char 37\\relax}, compared to the 10 layer configuration, for the three-band training leading to a high “OA” of 75.07 %times75.07percent75.07\\text{\\,}\\mathrm{\\char 37\\relax}.\nThe effect is not as significant for the 10-band configuration.\nThe relatively low impact on the 10-band configuration can be due to a discrepancy between the high-level RGB image feature representation and the ten-band feature representation from the initial weights of the pre-trained VisualBERT model, since VisualBERT was pre-trained on RGB images.\nHowever, analyzing the results for all 10-band configurations, one can observe that these models improve the performance in all metrics compared to their three-band counterparts, except for the 12-layer configuration in the “Yes/No” category, where the accuracy is slightly lower by less than 0.2 %times0.2percent0.2\\text{\\,}\\mathrm{\\char 37\\relax}.\nMost of the observed performance increase is due to the notably higher accuracies in the “LULC” category.\nFor example, the smallest configuration trained on 10 bands with l=4𝑙4l=4 is more than 5 %times5percent5\\text{\\,}\\mathrm{\\char 37\\relax} better than the same architecture trained on the 3 band variant in the “LULC” category.\nAn improved “LULC” performance is expected, since some LULC classes, such as different types of vegetation, greatly benefit from additional spectral information.\nThe smallest model trained on ten bands even outperforms the largest and best-performing configuration trained on three bands by 0.6 %times0.6percent0.6\\text{\\,}\\mathrm{\\char 37\\relax} in “AA” and almost by 0.2 %times0.2percent0.2\\text{\\,}\\mathrm{\\char 37\\relax} in “OA”.\nWhen training with ten bands, it can be observed that larger configurations of our proposed VBFusion architecture do not necessarily lead to higher accuracies.\nThe best performing ten-band trained configuration utilizes 8 layers and reaches an “OA” of 76.10 %times76.10percent76.10\\text{\\,}\\mathrm{\\char 37\\relax}, while the full 12 layer configuration has the second lowest “OA” with 75.29 %times75.29percent75.29\\text{\\,}\\mathrm{\\char 37\\relax}.\nBy analyzing the results, one can conclude that the proposed VBFusion architecture discovers the underlying relationship between both the image and question modality better than models that utilize a simple feature combination as the fusion module.\nFurthermore, the results show the importance of utilizing additional spectral bands, when available, to better model the contents of intricate multispectral imagery for VQA systems."
        ]
    }
}