{
    "S3.T1": {
        "caption": "Table 1: Accuracies obtained by using SkipRes and VBFusion with different number lğ‘™l of layers (RSVQA-LR dataset).",
        "table": "<table id=\"S3.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.1.2\" class=\"ltx_td ltx_align_left ltx_align_bottom ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T1.3.1.2.1\" class=\"ltx_text\">Architecture</span></th>\n<th id=\"S3.T1.3.1.1\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_column ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T1.3.1.1.1\" class=\"ltx_text\"><math id=\"S3.T1.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"l\" display=\"inline\"><semantics id=\"S3.T1.3.1.1.1.m1.1a\"><mi id=\"S3.T1.3.1.1.1.m1.1.1\" xref=\"S3.T1.3.1.1.1.m1.1.1.cmml\">l</mi><annotation-xml encoding=\"MathML-Content\" id=\"S3.T1.3.1.1.1.m1.1b\"><ci id=\"S3.T1.3.1.1.1.m1.1.1.cmml\" xref=\"S3.T1.3.1.1.1.m1.1.1\">ğ‘™</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T1.3.1.1.1.m1.1c\">l</annotation></semantics></math></span></th>\n<th id=\"S3.T1.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\">Question Type</th>\n<th id=\"S3.T1.3.1.4\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T1.3.1.4.1\" class=\"ltx_text\">AA</span></th>\n<th id=\"S3.T1.3.1.5\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_column ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T1.3.1.5.1\" class=\"ltx_text\">OA</span></th>\n</tr>\n<tr id=\"S3.T1.3.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Count</th>\n<th id=\"S3.T1.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Presence</th>\n<th id=\"S3.T1.3.2.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Comparison</th>\n<th id=\"S3.T1.3.2.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">Rural/Urban</th>\n</tr>\n<tr id=\"S3.T1.3.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">SkipRes [<cite class=\"ltx_cite ltx_citemacro_citet\"><a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a></cite>]</th>\n<th id=\"S3.T1.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_t\">â€“</th>\n<th id=\"S3.T1.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">67.01</th>\n<th id=\"S3.T1.3.3.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">87.46</th>\n<th id=\"S3.T1.3.3.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">81.50</th>\n<th id=\"S3.T1.3.3.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T1.3.3.2.6.1\" class=\"ltx_text ltx_font_bold\">90.00</span></th>\n<th id=\"S3.T1.3.3.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">81.49</th>\n<th id=\"S3.T1.3.3.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">79.08</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.3.4.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.4.1.1\" class=\"ltx_td ltx_align_left ltx_align_middle ltx_th ltx_th_row ltx_border_bb\" rowspan=\"5\"><span id=\"S3.T1.3.4.1.1.1\" class=\"ltx_text\">VBFusion</span></th>\n<th id=\"S3.T1.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">4</th>\n<td id=\"S3.T1.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">68.17</td>\n<td id=\"S3.T1.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">88.02</td>\n<td id=\"S3.T1.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">88.51</td>\n<td id=\"S3.T1.3.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">87.00</td>\n<td id=\"S3.T1.3.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">82.92</td>\n<td id=\"S3.T1.3.4.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">82.36</td>\n</tr>\n<tr id=\"S3.T1.3.5.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">6</th>\n<td id=\"S3.T1.3.5.2.2\" class=\"ltx_td ltx_align_center\">68.17</td>\n<td id=\"S3.T1.3.5.2.3\" class=\"ltx_td ltx_align_center\">88.87</td>\n<td id=\"S3.T1.3.5.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.3.5.2.4.1\" class=\"ltx_text ltx_font_bold\">88.83</span></td>\n<td id=\"S3.T1.3.5.2.5\" class=\"ltx_td ltx_align_center\">84.00</td>\n<td id=\"S3.T1.3.5.2.6\" class=\"ltx_td ltx_align_center\">82.47</td>\n<td id=\"S3.T1.3.5.2.7\" class=\"ltx_td ltx_align_center\">82.71</td>\n</tr>\n<tr id=\"S3.T1.3.6.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.6.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">8</th>\n<td id=\"S3.T1.3.6.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.3.6.3.2.1\" class=\"ltx_text ltx_font_bold\">69.36</span></td>\n<td id=\"S3.T1.3.6.3.3\" class=\"ltx_td ltx_align_center\">89.00</td>\n<td id=\"S3.T1.3.6.3.4\" class=\"ltx_td ltx_align_center\">83.46</td>\n<td id=\"S3.T1.3.6.3.5\" class=\"ltx_td ltx_align_center\">88.00</td>\n<td id=\"S3.T1.3.6.3.6\" class=\"ltx_td ltx_align_center\">82.45</td>\n<td id=\"S3.T1.3.6.3.7\" class=\"ltx_td ltx_align_center\">80.99</td>\n</tr>\n<tr id=\"S3.T1.3.7.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.7.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S3.T1.3.7.4.2\" class=\"ltx_td ltx_align_center\">67.73</td>\n<td id=\"S3.T1.3.7.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.3.7.4.3.1\" class=\"ltx_text ltx_font_bold\">89.48</span></td>\n<td id=\"S3.T1.3.7.4.4\" class=\"ltx_td ltx_align_center\">86.68</td>\n<td id=\"S3.T1.3.7.4.5\" class=\"ltx_td ltx_align_center\">88.00</td>\n<td id=\"S3.T1.3.7.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T1.3.7.4.6.1\" class=\"ltx_text ltx_font_bold\">82.97</span></td>\n<td id=\"S3.T1.3.7.4.7\" class=\"ltx_td ltx_align_center\">81.94</td>\n</tr>\n<tr id=\"S3.T1.3.8.5\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.8.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">12</th>\n<td id=\"S3.T1.3.8.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">68.14</td>\n<td id=\"S3.T1.3.8.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">89.27</td>\n<td id=\"S3.T1.3.8.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">88.71</td>\n<td id=\"S3.T1.3.8.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">85.00</td>\n<td id=\"S3.T1.3.8.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">82.78</td>\n<td id=\"S3.T1.3.8.5.7\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T1.3.8.5.7.1\" class=\"ltx_text ltx_font_bold\">82.78</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "TableÂ 1 shows the results obtained on the RSVQA-LR dataset. By analyzing the table, one can see that our proposed VBFusion architecture outperforms the SkipRes model in all metrics except â€œRural/Urbanâ€, independently of the number of layers.\nThe most significant difference occurs in the â€œComparisonâ€ accuracy, which represents the most challenging question type.\nHere, a model is required to count the number of occurrences of a selected object with a specific positional relationship to a reference object.\nFor this type of question, our 6-layer VBFusion architecture significantly outperforms SkipRes by more than 7Â %times7percent7\\text{\\,}\\mathrm{\\char 37\\relax}.\nFor the â€œCountâ€ questions the differences are smaller, e.g., VBFusion with 10 layers outperforms the SkipRes model by only 0.7Â %times0.7percent0.7\\text{\\,}\\mathrm{\\char 37\\relax}.\nIn the summarizing metrics â€œAAâ€, and â€œOAâ€, all VBFusion models, independently of the number of layers, significantly outperform SkipRes.\nFurthermore, no significant improvement is observed when increasing the number lğ‘™l of VisualBert layers.\nAlthough complexity increases, when using more layers, performance changes only slightly.\nThis observation aligns with the findings of [22] that large transformers struggle on small datasets.\nTo benefit from the larger transformer configurations, more samples are probably required.\nIn the case of the â€œRural/Urbanâ€ questions, the underperformance can be explained with the same argument; there are too few questions of this type.\nThis is because the â€œRural/Urbanâ€ question type makes up the smallest proportion of the dataset (1Â %times1percent1\\text{\\,}\\mathrm{\\char 37\\relax}).\nBy analyzing the results, one can conclude that the proposed VBFusion architecture is able to improve the performance compared to the SkipRes architecture. It is worth emphasizing that the datasets used in the experiments are benchmarks, whereas in many real applications the VQA is expected to be applied to much larger archives. Due to the nature of the large transformer-based architecture, we expect that the gain achieved by the VBFusion architecture will be increased for large-scale RS VQA problems and also for the cases of when much more complex question types are present."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Accuracies obtained by using SkipRes and VBFusion with different number lğ‘™l of layers (RSVQAxBEN dataset).",
        "table": "<table id=\"S4.T2.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.1.2\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.2.1\" class=\"ltx_text\">Architecture</span></th>\n<th id=\"S4.T2.3.1.1\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.1.1\" class=\"ltx_text\"><math id=\"S4.T2.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"l\" display=\"inline\"><semantics id=\"S4.T2.3.1.1.1.m1.1a\"><mi id=\"S4.T2.3.1.1.1.m1.1.1\" xref=\"S4.T2.3.1.1.1.m1.1.1.cmml\">l</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.3.1.1.1.m1.1b\"><ci id=\"S4.T2.3.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.3.1.1.1.m1.1.1\">ğ‘™</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.3.1.1.1.m1.1c\">l</annotation></semantics></math></span></th>\n<th id=\"S4.T2.3.1.3\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_th ltx_th_row ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.3.1\" class=\"ltx_text\">Number of Bands</span></th>\n<td id=\"S4.T2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\">Question Type</td>\n<td id=\"S4.T2.3.1.5\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.5.1\" class=\"ltx_text\">AA</span></td>\n<td id=\"S4.T2.3.1.6\" class=\"ltx_td ltx_align_center ltx_align_bottom ltx_border_tt\" rowspan=\"2\"><span id=\"S4.T2.3.1.6.1\" class=\"ltx_text\">OA</span></td>\n</tr>\n<tr id=\"S4.T2.3.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.3.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">LULC</td>\n<td id=\"S4.T2.3.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Yes/No</td>\n</tr>\n<tr id=\"S4.T2.3.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">SkipRes [<cite class=\"ltx_cite ltx_citemacro_citet\"><a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a></cite>]</th>\n<th id=\"S4.T2.3.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">â€“</th>\n<th id=\"S4.T2.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">20.68</td>\n<td id=\"S4.T2.3.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">80.02</td>\n<td id=\"S4.T2.3.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">50.35</td>\n<td id=\"S4.T2.3.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\">69.92</td>\n</tr>\n<tr id=\"S4.T2.3.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" rowspan=\"10\"><span id=\"S4.T2.3.4.3.1.1\" class=\"ltx_text\">VBFusion</span></th>\n<th id=\"S4.T2.3.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" rowspan=\"2\"><span id=\"S4.T2.3.4.3.2.1\" class=\"ltx_text\">4</span></th>\n<th id=\"S4.T2.3.4.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">20.40</td>\n<td id=\"S4.T2.3.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">83.86</td>\n<td id=\"S4.T2.3.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">52.13</td>\n<td id=\"S4.T2.3.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_t\">73.06</td>\n</tr>\n<tr id=\"S4.T2.3.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S4.T2.3.5.4.2\" class=\"ltx_td ltx_align_center\">25.72</td>\n<td id=\"S4.T2.3.5.4.3\" class=\"ltx_td ltx_align_center\">85.41</td>\n<td id=\"S4.T2.3.5.4.4\" class=\"ltx_td ltx_align_center\">55.56</td>\n<td id=\"S4.T2.3.5.4.5\" class=\"ltx_td ltx_align_center\">75.26</td>\n</tr>\n<tr id=\"S4.T2.3.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.6.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span id=\"S4.T2.3.6.5.1.1\" class=\"ltx_text\">6</span></th>\n<th id=\"S4.T2.3.6.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">21.66</td>\n<td id=\"S4.T2.3.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">84.58</td>\n<td id=\"S4.T2.3.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">53.12</td>\n<td id=\"S4.T2.3.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\">73.88</td>\n</tr>\n<tr id=\"S4.T2.3.7.6\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.7.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S4.T2.3.7.6.2\" class=\"ltx_td ltx_align_center\">25.88</td>\n<td id=\"S4.T2.3.7.6.3\" class=\"ltx_td ltx_align_center\">85.48</td>\n<td id=\"S4.T2.3.7.6.4\" class=\"ltx_td ltx_align_center\">55.68</td>\n<td id=\"S4.T2.3.7.6.5\" class=\"ltx_td ltx_align_center\">75.34</td>\n</tr>\n<tr id=\"S4.T2.3.8.7\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.8.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span id=\"S4.T2.3.8.7.1.1\" class=\"ltx_text\">8</span></th>\n<th id=\"S4.T2.3.8.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">20.07</td>\n<td id=\"S4.T2.3.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">84.37</td>\n<td id=\"S4.T2.3.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\">52.22</td>\n<td id=\"S4.T2.3.8.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\">73.43</td>\n</tr>\n<tr id=\"S4.T2.3.9.8\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.9.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S4.T2.3.9.8.2\" class=\"ltx_td ltx_align_center\">25.04</td>\n<td id=\"S4.T2.3.9.8.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.3.9.8.3.1\" class=\"ltx_text ltx_font_bold\">86.56</span></td>\n<td id=\"S4.T2.3.9.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.3.9.8.4.1\" class=\"ltx_text ltx_font_bold\">55.80</span></td>\n<td id=\"S4.T2.3.9.8.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.3.9.8.5.1\" class=\"ltx_text ltx_font_bold\">76.10</span></td>\n</tr>\n<tr id=\"S4.T2.3.10.9\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.10.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" rowspan=\"2\"><span id=\"S4.T2.3.10.9.1.1\" class=\"ltx_text\">10</span></th>\n<th id=\"S4.T2.3.10.9.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_t\">20.82</td>\n<td id=\"S4.T2.3.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\">85.13</td>\n<td id=\"S4.T2.3.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_t\">52.97</td>\n<td id=\"S4.T2.3.10.9.6\" class=\"ltx_td ltx_align_center ltx_border_t\">74.19</td>\n</tr>\n<tr id=\"S4.T2.3.11.10\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.11.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\">10</th>\n<td id=\"S4.T2.3.11.10.2\" class=\"ltx_td ltx_align_center\">25.19</td>\n<td id=\"S4.T2.3.11.10.3\" class=\"ltx_td ltx_align_center\">85.95</td>\n<td id=\"S4.T2.3.11.10.4\" class=\"ltx_td ltx_align_center\">55.57</td>\n<td id=\"S4.T2.3.11.10.5\" class=\"ltx_td ltx_align_center\">75.61</td>\n</tr>\n<tr id=\"S4.T2.3.12.11\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.12.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\" rowspan=\"2\"><span id=\"S4.T2.3.12.11.1.1\" class=\"ltx_text\">12</span></th>\n<th id=\"S4.T2.3.12.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\">3</th>\n<td id=\"S4.T2.3.12.11.3\" class=\"ltx_td ltx_align_center ltx_border_t\">24.33</td>\n<td id=\"S4.T2.3.12.11.4\" class=\"ltx_td ltx_align_center ltx_border_t\">85.47</td>\n<td id=\"S4.T2.3.12.11.5\" class=\"ltx_td ltx_align_center ltx_border_t\">54.90</td>\n<td id=\"S4.T2.3.12.11.6\" class=\"ltx_td ltx_align_center ltx_border_t\">75.07</td>\n</tr>\n<tr id=\"S4.T2.3.13.12\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.13.12.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\">10</th>\n<td id=\"S4.T2.3.13.12.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.3.13.12.2.1\" class=\"ltx_text ltx_font_bold\">26.26</span></td>\n<td id=\"S4.T2.3.13.12.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">85.34</td>\n<td id=\"S4.T2.3.13.12.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.3.13.12.4.1\" class=\"ltx_text ltx_font_bold\">55.80</span></td>\n<td id=\"S4.T2.3.13.12.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">75.29</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "TableÂ 2 reports the accuracies for the three- and ten-band RSVQAxBEN dataset variants.\nBy analyzing the tables one can observe that the proposed architecture VBFusion outperforms the SkipRes architecture, irrespective of the layer configuration, in both summarizing metrics â€œAAâ€ and â€œOAâ€.\nEven the smallest configuration with 4 layers trained on three bands improves the â€œAAâ€ by almost 2Â %times2percent2\\text{\\,}\\mathrm{\\char 37\\relax} and the â€œOAâ€ by more than 3Â %times3percent3\\text{\\,}\\mathrm{\\char 37\\relax}.\nWhen we compare the performance of the proposed architecture trained on the three-band variant, most of the improvements are attributed to the â€œYes/Noâ€ related questions.\nWe can also see that in the 3-band scenario, deeper and larger VBFusion models lead to higher â€œOAâ€ scores.\nThis pattern is in contrast to the generally similar performance in the â€œLULCâ€ question category among the layer configurations up to l=10ğ‘™10l=10 trained on three bands.\nInterestingly, a performance jump can be observed for the â€œLULCâ€ questions with the largest layer configuration (l=12ğ‘™12l=12).\nA possible reason is that with 12 layers, all pre-trained VisualBERT layers are utilized and none are rejected.\nTherefore, the initial weights can produce more meaningful high-level embeddings than the pruned models.\nThis seems to be especially important for the complex â€œLULCâ€ questions, where the model is required to deeply understand the contents of intricate multispectral images and connect them to the associated question.\nThe performance improvement exists for the three- and ten-band variants, although it is more dominant in the three-band configuration.\nWith all 12 layers, the â€œLULCâ€ accuracy increases by 3.5Â %times3.5percent3.5\\text{\\,}\\mathrm{\\char 37\\relax}, compared to the 10 layer configuration, for the three-band training leading to a high â€œOAâ€ of 75.07Â %times75.07percent75.07\\text{\\,}\\mathrm{\\char 37\\relax}.\nThe effect is not as significant for the 10-band configuration.\nThe relatively low impact on the 10-band configuration can be due to a discrepancy between the high-level RGB image feature representation and the ten-band feature representation from the initial weights of the pre-trained VisualBERT model, since VisualBERT was pre-trained on RGB images.\nHowever, analyzing the results for all 10-band configurations, one can observe that these models improve the performance in all metrics compared to their three-band counterparts, except for the 12-layer configuration in the â€œYes/Noâ€ category, where the accuracy is slightly lower by less than 0.2Â %times0.2percent0.2\\text{\\,}\\mathrm{\\char 37\\relax}.\nMost of the observed performance increase is due to the notably higher accuracies in the â€œLULCâ€ category.\nFor example, the smallest configuration trained on 10 bands with l=4ğ‘™4l=4 is more than 5Â %times5percent5\\text{\\,}\\mathrm{\\char 37\\relax} better than the same architecture trained on the 3 band variant in the â€œLULCâ€ category.\nAn improved â€œLULCâ€ performance is expected, since some LULC classes, such as different types of vegetation, greatly benefit from additional spectral information.\nThe smallest model trained on ten bands even outperforms the largest and best-performing configuration trained on three bands by 0.6Â %times0.6percent0.6\\text{\\,}\\mathrm{\\char 37\\relax} in â€œAAâ€ and almost by 0.2Â %times0.2percent0.2\\text{\\,}\\mathrm{\\char 37\\relax} in â€œOAâ€.\nWhen training with ten bands, it can be observed that larger configurations of our proposed VBFusion architecture do not necessarily lead to higher accuracies.\nThe best performing ten-band trained configuration utilizes 8 layers and reaches an â€œOAâ€ of 76.10Â %times76.10percent76.10\\text{\\,}\\mathrm{\\char 37\\relax}, while the full 12 layer configuration has the second lowest â€œOAâ€ with 75.29Â %times75.29percent75.29\\text{\\,}\\mathrm{\\char 37\\relax}.\nBy analyzing the results, one can conclude that the proposed VBFusion architecture discovers the underlying relationship between both the image and question modality better than models that utilize a simple feature combination as the fusion module.\nFurthermore, the results show the importance of utilizing additional spectral bands, when available, to better model the contents of intricate multispectral imagery for VQA systems."
        ]
    }
}