{
    "id_table_1": {
        "caption": "Table 1.  Prompt example for task description decomposition.",
        "table": "S2.T1.3",
        "footnotes": [],
        "references": [
            "Given a high-level task description, we propose an automated approach to break down the actions and execute them on the dynamic UI to trigger the app activity for testing.  The overview of our approach  CAT  is shown in Figure  1 , which is divided into two main phases: (i) the  Task Description Decomposition  phase, which decomposes task description into multiple potential action steps, including action types, target elements, and input values;  (ii) the  UI Automation Execution  phase which aligns the actions with the dynamic UI elements to accomplish UI automation.",
            "A LLMs prompt example to generate potential actions for task description is shown in Table  1 , including  Instructions  +  One-shot Example  +  Testing Task Description .  Specifically, we first instruct the LLMs to outline the objective goal, which is to dissect the task description into the potential sequence of actions.  According to a small pilot study, we retrieve the top-1 representative examples for few-shot learning (as known as one-shot learning), aiding in the recognition of industrial app usage and output patterns.  Next, we present the testing task description as the test prompt and ask for the decomposed actions.  Due to the advantage of instruction prompting and few-shot learning, the LLMs will consistently generate a numeric list to represent the sequence of actions in the same format as our example output, which can be inferred using regular expressions.",
            "Consider the target element ( e t  a  r  g  e  t subscript e t a r g e t e_{target} italic_e start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT ) in the executive actions and the elements in the current UI screen ( { e 1 , e 2 , ... , e n } subscript e 1 subscript e 2 ... subscript e n \\{e_{1},e_{2},...,e_{n}\\} { italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } ).  Similar to the previous methods in Section  2.1.1 , we first use the transformer-based model to encode the UI elements into vectors.  We then use the similarity measurement method to compute the lexical similarity between UI elements and the target element:  S i m i l a r i t y ( e n , e t  a  r  g  e  t ) ) = c o s ( E n c o d e ( e n ) , E n c o d e ( e t  a  r  g  e  t ) ) Similarity(e_{n},e_{target}))=cos(Encode(e_{n}),Encode(e_{target})) italic_S italic_i italic_m italic_i italic_l italic_a italic_r italic_i italic_t italic_y ( italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT ) ) = italic_c italic_o italic_s ( italic_E italic_n italic_c italic_o italic_d italic_e ( italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_E italic_n italic_c italic_o italic_d italic_e ( italic_e start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT ) ) .  To match the most similar UI element on the UI screen to the target element, we identify the UI element with the highest similarity value.  Additionally, we establish a threshold value to ascertain whether the target element is matched, or if it may contain semantic mismatches, which would necessitate optimization by LLMs.",
            "Baselines.   We set up four ablation studies as baselines to compare with our approach.  Given that the  CAT  comprises two main phases, we perform variations of our approach for each phase.  In Section  2.1 , we introduce a retrieval-based method to select the top-1 examples to be used in the prompt for few-shot learning.  Thus, we examine the search space of no examples ( 0-shot RAG ) and N selected examples ( N-shot RAG ).",
            "Experimental Setup.   To answer RQ2, we evaluate the comparison of our approach to the state-of-the-art baselines.  We also use the experimental dataset collected in RQ1 (Section  3.1 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Prompt example for UI element mapping.",
        "table": "S2.T2.2",
        "footnotes": [],
        "references": [
            "Consider the target element ( e t  a  r  g  e  t subscript e t a r g e t e_{target} italic_e start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT ) in the executive actions and the elements in the current UI screen ( { e 1 , e 2 , ... , e n } subscript e 1 subscript e 2 ... subscript e n \\{e_{1},e_{2},...,e_{n}\\} { italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , ... , italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT } ).  Similar to the previous methods in Section  2.1.1 , we first use the transformer-based model to encode the UI elements into vectors.  We then use the similarity measurement method to compute the lexical similarity between UI elements and the target element:  S i m i l a r i t y ( e n , e t  a  r  g  e  t ) ) = c o s ( E n c o d e ( e n ) , E n c o d e ( e t  a  r  g  e  t ) ) Similarity(e_{n},e_{target}))=cos(Encode(e_{n}),Encode(e_{target})) italic_S italic_i italic_m italic_i italic_l italic_a italic_r italic_i italic_t italic_y ( italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , italic_e start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT ) ) = italic_c italic_o italic_s ( italic_E italic_n italic_c italic_o italic_d italic_e ( italic_e start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ) , italic_E italic_n italic_c italic_o italic_d italic_e ( italic_e start_POSTSUBSCRIPT italic_t italic_a italic_r italic_g italic_e italic_t end_POSTSUBSCRIPT ) ) .  To match the most similar UI element on the UI screen to the target element, we identify the UI element with the highest similarity value.  Additionally, we establish a threshold value to ascertain whether the target element is matched, or if it may contain semantic mismatches, which would necessitate optimization by LLMs.",
            "LLMs are employed to specifically address occasional mismatches in UI elements.  Table  2  presents an example of the prompts given to LLMs.  The prompt begins with an instruction outlining the objective of identifying UI elements on the screen that are semantically related to the target element.  A challenge in using LLMs for UI element mapping is their limitation in processing large text inputs, while the UI representation (i.e., view hierarchy) is typically lengthy - averaging thousands of tokens for each UI.  Although recent LLMs can handle visual inputs such as UI screens, recent research  (Bubeck et al . ,  2023 )  has highlighted constraints in visual UI understanding.  To address this, we propose a heuristic method to simplify nested layouts and extract atomic elements by traversing the view hierarchy tree using a depth-first search.  Specifically, we iterate through each node, starting from the root of the view hierarchy, and remove layouts that contain only one node, continuing to search its child node.  With the simplified UI representation, we prompt the LLMs to identify which UI element is semantically related to the target element.",
            "Baselines.   We set up four ablation studies as baselines to compare with our approach.  Given that the  CAT  comprises two main phases, we perform variations of our approach for each phase.  In Section  2.1 , we introduce a retrieval-based method to select the top-1 examples to be used in the prompt for few-shot learning.  Thus, we examine the search space of no examples ( 0-shot RAG ) and N selected examples ( N-shot RAG ).",
            "In Section  2.2 , we present an optimizer that utilizes LLMs to address the issues of mismatched UI elements on the dynamic UI screen.  Thus, we consider a variant of  CAT  (no optimizer)  to compare the performance of our approach with and without LLMs as the complementary method.",
            "In Section  2.3 , we detail the implementation of our approach, employing ChatGPT as the LLMs. As a variant, we set up an ablation study using the open-sourced LLMs LLaMA70B  (Touvron et al . ,  2023 ) , referred to as  CAT  (LLaMA70B) ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Performance comparison of ablation studies.",
        "table": "S3.T3.3",
        "footnotes": [],
        "references": [
            "In Section  2.3 , we detail the implementation of our approach, employing ChatGPT as the LLMs. As a variant, we set up an ablation study using the open-sourced LLMs LLaMA70B  (Touvron et al . ,  2023 ) , referred to as  CAT  (LLaMA70B) .",
            "Results.   Table  3  illustrates the performance of  CAT  in executing UI automation for task descriptions.  Our approach achieves an average completion rate of 90% at a cost of $0.34, outperforming the ablation baselines.  We observe that the method excluding few-shot learning (0-shot RAG) only attains a 50% completion rate due to the reason that the LLMs lack of knowledge and experience with certain apps.  In comparison to 0-shot RAG, the use of few-shot examples can significantly help LLMs in understanding app usage knowledge, increasing the completion rate by 40% for 1-shot learning in our approach.  However, supplementing extensive examples (N-shot RAG) does not enhance the approachs performance.  This is because the longer context of the examples might lead the LLMs to display tendencies towards instruction forgetting, format errors, and abnormal reasoning.  Instead, it incurs a much higher cost, making it 150% more expensive than our approach.",
            "Experimental Setup.   To answer RQ2, we evaluate the comparison of our approach to the state-of-the-art baselines.  We also use the experimental dataset collected in RQ1 (Section  3.1 )."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Performance comparison of state-of-the-art.",
        "table": "S3.T4.3",
        "footnotes": [],
        "references": [
            "Results.   Table  4  presents the performance comparison with the baselines.  The machine learning-based method (Seq2Act), while not incurring any financial costs, only attains completion rates of 35%, which is 55% lower than our approach  CAT .  Among the baselines, AdbGPT exhibits the best performance, achieving a completion rate of 90%. However, it is costly, averaging $1.07 for UI automation due to the extensive use of LLMs.  In contrast, our approach,  CAT , which integrates machine-learning methods and LLMs optimally, saves $1,467 without compromising the completion rate."
        ]
    },
    "global_footnotes": [
        "WeChat is among the most popular messenger apps in the world with over 1.67 billion monthly active users."
    ]
}