{
    "PAPER'S NUMBER OF TABLES": 1,
    "S3.T1": {
        "caption": "TABLE I: Taxonomy of representative algorithms for DRL.",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\" colspan=\"2\"><span id=\"S3.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Types</span></td>\n<td id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\"><span id=\"S3.T1.1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Representative algorithms</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\" colspan=\"2\">Value-based</td>\n<td id=\"S3.T1.1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.2.2.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">\n<span id=\"S3.T1.1.2.2.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T1.1.2.2.2.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.2.2.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Deep Q-Network (DQN) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">37</a>]</cite>, Double Deep Q-Network (DDQN) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.2.2.2.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.2.2.2.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">DDQN with proportional prioritization <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite></span></span>\n</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\" colspan=\"2\">Policy-based</td>\n<td id=\"S3.T1.1.3.3.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.3.3.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">\n<span id=\"S3.T1.1.3.3.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T1.1.3.3.2.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.3.3.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">REINFORCE <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>]</cite>, Q-prop <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib41\" title=\"\" class=\"ltx_ref\">41</a>]</cite></span></span>\n</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\" colspan=\"2\">Actor-critic</td>\n<td id=\"S3.T1.1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.4.4.2.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">\n<span id=\"S3.T1.1.4.4.2.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T1.1.4.4.2.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.4.4.2.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Soft Actor-Critic (SAC) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">42</a>]</cite>, Asynchronous Advantage Actor Critic (A3C) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.4.4.2.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.4.4.2.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Deep Deterministic Policy Gradient (DDPG) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">44</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.4.4.2.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.4.4.2.1.1.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Distributed Distributional Deep Deterministic Policy Radients (D4PG) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">45</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.4.4.2.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.4.4.2.1.1.1.4.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Twin Delayed Deep Deterministic (TD3) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.4.4.2.1.1.1.5\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.4.4.2.1.1.1.5.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Trust Region Policy Optimization (TRPO) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">47</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.4.4.2.1.1.1.6\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.4.4.2.1.1.1.6.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Proximal Policy Optimization (PPO) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">48</a>]</cite></span></span>\n</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.5.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.5.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.5.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"><span id=\"S3.T1.1.5.5.1.1.1.1\" class=\"ltx_text\">Advanced</span></span>\n</span>\n</td>\n<td id=\"S3.T1.1.5.5.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.5.2.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">POMDP</span>\n</span>\n</td>\n<td id=\"S3.T1.1.5.5.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.5.5.3.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">\n<span id=\"S3.T1.1.5.5.3.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T1.1.5.5.3.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.5.5.3.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Deep Belief Q-Network (DBQN) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.5.5.3.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.5.5.3.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Deep Recurrent Q-Network (DRQN) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">50</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.5.5.3.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.5.5.3.1.1.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Recurrent Deterministic Policy Gradients (RDPG) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite></span></span>\n</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.6.6.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.6.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.6.1.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\"></span>\n</span>\n</td>\n<td id=\"S3.T1.1.6.6.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.6.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.6.2.1.1\" class=\"ltx_p\" style=\"width:56.9pt;\">Multi-agents</span>\n</span>\n</td>\n<td id=\"S3.T1.1.6.6.3\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">\n<span id=\"S3.T1.1.6.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.6.6.3.1.1\" class=\"ltx_p\" style=\"width:284.5pt;\">\n<span id=\"S3.T1.1.6.6.3.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S3.T1.1.6.6.3.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.6.6.3.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Multi-Agent Importance Sampling (MAIS) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.6.6.3.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.6.6.3.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Coordinated Multi-agent DQN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib53\" title=\"\" class=\"ltx_ref\">53</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.6.6.3.1.1.1.3\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.6.6.3.1.1.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Multi-agent Fingerprints (MAF) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.6.6.3.1.1.1.4\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.6.6.3.1.1.1.4.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Counterfactual Multiagent Policy Gradient (COMAPG) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib54\" title=\"\" class=\"ltx_ref\">54</a>]</cite>,</span></span>\n<span id=\"S3.T1.1.6.6.3.1.1.1.5\" class=\"ltx_tr\">\n<span id=\"S3.T1.1.6.6.3.1.1.1.5.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\" style=\"padding-top:0.5pt;padding-bottom:0.5pt;\">Multi-Agent DDPG (MADDPG) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib55\" title=\"\" class=\"ltx_ref\">55</a>]</cite></span></span>\n</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In addition to the value-based DRL algorithm such as DQN, we summarize a variety of classical DRL algorithms according to algorithm types by referring to some DRL related surveys [38] in Table I, including not only the policy-based and actor-critic DRL algorithms, but also the advanced DRL algorithms of Partially Observable Markov Decision Process (POMDP) and multi-agents."
        ]
    }
}