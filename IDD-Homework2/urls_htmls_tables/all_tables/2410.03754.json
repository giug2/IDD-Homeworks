{
    "id_table_1": {
        "caption": "Table 1:  Performance comparison of various flavors of RAIDD on the validation set.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "RAG systems, especially those which rely on embedding cosine similarity or BM25 to measure relevance, are fast and remarkably effective for answering questions whose answers are explicitly stated in the text. However, from a users perspective, this is only marginally more effective than a simple  ctrl+f  search. We expect more from the systems that we call artificially intelligent. In particular, we expect them to be able to answer questions whose answers are not explicitly stated in the text, but can be easily inferred from the text. Consider the example in Figure  1 , using cosine similarity between the query and text; the retriever latches onto the text which most explicitly describes commentary on the artists work and ignores the text which contains the answer but does not contain words like regarded. This is a common failure mode for RAG systems, and it demonstrates how the retriever can be a hindrance to what is otherwise a powerful model for natural language understanding."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance comparison on test set. Having tried different configurations on the validation set (Table  1 ), we settled on a chunk size of 256, overlap of 50, and  k k k italic_k  of 8 for the test set.",
        "table": "S3.T2.3",
        "footnotes": [],
        "references": [
            "Applications of RAIDD that outperform vanilla RAG in QA accuracy by 15% 2"
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Per-Task performance comparison on the test set. CP-CCR (Correct Prediction - Correct Chunk Retrieved) and IP-CCR (Incorrect Prediction - Correct Chunk Retrieved) measures whether the retrieved chunks for both correct and incorrect predictions match the ground truth context required for answering the question. This helps to measure the performance of the methods in enhancing context retrieval.  Bold  denotes better than baseline accuracy.",
        "table": "S3.T3.3",
        "footnotes": [],
        "references": [
            "Table  3  highlights the performance of various augmentation methods applied to RAG system across multiple tasks: timeline reorder, multiple information retrieval, comprehension and reasoning, and computation (see appendix  A  for examples). These results give a clearer understanding of how each augmentation strategy impacts performance in specific domains. Heres a detailed breakdown:"
        ]
    },
    "global_footnotes": [
        "https://github.com/isaacwasserman/LongRAG",
        "task dependent"
    ]
}