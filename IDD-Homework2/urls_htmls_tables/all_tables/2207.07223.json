{
    "PAPER'S NUMBER OF TABLES": 12,
    "S4.T1": {
        "caption": "Table 1: Statistics of the Datasets",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">#Training Clients</span></th>\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">#Test Clients</span></th>\n<th id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">#Training Samples</span></th>\n<th id=\"S4.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">#Test Samples</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">CIFAR-100</span></th>\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">500</td>\n<td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td id=\"S4.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">50,000</td>\n<td id=\"S4.T1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">10,000</td>\n</tr>\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T1.1.3.2.1.1\" class=\"ltx_text ltx_font_bold\">EMNIST-62</span></th>\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\">3,400</td>\n<td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\">3,400</td>\n<td id=\"S4.T1.1.3.2.4\" class=\"ltx_td ltx_align_center\">671,585</td>\n<td id=\"S4.T1.1.3.2.5\" class=\"ltx_td ltx_align_center\">77,483</td>\n</tr>\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\"><span id=\"S4.T1.1.4.3.1.1\" class=\"ltx_text ltx_font_bold\">Stack Overflow</span></th>\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\">342,477</td>\n<td id=\"S4.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">204,088</td>\n<td id=\"S4.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">135,818,730</td>\n<td id=\"S4.T1.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">16,586,035</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Impact of client and server learning rates.\nTablesÂ 6-11 report how the test Aâ€‹câ€‹câ€‹uâ€‹râ€‹aâ€‹câ€‹yğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦Accuracy changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the Aâ€‹câ€‹câ€‹uâ€‹râ€‹aâ€‹câ€‹yğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦Accuracy scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the Aâ€‹câ€‹câ€‹uâ€‹râ€‹aâ€‹câ€‹yğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦Accuracy values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table 12 for the implementation details of the server and client learning rates used in our current experiments.",
            "Unless otherwise explicitly stated, we used the following default parameter settings in the experiments, as shown in Table 12."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Final Accuracy on CIFAR-100",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></th>\n<th id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">SGDM</span></th>\n<th id=\"S5.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Adam</span></th>\n<th id=\"S5.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AdaGrad</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedLocal</th>\n<td id=\"S5.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.384</td>\n<td id=\"S5.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.009</td>\n<td id=\"S5.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.113</td>\n</tr>\n<tr id=\"S5.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SCAFFOLD</th>\n<td id=\"S5.T2.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.010</td>\n<td id=\"S5.T2.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.010</td>\n<td id=\"S5.T2.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.010</td>\n</tr>\n<tr id=\"S5.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedLin</th>\n<td id=\"S5.T2.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.440</td>\n<td id=\"S5.T2.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.440</td>\n<td id=\"S5.T2.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.440</td>\n</tr>\n<tr id=\"S5.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedAvg</th>\n<td id=\"S5.T2.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.324</td>\n<td id=\"S5.T2.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.324</td>\n<td id=\"S5.T2.1.5.4.4\" class=\"ltx_td ltx_align_center\">0.324</td>\n</tr>\n<tr id=\"S5.T2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFL</th>\n<td id=\"S5.T2.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.293</td>\n<td id=\"S5.T2.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.346</td>\n<td id=\"S5.T2.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.135</td>\n</tr>\n<tr id=\"S5.T2.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CLIMB</th>\n<td id=\"S5.T2.1.7.6.2\" class=\"ltx_td ltx_align_center\">0.010</td>\n<td id=\"S5.T2.1.7.6.3\" class=\"ltx_td ltx_align_center\">0.010</td>\n<td id=\"S5.T2.1.7.6.4\" class=\"ltx_td ltx_align_center\">0.010</td>\n</tr>\n<tr id=\"S5.T2.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">STEM</th>\n<td id=\"S5.T2.1.8.7.2\" class=\"ltx_td ltx_align_center\">0.014</td>\n<td id=\"S5.T2.1.8.7.3\" class=\"ltx_td ltx_align_center\">0.014</td>\n<td id=\"S5.T2.1.8.7.4\" class=\"ltx_td ltx_align_center\">0.014</td>\n</tr>\n<tr id=\"S5.T2.1.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MimeLite</th>\n<td id=\"S5.T2.1.9.8.2\" class=\"ltx_td ltx_align_center\">0.427</td>\n<td id=\"S5.T2.1.9.8.3\" class=\"ltx_td ltx_align_center\">0.009</td>\n<td id=\"S5.T2.1.9.8.4\" class=\"ltx_td ltx_align_center\">0.009</td>\n</tr>\n<tr id=\"S5.T2.1.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedOpt</th>\n<td id=\"S5.T2.1.10.9.2\" class=\"ltx_td ltx_align_center\">0.425</td>\n<td id=\"S5.T2.1.10.9.3\" class=\"ltx_td ltx_align_center\">0.443</td>\n<td id=\"S5.T2.1.10.9.4\" class=\"ltx_td ltx_align_center\">0.301</td>\n</tr>\n<tr id=\"S5.T2.1.11.10\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.11.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">FedDA</th>\n<td id=\"S5.T2.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T2.1.11.10.2.1\" class=\"ltx_text ltx_font_bold\">0.518</span></td>\n<td id=\"S5.T2.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T2.1.11.10.3.1\" class=\"ltx_text ltx_font_bold\">0.510</span></td>\n<td id=\"S5.T2.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T2.1.11.10.4.1\" class=\"ltx_text ltx_font_bold\">0.488</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            []
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Final Accuracy on EMNIST",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></th>\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">SGDM</span></th>\n<th id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Adam</span></th>\n<th id=\"S5.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T3.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AdaGrad</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedLocal</th>\n<td id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.834</td>\n<td id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.055</td>\n<td id=\"S5.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.806</td>\n</tr>\n<tr id=\"S5.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SCAFFOLD</th>\n<td id=\"S5.T3.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.794</td>\n<td id=\"S5.T3.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.794</td>\n<td id=\"S5.T3.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.794</td>\n</tr>\n<tr id=\"S5.T3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedLin</th>\n<td id=\"S5.T3.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.805</td>\n<td id=\"S5.T3.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.805</td>\n<td id=\"S5.T3.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.805</td>\n</tr>\n<tr id=\"S5.T3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedAvg</th>\n<td id=\"S5.T3.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.850</td>\n<td id=\"S5.T3.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.850</td>\n<td id=\"S5.T3.1.5.4.4\" class=\"ltx_td ltx_align_center\">0.850</td>\n</tr>\n<tr id=\"S5.T3.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFL</th>\n<td id=\"S5.T3.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.848</td>\n<td id=\"S5.T3.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.055</td>\n<td id=\"S5.T3.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.047</td>\n</tr>\n<tr id=\"S5.T3.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CLIMB</th>\n<td id=\"S5.T3.1.7.6.2\" class=\"ltx_td ltx_align_center\">0.843</td>\n<td id=\"S5.T3.1.7.6.3\" class=\"ltx_td ltx_align_center\">0.843</td>\n<td id=\"S5.T3.1.7.6.4\" class=\"ltx_td ltx_align_center\">0.843</td>\n</tr>\n<tr id=\"S5.T3.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">STEM</th>\n<td id=\"S5.T3.1.8.7.2\" class=\"ltx_td ltx_align_center\">0.051</td>\n<td id=\"S5.T3.1.8.7.3\" class=\"ltx_td ltx_align_center\">0.051</td>\n<td id=\"S5.T3.1.8.7.4\" class=\"ltx_td ltx_align_center\">0.051</td>\n</tr>\n<tr id=\"S5.T3.1.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MimeLite</th>\n<td id=\"S5.T3.1.9.8.2\" class=\"ltx_td ltx_align_center\">0.835</td>\n<td id=\"S5.T3.1.9.8.3\" class=\"ltx_td ltx_align_center\">0.851</td>\n<td id=\"S5.T3.1.9.8.4\" class=\"ltx_td ltx_align_center\">0.821</td>\n</tr>\n<tr id=\"S5.T3.1.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedOpt</th>\n<td id=\"S5.T3.1.10.9.2\" class=\"ltx_td ltx_align_center\">0.838</td>\n<td id=\"S5.T3.1.10.9.3\" class=\"ltx_td ltx_align_center\">0.847</td>\n<td id=\"S5.T3.1.10.9.4\" class=\"ltx_td ltx_align_center\">0.840</td>\n</tr>\n<tr id=\"S5.T3.1.11.10\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.11.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">FedDA</th>\n<td id=\"S5.T3.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T3.1.11.10.2.1\" class=\"ltx_text ltx_font_bold\">0.860</span></td>\n<td id=\"S5.T3.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T3.1.11.10.3.1\" class=\"ltx_text ltx_font_bold\">0.853</span></td>\n<td id=\"S5.T3.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T3.1.11.10.4.1\" class=\"ltx_text ltx_font_bold\">0.868</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            []
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Final Mean Squared Error on EMNIST for Autoencoder",
        "table": "<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></th>\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">SGDM</span></th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Adam</span></th>\n<th id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T4.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AdaGrad</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedLocal</th>\n<td id=\"S5.T4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0169</td>\n<td id=\"S5.T4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0289</td>\n<td id=\"S5.T4.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0168</td>\n</tr>\n<tr id=\"S5.T4.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedAvg</th>\n<td id=\"S5.T4.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.0171</td>\n<td id=\"S5.T4.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.0171</td>\n<td id=\"S5.T4.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.0171</td>\n</tr>\n<tr id=\"S5.T4.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFL</th>\n<td id=\"S5.T4.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.0168</td>\n<td id=\"S5.T4.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.0290</td>\n<td id=\"S5.T4.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.0291</td>\n</tr>\n<tr id=\"S5.T4.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MimeLite</th>\n<td id=\"S5.T4.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.0183</td>\n<td id=\"S5.T4.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.0307</td>\n<td id=\"S5.T4.1.5.4.4\" class=\"ltx_td ltx_align_center\">0.0287</td>\n</tr>\n<tr id=\"S5.T4.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedOpt</th>\n<td id=\"S5.T4.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.0175</td>\n<td id=\"S5.T4.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.0173</td>\n<td id=\"S5.T4.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.0145</td>\n</tr>\n<tr id=\"S5.T4.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">FedDA</th>\n<td id=\"S5.T4.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T4.1.7.6.2.1\" class=\"ltx_text ltx_font_bold\">0.0167</span></td>\n<td id=\"S5.T4.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T4.1.7.6.3.1\" class=\"ltx_text ltx_font_bold\">0.0166</span></td>\n<td id=\"S5.T4.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T4.1.7.6.4.1\" class=\"ltx_text ltx_font_bold\">0.0132</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we have evaluated the performance of our FedDAÂ model and other comparison methods in serval representative federated datasets and learning tasks to date. We show that FedDAÂ with decoupling and full batch gradient techniques is able to achieve faster convergence and higher test accuracy in cross-device settings against several state-of-the-art federated optimization methods. The experiments exactly follow the same settings described by a recent federated optimization method, FedOptÂ ",
                "(Reddi etÂ al., ",
                "2021a",
                ")",
                ".",
                "Datasets.",
                " We focus on three popular computer vision and natural language processing tasks over three representative benchmark datasets respectively: (1) image classification over CIFAR-100Â ",
                "(Krizhevsky, ",
                "2009",
                ")",
                ". We train ResNet-18 by replacing batch norm with group normÂ ",
                "(Hsieh etÂ al., ",
                "2020",
                ")",
                "; (2) image classification over EMNISTÂ ",
                "(Hsieh etÂ al., ",
                "2020",
                ")",
                ". We train a CNN for character recognition; and (3) text classification over Stack OverflowÂ ",
                "(TensorFlow, ",
                "2019",
                ")",
                ". We perform tag prediction using logistic regression on bag-of-words vectors. We select the 10,000 most frequently used words, the 500 most frequent tags and a one-versus-rest classification strategy, by following the same strategy in FedOptÂ ",
                "(Reddi etÂ al., ",
                "2021a",
                ")",
                ". The detailed descriptions of the federated datasets and learning tasks are presented in AppendixÂ ",
                "A.5",
                ".",
                "Baselines.",
                " We compare the FedDAÂ model with nine state-of-the-art federated learning models, including five regular federated learning and four federated optimization approaches.\n",
                "FedAvg",
                " is a classical as well as practical method for the federated learning of deep networks based on iterative model averagingÂ ",
                "(McMahan etÂ al., ",
                "2017a",
                ")",
                ".\n",
                "SCAFFOLD",
                " is a algorithm which uses control variates to correct for the client-drift in its local updatesÂ ",
                "(Karimireddy etÂ al., ",
                "2020d",
                ")",
                ".\n",
                "FedLin",
                " is an algorithmic framework to tackle the key challenges of objective heterogeneity, systems heterogeneity, and imprecise communication in FLÂ ",
                "(Mitra etÂ al., ",
                "2021a",
                ")",
                ".\n",
                "STEM",
                " is a stochastic two-sided momentum algorithm, that utilizes certain momentum-assisted stochastic gradient directions for both the client and server updatesÂ ",
                "(Mitra etÂ al., ",
                "2021a",
                ")",
                ".\n",
                "CLIMB",
                " is an agnostic constrained learning formulation to tackle the class imbalance problem in FL without requiring further information beyond the standard FL objectiveÂ ",
                "(",
                "Anon22d",
                ")",
                ".\n",
                "MFL",
                " performs momentum gradient descent in local update step of FL system to solve the distributed machine learning problemÂ ",
                "(Liu etÂ al., ",
                "2020a",
                ")",
                ".\n",
                "FedOpt",
                " is a flexible algorithmic framework that allows the clients and the server to choose different optimization methods more general than stochastic gradient descent (SGD) in FedAvgÂ ",
                "(Reddi etÂ al., ",
                "2021a",
                ")",
                ".\n",
                "MimeLite",
                " uses a combination of control-variates and server-level optimizer state at every client-update step to ensure that each local update mimics that of the centralized method run on i.i.d. dataÂ ",
                "(Karimireddy etÂ al., ",
                "2021",
                ")",
                ".\n",
                "Local Adaptivity (FedLocal)",
                " proposes techniques that enable the use of adaptive optimization methods for local updates at clients in federated learningÂ ",
                "(Wang etÂ al., ",
                "2021b",
                ")",
                ".\nTo our best knowledge, this work is the first to certify the group fairness of classifiers with theoretical input-agnostic guarantees, while there is no need to know the shift between training and deployment datasets with respect to sensitive attributes. All nine baselines used in our experiments either do not use momentum, or use momentum without momentum aggregation, or use momentum with aggregation but restart momentum at each FL round (FedLocal). Our FedDAÂ method keeps the momentum aggregation in the entire FL process, which results in faster convergence but larger oscillation.",
                "Evaluation metrics.",
                " We use two popular measures in federated learning and plot the measure curves with increasing training rounds to verify the convergence of different methods: ",
                "Accuracy",
                " and ",
                "Loss Function Values (Loss)",
                "Â ",
                "(Karimireddy etÂ al., ",
                "2020d",
                "; Mitra etÂ al., ",
                "2021a",
                "; Liu etÂ al., ",
                "2020a",
                "; Reddi etÂ al., ",
                "2021a",
                "; Karimireddy etÂ al., ",
                "2021",
                "; Wang etÂ al., ",
                "2021b",
                ")",
                ". A larger Accuracy or a smaller Loss score indicates a better federated learning result. We run 1,500 rounds of training on the EMNIST and Stack Overflow datasets, and 4,000 rounds over the CIFAR-100 dataset. In addition, we use final Accuracy to evaluate the quality of the federated learning algorithms.",
                "Convergence on CIFAR-100 and EMNIST.",
                " Figures ",
                "1",
                " and ",
                "3",
                " exhibit the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " curves of ten federated learning models for image classification over CIFAR-100 and character recognition on EMNIST respectively.\nIt is obvious that the performance curves by federated learning algorithms initially keep increasing with training rounds and remains relatively stable when the curves are beyond convergence points, i.e., turning points from a sharp ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " increase to a flat curve. This phenomenon indicates that most federated learning algorithms are able to converge to the invariant solutions after enough training rounds. However, among five regular federated learning and five federated optimization approaches, our FedDAÂ federated optimization method can significantly speedup the convergence on two datasets in most experiments, showing the superior performance of FedDAÂ in federated settings. Compared to the learning results by other federated learning models, based on training rounds at convergence points, FedDA, on average, achieves 34.3% and 22.6% convergence improvement on two datasets respectively.",
                "Loss on CIFAR-100 and EMNIST.",
                " Figures ",
                "2",
                " and ",
                "4",
                " present the ",
                "L",
                "â€‹",
                "o",
                "â€‹",
                "s",
                "â€‹",
                "s",
                "ğ¿",
                "ğ‘œ",
                "ğ‘ ",
                "ğ‘ ",
                "Loss",
                " curves achieved by ten federated learning models on two datasets respectively.\nWe have observed obvious that the reverse trends, in comparison with the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " curves. In most experiments, our FedDAÂ federated optimization method is able to achieve the fastest convergence, especially, FedDAÂ can converge within less than 200 training rounds and then always keep stable on the EMNIST dataset. A reasonable explanation is that FedDAÂ fully utilizes the global momentum on each local iteration as well as employ full batch gradients to mimic centralized optimization in the end of the training process for accelerating the training convergence.",
                "Final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " on CIFAR-100 and EMNIST.",
                " Tables ",
                "2",
                "-",
                "4",
                " show the quality of ten federated learning algorithms over CIFAR-100 and EMNIST respectively.\nNotice that the performance achieved by five regular federated learning algorithms, including FedAvg, SCAFFOLD, FedLin, STEM, and CLIMB, keep unchanged when using different optimizers, such as SGDM, Adam, and AdaGrad, while five federated optimization approaches, including MFL, FedOpt, Mime, FedLocal, and our FedDAÂ model obtain different performance.\nWe have observed that our FedDAÂ federated optimization solution outperforms the competitor methods in most experiments. FedDAÂ achieves the highest ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " values (",
                ">",
                ">",
                " 0.488 over CIFAR-100 and ",
                ">",
                ">",
                " 0.853 on EMNIST respectively), which are better than other nine baseline methods in all tests.\nA reasonable explanation is that the combination of decoupling and full batch gradient techniques is able to achieve faster convergence as well as higher test accuracy in cross-device settings. In addition, the promising performance of FedDAÂ over both datasets implies that FedDAÂ has great potential as a general federated optimization solution to learning tasks over federated datasets, which is desirable in practice.",
                "Impact of local iteration numbers.",
                " Figure ",
                "5",
                " shows the impact of the numbers of local iterations in our FedDAÂ model with the adaptive optimizers over the datasets of CIFAR-100, EMNIST, and Stack Overflow respectively. The performance curves initially raise when the local iteration number increases and then keep relatively stable or even drop if the local iteration number keeps increasing. This demonstrates that there must exist a suitable local iteration number for the FL training. A too large number may make the clients overfit to their local datasets, such that the local models are far away from globally optimal models and the FL training achieves slower convergence. On the other hand, a too small number may result in slow convergence of local training and also increase the difficulty of convergence of global training. Thus, it is important to choose the appropriate numbers for well balancing the local training and global training. Notice that the final accuracy of AdaGrad and SGDM is closed to 0 on the EMNIST dataset when the local iteration number is larger than 10. A reasonable explanation is EMNIST is a simple dataset and a large local iteration number makes local models converge to their local minimum, which may be distant from the minimum of global model. This leads to lower accuracy of global model.",
                "Impact of training round numbers.",
                " Figures ",
                "6",
                " (a)-(c) present the performance achieved by our FedDAÂ method with varying the numbers of training rounds from 100 to 2,000, from 10 to 1,000, and from 50 to 1,000 on three datasets. It is obvious that the performance curves with each optimizer keep increasing with the increased number of training rounds. This phenomenon indicates that the accuracy in the federated settings are sensitive to training rounds. This is because the special data and system heterogeneity issues in the FL increase the difficulty in converging in a short time and the FL models need more training rounds to obtain the desired learning results. However, as shown in the above experiments of convergence in Figures ",
                "1",
                "-",
                "4",
                ", our FedDAÂ method presents superior convergence performance, compared with other FL algorithms, including both regular federated learning and federated optimization approaches."
            ]
        ]
    },
    "A1.T5": {
        "caption": "Table 5: Final Accuracy on Stack Overflow",
        "table": "<table id=\"A1.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T5.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></th>\n<th id=\"A1.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A1.T5.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">SGDM</span></th>\n<th id=\"A1.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A1.T5.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Adam</span></th>\n<th id=\"A1.T5.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A1.T5.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AdaGrad</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T5.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedLocal</th>\n<td id=\"A1.T5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.152</td>\n<td id=\"A1.T5.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.576</td>\n<td id=\"A1.T5.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.229</td>\n</tr>\n<tr id=\"A1.T5.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SCAFFOLD</th>\n<td id=\"A1.T5.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.250</td>\n<td id=\"A1.T5.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.250</td>\n<td id=\"A1.T5.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.250</td>\n</tr>\n<tr id=\"A1.T5.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedLin</th>\n<td id=\"A1.T5.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.224</td>\n<td id=\"A1.T5.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.224</td>\n<td id=\"A1.T5.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.224</td>\n</tr>\n<tr id=\"A1.T5.1.5.4\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedAvg</th>\n<td id=\"A1.T5.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.252</td>\n<td id=\"A1.T5.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.252</td>\n<td id=\"A1.T5.1.5.4.4\" class=\"ltx_td ltx_align_center\">0.252</td>\n</tr>\n<tr id=\"A1.T5.1.6.5\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFL</th>\n<td id=\"A1.T5.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.134</td>\n<td id=\"A1.T5.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.101</td>\n<td id=\"A1.T5.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.215</td>\n</tr>\n<tr id=\"A1.T5.1.7.6\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CLIMB</th>\n<td id=\"A1.T5.1.7.6.2\" class=\"ltx_td ltx_align_center\">0.302</td>\n<td id=\"A1.T5.1.7.6.3\" class=\"ltx_td ltx_align_center\">0.302</td>\n<td id=\"A1.T5.1.7.6.4\" class=\"ltx_td ltx_align_center\">0.302</td>\n</tr>\n<tr id=\"A1.T5.1.8.7\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">STEM</th>\n<td id=\"A1.T5.1.8.7.2\" class=\"ltx_td ltx_align_center\">0.196</td>\n<td id=\"A1.T5.1.8.7.3\" class=\"ltx_td ltx_align_center\">0.196</td>\n<td id=\"A1.T5.1.8.7.4\" class=\"ltx_td ltx_align_center\">0.196</td>\n</tr>\n<tr id=\"A1.T5.1.9.8\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MimeLite</th>\n<td id=\"A1.T5.1.9.8.2\" class=\"ltx_td ltx_align_center\">0.271</td>\n<td id=\"A1.T5.1.9.8.3\" class=\"ltx_td ltx_align_center\">0.211</td>\n<td id=\"A1.T5.1.9.8.4\" class=\"ltx_td ltx_align_center\">0.078</td>\n</tr>\n<tr id=\"A1.T5.1.10.9\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedOpt</th>\n<td id=\"A1.T5.1.10.9.2\" class=\"ltx_td ltx_align_center\">0.225</td>\n<td id=\"A1.T5.1.10.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.1.10.9.3.1\" class=\"ltx_text ltx_font_bold\">0.642</span></td>\n<td id=\"A1.T5.1.10.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.1.10.9.4.1\" class=\"ltx_text ltx_font_bold\">0.691</span></td>\n</tr>\n<tr id=\"A1.T5.1.11.10\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.11.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">FedDA</th>\n<td id=\"A1.T5.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"A1.T5.1.11.10.2.1\" class=\"ltx_text ltx_font_bold\">0.273</span></td>\n<td id=\"A1.T5.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"A1.T5.1.11.10.3.1\" class=\"ltx_text ltx_font_bold\">0.642</span></td>\n<td id=\"A1.T5.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.674</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Final Aâ€‹câ€‹câ€‹uâ€‹râ€‹aâ€‹câ€‹yğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦Accuracy over Stack Overflow. Table 5 presents the final Aâ€‹câ€‹câ€‹uâ€‹râ€‹aâ€‹câ€‹yğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦Accuracy scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDAÂ is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final Aâ€‹câ€‹câ€‹uâ€‹râ€‹aâ€‹câ€‹yğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦Accuracy scores achieved by FedDAÂ averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDAÂ method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training."
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Final Accuracy with SGDM Optimizer and Varying Client Learning Rate",
        "table": "<table id=\"A1.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T6.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T6.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Server Learning Rate</span></th>\n<th id=\"A1.T6.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T6.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Client Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T6.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T6.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.1</th>\n<td id=\"A1.T6.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.521 / 1</td>\n<td id=\"A1.T6.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.448 / 3.3</td>\n<td id=\"A1.T6.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.236 / 10</td>\n<td id=\"A1.T6.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.099 / 33</td>\n</tr>\n<tr id=\"A1.T6.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T6.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.1</th>\n<td id=\"A1.T6.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.778 / 1</td>\n<td id=\"A1.T6.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.819 / 3.3</td>\n<td id=\"A1.T6.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.783 / 10</td>\n<td id=\"A1.T6.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.052 / 33</td>\n</tr>\n<tr id=\"A1.T6.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T6.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">0.1</th>\n<td id=\"A1.T6.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.174 / 1</td>\n<td id=\"A1.T6.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.262 / 10</td>\n<td id=\"A1.T6.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.250 / 100</td>\n<td id=\"A1.T6.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.002 / 1,000</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " and ",
                "L",
                "â€‹",
                "o",
                "â€‹",
                "s",
                "â€‹",
                "s",
                "ğ¿",
                "ğ‘œ",
                "ğ‘ ",
                "ğ‘ ",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDAÂ achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDAÂ is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores achieved by FedDAÂ averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDAÂ method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTablesÂ ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Final Accuracy with Adam Optimizer and Varying Client Learning Rate",
        "table": "<table id=\"A1.T7.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T7.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T7.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T7.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T7.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T7.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Server Learning Rate</span></th>\n<th id=\"A1.T7.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T7.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Client Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T7.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T7.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T7.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.1</th>\n<td id=\"A1.T7.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.179 / 1</td>\n<td id=\"A1.T7.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.317 / 3.3</td>\n<td id=\"A1.T7.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.269 / 10</td>\n<td id=\"A1.T7.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.179 / 33</td>\n</tr>\n<tr id=\"A1.T7.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T7.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T7.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.03</th>\n<td id=\"A1.T7.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.488 / 0.01</td>\n<td id=\"A1.T7.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.836 / 0.1</td>\n<td id=\"A1.T7.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.842 / 0.3</td>\n<td id=\"A1.T7.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.804 / 1</td>\n</tr>\n<tr id=\"A1.T7.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T7.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T7.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">0.3</th>\n<td id=\"A1.T7.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.409 / 0.1</td>\n<td id=\"A1.T7.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.492 / 1</td>\n<td id=\"A1.T7.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.473 / 10</td>\n<td id=\"A1.T7.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.459 / 100</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " and ",
                "L",
                "â€‹",
                "o",
                "â€‹",
                "s",
                "â€‹",
                "s",
                "ğ¿",
                "ğ‘œ",
                "ğ‘ ",
                "ğ‘ ",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDAÂ achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDAÂ is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores achieved by FedDAÂ averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDAÂ method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTablesÂ ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Final Accuracy with AdaGrad Optimizer and Varying Client Learning Rate",
        "table": "<table id=\"A1.T8.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T8.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T8.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T8.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T8.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T8.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Server Learning Rate</span></th>\n<th id=\"A1.T8.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T8.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Client Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T8.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T8.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T8.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.1</th>\n<td id=\"A1.T8.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.462 / 0.03</td>\n<td id=\"A1.T8.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.488 / 0.1</td>\n<td id=\"A1.T8.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.410 / 0.3</td>\n<td id=\"A1.T8.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.259 / 1</td>\n</tr>\n<tr id=\"A1.T8.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T8.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T8.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.1</th>\n<td id=\"A1.T8.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.055 / 0.00001</td>\n<td id=\"A1.T8.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.055 / 0.001</td>\n<td id=\"A1.T8.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.055 / 0.01</td>\n<td id=\"A1.T8.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.868 / 0.1</td>\n</tr>\n<tr id=\"A1.T8.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T8.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T8.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">10</th>\n<td id=\"A1.T8.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.576 /1</td>\n<td id=\"A1.T8.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.651 / 10</td>\n<td id=\"A1.T8.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.629 / 30</td>\n<td id=\"A1.T8.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.632 / 100</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " and ",
                "L",
                "â€‹",
                "o",
                "â€‹",
                "s",
                "â€‹",
                "s",
                "ğ¿",
                "ğ‘œ",
                "ğ‘ ",
                "ğ‘ ",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDAÂ achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDAÂ is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores achieved by FedDAÂ averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDAÂ method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTablesÂ ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T9": {
        "caption": "Table 9: Final Accuracy with SGDM Optimizer and Varying Server Learning Rate",
        "table": "<table id=\"A1.T9.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T9.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T9.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T9.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T9.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Client Learning Rate</span></th>\n<th id=\"A1.T9.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T9.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Server Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T9.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T9.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.03</th>\n<td id=\"A1.T9.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.374 / 1</td>\n<td id=\"A1.T9.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.522 / 3.3</td>\n<td id=\"A1.T9.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.356 / 10</td>\n<td id=\"A1.T9.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.273 / 33</td>\n</tr>\n<tr id=\"A1.T9.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T9.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.1</th>\n<td id=\"A1.T9.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.859 / 0.3</td>\n<td id=\"A1.T9.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.861 / 1</td>\n<td id=\"A1.T9.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.778 / 3.3</td>\n<td id=\"A1.T9.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.783 / 10</td>\n</tr>\n<tr id=\"A1.T9.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T9.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">100</th>\n<td id=\"A1.T9.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.191 / 0.001</td>\n<td id=\"A1.T9.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.256 / 0.003</td>\n<td id=\"A1.T9.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.204 / 0.01</td>\n<td id=\"A1.T9.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.292 / 0.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " and ",
                "L",
                "â€‹",
                "o",
                "â€‹",
                "s",
                "â€‹",
                "s",
                "ğ¿",
                "ğ‘œ",
                "ğ‘ ",
                "ğ‘ ",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDAÂ achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDAÂ is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores achieved by FedDAÂ averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDAÂ method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTablesÂ ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T10": {
        "caption": "Table 10: Final Accuracy with Adam Optimizer and Varying Server Learning Rate",
        "table": "<table id=\"A1.T10.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T10.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T10.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T10.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T10.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Client Learning Rate</span></th>\n<th id=\"A1.T10.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T10.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Server Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T10.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T10.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.03</th>\n<td id=\"A1.T10.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.510 / 0.33</td>\n<td id=\"A1.T10.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.183 / 3.3</td>\n<td id=\"A1.T10.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.010 / 10</td>\n<td id=\"A1.T10.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.010 / 33</td>\n</tr>\n<tr id=\"A1.T10.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T10.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.03</th>\n<td id=\"A1.T10.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.546 / 0.1</td>\n<td id=\"A1.T10.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.803 / 1</td>\n<td id=\"A1.T10.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.051 / 3.3</td>\n<td id=\"A1.T10.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.051 / 10</td>\n</tr>\n<tr id=\"A1.T10.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T10.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">100</th>\n<td id=\"A1.T10.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.425 / 0.1</td>\n<td id=\"A1.T10.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.522 / 0.3</td>\n<td id=\"A1.T10.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.641 / 1</td>\n<td id=\"A1.T10.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.633 / 10</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " and ",
                "L",
                "â€‹",
                "o",
                "â€‹",
                "s",
                "â€‹",
                "s",
                "ğ¿",
                "ğ‘œ",
                "ğ‘ ",
                "ğ‘ ",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDAÂ achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDAÂ is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores achieved by FedDAÂ averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDAÂ method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTablesÂ ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T11": {
        "caption": "Table 11: Final Accuracy with AdaGrad Optimizer and Varying Server Learning Rate",
        "table": "<table id=\"A1.T11.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T11.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T11.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T11.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T11.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Client Learning Rate</span></th>\n<th id=\"A1.T11.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T11.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Server Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T11.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T11.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.03</th>\n<td id=\"A1.T11.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.352 / 0.03</td>\n<td id=\"A1.T11.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.462 / 0.1</td>\n<td id=\"A1.T11.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.466 / 0.3</td>\n<td id=\"A1.T11.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.312 / 1</td>\n</tr>\n<tr id=\"A1.T11.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T11.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.03</th>\n<td id=\"A1.T11.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.806 / 0.1</td>\n<td id=\"A1.T11.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.055 / 3.3</td>\n<td id=\"A1.T11.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.055 / 10</td>\n<td id=\"A1.T11.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.055 / 33</td>\n</tr>\n<tr id=\"A1.T11.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T11.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">100</th>\n<td id=\"A1.T11.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.227 / 0.1</td>\n<td id=\"A1.T11.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.306 / 0.3</td>\n<td id=\"A1.T11.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.397 / 1</td>\n<td id=\"A1.T11.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.632 / 10</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " and ",
                "L",
                "â€‹",
                "o",
                "â€‹",
                "s",
                "â€‹",
                "s",
                "ğ¿",
                "ğ‘œ",
                "ğ‘ ",
                "ğ‘ ",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDAÂ achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDAÂ is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores achieved by FedDAÂ averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDAÂ method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTablesÂ ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "â€‹",
                "c",
                "â€‹",
                "c",
                "â€‹",
                "u",
                "â€‹",
                "r",
                "â€‹",
                "a",
                "â€‹",
                "c",
                "â€‹",
                "y",
                "ğ´",
                "ğ‘",
                "ğ‘",
                "ğ‘¢",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘",
                "ğ‘¦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T12": {
        "caption": "Table 12: Hyperparameter Settings",
        "table": "<table id=\"A1.T12.2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T12.2.2.3.1\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T12.2.2.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Parameter</span></td>\n<td id=\"A1.T12.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T12.2.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Value</span></td>\n</tr>\n<tr id=\"A1.T12.2.2.4.2\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Training rounds for EMNIST and Stack Overflow</td>\n<td id=\"A1.T12.2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1,500</td>\n</tr>\n<tr id=\"A1.T12.2.2.5.3\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Training rounds for CIFAR-100</td>\n<td id=\"A1.T12.2.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">4,000</td>\n</tr>\n<tr id=\"A1.T12.1.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T12.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Momentum parameter <math id=\"A1.T12.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\beta_{1}\" display=\"inline\"><semantics id=\"A1.T12.1.1.1.1.m1.1a\"><msub id=\"A1.T12.1.1.1.1.m1.1.1\" xref=\"A1.T12.1.1.1.1.m1.1.1.cmml\"><mi id=\"A1.T12.1.1.1.1.m1.1.1.2\" xref=\"A1.T12.1.1.1.1.m1.1.1.2.cmml\">Î²</mi><mn id=\"A1.T12.1.1.1.1.m1.1.1.3\" xref=\"A1.T12.1.1.1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.T12.1.1.1.1.m1.1b\"><apply id=\"A1.T12.1.1.1.1.m1.1.1.cmml\" xref=\"A1.T12.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.T12.1.1.1.1.m1.1.1.1.cmml\" xref=\"A1.T12.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"A1.T12.1.1.1.1.m1.1.1.2.cmml\" xref=\"A1.T12.1.1.1.1.m1.1.1.2\">ğ›½</ci><cn type=\"integer\" id=\"A1.T12.1.1.1.1.m1.1.1.3.cmml\" xref=\"A1.T12.1.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T12.1.1.1.1.m1.1c\">\\beta_{1}</annotation></semantics></math>\n</td>\n<td id=\"A1.T12.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.9</td>\n</tr>\n<tr id=\"A1.T12.2.2.2\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Second moment parameter <math id=\"A1.T12.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\beta_{2}\" display=\"inline\"><semantics id=\"A1.T12.2.2.2.1.m1.1a\"><msub id=\"A1.T12.2.2.2.1.m1.1.1\" xref=\"A1.T12.2.2.2.1.m1.1.1.cmml\"><mi id=\"A1.T12.2.2.2.1.m1.1.1.2\" xref=\"A1.T12.2.2.2.1.m1.1.1.2.cmml\">Î²</mi><mn id=\"A1.T12.2.2.2.1.m1.1.1.3\" xref=\"A1.T12.2.2.2.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.T12.2.2.2.1.m1.1b\"><apply id=\"A1.T12.2.2.2.1.m1.1.1.cmml\" xref=\"A1.T12.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.T12.2.2.2.1.m1.1.1.1.cmml\" xref=\"A1.T12.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"A1.T12.2.2.2.1.m1.1.1.2.cmml\" xref=\"A1.T12.2.2.2.1.m1.1.1.2\">ğ›½</ci><cn type=\"integer\" id=\"A1.T12.2.2.2.1.m1.1.1.3.cmml\" xref=\"A1.T12.2.2.2.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T12.2.2.2.1.m1.1c\">\\beta_{2}</annotation></semantics></math>\n</td>\n<td id=\"A1.T12.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.99</td>\n</tr>\n<tr id=\"A1.T12.2.2.6.4\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with SGDM on CIFAR-100</td>\n<td id=\"A1.T12.2.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.03</td>\n</tr>\n<tr id=\"A1.T12.2.2.7.5\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with Adam on CIFAR-100</td>\n<td id=\"A1.T12.2.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.03</td>\n</tr>\n<tr id=\"A1.T12.2.2.8.6\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with AdaGrad on CIFAR-100</td>\n<td id=\"A1.T12.2.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.9.7\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with SGDM on CIFAR-100</td>\n<td id=\"A1.T12.2.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.3</td>\n</tr>\n<tr id=\"A1.T12.2.2.10.8\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with Adam on CIFAR-100</td>\n<td id=\"A1.T12.2.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.3</td>\n</tr>\n<tr id=\"A1.T12.2.2.11.9\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.11.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with AdaGrad on CIFAR-100</td>\n<td id=\"A1.T12.2.2.11.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.12.10\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.12.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with SGDM on CIFAR-100</td>\n<td id=\"A1.T12.2.2.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.13.11\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.13.11.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with Adam on CIFAR-100</td>\n<td id=\"A1.T12.2.2.13.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.14.12\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.14.12.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with AdaGrad on CIFAR-100</td>\n<td id=\"A1.T12.2.2.14.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.15.13\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.15.13.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with Adam on CIFAR-100</td>\n<td id=\"A1.T12.2.2.15.13.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.16.14\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.16.14.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with AdaGrad on CIFAR-100</td>\n<td id=\"A1.T12.2.2.16.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.17.15\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.17.15.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with SGDM on EMNIST</td>\n<td id=\"A1.T12.2.2.17.15.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.18.16\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.18.16.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with Adam on EMNIST</td>\n<td id=\"A1.T12.2.2.18.16.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.19.17\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.19.17.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with AdaGrad on EMNIST</td>\n<td id=\"A1.T12.2.2.19.17.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.20.18\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.20.18.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with SGDM on EMNIST</td>\n<td id=\"A1.T12.2.2.20.18.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1</td>\n</tr>\n<tr id=\"A1.T12.2.2.21.19\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.21.19.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with Adam on EMNIST</td>\n<td id=\"A1.T12.2.2.21.19.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.22.20\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.22.20.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with AdaGrad on EMNIST</td>\n<td id=\"A1.T12.2.2.22.20.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.23.21\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.23.21.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with SGDM on EMNIST</td>\n<td id=\"A1.T12.2.2.23.21.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10</td>\n</tr>\n<tr id=\"A1.T12.2.2.24.22\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.24.22.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with Adam on EMNIST</td>\n<td id=\"A1.T12.2.2.24.22.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10</td>\n</tr>\n<tr id=\"A1.T12.2.2.25.23\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.25.23.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with AdaGrad on EMNIST</td>\n<td id=\"A1.T12.2.2.25.23.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.26.24\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.26.24.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with Adam on EMNIST</td>\n<td id=\"A1.T12.2.2.26.24.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.27.25\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.27.25.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with AdaGrad on EMNIST</td>\n<td id=\"A1.T12.2.2.27.25.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.28.26\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.28.26.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with SGDM on Stack Overflow</td>\n<td id=\"A1.T12.2.2.28.26.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">100</td>\n</tr>\n<tr id=\"A1.T12.2.2.29.27\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.29.27.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with Adam on Stack Overflow</td>\n<td id=\"A1.T12.2.2.29.27.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">100</td>\n</tr>\n<tr id=\"A1.T12.2.2.30.28\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.30.28.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with AdaGrad on Stack Overflow</td>\n<td id=\"A1.T12.2.2.30.28.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">100</td>\n</tr>\n<tr id=\"A1.T12.2.2.31.29\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.31.29.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with SGDM on Stack Overflow</td>\n<td id=\"A1.T12.2.2.31.29.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10</td>\n</tr>\n<tr id=\"A1.T12.2.2.32.30\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.32.30.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with Adam on Stack Overflow</td>\n<td id=\"A1.T12.2.2.32.30.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1</td>\n</tr>\n<tr id=\"A1.T12.2.2.33.31\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.33.31.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with AdaGrad on Stack Overflow</td>\n<td id=\"A1.T12.2.2.33.31.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10</td>\n</tr>\n<tr id=\"A1.T12.2.2.34.32\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.34.32.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with SGDM on Stack Overflow</td>\n<td id=\"A1.T12.2.2.34.32.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.35.33\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.35.33.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with Adam on Stack Overflow</td>\n<td id=\"A1.T12.2.2.35.33.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1</td>\n</tr>\n<tr id=\"A1.T12.2.2.36.34\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.36.34.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with AdaGrad on Stack Overflow</td>\n<td id=\"A1.T12.2.2.36.34.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.37.35\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.37.35.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with Adam on Stack Overflow</td>\n<td id=\"A1.T12.2.2.37.35.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.00001</td>\n</tr>\n<tr id=\"A1.T12.2.2.38.36\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.38.36.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with AdaGrad on Stack Overflow</td>\n<td id=\"A1.T12.2.2.38.36.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.00001</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Impact of client and server learning rates.\nTablesÂ 6-11 report how the test Aâ€‹câ€‹câ€‹uâ€‹râ€‹aâ€‹câ€‹yğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦Accuracy changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the Aâ€‹câ€‹câ€‹uâ€‹râ€‹aâ€‹câ€‹yğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦Accuracy scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the Aâ€‹câ€‹câ€‹uâ€‹râ€‹aâ€‹câ€‹yğ´ğ‘ğ‘ğ‘¢ğ‘Ÿğ‘ğ‘ğ‘¦Accuracy values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table 12 for the implementation details of the server and client learning rates used in our current experiments.",
            "Unless otherwise explicitly stated, we used the following default parameter settings in the experiments, as shown in Table 12."
        ]
    }
}