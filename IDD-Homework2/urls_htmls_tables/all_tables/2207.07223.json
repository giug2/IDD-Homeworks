{
    "PAPER'S NUMBER OF TABLES": 12,
    "S4.T1": {
        "caption": "Table 1: Statistics of the Datasets",
        "table": "<table id=\"S4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"S4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">#Training Clients</span></th>\n<th id=\"S4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">#Test Clients</span></th>\n<th id=\"S4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">#Training Samples</span></th>\n<th id=\"S4.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">#Test Samples</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">CIFAR-100</span></th>\n<td id=\"S4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">500</td>\n<td id=\"S4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td id=\"S4.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">50,000</td>\n<td id=\"S4.T1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">10,000</td>\n</tr>\n<tr id=\"S4.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T1.1.3.2.1.1\" class=\"ltx_text ltx_font_bold\">EMNIST-62</span></th>\n<td id=\"S4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center\">3,400</td>\n<td id=\"S4.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\">3,400</td>\n<td id=\"S4.T1.1.3.2.4\" class=\"ltx_td ltx_align_center\">671,585</td>\n<td id=\"S4.T1.1.3.2.5\" class=\"ltx_td ltx_align_center\">77,483</td>\n</tr>\n<tr id=\"S4.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\"><span id=\"S4.T1.1.4.3.1.1\" class=\"ltx_text ltx_font_bold\">Stack Overflow</span></th>\n<td id=\"S4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b\">342,477</td>\n<td id=\"S4.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">204,088</td>\n<td id=\"S4.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">135,818,730</td>\n<td id=\"S4.T1.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">16,586,035</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Impact of client and server learning rates.\nTables 6-11 report how the test A​c​c​u​r​a​c​y𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦Accuracy changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the A​c​c​u​r​a​c​y𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦Accuracy scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the A​c​c​u​r​a​c​y𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦Accuracy values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table 12 for the implementation details of the server and client learning rates used in our current experiments.",
            "Unless otherwise explicitly stated, we used the following default parameter settings in the experiments, as shown in Table 12."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Final Accuracy on CIFAR-100",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></th>\n<th id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">SGDM</span></th>\n<th id=\"S5.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Adam</span></th>\n<th id=\"S5.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T2.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AdaGrad</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedLocal</th>\n<td id=\"S5.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.384</td>\n<td id=\"S5.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.009</td>\n<td id=\"S5.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.113</td>\n</tr>\n<tr id=\"S5.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SCAFFOLD</th>\n<td id=\"S5.T2.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.010</td>\n<td id=\"S5.T2.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.010</td>\n<td id=\"S5.T2.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.010</td>\n</tr>\n<tr id=\"S5.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedLin</th>\n<td id=\"S5.T2.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.440</td>\n<td id=\"S5.T2.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.440</td>\n<td id=\"S5.T2.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.440</td>\n</tr>\n<tr id=\"S5.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedAvg</th>\n<td id=\"S5.T2.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.324</td>\n<td id=\"S5.T2.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.324</td>\n<td id=\"S5.T2.1.5.4.4\" class=\"ltx_td ltx_align_center\">0.324</td>\n</tr>\n<tr id=\"S5.T2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFL</th>\n<td id=\"S5.T2.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.293</td>\n<td id=\"S5.T2.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.346</td>\n<td id=\"S5.T2.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.135</td>\n</tr>\n<tr id=\"S5.T2.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CLIMB</th>\n<td id=\"S5.T2.1.7.6.2\" class=\"ltx_td ltx_align_center\">0.010</td>\n<td id=\"S5.T2.1.7.6.3\" class=\"ltx_td ltx_align_center\">0.010</td>\n<td id=\"S5.T2.1.7.6.4\" class=\"ltx_td ltx_align_center\">0.010</td>\n</tr>\n<tr id=\"S5.T2.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">STEM</th>\n<td id=\"S5.T2.1.8.7.2\" class=\"ltx_td ltx_align_center\">0.014</td>\n<td id=\"S5.T2.1.8.7.3\" class=\"ltx_td ltx_align_center\">0.014</td>\n<td id=\"S5.T2.1.8.7.4\" class=\"ltx_td ltx_align_center\">0.014</td>\n</tr>\n<tr id=\"S5.T2.1.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MimeLite</th>\n<td id=\"S5.T2.1.9.8.2\" class=\"ltx_td ltx_align_center\">0.427</td>\n<td id=\"S5.T2.1.9.8.3\" class=\"ltx_td ltx_align_center\">0.009</td>\n<td id=\"S5.T2.1.9.8.4\" class=\"ltx_td ltx_align_center\">0.009</td>\n</tr>\n<tr id=\"S5.T2.1.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedOpt</th>\n<td id=\"S5.T2.1.10.9.2\" class=\"ltx_td ltx_align_center\">0.425</td>\n<td id=\"S5.T2.1.10.9.3\" class=\"ltx_td ltx_align_center\">0.443</td>\n<td id=\"S5.T2.1.10.9.4\" class=\"ltx_td ltx_align_center\">0.301</td>\n</tr>\n<tr id=\"S5.T2.1.11.10\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.11.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">FedDA</th>\n<td id=\"S5.T2.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T2.1.11.10.2.1\" class=\"ltx_text ltx_font_bold\">0.518</span></td>\n<td id=\"S5.T2.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T2.1.11.10.3.1\" class=\"ltx_text ltx_font_bold\">0.510</span></td>\n<td id=\"S5.T2.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T2.1.11.10.4.1\" class=\"ltx_text ltx_font_bold\">0.488</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            []
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Final Accuracy on EMNIST",
        "table": "<table id=\"S5.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></th>\n<th id=\"S5.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">SGDM</span></th>\n<th id=\"S5.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T3.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Adam</span></th>\n<th id=\"S5.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T3.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AdaGrad</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedLocal</th>\n<td id=\"S5.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.834</td>\n<td id=\"S5.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.055</td>\n<td id=\"S5.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.806</td>\n</tr>\n<tr id=\"S5.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SCAFFOLD</th>\n<td id=\"S5.T3.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.794</td>\n<td id=\"S5.T3.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.794</td>\n<td id=\"S5.T3.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.794</td>\n</tr>\n<tr id=\"S5.T3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedLin</th>\n<td id=\"S5.T3.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.805</td>\n<td id=\"S5.T3.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.805</td>\n<td id=\"S5.T3.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.805</td>\n</tr>\n<tr id=\"S5.T3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedAvg</th>\n<td id=\"S5.T3.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.850</td>\n<td id=\"S5.T3.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.850</td>\n<td id=\"S5.T3.1.5.4.4\" class=\"ltx_td ltx_align_center\">0.850</td>\n</tr>\n<tr id=\"S5.T3.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFL</th>\n<td id=\"S5.T3.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.848</td>\n<td id=\"S5.T3.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.055</td>\n<td id=\"S5.T3.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.047</td>\n</tr>\n<tr id=\"S5.T3.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CLIMB</th>\n<td id=\"S5.T3.1.7.6.2\" class=\"ltx_td ltx_align_center\">0.843</td>\n<td id=\"S5.T3.1.7.6.3\" class=\"ltx_td ltx_align_center\">0.843</td>\n<td id=\"S5.T3.1.7.6.4\" class=\"ltx_td ltx_align_center\">0.843</td>\n</tr>\n<tr id=\"S5.T3.1.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">STEM</th>\n<td id=\"S5.T3.1.8.7.2\" class=\"ltx_td ltx_align_center\">0.051</td>\n<td id=\"S5.T3.1.8.7.3\" class=\"ltx_td ltx_align_center\">0.051</td>\n<td id=\"S5.T3.1.8.7.4\" class=\"ltx_td ltx_align_center\">0.051</td>\n</tr>\n<tr id=\"S5.T3.1.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MimeLite</th>\n<td id=\"S5.T3.1.9.8.2\" class=\"ltx_td ltx_align_center\">0.835</td>\n<td id=\"S5.T3.1.9.8.3\" class=\"ltx_td ltx_align_center\">0.851</td>\n<td id=\"S5.T3.1.9.8.4\" class=\"ltx_td ltx_align_center\">0.821</td>\n</tr>\n<tr id=\"S5.T3.1.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedOpt</th>\n<td id=\"S5.T3.1.10.9.2\" class=\"ltx_td ltx_align_center\">0.838</td>\n<td id=\"S5.T3.1.10.9.3\" class=\"ltx_td ltx_align_center\">0.847</td>\n<td id=\"S5.T3.1.10.9.4\" class=\"ltx_td ltx_align_center\">0.840</td>\n</tr>\n<tr id=\"S5.T3.1.11.10\" class=\"ltx_tr\">\n<th id=\"S5.T3.1.11.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">FedDA</th>\n<td id=\"S5.T3.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T3.1.11.10.2.1\" class=\"ltx_text ltx_font_bold\">0.860</span></td>\n<td id=\"S5.T3.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T3.1.11.10.3.1\" class=\"ltx_text ltx_font_bold\">0.853</span></td>\n<td id=\"S5.T3.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T3.1.11.10.4.1\" class=\"ltx_text ltx_font_bold\">0.868</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            []
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Final Mean Squared Error on EMNIST for Autoencoder",
        "table": "<table id=\"S5.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></th>\n<th id=\"S5.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">SGDM</span></th>\n<th id=\"S5.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Adam</span></th>\n<th id=\"S5.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T4.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AdaGrad</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedLocal</th>\n<td id=\"S5.T4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0169</td>\n<td id=\"S5.T4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0289</td>\n<td id=\"S5.T4.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.0168</td>\n</tr>\n<tr id=\"S5.T4.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedAvg</th>\n<td id=\"S5.T4.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.0171</td>\n<td id=\"S5.T4.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.0171</td>\n<td id=\"S5.T4.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.0171</td>\n</tr>\n<tr id=\"S5.T4.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFL</th>\n<td id=\"S5.T4.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.0168</td>\n<td id=\"S5.T4.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.0290</td>\n<td id=\"S5.T4.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.0291</td>\n</tr>\n<tr id=\"S5.T4.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MimeLite</th>\n<td id=\"S5.T4.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.0183</td>\n<td id=\"S5.T4.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.0307</td>\n<td id=\"S5.T4.1.5.4.4\" class=\"ltx_td ltx_align_center\">0.0287</td>\n</tr>\n<tr id=\"S5.T4.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedOpt</th>\n<td id=\"S5.T4.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.0175</td>\n<td id=\"S5.T4.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.0173</td>\n<td id=\"S5.T4.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.0145</td>\n</tr>\n<tr id=\"S5.T4.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T4.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">FedDA</th>\n<td id=\"S5.T4.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T4.1.7.6.2.1\" class=\"ltx_text ltx_font_bold\">0.0167</span></td>\n<td id=\"S5.T4.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T4.1.7.6.3.1\" class=\"ltx_text ltx_font_bold\">0.0166</span></td>\n<td id=\"S5.T4.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T4.1.7.6.4.1\" class=\"ltx_text ltx_font_bold\">0.0132</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we have evaluated the performance of our FedDA model and other comparison methods in serval representative federated datasets and learning tasks to date. We show that FedDA with decoupling and full batch gradient techniques is able to achieve faster convergence and higher test accuracy in cross-device settings against several state-of-the-art federated optimization methods. The experiments exactly follow the same settings described by a recent federated optimization method, FedOpt ",
                "(Reddi et al., ",
                "2021a",
                ")",
                ".",
                "Datasets.",
                " We focus on three popular computer vision and natural language processing tasks over three representative benchmark datasets respectively: (1) image classification over CIFAR-100 ",
                "(Krizhevsky, ",
                "2009",
                ")",
                ". We train ResNet-18 by replacing batch norm with group norm ",
                "(Hsieh et al., ",
                "2020",
                ")",
                "; (2) image classification over EMNIST ",
                "(Hsieh et al., ",
                "2020",
                ")",
                ". We train a CNN for character recognition; and (3) text classification over Stack Overflow ",
                "(TensorFlow, ",
                "2019",
                ")",
                ". We perform tag prediction using logistic regression on bag-of-words vectors. We select the 10,000 most frequently used words, the 500 most frequent tags and a one-versus-rest classification strategy, by following the same strategy in FedOpt ",
                "(Reddi et al., ",
                "2021a",
                ")",
                ". The detailed descriptions of the federated datasets and learning tasks are presented in Appendix ",
                "A.5",
                ".",
                "Baselines.",
                " We compare the FedDA model with nine state-of-the-art federated learning models, including five regular federated learning and four federated optimization approaches.\n",
                "FedAvg",
                " is a classical as well as practical method for the federated learning of deep networks based on iterative model averaging ",
                "(McMahan et al., ",
                "2017a",
                ")",
                ".\n",
                "SCAFFOLD",
                " is a algorithm which uses control variates to correct for the client-drift in its local updates ",
                "(Karimireddy et al., ",
                "2020d",
                ")",
                ".\n",
                "FedLin",
                " is an algorithmic framework to tackle the key challenges of objective heterogeneity, systems heterogeneity, and imprecise communication in FL ",
                "(Mitra et al., ",
                "2021a",
                ")",
                ".\n",
                "STEM",
                " is a stochastic two-sided momentum algorithm, that utilizes certain momentum-assisted stochastic gradient directions for both the client and server updates ",
                "(Mitra et al., ",
                "2021a",
                ")",
                ".\n",
                "CLIMB",
                " is an agnostic constrained learning formulation to tackle the class imbalance problem in FL without requiring further information beyond the standard FL objective ",
                "(",
                "Anon22d",
                ")",
                ".\n",
                "MFL",
                " performs momentum gradient descent in local update step of FL system to solve the distributed machine learning problem ",
                "(Liu et al., ",
                "2020a",
                ")",
                ".\n",
                "FedOpt",
                " is a flexible algorithmic framework that allows the clients and the server to choose different optimization methods more general than stochastic gradient descent (SGD) in FedAvg ",
                "(Reddi et al., ",
                "2021a",
                ")",
                ".\n",
                "MimeLite",
                " uses a combination of control-variates and server-level optimizer state at every client-update step to ensure that each local update mimics that of the centralized method run on i.i.d. data ",
                "(Karimireddy et al., ",
                "2021",
                ")",
                ".\n",
                "Local Adaptivity (FedLocal)",
                " proposes techniques that enable the use of adaptive optimization methods for local updates at clients in federated learning ",
                "(Wang et al., ",
                "2021b",
                ")",
                ".\nTo our best knowledge, this work is the first to certify the group fairness of classifiers with theoretical input-agnostic guarantees, while there is no need to know the shift between training and deployment datasets with respect to sensitive attributes. All nine baselines used in our experiments either do not use momentum, or use momentum without momentum aggregation, or use momentum with aggregation but restart momentum at each FL round (FedLocal). Our FedDA method keeps the momentum aggregation in the entire FL process, which results in faster convergence but larger oscillation.",
                "Evaluation metrics.",
                " We use two popular measures in federated learning and plot the measure curves with increasing training rounds to verify the convergence of different methods: ",
                "Accuracy",
                " and ",
                "Loss Function Values (Loss)",
                " ",
                "(Karimireddy et al., ",
                "2020d",
                "; Mitra et al., ",
                "2021a",
                "; Liu et al., ",
                "2020a",
                "; Reddi et al., ",
                "2021a",
                "; Karimireddy et al., ",
                "2021",
                "; Wang et al., ",
                "2021b",
                ")",
                ". A larger Accuracy or a smaller Loss score indicates a better federated learning result. We run 1,500 rounds of training on the EMNIST and Stack Overflow datasets, and 4,000 rounds over the CIFAR-100 dataset. In addition, we use final Accuracy to evaluate the quality of the federated learning algorithms.",
                "Convergence on CIFAR-100 and EMNIST.",
                " Figures ",
                "1",
                " and ",
                "3",
                " exhibit the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " curves of ten federated learning models for image classification over CIFAR-100 and character recognition on EMNIST respectively.\nIt is obvious that the performance curves by federated learning algorithms initially keep increasing with training rounds and remains relatively stable when the curves are beyond convergence points, i.e., turning points from a sharp ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " increase to a flat curve. This phenomenon indicates that most federated learning algorithms are able to converge to the invariant solutions after enough training rounds. However, among five regular federated learning and five federated optimization approaches, our FedDA federated optimization method can significantly speedup the convergence on two datasets in most experiments, showing the superior performance of FedDA in federated settings. Compared to the learning results by other federated learning models, based on training rounds at convergence points, FedDA, on average, achieves 34.3% and 22.6% convergence improvement on two datasets respectively.",
                "Loss on CIFAR-100 and EMNIST.",
                " Figures ",
                "2",
                " and ",
                "4",
                " present the ",
                "L",
                "​",
                "o",
                "​",
                "s",
                "​",
                "s",
                "𝐿",
                "𝑜",
                "𝑠",
                "𝑠",
                "Loss",
                " curves achieved by ten federated learning models on two datasets respectively.\nWe have observed obvious that the reverse trends, in comparison with the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " curves. In most experiments, our FedDA federated optimization method is able to achieve the fastest convergence, especially, FedDA can converge within less than 200 training rounds and then always keep stable on the EMNIST dataset. A reasonable explanation is that FedDA fully utilizes the global momentum on each local iteration as well as employ full batch gradients to mimic centralized optimization in the end of the training process for accelerating the training convergence.",
                "Final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " on CIFAR-100 and EMNIST.",
                " Tables ",
                "2",
                "-",
                "4",
                " show the quality of ten federated learning algorithms over CIFAR-100 and EMNIST respectively.\nNotice that the performance achieved by five regular federated learning algorithms, including FedAvg, SCAFFOLD, FedLin, STEM, and CLIMB, keep unchanged when using different optimizers, such as SGDM, Adam, and AdaGrad, while five federated optimization approaches, including MFL, FedOpt, Mime, FedLocal, and our FedDA model obtain different performance.\nWe have observed that our FedDA federated optimization solution outperforms the competitor methods in most experiments. FedDA achieves the highest ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " values (",
                ">",
                ">",
                " 0.488 over CIFAR-100 and ",
                ">",
                ">",
                " 0.853 on EMNIST respectively), which are better than other nine baseline methods in all tests.\nA reasonable explanation is that the combination of decoupling and full batch gradient techniques is able to achieve faster convergence as well as higher test accuracy in cross-device settings. In addition, the promising performance of FedDA over both datasets implies that FedDA has great potential as a general federated optimization solution to learning tasks over federated datasets, which is desirable in practice.",
                "Impact of local iteration numbers.",
                " Figure ",
                "5",
                " shows the impact of the numbers of local iterations in our FedDA model with the adaptive optimizers over the datasets of CIFAR-100, EMNIST, and Stack Overflow respectively. The performance curves initially raise when the local iteration number increases and then keep relatively stable or even drop if the local iteration number keeps increasing. This demonstrates that there must exist a suitable local iteration number for the FL training. A too large number may make the clients overfit to their local datasets, such that the local models are far away from globally optimal models and the FL training achieves slower convergence. On the other hand, a too small number may result in slow convergence of local training and also increase the difficulty of convergence of global training. Thus, it is important to choose the appropriate numbers for well balancing the local training and global training. Notice that the final accuracy of AdaGrad and SGDM is closed to 0 on the EMNIST dataset when the local iteration number is larger than 10. A reasonable explanation is EMNIST is a simple dataset and a large local iteration number makes local models converge to their local minimum, which may be distant from the minimum of global model. This leads to lower accuracy of global model.",
                "Impact of training round numbers.",
                " Figures ",
                "6",
                " (a)-(c) present the performance achieved by our FedDA method with varying the numbers of training rounds from 100 to 2,000, from 10 to 1,000, and from 50 to 1,000 on three datasets. It is obvious that the performance curves with each optimizer keep increasing with the increased number of training rounds. This phenomenon indicates that the accuracy in the federated settings are sensitive to training rounds. This is because the special data and system heterogeneity issues in the FL increase the difficulty in converging in a short time and the FL models need more training rounds to obtain the desired learning results. However, as shown in the above experiments of convergence in Figures ",
                "1",
                "-",
                "4",
                ", our FedDA method presents superior convergence performance, compared with other FL algorithms, including both regular federated learning and federated optimization approaches."
            ]
        ]
    },
    "A1.T5": {
        "caption": "Table 5: Final Accuracy on Stack Overflow",
        "table": "<table id=\"A1.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T5.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Optimizer</span></th>\n<th id=\"A1.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A1.T5.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">SGDM</span></th>\n<th id=\"A1.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A1.T5.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Adam</span></th>\n<th id=\"A1.T5.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"A1.T5.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">AdaGrad</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T5.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedLocal</th>\n<td id=\"A1.T5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.152</td>\n<td id=\"A1.T5.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.576</td>\n<td id=\"A1.T5.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.229</td>\n</tr>\n<tr id=\"A1.T5.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">SCAFFOLD</th>\n<td id=\"A1.T5.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.250</td>\n<td id=\"A1.T5.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.250</td>\n<td id=\"A1.T5.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.250</td>\n</tr>\n<tr id=\"A1.T5.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedLin</th>\n<td id=\"A1.T5.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.224</td>\n<td id=\"A1.T5.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.224</td>\n<td id=\"A1.T5.1.4.3.4\" class=\"ltx_td ltx_align_center\">0.224</td>\n</tr>\n<tr id=\"A1.T5.1.5.4\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedAvg</th>\n<td id=\"A1.T5.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.252</td>\n<td id=\"A1.T5.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.252</td>\n<td id=\"A1.T5.1.5.4.4\" class=\"ltx_td ltx_align_center\">0.252</td>\n</tr>\n<tr id=\"A1.T5.1.6.5\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MFL</th>\n<td id=\"A1.T5.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.134</td>\n<td id=\"A1.T5.1.6.5.3\" class=\"ltx_td ltx_align_center\">0.101</td>\n<td id=\"A1.T5.1.6.5.4\" class=\"ltx_td ltx_align_center\">0.215</td>\n</tr>\n<tr id=\"A1.T5.1.7.6\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">CLIMB</th>\n<td id=\"A1.T5.1.7.6.2\" class=\"ltx_td ltx_align_center\">0.302</td>\n<td id=\"A1.T5.1.7.6.3\" class=\"ltx_td ltx_align_center\">0.302</td>\n<td id=\"A1.T5.1.7.6.4\" class=\"ltx_td ltx_align_center\">0.302</td>\n</tr>\n<tr id=\"A1.T5.1.8.7\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">STEM</th>\n<td id=\"A1.T5.1.8.7.2\" class=\"ltx_td ltx_align_center\">0.196</td>\n<td id=\"A1.T5.1.8.7.3\" class=\"ltx_td ltx_align_center\">0.196</td>\n<td id=\"A1.T5.1.8.7.4\" class=\"ltx_td ltx_align_center\">0.196</td>\n</tr>\n<tr id=\"A1.T5.1.9.8\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.9.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">MimeLite</th>\n<td id=\"A1.T5.1.9.8.2\" class=\"ltx_td ltx_align_center\">0.271</td>\n<td id=\"A1.T5.1.9.8.3\" class=\"ltx_td ltx_align_center\">0.211</td>\n<td id=\"A1.T5.1.9.8.4\" class=\"ltx_td ltx_align_center\">0.078</td>\n</tr>\n<tr id=\"A1.T5.1.10.9\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.10.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">FedOpt</th>\n<td id=\"A1.T5.1.10.9.2\" class=\"ltx_td ltx_align_center\">0.225</td>\n<td id=\"A1.T5.1.10.9.3\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.1.10.9.3.1\" class=\"ltx_text ltx_font_bold\">0.642</span></td>\n<td id=\"A1.T5.1.10.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"A1.T5.1.10.9.4.1\" class=\"ltx_text ltx_font_bold\">0.691</span></td>\n</tr>\n<tr id=\"A1.T5.1.11.10\" class=\"ltx_tr\">\n<th id=\"A1.T5.1.11.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">FedDA</th>\n<td id=\"A1.T5.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"A1.T5.1.11.10.2.1\" class=\"ltx_text ltx_font_bold\">0.273</span></td>\n<td id=\"A1.T5.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"A1.T5.1.11.10.3.1\" class=\"ltx_text ltx_font_bold\">0.642</span></td>\n<td id=\"A1.T5.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.674</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Final A​c​c​u​r​a​c​y𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦Accuracy over Stack Overflow. Table 5 presents the final A​c​c​u​r​a​c​y𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦Accuracy scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDA is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final A​c​c​u​r​a​c​y𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦Accuracy scores achieved by FedDA averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDA method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training."
        ]
    },
    "A1.T6": {
        "caption": "Table 6: Final Accuracy with SGDM Optimizer and Varying Client Learning Rate",
        "table": "<table id=\"A1.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T6.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T6.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Server Learning Rate</span></th>\n<th id=\"A1.T6.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T6.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Client Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T6.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T6.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.1</th>\n<td id=\"A1.T6.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.521 / 1</td>\n<td id=\"A1.T6.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.448 / 3.3</td>\n<td id=\"A1.T6.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.236 / 10</td>\n<td id=\"A1.T6.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.099 / 33</td>\n</tr>\n<tr id=\"A1.T6.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T6.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.1</th>\n<td id=\"A1.T6.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.778 / 1</td>\n<td id=\"A1.T6.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.819 / 3.3</td>\n<td id=\"A1.T6.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.783 / 10</td>\n<td id=\"A1.T6.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.052 / 33</td>\n</tr>\n<tr id=\"A1.T6.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T6.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T6.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">0.1</th>\n<td id=\"A1.T6.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.174 / 1</td>\n<td id=\"A1.T6.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.262 / 10</td>\n<td id=\"A1.T6.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.250 / 100</td>\n<td id=\"A1.T6.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.002 / 1,000</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " and ",
                "L",
                "​",
                "o",
                "​",
                "s",
                "​",
                "s",
                "𝐿",
                "𝑜",
                "𝑠",
                "𝑠",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDA achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDA is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores achieved by FedDA averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDA method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTables ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T7": {
        "caption": "Table 7: Final Accuracy with Adam Optimizer and Varying Client Learning Rate",
        "table": "<table id=\"A1.T7.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T7.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T7.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T7.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T7.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T7.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Server Learning Rate</span></th>\n<th id=\"A1.T7.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T7.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Client Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T7.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T7.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T7.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.1</th>\n<td id=\"A1.T7.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.179 / 1</td>\n<td id=\"A1.T7.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.317 / 3.3</td>\n<td id=\"A1.T7.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.269 / 10</td>\n<td id=\"A1.T7.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.179 / 33</td>\n</tr>\n<tr id=\"A1.T7.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T7.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T7.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.03</th>\n<td id=\"A1.T7.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.488 / 0.01</td>\n<td id=\"A1.T7.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.836 / 0.1</td>\n<td id=\"A1.T7.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.842 / 0.3</td>\n<td id=\"A1.T7.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.804 / 1</td>\n</tr>\n<tr id=\"A1.T7.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T7.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T7.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">0.3</th>\n<td id=\"A1.T7.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.409 / 0.1</td>\n<td id=\"A1.T7.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.492 / 1</td>\n<td id=\"A1.T7.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.473 / 10</td>\n<td id=\"A1.T7.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.459 / 100</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " and ",
                "L",
                "​",
                "o",
                "​",
                "s",
                "​",
                "s",
                "𝐿",
                "𝑜",
                "𝑠",
                "𝑠",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDA achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDA is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores achieved by FedDA averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDA method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTables ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T8": {
        "caption": "Table 8: Final Accuracy with AdaGrad Optimizer and Varying Client Learning Rate",
        "table": "<table id=\"A1.T8.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T8.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T8.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T8.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T8.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T8.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Server Learning Rate</span></th>\n<th id=\"A1.T8.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T8.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Client Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T8.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T8.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T8.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.1</th>\n<td id=\"A1.T8.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.462 / 0.03</td>\n<td id=\"A1.T8.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.488 / 0.1</td>\n<td id=\"A1.T8.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.410 / 0.3</td>\n<td id=\"A1.T8.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.259 / 1</td>\n</tr>\n<tr id=\"A1.T8.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T8.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T8.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.1</th>\n<td id=\"A1.T8.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.055 / 0.00001</td>\n<td id=\"A1.T8.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.055 / 0.001</td>\n<td id=\"A1.T8.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.055 / 0.01</td>\n<td id=\"A1.T8.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.868 / 0.1</td>\n</tr>\n<tr id=\"A1.T8.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T8.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T8.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">10</th>\n<td id=\"A1.T8.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.576 /1</td>\n<td id=\"A1.T8.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.651 / 10</td>\n<td id=\"A1.T8.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.629 / 30</td>\n<td id=\"A1.T8.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.632 / 100</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " and ",
                "L",
                "​",
                "o",
                "​",
                "s",
                "​",
                "s",
                "𝐿",
                "𝑜",
                "𝑠",
                "𝑠",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDA achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDA is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores achieved by FedDA averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDA method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTables ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T9": {
        "caption": "Table 9: Final Accuracy with SGDM Optimizer and Varying Server Learning Rate",
        "table": "<table id=\"A1.T9.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T9.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T9.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T9.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T9.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Client Learning Rate</span></th>\n<th id=\"A1.T9.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T9.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Server Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T9.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T9.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.03</th>\n<td id=\"A1.T9.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.374 / 1</td>\n<td id=\"A1.T9.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.522 / 3.3</td>\n<td id=\"A1.T9.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.356 / 10</td>\n<td id=\"A1.T9.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.273 / 33</td>\n</tr>\n<tr id=\"A1.T9.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T9.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.1</th>\n<td id=\"A1.T9.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.859 / 0.3</td>\n<td id=\"A1.T9.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.861 / 1</td>\n<td id=\"A1.T9.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.778 / 3.3</td>\n<td id=\"A1.T9.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.783 / 10</td>\n</tr>\n<tr id=\"A1.T9.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T9.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T9.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">100</th>\n<td id=\"A1.T9.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.191 / 0.001</td>\n<td id=\"A1.T9.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.256 / 0.003</td>\n<td id=\"A1.T9.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.204 / 0.01</td>\n<td id=\"A1.T9.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.292 / 0.1</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " and ",
                "L",
                "​",
                "o",
                "​",
                "s",
                "​",
                "s",
                "𝐿",
                "𝑜",
                "𝑠",
                "𝑠",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDA achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDA is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores achieved by FedDA averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDA method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTables ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T10": {
        "caption": "Table 10: Final Accuracy with Adam Optimizer and Varying Server Learning Rate",
        "table": "<table id=\"A1.T10.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T10.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T10.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T10.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T10.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Client Learning Rate</span></th>\n<th id=\"A1.T10.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T10.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Server Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T10.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T10.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.03</th>\n<td id=\"A1.T10.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.510 / 0.33</td>\n<td id=\"A1.T10.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.183 / 3.3</td>\n<td id=\"A1.T10.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.010 / 10</td>\n<td id=\"A1.T10.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.010 / 33</td>\n</tr>\n<tr id=\"A1.T10.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T10.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.03</th>\n<td id=\"A1.T10.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.546 / 0.1</td>\n<td id=\"A1.T10.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.803 / 1</td>\n<td id=\"A1.T10.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.051 / 3.3</td>\n<td id=\"A1.T10.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.051 / 10</td>\n</tr>\n<tr id=\"A1.T10.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T10.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T10.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">100</th>\n<td id=\"A1.T10.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.425 / 0.1</td>\n<td id=\"A1.T10.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.522 / 0.3</td>\n<td id=\"A1.T10.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.641 / 1</td>\n<td id=\"A1.T10.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.633 / 10</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " and ",
                "L",
                "​",
                "o",
                "​",
                "s",
                "​",
                "s",
                "𝐿",
                "𝑜",
                "𝑠",
                "𝑠",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDA achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDA is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores achieved by FedDA averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDA method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTables ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T11": {
        "caption": "Table 11: Final Accuracy with AdaGrad Optimizer and Varying Server Learning Rate",
        "table": "<table id=\"A1.T11.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A1.T11.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T11.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"A1.T11.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"A1.T11.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Client Learning Rate</span></th>\n<th id=\"A1.T11.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_t\" colspan=\"4\"><span id=\"A1.T11.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Accuracy / Server Learning Rate</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T11.1.2.1\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">CIFAR-100</th>\n<th id=\"A1.T11.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.03</th>\n<td id=\"A1.T11.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.352 / 0.03</td>\n<td id=\"A1.T11.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.462 / 0.1</td>\n<td id=\"A1.T11.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">0.466 / 0.3</td>\n<td id=\"A1.T11.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">0.312 / 1</td>\n</tr>\n<tr id=\"A1.T11.1.3.2\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\">EMNIST</th>\n<th id=\"A1.T11.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.03</th>\n<td id=\"A1.T11.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.806 / 0.1</td>\n<td id=\"A1.T11.1.3.2.4\" class=\"ltx_td ltx_align_center\">0.055 / 3.3</td>\n<td id=\"A1.T11.1.3.2.5\" class=\"ltx_td ltx_align_center\">0.055 / 10</td>\n<td id=\"A1.T11.1.3.2.6\" class=\"ltx_td ltx_align_center\">0.055 / 33</td>\n</tr>\n<tr id=\"A1.T11.1.4.3\" class=\"ltx_tr\">\n<th id=\"A1.T11.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\">Stack Overflow</th>\n<th id=\"A1.T11.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">100</th>\n<td id=\"A1.T11.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b\">0.227 / 0.1</td>\n<td id=\"A1.T11.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b\">0.306 / 0.3</td>\n<td id=\"A1.T11.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b\">0.397 / 1</td>\n<td id=\"A1.T11.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b\">0.632 / 10</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this section, we conduct more experiments to validate the accuracy and convergence of our proposed FedDAmethod and evaluate the sensitivity of client and server learning rates in our momentum decoupling adaptive optimization method for the FL task.",
                "Convergence and loss over Stack Overflow.",
                " Figures ",
                "7",
                "-",
                "8",
                " present the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " and ",
                "L",
                "​",
                "o",
                "​",
                "s",
                "​",
                "s",
                "𝐿",
                "𝑜",
                "𝑠",
                "𝑠",
                "Loss",
                " curves of ten federated learning algorithms on Stack Overflow.\nSimilar trends are observed for the performance comparison: FedDA achieves the 75.4% convergence improvement, which are obviously better than other methods in most experiments. This demonstrates that the full batch gradient techniques that mimic centralized optimization in the end of the training process are able to ensure the convergence and overcome the possible inconsistency caused by adaptive optimization methods.",
                "Final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " over Stack Overflow.",
                " Table ",
                "5",
                " presents the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores of ten federated learning algorithms on Stack Overflow. We have observed similar trends: the accuracy achieved by our FedDA is the highest in most tests. Especially, as shown the experiment with SGDM as the optimizer, compared to the best competitors among ten federated learning algorithms, the final ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores achieved by FedDA averagely achieves 22.3% improvement. A rational guess is that the global momentum in our FedDA method makes the best effort to mimic the role of momentum in centralized training, which can accelerate the convergence of FL training.",
                "Impact of client and server learning rates.",
                "\nTables ",
                "6",
                "-",
                "11",
                " report how the test ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the ",
                "A",
                "​",
                "c",
                "​",
                "c",
                "​",
                "u",
                "​",
                "r",
                "​",
                "a",
                "​",
                "c",
                "​",
                "y",
                "𝐴",
                "𝑐",
                "𝑐",
                "𝑢",
                "𝑟",
                "𝑎",
                "𝑐",
                "𝑦",
                "Accuracy",
                " values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table ",
                "12",
                " for the implementation details of the server and client learning rates used in our current experiments."
            ]
        ]
    },
    "A1.T12": {
        "caption": "Table 12: Hyperparameter Settings",
        "table": "<table id=\"A1.T12.2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T12.2.2.3.1\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T12.2.2.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Parameter</span></td>\n<td id=\"A1.T12.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\"><span id=\"A1.T12.2.2.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Value</span></td>\n</tr>\n<tr id=\"A1.T12.2.2.4.2\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Training rounds for EMNIST and Stack Overflow</td>\n<td id=\"A1.T12.2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1,500</td>\n</tr>\n<tr id=\"A1.T12.2.2.5.3\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Training rounds for CIFAR-100</td>\n<td id=\"A1.T12.2.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">4,000</td>\n</tr>\n<tr id=\"A1.T12.1.1.1\" class=\"ltx_tr\">\n<td id=\"A1.T12.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Momentum parameter <math id=\"A1.T12.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\beta_{1}\" display=\"inline\"><semantics id=\"A1.T12.1.1.1.1.m1.1a\"><msub id=\"A1.T12.1.1.1.1.m1.1.1\" xref=\"A1.T12.1.1.1.1.m1.1.1.cmml\"><mi id=\"A1.T12.1.1.1.1.m1.1.1.2\" xref=\"A1.T12.1.1.1.1.m1.1.1.2.cmml\">β</mi><mn id=\"A1.T12.1.1.1.1.m1.1.1.3\" xref=\"A1.T12.1.1.1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.T12.1.1.1.1.m1.1b\"><apply id=\"A1.T12.1.1.1.1.m1.1.1.cmml\" xref=\"A1.T12.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.T12.1.1.1.1.m1.1.1.1.cmml\" xref=\"A1.T12.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"A1.T12.1.1.1.1.m1.1.1.2.cmml\" xref=\"A1.T12.1.1.1.1.m1.1.1.2\">𝛽</ci><cn type=\"integer\" id=\"A1.T12.1.1.1.1.m1.1.1.3.cmml\" xref=\"A1.T12.1.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T12.1.1.1.1.m1.1c\">\\beta_{1}</annotation></semantics></math>\n</td>\n<td id=\"A1.T12.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.9</td>\n</tr>\n<tr id=\"A1.T12.2.2.2\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Second moment parameter <math id=\"A1.T12.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\beta_{2}\" display=\"inline\"><semantics id=\"A1.T12.2.2.2.1.m1.1a\"><msub id=\"A1.T12.2.2.2.1.m1.1.1\" xref=\"A1.T12.2.2.2.1.m1.1.1.cmml\"><mi id=\"A1.T12.2.2.2.1.m1.1.1.2\" xref=\"A1.T12.2.2.2.1.m1.1.1.2.cmml\">β</mi><mn id=\"A1.T12.2.2.2.1.m1.1.1.3\" xref=\"A1.T12.2.2.2.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"A1.T12.2.2.2.1.m1.1b\"><apply id=\"A1.T12.2.2.2.1.m1.1.1.cmml\" xref=\"A1.T12.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"A1.T12.2.2.2.1.m1.1.1.1.cmml\" xref=\"A1.T12.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"A1.T12.2.2.2.1.m1.1.1.2.cmml\" xref=\"A1.T12.2.2.2.1.m1.1.1.2\">𝛽</ci><cn type=\"integer\" id=\"A1.T12.2.2.2.1.m1.1.1.3.cmml\" xref=\"A1.T12.2.2.2.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"A1.T12.2.2.2.1.m1.1c\">\\beta_{2}</annotation></semantics></math>\n</td>\n<td id=\"A1.T12.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.99</td>\n</tr>\n<tr id=\"A1.T12.2.2.6.4\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with SGDM on CIFAR-100</td>\n<td id=\"A1.T12.2.2.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.03</td>\n</tr>\n<tr id=\"A1.T12.2.2.7.5\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with Adam on CIFAR-100</td>\n<td id=\"A1.T12.2.2.7.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.03</td>\n</tr>\n<tr id=\"A1.T12.2.2.8.6\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.8.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with AdaGrad on CIFAR-100</td>\n<td id=\"A1.T12.2.2.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.9.7\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.9.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with SGDM on CIFAR-100</td>\n<td id=\"A1.T12.2.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">3.3</td>\n</tr>\n<tr id=\"A1.T12.2.2.10.8\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.10.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with Adam on CIFAR-100</td>\n<td id=\"A1.T12.2.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.3</td>\n</tr>\n<tr id=\"A1.T12.2.2.11.9\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.11.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with AdaGrad on CIFAR-100</td>\n<td id=\"A1.T12.2.2.11.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.12.10\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.12.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with SGDM on CIFAR-100</td>\n<td id=\"A1.T12.2.2.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.13.11\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.13.11.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with Adam on CIFAR-100</td>\n<td id=\"A1.T12.2.2.13.11.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.14.12\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.14.12.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with AdaGrad on CIFAR-100</td>\n<td id=\"A1.T12.2.2.14.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.15.13\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.15.13.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with Adam on CIFAR-100</td>\n<td id=\"A1.T12.2.2.15.13.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.16.14\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.16.14.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with AdaGrad on CIFAR-100</td>\n<td id=\"A1.T12.2.2.16.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.17.15\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.17.15.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with SGDM on EMNIST</td>\n<td id=\"A1.T12.2.2.17.15.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.18.16\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.18.16.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with Adam on EMNIST</td>\n<td id=\"A1.T12.2.2.18.16.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.19.17\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.19.17.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with AdaGrad on EMNIST</td>\n<td id=\"A1.T12.2.2.19.17.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.20.18\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.20.18.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with SGDM on EMNIST</td>\n<td id=\"A1.T12.2.2.20.18.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1</td>\n</tr>\n<tr id=\"A1.T12.2.2.21.19\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.21.19.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with Adam on EMNIST</td>\n<td id=\"A1.T12.2.2.21.19.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.22.20\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.22.20.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with AdaGrad on EMNIST</td>\n<td id=\"A1.T12.2.2.22.20.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.23.21\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.23.21.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with SGDM on EMNIST</td>\n<td id=\"A1.T12.2.2.23.21.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10</td>\n</tr>\n<tr id=\"A1.T12.2.2.24.22\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.24.22.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with Adam on EMNIST</td>\n<td id=\"A1.T12.2.2.24.22.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10</td>\n</tr>\n<tr id=\"A1.T12.2.2.25.23\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.25.23.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with AdaGrad on EMNIST</td>\n<td id=\"A1.T12.2.2.25.23.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.26.24\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.26.24.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with Adam on EMNIST</td>\n<td id=\"A1.T12.2.2.26.24.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.27.25\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.27.25.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with AdaGrad on EMNIST</td>\n<td id=\"A1.T12.2.2.27.25.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.1</td>\n</tr>\n<tr id=\"A1.T12.2.2.28.26\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.28.26.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with SGDM on Stack Overflow</td>\n<td id=\"A1.T12.2.2.28.26.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">100</td>\n</tr>\n<tr id=\"A1.T12.2.2.29.27\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.29.27.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with Adam on Stack Overflow</td>\n<td id=\"A1.T12.2.2.29.27.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">100</td>\n</tr>\n<tr id=\"A1.T12.2.2.30.28\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.30.28.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Client learning rate with AdaGrad on Stack Overflow</td>\n<td id=\"A1.T12.2.2.30.28.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">100</td>\n</tr>\n<tr id=\"A1.T12.2.2.31.29\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.31.29.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with SGDM on Stack Overflow</td>\n<td id=\"A1.T12.2.2.31.29.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10</td>\n</tr>\n<tr id=\"A1.T12.2.2.32.30\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.32.30.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with Adam on Stack Overflow</td>\n<td id=\"A1.T12.2.2.32.30.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1</td>\n</tr>\n<tr id=\"A1.T12.2.2.33.31\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.33.31.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Server learning rate with AdaGrad on Stack Overflow</td>\n<td id=\"A1.T12.2.2.33.31.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">10</td>\n</tr>\n<tr id=\"A1.T12.2.2.34.32\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.34.32.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with SGDM on Stack Overflow</td>\n<td id=\"A1.T12.2.2.34.32.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.35.33\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.35.33.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with Adam on Stack Overflow</td>\n<td id=\"A1.T12.2.2.35.33.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">1</td>\n</tr>\n<tr id=\"A1.T12.2.2.36.34\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.36.34.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Local iteration number with AdaGrad on Stack Overflow</td>\n<td id=\"A1.T12.2.2.36.34.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">5</td>\n</tr>\n<tr id=\"A1.T12.2.2.37.35\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.37.35.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with Adam on Stack Overflow</td>\n<td id=\"A1.T12.2.2.37.35.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.00001</td>\n</tr>\n<tr id=\"A1.T12.2.2.38.36\" class=\"ltx_tr\">\n<td id=\"A1.T12.2.2.38.36.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">Fuzz factor with AdaGrad on Stack Overflow</td>\n<td id=\"A1.T12.2.2.38.36.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-left:2.0pt;padding-right:2.0pt;\">0.00001</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Impact of client and server learning rates.\nTables 6-11 report how the test A​c​c​u​r​a​c​y𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦Accuracy changes with server and client learning rates on three datasets by fixing the server learning rates and changing the client learning rates, or by utilizing the reverse settings. We have observed that the A​c​c​u​r​a​c​y𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦Accuracy scores oscillate within the range of 0.002 and 0.868 when changing the client learning rates, while the A​c​c​u​r​a​c​y𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦Accuracy values fluctuate between 0.010 and 0.861. This demonstrates that it is crucial to choose the optimal learning rates for the training on the clients and server to achieve the competitive performance. Please refer to Table 12 for the implementation details of the server and client learning rates used in our current experiments.",
            "Unless otherwise explicitly stated, we used the following default parameter settings in the experiments, as shown in Table 12."
        ]
    }
}