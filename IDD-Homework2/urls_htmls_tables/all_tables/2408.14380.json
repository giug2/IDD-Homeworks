{
    "id_table_1": {
        "caption": "Table 1:  Overall F1 and MCC results on simple prompts,  Ln  stands for knowledge enhancement in layer n.",
        "table": "A1.T3.1",
        "footnotes": [],
        "references": [
            "To address this challenge, our work proposes an innovative approach of probing intrinsic manipulation of causality for LLMs.  As shown in Fig.  1 , firstly we construct a classification dataset for detecting entities and relationships of causality in sentences.  Then we guide behaviors of LLMs by hierarchically add  shortcuts  on this classification task.  We integrate retrieval augmented generation (RAG) and in context learning (ICL) for providing shortcuts.  This takes into account the effects of prompts and pretrained knowledge into consideration while probing.  Finally, we observe performance variance under different RAG and ICL, to probe intrinsic manipulation of causality.  We conduct experiments on LLMs in various parameters sizes and domain knowledge.  The experimental results show that LLMs are sensitive to global semantics in classification, and show a certain ability to identify causal entities with guidance. But they do not have direct cognition of causal relationships, lacking a fixed processing route for causality. This leads to sub-optimal performance in more complex problem scenarios for causality, indicating necessity for further attention in LLMs training.",
            "In this section, we propose a hierarchical probing approach.  As shown in Fig.  1 , our method provides \"shortcuts\" hierarchically to LLMs in classification tasks.  These shortcuts include necessary steps for causality manipulation, like entities recognition and alignment, causal relation cognition.  By comparing whether these shortcuts are beneficial to tasks performance, intrinsic manipulation of causality is probed.  We exploit a combination of RAG and ICL for providing shortcuts to guide LLMs.  For evaluation, we rewrite classification tasks in Sec.  3  into a question and answer form, requesting LLMs to judge whether causality of the sentence is right.",
            "We add different augmentation for LLMs, forming a hierarchical structure in Fig.  1 , notated as  layers .  For each layer, we retrieve most relevant sentences and attach them in questions for LLMs.  From layer 1 to layer 3, we provide more complex guidance from shortcuts, representing a more ideal and detailed manipulation of causality. And we aim to probe whether models show identical manipulation as guided, which can be observed from performance changes.",
            "We provide prompts with necessary information, following a prompt framework in community  5 5 5 https://www.promptingguide.ai/ .  We do not conduct further prompt engineering, since we believe that LLMs should comprehends natural prompts.  Prompt includes instructions, contexts, input data and output indicator, introduction of these components can be found in Appendix.  E.1 .",
            "We probe LLMs with different parameters size (GPT-3.5 and ChatGLM) on simple prompts, and conduct parallel experiments on supervised BERT for comparison.  Results are shown in Table  1 .",
            "When using a simple prompt, we directly connect the additional knowledge with the question content in a straightforward manner, as illustrated by the example in Table  5 . In contrast, when using an advanced prompt, we employ multi-turn dialogues to emphasize the task content and separate the parts that provide knowledge from those that pose questions. This approach allows the model to understand the task content, and the boundaries between knowledge and questions more clearly. Examples of this can be found in Table  E.1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Overall results for advanced prompts,  Ln  stands for knowledge enhancement in layer n.",
        "table": "A3.T4.1",
        "footnotes": [],
        "references": [
            "For classification, we sample original sentences from CMedCausal as  positive instances , as they contain correct causation.  And we produce  negative instances  with certain manners (notated as  Actions  or  Act. ).  Trough different actions, causation between entities are disturbed, but other parts of sentences are preserved in the best effort.  Fig.  2  gives instance of three actions.",
            "The  simple prompt  provides components mentioned above, but no additional guidance. It is used to probe native thinking process directly.  In order to better exploit causality ability of LLMs, we use  advanced prompt  for additional experiments, indicating an upper bound.  Advanced prompt integrates chain of thoughts similar to  (Wei et al.,  2022b ) , which prompts models with types of mistakes (e.g. wrong orders of entities in causality), and instruct models to give an explanation and then conduct classification.  Since final sentences concatenated will be very long, we append some part of sentence (i.e. knowledge provided) into history with a multiple rounds dialog.  Examples of simple prompt and advanced prompt can be found in Appendix  E.2 .",
            "We integrate advanced prompt to better exploit ability of LLMs, results are shown in Table  2 .  This is main evidence for subsequent probing."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  The statistical results of the dataset include the sentence length, the number of relations contained in each sentence, and the specific quantity of each relation.",
        "table": "A5.T5.1",
        "footnotes": [],
        "references": [
            "In this section, we propose a hierarchical probing approach.  As shown in Fig.  1 , our method provides \"shortcuts\" hierarchically to LLMs in classification tasks.  These shortcuts include necessary steps for causality manipulation, like entities recognition and alignment, causal relation cognition.  By comparing whether these shortcuts are beneficial to tasks performance, intrinsic manipulation of causality is probed.  We exploit a combination of RAG and ICL for providing shortcuts to guide LLMs.  For evaluation, we rewrite classification tasks in Sec.  3  into a question and answer form, requesting LLMs to judge whether causality of the sentence is right.",
            "(1) Experimental results of MCC show that LLMs have weak causality ability on given classification task.  But it is not comparable with supervised models like BERT.  (2) Performance of LLMs varies. Reasons may include parameters, training strategies and domain knowledge. We discuss this in Appendix  G .  (3) Additionally, models show preferences for different actions in dataset, as shown in Fig.  3 .",
            "In the dataset, all data can be annotated in the triplet form of <Head, Tail, Relation>. We have conducted statistical analysis on the average length of data and the number of triplets corresponding to each relation in the dataset. The specific statistical results can be found in Table  3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Examples of Sentences and Corresponding Knowledge",
        "table": "A5.SS1.tab1.1",
        "footnotes": [],
        "references": [
            "Table  4  provides examples of the corresponding knowledge provided to the model in Layer 3 when answering questions. To ensure the reliability of the knowledge provided in Layer 3, we randomly selected 50 samples to check whether the additional medical knowledge provided is related to the content of the question or the entities mentioned in the question. In the 50 samples examined, 43 of them had medical entities in the knowledge section that were related to the question statement, while the rest were unrelated. There were 31 samples that provided clear descriptive help for the causal relationship judgment of the question statement, whereas 19 did not offer significant useful information.",
            "To engage retrieval pipeline, we divide each sentence into chunks, allowing for overlap between them, and then encode each chunk using Sentence Transformer  (Reimers and Gurevych,  2019 ) .  We treat input of LLMs as a query and utilize FAISS (Facebook AI Similarity Search)  (Johnson et al.,  2021 )  to efficiently match the encoded query with locally stored sentence vectors, retrieving the top  k k k italic_k  (set to 2 practically) most relevant chunks.  Since the retrieved sentence fragments may be incomplete, directly providing this knowledge to models would result in receiving inadequate information or incoherent statements.  To address this issue, we control locations of retrieved text fragments in the original text and individually expand the head and tail of the fragment until it forms a complete sentence.  Specifically, due to the large volume of textual data in the medical knowledge graph at Layer 3, directly using Langchain for item-by-item matching is highly inefficient. Therefore, we have adopted a hierarchical retrieval strategy. As shown in Fig.  4 , we first match the input text with disease names in the knowledge graph to select the most relevant diseases. Then, we match the input text with the textual descriptions corresponding to the selected diseases to identify the medical knowledge most relevant to the input text. This selected medical knowledge is ultimately integrated into the external medical knowledge required at Layer 3."
        ]
    },
    "global_footnotes": [
        "Our code and implementation are available at",
        ".",
        "Cause and effect of CMedCausal usally contains medical named entities.",
        "The sequence starts with first sentence contains causal mentions and ends with the last sentence contains causal mentions."
    ]
}