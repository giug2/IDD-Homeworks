{
    "id_table_1": {
        "caption": "Table 1.  Evaluation of the inference costs on MSR-VTT dataset. We report the R@1 metric for the text-to-video task.   Blue  means stage 1 generative time cost, and   red  means stage 2 time cost. The improvement of inference time refers to the compression ratio of T2VIndexer, which is obtained by dividing the inference time with T2VIndexer by the inference time of the baseline.",
        "table": "S4.T1.5.1",
        "footnotes": [],
        "references": [
            "Existing text-video retrieval methods can be divided into two categories, namely  one-stream  and  two-stream  approaches. One-stream approaches  (Zhu and Yang,  2020 )   (Lei et al . ,  2021 )   (Luo et al . ,  2020 )  adopts deep models for feature-level interactions between each text-video pair to predict its similarity score, which require online feature extraction and fail to be applied for the time-sensitive retrieval stage. Thus, the efficient two-stream approaches  (Gabeur et al . ,  2020 )   (Luo et al . ,  2022 )   (Wu et al . ,  2023 )  are widely applied. As shown in Figure  1  (a), they encode each video and text independently into dense embeddings and then adopt simple matching functions to measure their similarity. Since there are no text-video interactions in the encoding stage, two-stream approaches allows offline data embedding extraction and alleviating online computation. Some recent works begin to focus on the issues of reducing the high computational overload of dense video embedding by sparsely sampling a few clips  (Lei et al . ,  2021 )  (see Figure  1  (b)). However, all the existing solutions require to measure the query-video similarities and then rank videos for the entire video set ( i.e. , one-to-all retrieval framework). Thus, their online retrieval time grows linearly with the increase of retrieved videos, which limits their scalability on large-scale scenarios.",
            "To address the above issue, we explore to fundamentally change the traditional one-to-all embedding retrieval framework by a generative deep model that directly generates video identifiers and retrieves video candidates with constant time complexity. As illustrated in Figure  1  (c), our target of this work is not to propose a new model on text-video retrieval. We mainly investigate how to design a model-based indexer that effectively retrieves query-relevant video candidates, which shortens the overall retrieval time while maintaining the retrieval accuracy of state-of-the-art ranking models. To this end, we propose a sequence-to-sequence generative network that supports  T ext query to  V ideo candidate  Index , named as  T2VIndexer . The model is based on the encoder-decoder that feeds the query into the encoder and generates the identifier of the video candidate through the decoder. It is trained by query-identifier pairs that supports controllable video recall at different semantic grained. During inference, the top  K K K italic_K  videos are directly retrieved by beam search and identifier constrain.",
            "The purpose of T2VIndexer is to improve retrieval efficiency while maintaining accuracy. In Table  1 , we analyzed the accuracy and efficiency of T2VIndexer under different candidate sets with a single RTX3090 GPU and 8255C CPU. In the efficiency analysis, we imposed some restrictions to simulate real application scenarios. First, the time cost from receiving the query was calculated, without considering the offline phase, such as the construction of Vi-SemTree and the allocation of SemID. Second, each query in the Test set was retrieved one by one to return the target video, instead of obtaining all Queries and returning the overall results at once. From the Table  1 , it can be seen that T2VIndexer significantly reduces inference time while maintaining the baseline effect. For example, compared with the traditional method under 1000 candidate videos, T2VIndexer reduces the time cost by 50%. The efficiency improvement increases gradually with the size of the candidate set. Under 10,000 candidate videos, the time compression reaches 30%. In addition, we also analyzed the performance of three other datasets, as shown in Table  2 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Evaluation of the inference costs on MSVD, DiDeMo and ActivityNet dataset. We report the R@1 metric for the text-to-video task.   Blue  means stage 1 generative time cost, and   red  means stage 2 time cost.",
        "table": "S4.T2.5.1",
        "footnotes": [],
        "references": [
            "The text-video retrieval involves a text query  t t t italic_t  and a gallery of videos  V V V italic_V . The objective is to retrieve videos  { v j }  V subscript v j V \\{v_{j}\\}\\in V { italic_v start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT }  italic_V  that are semantically relevant to the query. As shown in Figure  2 , our goal is to directly retrieve targeted videos by generating the video identifiers based on natural language queries. To this end, we design a sequence-to-sequence generative model that takes query  t t t italic_t  as input and outputs the video identifier for video index. We first propose a semantic-aware tree structure to encode video identifiers, called SemID, which encodes the multi-grained semantics of videos by a sequence for controllable recall while maintaining the sequence length as short as possible for fast encoding. To augment the semantic expression of queries for more generalized model learning, we propose to utilize a Multi-modal Large Language Model (MLLM)  (Ye et al . ,  2023 )  to generate a set of multi-view queries for each video, thereby enriching the contextual semantics encapsulated by the SemIDs for diverse queries. The model architecture, training and inference strategies are introduced in the end.",
            "Hierarchical Vi-SemTree Building.   We use a tree structure to encode videos, which helps preserve semantic information and ensures that similar semantic videos are also similar in the identifier. Moreover, by controlling the depth  d d d italic_d  of the tree, we can achieve different levels of granularity and control the sequence length. Following NCI (Wang et al . ,  2022 ) , we use hierarchical  k k k italic_k -means method for video feature  F ^ ^ F \\hat{F} over^ start_ARG italic_F end_ARG , as shown in the training stage (a) of Figure  2 . First, we use the  k k k italic_k -means algorithm to divide the training set videos into  k k k italic_k  clusters based on their representation similarity. Each cluster is a tree node and contains a group of semantically similar videos, which serves as a layer of the tree structure. For each cluster, if the number of videos is greater than  c c c italic_c , we use the  k k k italic_k -means algorithm to further divide the cluster, generating the next layer of the tree, which is more granular at the semantic level. We repeat this process until we obtain a tree structure  T T T italic_T  with the root  r r r italic_r , where semantically similar videos are located in the same path. Following previous work, we adopt cosine similarity as the metric for measuring the similarity between two video representations.",
            "Given videos rich semantic diversity, a single video can match multiple queries, but current models only generate limited descriptions, undermining semantic understanding. To address this, we introduced a multi-view query expansion strategy during Training stage (b) in Figure  2 , enhancing semantic depth.  Utilizing the Multimodal Large Language Model (MLLM), which excels in generating nuanced descriptions from various angles, we created 50 diverse queries per video, ensuring comprehensive semantic coverage. This approach surpasses traditional dense video caption models by offering detailed, multi-faceted insights into video content.",
            "However, unlike standard decoding tasks, the same token appearing at different positions in the Vi-SemTree space has different meanings because they are in different layers of the Vi-SemTree. For example, as shown in the framework Figure  2 , for SemID  0 0  0 1  0 2 subscript 0 0 subscript 0 1 subscript 0 2 0_{0}-0_{1}-0_{2} 0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , the token  0 1 subscript 0 1 0_{1} 0 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  in the first layer represents the semantic of the sports, while  0 2 subscript 0 2 0_{2} 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  in the second layer only represents basketball. In addition, even if they are in the same layer, the same token  0 2 subscript 0 2 0_{2} 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  in SemID  0 0  0 1  0 2 subscript 0 0 subscript 0 1 subscript 0 2 0_{0}-0_{1}-0_{2} 0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  and  0 0  2 1  0 2 subscript 0 0 subscript 2 1 subscript 0 2 0_{0}-2_{1}-0_{2} 0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  expresses different concept due to their different prefixes. To identify different representations at different positions during decoding, we were inspired by NCI  (Wang et al . ,  2022 )  in document retrieval and used the Prefix-Aware Weight-Adaptor (PAWA) decoder, as shown in Figure  3 .",
            "Coarse-to-fine Inference.   During the training stage, since the videos in the test set are not visible, this part of the videos is not assigned a SemID. In order to cover test set, a new video needs to be assigned a SemID as its identifier as shown in Figure  2  Inference Stage. The new video will get the representation in the same way as the training set using CLIP encoder, and then the video cosine similarity with the leaf nodes in the tree will be calculated. The video will be inserted into the leaf node with the highest similarity and inherit the SemID of the leaf node. For the input query  q q q italic_q , the probability  p  ( L p  a  t  h t  r  u  n  c | q ,  ) p conditional superscript subscript L p a t h t r u n c q  p(L_{path}^{trunc}|q,\\theta) italic_p ( italic_L start_POSTSUBSCRIPT italic_p italic_a italic_t italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t italic_r italic_u italic_n italic_c end_POSTSUPERSCRIPT | italic_q , italic_ )  is calculated by the generative model trained, and the target SemID is obtained to achieve direct positioning of a group of target videos. In the decoding process, considering the semantic summarization ability of natural language query, which makes videos that meet the description may not be in the same path, we use the Beam Search algorithm for decoding. This enables us to retrieve the top k SemIDs that meet the description, to adapt to this one-to-many issue. Based on the generated SemIDs, we obtain a small candidate set  V c  a  n  d = { V S  e  m  I  D 1 , V S  e  m  I  D 2 , ... , V S  e  m  I  D k } subscript V c a n d subscript V S e m I subscript D 1 subscript V S e m I subscript D 2 ... subscript V S e m I subscript D k V_{cand}=\\{V_{SemID_{1}},V_{SemID_{2}},...,V_{SemID_{k}}\\} italic_V start_POSTSUBSCRIPT italic_c italic_a italic_n italic_d end_POSTSUBSCRIPT = { italic_V start_POSTSUBSCRIPT italic_S italic_e italic_m italic_I italic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , italic_V start_POSTSUBSCRIPT italic_S italic_e italic_m italic_I italic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT , ... , italic_V start_POSTSUBSCRIPT italic_S italic_e italic_m italic_I italic_D start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT }  containing the target videos, where  S  e  m  I  D i S e m I subscript D i SemID_{i} italic_S italic_e italic_m italic_I italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  represents a group of videos corresponding to the i-th top SemID generated.",
            "The purpose of T2VIndexer is to improve retrieval efficiency while maintaining accuracy. In Table  1 , we analyzed the accuracy and efficiency of T2VIndexer under different candidate sets with a single RTX3090 GPU and 8255C CPU. In the efficiency analysis, we imposed some restrictions to simulate real application scenarios. First, the time cost from receiving the query was calculated, without considering the offline phase, such as the construction of Vi-SemTree and the allocation of SemID. Second, each query in the Test set was retrieved one by one to return the target video, instead of obtaining all Queries and returning the overall results at once. From the Table  1 , it can be seen that T2VIndexer significantly reduces inference time while maintaining the baseline effect. For example, compared with the traditional method under 1000 candidate videos, T2VIndexer reduces the time cost by 50%. The efficiency improvement increases gradually with the size of the candidate set. Under 10,000 candidate videos, the time compression reaches 30%. In addition, we also analyzed the performance of three other datasets, as shown in Table  2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Evaluation of the inference costs on large-scale dataset split from TGIF  (Li et al . ,  2016 ) . We report the R@1 metric for the text-to-video task.   Blue  means stage 1 generative time cost, and   red  means stage 2 time cost.",
        "table": "S4.T3.5",
        "footnotes": [],
        "references": [
            "To achieve direct positioning of the target video through SemID, we chose to use a sequence-to-sequence generative model to directly generate the corresponding SemID based on the input query, as shown in Figure  3 . First, the input query is added with position embedding and input into the transformer encoder to obtain the representation  f t subscript f t f_{t} italic_f start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT . The probability of generating the SemID sequence as follows to construct,",
            "However, unlike standard decoding tasks, the same token appearing at different positions in the Vi-SemTree space has different meanings because they are in different layers of the Vi-SemTree. For example, as shown in the framework Figure  2 , for SemID  0 0  0 1  0 2 subscript 0 0 subscript 0 1 subscript 0 2 0_{0}-0_{1}-0_{2} 0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , the token  0 1 subscript 0 1 0_{1} 0 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  in the first layer represents the semantic of the sports, while  0 2 subscript 0 2 0_{2} 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  in the second layer only represents basketball. In addition, even if they are in the same layer, the same token  0 2 subscript 0 2 0_{2} 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  in SemID  0 0  0 1  0 2 subscript 0 0 subscript 0 1 subscript 0 2 0_{0}-0_{1}-0_{2} 0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  and  0 0  2 1  0 2 subscript 0 0 subscript 2 1 subscript 0 2 0_{0}-2_{1}-0_{2} 0 start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT - 2 start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - 0 start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  expresses different concept due to their different prefixes. To identify different representations at different positions during decoding, we were inspired by NCI  (Wang et al . ,  2022 )  in document retrieval and used the Prefix-Aware Weight-Adaptor (PAWA) decoder, as shown in Figure  3 .",
            "To further investigate the effectiveness of T2VIndexer in real retrieval scenarios, we evaluate on larger scale data. Due to the limited size of the existing dataset test sets, which mostly consist of 1000 candidate videos, we decided to redivide the TGIF dataset  (Li et al . ,  2016 )  into 50,000 training data and 50,000 testing data in a 5:5 ratio. Both the baseline and the T2VIndexer generative model will be trained solely on the 50,000 training data. Table  3  displays our test results, with each block representing a set of test results. It is evident from the results that in large-scale retrieval scenarios, T2VIndexer demonstrates more significant improvements in both efficiency and accuracy compared to smaller-scale data."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Comparison with Existing One-stream approaches and Two-stream approaches. Our Re-implemented methods are denoted by the superscript *. The highest retrieval recall in each block is marked with  underline . The recall of our models is marked with blue color when it is better than the baseline model.",
        "table": "S4.T4.3.1",
        "footnotes": [],
        "references": [
            "Table  4  is divided into three sections: Interactive, Independent, and our T2VIndexer, each based on various baselines. Two-stream methods in the first two sections typically perform well due to CLIPs strong pretraining. In the third section, T2VIndexer was built using CLIP4CLIP, mPLUG, and CLIP-VIP. These models performance improved sequentially, allowing T2VIndexer to demonstrate varying effectiveness. On the MSR-VTT dataset, T2VIndexer boosted CLIP4CLIPs R@1 by 3.3%, but its accuracy gain lessened with better baselines, peaking at a 1.0% increase with CLIP-VIP.",
            "We further demonstrated the ability of T2VIndexer to locate target videos through visualization. Three examples are shown in Figure  4 , where the SemID generated by T2VIndexer has a high semantic similarity with the target SemID, even in wrong mapping cases. Specifically, for the query video game clip showing here different characters, the SemID generated by T2VIndexer, 0-9-21, has a stronger matching relationship with the query than ground-truth video. This indicates that the model has effectively learned the mapping between natural language space and SemID space, achieving retrieve target videos directly."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Ablation Study on MSR-VTT-1kA",
        "table": "S4.T5.1.1",
        "footnotes": [],
        "references": [
            "To further investigate the impact of different components on the models performance, we report the ablation results on the MSR-VTT dataset in Table  5 . (1)  Without query expansion (w/o query expansion)  This means that only the captions provided in the training set are used as the training basis. This part has the most significant impact on the models results. During the SemID generation process by T2VIndexer, the original video is not seen. If the semantics of the original video are not injected into SemID through queries during the training phase, the model will fail to establish a relationship between the text and SemID and will not be able to correctly generate SemID for queries not seen during training. (2)  Without Multi-view query expansion (w/o Multi-view query expansion)  This indicates not using an MLLM (Multilingual Language Models) to generate multi-view descriptions, and only using a dense caption model for generating descriptions. The results suggest that the semantic expansion of SemID allows the model to learn richer information, which can better apply SemID to the test set. (3)  Without Vi-SemTree and SemID (w/o Vi-SemTree and SemID)  This means that a single  K K K italic_K -means clustering is performed, and each cluster is assigned a number as the identifier for the videos. The experiment confirms our theoretical premise that structured pre-injection of prior knowledge facilitates superior generalization.",
            "We analyzed various model setups. Figure  5  (a) illustrates the top  k k k italic_k  SemIDs selected via beam search at  t = 2 t 2 t=2 italic_t = 2 , affects recall and candidate count. As  k k k italic_k  grows, second-stage recall dips due to more candidates, but overall recall peaks at 55.1% with  k = 11 k 11 k=11 italic_k = 11 . Figure  5  (b) examines the impact of different  t t t italic_t  values on recall and candidate sets when  k = 11 k 11 k=11 italic_k = 11 , showing optimal results at  t = 2 t 2 t=2 italic_t = 2 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  Ablation Study on MLLMs",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": [
            "Utilizing Multi-Modal Large Language Models (MLLMs) for query expansion effectively enhances the models generalization capabilities. Various MLLMs show little difference in the quality of query expansion, so the models effectiveness does not rely on a specific MLLM. Apart from mPLUG-owl tested in the paper, we have also conducted supplementary tests with Minigpt-4 and LLaVA. As evident from the results in Table  6  on the MSR-VTT dataset, different MLLMs have a minor impact on retrieval accuracy."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Ablation Study on different video feature extractors on MSR-VTT with CLIP-VIP as baseline.",
        "table": "S4.T7.1",
        "footnotes": [],
        "references": [
            "Different methods of video feature extraction will affect the structure of the Vi-Sem tree, thereby influencing the overall models effectiveness. In Table 7 , we tested three different video feature extraction methods: S3D based on pixel features, CLIP image encoder based on semantic features of frames, and VideoMAE pre-trained for video recognition. From the results comparison, it can be observed that the method based on pixel features performs the worst, even falling below the baseline model CLIP-VIP. This is due to its tree structure reflecting pixel info rather than relevant semantic info, leading to poor performance in the video pre-select stage and inability to return correct video clusters for precise recall."
        ]
    },
    "id_table_8": {
        "caption": "Table 8.  Ensemble framework comparative analysis. Pre-select size represents the number of videos pre-selected in the first stage.",
        "table": "S5.T8.1.1",
        "footnotes": [],
        "references": [
            "Although T2VIndexer has achieved certain results in efficient text-video retrieval, there still exists performance limitations. It uses a two-stage process: pre-select and precise retrieval, akin to an ensemble method. We compared it with a two-stage ensemble of mPLUG and CLIP-VIP, shown in Table  8 . While ensemble methods enhance accuracy at the cost of efficiency, they outperform T2VIndexers generative approach. However, T2VIndexer excels in efficiency by directly targeting candidate sets, unlike existing models that process all candidates. Additionally, existing pipelines struggle with new videos, requiring costly similarity calculations and sorting for each insertion into Vi-SemTree, averaging 200 ms per video in the MSR-VTT test set. Our future efforts will enhance the generative stages accuracy for better precision and aim to cut new data processing time and boost flexibility."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "S5.T8.1.1.2.2.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "",
        "table": "S5.T8.1.1.2.2.2.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "S5.T8.1.1.2.2.3.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "S5.T8.1.1.2.2.4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "S5.T8.1.1.2.2.5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "S5.T8.1.1.2.2.6.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}