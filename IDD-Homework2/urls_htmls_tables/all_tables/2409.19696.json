{
    "id_table_1": {
        "caption": "Table 1 :  On each dataset, we compare the Precision (%) and Recall (%) of  DeFT  with CLIP label-match and small-loss to evaluate the clean sample selection performance.    \\Delta roman_  is the difference between the performance of  DeFT  and small-loss.",
        "table": "S5.T1.52.52",
        "footnotes": [],
        "references": [
            "To discern the most effective method for CLIP adaptation, we conduct empirical studies on various datasets as shown in Figure  1 . Specifically, we utilize three fine-tuning approaches to adapt the pre-trained CLIP model and compare their performance on both noisy and clean datasets, including 1)  FFT  which updates the entire model parameters, 2)  VPT  which fixes pre-trained model parameters and prepends a small subset of extra learnable parameters to the visual encoder during fine-tuning, and 3)  VLPT  (Vision-Language Prompt Tuning) which integrates both visual and textual learnable prompts into the fixed pre-trained model for fine-tuning. For FFT and VPT, we learn an additional linear classifier, while VLPT directly utilizes the learned textual prompts for classification. We present our three primary findings in the following.",
            "Intuitively, utilizing FFT for CLIP adaptation yields improved performance by leveraging its substantial capacity to incorporate task-specific representations. However, the efficacy of FFT diminishes notably when applied to datasets containing noisy labels, as shown in Figure  1(a) . This decline is attributed to the pronounced distortion of representations with an escalating noise ratio  [ 1 ] . In contrast, VPT benefits representation learning when adapting CLIP on noisy data, particularly under high levels of noise. As only a small set of parameters is introduced, VPT efficiently retains the generalization ability from image-text pre-training while enhancing classification performance on downstream tasks, making it a robust and effective fine-tuning approach for handling noisy downstream datasets.",
            "Classification with learnable textual prompts (e.g., CoOp  [ 52 ] ) leverages the multi-modal information in pre-trained vision-language models and enhances the alignment of visual and textual representations on downstream tasks. Recent literature has substantiated its robustness with a frozen visual encoder in the context of few-shot learning  [ 40 ] . From Figure  1(a) , we observe that VLPT with a textual classifier (green line) consistently surpasses VPT with a traditional linear classifier for classification (orange line), especially under severe noise. The improvement in performance across diverse noise ratios further affirms the robustness of learnable textual prompts in mitigating the impact of label noise for model adaptation.",
            "Although previous findings show that FFT is more susceptible to noisy labels compared to VPT and VLPT, Figure  1(b)  demonstrates its enhanced discriminative ability when training data is clean. This superiority is particularly significant on fine-grained datasets such as Stanford-Cars and CUB-200-2011, with an average improvement of 2.81% against VPT. It is important to note that VLPT exhibits the worst performance on clean datasets. This is primarily due to the implicit regularization of pre-trained textual information when tuning the context of textual prompts, as explained in the recent study  [ 40 ] .",
            "To optimize the noisy label detector, we need to construct positive and negative training samples for binary classification, i.e., images with correctly and wrongly assigned labels. Inspired by the prior work on negative learning  [ 20 ] , we designate each image with a randomly picked complementary label  y     y \\bar{y} over  start_ARG italic_y end_ARG  to form negative samples. For positive samples, we initiate with the original supervision in  D D \\mathcal{D} caligraphic_D  and refine it with pseudo labels  y ^ ^ y \\hat{y} over^ start_ARG italic_y end_ARG  generated by Eq. ( 1 ). The training loss is then formulated as follows:",
            "Additionally, to enhance task-specific representations of the noisy label detector, we harness PEFT for the adaptation of the visual encoder, considering its robustness under massive noisy labels compared with FFT, as per our previous finding (see Figure  1(a) ). This is achieved by maximizing the alignment between image embeddings and their corresponding positive textual features:",
            "Although the learned positive textual prompt can be readily employed for classification, its performance may be suboptimal on curated clean datasets, as demonstrated in our previous finding (see Figure  1(b) ). To mitigate this problem, we introduce the model adaptation phase which learns a linear classifier using selected clean samples, i.e.,  D clean superscript D clean \\mathcal{D}^{\\text{clean}} caligraphic_D start_POSTSUPERSCRIPT clean end_POSTSUPERSCRIPT . Moreover, our findings also indicate that FFT outperforms PEFT on clean datasets, so we remove the PEFT modules and fully fine-tune the pre-trained model parameters for model adaptation. We minimize the classic cross-entropy loss  l c  e  ( x , y ) =  log  exp  ( z y )  k = 1 K exp  ( z k ) subscript l c e x y subscript z y superscript subscript k 1 K subscript z k \\ell_{ce}(\\boldsymbol{x},y)=-\\log\\frac{\\exp(z_{y})}{\\sum_{k=1}^{K}\\exp(z_{k})} roman_l start_POSTSUBSCRIPT italic_c italic_e end_POSTSUBSCRIPT ( bold_italic_x , italic_y ) = - roman_log divide start_ARG roman_exp ( italic_z start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) end_ARG start_ARG  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG , where  z = { z k } k = 1 K z superscript subscript subscript z k k 1 K \\boldsymbol{z}=\\{z_{k}\\}_{k=1}^{K} bold_italic_z = { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT  represents the logits predicted by the linear classifier. It is noteworthy that with the clean subset of training data constructed by Eq. ( 5 ), the second phase can be applied universally to a wide range of pre-trained models, regardless of their backbones. The versatility of our approach is demonstrated in the experiments.",
            "We conduct a thorough evaluation of  DeFT  to justify its effectiveness in detecting noisy labels. For this purpose, we simulate various types and ratios of label noise on four benchmark datasets. We compare our approach with two other sample selection strategies: 1)  Label-match  strategy, where a training sample is deemed clean if its given label matches that assigned by the CLIP zero-shot classifier, and 2)  Small-loss  strategy, which selects a proportion (noise ratio) of confident samples in each mini-batch, as commonly used in previous studies  [ 9 ,  24 ,  18 ] . The precision and recall of sample selection are reported in Table  1 . The results show that our proposed noisy label detector outperforms the compared strategies, with significant improvements observed in all dataset settings. The trivial label-match strategy tends to be conservative, leading to low selection recall of clean samples. In contrast, the small-loss strategy and  DeFT  achieve a better trade-off between precision and recall, making better use of training samples. Moreover, leveraging the informative multi-model information in the pre-trained vision-language model,  DeFT  surpasses the small-loss strategy in detecting noisy labels, particularly under severe noise settings and fine-grained classification tasks. For instance, in Tiny-ImageNet with 60% symmetric noise,  DeFT  demonstrates significant enhancements of 4.58% and 4.55% in precision and recall, respectively, as well as an improvement of 9.78% and 14.42% in CUB-200-2011. Additionally,  DeFT  reduces the need for prior knowledge of noise ratios, making it a practical and effective approach to detect label noise in real-world tasks.",
            "Table  1  has demonstrated the superiority of  DeFT  in detecting noisy labels based on CLIP ViT-B/16. To evaluate the impact of different visual backbones on noise detection, we re-implement all methods using CLIP ViT-B/32 as the backbone and report the results in Table  5 . Notably, while the small-loss strategy demonstrates comparable performance on noisy CIFAR-100 and Tiny-ImageNet, its efficacy significantly diminishes on fine-grained datasets such as Stanford-Cars and CUB-200-2011, which exhibits its sensitivity to visual backbones. In contrast,  DeFT  maintains consistently superior performance on both ViT-B/16 and ViT-B/32, underscoring its resilience across different pre-trained visual backbones.",
            "Table  10  presents the test accuracy obtained by applying PEFT to adapt the pre-trained CLIP model. There are three noteworthy observations in comparison to the results in Table  2 . Firstly, previous label-noise learning methods consistently demonstrate their efficacy in handling noisy datasets compared to the vanilla methods (CE), showcasing a clear improvement in performance across various datasets and noise settings. Secondly, despite the bad performance of GMM on fine-grained datasets in Table  2 , it achieves the best performance among baselines when utilizing PEFT to adapt the pre-trained model, possibly due to the more stable training process with fewer learnable parameters. Lastly,  DeFT  continues to outperform all compared methods in almost all cases when applying PEFT during the model adaptation phase ( DeFT  w/ PEFT), and its performance can be further enhanced by replacing PEFT with FFT ( DeFT  w/ FFT), confirming the validity of the proposed framework.",
            "DeFT  is a two-phase framework consisting of a noisy label detection phase and a model adaptation phase. The pseudo-code is presented in Algorithm  1 .",
            "In the model adaptation phase, we apply FFT to a range of models that are pre-trained on ImageNet-1K. For a fair comparison, we follow the optimizer settings outlined in  [ 1 ]  and initialize each model with the corresponding pre-trained weights obtained from HuggingFace  1 1 1 https://huggingface.co/ . The specific configurations are described in Table  11 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Test accuracy (%) on synthetic datasets with  symmetric  and  instance-dependent  label noise.",
        "table": "S5.T2.2.1",
        "footnotes": [],
        "references": [
            "In the first phase, we propose to construct a noisy label detector by learning positive and negative textual prompts for each class. The positive prompt aims to uncover distinguishable features specific to the class, while the negative prompt functions as a learnable threshold for identifying noisy labels. Our noisy label detector can be derived from the similarity between embeddings of a training image and positive/negative prompts associated with its assigned class. If the image exhibits a higher similarity score with the positive prompt than the negative prompt, it is considered a correctly labeled sample, i.e., a clean sample. This design draws inspiration from previous studies that have shown the robustness of prompt tuning to noisy labels, particularly in the presence of high noise ratios  [ 40 ] . To optimize the positive prompt, we align its representation with image features from the corresponding class during training. For the negative prompts, we introduce a novel negative learning objective. It is important to note that calculating similarities between images and textual prompts requires a strong alignment between these two modalities in the downstream task. To achieve this, we utilize PEFT, such as Visual Prompt Tuning (VPT)  [ 17 ] , for the adaptation of the visual encoder. This decision is based on a key finding that FFT can distort pre-trained feature representations when noisy labels are present, which is discussed in Section  3.2 .",
            "Based on the findings in Section  3.2 , we now present a novel denoising fine-tuning framework,  DeFT , which comprises two pivotal phases. In the first phase,  DeFT  learns dual textual prompts to separate clean and noisy samples while adapting the visual encoder utilizing PEFT methods. In the second phase,  DeFT  re-adapts the pre-trained model using FFT, leveraging the curated clean samples to further boost visual recognition performance. Figure  2  illustrates our proposed framework.",
            "Table  2  presents the results on four synthetic noisy datasets. Compared with CE, the adoption of noise-robust loss functions such as ELR and SCE improves the classification performance on CIFAR-100 and Tiny-ImageNet under certain noisy label settings, e.g., ELR achieves the best performance on CIFAR-100 with 40% instance-dependent noise. However, these methods are not always effective and may even worsen the performance under varying types and ratios of noise, compared to the sample selection paradigm like GMM and  DeFT . While GMM outperforms ELR and SCE in most cases, its performance degrades dreadfully on fine-grained datasets. Nevertheless,  DeFT  retains the most robust performance and generally advances all compared methods, especially on Stanford-Cars and CUB-200-2011. For example,  DeFT  boosts the test accuracy by an average of 4.34% on Stanford-Cars compared to the best results of comparison methods. The results demonstrate the effectiveness of our proposed approach in tackling both symmetric and instance-dependent label noise.",
            "Table  10  presents the test accuracy obtained by applying PEFT to adapt the pre-trained CLIP model. There are three noteworthy observations in comparison to the results in Table  2 . Firstly, previous label-noise learning methods consistently demonstrate their efficacy in handling noisy datasets compared to the vanilla methods (CE), showcasing a clear improvement in performance across various datasets and noise settings. Secondly, despite the bad performance of GMM on fine-grained datasets in Table  2 , it achieves the best performance among baselines when utilizing PEFT to adapt the pre-trained model, possibly due to the more stable training process with fewer learnable parameters. Lastly,  DeFT  continues to outperform all compared methods in almost all cases when applying PEFT during the model adaptation phase ( DeFT  w/ PEFT), and its performance can be further enhanced by replacing PEFT with FFT ( DeFT  w/ FFT), confirming the validity of the proposed framework."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Test accuracy (%) on datasets with real-world label noise.",
        "table": "S5.T3.2.1",
        "footnotes": [],
        "references": [
            "In the first phase, we propose to construct a noisy label detector by learning positive and negative textual prompts for each class. The positive prompt aims to uncover distinguishable features specific to the class, while the negative prompt functions as a learnable threshold for identifying noisy labels. Our noisy label detector can be derived from the similarity between embeddings of a training image and positive/negative prompts associated with its assigned class. If the image exhibits a higher similarity score with the positive prompt than the negative prompt, it is considered a correctly labeled sample, i.e., a clean sample. This design draws inspiration from previous studies that have shown the robustness of prompt tuning to noisy labels, particularly in the presence of high noise ratios  [ 40 ] . To optimize the positive prompt, we align its representation with image features from the corresponding class during training. For the negative prompts, we introduce a novel negative learning objective. It is important to note that calculating similarities between images and textual prompts requires a strong alignment between these two modalities in the downstream task. To achieve this, we utilize PEFT, such as Visual Prompt Tuning (VPT)  [ 17 ] , for the adaptation of the visual encoder. This decision is based on a key finding that FFT can distort pre-trained feature representations when noisy labels are present, which is discussed in Section  3.2 .",
            "Based on the findings in Section  3.2 , we now present a novel denoising fine-tuning framework,  DeFT , which comprises two pivotal phases. In the first phase,  DeFT  learns dual textual prompts to separate clean and noisy samples while adapting the visual encoder utilizing PEFT methods. In the second phase,  DeFT  re-adapts the pre-trained model using FFT, leveraging the curated clean samples to further boost visual recognition performance. Figure  2  illustrates our proposed framework.",
            "The results reported in Table  3  demonstrate the superiority of our method on real-world datasets, as  DeFT  surpasses other methods by a significant margin on all datasets. Notably, while most compared methods show little improvement on Clothing1M due to its fine-grained nature, both ELR and  DeFT  demonstrate substantial improvement. However, ELR exhibits a performance decrease on WebVision. Moreover, directly fine-tuning pre-trained CLIP models with cross-entropy (CE) achieves competitive results on WebVision, yet  DeFT  elevates this performance further, showcasing its ability to combat label noise in practical situations.",
            "In this experiment, we emphasize the necessity of the model adaptation phase in  DeFT . Figure  3  demonstrates the test accuracy of  DeFT  without the model adaptation phase (w/o adap.), adaptation utilizing PEFT and FFT. Results show that fully fine-tuning the model on selected clean samples yields the best classification performance, especially on fine-grained datasets, such as Stanford-Cars and CUB-200-2011. The results unveil two primary insights: 1) the noisy label detector in  DeFT  exhibits strong capabilities in detecting label noise, yielding a nearly clean subset from noisy downstream datasets, and 2) employing FFT for model adaptation is more effective in mitigating significant domain shifts between downstream data and pre-training data, particularly evident in fine-grained datasets."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Test accuracy (%) using various pre-trained models on Clothing1M. Partial results are sourced from  [ 1 ] . The best results across all methods are highlighted in bold, with the second-best results indicated by underscores.",
        "table": "S5.T4.2",
        "footnotes": [
            ""
        ],
        "references": [
            "It is noteworthy that  DeFT  can seamlessly integrate with various pre-trained visual backbones during the model adaptation phase. To demonstrate the versatility of our proposed denoising fine-tuning framework, we conduct experiments on Clothing1M and apply FFT to a range of pre-trained models, including ViT-B/16  [ 5 ] , ResNet-50  [ 11 ] , ConvNeXt-T  [ 27 ] , and MAE-VIT-B  [ 10 ] . The results are presented in Table  4 , where GCE  [ 50 ]  employs a noise-robust loss function akin to SCE. As a recent work, TURN  [ 1 ]  is an improved version of GMM, which applies linear probing to initialize the classifier and then uses FFT for adaptation. Compared with previous methods,  DeFT  exhibits the best performance on average. In particular,  DeFT  outperforms TURN by 4.51% and 3.27% when adopting ResNet-50 and MAE-ViT-B as the target pre-trained models, respectively.",
            "To further substantiate the advantages of PEFT over FFT for adapting visual backbones in the presence of massive noisy labels, we conduct a comprehensive evaluation of various PEFT techniques for image classification, including 1)  Visual Prompt Tuning  (VPT)  [ 17 ]  which prepends learnable prompts to the input at each layer, 2)  Low-Rank Adapter  (LoRA)  [ 12 ]  which injects trainable rank decomposition matrices into each layer, 3)  AdaptFormer   [ 3 ]  which replaces the original Multi-Layer Perceptron (MLP) block with AdaptMLP that incorporates a trainable bottleneck module, and 4)  Bias-terms Fine-tuning  (BitFit)  [ 49 ]  which is a sparse fine-tuning method where only the bias-terms of the model are being modified. The results are shown in Figure  4 . Notably, nearly all PEFT methods outperform FFT when the noise ratio exceeds 0.4, and the performance gap becomes more pronounced with the increase in the noise ratio. These results comprehensively underscore the efficacy of PEFT over FFT in mitigating the distortion of representations when adapting visual backbones under severe noise."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Precision and recall of noisy label detection with CLIP ViT-B/32 as the visual backbone.",
        "table": "A1.T5.52.52",
        "footnotes": [],
        "references": [
            "Obviously, selecting samples with the clean probability  p i  k clean > 0.5 superscript subscript p i k clean 0.5 p_{ik}^{\\text{clean}}>0.5 italic_p start_POSTSUBSCRIPT italic_i italic_k end_POSTSUBSCRIPT start_POSTSUPERSCRIPT clean end_POSTSUPERSCRIPT > 0.5  is equivalent to the specified criteria in Eq. ( 5 ). In this way, the original threshold-based selection approach can be transformed into  K K K italic_K  binary classification tasks to determine whether a sample is clean or not. Concretely, given a sample  ( x i , y i ) subscript x i subscript y i (\\boldsymbol{x}_{i},y_{i}) ( bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  from noisy datasets  D D \\mathcal{D} caligraphic_D , we can employ the dual prompts of class  y i subscript y i y_{i} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as a binary classifier to identify label noise. If the classifier outputs a positive prediction of  x i subscript x i \\boldsymbol{x}_{i} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , it is categorized as a clean sample.",
            "Although the learned positive textual prompt can be readily employed for classification, its performance may be suboptimal on curated clean datasets, as demonstrated in our previous finding (see Figure  1(b) ). To mitigate this problem, we introduce the model adaptation phase which learns a linear classifier using selected clean samples, i.e.,  D clean superscript D clean \\mathcal{D}^{\\text{clean}} caligraphic_D start_POSTSUPERSCRIPT clean end_POSTSUPERSCRIPT . Moreover, our findings also indicate that FFT outperforms PEFT on clean datasets, so we remove the PEFT modules and fully fine-tune the pre-trained model parameters for model adaptation. We minimize the classic cross-entropy loss  l c  e  ( x , y ) =  log  exp  ( z y )  k = 1 K exp  ( z k ) subscript l c e x y subscript z y superscript subscript k 1 K subscript z k \\ell_{ce}(\\boldsymbol{x},y)=-\\log\\frac{\\exp(z_{y})}{\\sum_{k=1}^{K}\\exp(z_{k})} roman_l start_POSTSUBSCRIPT italic_c italic_e end_POSTSUBSCRIPT ( bold_italic_x , italic_y ) = - roman_log divide start_ARG roman_exp ( italic_z start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT ) end_ARG start_ARG  start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT roman_exp ( italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT ) end_ARG , where  z = { z k } k = 1 K z superscript subscript subscript z k k 1 K \\boldsymbol{z}=\\{z_{k}\\}_{k=1}^{K} bold_italic_z = { italic_z start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT  represents the logits predicted by the linear classifier. It is noteworthy that with the clean subset of training data constructed by Eq. ( 5 ), the second phase can be applied universally to a wide range of pre-trained models, regardless of their backbones. The versatility of our approach is demonstrated in the experiments.",
            "We adopt the pre-trained CLIP  [ 28 ]  with the Transformer as the text encoder and the ViT-B/16 as the image encoder. We use the SGD optimizer with a momentum of 0.9, a weight decay of  5 5 5 5  \\times  10  4 superscript 10 4 10^{-4} 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , and a batch size of 64. We run 10 epochs for both the noisy label detection phase and the model adaptation phase with learning rates  3 3 3 3  \\times  10  2 superscript 10 2 10^{-2} 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT  and  5 5 5 5  \\times  10  4 superscript 10 4 10^{-4} 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , respectively. In the noisy label detection phase, we employ VPT  [ 17 ]  and CoOp  [ 52 ]  to adapt visual encoder and textual encoder respectively, and perform model warm-up for 1 epoch on all datasets. When identifying noisy labels on real-world datasets, we augment the condition in Eq. ( 5 ) with a constraint that the training label should be consistent with its predicted label. This is due to the existence of more complex noise patterns in real-world tasks. All experiments are conducted on a single NVIDIA GeForce RTX 3090.",
            "Table  1  has demonstrated the superiority of  DeFT  in detecting noisy labels based on CLIP ViT-B/16. To evaluate the impact of different visual backbones on noise detection, we re-implement all methods using CLIP ViT-B/32 as the backbone and report the results in Table  5 . Notably, while the small-loss strategy demonstrates comparable performance on noisy CIFAR-100 and Tiny-ImageNet, its efficacy significantly diminishes on fine-grained datasets such as Stanford-Cars and CUB-200-2011, which exhibits its sensitivity to visual backbones. In contrast,  DeFT  maintains consistently superior performance on both ViT-B/16 and ViT-B/32, underscoring its resilience across different pre-trained visual backbones."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Precision, recall, and F1-score of noisy label detection under 80%  symmetric  noise and 50%  instance-dependent  noise.",
        "table": "A1.T7.fig1.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "Table 7 :  F1-score (%) of noisy label detection. For GMM, we utilize both PEFT and FFT to adapt the pre-trained model.",
        "table": "A1.T7.fig2.1.1",
        "footnotes": [],
        "references": [
            "We conduct experiments on synthetic noisy datasets with significant label noise to further validate the robustness of the proposed noisy label detector. It is noteworthy that two adjustments are implemented in order to train a more effective model: 1) the learning rate is adjusted from  3 3 3 3  \\times  10  2 superscript 10 2 10^{-2} 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT  to  1 1 1 1  \\times  10  2 superscript 10 2 10^{-2} 10 start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT , and 2) For  DeFT , a small weight (specifically, 0.25) is introduced to the positive loss component in Eq. ( 7 ) to mitigate the impact of noisy pseudo labels in high levels of noise. Table  7  reports the precision, recall, and their harmonic mean (F1-score) for noise detection, where  DeFT  exhibits a robust capability in identifying severe noise. For example, a remarkable improvement in the F1-score is achieved on CUB-200-2011. These results strongly reaffirm the superior performance of  DeFT  across various noise ratios.",
            "DivideMix  [ 24 ]  introduces the use of a Gaussian Mixture Model (GMM) for the dynamic separation of clean and noisy samples with an adjustable threshold. Table  7  summarizes the performance of GMM and  DeFT  in detecting label noise across diverse datasets and noise settings. To fine-tune the pre-trained model for GMM, we independently apply PEFT and FFT. The results reveal that  DeFT  consistently exhibits superior noise detection capability, particularly evident in fine-grained datasets with high noise ratios, such as Stanford-Cars with 60% symmetric noise and CUB-200-2011 with 40% instance-dependent noise. Intriguingly, implementing GMM with PEFT yields better noise detection when compared to utilizing FFT. This difference can be attributed to the potentially unstable training process of FFT, where the loss value may undergo significant fluctuations due to an abundance of tunable parameters."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Noise detection results on CIFAR-100N.",
        "table": "A1.T8.2.1",
        "footnotes": [],
        "references": [
            "The CIFAR-100N dataset  [ 38 ] , which serves as a real-world benchmark by providing CIFAR-100 with human-annotated noisy labels, allows us to evaluate the efficacy of  DeFT  for noise detection in practical scenarios. The results presented in Table  8  reveal the distinctive performance characteristics of various sample selection strategies. For example, the label-match strategy demonstrates a propensity for being greedy and yields the highest precision, whereas the GMM and recent sample selection-based works tends to be conservative and thus achieve a higher recall. Nevertheless,  DeFT  stands out with the highest F1-score across all strategies, striking a judicious balance between precision and recall in the identification of real-world noisy labels."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Sensitivity analysis of promt length and training epoch.",
        "table": "A1.T9.2.1",
        "footnotes": [],
        "references": [
            "We explore the sensitivity of our noise label detector to various hyperparameter settings, specifically the length of the visual/textual prompt and the number of training epochs. Table  9  presents the F1-score on CIFAR-100 with 40% symmetric noise. The results indicate that the length of the visual prompt has a marginal impact on the noisy label detection. For the textual prompt length, once the context length exceeds 8, the performance is enhanced slightly as the length increases. Additionally, variations in the number of training epochs have minimal impact on performance. In order to strike a balance between effectiveness and efficiency, we set the visual prompt length to 20, the textual prompt length to 16, and the training epoch to 10 in all experiments."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Test accuracy (%) on synthetic datasets with  symmetric  and  instance-dependent  label noise. For all approaches, we utilize PEFT to adapt the pre-trained CLIP model. The results of  DeFT  when applying FFT for adaptation are also reported.",
        "table": "A1.T10.2.1",
        "footnotes": [],
        "references": [
            "Table  10  presents the test accuracy obtained by applying PEFT to adapt the pre-trained CLIP model. There are three noteworthy observations in comparison to the results in Table  2 . Firstly, previous label-noise learning methods consistently demonstrate their efficacy in handling noisy datasets compared to the vanilla methods (CE), showcasing a clear improvement in performance across various datasets and noise settings. Secondly, despite the bad performance of GMM on fine-grained datasets in Table  2 , it achieves the best performance among baselines when utilizing PEFT to adapt the pre-trained model, possibly due to the more stable training process with fewer learnable parameters. Lastly,  DeFT  continues to outperform all compared methods in almost all cases when applying PEFT during the model adaptation phase ( DeFT  w/ PEFT), and its performance can be further enhanced by replacing PEFT with FFT ( DeFT  w/ FFT), confirming the validity of the proposed framework."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Configurations of different pre-trained models.",
        "table": "A1.T11.24",
        "footnotes": [],
        "references": [
            "In the model adaptation phase, we apply FFT to a range of models that are pre-trained on ImageNet-1K. For a fair comparison, we follow the optimizer settings outlined in  [ 1 ]  and initialize each model with the corresponding pre-trained weights obtained from HuggingFace  1 1 1 https://huggingface.co/ . The specific configurations are described in Table  11 ."
        ]
    }
}