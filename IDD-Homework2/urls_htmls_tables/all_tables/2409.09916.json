{
    "id_table_1": {
        "caption": "Table 1 :  Performances of  \\ourmodel -9B and various open- and closed-source baselines across 7 contextual question answering tasks in ContextualBench. Bold numbers mean best of all, while underlined numbers mean best among open-source models. PopQA is measured in easy-match accuracy (EasyEM), while the rest are measured in exact-match accuracy (EM). The Appendix presents the full results in metrics.",
        "table": "S3.T1.5.1",
        "footnotes": [],
        "references": [
            "There is limited well established evaluation standards for measuring progress in contextual comprehension qualities of LLMs. It is worth noting that Command-R(+)  [ 36 ]  and RAG-2.0  [ 37 ]  evaluated their proposed models on non-overlapping metrics  [ 49 ,  23 ]  with inconsistent or undisclosed setups, which causes difficulties in aligning results and comparisons across different studies. To reliably evaluate our  \\ourmodel model as well as other well-known baselines, in this work, we also introduce ContextualBench  2 2 2 https://huggingface.co/datasets/Salesforce/ContextualBench , which is a compilation of many popular RAG and contextual benchmarks, such as HotpotQA and TriviaQA  [ 49 ,  15 ,  25 ,  23 ,  13 ,  42 ,  18 ] , that standardizes the evaluation setup leading to consistent and reproducible evaluation results. In experiments, we show that our  \\ourmodel -9B model is both a well-rounded and high performing model, achieving state-of-the-art performance in three of the seven benchmarks in ContextualBench; see  Figure   1  for a preview of our results.  \\ourmodel -9B outperforms or is competitive with GPT-4o  [ 30 ]  on all tasks in ContextualBench. It also outperforms powerful contextual models, such as Command-R+  [ 36 ] , on a variety of tasks despite having 10 times fewer parameters. Compared to comparable baselines, our model is also shown to be resilient to factual alterations and unanswerability tests in the context. Lastly, despite being trained with a focus on RAG and contextual applications, our model is still competitive as a regular instruction-tuned LLM, with strong and comparable performances in standard benchmarks like MMLU or GSM8K  [ 12 ,  7 ,  6 ] , as well as function-calling ability  [ 48 ] .",
            "In this section, we provide more insights into  \\ourmodel . First, we introduce a novel chat template comprising two new chat roles with specific functions ( Section   2.1 ). Then, we briefly discuss the training process of  \\ourmodel ( Section   2.2 ).",
            "Table   1  compares the performance of our 9B  \\ourmodel model on ContextualBench against state-of-the-art large models as well as comparable ones across the 7 question answering tasks. PopQA scores are measured in easy matching, while the remaining are measured in exact matching. As shown, GPT-4o  [ 30 ]  unsurprisingly aces most of the benchmarks. However, given its small size, our  \\ourmodel -9B model significantly outperforms strong open-source baselines such as Command-R and Command-R+ that have up to 10 times larger parameter counts. Remarkably, it achieves the state of the art in TruthfulQA, 2WikihopQA and HotpotQA in contextual settings. Overall, it also achieves the state of the art average performance, demonstrating our models strong ability across many contextual tasks. In particular, our model excels at 2WikiHopQA, with nearly a 25% increase in performance compared to GPT-4o.  Meanwhile, our 9B model consistently outperforms Llama-3.1 8B Instruct and gemma-2-9b-it across most benchmarks."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Standard LM-eval-harness benchmarks  [ 11 ] :  \\ourmodel -9B maintains relative competitiveness in standard world knowledge and reasoning abilities against comparable baselines.",
        "table": "S3.T2.5.1",
        "footnotes": [],
        "references": [
            "In this section, we provide more insights into  \\ourmodel . First, we introduce a novel chat template comprising two new chat roles with specific functions ( Section   2.1 ). Then, we briefly discuss the training process of  \\ourmodel ( Section   2.2 ).",
            "To overcome such complexities, we introduce two more optional roles (turns) in the conversational template for  \\ourmodel :  Thought  and  Observation . As shown in  Figure   2 , the  Observation  role is designated to house any contextual information acquired from external sources, which can be either retrieved documents for retrieval use case or function calls results in agentic tool use scenario. Meanwhile, the  Thought  role is designed for the model to speak out any internal reasoning, or tool use syntax to invoke certain function calls. The benefits of this change are manifold. First, it allows easy masking during training. Specifically,  System ,  User  and  Observation  turns may not be trainable as they are input information for the model to generate responses. Like  Assistant , on the other hand,  Thought  turns are to be included in the fine-tuning loss to train the model to produce such thoughts. Second, the separation and clarification of roles facilitate instruction hierarchy enforcement  [ 44 ] , making LLMs safer by ensuring them to respect the  System  prompt and refuse to follow malicious instructions injected in  User  and  Observation  turns. Third, the additional roles streamline the process of building reliable and secure RAG and agentic applications, by allowing developers to display or hide internal data processing steps, and avoid having to parse customized key words from the  Assistant  output.",
            "As shown in  Table   2 , our  \\ourmodel model performs competitively in terms of world knowledge, common sense and reasoning abilities, despite the fact that it is optimized for contextual and retrieval use cases.  Particularly, our 9B model outperforms Command-R  [ 36 ]  with 35B parameters in MMLU, GSM8K, TruthfulQA as well as ARC-C. Meanwhile it remains competitive to Llama-3.1-Instruct  [ 9 ]  and gemma-2-9b-it  [ 38 ] ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Scores on Berkeley function calling benchmark  [ 48 ] :  \\ourmodel -9B exhibits competitive function calling abilities.",
        "table": "S3.T3.5",
        "footnotes": [],
        "references": [
            "Figure   3  shows the average non-strict matching accuracy scores of different LLMs over 3 tasks under FaithEval suite.  As shown, other baselines such as GPT-4o exhibit high variations when the facts change in Counterfactual and Unknown settings. Particularly, GPT-4o scores low in Counterfactual setting perhaps because large models may have higher knowledge inertia and stronger resistance to factual changes.  Meanwhile, our  \\ourmodel -9B scores consistently highly, even when the context information is altered. This demonstrates that our model is usefully resilient and faithful to unseen contextual information. It also means that the model is more adaptable to the ever-changing world. Plus, our model is more capable of identifying contradiction in the contexts, as well as resisting against its own parametric knowledge when contextual information presented is counter-intuitive. In other words, the model remains more faithful to the context even if the context contradicts its pre-trained knowledge.",
            "Our model is also trained with function calling with a focus to support dynamic and multi-hop interactions with external tools to retrieve high-quality contextual information. As such, we compare our model with certain popular baselines in the Berkeley function calling task  [ 48 ] . As reported in  Table   3 , our  \\ourmodel performs competitively against comparable baselines such as Llama-3-8B-Instruct  [ 9 ]"
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  ContextualBench scores for HotpotQA, 2WikihopQA and Musique, as measured in easy matching, strict matching accuracy and the F1 score.",
        "table": "A1.T4.4.1",
        "footnotes": [],
        "references": [
            "Tables   4  and  5  present the complete results of ContextualBench across 7 benchmarks as measured in different metrics. As these metrics measure different aspects of the question-answering responses, the numbers provide more insights into the performance comparison between for models. For instance, a model that is too verbose may score low in EM and high in F1, and vice-versa."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  ContextualBench scores for TriviaQA, TruthfulQA, PopQA and NQ, as measured in easy matching, strict matching accuracy and the F1 score.",
        "table": "A1.T5.4.1",
        "footnotes": [],
        "references": [
            "Tables   4  and  5  present the complete results of ContextualBench across 7 benchmarks as measured in different metrics. As these metrics measure different aspects of the question-answering responses, the numbers provide more insights into the performance comparison between for models. For instance, a model that is too verbose may score low in EM and high in F1, and vice-versa."
        ]
    },
    "global_footnotes": [
        "The model will be made available via API and later fully open-sourced.",
        "ReAct uses arbitrarily literal phrases like Thought:, Result: and Final Answer: within the",
        "turn to parse reasoning, tool outputs and answers respectively, which the LLM may not always comply with."
    ]
}