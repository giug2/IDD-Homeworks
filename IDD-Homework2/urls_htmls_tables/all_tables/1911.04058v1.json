{
    "S5.T1": {
        "caption": "Table 1: The statistics of VQA 2.0 and VizWiz dataset. Numbers denote train/validation/test information, respectively.",
        "table": "<table id=\"S5.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"></th>\n<th id=\"S5.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">VQA 2.0</span></th>\n<th id=\"S5.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">VizWiz</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"># images</th>\n<td id=\"S5.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">83K/41K</td>\n<td id=\"S5.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">20K/3K/8K</td>\n</tr>\n<tr id=\"S5.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"># questions</th>\n<td id=\"S5.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">443K/214K/448K</td>\n<td id=\"S5.T1.1.3.2.3\" class=\"ltx_td ltx_align_center\">20K/3K/8K</td>\n</tr>\n<tr id=\"S5.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"># answers</th>\n<td id=\"S5.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">4.4M/2.1M/NA</td>\n<td id=\"S5.T1.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.2M/0.03M/NA</td>\n</tr>\n<tr id=\"S5.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r\"># unique answers</th>\n<td id=\"S5.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">3,126</td>\n<td id=\"S5.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b\">58,789</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Two standard VQA benchmarks are used in our experiments, VQA 2.0 [7] and VizWiz [9]. A comparison of the statistics for these datasets are listed in Table 1, which shows that the scale of VizWiz is much smaller in terms of the numbers of images and questions. Although VizWiz has more unique answers, only 824 out of its top 3,000 answers overlap with the top 3,000 answers in VQA 2.0. This explains why models trained on VQA 2.0 perform poorly on VizWiz, and their limited transferability. We find 28.63% of questions in VizWiz are even not answerable due to reasons mentioned before, making the domain gap even more significant. Figure 2 shows some examples from both VQA 2.0 and VizWiz datasets."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Accuracy (in %) of different methods on VizWiz.",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"> <span id=\"S5.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">VizWiz baseline</td>\n<td id=\"S5.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"> 47.50</td>\n</tr>\n<tr id=\"S5.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">BAN</td>\n<td id=\"S5.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"> 51.40</td>\n</tr>\n<tr id=\"S5.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Pythia<span id=\"footnote2\" class=\"ltx_note ltx_role_footnote\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">2</sup><span class=\"ltx_tag ltx_tag_note\">2</span>Please note that, the <math id=\"footnote2.m1.1\" class=\"ltx_Math\" alttext=\"54.72\\%\" display=\"inline\"><semantics id=\"footnote2.m1.1b\"><mrow id=\"footnote2.m1.1.1\" xref=\"footnote2.m1.1.1.cmml\"><mn id=\"footnote2.m1.1.1.2\" xref=\"footnote2.m1.1.1.2.cmml\">54.72</mn><mo id=\"footnote2.m1.1.1.1\" xref=\"footnote2.m1.1.1.1.cmml\">%</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"footnote2.m1.1c\"><apply id=\"footnote2.m1.1.1.cmml\" xref=\"footnote2.m1.1.1\"><csymbol cd=\"latexml\" id=\"footnote2.m1.1.1.1.cmml\" xref=\"footnote2.m1.1.1.1\">percent</csymbol><cn type=\"float\" id=\"footnote2.m1.1.1.2.cmml\" xref=\"footnote2.m1.1.1.2\">54.72</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"footnote2.m1.1d\">54.72\\%</annotation></semantics></math> accuracy for Pythia was obtained by fine-tuning from the model pretrained on the VQA 2.0 dataset.</span></span></span>\n</td>\n<td id=\"S5.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"> 54.72</td>\n</tr>\n<tr id=\"S5.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">Ours</td>\n<td id=\"S5.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"> <span id=\"S5.T2.1.5.4.2.1\" class=\"ltx_text ltx_font_bold\">55.87</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Adaptation from VQA 2.0 to VizWiz As discussed in previous sections, we first pretrain a source model on the VQA 2.0 dataset, and then adapt the pretrained source model to the target dataset VizWiz. The results of our proposed method and other leading methods are shown in Table 2.",
            "We first compare our method with the original VizWiz baseline proposed by [9], the previous state-of-the-art VQA model BAN by [14] and the current state-of-the-art VQA model Pythia by [24]. It is clear that our method outperforms the state-of-the-art models by a significant margin from Table 2."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Accuracy (in %) comparison for our base model. Target only denotes training from scratch, Fine-tune means fine-tuning and DA presents our domain adaptation method.",
        "table": "<table id=\"S5.T3.7\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.7.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.7.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.7.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Target only</span></td>\n<td id=\"S5.T3.7.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> <span id=\"S5.T3.7.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Fine-tune</span>\n</td>\n<td id=\"S5.T3.7.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"> <span id=\"S5.T3.7.1.1.3.1\" class=\"ltx_text ltx_font_bold\">DA</span>\n</td>\n</tr>\n<tr id=\"S5.T3.7.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.7.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">53.11</td>\n<td id=\"S5.T3.7.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"> 53.97</td>\n<td id=\"S5.T3.7.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"> 55.87</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "In order to validate that the better performance of our method is not due to a strong base model, we additionally report the results of our method in Table 3, with 1) training our single base model from scratch using only the VizWiz dataset (Target only), 2) fine-tuning from the model pretrained on the VQA 2.0 dataset (Fine-tune), and 3) our proposed domain adaptation method (DA). From Table 3, it shows that our model fine-tuned from VQA 2.0 is about 0.750.750.75 percent worse than Pythia fine-tuned from VQA 2.0 (53.97%percent53.9753.97\\% vs. 54.72%percent54.7254.72\\%), indicating that the better performance of our final model than the state-of-the-art is not from a strong base model. Moreover, the accuracy of our base model trained from scratch is 53.11%percent53.1153.11\\%, falling behind 0.60.60.6 percent to Pythia trained from scratch, which is consistent with our observation that our method even with a weaker base model can achieve superior final results."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Results breakdown into different categories of different methods for domain adaptation from VQA 2.0 to VizWiz. Breakdown numbers are performance on VizWiz test-dev split.",
        "table": "<table id=\"S5.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.3.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">(Accuracy in %)</th>\n<td id=\"S5.T4.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> <span id=\"S5.T4.3.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Overall</span>\n</td>\n<td id=\"S5.T4.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> <span id=\"S5.T4.3.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Yes/No</span>\n</td>\n<td id=\"S5.T4.3.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> <span id=\"S5.T4.3.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Number</span>\n</td>\n<td id=\"S5.T4.3.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> <span id=\"S5.T4.3.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Answerable</span>\n</td>\n<td id=\"S5.T4.3.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\"> <span id=\"S5.T4.3.1.1.6.1\" class=\"ltx_text ltx_font_bold\">Other</span>\n</td>\n</tr>\n<tr id=\"S5.T4.3.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">VizWiz baseline</th>\n<td id=\"S5.T4.3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 47.50</td>\n<td id=\"S5.T4.3.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 66.90</td>\n<td id=\"S5.T4.3.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 22.00</td>\n<td id=\"S5.T4.3.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 77.00</td>\n<td id=\"S5.T4.3.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"> 29.40</td>\n</tr>\n<tr id=\"S5.T4.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">BAN</th>\n<td id=\"S5.T4.3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 51.40</td>\n<td id=\"S5.T4.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 68.10</td>\n<td id=\"S5.T4.3.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 17.90</td>\n<td id=\"S5.T4.3.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> <span id=\"S5.T4.3.3.3.5.1\" class=\"ltx_text ltx_font_bold\">85.30</span>\n</td>\n<td id=\"S5.T4.3.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"> 31.50</td>\n</tr>\n<tr id=\"S5.T4.3.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Pythia</th>\n<td id=\"S5.T4.3.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 54.22</td>\n<td id=\"S5.T4.3.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> <span id=\"S5.T4.3.4.4.3.1\" class=\"ltx_text ltx_font_bold\">74.83</span>\n</td>\n<td id=\"S5.T4.3.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 31.11</td>\n<td id=\"S5.T4.3.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> 84.08</td>\n<td id=\"S5.T4.3.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"> 35.03</td>\n</tr>\n<tr id=\"S5.T4.3.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.3.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">Ours</th>\n<td id=\"S5.T4.3.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"> <span id=\"S5.T4.3.5.5.2.1\" class=\"ltx_text ltx_font_bold\">55.87</span>\n</td>\n<td id=\"S5.T4.3.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"> 74.33</td>\n<td id=\"S5.T4.3.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"> <span id=\"S5.T4.3.5.5.4.1\" class=\"ltx_text ltx_font_bold\">32.00</span>\n</td>\n<td id=\"S5.T4.3.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"> 83.32</td>\n<td id=\"S5.T4.3.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"> <span id=\"S5.T4.3.5.5.6.1\" class=\"ltx_text ltx_font_bold\">38.53</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Results breakdown into answer categories Table 4 shows the accuracy breakdown into different answer categories. The results show that our model achieves new state-of-the-art performance on “Number” and “Other” categories as well as overall accuracy. Note that the overall accuracy for Pythia in this table is 54.22%percent54.2254.22\\% instead of 54.72%percent54.7254.72\\% which we were unable to reproduce using the released code and there are no breakdown numbers reported associated with it. The best we can achieve with Pythia (after fine-tuning from VQA 2.0) is 54.22%percent54.2254.22\\% and the corresponding breakdown numbers are reported in the table."
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Ablation study of our proposed method.",
        "table": "<table id=\"S5.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></th>\n<th id=\"S5.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T5.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span></th>\n<th id=\"S5.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T5.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Improvement</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Target only</th>\n<td id=\"S5.T5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">53.11</td>\n<td id=\"S5.T5.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n</tr>\n<tr id=\"S5.T5.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">(+ Fine-tune)</th>\n<td id=\"S5.T5.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">53.97</td>\n<td id=\"S5.T5.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">+ 0.86</td>\n</tr>\n<tr id=\"S5.T5.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">+ MMD on V and Q, CLS</th>\n<td id=\"S5.T5.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.46</td>\n<td id=\"S5.T5.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">+ 1.49</td>\n</tr>\n<tr id=\"S5.T5.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">+ MMD, GRL on joint</th>\n<td id=\"S5.T5.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.87</td>\n<td id=\"S5.T5.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">+ 0.41</td>\n</tr>\n<tr id=\"S5.T5.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">+ Ensemble of 3 models</th>\n<td id=\"S5.T5.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">56.20</td>\n<td id=\"S5.T5.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">+ 0.33</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": []
    },
    "S5.T6": {
        "caption": "Table 6: Accuracy (in %) comparisons of our method with state-of-the-art domain adaptation methods.",
        "table": "<table id=\"S5.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T6.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">VizWiz</span></th>\n<th id=\"S5.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"> <span id=\"S5.T6.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Accuracy</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Fine-tune</th>\n<td id=\"S5.T6.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">53.97</td>\n</tr>\n<tr id=\"S5.T6.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">DANN+</th>\n<td id=\"S5.T6.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">53.65</td>\n</tr>\n<tr id=\"S5.T6.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">ADDA+</th>\n<td id=\"S5.T6.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">54.06</td>\n</tr>\n<tr id=\"S5.T6.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">WDGRL+</th>\n<td id=\"S5.T6.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">54.28</td>\n</tr>\n<tr id=\"S5.T6.1.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">SDT</th>\n<td id=\"S5.T6.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">54.56</td>\n</tr>\n<tr id=\"S5.T6.1.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T6.1.7.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">Ours</th>\n<td id=\"S5.T6.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T6.1.7.6.2.1\" class=\"ltx_text ltx_font_bold\">55.87</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Comparisons on domain adaptation methods We compare our multi-modal domain adaptation method with some popular domain adaptation methods, including DANN [4], ADDA [25], WDGRL [23], and SDT [11]. Note that DANN, ADDA and WDGRL were originally designed for unsupervised domain adaptation. For fair comparison, we fine-tune the model using target labels after unsupervised adaptation (hence they are indicated by a suffix ‘+’). SDT is currently the most popular and best-performing supervised domain adaptation method. The results shown in Table 6 illustrate that compared to direct fine-tuning, the existing domain adaptation methods do not help much (DANN performs even worse) in the multi-modal task, while our method outperforms both direct fine-tuning and existing domain adaptation methods by a notable margin."
        ]
    },
    "S5.T7": {
        "caption": "Table 7: Results comparison using less data.",
        "table": "<table id=\"S5.T7.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T7.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T7.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Target data used</span></th>\n<th id=\"S5.T7.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T7.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Target only</span></th>\n<th id=\"S5.T7.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T7.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Fine-tune</span></th>\n<th id=\"S5.T7.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T7.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">DA</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T7.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">1/8</th>\n<td id=\"S5.T7.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">39.51</td>\n<td id=\"S5.T7.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">43.39</td>\n<td id=\"S5.T7.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">45.02</td>\n</tr>\n<tr id=\"S5.T7.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">1/4</th>\n<td id=\"S5.T7.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">43.75</td>\n<td id=\"S5.T7.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.71</td>\n<td id=\"S5.T7.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">48.93</td>\n</tr>\n<tr id=\"S5.T7.1.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">1/2</th>\n<td id=\"S5.T7.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.48</td>\n<td id=\"S5.T7.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.12</td>\n<td id=\"S5.T7.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">52.32</td>\n</tr>\n<tr id=\"S5.T7.1.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T7.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">All data</th>\n<td id=\"S5.T7.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">53.11</td>\n<td id=\"S5.T7.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">53.97</td>\n<td id=\"S5.T7.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">55.87</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Adaptation with fewer target training samples We also validate the robustness of our framework by reducing the target training dataset size. We experiment with various target sizes of 1/8 (2,500), 1/4 (5,000), 1/2 (10,000) and all data (20,000). The results are shown in Table 7. We can observe that with the increase of the amount of training data, the performance gain over fine-tuning is decreasing. We conjecture that this is because when we have limited amount of target data, having more prior knowledge is beneficial to model performance, while having more target data will make prior knowledge less helpful. However, our method can stably improve the performance because it sufficiently makes use of target data and source data. It is more promising that our domain adaptation method using fewer samples can achieve comparable or better performance compared to training from scratch using doubled amount of data (especially when target data is scarce), e.g., our method using 1/4 data (48.93%) outperforms training from scratch using 1/2 data (47.48%)."
        ]
    },
    "S5.T8": {
        "caption": "Table 8: Accuracy (in %) comparison for our single base model adapted from VizWiz to VQA 2.0.",
        "table": "<table id=\"S5.T8.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T8.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T8.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T8.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Target only</span></td>\n<td id=\"S5.T8.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"> <span id=\"S5.T8.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Fine-tune</span>\n</td>\n<td id=\"S5.T8.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"> <span id=\"S5.T8.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">DA</span>\n</td>\n</tr>\n<tr id=\"S5.T8.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T8.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">68.89</td>\n<td id=\"S5.T8.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"> 69.25</td>\n<td id=\"S5.T8.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"> 70.06</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Adaptation from VizWiz to VQA 2.0 In order to further validate the robustness of our method, we reverse the source domain and the target domain and perform adaptation. We pretrain the source model on VizWiz and adapt the source model to VQA 2.0. The results are shown in Table 8, from which we still can observe a significant improvement for our method against fine-tuning. As a comparison, the performance of BAN and Pythia trained from scratch are 69.08% and 69.21%, and our DA model achieves comparable performance to the state-of-the-art on VQA 2.0."
        ]
    }
}