{
    "id_table_1": {
        "caption": "Table 1:  One example from CANARD dataset.  u 1 subscript u 1 u_{1} italic_u start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT - u 2 subscript u 2 u_{2} italic_u start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  are 2 turns of contextual utterances,  u 3 subscript u 3 u_{3} italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT  denotes the incomplete utterance,  u 3  superscript subscript u 3 u_{3}^{*} italic_u start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  denote the golden rewritten utterance,  e 1 / e 1  subscript e 1 superscript subscript e 1 e_{1}/e_{1}^{*} italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT / italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  and  e 2 / e 2  subscript e 2 superscript subscript e 2 e_{2}/e_{2}^{*} italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT / italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  denote two candidate example pairs of incomplete utterance and rewritten utterance which can be prompted to the LLM.",
        "table": "S1.T2.8",
        "footnotes": [],
        "references": [
            "What if we select the examples only by metrics of complexity and abundance?  To address this issue, we select examples from the candidate set to construct the demonstration by metrics of the length of utterance, the number of POS types, the number of text chunks respectively and conduct the experiments on CANARD dataset.  In Table  10 , if selecting the examples only by the metric of number of text chunks, the ICL results will drop by about 2.1 ROUGE score, 0.9 BLEU score, 2.9 F-score on CANARD dataset. It will drop more if selecting the examples only by the metric of utterance length or POS types. It shows only selecting examples by complexity and abundance, the LLM performance will not necessarily improve. Balancing the abundance and the similarity with the test case of examples is important to improve the ICL ability of LLM. Our approach attains the balance of complexity and abundance, as well as semantic similarity with the test case without explicit assignment.",
            "In this part, we conduct the experiments by applying our approach with gpt3.5 in TASK dataset. In Table  11 , our RLS outperforms random selecting examples by about 4.4 ROUGE score, 9.5 BLEU score and 9.4 F-score in TASK dataset. It demonstrates the efficiency of directly utilizing LLM feedback to train the LM retriever with policy-based RL and improve the analogy ability of larger LLM. With the emergence of larger LLM, it is promising for our approach to select appropriate examples to improve the ICL performance of LLM.",
            "In this part, we probe the effect of the order of examples in the demonstration. We arrange examples in the demonstration by the order of sampling with Eq. 3  or the reverse order and conduct the experiments on TASK dataset. Table  12  shows if arranging the examples by the reverse order of sampling, our approach shows comparable performance . It demonstrates the stable efficiency of our directly selecting the examples that can improve the analogy ability of LLM by policy gradient."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The Rouge score of ChatGLM by use of different examples in prompts. Sparse denotes selecting the example by sparse retrieval methods like BM25, Dense denotes selecting the example by dense representations by PTM.",
        "table": "S1.T2.22",
        "footnotes": [],
        "references": [
            "We take a case from TASK dataset in Table  2  and  2 , where the incomplete utterance is How about Mediterranean food? and the omitted part is the postpositive attributive in expensive price range. By metrics of sparse retrieval methods like BM25  Robertson et al. ( 2009 )  or dense retrieval methods like PTM  Liu et al. ( 2022 ) , the example incomplete utterance  e 1 subscript e 1 e_{1} italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  as well as its contexts and rewritten utterance should be selected to be in-context examples. However, the performance of LLM for this question with example  e 1 subscript e 1 e_{1} italic_e start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  drops by 5 ROUGE score compared with example  e 2 subscript e 2 e_{2} italic_e start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT .",
            "Demonstration selection is crucial to ICL and  Liu et al. ( 2021 )  showed that downstream performance can  vary widely depending on the choice of in-context  examples.   Liu et al. ( 2022 )  utilize sentence representations of PTM to select the examples with more cosine similarity.  Sorensen et al. ( 2022 )  and  Gonen et al. ( 2022 )  argue that mutual information and perplexity are also valuable selection metrics which do not need labeled examples and specific LLM.   Levy et al. ( 2022 )  select diverse demonstrations to collectively cover all of the structures required  in the outputs.   Kim et al. ( 2022 )  generate  demonstrations for ICL from  PLM itself to minimize the reliance on the  external demonstration.   Rubin et al. ( 2021 )  first build an unsupervised  retriever like BM25 to recall similar  examples as candidates and then construct a supervised  retriever to select demonstrations from candidates  However, these methods fail to directly select the examples into demonstrations that can improve the analogy ability of LLM for IUR task and some useful examples like in table  2  will be neglected.",
            "With the policy gradient computed, we optimize the parameters of policy model defined in section  4.2 . We compute the performance on validation set to control the termination of iterating process. In practice, we sample a small subset from the original training dataset to compose the candidate example set  C C C italic_C  and another disjoint subset to form  T T T italic_T  to train our RLS algorithm.",
            "In this part, we probe the effect of the order of examples in the demonstration. We arrange examples in the demonstration by the order of sampling with Eq. 3  or the reverse order and conduct the experiments on TASK dataset. Table  12  shows if arranging the examples by the reverse order of sampling, our approach shows comparable performance . It demonstrates the stable efficiency of our directly selecting the examples that can improve the analogy ability of LLM by policy gradient."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:   Statistics of datasets. Con. len, Cur. len, Rew. len denote the length of contexts, incomplete utterance and rewritten utterance respectively.",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "Intuitively, if a subset of candidate examples leads to increasing performance of LLM to rewrite the current incomplete utterance, RLS should assign high scores to them; the more the performance  increases, the higher the example scores should be. For each case in training set, we input the demonstration formulated as section  3  containing the examples selected by the current policy into the LLM. The performance metrics of outputs from the LLM against the golden rewritten utterance are utilized as the rewards of the policy.",
            "Following  Liu et al. ( 2022 )  and  Rubin et al. ( 2021 )  we utilize SentenceBERT  Reimers and Gurevych ( 2019 )  as LM selector for English datasets and bert-base-Chinese for Chinese datasets. The sizes of candidate example set  C C C italic_C  and training set  T T T italic_T  are 500. The learning rate is set to be 1e-5. The task instruction in section  3  is designed as Rewrite an incomplete utterance into an utterance which is semantically equivalent but self-contained to be understood without context. The sentence structure and expression should be consistent. for English dataset and its translated version for Chinese dataset.",
            "In this part, we probe the effect of the order of examples in the demonstration. We arrange examples in the demonstration by the order of sampling with Eq. 3  or the reverse order and conduct the experiments on TASK dataset. Table  12  shows if arranging the examples by the reverse order of sampling, our approach shows comparable performance . It demonstrates the stable efficiency of our directly selecting the examples that can improve the analogy ability of LLM by policy gradient."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:   ICL Evaluations of ChatGLM with 5-shot demonstrations on CANARD dataset. Our approach significantly outperforms existing example selection methods, where p-values of ROUGE, BLEU and F-score are smaller  than 0.001.",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "With the policy gradient computed, we optimize the parameters of policy model defined in section  4.2 . We compute the performance on validation set to control the termination of iterating process. In practice, we sample a small subset from the original training dataset to compose the candidate example set  C C C italic_C  and another disjoint subset to form  T T T italic_T  to train our RLS algorithm.",
            "In Table  4 , our approach outperforms all the baselines by about 1.2-1.7 ROUGE score, 1.3-2.5 BLEU score, 0.4 F2 score and 0.6 F3 score in CANARD dataset. It demonstrates the efficiency of directly utilizing feedback by LLM and RL policy gradient to train the LM selector. With the examples selected by our method, the performances of LLM are significantly improved. Our model not only improves the overall quality of utterance rewritten, but also captures the important words from the context.",
            "In Table  4 , the number of candidates  #  C # C \\#C # italic_C  and training samples  #  T # T \\#T # italic_T  are 500. In this part, we do further experiments with  #  C = 100 , #  T = 100 formulae-sequence # C 100 # T 100 \\mathbf{\\#C}=100,\\mathbf{\\#T}=100 # bold_C = 100 , # bold_T = 100 ;  #  C = 100 , #  T = 500 formulae-sequence # C 100 # T 500 \\mathbf{\\#C}=100,\\mathbf{\\#T}=500 # bold_C = 100 , # bold_T = 500 ;  #  C = 500 , #  T = 100 formulae-sequence # C 500 # T 100 \\mathbf{\\#C}=500,\\mathbf{\\#T}=100 # bold_C = 500 , # bold_T = 100  and freeze other hyperparameters respectively. In Table  6 , generally, with more candidates and training samples, our approach will select better examples for the demonstration. With 100 candidates and 100 training samples, our approach beats random selection by about 2.0 ROUGE score, 2.5 BLEU score and 2.0 F-score. It is also comparable to the competitive baseline BSR with 500 candidates and training samples. It shows the efficiency to utilize direct LLM feedback to train the LM retriever.  Compared with setting 100-500, our approach outperforms by about 0.7 ROUGE score, 0.8 BLEU score and 0.9 F-score. It demonstrates our ability to select better examples to improve the analogy ability of LLM With more candidates.  Compared with setting 500-100, our approach outperforms by about 1.4 ROUGE score, 1.9 BLEU score and 0.2 F-score. It shows more training samples improve the selection of our approach from the candidate set for IUR task.",
            "In Table  4 , we conduct the experiments with 5-shot demonstrations. In this part, we do further experiments with 3-shot and 4-shot demonstrations. In Table  7 , generally with more examples in the demonstration, our approach improves the ICL performance of LLM for IUR task. Especially, with more demonstration examples, our approach derives more improvement compared with the competitive baseline EPR. It demonstrates the efficiency of our RLS by directly utilizing LLM feedback to train the LM retriever and improve the ICL performance of LLM."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:   ICL Evaluations of ChatGLM with 5-shot demonstrations on Task and REWRITE dataset. Our approach significantly outperforms existing example selection methods, where p-values of ROUGE, BLEU and F-score are smaller  than 0.001.",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": []
    },
    "id_table_6": {
        "caption": "Table 6:   Evaluations of ChatGLM with different sizes of candidates and training samples on CANARD dataset. 500-100 denotes experiments with 500 candidates and 100 training samples, and so on.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "In Table  4 , the number of candidates  #  C # C \\#C # italic_C  and training samples  #  T # T \\#T # italic_T  are 500. In this part, we do further experiments with  #  C = 100 , #  T = 100 formulae-sequence # C 100 # T 100 \\mathbf{\\#C}=100,\\mathbf{\\#T}=100 # bold_C = 100 , # bold_T = 100 ;  #  C = 100 , #  T = 500 formulae-sequence # C 100 # T 500 \\mathbf{\\#C}=100,\\mathbf{\\#T}=500 # bold_C = 100 , # bold_T = 500 ;  #  C = 500 , #  T = 100 formulae-sequence # C 500 # T 100 \\mathbf{\\#C}=500,\\mathbf{\\#T}=100 # bold_C = 500 , # bold_T = 100  and freeze other hyperparameters respectively. In Table  6 , generally, with more candidates and training samples, our approach will select better examples for the demonstration. With 100 candidates and 100 training samples, our approach beats random selection by about 2.0 ROUGE score, 2.5 BLEU score and 2.0 F-score. It is also comparable to the competitive baseline BSR with 500 candidates and training samples. It shows the efficiency to utilize direct LLM feedback to train the LM retriever.  Compared with setting 100-500, our approach outperforms by about 0.7 ROUGE score, 0.8 BLEU score and 0.9 F-score. It demonstrates our ability to select better examples to improve the analogy ability of LLM With more candidates.  Compared with setting 500-100, our approach outperforms by about 1.4 ROUGE score, 1.9 BLEU score and 0.2 F-score. It shows more training samples improve the selection of our approach from the candidate set for IUR task."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:   Evaluations of ChatGLM with different numbers of examples in the demonstration on CANARD dataset. EPR-3 denotes the baseline EPR with 3 examples in the demonstration.",
        "table": "S5.T7.1",
        "footnotes": [],
        "references": [
            "In Table  4 , we conduct the experiments with 5-shot demonstrations. In this part, we do further experiments with 3-shot and 4-shot demonstrations. In Table  7 , generally with more examples in the demonstration, our approach improves the ICL performance of LLM for IUR task. Especially, with more demonstration examples, our approach derives more improvement compared with the competitive baseline EPR. It demonstrates the efficiency of our RLS by directly utilizing LLM feedback to train the LM retriever and improve the ICL performance of LLM."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:   Comparing with SFT model in few-shot setting.",
        "table": "S5.T8.1",
        "footnotes": [],
        "references": [
            "In this part, we compare our approach with the existing state-of-the-art QUEEN  Liu et al. ( 2020 )  in IUR field. QUEEN tackles IUR task by finetuning PTM  Devlin et al. ( 2018 )  and constructing the word edit matrix. Different from QUEEN , our approach utilizes LM as a proxy to select appropriate examples and parameters of the answer generator ChatGLM are fixed. To keep a fair comparison, we assign the candidate set and training set in our approach as the training set of QUEEN.  In Table  8 , our approach QUEEN by 9.2 F1 score, 7.3 F2 score and 4.9 F3 score. F-score concentrate on the words from the context, which are argued to be harder to copy  Pan et al. ( 2019 ) . It shows our efficiency to capture important words from the context to rewrite the incomplete utterance. Considering our RLS approach does not depend on the LLM server (ChatGLM in our experiments), it is promising for our approach to derive better results for IUR task with stronger LLM."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:   Complexity and abundance of example selected. Incomplete denotes the metrics of incomplete utterance.Rewritten denotes the metrics of rewritten utterance,  Length, POS, and Chunk denote utterance length, number of POS types and number of text chunks respectively.",
        "table": "S5.T9.1",
        "footnotes": [],
        "references": [
            "We assume that other than the textual and semantic similarity of examples with the current test case, the complexity and abundance of examples in the demonstrations also matter for IUR task.  We evaluate the complexity and abundance of examples by three metrics of incomplete utterance and rewritten utterance: 1. the length of utterance; 2. the number of Part Of Speech (POS) tagging types  Kumawat and Jain ( 2015 ) . 3. the number of text chunks  Ramshaw and Marcus ( 1999 ) . We assume the examples are more complex and abundant with longer utterances, more POS tag types and more text chunks. In practice, we adopt SpaCy to do POS tagging and text chunking.  In Table  9 , the examples selected by our policy-based RL framework have longer utterances, more POS types and more text chunks though without explicit assignment, We argue that the complexity and abundance of examples in the demonstration are important to improve the analogy ability of LLM."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:   ICL Evaluations of ChatGLM with 5-shot demonstrations on CANARD dataset.",
        "table": "S5.T10.1",
        "footnotes": [],
        "references": [
            "What if we select the examples only by metrics of complexity and abundance?  To address this issue, we select examples from the candidate set to construct the demonstration by metrics of the length of utterance, the number of POS types, the number of text chunks respectively and conduct the experiments on CANARD dataset.  In Table  10 , if selecting the examples only by the metric of number of text chunks, the ICL results will drop by about 2.1 ROUGE score, 0.9 BLEU score, 2.9 F-score on CANARD dataset. It will drop more if selecting the examples only by the metric of utterance length or POS types. It shows only selecting examples by complexity and abundance, the LLM performance will not necessarily improve. Balancing the abundance and the similarity with the test case of examples is important to improve the ICL ability of LLM. Our approach attains the balance of complexity and abundance, as well as semantic similarity with the test case without explicit assignment."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:   ICL Evaluations of gpt3.5 with 5-shot demonstrations on TASK dataset.",
        "table": "S5.T11.1",
        "footnotes": [],
        "references": [
            "In this part, we conduct the experiments by applying our approach with gpt3.5 in TASK dataset. In Table  11 , our RLS outperforms random selecting examples by about 4.4 ROUGE score, 9.5 BLEU score and 9.4 F-score in TASK dataset. It demonstrates the efficiency of directly utilizing LLM feedback to train the LM retriever with policy-based RL and improve the analogy ability of larger LLM. With the emergence of larger LLM, it is promising for our approach to select appropriate examples to improve the ICL performance of LLM."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:   ICL Evaluations of example orders with ChatGLM on TASK dataset.",
        "table": "S5.T12.1",
        "footnotes": [],
        "references": [
            "In this part, we probe the effect of the order of examples in the demonstration. We arrange examples in the demonstration by the order of sampling with Eq. 3  or the reverse order and conduct the experiments on TASK dataset. Table  12  shows if arranging the examples by the reverse order of sampling, our approach shows comparable performance . It demonstrates the stable efficiency of our directly selecting the examples that can improve the analogy ability of LLM by policy gradient."
        ]
    },
    "global_footnotes": [
        "https://github.com/THUDM/ChatGLM-6B"
    ]
}