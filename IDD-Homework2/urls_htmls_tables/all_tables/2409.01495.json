{
    "id_table_1": {
        "caption": "",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "This  session-based paradigm  of interaction with LLMs results from several factors and one of them is the context window limit.  Recent LLMs are typically pre-trained with around 4K window size  (Dubey et al.,  2024 )  (or higher for some proprietary models), with potentially long context fine-tuning up to 32K or more.  In inference time, some can scale to millions of input tokens such as Gemini 2 2 2 https://gemini.google.com/app .  As large as it seems, it is still far from enough to digest context that could be fed to an OS for it to be meaningfully stateful:  A single HD image can take more than 1K tokens to represent;  one web search could return 10 web pages each with a few thousand tokens;  and a repo-level code can easily go up to thousands of lines.  While the session-based paradigm can already solve many problems, many daily tasks still require accessing long context.  To develop an LM OS that can assist with real-world tasks and stay stateful throughout the process, one must manage the context information in a life-long manner (Figure  1 ).   We believe the lack of a principled architecture for managing the life cycle of context is one of the main obstacles to shifting from the session-based paradigm to the OS paradigm .",
            "Intuitively, through Eq.( 1 ) and Eq.( 2 ),  we achieve a dynamic retrieval scheme that searches and aggregates context at different levels of granularities suiting the need of the current task with pure self-attention operations .  At each level, we aggregate all context into  r ~ l subscript ~ r l \\tilde{r}_{l} over~ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  through self-attention, making sure all information at this level of granularity is gathered.  And then based on the attention  a l subscript a l {\\bm{a}}_{l} bold_italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , we further identify the parts that the models are more interested in, which presumably are the context that are more relevant, and thus require more fine-grained information.  We collect the top  C C C italic_C  of those and their lower-level embeddings and proceed with a lower-level search until we hit the bottom.",
            "Results .  We show results in Table  1 .  The experiment shows that our method achieves 75% of the 6-shot ICL performance, indicating that the model has successfully picked the correct examples.  To further validate this aspect, we track the top-level attentions and compare the top indices with those of the relevant examples, the match rate is 64%, meaning for 64% of the test cases the model finds all the correct examples.  Still, the score can be significantly improved by scaling the experiments.  We leave this part in future development."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "S5.T1.1.4.4.1.1",
        "footnotes": [],
        "references": [
            "a l + 1 subscript a l 1 {\\bm{a}}_{l+1} bold_italic_a start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT  is the attention of  r ~ l + 1 subscript ~ r l 1 \\tilde{r}_{l+1} over~ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_l + 1 end_POSTSUBSCRIPT  on the embedding at level  l + 1 l 1 l+1 italic_l + 1 . We define the  TopC  (  ) TopC  \\text{TopC}(\\cdot) TopC (  )  function, which will  (1) gather the top  C C C italic_C  indices at level  l + 1 l 1 l+1 italic_l + 1  that has the highest attention score;  and (2) further gather the embeddings at level  l l l italic_l  that were attended by the embeddings indexed by the top  C C C italic_C  indices, making it  m ~ l , C subscript ~ m l C \\tilde{{\\bm{m}}}_{l,C} over~ start_ARG bold_italic_m end_ARG start_POSTSUBSCRIPT italic_l , italic_C end_POSTSUBSCRIPT .  In Eq.( 2 ), the  m ~ l , C subscript ~ m l C \\tilde{{\\bm{m}}}_{l,C} over~ start_ARG bold_italic_m end_ARG start_POSTSUBSCRIPT italic_l , italic_C end_POSTSUBSCRIPT  of length  C  k C k C*k italic_C  italic_k  (recall that each embedding attends to  k k k italic_k  lower-level embeddings) becomes the input of lower-level self-attention together with the lower-level retrieval embedding  r ~ l subscript ~ r l \\tilde{r}_{l} over~ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT .",
            "Intuitively, through Eq.( 1 ) and Eq.( 2 ),  we achieve a dynamic retrieval scheme that searches and aggregates context at different levels of granularities suiting the need of the current task with pure self-attention operations .  At each level, we aggregate all context into  r ~ l subscript ~ r l \\tilde{r}_{l} over~ start_ARG italic_r end_ARG start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  through self-attention, making sure all information at this level of granularity is gathered.  And then based on the attention  a l subscript a l {\\bm{a}}_{l} bold_italic_a start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , we further identify the parts that the models are more interested in, which presumably are the context that are more relevant, and thus require more fine-grained information.  We collect the top  C C C italic_C  of those and their lower-level embeddings and proceed with a lower-level search until we hit the bottom."
        ]
    },
    "global_footnotes": [
        "https://gemini.google.com/app"
    ]
}