{
    "id_table_1": {
        "caption": "Table 1:    Comparing the scale of modern continued pretraining (CPT) works with our small corpus setting.  Prior work adapts language models to broad domains with diverse, large-scale corpora.  We aim to downscale continued pretraining to small corpora; we use a corpus that is 10,000  \\times   smaller than the smallest modern corpus for domain-adaptive CPT.",
        "table": "S2.T1.3.1",
        "footnotes": [],
        "references": [
            "To address this shortcoming, we propose EntiGraph, an entity-centric augmentation algorithm.  EntiGraph first breaks down a text corpus into a list of entities and then uses a language model to generate text descriptions about relations among the extracted entities, iteratively filling in the knowledge graph underlying the corpus (Figure  1 ).",
            "Continual or continued  pretraining  works  (Gururangan et al.,  2020 )  successfully adapt pretrained large language models to broad target domains such as code  (Roziere et al.,  2024 ) , medicine  (Chen et al.,  2023 ) , or mathematics  (Lewkowycz et al.,  2022 ; Shao et al.,  2024 ; Azerbayev et al.,  2024 )  by collecting massive datasets (often  > > > 100B tokens, shown in Table  1 ) and developing efficient training recipes using causal language modeling  (Gupta et al.,  2023 ; Ibrahim et al.,  2024 ; Parmar et al.,  2024 ) .  This work aims to extend the success of continued pretraining to small, specialized domains such as proprietary document stores.  Observing that standard continued pretraining is ineffective on small corpora, we propose a knowledge graphinspired approach to synthesize a diverse related corpus and find it more amenable to learning.",
            "We focus on learning parametric knowledge from a small text corpus.  Our goal is to continually pretrain a language model to acquire the knowledge of a niche corpus of documents.  Observing that simple continued pretraining is ineffective ( 4 ), we propose to use synthetic continued pretraining, which first uses the small corpus to synthesize a larger one more amenable to learning, and then continues pretraining on the synthetic corpus.  In this section, we first outline this problem setting and our evaluation approach in more detail ( 2.1 ).  Then, we provide a concrete instantiation of synthetic continued pretraining using a data augmentation algorithm called EntiGraph ( 2.2 ).",
            "Next, we present EntiGraph, our instantiation of a synthetic data augmentation algorithm  A synth subscript A synth \\mathcal{A}_{\\text{synth}} caligraphic_A start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT .  At a high level, EntiGraph generates diverse representations of knowledge from a small corpus  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  by using a prompted LLM to synthesize a knowledge graph representation of  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT .  EntiGraph consists of two steps/prompts: extracting entities from the document and analyzing relations among an arbitrary subset of the entities (Figure  1 ).  Altogether, this hierarchical prompting strategy  externalizes  the problem of generating diverse synthetic text to a combinatorial structurenamely, a graph relating various entities appearing in the corpus documents.  In what follows, we provide abbreviated prompts to illustrate the algorithm, and defer full prompts to Appendix  G.1 .",
            "Finally, we collect all sampled synthetic texts from Step 2 as the EntiGraph output:  D EntiGraph = { D ~ E i 1  ...  E i k , ... } subscript D EntiGraph subscript ~ D subscript E subscript i 1 ... subscript E subscript i k ... {\\mathcal{D}}_{\\text{EntiGraph}}=\\{\\widetilde{D}_{E_{i_{1}}\\dots E_{i_{k}}},\\dots\\} caligraphic_D start_POSTSUBSCRIPT EntiGraph end_POSTSUBSCRIPT = { over~ start_ARG italic_D end_ARG start_POSTSUBSCRIPT italic_E start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ... italic_E start_POSTSUBSCRIPT italic_i start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUBSCRIPT end_POSTSUBSCRIPT , ... } .  Altogether, we described a data augmentation algorithm mapping a small source corpus  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  to a larger synthetic corpus  D EntiGraph subscript D EntiGraph {\\mathcal{D}}_{\\text{EntiGraph}} caligraphic_D start_POSTSUBSCRIPT EntiGraph end_POSTSUBSCRIPT , as in ( 1 ).",
            "In this section, we describe in detail how we evaluate a given data augmentation algorithm  A synth subscript A synth \\mathcal{A}_{\\text{synth}} caligraphic_A start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT .  As described in the problem setup ( 2.1 ), we evaluate such algorithms  A synth subscript A synth \\mathcal{A}_{\\text{synth}} caligraphic_A start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT  by evaluating whether a language model continually pretrained on their output synthetic corpus  A synth  ( D source ) subscript A synth subscript D source \\mathcal{A}_{\\text{synth}}({\\mathcal{D}}_{\\text{source}}) caligraphic_A start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT )  can accurately answer test queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT  about the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT .",
            "In our main experiments, we use queries that are unambiguous even without the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT , and disallow the model from accessing  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  while answering the queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT  ( 2.1 ).  This allows us to evaluate which data augmentation algorithm best promotes the acquisition of parametric knowledge through synthetic CPT.  Later, in  5 , we consider an open-book setting where the model can access both the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  and test queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT  at the same time, in order to test how the parametric knowledge acquired through synthetic CPT composes with non-parametric access to knowledge through retrieval  (Lewis et al.,  2020 ) .",
            "We continually pretrain Llama 3 8B  (Dubey et al.,  2024 )  with standard causal language modeling on the 455M token EntiGraph corpus 3 3 3 Model  https://huggingface.co/zitongyang/llama-3-8b-entigraph-quality . .  In  4.1 , we describe our continued pretraining procedure and introduce two natural baselines.  In  4.2 , we evaluate all methods on the QuALITY test queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT .  In  4.3 , we show that synthetic CPT using EntiGraph is compatible with downstream instruction tuning  (Ouyang et al.,  2022 ) , an important feature of real pretraining data.",
            "Another simple synthetic data augmentation procedure is to rephrase QuALITY articles many times.  As discussed in  1.1 ,  Maini et al. ( 2024 )  and  Ovadia et al. ( 2024 )  execute a systematic extension of this idea.  Based on their approaches, we craft three fixed prompts (easy, medium, and hard rephrase) and repeatedly apply them to the QuALITY articles at temperature 1.0 4 4 4 Note that  Maini et al. ( 2024 )  also includes a fourth prompt that generates synthetic QA pairs.  We defer this task-specific QA finetuning approach to Appendix  D  and focus on task-agnostic baselines that teach generic knowledge about QuALITY articles. .  We refer to this data augmentation algorithm as the Rephrase baseline.  We stopped generating paraphrases at 38M tokens, where we observed a clear gap in QA evaluations from EntiGraph CPT and a slower scaling trend (Figure  2 ).  We will refer to this data as the Rephrase corpus and the continually pretrained Llama 3 8B Base models as the Rephrase CPT.",
            "Each QuALITY question is a four-choice, single-answer multiple choice question (similar to MMLU,  Hendrycks et al. ( 2021 ) ).  We evaluate with 5-shot chain-of-thought prompting  (Brown et al.,  2020 ; Wei et al.,  2024 )  and provide our prompt in Appendix  H.1 .",
            "It may seem surprising that simply rewriting the factual content of the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  can improve performance at all ( 4 ), as the EntiGraph data augmentation algorithm does not explicitly add new factual information beyond  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT .  In this section, we build a mathematical model based on a stochastic process on graphs to offer an explanation for this phenomenon.  We postulate that EntiGraph does not create knowledge  de novo ; rather, it simply rearranges the knowledge of  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  into a layout more amenable to learning.  For example, in  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT , the entity pair  ( A , B ) A B (A,B) ( italic_A , italic_B )  may appear together in some sentences and  ( B , C ) B C (B,C) ( italic_B , italic_C )  in others.  As a result, models trained directly on  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  with a next-token prediction objective may learn the  ( A , B ) A B (A,B) ( italic_A , italic_B )  relation and the  ( B , C ) B C (B,C) ( italic_B , italic_C )  relation, but not the relation between  A A A italic_A  and  C C C italic_C   (Akyurek et al.,  2024 ) .  We will build a mathematical model that formalizes this intuition ( 6.1 ).  Based on this model, we provide a quantitative prediction that the scaling trend of EntiGraph CPT follows a mixture-of-exponential shape ( 6.3 ), which fits well with our empirically observed scaling trend (Figure  4 ).",
            "Even though Theorem  1  provides mathematically rigorous upper and lower bounds on the scaling trend of  Acc  ( M t ) Acc subscript M t \\mathsf{Acc}({\\bm{M}}_{t}) sansserif_Acc ( bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) , the exact growth curve is more intricate, as we will show next.",
            "Continual learning is rooted in historical work on connectionist networks  (McCloskey & Cohen,  1989 ; Ratcliff,  1990 )  and considers learning with tasks arriving in an online manner  (Schlimmer & Fisher,  1986 ; Grossberg,  2012 ) .  The main focus is on mitigating a neural nets catastrophic forgetting of previously encountered tasks  (Robins,  1995 ; Goodfellow et al.,  2015 ; Kemker et al.,  2018 ) .  Approaches include regularizing parameter updates to preserve important parameters  (Nguyen et al.,  2017 ; Zenke et al.,  2017 ; Kirkpatrick et al.,  2017 ) ; dynamically modifying the architecture  (Rusu et al.,  2016 ; Golkar et al.,  2019 ) ; and recalling or replaying previous experiences  (Rebuffi et al.,  2017 ; Shin et al.,  2017 ; Lopez-Paz & Ranzato,  2017 ) .  Modern works in continued pretraining (cf.  1.1 ) effectively mitigate catastrophic forgetting by scaling parameter count  (Ramasesh et al.,  2022 )  and mixing in updates on pretraining data  (Ouyang et al.,  2022 ) .",
            "We follow the same set as in  2.1  and  3  except that we do not prompt  LM synth subscript LM synth \\mathsf{LM}_{\\text{synth}} sansserif_LM start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT  to generate general knowledge about QuALTY articles.  Instead, we prompt  LM synth subscript LM synth \\mathsf{LM}_{\\text{synth}} sansserif_LM start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT  to generate QA pairs directly:",
            "We repeat this prompt many times at temperature 1.0, resulting in 28M tokens on synthetic question answer pairs.  We perform the same continued pretraining procedure in  4.1  on Llama 3 8B and refer to this model as QA SFT.",
            "Our full few-shot chain-of-thought evaluation prompts for the open-book setting are provided in the codebase.  Similar to the closed-book QA evaluation prompt, we manually write and fact-check in-context learning examples about well-known books, to avoid leaking knowledge from the QuALITY articles.  In early experiments, we found that placing the retrieved contexts first, followed by the question and answer choices after, significantly improved performance compared to question-then-contexts; we use this format throughout the retrieval experiments.  We treat as a hyperparameter whether the reranked chunks are ordered from the best match to worst ( best_first ) or from the worst match to best ( best_last ).  When performing few-shot evaluation, we follow the sampling procedure used in the closed-book experiments (Appendix  H.1 ).  Specifically, we generate 64 responses for each question, and filter out responses that do not parse to one of the four choices.  Lastly, we randomly select one of the valid responses as the models final answer.",
            "In this section, we prove Theorem  1  and provide the derivations for several other approximation formulas.",
            "With Lemma  F.1  and Lemma  F.2 , we can now give useful estimates of  | R | R |{\\mathcal{R}}| | caligraphic_R | .  In particular, for any   > 0  0 \\varepsilon>0 italic_ > 0 ,",
            "We generate two synthetic corpora in this paper: EntiGraph (Appendix  G.1 ) and the Rephrase baseline (Appendix  G.2 ).  In our experiments, the  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  is a collection of documents  D D D italic_D , and our synthetic augmentation procedure is applied to each document  D  D source D subscript D source D\\in{\\mathcal{D}}_{\\text{source}} italic_D  caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT .  We will focus on a single document  D D D italic_D  for the remainder of this section.",
            "Concretely, we apply the same instruction procedure described in  4.3  to the Raw CPT and Rephrase CPT models from  4.1 , obtaining two additional instruction-tuned models that have knowledge about QuALITY books.  In addition, we also subsample 29M tokens out of the 455M token EntiGraph corpus to token-match the Raw and Rephrase corpus, and refer to the corresponding instruction tuned model as EntiGraph-29M."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:    QuALITY question-answering accuracy and recall rate in the open-book retrieval-augmented generation (RAG) setting.  EntiGraph CPT and Llama 3 8B Base are used in a RAG pipeline (cf.  5  for setup details).  Recall@ 8 8 8 8  is defined as the proportion of questions for which the salient article appears in the top  8 8 8 8  reranked document chunks.  GPT-4 and GPT-3.5 Oracle RAG provide an upper bound with a perfect retriever, by placing the entire relevant document in-context.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "In our main experiments ( 5 ), we use EntiGraph to generate 455M synthetic tokens from 1.3M real tokens using  gpt-4-turbo   (OpenAI et al.,  2024 ) .  Then, we continually pretrain Llama 3 8B  (Dubey et al.,  2024 )  on the synthetic tokens and evaluate its QA accuracy on the QuALITY question set.  We observe a log-linear scaling trend in the accuracy as the number of tokens increases, up to 455M synthetic tokens ( 4.2 ).  At the endpoint, we find that synthetic continued pretraining with 455M EntiGraph tokens provides 80% of the accuracy improvement of having those source documents available at inference time ( 5 ).  Beyond QA accuracy, we also perform instruction tuning on the continually pretrained model and find that it is capable of following open-ended instructions (e.g., summarization) related to the QuALITY books ( 4.3 ).",
            "We propose to learn from small corpora with  synthetic continued pretraining converting the small corpus into a large, diverse, synthetic corpus and continuing pretraining on itand instantiate this approach using the  EntiGraph  synthetic data augmentation algorithm ( 2.2 ).",
            "We demonstrate that continued pretraining on the EntiGraph-synthesized corpus yields a QA accuracy scaling trend that is log-linear in the synthetic token count, significantly outperforming continued pretraining on the original documents or paraphrases ( 4.2 ).  Furthermore, we show that instruction tuning the EntiGraph continually pretrained model enables it to follow more diverse queries related to the source documents ( 4.3 ).",
            "We focus on learning parametric knowledge from a small text corpus.  Our goal is to continually pretrain a language model to acquire the knowledge of a niche corpus of documents.  Observing that simple continued pretraining is ineffective ( 4 ), we propose to use synthetic continued pretraining, which first uses the small corpus to synthesize a larger one more amenable to learning, and then continues pretraining on the synthetic corpus.  In this section, we first outline this problem setting and our evaluation approach in more detail ( 2.1 ).  Then, we provide a concrete instantiation of synthetic continued pretraining using a data augmentation algorithm called EntiGraph ( 2.2 ).",
            "In this section, we describe in detail how we evaluate a given data augmentation algorithm  A synth subscript A synth \\mathcal{A}_{\\text{synth}} caligraphic_A start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT .  As described in the problem setup ( 2.1 ), we evaluate such algorithms  A synth subscript A synth \\mathcal{A}_{\\text{synth}} caligraphic_A start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT  by evaluating whether a language model continually pretrained on their output synthetic corpus  A synth  ( D source ) subscript A synth subscript D source \\mathcal{A}_{\\text{synth}}({\\mathcal{D}}_{\\text{source}}) caligraphic_A start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT )  can accurately answer test queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT  about the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT .",
            "In our main experiments, we use queries that are unambiguous even without the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT , and disallow the model from accessing  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  while answering the queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT  ( 2.1 ).  This allows us to evaluate which data augmentation algorithm best promotes the acquisition of parametric knowledge through synthetic CPT.  Later, in  5 , we consider an open-book setting where the model can access both the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  and test queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT  at the same time, in order to test how the parametric knowledge acquired through synthetic CPT composes with non-parametric access to knowledge through retrieval  (Lewis et al.,  2020 ) .",
            "In our continued pretraining setting, we must select a corpus  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  that is not already well-represented in standard pretraining datasets.  As an initial test of the obscurity of the QuALITY corpus  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT , we evaluate GPT-3.5  (Brown et al.,  2020 )  and GPT-4  (OpenAI et al.,  2024 )  on  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT .  In the closed-book setting, we find GPT-3.5 accuracy at 44.81% and GPT-4 accuracy at 51.30% (Figure  2 ).  In the open-book setting (full access to  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT ), we find GPT-3.5 accuracy at 72.60% and GPT-4 accuracy at 86.09% (Table  3 ).  Based on the large (  similar-to \\sim  30%) improvement when  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  is provided, we conclude that the QuALITY corpus  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  is sufficiently niche to serve as an appropriate testbed.",
            "We continually pretrain Llama 3 8B  (Dubey et al.,  2024 )  with standard causal language modeling on the 455M token EntiGraph corpus 3 3 3 Model  https://huggingface.co/zitongyang/llama-3-8b-entigraph-quality . .  In  4.1 , we describe our continued pretraining procedure and introduce two natural baselines.  In  4.2 , we evaluate all methods on the QuALITY test queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT .  In  4.3 , we show that synthetic CPT using EntiGraph is compatible with downstream instruction tuning  (Ouyang et al.,  2022 ) , an important feature of real pretraining data.",
            "In our main continued pretraining experiment, we continually pretrain Llama 3 8B Base on the 455M token EntiGraph corpus for 2 epochs with replay on RedPajama dataset  (TogetherAI,  2023 ) .  For the remainder of the work, we will refer to this continually pretrained model as EntiGraph CPT.  We provide details on continued pretraining setup in Appendix  C .  Next, we describe two baselines which we compare to EntiGraph CPT in closed-book QA ( 4.2 ).",
            "Another simple synthetic data augmentation procedure is to rephrase QuALITY articles many times.  As discussed in  1.1 ,  Maini et al. ( 2024 )  and  Ovadia et al. ( 2024 )  execute a systematic extension of this idea.  Based on their approaches, we craft three fixed prompts (easy, medium, and hard rephrase) and repeatedly apply them to the QuALITY articles at temperature 1.0 4 4 4 Note that  Maini et al. ( 2024 )  also includes a fourth prompt that generates synthetic QA pairs.  We defer this task-specific QA finetuning approach to Appendix  D  and focus on task-agnostic baselines that teach generic knowledge about QuALITY articles. .  We refer to this data augmentation algorithm as the Rephrase baseline.  We stopped generating paraphrases at 38M tokens, where we observed a clear gap in QA evaluations from EntiGraph CPT and a slower scaling trend (Figure  2 ).  We will refer to this data as the Rephrase corpus and the continually pretrained Llama 3 8B Base models as the Rephrase CPT.",
            "We find that continued pretraining on the 455M token EntiGraph corpus improves closed-book QA accuracy from 39.49% (for Llama 3 8B Base) to 56.22% (Figure  2 ).  A natural question is how performance scales as we synthesize and train on more tokens with EntiGraph.  To test this, we randomly subsample without replacement the EntiGraph corpus with varying sample sizes, continually pretrain Llama 3 8B Base on each subsample, and plot QuALITY accuracy with respect to sample size in Figure  2 .  We observe log-linear scaling of the accuracy in the number of synthetic tokens used for continued pretraining, up to 455M tokens.  We will mathematically investigate the scaling properties of EntiGraph in detail in  6 .  In broad strokes, we postulate that QuALITY accuracy follows a mixture-of-exponential shape and follows three stages: (i) linear growth, (ii) log-linear growth, and (iii) asymptotic plateau.",
            "Raw CPT performs even worse than Llama 3 8B Base (dashed black line in Figure  2 ).  We postulate two reasons for this:  (i) The Raw corpus follows a narrower, different distribution than the Llama 3 8B pretraining corpus, and heavily training on these tokens may harm the overall English capabilities of the model.  (ii) The limited diversity of knowledge representations in the Raw corpus leads to limited knowledge acquisition due to problems such as the reversal curse  (Berglund et al.,  2023 ) .  Rephrase CPT scales poorly compared with EntiGraph (Figure  2 ), suggesting that for synthetic CPT to scale, the synthetic data must be sufficiently diverse.  EntiGraph tackles this problem using a hierarchical prompting strategy, which externalizes diversity to the combinatorial relationships encoded in entity knowledge graphs.",
            "We first present a few qualitative examples to demonstrate EntiGraph Instructs ability to follow instructions related to QuALITY articles.  As a first test, we ask the model to summarize a QuALITY article given an explicit reference to the title and author, but no access to the article itself (Table  2 , top row).  This article provides context for the coming examples.  Next, we show that even without an explicit reference to the title and author, knowledge of the article is stored in the models parameters and can affect its behavior (Table  2 , middle row).  Finally, we provide an example where the model performs a comparison using knowledge across two articles (Table  2 , bottom row).  Albeit artificial, this shows that even though EntiGraph does not synthesize data that simultaneously involves multiple articles, the model can reason about their interaction using its parametric knowledge.  We provide the full responses in Table  5 .",
            "We use a simple, automated evaluation metric based on pyramid evaluation  (Nenkova et al.,  2007 ; Gao et al.,  2019 )  that measures both the hallucination rate and how well the summary captures the salient claims of the original article.  Our approach uses GPT-4 to (1) split the summary into atomic claims  (Min et al.,  2023 ) , (2) decide whether each claim is true/false based on the source article, and (3) determine if true claims are salient to the articles main message.  We hence obtain the count of false and salient claims for each summary, normalize these by the corresponding count from the human summary, and report the average of these normalized metrics in Figure  3 .  Appendix  H.2  provides further details.",
            "In Figure  3 , we compare four summarizers: EntiGraph Instruct, Raw Instruct, GPT-3.5, and GPT-4.  We provide each summarizer with two different promptsasking for progressively more detailed summaries.  We provide exact prompts in Appendix  H.2 , as well as a smaller-scale token-matched comparison to Rephrase CPT in Appendix  H.3 , where we find EntiGraph CPT has consistently lower false claims relative to Rephrase CPT.  As we request more detailed summaries, Raw Instruct consistently hallucinates and generates more false claims with little improvement in the number of salient claims.  In contrast, EntiGraph Instruct can generate more salient claims as the summary gets longer, with a small increase in the number of false claims (similar to GPT-3.5 and GPT-4 levels).  The gaps in both salient and false claim rates are sufficiently large that these results likely hold beyond our particular metric.  We complement the automated evaluation metrics above with several qualitative examples in Appendix  H.2 .",
            "These results also contextualize the effectiveness of EntiGraph in the closed-book, parametric knowledge setting ( 4 ).  Comparing Figure  2  and Table  3 , we observe that adding RAG to Llama 3 8B Base improves accuracy by  20.86 % percent 20.86 20.86\\% 20.86 %  ( 39.49 %  60.35 %  percent 39.49 percent 60.35 39.49\\%\\rightarrow 60.35\\% 39.49 %  60.35 % ).  On the other hand, continued pretraining of Llama 3 8B Base on the EntiGraph corpus improves accuracy by  16.73 % percent 16.73 16.73\\% 16.73 %  ( 39.49 %  56.22 %  percent 39.49 percent 56.22 39.49\\%\\rightarrow 56.22\\% 39.49 %  56.22 % ).  Hence, EntiGraph continued pretraining provides  > 80 % absent percent 80 >\\!80\\% > 80 %  of the absolute performance improvement of RAG, even in a small corpus setting where RAG recall is nearly perfect.",
            "This mirrors the relation analysis step for the EntiGraph synthetic data augmentation algorithm (introduced in  2.2 ).  With the setup above, the index  t t t italic_t  is analogous to the number of synthetic tokens that the model has generated, and the models knowledge is captured by how many ones the matrix  M t subscript M t {\\bm{M}}_{t} bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  contains.  To make this connection precise, we define the link density (or accuracy) of  M t subscript M t {\\bm{M}}_{t} bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  to be",
            "In this context, the parameter  C C C italic_C  governs the link density  Acc  ( M t ) Acc subscript M t \\mathsf{Acc}({\\bm{M}}_{t}) sansserif_Acc ( bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  as  t    t t\\to\\infty italic_t   .  In our model,  C C C italic_C  is determined by the proportion of reachable pairs of vertices in the initial matrix  M 0 subscript M 0 {\\bm{M}}_{0} bold_italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .  Here, we are essentially filling out the deductive closure (i.e., all the facts or relations that can be deduced from  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT ;  Stine ( 1976 ); Akyurek et al. ( 2024 ) ) of the original dataif some facts cannot be deduced, then  Acc  ( M t ) Acc subscript M t \\mathsf{Acc}({\\bm{M}}_{t}) sansserif_Acc ( bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  cannot approach  1 1 1 1 .  The measure    (  )   \\mu(\\cdot) italic_ (  )  is the probability mass function on  k k k italic_k , which controls the proportion of pairs of vertices with a specific decay rate. The parameters    (  )   \\mu(\\cdot) italic_ (  )  depend on  M 0 subscript M 0 {\\bm{M}}_{0} bold_italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  in a more intricate manner.  We find that the formula in ( 2 ) accurately fits the empirical scaling trend of EntiGraph CPT accuracy up to 455M synthetic tokens (Figure  4 ).",
            "We provide additional details on the QuALITY dataset below.  For each book, we execute entity extraction (Step 1,  2.2 ) and then analyze all pair-wise relations between entities and a subset of all triplet relations (Step 2,  2.2 ).  We provide summary statistics for the Raw and EntiGraph corpora in Figure  6 .",
            "We follow the same set as in  2.1  and  3  except that we do not prompt  LM synth subscript LM synth \\mathsf{LM}_{\\text{synth}} sansserif_LM start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT  to generate general knowledge about QuALTY articles.  Instead, we prompt  LM synth subscript LM synth \\mathsf{LM}_{\\text{synth}} sansserif_LM start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT  to generate QA pairs directly:",
            "With Lemma  F.1  and Lemma  F.2 , we can now give useful estimates of  | R | R |{\\mathcal{R}}| | caligraphic_R | .  In particular, for any   > 0  0 \\varepsilon>0 italic_ > 0 ,",
            "We generate two synthetic corpora in this paper: EntiGraph (Appendix  G.1 ) and the Rephrase baseline (Appendix  G.2 ).  In our experiments, the  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  is a collection of documents  D D D italic_D , and our synthetic augmentation procedure is applied to each document  D  D source D subscript D source D\\in{\\mathcal{D}}_{\\text{source}} italic_D  caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT .  We will focus on a single document  D D D italic_D  for the remainder of this section.",
            "The EntiGraph procedure is described in detail in  2.2 .  We will recap the three steps below.",
            "The first step is to extract the salient entities from the document  D D D italic_D  using the  entity_extraction  operation (Step 1,  2.2 ).  The complete  entity_extraction  prompt is as follows:"
        ]
    },
    "id_table_3": {
        "caption": "Table 4:  Summarization prompt for EntiGraph Instruct, Raw Instruct, and Reprhase Instruct.",
        "table": "S5.T3.4.4",
        "footnotes": [],
        "references": [
            "In our main experiments ( 5 ), we use EntiGraph to generate 455M synthetic tokens from 1.3M real tokens using  gpt-4-turbo   (OpenAI et al.,  2024 ) .  Then, we continually pretrain Llama 3 8B  (Dubey et al.,  2024 )  on the synthetic tokens and evaluate its QA accuracy on the QuALITY question set.  We observe a log-linear scaling trend in the accuracy as the number of tokens increases, up to 455M synthetic tokens ( 4.2 ).  At the endpoint, we find that synthetic continued pretraining with 455M EntiGraph tokens provides 80% of the accuracy improvement of having those source documents available at inference time ( 5 ).  Beyond QA accuracy, we also perform instruction tuning on the continually pretrained model and find that it is capable of following open-ended instructions (e.g., summarization) related to the QuALITY books ( 4.3 ).",
            "We demonstrate that continued pretraining on the EntiGraph-synthesized corpus yields a QA accuracy scaling trend that is log-linear in the synthetic token count, significantly outperforming continued pretraining on the original documents or paraphrases ( 4.2 ).  Furthermore, we show that instruction tuning the EntiGraph continually pretrained model enables it to follow more diverse queries related to the source documents ( 4.3 ).",
            "In our continued pretraining setting, we must select a corpus  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  that is not already well-represented in standard pretraining datasets.  As an initial test of the obscurity of the QuALITY corpus  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT , we evaluate GPT-3.5  (Brown et al.,  2020 )  and GPT-4  (OpenAI et al.,  2024 )  on  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT .  In the closed-book setting, we find GPT-3.5 accuracy at 44.81% and GPT-4 accuracy at 51.30% (Figure  2 ).  In the open-book setting (full access to  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT ), we find GPT-3.5 accuracy at 72.60% and GPT-4 accuracy at 86.09% (Table  3 ).  Based on the large (  similar-to \\sim  30%) improvement when  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  is provided, we conclude that the QuALITY corpus  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  is sufficiently niche to serve as an appropriate testbed.",
            "We continually pretrain Llama 3 8B  (Dubey et al.,  2024 )  with standard causal language modeling on the 455M token EntiGraph corpus 3 3 3 Model  https://huggingface.co/zitongyang/llama-3-8b-entigraph-quality . .  In  4.1 , we describe our continued pretraining procedure and introduce two natural baselines.  In  4.2 , we evaluate all methods on the QuALITY test queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT .  In  4.3 , we show that synthetic CPT using EntiGraph is compatible with downstream instruction tuning  (Ouyang et al.,  2022 ) , an important feature of real pretraining data.",
            "The first natural baseline is to continually pretrain Llama 3 8B Base on the 1.3M token Raw corpus (the raw QuALITY articles  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT , defined in  3 ).  We jointly tune the number of epochs and RedPajama replay rate, and refer to this continually pretrained model as Raw CPT.  Further tuning details are provided in Appendix  C .",
            "We use a simple, automated evaluation metric based on pyramid evaluation  (Nenkova et al.,  2007 ; Gao et al.,  2019 )  that measures both the hallucination rate and how well the summary captures the salient claims of the original article.  Our approach uses GPT-4 to (1) split the summary into atomic claims  (Min et al.,  2023 ) , (2) decide whether each claim is true/false based on the source article, and (3) determine if true claims are salient to the articles main message.  We hence obtain the count of false and salient claims for each summary, normalize these by the corresponding count from the human summary, and report the average of these normalized metrics in Figure  3 .  Appendix  H.2  provides further details.",
            "In Figure  3 , we compare four summarizers: EntiGraph Instruct, Raw Instruct, GPT-3.5, and GPT-4.  We provide each summarizer with two different promptsasking for progressively more detailed summaries.  We provide exact prompts in Appendix  H.2 , as well as a smaller-scale token-matched comparison to Rephrase CPT in Appendix  H.3 , where we find EntiGraph CPT has consistently lower false claims relative to Rephrase CPT.  As we request more detailed summaries, Raw Instruct consistently hallucinates and generates more false claims with little improvement in the number of salient claims.  In contrast, EntiGraph Instruct can generate more salient claims as the summary gets longer, with a small increase in the number of false claims (similar to GPT-3.5 and GPT-4 levels).  The gaps in both salient and false claim rates are sufficiently large that these results likely hold beyond our particular metric.  We complement the automated evaluation metrics above with several qualitative examples in Appendix  H.2 .",
            "We observe in Table  3  that EntiGraph CPT outperforms Llama 3 8B Base, the model from which it is continually pretrained.  These results demonstrate that the knowledge internalized through synthetic CPT is complementary to that accessed during RAG, and demonstrate a competitive new recipe for small corpus QA: (1) synthetic data augmentation, (2) continued pretraining, and (3) RAG.",
            "These results also contextualize the effectiveness of EntiGraph in the closed-book, parametric knowledge setting ( 4 ).  Comparing Figure  2  and Table  3 , we observe that adding RAG to Llama 3 8B Base improves accuracy by  20.86 % percent 20.86 20.86\\% 20.86 %  ( 39.49 %  60.35 %  percent 39.49 percent 60.35 39.49\\%\\rightarrow 60.35\\% 39.49 %  60.35 % ).  On the other hand, continued pretraining of Llama 3 8B Base on the EntiGraph corpus improves accuracy by  16.73 % percent 16.73 16.73\\% 16.73 %  ( 39.49 %  56.22 %  percent 39.49 percent 56.22 39.49\\%\\rightarrow 56.22\\% 39.49 %  56.22 % ).  Hence, EntiGraph continued pretraining provides  > 80 % absent percent 80 >\\!80\\% > 80 %  of the absolute performance improvement of RAG, even in a small corpus setting where RAG recall is nearly perfect.",
            "It may seem surprising that simply rewriting the factual content of the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  can improve performance at all ( 4 ), as the EntiGraph data augmentation algorithm does not explicitly add new factual information beyond  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT .  In this section, we build a mathematical model based on a stochastic process on graphs to offer an explanation for this phenomenon.  We postulate that EntiGraph does not create knowledge  de novo ; rather, it simply rearranges the knowledge of  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  into a layout more amenable to learning.  For example, in  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT , the entity pair  ( A , B ) A B (A,B) ( italic_A , italic_B )  may appear together in some sentences and  ( B , C ) B C (B,C) ( italic_B , italic_C )  in others.  As a result, models trained directly on  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  with a next-token prediction objective may learn the  ( A , B ) A B (A,B) ( italic_A , italic_B )  relation and the  ( B , C ) B C (B,C) ( italic_B , italic_C )  relation, but not the relation between  A A A italic_A  and  C C C italic_C   (Akyurek et al.,  2024 ) .  We will build a mathematical model that formalizes this intuition ( 6.1 ).  Based on this model, we provide a quantitative prediction that the scaling trend of EntiGraph CPT follows a mixture-of-exponential shape ( 6.3 ), which fits well with our empirically observed scaling trend (Figure  4 ).",
            "In this section, we derive rigorous upper and lower bounds on the scaling trend of  Acc  ( M t ) Acc subscript M t \\mathsf{Acc}({\\bm{M}}_{t}) sansserif_Acc ( bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) . We show that  Acc  ( M t ) Acc subscript M t \\mathsf{Acc}({\\bm{M}}_{t}) sansserif_Acc ( bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  as a function of  t t t italic_t  can be bounded above and below by two exponential functions with different growth rates.  Note that these two bounds do not necessarily imply that  Acc  ( M t ) Acc subscript M t \\mathsf{Acc}({\\bm{M}}_{t}) sansserif_Acc ( bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  itself grows exponentially. We will provide a precise formula for its growth in  6.3  via an approximation through a Poisson branching process.",
            "We follow the same set as in  2.1  and  3  except that we do not prompt  LM synth subscript LM synth \\mathsf{LM}_{\\text{synth}} sansserif_LM start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT  to generate general knowledge about QuALTY articles.  Instead, we prompt  LM synth subscript LM synth \\mathsf{LM}_{\\text{synth}} sansserif_LM start_POSTSUBSCRIPT synth end_POSTSUBSCRIPT  to generate QA pairs directly:",
            "At inference time, the RAG system receives a test query  q  Q test q subscript Q test q\\in{\\mathcal{Q}}_{\\text{test}} italic_q  caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT .  Each query  q q q italic_q  is contextualized with the article title and author name, as described in  3 , and contains its four possible answer choices (QuALITY is a 4-choice, multiple choice dataset).  In Stage 2, we embed the query with the API-based embedding model, retrieve  K K K italic_K  document chunks using an approximate nearest-neighbor search, and lastly, select the  k < K k K k<K italic_k < italic_K  most relevant chunks using an API-based reranker.",
            "Next, we use a reranker to filter the  K K K italic_K  retrieved document chunks to a smaller number of reranked chunks  k k k italic_k .  Rerankers are known to significantly improve recall (the proportion of the time that the salient article is contained in the top chunks), and indeed, the recall of our RAG pipelines is near-perfect (Table  3  in  5 ).  Specifically, we pass the query  q q q italic_q  and the list of  K K K italic_K  retrieved document chunks to a state-of-the-art rerankerCohere  rerank-english-v3.0   (Cohere,  2024 ) which returns a list of the  K K K italic_K  chunks in order from most to least semantically relevant for the query.  We take the  k k k italic_k  highest scoring chunks and place them in our few-shot prompt.",
            "where  ( a , z 1 , z 1 ,  , z k , b ) a superscript z 1 superscript z 1  superscript z k b (a,z^{1},z^{1},\\cdots,z^{k},b) ( italic_a , italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT , italic_z start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT ,  , italic_z start_POSTSUPERSCRIPT italic_k end_POSTSUPERSCRIPT , italic_b )  represents the shortest path in  M 0 subscript M 0 {\\bm{M}}_{0} bold_italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  connecting  a a a italic_a  and  b b b italic_b . Equivalently, if we consider the tree generated by a breadth-first search in  M 0 subscript M 0 {\\bm{M}}_{0} bold_italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  rooted at  i i i italic_i , then since  i  j similar-to i j i\\sim j italic_i  italic_j ,  j j j italic_j  will be in the tree, and the numerator counts the total number of offspring of  j j j italic_j  in the tree, including  j j j italic_j  itself. This is the point at which a rigorous mathematical characterization of the tree becomes challenging. Instead, we approximate the tree and analyze its behavior. It is well-known that when  p =  / V p  V p=\\lambda/V italic_p = italic_ / italic_V , the cluster growth (or the breadth-first search at a vertex) can be approximated by a Poisson (  )  (\\lambda) ( italic_ )  branching process (see e.g.,  Hofstad ( 2016 ); Durrett ( 2010 ) ).  For fixed vertex  i i i italic_i , we define  T T T italic_T  as a GaltonWatson tree rooted at  i i i italic_i  with Poisson (  )  (\\lambda) ( italic_ )  offspring distribution with depth  L L L italic_L . We use  T T T italic_T  to approximate the exploration process at  i i i italic_i . For  0  l  L 0 l L 0\\leq\\ell\\leq L 0  roman_l  italic_L , the number of vertices at level  L  l L l L-\\ell italic_L - roman_l  is approximately   L  l superscript  L l \\lambda^{L-\\ell} italic_ start_POSTSUPERSCRIPT italic_L - roman_l end_POSTSUPERSCRIPT . Given that the total number of vertices in  T T T italic_T  is approximately  ( 1    (  ) )  V 1   V (1-\\rho(\\lambda))V ( 1 - italic_ ( italic_ ) ) italic_V , the number of vertices at level  L  l L l L-\\ell italic_L - roman_l  is also  ( 1    (  ) )  V  (   1 ) /  l + 1 1   V  1 superscript  l 1 (1-\\rho(\\lambda))V(\\lambda-1)/\\lambda^{\\ell+1} ( 1 - italic_ ( italic_ ) ) italic_V ( italic_ - 1 ) / italic_ start_POSTSUPERSCRIPT roman_l + 1 end_POSTSUPERSCRIPT . For each vertex at level  L  l L l L-\\ell italic_L - roman_l , the number of its offspring (including itself) equals  k k k italic_k  with probability  p l  ( k ) subscript p l k p_{\\ell}(k) italic_p start_POSTSUBSCRIPT roman_l end_POSTSUBSCRIPT ( italic_k ) . In this case, the numerator in ( 3 ) equals  k k k italic_k . Combining the above, there are around  ( 1    (  ) )  V  p l  ( k )  ( 1    (  ) )  V  (   1 ) /  l + 1  1   V subscript p l k 1   V  1 superscript  l 1 (1-\\rho(\\lambda))V\\cdot p_{\\ell}(k)(1-\\rho(\\lambda))V(\\lambda-1)/\\lambda^{\\ell%  +1} ( 1 - italic_ ( italic_ ) ) italic_V  italic_p start_POSTSUBSCRIPT roman_l end_POSTSUBSCRIPT ( italic_k ) ( 1 - italic_ ( italic_ ) ) italic_V ( italic_ - 1 ) / italic_ start_POSTSUPERSCRIPT roman_l + 1 end_POSTSUPERSCRIPT  vertex pairs  ( i , j ) i j (i,j) ( italic_i , italic_j )  in the graph such that  i  L X i subscript L X i\\in{\\mathcal{L}}_{X} italic_i  caligraphic_L start_POSTSUBSCRIPT italic_X end_POSTSUBSCRIPT ,  j  L Y j subscript L Y j\\in{\\mathcal{L}}_{Y} italic_j  caligraphic_L start_POSTSUBSCRIPT italic_Y end_POSTSUBSCRIPT ,  q i , j = k / V  ( V  1 ) subscript q i j k V V 1 q_{i,j}=k/V(V-1) italic_q start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT = italic_k / italic_V ( italic_V - 1 )  and  j j j italic_j  is located at the  L  l L l L-\\ell italic_L - roman_l  level in the tree  T T T italic_T . Ultimately, we arrive at an approximation of the form",
            "We design a three-stage evaluation procedure:  (i) In the first stage, we use GPT-4 6 6 6 Specifically, we use the  gpt-4-turbo  model as of Aug. 19, 2024.  to break the summary into atomic claims, similar to  Min et al. ( 2023 ) ;  (ii) In the second stage, we provide both the list of claims and the source article to a judge model (also GPT-4).  We ask the judge model to determine whether each claim is true or false, based on the source article.  If the claim is true, we further ask the model to determine whether the claim is salient (contributes to the main message of the article) or cosmetic (factual details that do not help understand the main message).  (iii) Finally, for each summary, we obtain its number of false and salient claims and normalize it by the corresponding count from the human summary.  We report the average of these normalized metrics across the QuALITY corpus articles in Figure  3 .",
            "Concretely, we apply the same instruction procedure described in  4.3  to the Raw CPT and Rephrase CPT models from  4.1 , obtaining two additional instruction-tuned models that have knowledge about QuALITY books.  In addition, we also subsample 29M tokens out of the 455M token EntiGraph corpus to token-match the Raw and Rephrase corpus, and refer to the corresponding instruction tuned model as EntiGraph-29M."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:  Complete instruction following example used in Table  2  from Section  4.3 .",
        "table": "A8.T4.1",
        "footnotes": [],
        "references": [
            "In our main experiments ( 5 ), we use EntiGraph to generate 455M synthetic tokens from 1.3M real tokens using  gpt-4-turbo   (OpenAI et al.,  2024 ) .  Then, we continually pretrain Llama 3 8B  (Dubey et al.,  2024 )  on the synthetic tokens and evaluate its QA accuracy on the QuALITY question set.  We observe a log-linear scaling trend in the accuracy as the number of tokens increases, up to 455M synthetic tokens ( 4.2 ).  At the endpoint, we find that synthetic continued pretraining with 455M EntiGraph tokens provides 80% of the accuracy improvement of having those source documents available at inference time ( 5 ).  Beyond QA accuracy, we also perform instruction tuning on the continually pretrained model and find that it is capable of following open-ended instructions (e.g., summarization) related to the QuALITY books ( 4.3 ).",
            "We demonstrate that continued pretraining on the EntiGraph-synthesized corpus yields a QA accuracy scaling trend that is log-linear in the synthetic token count, significantly outperforming continued pretraining on the original documents or paraphrases ( 4.2 ).  Furthermore, we show that instruction tuning the EntiGraph continually pretrained model enables it to follow more diverse queries related to the source documents ( 4.3 ).",
            "Recent approaches synthesize  pretraining  data using hierarchical prompting methods to promote dataset diversity.   Eldan & Li ( 2023 )  prompt API-based LLMs to generate childrens stories containing sampled keywords, and demonstrate that even small language models trained on their dataset can generate fluent text.   Gunasekar et al. ( 2023 )  synthesize a diverse dataset of textbooks and code exercises by conditioning on topic, target audience, and function names, and later release strong LLMs pretrained on synthetic data in follow-up work  (Li et al.,  2023b ; Abdin et al.,  2023 ;  2024 ) .  However, their datasets and prompts are not publicly available.   Maini et al. ( 2024 )  prompt an LM to rephrase documents for pretraining, improving training efficiency.  Different from all above works, our focus is teaching a pretrained LLM the knowledge of a small corpus.   Mecklenburg et al. ( 2024 )  consider task-specific finetuning and propose a fact-based synthetic QA generation procedure, but do not show improvement on generic instruction following tasks beyond simple QA.  We instead focus on teaching a model generally useful knowledge about a small corpus, untied to a particular downstream task.   Ovadia et al. ( 2024 )  continually pretrain Llama 2based language models on synthetic paraphrases of Wikipedia articles, but do not observe consistent performance improvements.  We adapt the approach of  Maini et al. ( 2024 )  and  Mecklenburg et al. ( 2024 )  to our small corpus setting as the Rephrase baseline in  4 .  We find that our graph-based augmentation algorithm outperforms it, likely because our approach enforces diversity through entity-based generation.",
            "We focus on learning parametric knowledge from a small text corpus.  Our goal is to continually pretrain a language model to acquire the knowledge of a niche corpus of documents.  Observing that simple continued pretraining is ineffective ( 4 ), we propose to use synthetic continued pretraining, which first uses the small corpus to synthesize a larger one more amenable to learning, and then continues pretraining on the synthetic corpus.  In this section, we first outline this problem setting and our evaluation approach in more detail ( 2.1 ).  Then, we provide a concrete instantiation of synthetic continued pretraining using a data augmentation algorithm called EntiGraph ( 2.2 ).",
            "We continually pretrain Llama 3 8B  (Dubey et al.,  2024 )  with standard causal language modeling on the 455M token EntiGraph corpus 3 3 3 Model  https://huggingface.co/zitongyang/llama-3-8b-entigraph-quality . .  In  4.1 , we describe our continued pretraining procedure and introduce two natural baselines.  In  4.2 , we evaluate all methods on the QuALITY test queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT .  In  4.3 , we show that synthetic CPT using EntiGraph is compatible with downstream instruction tuning  (Ouyang et al.,  2022 ) , an important feature of real pretraining data.",
            "In our main continued pretraining experiment, we continually pretrain Llama 3 8B Base on the 455M token EntiGraph corpus for 2 epochs with replay on RedPajama dataset  (TogetherAI,  2023 ) .  For the remainder of the work, we will refer to this continually pretrained model as EntiGraph CPT.  We provide details on continued pretraining setup in Appendix  C .  Next, we describe two baselines which we compare to EntiGraph CPT in closed-book QA ( 4.2 ).",
            "Our RAG pipeline follows established best practices  (Lewis et al.,  2020 ; Gao et al.,  2024 ) .  It involves an offline stage which indexes document chunks, followed by inference-time retrieval, reranking, and placement of those chunks in a few-shot LM prompt.  Throughout, we use OpenAI  text-embedding-3-large   (Neelakantan et al.,  2022 )  as our API-based embedding model, FAISS as our similarity search index  (Douze et al.,  2024 ) , and Cohere  rerank-english-v3.0   (Cohere,  2024 )  as our reranker.  Following the evaluation procedure detailed in  4 , we evaluate parallel RAG pipelines on the QuALITY multiple choice test set using few-shot chain-of-thought prompting.  All hyperparameters are tuned separately for each LMs RAG pipeline.  We refer the reader to Appendix  E  for further details on our RAG evaluation setup.",
            "These results also contextualize the effectiveness of EntiGraph in the closed-book, parametric knowledge setting ( 4 ).  Comparing Figure  2  and Table  3 , we observe that adding RAG to Llama 3 8B Base improves accuracy by  20.86 % percent 20.86 20.86\\% 20.86 %  ( 39.49 %  60.35 %  percent 39.49 percent 60.35 39.49\\%\\rightarrow 60.35\\% 39.49 %  60.35 % ).  On the other hand, continued pretraining of Llama 3 8B Base on the EntiGraph corpus improves accuracy by  16.73 % percent 16.73 16.73\\% 16.73 %  ( 39.49 %  56.22 %  percent 39.49 percent 56.22 39.49\\%\\rightarrow 56.22\\% 39.49 %  56.22 % ).  Hence, EntiGraph continued pretraining provides  > 80 % absent percent 80 >\\!80\\% > 80 %  of the absolute performance improvement of RAG, even in a small corpus setting where RAG recall is nearly perfect.",
            "It may seem surprising that simply rewriting the factual content of the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  can improve performance at all ( 4 ), as the EntiGraph data augmentation algorithm does not explicitly add new factual information beyond  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT .  In this section, we build a mathematical model based on a stochastic process on graphs to offer an explanation for this phenomenon.  We postulate that EntiGraph does not create knowledge  de novo ; rather, it simply rearranges the knowledge of  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  into a layout more amenable to learning.  For example, in  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT , the entity pair  ( A , B ) A B (A,B) ( italic_A , italic_B )  may appear together in some sentences and  ( B , C ) B C (B,C) ( italic_B , italic_C )  in others.  As a result, models trained directly on  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  with a next-token prediction objective may learn the  ( A , B ) A B (A,B) ( italic_A , italic_B )  relation and the  ( B , C ) B C (B,C) ( italic_B , italic_C )  relation, but not the relation between  A A A italic_A  and  C C C italic_C   (Akyurek et al.,  2024 ) .  We will build a mathematical model that formalizes this intuition ( 6.1 ).  Based on this model, we provide a quantitative prediction that the scaling trend of EntiGraph CPT follows a mixture-of-exponential shape ( 6.3 ), which fits well with our empirically observed scaling trend (Figure  4 ).",
            "where the expectation is taken over the randomness arising from the synthetic data generation process and not the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT . For a matrix  M M M italic_M , we use   M  1 subscript norm M 1 \\|M\\|_{1}  italic_M  start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  to denote   i , j | M i , j | subscript i j subscript M i j \\sum_{i,j}|M_{i,j}|  start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT | italic_M start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT | . We use the notation  Acc Acc \\mathsf{Acc} sansserif_Acc  as this is intended to emulate the accuracy on QuALITY test queries studied in the experimental sections ( 4  and  5 ).",
            "In this context, the parameter  C C C italic_C  governs the link density  Acc  ( M t ) Acc subscript M t \\mathsf{Acc}({\\bm{M}}_{t}) sansserif_Acc ( bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  as  t    t t\\to\\infty italic_t   .  In our model,  C C C italic_C  is determined by the proportion of reachable pairs of vertices in the initial matrix  M 0 subscript M 0 {\\bm{M}}_{0} bold_italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT .  Here, we are essentially filling out the deductive closure (i.e., all the facts or relations that can be deduced from  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT ;  Stine ( 1976 ); Akyurek et al. ( 2024 ) ) of the original dataif some facts cannot be deduced, then  Acc  ( M t ) Acc subscript M t \\mathsf{Acc}({\\bm{M}}_{t}) sansserif_Acc ( bold_italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT )  cannot approach  1 1 1 1 .  The measure    (  )   \\mu(\\cdot) italic_ (  )  is the probability mass function on  k k k italic_k , which controls the proportion of pairs of vertices with a specific decay rate. The parameters    (  )   \\mu(\\cdot) italic_ (  )  depend on  M 0 subscript M 0 {\\bm{M}}_{0} bold_italic_M start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  in a more intricate manner.  We find that the formula in ( 2 ) accurately fits the empirical scaling trend of EntiGraph CPT accuracy up to 455M synthetic tokens (Figure  4 ).",
            "We repeat this prompt many times at temperature 1.0, resulting in 28M tokens on synthetic question answer pairs.  We perform the same continued pretraining procedure in  4.1  on Llama 3 8B and refer to this model as QA SFT.",
            "Concretely, we apply the same instruction procedure described in  4.3  to the Raw CPT and Rephrase CPT models from  4.1 , obtaining two additional instruction-tuned models that have knowledge about QuALITY books.  In addition, we also subsample 29M tokens out of the 455M token EntiGraph corpus to token-match the Raw and Rephrase corpus, and refer to the corresponding instruction tuned model as EntiGraph-29M."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A8.T5.1",
        "footnotes": [],
        "references": [
            "In our main experiments ( 5 ), we use EntiGraph to generate 455M synthetic tokens from 1.3M real tokens using  gpt-4-turbo   (OpenAI et al.,  2024 ) .  Then, we continually pretrain Llama 3 8B  (Dubey et al.,  2024 )  on the synthetic tokens and evaluate its QA accuracy on the QuALITY question set.  We observe a log-linear scaling trend in the accuracy as the number of tokens increases, up to 455M synthetic tokens ( 4.2 ).  At the endpoint, we find that synthetic continued pretraining with 455M EntiGraph tokens provides 80% of the accuracy improvement of having those source documents available at inference time ( 5 ).  Beyond QA accuracy, we also perform instruction tuning on the continually pretrained model and find that it is capable of following open-ended instructions (e.g., summarization) related to the QuALITY books ( 4.3 ).",
            "We complement the main experiments with an open-book setup ( 5 ), providing the model with access to the source documents when answering queries.  We demonstrate that the knowledge acquired through synthetic continued pretraining with EntiGraph is  complementary  to the knowledge accessed through retrieval-augmented generation (RAG,  Lewis et al. ( 2020 ) )RAG with the EntiGraph continually pretrained model outperforms RAG with the base model.",
            "In our main experiments, we use queries that are unambiguous even without the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT , and disallow the model from accessing  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  while answering the queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT  ( 2.1 ).  This allows us to evaluate which data augmentation algorithm best promotes the acquisition of parametric knowledge through synthetic CPT.  Later, in  5 , we consider an open-book setting where the model can access both the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT  and test queries  Q test subscript Q test {\\mathcal{Q}}_{\\text{test}} caligraphic_Q start_POSTSUBSCRIPT test end_POSTSUBSCRIPT  at the same time, in order to test how the parametric knowledge acquired through synthetic CPT composes with non-parametric access to knowledge through retrieval  (Lewis et al.,  2020 ) .",
            "We first present a few qualitative examples to demonstrate EntiGraph Instructs ability to follow instructions related to QuALITY articles.  As a first test, we ask the model to summarize a QuALITY article given an explicit reference to the title and author, but no access to the article itself (Table  2 , top row).  This article provides context for the coming examples.  Next, we show that even without an explicit reference to the title and author, knowledge of the article is stored in the models parameters and can affect its behavior (Table  2 , middle row).  Finally, we provide an example where the model performs a comparison using knowledge across two articles (Table  2 , bottom row).  Albeit artificial, this shows that even though EntiGraph does not synthesize data that simultaneously involves multiple articles, the model can reason about their interaction using its parametric knowledge.  We provide the full responses in Table  5 .",
            "where the expectation is taken over the randomness arising from the synthetic data generation process and not the source documents  D source subscript D source {\\mathcal{D}}_{\\text{source}} caligraphic_D start_POSTSUBSCRIPT source end_POSTSUBSCRIPT . For a matrix  M M M italic_M , we use   M  1 subscript norm M 1 \\|M\\|_{1}  italic_M  start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  to denote   i , j | M i , j | subscript i j subscript M i j \\sum_{i,j}|M_{i,j}|  start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT | italic_M start_POSTSUBSCRIPT italic_i , italic_j end_POSTSUBSCRIPT | . We use the notation  Acc Acc \\mathsf{Acc} sansserif_Acc  as this is intended to emulate the accuracy on QuALITY test queries studied in the experimental sections ( 4  and  5 ).",
            "where we use a convenient change of variable  T = t  V  ( V  1 ) T t V V 1 T=tV(V-1) italic_T = italic_t italic_V ( italic_V - 1 ) .  It is important to note that the choice of  log  t t \\log t roman_log italic_t  in the second phase is not necessarily canonical.  In fact, the bound holds for any well-behaved monotone increasing concave function as a replacement for  log  t t \\log t roman_log italic_t .  Our representation here is motivated by two factors: first, it aligns with the performance observed in our EntiGraph CPT numerical results, and second, it reflects the gradual slowdown in growth. We illustrate the three phases in Figure  5 , which present a simulation of the toy model with  p = 0.03 p 0.03 p=0.03 italic_p = 0.03 .",
            "We provide additional details on our open-book experimental setup below, including our retrieval-augmented generation (RAG,  Lewis et al. ( 2020 ); Gao et al. ( 2024 ) ) pipeline.  As mentioned in  5 , we use a standard two-stage RAG pipeline: first, an offline stage which indexes document chunks; second, inference-time retrieval, reranking, and placement of those chunks in a few-shot LM prompt.",
            "Next, we use a reranker to filter the  K K K italic_K  retrieved document chunks to a smaller number of reranked chunks  k k k italic_k .  Rerankers are known to significantly improve recall (the proportion of the time that the salient article is contained in the top chunks), and indeed, the recall of our RAG pipelines is near-perfect (Table  3  in  5 ).  Specifically, we pass the query  q q q italic_q  and the list of  K K K italic_K  retrieved document chunks to a state-of-the-art rerankerCohere  rerank-english-v3.0   (Cohere,  2024 ) which returns a list of the  K K K italic_K  chunks in order from most to least semantically relevant for the query.  We take the  k k k italic_k  highest scoring chunks and place them in our few-shot prompt."
        ]
    },
    "global_footnotes": [
        "Code",
        ".",
        "Data",
        ".",
        "Model",
        ".",
        "Note that",
        "also includes a fourth prompt that generates synthetic QA pairs.  We defer this task-specific QA finetuning approach to Appendix",
        "and focus on task-agnostic baselines that teach generic knowledge about QuALITY articles.",
        "OpenAI API pricing, Sep 2024",
        "Specifically, we use the",
        "model as of Aug. 19, 2024."
    ]
}