{
    "PAPER'S NUMBER OF TABLES": 4,
    "S5.T1": {
        "caption": "TABLE I: Datasets, Models, and Learning Configurations.",
        "table": "<table id=\"S5.T1.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"S5.T1.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Distribution</span></th>\n<th id=\"S5.T1.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Client</span></th>\n<th id=\"S5.T1.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Client/Round</span></th>\n<th id=\"S5.T1.1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.1.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Samples/Client</span></th>\n<th id=\"S5.T1.1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.1.1.1.1.6.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S5.T1.1.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.1.1.1.1.7.1\" class=\"ltx_text ltx_font_bold\">Local Epoch</span></th>\n<th id=\"S5.T1.1.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.1.1.1.1.8.1\" class=\"ltx_text ltx_font_bold\">Global Round</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Fashion-MNIST</td>\n<td id=\"S5.T1.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">IID</td>\n<td id=\"S5.T1.1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">6000</td>\n<td id=\"S5.T1.1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">100</td>\n<td id=\"S5.T1.1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n<td id=\"S5.T1.1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">CNN</td>\n<td id=\"S5.T1.1.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">10</td>\n<td id=\"S5.T1.1.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">180</td>\n</tr>\n<tr id=\"S5.T1.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">Shakespeare</td>\n<td id=\"S5.T1.1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b\">non-IID</td>\n<td id=\"S5.T1.1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b\">715</td>\n<td id=\"S5.T1.1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_b\">100</td>\n<td id=\"S5.T1.1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_b\">52</td>\n<td id=\"S5.T1.1.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_b\">RNN</td>\n<td id=\"S5.T1.1.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_b\">1</td>\n<td id=\"S5.T1.1.1.3.2.8\" class=\"ltx_td ltx_align_center ltx_border_b\">1000</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Datasets and Models.",
                " We assess our approach using two common benchmark datasets (listed in Table",
                "V-A",
                ") for differentially private machine/federated learning. Fashion-MNIST",
                "[",
                "36",
                "]",
                ", a widely employed image classification dataset, comprises 10 image categories with 60,000 training samples and 10,000 testing samples. Each grayscale image is 28  28 pixels. Our image classification task uses a CNN model with two 5  5 convolutional layers, followed by 2  2 max pooling and ReLU activation. It includes a fully-connected layer with a 512-dimensional output and a classifier head. This model has approximately ",
                "1.6",
                "1.6",
                "1.6",
                " million parameters. Our experiments employ an IID and cross-device setup, dividing the dataset into 6,000 clients, each with 10 training samples.",
                "The Shakespeare dataset",
                "[",
                "23",
                ", ",
                "29",
                "]",
                " is a natural non-IID federated dataset for text generation tasks. The dataset consists of 37784 data samples from 715 clients, each representing a speaking role with at least two lines. We use a recurrent neural network (RNN) model for this dataset, which takes a sequence of characters as input and employs an embedding layer to transform each character into an 8-dimensional feature representation. These embedded characters are then processed through two LSTM layers, each consisting of 256 nodes. Finally, a densely connected softmax output layer is applied. We adopt a vocabulary size of 90 in our experiments. Our model is trained to predict a sequence of 80 characters by taking in a sequence of 80 characters, where the input sequence is shifted by one position. Consequently, the model has an output dimension of 80  90, totaling around 0.8 million parameters.",
                "Configurations.",
                " Our experiments are conducted using PyTorch and executed on NVIDIA RTX A6000 GPUs. For both datasets, the server randomly selects ",
                "s",
                "=",
                "100",
                "",
                "100",
                "s=100",
                " clients to participate in training during each round. We employ the SGD with momentum as the local optimizer for our methods. Specifically, for Fashion-MNIST, we set the momentum coefficient as 0.5, and the local learning rate (",
                "畏",
                "",
                "\\eta",
                ") is 0.125 and decays at a rate of 0.99 in each round. The batch size is set as 10, and the local epoch is set as 10, i.e., ",
                "",
                "=",
                "10",
                "",
                "10",
                "\\tau=10",
                ". For Shakespeare, local momentum is set at 0.9 with a local learning rate of 1.0, decaying at 0.99 every 50 rounds. Batch size is 4, and local epochs are 1. We set ",
                "T",
                "=",
                "180",
                "",
                "180",
                "T=180",
                " and ",
                "T",
                "=",
                "1000",
                "",
                "1000",
                "T=1000",
                " for Fashion-MNIST and Shakespeare, respectively.",
                "We calculate the end-to-end privacy loss using the API in ",
                "[",
                "39",
                "]",
                ". We set the privacy parameter ",
                "未",
                "",
                "\\delta",
                " following the methodologies employed in ",
                "[",
                "28",
                "]",
                " and ",
                "[",
                "1",
                "]",
                ". The default value of the noise multiplier in our algorithm is set as ",
                "",
                "=",
                "1.4",
                "",
                "1.4",
                "\\sigma=1.4",
                " for both datasets. For the clipping threshold, as we mentioned, it can reduce the Byzantine and DP perturbation, but it will also lead to an increased gradient variance if it is too small. The model compression in our approach allows us to use a smaller clipping threshold. Therefore, for our approach, we tune the clipping threshold and model compression ratio (defined as ",
                "p",
                ":=",
                "k",
                "/",
                "d",
                "assign",
                "",
                "",
                "",
                "p:=k/d",
                ") for both datasets by doing a grid search and finally have ",
                "p",
                "=",
                "0.3",
                ",",
                "C",
                "=",
                "0.5",
                "formulae-sequence",
                "",
                "0.3",
                "",
                "0.5",
                "p=0.3,C=0.5",
                " for the Fashion-MNIST dataset and ",
                "p",
                "=",
                "0.3",
                ",",
                "C",
                "=",
                "1.0",
                "formulae-sequence",
                "",
                "0.3",
                "",
                "1.0",
                "p=0.3,C=1.0",
                " for the Shakespeare dataset.",
                "Baselines.",
                " We compare our approach with the state-of-the-art baselines, including four Byzantine-robust aggregators (namely Trimmed Mean",
                "[",
                "38",
                "]",
                ", Median",
                "[",
                "38",
                "]",
                ", Krum",
                "[",
                "3",
                "]",
                " and Bulyan",
                "[",
                "10",
                "]",
                "), SparseFed",
                "[",
                "27",
                "]",
                ", which uses global model sparsification to achieve robustness against model poisoning attacks, and Flame ",
                "[",
                "26",
                "]",
                " which also applies DP noise to improve robustness against model poisoning attacks.\nThe Byzantine-robust aggregators basically replace the averaging step of FedAvg with a robust aggregation rule. For example, in the Trimmed Mean method, the server collects the values of a specific model parameter from all local model updates received from the clients and arranges them in ascending order. To enhance robustness, it removes extreme values using a parameter ",
                "f",
                "",
                "f",
                ". In the experiments, we set the robustness parameter ",
                "f",
                "=",
                "10",
                "",
                "10",
                "f=10",
                ". SparseFed is a defense method based on sparsification. It employs top-",
                "k",
                "",
                "k",
                " sparsification on the aggregated model update and incorporates model clipping and global momentum with error feedback to defend against model poisoning attacks. Flame is close to our work in privacy protection, which utilizes model clustering and DP mechanism on the server side to defend against model poisoning attacks. It assumes that the server is trusted.",
                "Attacks.",
                "\nWe implement two Byzantine attacks, namely the Fang attack",
                "[",
                "7",
                "]",
                " and the AGR attack",
                "[",
                "30",
                "]",
                ". Fang attack is an aggregator-known attack method that requires the knowledge of the servers aggregator. On the other hand, the AGR attack provides a general framework for Byzantine attacks, making it applicable to optimize attacks for any given aggregation rule, regardless of whether full knowledge or partial knowledge is available. Both attack methods formulate the attack as an optimization problem, aiming to maximize the deviation of the global model update in the opposite direction of the benign updates. To ensure comprehensive evaluation, we consider the most powerful versions of the attacks based on the principles of the Cannikin Law (or Wooden Bucket Theory). For instance, we utilize the Fang attack specifically designed for Krum to evaluate the Krum and Bulyan defense methods, and we use the AGR attack designed specifically for Median to evaluate the performance of Median. In short, we always select the Fang/AGR attack with the best-attacking performance to evaluate the robustness of these defending methods."
            ]
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: Performance of FedVRDP against Byzantine attacks on Fashion-MNIST, compared to state-of-the-art defenses.",
        "table": "<table id=\"S5.T2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Defense</span></td>\n<td id=\"S5.T2.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Fang(%)</span></td>\n<td id=\"S5.T2.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">AGR(%)</span></td>\n<td id=\"S5.T2.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T2.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Duration(Hrs)</span></td>\n</tr>\n<tr id=\"S5.T2.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAvg</td>\n<td id=\"S5.T2.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">85.94</td>\n<td id=\"S5.T2.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.24</td>\n</tr>\n<tr id=\"S5.T2.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">No Defense</td>\n<td id=\"S5.T2.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35.37</td>\n<td id=\"S5.T2.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10.00</td>\n<td id=\"S5.T2.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.25/0.25</td>\n</tr>\n<tr id=\"S5.T2.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Trimmed Mean<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">38</a>]</cite>\n</td>\n<td id=\"S5.T2.1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">40.33</td>\n<td id=\"S5.T2.1.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">11.05</td>\n<td id=\"S5.T2.1.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">0.33/0.29</td>\n</tr>\n<tr id=\"S5.T2.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Median<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">38</a>]</cite>\n</td>\n<td id=\"S5.T2.1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">35.37</td>\n<td id=\"S5.T2.1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">10.00</td>\n<td id=\"S5.T2.1.1.5.5.4\" class=\"ltx_td ltx_align_center\">0.31/0.26</td>\n</tr>\n<tr id=\"S5.T2.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Krum<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>]</cite>\n</td>\n<td id=\"S5.T2.1.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">29.44</td>\n<td id=\"S5.T2.1.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">66.53</td>\n<td id=\"S5.T2.1.1.6.6.4\" class=\"ltx_td ltx_align_center\">0.28/0.46</td>\n</tr>\n<tr id=\"S5.T2.1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Bulyan<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>\n</td>\n<td id=\"S5.T2.1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">63.17</td>\n<td id=\"S5.T2.1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">61.91</td>\n<td id=\"S5.T2.1.1.7.7.4\" class=\"ltx_td ltx_align_center\">1.38/1.45</td>\n</tr>\n<tr id=\"S5.T2.1.1.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\">SparseFed<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite>\n</td>\n<td id=\"S5.T2.1.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">23.76</td>\n<td id=\"S5.T2.1.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">10.00</td>\n<td id=\"S5.T2.1.1.8.8.4\" class=\"ltx_td ltx_align_center\">0.28/0.26</td>\n</tr>\n<tr id=\"S5.T2.1.1.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.9.9.1.1\" class=\"ltx_text ltx_font_bold\">Ours</span></td>\n<td id=\"S5.T2.1.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.9.9.2.1\" class=\"ltx_text ltx_font_bold\">74.62</span></td>\n<td id=\"S5.T2.1.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T2.1.1.9.9.3.1\" class=\"ltx_text ltx_font_bold\">77.34</span></td>\n<td id=\"S5.T2.1.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">0.41/0.31</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Effectiveness of FedVRDP.",
                "\nWe first evaluate the effectiveness of our method in defending against the Fang attack and AGR attack, aiming to demonstrate its superiority over existing defense approaches. Table",
                "V-B",
                " presents the test accuracy results obtained when subjecting our method to a Byzantine attack on the Fashion-MNIST dataset. We consider a powerful attack setting where 20% of the clients in the FL system are Byzantine. We also note the experiments runtime (i.e., the duration) and report averaged results from 5 trials.",
                "From the results, we can observe that among the defense baselines, only the Bulyan shows some resistance to the Fang attack, potentially benefiting from its multi-iteration model cleaning approach. However, Bulyan is limited to scenarios with less than 25% compromised clients, and it will become ineffective beyond that threshold. Regarding the AGR attack, the Trimmed Mean, Median, and SparseFed methods exhibit behavior similar to that of a random classifier, with an accuracy of around 10%. In comparison, our method outperforms Bulyan and Krum by +15.43% and +10.81%, respectively. In terms of running time, our method demonstrates an average increase of 6.6 and 5.4 minutes per round compared to the No Defense setting and SparseFed, respectively. These results highlight the advantages of our approach over Bulyan, which requires significantly longer times of 69.9 and 68.7 minutes.",
                "On the Shakespeare dataset, as demonstrated in Table",
                "V-B",
                ", our method exhibits a significant performance advantage over other Byzantine-robust approaches when subjected to Fang attack. When facing AGR attack, our method achieves comparable results to Bulyan, with only a slight difference of 0.1% in test accuracy. It is worth noting, however, that the total training duration of Bulyan is approximately three times longer than our approach. Furthermore, our method prioritizes privacy preservation, whereas Bulyan does not offer any privacy guarantee. As a result, Bulyan may not be suitable if system efficiency and client privacy are important considerations.",
                "Privacy Guarantee.",
                " In terms of privacy protection, we compare the test accuracy of our method with that of DPFed and Flame",
                "[",
                "26",
                "]",
                ", both of which utilize DP techniques to ensure system privacy or robustness. In our method, we apply a noise multiplier ",
                "",
                "",
                "\\sigma",
                " of 0.14 to achieve the desired privacy level. Similarly, for DPFed, a noise multiplier ",
                "",
                "",
                "\\sigma",
                " of 0.10 is applied. In the case of Flame, we conduct experiments with various noise multipliers and calculate its privacy loss",
                "[",
                "26",
                "]",
                ". Our experiment encompasses both the Fashion-MNIST and Shakespeare datasets, allowing us to assess the performance of our method under Byzantine attacks while operating within a limited privacy budget. Specifically, we apply the Fang attack and AGR attack with 25% Byzantine clients to evaluate the robustness and privacy guarantees of our approach.",
                "As shown in Table",
                "IV",
                ", DPFed achieves a strong differential privacy guarantee, with privacy losses bounded by ",
                "系",
                "=",
                "2.02",
                "italic-系",
                "2.02",
                "\\epsilon=2.02",
                " and ",
                "11.51",
                "11.51",
                "11.51",
                " for Fashion-MNIST and Shakespeare, respectively. However, it lacks resilience against Byzantine attacks when a significant number of clients exhibit malicious behavior, resulting in outputs that are nearly random. On the other hand, Flame demonstrates robustness against Byzantine attacks only when the level of differential privacy noise is trivial. It becomes vulnerable to compromise when the magnitude of the noise is substantial, leading to a failure to provide a satisfactory privacy guarantee. In contrast, our method excels in achieving excellent performance in both privacy protection and Byzantine robustness across both the image classification and word generation tasks.",
                "Impact of Byzantine Clients Percentage.",
                " We evaluate the performance of our method and baselines with respect to different percentages of Byzantine clients. Specifically, we vary the percentage of Byzantine clients from 10% to 20% and report the corresponding testing accuracy of the defense methods on Fashion-MNIST dataset in Figure",
                "2",
                " for the Fang attack and Figure",
                "3",
                " for the AGR attack, respectively.\nWe observe that as the percentage of Byzantine clients increases from 10% to 20%, our method, along with Bulyan, can always maintain a test accuracy above 60% against Fang attack, while other methods exhibit accuracies below 60% when the percentage of Byzantine clients exceeds 15%. Our method also demonstrates stability against AGR attack and outperforms other methods by a significant margin. As the percentage of Byzantine clients increases from 10% to 20%, our testing accuracy under AGR attack only drops by 3.15%."
            ]
        ]
    },
    "S5.T3": {
        "caption": "TABLE III: Performance of FedVRDP against Byzantine attacks on Shakespeare, compared to state-of-the-art defenses.",
        "table": "<table id=\"S5.T3.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Defense</span></td>\n<td id=\"S5.T3.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Fang(%)</span></td>\n<td id=\"S5.T3.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T3.1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">AGR(%)</span></td>\n<td id=\"S5.T3.1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T3.1.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Duration(Hrs)</span></td>\n</tr>\n<tr id=\"S5.T3.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAvg</td>\n<td id=\"S5.T3.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">64.75</td>\n<td id=\"S5.T3.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">2.43</td>\n</tr>\n<tr id=\"S5.T3.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">No Defense</td>\n<td id=\"S5.T3.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">34.77</td>\n<td id=\"S5.T3.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.86</td>\n<td id=\"S5.T3.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">2.43/2.43</td>\n</tr>\n<tr id=\"S5.T3.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Trimmed Mean<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">38</a>]</cite>\n</td>\n<td id=\"S5.T3.1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">39.05</td>\n<td id=\"S5.T3.1.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">46.16</td>\n<td id=\"S5.T3.1.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">2.76/2.60</td>\n</tr>\n<tr id=\"S5.T3.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Median<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">38</a>]</cite>\n</td>\n<td id=\"S5.T3.1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">35.18</td>\n<td id=\"S5.T3.1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">37.18</td>\n<td id=\"S5.T3.1.1.5.5.4\" class=\"ltx_td ltx_align_center\">2.67/2.45</td>\n</tr>\n<tr id=\"S5.T3.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Krum<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>]</cite>\n</td>\n<td id=\"S5.T3.1.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">41.88</td>\n<td id=\"S5.T3.1.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">48.18</td>\n<td id=\"S5.T3.1.1.6.6.4\" class=\"ltx_td ltx_align_center\">2.50/2.81</td>\n</tr>\n<tr id=\"S5.T3.1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">Bulyan<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib10\" title=\"\" class=\"ltx_ref\">10</a>]</cite>\n</td>\n<td id=\"S5.T3.1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">44.48</td>\n<td id=\"S5.T3.1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T3.1.1.7.7.3.1\" class=\"ltx_text ltx_font_bold\">55.94</span></td>\n<td id=\"S5.T3.1.1.7.7.4\" class=\"ltx_td ltx_align_center\">7.81/8.10</td>\n</tr>\n<tr id=\"S5.T3.1.1.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\">SparseFed<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite>\n</td>\n<td id=\"S5.T3.1.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">40.25</td>\n<td id=\"S5.T3.1.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">34.85</td>\n<td id=\"S5.T3.1.1.8.8.4\" class=\"ltx_td ltx_align_center\">3.12/2.60</td>\n</tr>\n<tr id=\"S5.T3.1.1.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T3.1.1.9.9.1.1\" class=\"ltx_text ltx_font_bold\">Ours</span></td>\n<td id=\"S5.T3.1.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T3.1.1.9.9.2.1\" class=\"ltx_text ltx_font_bold\">48.12</span></td>\n<td id=\"S5.T3.1.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">55.84</td>\n<td id=\"S5.T3.1.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">2.96/2.82</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Effectiveness of FedVRDP.",
                "\nWe first evaluate the effectiveness of our method in defending against the Fang attack and AGR attack, aiming to demonstrate its superiority over existing defense approaches. Table",
                "V-B",
                " presents the test accuracy results obtained when subjecting our method to a Byzantine attack on the Fashion-MNIST dataset. We consider a powerful attack setting where 20% of the clients in the FL system are Byzantine. We also note the experiments runtime (i.e., the duration) and report averaged results from 5 trials.",
                "From the results, we can observe that among the defense baselines, only the Bulyan shows some resistance to the Fang attack, potentially benefiting from its multi-iteration model cleaning approach. However, Bulyan is limited to scenarios with less than 25% compromised clients, and it will become ineffective beyond that threshold. Regarding the AGR attack, the Trimmed Mean, Median, and SparseFed methods exhibit behavior similar to that of a random classifier, with an accuracy of around 10%. In comparison, our method outperforms Bulyan and Krum by +15.43% and +10.81%, respectively. In terms of running time, our method demonstrates an average increase of 6.6 and 5.4 minutes per round compared to the No Defense setting and SparseFed, respectively. These results highlight the advantages of our approach over Bulyan, which requires significantly longer times of 69.9 and 68.7 minutes.",
                "On the Shakespeare dataset, as demonstrated in Table",
                "V-B",
                ", our method exhibits a significant performance advantage over other Byzantine-robust approaches when subjected to Fang attack. When facing AGR attack, our method achieves comparable results to Bulyan, with only a slight difference of 0.1% in test accuracy. It is worth noting, however, that the total training duration of Bulyan is approximately three times longer than our approach. Furthermore, our method prioritizes privacy preservation, whereas Bulyan does not offer any privacy guarantee. As a result, Bulyan may not be suitable if system efficiency and client privacy are important considerations.",
                "Privacy Guarantee.",
                " In terms of privacy protection, we compare the test accuracy of our method with that of DPFed and Flame",
                "[",
                "26",
                "]",
                ", both of which utilize DP techniques to ensure system privacy or robustness. In our method, we apply a noise multiplier ",
                "",
                "",
                "\\sigma",
                " of 0.14 to achieve the desired privacy level. Similarly, for DPFed, a noise multiplier ",
                "",
                "",
                "\\sigma",
                " of 0.10 is applied. In the case of Flame, we conduct experiments with various noise multipliers and calculate its privacy loss",
                "[",
                "26",
                "]",
                ". Our experiment encompasses both the Fashion-MNIST and Shakespeare datasets, allowing us to assess the performance of our method under Byzantine attacks while operating within a limited privacy budget. Specifically, we apply the Fang attack and AGR attack with 25% Byzantine clients to evaluate the robustness and privacy guarantees of our approach.",
                "As shown in Table",
                "IV",
                ", DPFed achieves a strong differential privacy guarantee, with privacy losses bounded by ",
                "系",
                "=",
                "2.02",
                "italic-系",
                "2.02",
                "\\epsilon=2.02",
                " and ",
                "11.51",
                "11.51",
                "11.51",
                " for Fashion-MNIST and Shakespeare, respectively. However, it lacks resilience against Byzantine attacks when a significant number of clients exhibit malicious behavior, resulting in outputs that are nearly random. On the other hand, Flame demonstrates robustness against Byzantine attacks only when the level of differential privacy noise is trivial. It becomes vulnerable to compromise when the magnitude of the noise is substantial, leading to a failure to provide a satisfactory privacy guarantee. In contrast, our method excels in achieving excellent performance in both privacy protection and Byzantine robustness across both the image classification and word generation tasks.",
                "Impact of Byzantine Clients Percentage.",
                " We evaluate the performance of our method and baselines with respect to different percentages of Byzantine clients. Specifically, we vary the percentage of Byzantine clients from 10% to 20% and report the corresponding testing accuracy of the defense methods on Fashion-MNIST dataset in Figure",
                "2",
                " for the Fang attack and Figure",
                "3",
                " for the AGR attack, respectively.\nWe observe that as the percentage of Byzantine clients increases from 10% to 20%, our method, along with Bulyan, can always maintain a test accuracy above 60% against Fang attack, while other methods exhibit accuracies below 60% when the percentage of Byzantine clients exceeds 15%. Our method also demonstrates stability against AGR attack and outperforms other methods by a significant margin. As the percentage of Byzantine clients increases from 10% to 20%, our testing accuracy under AGR attack only drops by 3.15%."
            ]
        ]
    },
    "S5.T4": {
        "caption": "TABLE IV: Privacy and accuracy performance of our approach under Byzantine attacks, compared with the related methods. Results marked with * indicate that the training was interrupted, and we do not report values of 系italic-系{\\epsilon} exceeding 50.",
        "table": "<table id=\"S5.T4.5.3.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.5.3.3.4.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.5.3.3.4.1.1\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"></th>\n<th id=\"S5.T4.5.3.3.4.1.2\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"></th>\n<th id=\"S5.T4.5.3.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S5.T4.5.3.3.4.1.3.1\" class=\"ltx_text ltx_font_bold\">F-MNIST</span></th>\n<th id=\"S5.T4.5.3.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" colspan=\"3\"><span id=\"S5.T4.5.3.3.4.1.4.1\" class=\"ltx_text ltx_font_bold\">Shakespeare</span></th>\n</tr>\n<tr id=\"S5.T4.5.3.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.5.3.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\"><span id=\"S5.T4.5.3.3.3.4.1\" class=\"ltx_text ltx_font_bold\">Defense</span></th>\n<th id=\"S5.T4.3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\"><span id=\"S5.T4.3.1.1.1.1.1\" class=\"ltx_text\"><math id=\"S5.T4.3.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{\\sigma}\" display=\"inline\"><semantics id=\"S5.T4.3.1.1.1.1.1.m1.1a\"><mi id=\"S5.T4.3.1.1.1.1.1.m1.1.1\" xref=\"S5.T4.3.1.1.1.1.1.m1.1.1.cmml\"></mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.3.1.1.1.1.1.m1.1b\"><ci id=\"S5.T4.3.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T4.3.1.1.1.1.1.m1.1.1\"></ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.3.1.1.1.1.1.m1.1c\">\\bm{\\sigma}</annotation></semantics></math></span></th>\n<th id=\"S5.T4.4.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S5.T4.4.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{\\epsilon}\" display=\"inline\"><semantics id=\"S5.T4.4.2.2.2.2.m1.1a\"><mi class=\"ltx_mathvariant_bold-italic\" mathvariant=\"bold-italic\" id=\"S5.T4.4.2.2.2.2.m1.1.1\" xref=\"S5.T4.4.2.2.2.2.m1.1.1.cmml\">系</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.4.2.2.2.2.m1.1b\"><ci id=\"S5.T4.4.2.2.2.2.m1.1.1.cmml\" xref=\"S5.T4.4.2.2.2.2.m1.1.1\">bold-italic-系</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.4.2.2.2.2.m1.1c\">\\bm{\\epsilon}</annotation></semantics></math></th>\n<th id=\"S5.T4.5.3.3.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T4.5.3.3.3.5.1\" class=\"ltx_text ltx_font_bold\">Fang(%)</span></th>\n<th id=\"S5.T4.5.3.3.3.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T4.5.3.3.3.6.1\" class=\"ltx_text ltx_font_bold\">AGR(%)</span></th>\n<th id=\"S5.T4.5.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><math id=\"S5.T4.5.3.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{\\epsilon}\" display=\"inline\"><semantics id=\"S5.T4.5.3.3.3.3.m1.1a\"><mi class=\"ltx_mathvariant_bold-italic\" mathvariant=\"bold-italic\" id=\"S5.T4.5.3.3.3.3.m1.1.1\" xref=\"S5.T4.5.3.3.3.3.m1.1.1.cmml\">系</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.5.3.3.3.3.m1.1b\"><ci id=\"S5.T4.5.3.3.3.3.m1.1.1.cmml\" xref=\"S5.T4.5.3.3.3.3.m1.1.1\">bold-italic-系</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.5.3.3.3.3.m1.1c\">\\bm{\\epsilon}</annotation></semantics></math></th>\n<th id=\"S5.T4.5.3.3.3.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T4.5.3.3.3.7.1\" class=\"ltx_text ltx_font_bold\">Fang(%)</span></th>\n<th id=\"S5.T4.5.3.3.3.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T4.5.3.3.3.8.1\" class=\"ltx_text ltx_font_bold\">AGR(%)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.5.3.3.5.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.5.3.3.5.1.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<th id=\"S5.T4.5.3.3.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.001</th>\n<td id=\"S5.T4.5.3.3.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td id=\"S5.T4.5.3.3.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T4.5.3.3.5.1.4.1\" class=\"ltx_text ltx_font_bold\">69.38</span></td>\n<td id=\"S5.T4.5.3.3.5.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">73.27</td>\n<td id=\"S5.T4.5.3.3.5.1.6\" class=\"ltx_td ltx_align_center ltx_border_t\">-</td>\n<td id=\"S5.T4.5.3.3.5.1.7\" class=\"ltx_td ltx_align_center ltx_border_t\">42.19</td>\n<td id=\"S5.T4.5.3.3.5.1.8\" class=\"ltx_td ltx_align_center ltx_border_t\">54.67</td>\n</tr>\n<tr id=\"S5.T4.5.3.3.6.2\" class=\"ltx_tr\">\n<th id=\"S5.T4.5.3.3.6.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r\"></th>\n<th id=\"S5.T4.5.3.3.6.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.01</th>\n<td id=\"S5.T4.5.3.3.6.2.3\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T4.5.3.3.6.2.4\" class=\"ltx_td ltx_align_center\">19.58</td>\n<td id=\"S5.T4.5.3.3.6.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">73.48</td>\n<td id=\"S5.T4.5.3.3.6.2.6\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T4.5.3.3.6.2.7\" class=\"ltx_td ltx_align_center\">41.63*</td>\n<td id=\"S5.T4.5.3.3.6.2.8\" class=\"ltx_td ltx_align_center\">54.84</td>\n</tr>\n<tr id=\"S5.T4.5.3.3.7.3\" class=\"ltx_tr\">\n<th id=\"S5.T4.5.3.3.7.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T4.5.3.3.7.3.1.1\" class=\"ltx_text\">Flame<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite></span></th>\n<th id=\"S5.T4.5.3.3.7.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">0.10</th>\n<td id=\"S5.T4.5.3.3.7.3.3\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T4.5.3.3.7.3.4\" class=\"ltx_td ltx_align_center\">8.44*</td>\n<td id=\"S5.T4.5.3.3.7.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">7.51*</td>\n<td id=\"S5.T4.5.3.3.7.3.6\" class=\"ltx_td ltx_align_center\">-</td>\n<td id=\"S5.T4.5.3.3.7.3.7\" class=\"ltx_td ltx_align_center\">1.36*</td>\n<td id=\"S5.T4.5.3.3.7.3.8\" class=\"ltx_td ltx_align_center\">1.39*</td>\n</tr>\n<tr id=\"S5.T4.5.3.3.8.4\" class=\"ltx_tr\">\n<th id=\"S5.T4.5.3.3.8.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">DPFed</th>\n<th id=\"S5.T4.5.3.3.8.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">0.10</th>\n<td id=\"S5.T4.5.3.3.8.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">2.02</td>\n<td id=\"S5.T4.5.3.3.8.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">11.37</td>\n<td id=\"S5.T4.5.3.3.8.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">11.09</td>\n<td id=\"S5.T4.5.3.3.8.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">11.51</td>\n<td id=\"S5.T4.5.3.3.8.4.7\" class=\"ltx_td ltx_align_center ltx_border_t\">29.27</td>\n<td id=\"S5.T4.5.3.3.8.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\">39.95</td>\n</tr>\n<tr id=\"S5.T4.5.3.3.9.5\" class=\"ltx_tr\">\n<th id=\"S5.T4.5.3.3.9.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T4.5.3.3.9.5.1.1\" class=\"ltx_text ltx_font_bold\">Ours</span></th>\n<th id=\"S5.T4.5.3.3.9.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">0.14</th>\n<td id=\"S5.T4.5.3.3.9.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">1.01</td>\n<td id=\"S5.T4.5.3.3.9.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">65.34</td>\n<td id=\"S5.T4.5.3.3.9.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T4.5.3.3.9.5.5.1\" class=\"ltx_text ltx_font_bold\">74.97</span></td>\n<td id=\"S5.T4.5.3.3.9.5.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">6.99</td>\n<td id=\"S5.T4.5.3.3.9.5.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T4.5.3.3.9.5.7.1\" class=\"ltx_text ltx_font_bold\">42.38</span></td>\n<td id=\"S5.T4.5.3.3.9.5.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\"><span id=\"S5.T4.5.3.3.9.5.8.1\" class=\"ltx_text ltx_font_bold\">54.90</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "As shown in TableIV, DPFed achieves a strong differential privacy guarantee, with privacy losses bounded by 系=2.02italic-系2.02\\epsilon=2.02 and 11.5111.5111.51 for Fashion-MNIST and Shakespeare, respectively. However, it lacks resilience against Byzantine attacks when a significant number of clients exhibit malicious behavior, resulting in outputs that are nearly random. On the other hand, Flame demonstrates robustness against Byzantine attacks only when the level of differential privacy noise is trivial. It becomes vulnerable to compromise when the magnitude of the noise is substantial, leading to a failure to provide a satisfactory privacy guarantee. In contrast, our method excels in achieving excellent performance in both privacy protection and Byzantine robustness across both the image classification and word generation tasks."
        ]
    }
}