{
    "id_table_1": {
        "caption": "Table 1:  Comparison of Human Coherence Ratings for generated answers to a MultiHop dataset question, evaluating the performance of Naive RAG versus postQFRAP models.",
        "table": "A3.T1.1",
        "footnotes": [],
        "references": [
            "To address these limitations, we introduce  adRAP  (adaptive Recursive Abstractive Processing), an algorithm designed to efficiently update RAPTORs recursive-abstractive structure as new documents are added or removed.  By incrementally adjusting the structure, adRAP avoids full re-computation, preserving retrieval performance while significantly reducing computational overhead.  Furthermore, both RAPTOR and adRAP introduce memory overhead and require periodic maintenance when used with dynamic datasets.  As an alternative, we propose  postQFRAP , a post-retrieval method that applies query-focused recursive abstractive processing as a black-box layer, as illustrated in Figure  1 .  This post-processing method integrates seamlessly into any retrieval pipeline while significantly enhancing the quality of the retrieved context.  For example, naive RAG  (Gao et al.,  2023 )  can serve as the underlying model since it processes documents independently, allowing easy addition or removal of documents.  Moreover, by initially retrieving enough documents, questions requiring a broader understanding can be answered by passing the generated summary to the LLM, rather than passing all potentially relevant documents, thus mitigating challenges like limited context window size and information loss in large contexts  (Liu et al.,  2024 ; Yu et al.,  2024 ) .  Through extensive experiments on real-world datasets, we demonstrate that adRAP provides a good approximation of the RAPTOR tree, while postQFRAP effectively enhances retrieval quality.",
            "The clustering and summarization process is repeated recursively to obtain a multi-layered representation of the dataset.  This approach is outlined in Algorithm  1 .",
            "A key component of the recursive-abstractive tree construction (Algorithm  1 ) is its clustering algorithm, which poses challenges when handling dynamic datasets.  Given a fitted Gaussian Mixture Model (GMM)  I I \\mathcal{I} caligraphic_I  with  K K K italic_K  clusters defined by their means  {  k } k = 1 K superscript subscript subscript  k k 1 K \\{\\mu_{k}\\}_{k=1}^{K} { italic_ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , covariance matrices  {  k } k = 1 K superscript subscript subscript  k k 1 K \\{\\Sigma_{k}\\}_{k=1}^{K} { roman_ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , and mixing coefficients  {  k } k = 1 K superscript subscript subscript  k k 1 K \\{\\pi_{k}\\}_{k=1}^{K} { italic_ start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_k = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT , and given points  { x i } i = 1 n superscript subscript subscript x i i 1 n \\{x_{i}\\}_{i=1}^{n} { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT  assigned to these clusters, the goal is to assign a new point  x n + 1 subscript x n 1 x_{n+1} italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT  to one or more clusters.  This may involve updating the clustering structure or introducing new clusters.  While prior work addresses online GMMs  (Song and Wang,  2005 ; Declercq and Piater,  2008 ; Zhang and Scordilis,  2008 ) , our setting differs in that we start with a GMM fit on a dataset, we have access to all the points and we want to minimize the number of updated clusters, as each update requires multiple re-generated summaries.",
            "We augment  R R \\mathcal{R} caligraphic_R  as follows.  First, we retrieve  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  documents without imposing a token limit.  Then, we apply a query-focused version of Algorithm  1  to these  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  documents to build a recursive-abstractive tree.  The key modification is using query-focused summarization (see prompt in Appendix, Table  5 ).  Since the tree is constructed to answer  q q q italic_q , summarizing information relevant to  q q q italic_q  ensures that key details are preserved while recursively filtering out irrelevant content.  Additionally, we modify the clustering to rely solely on local embeddings, as retrieving  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  documents already serves as a global filtering step.  In other words, we assume the retrieved documents belong to the same global cluster.  We demonstrate in Appendix  B  that using the simpler one-step clustering preserves the quality of the generated context compared to the two-step approach.",
            "We compare the adRAP algorithm against Naive RAG, RAPTOR, and a greedy variant of adRAP, which assigns each new point to its most probable cluster without updating the GMM fit.  To compute adRAP, we first construct a full tree using 70% of the dataset. The remaining 30% is added using the adRAP algorithm (Section  4.4 ) where we set   c = 11 subscript  c 11 \\tau_{c}=11 italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = 11  and   n = max  ( 100 , | D 0 | ) subscript  n 100 subscript D 0 \\tau_{n}=\\max(100,\\sqrt{|D_{0}|}) italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = roman_max ( 100 , square-root start_ARG | italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | end_ARG )  for Algorithm  2 .  The choice of   c subscript  c \\tau_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  is based on the average cluster size in the full RAPTOR tree, which is always less than 10 (see appendix, Table  12 ).  For the greedy variant, a similar procedure is used.  To simulate a challenging scenario, we remove the last 30% of documents instead of random sampling.",
            "As shown in Figures  10 ,the results across all four datasets indicate that the difference between one-step and two-step clustering is minimal.  Furthermore, Figure  11  demonstrates that one-step clustering performs better in QASPER and QuALITY, worse in NarrativeQA, and is comparable to two-step clustering in MultiHop.  Overall, the differences in performance between the two algorithms are minor.  Based on these observations, we adopt the simpler and more efficient one-step clustering method in our algorithm.",
            "To determine the optimal value of  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  for our main experiments, we evaluated five different values,  k 0  { 10 , 20 , 40 , 60 , 80 } subscript k 0 10 20 40 60 80 k_{0}\\in\\{10,20,40,60,80\\} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  { 10 , 20 , 40 , 60 , 80 } , by comparing the performance on 100 questions from the validation sets of the NarrativeQA and QASPER datasets. It is important to emphasize that these questions are entirely distinct from the data used in our main experiments.  As in our primary experiments, we limited the final summary size to 2,000 tokens and considered three metrics: the number of answered questions, context relevance, and human coherence.  To account for the non-deterministic nature of the LLM evaluators, we repeated the evaluation process three times and reported the average performance along with the standard error in Figure  12 .",
            "We present the results in Figures  13  and  14 .  We observe that adding Query Expansion to postQFRAP does not improve performance and, in fact, reduces the comprehensiveness and coherence of the generated answers.",
            "We present in Table  10  the time taken and the number of summary calls made on each of the QASPER and QuALITY dataset for two different algorithms:",
            "Figures  15  and  16  show that, on the NarrativeQA dataset, adRAP performs comparably to RAPTOR and Greedy adRAP, while consistently outperforming Naive RAG.",
            "Figure  17  demonstrates that query-focused algorithms clearly outperform the baselines on the NarrativeQA dataset.  Notably, postQFRAP and one-shot summarization achieve comparable results.  However, as shown in Figure  18 , postQFRAP continues to significantly outperform all other algorithms in terms of comprehensiveness, diversity, and empowerment of the generated answers.",
            "In Table  11 , we present the sizes of the datasets used in our experiments.  In Table  12 , we present various statistics regarding the recursive-abstractive trees constructed from our datasets. The number of internal nodes is approximately  n / 6 n 6 n/6 italic_n / 6 , where  n n n italic_n  represents the total number of chunks in the dataset. Additionally, most cluster sizes range between 4 and 15, with nodes rarely belonging to more than one cluster."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of contexts and answers generated for a MultiHop question using Naive RAG and postQFRAP.",
        "table": "A4.T2.1",
        "footnotes": [],
        "references": [
            "The updated parameters match those from applying Equation  2  to the points  { x i } i = 1 n + 1 superscript subscript subscript x i i 1 n 1 \\{x_{i}\\}_{i=1}^{n+1} { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n + 1 end_POSTSUPERSCRIPT .  After updating the cluster parameters, we recompute the posterior for  x n + 1 subscript x n 1 x_{n+1} italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT  and assign it to clusters  { k :   ( z n + 1 , k ) > 0.1 } conditional-set k  subscript z n 1 k 0.1 \\{k:\\gamma(z_{n+1,k})>0.1\\} { italic_k : italic_ ( italic_z start_POSTSUBSCRIPT italic_n + 1 , italic_k end_POSTSUBSCRIPT ) > 0.1 } , without affecting other point assignments.  Although this remains an approximation, it has been shown to be an effective way to incrementally fit a GMM  (Neal and Hinton,  1998 ) .  The update is efficient, as its time is independent of  n n n italic_n , with only a few clusters being updated (those assigned to  x n + 1 subscript x n 1 x_{n+1} italic_x start_POSTSUBSCRIPT italic_n + 1 end_POSTSUBSCRIPT ).  When  n n n italic_n  is small, we perform full EM steps instead of updating using only the new point, as the smaller number of clusters makes this affordable.  This also yields more significant improvements, as clusterings with fewer points are more sensitive to new data.",
            "We summarize these ideas in Algorithm  2 .  The parameter   n subscript  n \\tau_{n} italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT  controls the trade-off between quality and computation time, determining whether to perform full or approximate EM updates based on  n n n italic_n .  Similarly,   c subscript  c \\tau_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  sets the cluster size threshold for triggering a potential split.",
            "To do so, the global reduced embedding  v g subscript v g v_{g} italic_v start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  is derived from  v v v italic_v  using the global UMAP model and assigned to the most probable cluster in the global clustering.  Since the global clustering includes all  | D 0 | subscript D 0 |D_{0}| | italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT |  nodes, it is considered stable and no dynamic adjustments are made.  Next, we focus on the global cluster to which  v g subscript v g v_{g} italic_v start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT  was assigned, applying the local UMAP model to compute a reduced local embedding  v l subscript v l v_{l} italic_v start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT .  The local clustering is updated using Algorithm  2 , potentially creating new nodes.",
            "In  T 0 subscript T 0 T_{0} italic_T start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , nodes with updated children regenerate their summaries and recompute embeddings, with updates propagated to their ancestors (up to five levels).  If new clusters are created at layer  i i i italic_i , this procedure is recursively applied at layer  i + 1 i 1 i+1 italic_i + 1 .  By design, only a few clusters are affected, minimizing the need for summary re-computation. To illustrate this, we compare in Appendix  H.2  the runtime and number of generated summaries between adRAP and a full re-computation of the tree. Moreover, the pseudo-code of adRAP is presented in Appendix  A .",
            "We compare the adRAP algorithm against Naive RAG, RAPTOR, and a greedy variant of adRAP, which assigns each new point to its most probable cluster without updating the GMM fit.  To compute adRAP, we first construct a full tree using 70% of the dataset. The remaining 30% is added using the adRAP algorithm (Section  4.4 ) where we set   c = 11 subscript  c 11 \\tau_{c}=11 italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = 11  and   n = max  ( 100 , | D 0 | ) subscript  n 100 subscript D 0 \\tau_{n}=\\max(100,\\sqrt{|D_{0}|}) italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = roman_max ( 100 , square-root start_ARG | italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | end_ARG )  for Algorithm  2 .  The choice of   c subscript  c \\tau_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  is based on the average cluster size in the full RAPTOR tree, which is always less than 10 (see appendix, Table  12 ).  For the greedy variant, a similar procedure is used.  To simulate a challenging scenario, we remove the last 30% of documents instead of random sampling.",
            "Figure  2  shows that adRAPs performance is generally on par with RAPTOR across most metrics, with the exception of context relevance, where adRAP falls short by at least 3%.  However, adRAP outperforms both the naive RAG and the greedy algorithm, particularly in the QuALITY dataset.  These findings are further corroborated by the head-to-head evaluations in Figures  3 ,  4 , and  5 .  Notably, in the QuALITY dataset, adRAP exceeds RAPTOR in metrics such as comprehensiveness, diversity, and empowerment, despite its lower performance in context relevance.  On the other hand, adRAP underperforms compared to RAPTOR in the MultiHop and QASPER datasets.",
            "To determine the optimal value of  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  for our main experiments, we evaluated five different values,  k 0  { 10 , 20 , 40 , 60 , 80 } subscript k 0 10 20 40 60 80 k_{0}\\in\\{10,20,40,60,80\\} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  { 10 , 20 , 40 , 60 , 80 } , by comparing the performance on 100 questions from the validation sets of the NarrativeQA and QASPER datasets. It is important to emphasize that these questions are entirely distinct from the data used in our main experiments.  As in our primary experiments, we limited the final summary size to 2,000 tokens and considered three metrics: the number of answered questions, context relevance, and human coherence.  To account for the non-deterministic nature of the LLM evaluators, we repeated the evaluation process three times and reported the average performance along with the standard error in Figure  12 .",
            "In Table  11 , we present the sizes of the datasets used in our experiments.  In Table  12 , we present various statistics regarding the recursive-abstractive trees constructed from our datasets. The number of internal nodes is approximately  n / 6 n 6 n/6 italic_n / 6 , where  n n n italic_n  represents the total number of chunks in the dataset. Additionally, most cluster sizes range between 4 and 15, with nodes rarely belonging to more than one cluster."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Prompt for Question Generation",
        "table": "A7.T3.1",
        "footnotes": [],
        "references": [
            "Finally, a summarization step is applied to all nodes at the last layer of the tree, instead of using a top- k k k italic_k  retrieval approach, to reduce redundancy in the results.  This process is detailed in Algorithm  3 .",
            "MultiHop  consists of news articles published between 2013 and 2023  (Tang and Yang,  2024 ) .  Although the original questions focus on retrieving and reasoning across multiple documents, they primarily target explicit fact retrieval.  To create more challenging questions requiring a broader understanding, we construct a RAPTOR tree on the dataset, sample chunks/summaries from the tree, and ask an LLM to generate questions based on those chunks. Details are provided in Appendix  H.3 .",
            "Figure  2  shows that adRAPs performance is generally on par with RAPTOR across most metrics, with the exception of context relevance, where adRAP falls short by at least 3%.  However, adRAP outperforms both the naive RAG and the greedy algorithm, particularly in the QuALITY dataset.  These findings are further corroborated by the head-to-head evaluations in Figures  3 ,  4 , and  5 .  Notably, in the QuALITY dataset, adRAP exceeds RAPTOR in metrics such as comprehensiveness, diversity, and empowerment, despite its lower performance in context relevance.  On the other hand, adRAP underperforms compared to RAPTOR in the MultiHop and QASPER datasets.",
            "All hyperparameters and model configurations used in the experiments are clearly detailed in Sections  6.3  and  6.4 .",
            "We compare postQFRAP, which uses a two-step hierarchical clustering algorithm as described in Section  3.3 , with a one-step approach that only applies local clustering (i.e., setting the UMAP parameter  n  _  n  e  i  g  h  b  o  r  s n _ n e i g h b o r s n\\_neighbors italic_n _ italic_n italic_e italic_i italic_g italic_h italic_b italic_o italic_r italic_s  to 10 and using GMMs once).",
            "We present the results in Figures  13  and  14 .  We observe that adding Query Expansion to postQFRAP does not improve performance and, in fact, reduces the comprehensiveness and coherence of the generated answers.",
            "To create more challenging questions that require a broader understanding of the dataset, we take the following approach. First, we construct a RAPTOR tree on top of the dataset.  Then, to generate a new question, we sample a node from the tree and prompt a LLM to create a question based on the text from that node. We provide the LLM a few high quality questions to improve its output. The key idea is that some RAPTOR nodes contain summaries of various chunks, meaning the generated question requires synthesizing and summarizing information from different documents to be answered. The prompt used for generating these questions can be found in Table  3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Prompt for Text Summarization",
        "table": "A7.T4.1",
        "footnotes": [],
        "references": [
            "After clustering, a large language model generates summaries for each cluster, providing a concise overview of the content.  The summary length is limited to 1,000 tokens to ensure the summaries remain manageable.  The specific prompt used for summarization is provided in the appendix (Table  4 ).",
            "We present the results of an additional dataset in Appendix  H.4 .  Moreover, dataset sizes and an analysis of the recursive-abstractive trees constructed for each dataset are provided in Appendix  H.6 .",
            "We compare the adRAP algorithm against Naive RAG, RAPTOR, and a greedy variant of adRAP, which assigns each new point to its most probable cluster without updating the GMM fit.  To compute adRAP, we first construct a full tree using 70% of the dataset. The remaining 30% is added using the adRAP algorithm (Section  4.4 ) where we set   c = 11 subscript  c 11 \\tau_{c}=11 italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = 11  and   n = max  ( 100 , | D 0 | ) subscript  n 100 subscript D 0 \\tau_{n}=\\max(100,\\sqrt{|D_{0}|}) italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = roman_max ( 100 , square-root start_ARG | italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | end_ARG )  for Algorithm  2 .  The choice of   c subscript  c \\tau_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  is based on the average cluster size in the full RAPTOR tree, which is always less than 10 (see appendix, Table  12 ).  For the greedy variant, a similar procedure is used.  To simulate a challenging scenario, we remove the last 30% of documents instead of random sampling.",
            "Figure  2  shows that adRAPs performance is generally on par with RAPTOR across most metrics, with the exception of context relevance, where adRAP falls short by at least 3%.  However, adRAP outperforms both the naive RAG and the greedy algorithm, particularly in the QuALITY dataset.  These findings are further corroborated by the head-to-head evaluations in Figures  3 ,  4 , and  5 .  Notably, in the QuALITY dataset, adRAP exceeds RAPTOR in metrics such as comprehensiveness, diversity, and empowerment, despite its lower performance in context relevance.  On the other hand, adRAP underperforms compared to RAPTOR in the MultiHop and QASPER datasets.",
            "All hyperparameters and model configurations used in the experiments are clearly detailed in Sections  6.3  and  6.4 .",
            "We present the results in Figures  13  and  14 .  We observe that adding Query Expansion to postQFRAP does not improve performance and, in fact, reduces the comprehensiveness and coherence of the generated answers."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Prompt for Query-Focused Text Summarization",
        "table": "A7.T5.1",
        "footnotes": [],
        "references": [
            "We augment  R R \\mathcal{R} caligraphic_R  as follows.  First, we retrieve  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  documents without imposing a token limit.  Then, we apply a query-focused version of Algorithm  1  to these  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  documents to build a recursive-abstractive tree.  The key modification is using query-focused summarization (see prompt in Appendix, Table  5 ).  Since the tree is constructed to answer  q q q italic_q , summarizing information relevant to  q q q italic_q  ensures that key details are preserved while recursively filtering out irrelevant content.  Additionally, we modify the clustering to rely solely on local embeddings, as retrieving  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  documents already serves as a global filtering step.  In other words, we assume the retrieved documents belong to the same global cluster.  We demonstrate in Appendix  B  that using the simpler one-step clustering preserves the quality of the generated context compared to the two-step approach.",
            "Figure  2  shows that adRAPs performance is generally on par with RAPTOR across most metrics, with the exception of context relevance, where adRAP falls short by at least 3%.  However, adRAP outperforms both the naive RAG and the greedy algorithm, particularly in the QuALITY dataset.  These findings are further corroborated by the head-to-head evaluations in Figures  3 ,  4 , and  5 .  Notably, in the QuALITY dataset, adRAP exceeds RAPTOR in metrics such as comprehensiveness, diversity, and empowerment, despite its lower performance in context relevance.  On the other hand, adRAP underperforms compared to RAPTOR in the MultiHop and QASPER datasets.",
            "All four datasets used in our experiments are publicly available:  MultiHop ,  NarrativeQA ,  QuALITY , and  QASPER .  Details of the preprocessing steps are provided in Appendix  H.5 .",
            "Figures  15  and  16  show that, on the NarrativeQA dataset, adRAP performs comparably to RAPTOR and Greedy adRAP, while consistently outperforming Naive RAG."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Prompt for Computing Human Coherence Rating",
        "table": "A7.T6.1",
        "footnotes": [],
        "references": [
            "We present the results of an additional dataset in Appendix  H.4 .  Moreover, dataset sizes and an analysis of the recursive-abstractive trees constructed for each dataset are provided in Appendix  H.6 .",
            "Some generated answers may lack coherence, either in their internal structure or in relation to the question.  Providing summarized content as context may help the model generate more coherent responses.  To evaluate this, we introduce a novel metric called  Human Coherence Rating , which prompts an LLM to assess whether an answer is coherent and resembles one that could plausibly be generated by a human expert.  The specific prompt used for this evaluation is shown in the appendix (Table  6 ), with a qualitative analysis of the metric provided in Appendix  C .",
            "Figure  6  shows that algorithms with query-focused summarization consistently outperforms other approaches across all metrics.  While one-shot summarization scores slightly higher in answered questions and human coherence, postQFRAP excels in context relevance, demonstrating the effectiveness of recursive summarization in filtering noise from input chunks.  The superiority of postQFRAP as a post-retrieval algorithm becomes apparent in head-to-head evaluations. As shown in Figures  7 ,  8 , and  9 , postQFRAP excels in comprehensiveness, diversity, and empowerment.  The lower directness scores are expected, as directness often contrasts with these qualities, as noted by  Edge et al. ( 2024 ) .  Overall, postQFRAPs recursive extraction produces a more diverse, comprehensive, and empowering context, enhancing the quality of the final answer.",
            "All hyperparameters and model configurations used in the experiments are clearly detailed in Sections  6.3  and  6.4 .",
            "Figures  15  and  16  show that, on the NarrativeQA dataset, adRAP performs comparably to RAPTOR and Greedy adRAP, while consistently outperforming Naive RAG."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Prompt for Question Answering",
        "table": "A7.T7.1",
        "footnotes": [],
        "references": [
            "A key factor in the evaluation is the prompt used for the Question-Answering model.  To focus on the effectiveness of retrieval algorithms, we instruct the model to rely solely on the provided context. The full prompt is in the Appendix (Table  7 ).",
            "Figure  6  shows that algorithms with query-focused summarization consistently outperforms other approaches across all metrics.  While one-shot summarization scores slightly higher in answered questions and human coherence, postQFRAP excels in context relevance, demonstrating the effectiveness of recursive summarization in filtering noise from input chunks.  The superiority of postQFRAP as a post-retrieval algorithm becomes apparent in head-to-head evaluations. As shown in Figures  7 ,  8 , and  9 , postQFRAP excels in comprehensiveness, diversity, and empowerment.  The lower directness scores are expected, as directness often contrasts with these qualities, as noted by  Edge et al. ( 2024 ) .  Overall, postQFRAPs recursive extraction produces a more diverse, comprehensive, and empowering context, enhancing the quality of the final answer.",
            "Figure  17  demonstrates that query-focused algorithms clearly outperform the baselines on the NarrativeQA dataset.  Notably, postQFRAP and one-shot summarization achieve comparable results.  However, as shown in Figure  18 , postQFRAP continues to significantly outperform all other algorithms in terms of comprehensiveness, diversity, and empowerment of the generated answers."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Prompt for One-Shot Context Summarization  [Zhang et al.,  2024 ]",
        "table": "A7.T8.1",
        "footnotes": [],
        "references": [
            "We compare postQFRAP with other post-retrieval methods: no processing (naive RAG) with  k = 7 , 20 k 7 20 k=7,20 italic_k = 7 , 20  retrieved documents, one-shot summarization, re-ranking, and postRAP.  One-shot summarization uses the controller from  Zhang et al. ( 2024 )  to directly generate 2,000 tokens (see prompt in Appendix, Table  8 ).  For re-ranking, we use the  ms-marco-MiniLM-L-12-v2 4 4 4 https://huggingface.co/cross-encoder/ms-marco-MiniLM-L-12-v2  cross-encoder from HuggingFace, retrieving 20 documents via naive RAG, then re-ranking them to keep the top 7.  Finally, postRAP is a variant of postQFRAP without query-focused summarization which retrieves the top- k k k italic_k  most similar chunks from the tree built on the  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  chunks.",
            "Figure  6  shows that algorithms with query-focused summarization consistently outperforms other approaches across all metrics.  While one-shot summarization scores slightly higher in answered questions and human coherence, postQFRAP excels in context relevance, demonstrating the effectiveness of recursive summarization in filtering noise from input chunks.  The superiority of postQFRAP as a post-retrieval algorithm becomes apparent in head-to-head evaluations. As shown in Figures  7 ,  8 , and  9 , postQFRAP excels in comprehensiveness, diversity, and empowerment.  The lower directness scores are expected, as directness often contrasts with these qualities, as noted by  Edge et al. ( 2024 ) .  Overall, postQFRAPs recursive extraction produces a more diverse, comprehensive, and empowering context, enhancing the quality of the final answer.",
            "Figure  17  demonstrates that query-focused algorithms clearly outperform the baselines on the NarrativeQA dataset.  Notably, postQFRAP and one-shot summarization achieve comparable results.  However, as shown in Figure  18 , postQFRAP continues to significantly outperform all other algorithms in terms of comprehensiveness, diversity, and empowerment of the generated answers."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Prompt for Q2D/PRF Query Expansion",
        "table": "A7.T9.1",
        "footnotes": [],
        "references": [
            "Figure  6  shows that algorithms with query-focused summarization consistently outperforms other approaches across all metrics.  While one-shot summarization scores slightly higher in answered questions and human coherence, postQFRAP excels in context relevance, demonstrating the effectiveness of recursive summarization in filtering noise from input chunks.  The superiority of postQFRAP as a post-retrieval algorithm becomes apparent in head-to-head evaluations. As shown in Figures  7 ,  8 , and  9 , postQFRAP excels in comprehensiveness, diversity, and empowerment.  The lower directness scores are expected, as directness often contrasts with these qualities, as noted by  Edge et al. ( 2024 ) .  Overall, postQFRAPs recursive extraction produces a more diverse, comprehensive, and empowering context, enhancing the quality of the final answer.",
            "where  LLM  ( prompt q ) LLM subscript prompt q \\text{LLM}(\\text{prompt}_{q}) LLM ( prompt start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT )  is the output of the Q2E/PRF prompt. The latter is found in Table  9 .  Then we use postQFRAP as before, by replacing  q q q italic_q  with the augmented query  q  superscript q  q^{\\prime} italic_q start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Comparison of time taken and the number of summary calls between building a tree on 70% of the dataset followed by adRAP, and computing the full tree twice: once on 70% of the dataset and again on the entire dataset.",
        "table": "A8.T10.1.1",
        "footnotes": [],
        "references": [
            "As shown in Figures  10 ,the results across all four datasets indicate that the difference between one-step and two-step clustering is minimal.  Furthermore, Figure  11  demonstrates that one-step clustering performs better in QASPER and QuALITY, worse in NarrativeQA, and is comparable to two-step clustering in MultiHop.  Overall, the differences in performance between the two algorithms are minor.  Based on these observations, we adopt the simpler and more efficient one-step clustering method in our algorithm.",
            "We present in Table  10  the time taken and the number of summary calls made on each of the QASPER and QuALITY dataset for two different algorithms:"
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Sizes of the used datasets",
        "table": "A8.T10.1.1.1.1.2.1",
        "footnotes": [],
        "references": [
            "As shown in Figures  10 ,the results across all four datasets indicate that the difference between one-step and two-step clustering is minimal.  Furthermore, Figure  11  demonstrates that one-step clustering performs better in QASPER and QuALITY, worse in NarrativeQA, and is comparable to two-step clustering in MultiHop.  Overall, the differences in performance between the two algorithms are minor.  Based on these observations, we adopt the simpler and more efficient one-step clustering method in our algorithm.",
            "In Table  11 , we present the sizes of the datasets used in our experiments.  In Table  12 , we present various statistics regarding the recursive-abstractive trees constructed from our datasets. The number of internal nodes is approximately  n / 6 n 6 n/6 italic_n / 6 , where  n n n italic_n  represents the total number of chunks in the dataset. Additionally, most cluster sizes range between 4 and 15, with nodes rarely belonging to more than one cluster."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  Statistics of the different recursive-abstractive trees we construct on the datasets",
        "table": "A8.T10.1.1.1.1.3.1",
        "footnotes": [],
        "references": [
            "We compare the adRAP algorithm against Naive RAG, RAPTOR, and a greedy variant of adRAP, which assigns each new point to its most probable cluster without updating the GMM fit.  To compute adRAP, we first construct a full tree using 70% of the dataset. The remaining 30% is added using the adRAP algorithm (Section  4.4 ) where we set   c = 11 subscript  c 11 \\tau_{c}=11 italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT = 11  and   n = max  ( 100 , | D 0 | ) subscript  n 100 subscript D 0 \\tau_{n}=\\max(100,\\sqrt{|D_{0}|}) italic_ start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT = roman_max ( 100 , square-root start_ARG | italic_D start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT | end_ARG )  for Algorithm  2 .  The choice of   c subscript  c \\tau_{c} italic_ start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT  is based on the average cluster size in the full RAPTOR tree, which is always less than 10 (see appendix, Table  12 ).  For the greedy variant, a similar procedure is used.  To simulate a challenging scenario, we remove the last 30% of documents instead of random sampling.",
            "To determine the optimal value of  k 0 subscript k 0 k_{0} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  for our main experiments, we evaluated five different values,  k 0  { 10 , 20 , 40 , 60 , 80 } subscript k 0 10 20 40 60 80 k_{0}\\in\\{10,20,40,60,80\\} italic_k start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  { 10 , 20 , 40 , 60 , 80 } , by comparing the performance on 100 questions from the validation sets of the NarrativeQA and QASPER datasets. It is important to emphasize that these questions are entirely distinct from the data used in our main experiments.  As in our primary experiments, we limited the final summary size to 2,000 tokens and considered three metrics: the number of answered questions, context relevance, and human coherence.  To account for the non-deterministic nature of the LLM evaluators, we repeated the evaluation process three times and reported the average performance along with the standard error in Figure  12 .",
            "In Table  11 , we present the sizes of the datasets used in our experiments.  In Table  12 , we present various statistics regarding the recursive-abstractive trees constructed from our datasets. The number of internal nodes is approximately  n / 6 n 6 n/6 italic_n / 6 , where  n n n italic_n  represents the total number of chunks in the dataset. Additionally, most cluster sizes range between 4 and 15, with nodes rarely belonging to more than one cluster."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A8.T10.1.1.1.1.4.1",
        "footnotes": [],
        "references": [
            "We present the results in Figures  13  and  14 .  We observe that adding Query Expansion to postQFRAP does not improve performance and, in fact, reduces the comprehensiveness and coherence of the generated answers."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A8.T10.1.1.1.1.5.1",
        "footnotes": [],
        "references": [
            "We present the results in Figures  13  and  14 .  We observe that adding Query Expansion to postQFRAP does not improve performance and, in fact, reduces the comprehensiveness and coherence of the generated answers."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A8.T11.1",
        "footnotes": [],
        "references": [
            "Figures  15  and  16  show that, on the NarrativeQA dataset, adRAP performs comparably to RAPTOR and Greedy adRAP, while consistently outperforming Naive RAG."
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "A8.T12.8",
        "footnotes": [],
        "references": [
            "Figures  15  and  16  show that, on the NarrativeQA dataset, adRAP performs comparably to RAPTOR and Greedy adRAP, while consistently outperforming Naive RAG."
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "A8.T12.8.9.1.3.1",
        "footnotes": [],
        "references": [
            "Figure  17  demonstrates that query-focused algorithms clearly outperform the baselines on the NarrativeQA dataset.  Notably, postQFRAP and one-shot summarization achieve comparable results.  However, as shown in Figure  18 , postQFRAP continues to significantly outperform all other algorithms in terms of comprehensiveness, diversity, and empowerment of the generated answers."
        ]
    },
    "id_table_18": {
        "caption": "",
        "table": "A8.T12.8.9.1.5.1",
        "footnotes": [],
        "references": [
            "Figure  17  demonstrates that query-focused algorithms clearly outperform the baselines on the NarrativeQA dataset.  Notably, postQFRAP and one-shot summarization achieve comparable results.  However, as shown in Figure  18 , postQFRAP continues to significantly outperform all other algorithms in terms of comprehensiveness, diversity, and empowerment of the generated answers."
        ]
    },
    "global_footnotes": []
}