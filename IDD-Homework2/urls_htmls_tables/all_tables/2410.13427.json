{
    "id_table_1": {
        "caption": "Table 1 :  Comparison of methods with DSC and SDSC, evaluated on the subset of SynthRAD 2023 training dataset  [ 36 ] , extracted for a test set in this work.",
        "table": "Pt0.Ax1.EGx1",
        "footnotes": [
            ""
        ],
        "references": [
            "The two main components of our solution are the Contrastive Unpaired Translation (CUT)  [ 25 ]  module and the Laplacian Pyramid Super-Resolution Network (LapSRN)  [ 19 ]  module, both utilized for 3-D volumetric data processing. They work separately and use different techniques for sampling the data from the whole space of gathered datasets. CUT is responsible for the MR-to-CT translation in an unpaired manner, it takes unpaired samples and tries to generate a synthetic CT image for the corresponding MR image without a vision of a direct mapping. LapSRN is trained directly and only on high-resolution CT images to create a pyramidal structure of increasing resolutions, with the usage of extreme data augmentation (which we discuss further) to enhance the generalization capabilities of the model. The full architecture is presented in Figure  2  and the inference process is presented in Figure  1 .",
            "We evaluate the proposed method on the subset of samples extracted from the training dataset from SynthRAD 2023 Challenge  [ 36 ]  as it consists of paired MR and CT images. We present the Dice coefficient (DSC) and surface Dice coefficient (SDSC) results, calculated from segmentation masks obtained using Hounsfield scale thresholding in Table  1 . Importantly, it should be noted that, as previously mentioned, skull stripping differs from skull segmentation. The results of skull stripping methods were followed by postprocessing, which involved manual thresholding for skull subtraction. Furthermore, we demonstrate that the medical imaging foundation model, MedSAM  [ 21 ] , fails to perform the complex task of skull segmentation from MR images. The failed results are presented in Figure  4(a)  for the bounding box approach and Figure  4(b)  for the experimental point prompt approach. We investigate the quality of generated synthetic CT via a small set of ablation studies which are presented in Figure  5 . They showcase the following settings:  ( i ) i (i) ( italic_i )  the performance of LapSRN using a test sample from the CUT module,  ( i  i ) i i (ii) ( italic_i italic_i )  generalization capabilities to the downstream task of translation on the MR image of the childs skull, and  ( i  i  i ) i i i (iii) ( italic_i italic_i italic_i )  the translation of the MR image of a defected (and reconstructed) skull. Finally, to demonstrate the generalization potential of the proposed solution from a quantitative perspective, we investigate additionally the MR subset of the HaN-Seg dataset  [ 28 ]  which was not present in the original training dataset. As the availability of paired MR and CT datasets is highly limited, besides SynthRAD 2023 dataset  [ 36 ] , we find HaN-Seg to be the most suitable option for this evaluation. Importantly, additional preprocessing in the form of image registration between CT and MR samples in this dataset was required to enable the quantitative evaluation. For comparison, we decided to train a state-of-the-art segmentation network, SwinUNETR  [ 10 ] , on the SynthRAD 2023 dataset  [ 36 ] , where the input was an MR image and the target was a skull mask derived from Hounsfield thresholding of its paired CT image. We also trained the model as a standard paired MR-to-CT translation and applied the same Hounsfield thresholding on the resulting synthetic CTs for consistency with the pipeline from Figure  1 . Table  2  shows that the proposed solution presents superior results in terms of the generalization capabilities in comparison to both SwinUNETR setups.",
            "Quantitative analysis of the CUT method shows that it can provide better results in terms of DSC and SDSC than the combination of skull stripping with subtraction and masking as shown in Table  1 . Importantly, we find out that MedSAM  [ 21 ]  struggles with the skull segmentation in MR images and fails to propagate the segmentation mask in both bounding-box and point-prompt (experimental) approaches. This can be potentially solved via task-specific fine-tuning (as the authors mention this MedSAM capability in their work). Good results of our method suggest that it is a promising direction for designing a fully unsupervised framework of skull segmentation from MR images, that can be used for downstream tasks (such as defect segmentation), and with the use of super-resolution LapSRN module, the requirement of high-resolution CT images for tasks like craniectomy or surgery planning is also met. Whats more, both CUT and LapSRN are relatively simple in terms of architectural design and also their learning objectives, hence they can be adapted to other translation tasks, or fine-tuned for other anatomical structures. With CUT we were able to obtain good generalization capabilities due to the very high diversity of the used dataset. Importantly, we also note several limitations of our methodology and leave it as a potential direction for further work. First of all, produced results sometimes include some blurriness; this can be attributed to using convolutional networks with a relatively small number of filters and a shallow network depth. This issue stems from the challenges of designing large networks for high-resolution volumetric data. Even the latest GPUs struggle with VRAM capacities when trying to fit large tensors and extensive 3-D networks. Secondly, while DSC results are better than other methods, they still require improvement to meet the demands of potential real-world medical applications. Finally, the primary motivation of this work was to achieve generalization capabilities for the segmentation task in an unsupervised and unpaired manner. Therefore, we do not compare our results with other methods used in MR-to-CT translation, as these are not primarily focused on segmentation. Nevertheless, we acknowledge that some existing translation methods may be more effective than CUT, and other super-resolution modules could outperform LapSRN. We consider this a potential area for further improvement, research, and investigation."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Evaluation of the generalization capabilities on the HaN-Seg dataset  [ 28 ]  in terms of DSC and SDSC.",
        "table": "Pt0.Ax1.EGx2",
        "footnotes": [
            ""
        ],
        "references": [
            "The two main components of our solution are the Contrastive Unpaired Translation (CUT)  [ 25 ]  module and the Laplacian Pyramid Super-Resolution Network (LapSRN)  [ 19 ]  module, both utilized for 3-D volumetric data processing. They work separately and use different techniques for sampling the data from the whole space of gathered datasets. CUT is responsible for the MR-to-CT translation in an unpaired manner, it takes unpaired samples and tries to generate a synthetic CT image for the corresponding MR image without a vision of a direct mapping. LapSRN is trained directly and only on high-resolution CT images to create a pyramidal structure of increasing resolutions, with the usage of extreme data augmentation (which we discuss further) to enhance the generalization capabilities of the model. The full architecture is presented in Figure  2  and the inference process is presented in Figure  1 .",
            "The idea of comparison behind Equation  2  and  3  is presented in Figure  3 . Moreover, the CUT methodology broadens the mentioned InfoNCE loss components to the intermediate outputs of  G enc subscript G enc G_{\\text{enc}} italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT  layers. This ensures that the features learned at multiple levels are aligned and collectively enhance the quality and consistency of the final generated output. If we denote the layers of the encoding part of  G G G italic_G  ( G enc subscript G enc G_{\\text{enc}} italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT ) as  L L L italic_L , then at the  l l l italic_l -th layer, we obtain a set of features for the real MR and real CT images as follows:  { z real  ( l ) MR } L = { F l  ( G enc l  ( x real MR ) ) } L subscript superscript subscript z real l MR L subscript subscript F l superscript subscript G enc l superscript subscript x real MR L \\{\\mathbf{z}_{\\text{real}(l)}^{\\text{MR}}\\}_{L}=\\{F_{l}(G_{\\text{enc}}^{l}(% \\mathbf{x}_{\\text{real}}^{\\text{MR}}))\\}_{L} { bold_z start_POSTSUBSCRIPT real ( italic_l ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT MR end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = { italic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( bold_x start_POSTSUBSCRIPT real end_POSTSUBSCRIPT start_POSTSUPERSCRIPT MR end_POSTSUPERSCRIPT ) ) } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT  and  { z real  ( l ) CT } L = { F l  ( G enc l  ( y real CT ) ) } L subscript superscript subscript z real l CT L subscript subscript F l superscript subscript G enc l superscript subscript y real CT L \\{\\mathbf{z}_{\\text{real}(l)}^{\\text{CT}}\\}_{L}=\\{F_{l}(G_{\\text{enc}}^{l}(% \\mathbf{y}_{\\text{real}}^{\\text{CT}}))\\}_{L} { bold_z start_POSTSUBSCRIPT real ( italic_l ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = { italic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT real end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT ) ) } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT . If we also address the synthetic and identity representations we obtain the following sets:  { z syn  ( l ) CT } L = { F l  ( G enc l  ( G  ( x real MR ) ) ) } L subscript superscript subscript z syn l CT L subscript subscript F l superscript subscript G enc l G superscript subscript x real MR L \\{\\mathbf{z}_{\\text{syn}(l)}^{\\text{CT}}\\}_{L}=\\{F_{l}(G_{\\text{enc}}^{l}(G(% \\mathbf{x}_{\\text{real}}^{\\text{MR}})))\\}_{L} { bold_z start_POSTSUBSCRIPT syn ( italic_l ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = { italic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( italic_G ( bold_x start_POSTSUBSCRIPT real end_POSTSUBSCRIPT start_POSTSUPERSCRIPT MR end_POSTSUPERSCRIPT ) ) ) } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT  and  { z idt  ( l ) CT } L = { F l  ( G enc l  ( G  ( y real CT ) ) ) } L subscript superscript subscript z idt l CT L subscript subscript F l superscript subscript G enc l G superscript subscript y real CT L \\{\\mathbf{z}_{\\text{idt}(l)}^{\\text{CT}}\\}_{L}=\\{F_{l}(G_{\\text{enc}}^{l}(G(% \\mathbf{y}_{\\text{real}}^{\\text{CT}})))\\}_{L} { bold_z start_POSTSUBSCRIPT idt ( italic_l ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = { italic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( italic_G ( bold_y start_POSTSUBSCRIPT real end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT ) ) ) } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT . Now, as we take  S l subscript S l S_{l} italic_S start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  patches at layer  l l l italic_l  we have a set of  S / s S s S/s italic_S / italic_s  negative patches  (  ) (-) ( - )  and  s s s italic_s  positive patch  ( + ) (+) ( + ) , and we are able to formulate PatchNCE loss in two setups, the first one as the main training objective of synthesizing CT images from MR inputs:",
            "The second component of the designed pipeline is the modification of the original LapSRN  [ 19 ]  used for super-resolution of 3-D CT volumes. It uses two sub-networks: feature extractor and image reconstructor, which progressively generate images at higher resolutions from the low-resolution ones. The feature extractor is responsible for extracting the features at a coarse (lower) level and generating feature maps at a finer (higher) level. It enhances the representation of the input by capturing important details. The image reconstructor upsamples the lower-resolution input and then via element-wise summation with residuals obtained from the feature extractor creates a higher-resolution output with improved visual quality. By its nature, progressive reconstruction provides task-dependent flexibility and adjustability, as by bypassing the pyramid at certain levels we can obtain representations at the resolution required for a given task. Furthermore, to address the requirement of high generalization capabilities, our LapSRN uses not only a set of diverse data augmentations (flipping, affine transformations, motion artifacts, blurriness, contrast), but is also trained on overlapping chunks of input tensors (see Figure  2 ). This configuration enhances the models generalization capabilities, enabling it to effectively super-resolve single chunks and capture inconsistencies in various skulls, such as those with defects. Additionally, it significantly reduces memory requirements during training, as the chunks can be processed individually or combined into small micro-batches within the mini-batch. LapSRN uses Charbonnier loss, where for  M M M italic_M  chunks of the input tensor, and pyramid of  L L L italic_L  levels, the Charbonnier loss is defined as:",
            "We evaluate the proposed method on the subset of samples extracted from the training dataset from SynthRAD 2023 Challenge  [ 36 ]  as it consists of paired MR and CT images. We present the Dice coefficient (DSC) and surface Dice coefficient (SDSC) results, calculated from segmentation masks obtained using Hounsfield scale thresholding in Table  1 . Importantly, it should be noted that, as previously mentioned, skull stripping differs from skull segmentation. The results of skull stripping methods were followed by postprocessing, which involved manual thresholding for skull subtraction. Furthermore, we demonstrate that the medical imaging foundation model, MedSAM  [ 21 ] , fails to perform the complex task of skull segmentation from MR images. The failed results are presented in Figure  4(a)  for the bounding box approach and Figure  4(b)  for the experimental point prompt approach. We investigate the quality of generated synthetic CT via a small set of ablation studies which are presented in Figure  5 . They showcase the following settings:  ( i ) i (i) ( italic_i )  the performance of LapSRN using a test sample from the CUT module,  ( i  i ) i i (ii) ( italic_i italic_i )  generalization capabilities to the downstream task of translation on the MR image of the childs skull, and  ( i  i  i ) i i i (iii) ( italic_i italic_i italic_i )  the translation of the MR image of a defected (and reconstructed) skull. Finally, to demonstrate the generalization potential of the proposed solution from a quantitative perspective, we investigate additionally the MR subset of the HaN-Seg dataset  [ 28 ]  which was not present in the original training dataset. As the availability of paired MR and CT datasets is highly limited, besides SynthRAD 2023 dataset  [ 36 ] , we find HaN-Seg to be the most suitable option for this evaluation. Importantly, additional preprocessing in the form of image registration between CT and MR samples in this dataset was required to enable the quantitative evaluation. For comparison, we decided to train a state-of-the-art segmentation network, SwinUNETR  [ 10 ] , on the SynthRAD 2023 dataset  [ 36 ] , where the input was an MR image and the target was a skull mask derived from Hounsfield thresholding of its paired CT image. We also trained the model as a standard paired MR-to-CT translation and applied the same Hounsfield thresholding on the resulting synthetic CTs for consistency with the pipeline from Figure  1 . Table  2  shows that the proposed solution presents superior results in terms of the generalization capabilities in comparison to both SwinUNETR setups."
        ]
    },
    "id_table_3": {
        "caption": "Table:  Information about datasets used in training of CUT and LapSRN modules.",
        "table": "Pt0.Ax1.EGx3",
        "footnotes": [],
        "references": [
            "The idea of comparison behind Equation  2  and  3  is presented in Figure  3 . Moreover, the CUT methodology broadens the mentioned InfoNCE loss components to the intermediate outputs of  G enc subscript G enc G_{\\text{enc}} italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT  layers. This ensures that the features learned at multiple levels are aligned and collectively enhance the quality and consistency of the final generated output. If we denote the layers of the encoding part of  G G G italic_G  ( G enc subscript G enc G_{\\text{enc}} italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT ) as  L L L italic_L , then at the  l l l italic_l -th layer, we obtain a set of features for the real MR and real CT images as follows:  { z real  ( l ) MR } L = { F l  ( G enc l  ( x real MR ) ) } L subscript superscript subscript z real l MR L subscript subscript F l superscript subscript G enc l superscript subscript x real MR L \\{\\mathbf{z}_{\\text{real}(l)}^{\\text{MR}}\\}_{L}=\\{F_{l}(G_{\\text{enc}}^{l}(% \\mathbf{x}_{\\text{real}}^{\\text{MR}}))\\}_{L} { bold_z start_POSTSUBSCRIPT real ( italic_l ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT MR end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = { italic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( bold_x start_POSTSUBSCRIPT real end_POSTSUBSCRIPT start_POSTSUPERSCRIPT MR end_POSTSUPERSCRIPT ) ) } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT  and  { z real  ( l ) CT } L = { F l  ( G enc l  ( y real CT ) ) } L subscript superscript subscript z real l CT L subscript subscript F l superscript subscript G enc l superscript subscript y real CT L \\{\\mathbf{z}_{\\text{real}(l)}^{\\text{CT}}\\}_{L}=\\{F_{l}(G_{\\text{enc}}^{l}(% \\mathbf{y}_{\\text{real}}^{\\text{CT}}))\\}_{L} { bold_z start_POSTSUBSCRIPT real ( italic_l ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = { italic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( bold_y start_POSTSUBSCRIPT real end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT ) ) } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT . If we also address the synthetic and identity representations we obtain the following sets:  { z syn  ( l ) CT } L = { F l  ( G enc l  ( G  ( x real MR ) ) ) } L subscript superscript subscript z syn l CT L subscript subscript F l superscript subscript G enc l G superscript subscript x real MR L \\{\\mathbf{z}_{\\text{syn}(l)}^{\\text{CT}}\\}_{L}=\\{F_{l}(G_{\\text{enc}}^{l}(G(% \\mathbf{x}_{\\text{real}}^{\\text{MR}})))\\}_{L} { bold_z start_POSTSUBSCRIPT syn ( italic_l ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = { italic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( italic_G ( bold_x start_POSTSUBSCRIPT real end_POSTSUBSCRIPT start_POSTSUPERSCRIPT MR end_POSTSUPERSCRIPT ) ) ) } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT  and  { z idt  ( l ) CT } L = { F l  ( G enc l  ( G  ( y real CT ) ) ) } L subscript superscript subscript z idt l CT L subscript subscript F l superscript subscript G enc l G superscript subscript y real CT L \\{\\mathbf{z}_{\\text{idt}(l)}^{\\text{CT}}\\}_{L}=\\{F_{l}(G_{\\text{enc}}^{l}(G(% \\mathbf{y}_{\\text{real}}^{\\text{CT}})))\\}_{L} { bold_z start_POSTSUBSCRIPT idt ( italic_l ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT = { italic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ( italic_G ( bold_y start_POSTSUBSCRIPT real end_POSTSUBSCRIPT start_POSTSUPERSCRIPT CT end_POSTSUPERSCRIPT ) ) ) } start_POSTSUBSCRIPT italic_L end_POSTSUBSCRIPT . Now, as we take  S l subscript S l S_{l} italic_S start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT  patches at layer  l l l italic_l  we have a set of  S / s S s S/s italic_S / italic_s  negative patches  (  ) (-) ( - )  and  s s s italic_s  positive patch  ( + ) (+) ( + ) , and we are able to formulate PatchNCE loss in two setups, the first one as the main training objective of synthesizing CT images from MR inputs:"
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "Pt0.Ax1.EGx4",
        "footnotes": [],
        "references": [
            "We implement the proposed networks with the use of PyTorch library  [ 26 ] . CUT and LapSRN subnetworks use 3-D convolutional layers. Whats important, for the CUT model we use instance normalization  [ 37 ]  as it prevents instance-specific mean and covariance shifts, hence it is highly beneficial for modality translation tasks. CUTs generator is a 9-layer ResNet-based network, with residual connections between downsampling and upsampling blocks, it uses instance normalization and ReLU nonlinearity besides the decision layer for which we use hyperbolic tangent. The discriminator is a 3-layer convolutional network with Leaky ReLU and instance normalization (like PatchGAN discriminator  [ 13 ] ). CUTs feature extractor is a simple 2-layer multilayer perceptron with ReLU which operates on 64 patches of features extracted from the generators flow. Hence, regarding Equation  4  and Equation  5 , we operate on a fixed amount of patches equal to 64, and a fixed amount of  G enc subscript G enc G_{\\text{enc}} italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT  layers equal to 9. LapSRNs image reconstructor is a 2-layer convolution/deconvolution network and the feature extractor is an 8-layer convolution/deconvolution network with Leaky ReLU nonlinearities. LapSRNs convolutional layers use 3  \\times  3  \\times  3 kernels with 64 filters with He initialization  [ 11 ] , deconvolutions use the kernel of 4  \\times  4  \\times  4 (upsampling by a factor of 2) and weights are initialized from a trilinear filter, as suggested in the original implementation  [ 19 ] .",
            "We evaluate the proposed method on the subset of samples extracted from the training dataset from SynthRAD 2023 Challenge  [ 36 ]  as it consists of paired MR and CT images. We present the Dice coefficient (DSC) and surface Dice coefficient (SDSC) results, calculated from segmentation masks obtained using Hounsfield scale thresholding in Table  1 . Importantly, it should be noted that, as previously mentioned, skull stripping differs from skull segmentation. The results of skull stripping methods were followed by postprocessing, which involved manual thresholding for skull subtraction. Furthermore, we demonstrate that the medical imaging foundation model, MedSAM  [ 21 ] , fails to perform the complex task of skull segmentation from MR images. The failed results are presented in Figure  4(a)  for the bounding box approach and Figure  4(b)  for the experimental point prompt approach. We investigate the quality of generated synthetic CT via a small set of ablation studies which are presented in Figure  5 . They showcase the following settings:  ( i ) i (i) ( italic_i )  the performance of LapSRN using a test sample from the CUT module,  ( i  i ) i i (ii) ( italic_i italic_i )  generalization capabilities to the downstream task of translation on the MR image of the childs skull, and  ( i  i  i ) i i i (iii) ( italic_i italic_i italic_i )  the translation of the MR image of a defected (and reconstructed) skull. Finally, to demonstrate the generalization potential of the proposed solution from a quantitative perspective, we investigate additionally the MR subset of the HaN-Seg dataset  [ 28 ]  which was not present in the original training dataset. As the availability of paired MR and CT datasets is highly limited, besides SynthRAD 2023 dataset  [ 36 ] , we find HaN-Seg to be the most suitable option for this evaluation. Importantly, additional preprocessing in the form of image registration between CT and MR samples in this dataset was required to enable the quantitative evaluation. For comparison, we decided to train a state-of-the-art segmentation network, SwinUNETR  [ 10 ] , on the SynthRAD 2023 dataset  [ 36 ] , where the input was an MR image and the target was a skull mask derived from Hounsfield thresholding of its paired CT image. We also trained the model as a standard paired MR-to-CT translation and applied the same Hounsfield thresholding on the resulting synthetic CTs for consistency with the pipeline from Figure  1 . Table  2  shows that the proposed solution presents superior results in terms of the generalization capabilities in comparison to both SwinUNETR setups."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "Pt0.Ax1.EGx5",
        "footnotes": [],
        "references": [
            "We implement the proposed networks with the use of PyTorch library  [ 26 ] . CUT and LapSRN subnetworks use 3-D convolutional layers. Whats important, for the CUT model we use instance normalization  [ 37 ]  as it prevents instance-specific mean and covariance shifts, hence it is highly beneficial for modality translation tasks. CUTs generator is a 9-layer ResNet-based network, with residual connections between downsampling and upsampling blocks, it uses instance normalization and ReLU nonlinearity besides the decision layer for which we use hyperbolic tangent. The discriminator is a 3-layer convolutional network with Leaky ReLU and instance normalization (like PatchGAN discriminator  [ 13 ] ). CUTs feature extractor is a simple 2-layer multilayer perceptron with ReLU which operates on 64 patches of features extracted from the generators flow. Hence, regarding Equation  4  and Equation  5 , we operate on a fixed amount of patches equal to 64, and a fixed amount of  G enc subscript G enc G_{\\text{enc}} italic_G start_POSTSUBSCRIPT enc end_POSTSUBSCRIPT  layers equal to 9. LapSRNs image reconstructor is a 2-layer convolution/deconvolution network and the feature extractor is an 8-layer convolution/deconvolution network with Leaky ReLU nonlinearities. LapSRNs convolutional layers use 3  \\times  3  \\times  3 kernels with 64 filters with He initialization  [ 11 ] , deconvolutions use the kernel of 4  \\times  4  \\times  4 (upsampling by a factor of 2) and weights are initialized from a trilinear filter, as suggested in the original implementation  [ 19 ] .",
            "We evaluate the proposed method on the subset of samples extracted from the training dataset from SynthRAD 2023 Challenge  [ 36 ]  as it consists of paired MR and CT images. We present the Dice coefficient (DSC) and surface Dice coefficient (SDSC) results, calculated from segmentation masks obtained using Hounsfield scale thresholding in Table  1 . Importantly, it should be noted that, as previously mentioned, skull stripping differs from skull segmentation. The results of skull stripping methods were followed by postprocessing, which involved manual thresholding for skull subtraction. Furthermore, we demonstrate that the medical imaging foundation model, MedSAM  [ 21 ] , fails to perform the complex task of skull segmentation from MR images. The failed results are presented in Figure  4(a)  for the bounding box approach and Figure  4(b)  for the experimental point prompt approach. We investigate the quality of generated synthetic CT via a small set of ablation studies which are presented in Figure  5 . They showcase the following settings:  ( i ) i (i) ( italic_i )  the performance of LapSRN using a test sample from the CUT module,  ( i  i ) i i (ii) ( italic_i italic_i )  generalization capabilities to the downstream task of translation on the MR image of the childs skull, and  ( i  i  i ) i i i (iii) ( italic_i italic_i italic_i )  the translation of the MR image of a defected (and reconstructed) skull. Finally, to demonstrate the generalization potential of the proposed solution from a quantitative perspective, we investigate additionally the MR subset of the HaN-Seg dataset  [ 28 ]  which was not present in the original training dataset. As the availability of paired MR and CT datasets is highly limited, besides SynthRAD 2023 dataset  [ 36 ] , we find HaN-Seg to be the most suitable option for this evaluation. Importantly, additional preprocessing in the form of image registration between CT and MR samples in this dataset was required to enable the quantitative evaluation. For comparison, we decided to train a state-of-the-art segmentation network, SwinUNETR  [ 10 ] , on the SynthRAD 2023 dataset  [ 36 ] , where the input was an MR image and the target was a skull mask derived from Hounsfield thresholding of its paired CT image. We also trained the model as a standard paired MR-to-CT translation and applied the same Hounsfield thresholding on the resulting synthetic CTs for consistency with the pipeline from Figure  1 . Table  2  shows that the proposed solution presents superior results in terms of the generalization capabilities in comparison to both SwinUNETR setups."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "Pt0.Ax1.EGx6",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "",
        "table": "S4.T1.2",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": []
    },
    "id_table_8": {
        "caption": "",
        "table": "S4.T2.2",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "Pt0.Ax1.tab1.2",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": []
    }
}