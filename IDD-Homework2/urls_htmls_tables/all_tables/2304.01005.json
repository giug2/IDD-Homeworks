{
    "PAPER'S NUMBER OF TABLES": 16,
    "S3.T1": {
        "caption": "Table 1: Multilingual Centralized and Federated with 4 clients Accuracy and Macro-F1 Scores for SemEval test dataset.",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Metrics</th>\n<th id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Baseline</th>\n<th id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">IID</th>\n<th id=\"S3.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Finetuned</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Accuracy</td>\n<td id=\"S3.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">43.4%</td>\n<td id=\"S3.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">45.1%</td>\n<td id=\"S3.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.8%</td>\n</tr>\n<tr id=\"S3.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">Macro-F1</td>\n<td id=\"S3.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">27.6%</td>\n<td id=\"S3.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">29.2%</td>\n<td id=\"S3.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">29.1%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Similar to the experiment in Table 8. Table 9 shows that Krum outperforms FedAVG in both the Non-IID and IID settings as well. Also in Table 10 reports the results of applying label flipping to the German Zero-shot scenario,",
            "Table 11 provides valuable insights into the performance of a Bert-Base model in a federated multilingual setting under both clean and attack scenarios. The results highlight the impact of toxic clients on the federated setting, showing a decrease in performance for FedAVG, Fed-IID, and Fed-Non-IID. This decrease in performance is particularly evident when the toxic data constitutes 50% of the overall data. However, the use of the Krum aggregation function can mitigate this drop in performance, as previously observed in literature.",
            "Table 13 shows the estimated amount of data transmitted during one epoch of federated learning using different models. For the M-MiniLM model, the estimated amount of data transmitted per client for one epoch would be 2.35 GB, which is within the resource constraints for modern devices.",
            "Table 16 presents a comparison between centralized and federated learning approaches using FedAVG and Krum aggregation functions in a label-flipping attack scenario for three different models. We observed that Krum outperforms FedAVG in all cases, indicating that the choice of aggregation function has a significant impact on the model’s performance. However, we also found that the model’s performance is heavily dependent on the architecture and the type of aggregation function used."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Unilingual Centralized and Federated Macro-F1 Accuracy Scores for SemEval test dataset.",
        "table": "<table id=\"S3.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Baseline</th>\n<th id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">IID</th>\n<th id=\"S3.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Finetuned</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Switch-Base-8</td>\n<td id=\"S3.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">33.2%</td>\n<td id=\"S3.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">37.3%</td>\n<td id=\"S3.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.6%</td>\n</tr>\n<tr id=\"S3.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Bert-Base</td>\n<td id=\"S3.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">36.9%</td>\n<td id=\"S3.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">37.4%</td>\n<td id=\"S3.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">38.1%</td>\n</tr>\n<tr id=\"S3.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">XLM-R</td>\n<td id=\"S3.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">35.9%</td>\n<td id=\"S3.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">36.7%</td>\n<td id=\"S3.T2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">37.6%</td>\n</tr>\n<tr id=\"S3.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">M-MiniLM</td>\n<td id=\"S3.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">33.3%</td>\n<td id=\"S3.T2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">33.9%</td>\n<td id=\"S3.T2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">35.9%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 2 presents the results of the unilingual models for the SemEval English test dataset. As can be seen, The Finetuned models generally perform better than their baseline and IID counterparts. For example, the Bert-Base model has a baseline score of 36.9%, while the IID score is 37.4%, and the Finetuned score is 38.1%. This shows that while the Finetuned process improves the model’s performance, the IID training approach is also effective and can achieve a score close to the Finetuned model.",
            "The exact computational overhead of the Krum algorithm compared to FedAvg or other aggregation algorithms depends on several factors, such as the number of participating clients, the size of the models, and the specific implementation of the algorithms. In some cases, the computational overhead of the Krum algorithm may be similar to that of FedAvg or even lower, depending on the specific scenario. Table12 shows that the Fed-Non-IID algorithm takes longer than the Fed-IID algorithm due to the varying sizes of language datasets among clients. The Krum algorithm takes ∼similar-to\\sim10% more time than FedAVG in our experiments."
        ]
    },
    "S3.T3": {
        "caption": "Table 3: Multilingual Centralized and Federated Macro-F1 Accuracy Scores for SemEval English Dataset.",
        "table": "<table id=\"S3.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S3.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Baseline</th>\n<th id=\"S3.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">IID</th>\n<th id=\"S3.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Non-IID</th>\n<th id=\"S3.T3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Finetuned</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Bert-Base</td>\n<td id=\"S3.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35.3%</td>\n<td id=\"S3.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.8%</td>\n<td id=\"S3.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.3%</td>\n<td id=\"S3.T3.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35.7%</td>\n</tr>\n<tr id=\"S3.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">XLM-R</td>\n<td id=\"S3.T3.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">33.4%</td>\n<td id=\"S3.T3.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">34.9%</td>\n<td id=\"S3.T3.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">34.9%</td>\n<td id=\"S3.T3.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">34.1%</td>\n</tr>\n<tr id=\"S3.T3.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T3.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">M-MiniLM</td>\n<td id=\"S3.T3.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">31.4%</td>\n<td id=\"S3.T3.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">32.7%</td>\n<td id=\"S3.T3.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">32.03%</td>\n<td id=\"S3.T3.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">32.9%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Turning to multilingual, Table 3 presents the Macro-F1 accuracy scores for centralized and FL multilingual models on the SemEval English dataset. The baseline accuracy for all models is improved by applying federated learning with the IID, which achieves slightly higher accuracy than the Non-IID setting.\nThis suggests that data distribution has an impact on model performance. Additionally, finetuning the models further improves the Baseline performance with only small differences observed between federated and Finetuned results."
        ]
    },
    "S3.T4": {
        "caption": "Table 4: Centralized and Federated Learning average results for the Twitter Multilingual (e.g Spanish, French, Italian) Dataset.",
        "table": "<table id=\"S3.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<td id=\"S3.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" colspan=\"4\">Average Test Results for Twitter Dataset</td>\n</tr>\n<tr id=\"S3.T4.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<td id=\"S3.T4.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Baseline</td>\n<td id=\"S3.T4.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">IID</td>\n<td id=\"S3.T4.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Non-IID</td>\n<td id=\"S3.T4.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Finetuned</td>\n</tr>\n<tr id=\"S3.T4.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Bert-Base</th>\n<td id=\"S3.T4.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.2%</td>\n<td id=\"S3.T4.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">30.6%</td>\n<td id=\"S3.T4.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">30.7%</td>\n<td id=\"S3.T4.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">30.2%</td>\n</tr>\n<tr id=\"S3.T4.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">XLM-R</th>\n<td id=\"S3.T4.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">26.5%</td>\n<td id=\"S3.T4.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">28.2%</td>\n<td id=\"S3.T4.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">28.02%</td>\n<td id=\"S3.T4.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">28.3%</td>\n</tr>\n<tr id=\"S3.T4.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\">M-MiniLM</th>\n<td id=\"S3.T4.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">24.2%</td>\n<td id=\"S3.T4.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">25.7%</td>\n<td id=\"S3.T4.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">26.1%</td>\n<td id=\"S3.T4.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">25.9%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Using the Twitter multilingual dataset, Table 4 outlines the trained multilingual Macro-F1 scores in centralized and FL setups. Similar observations to the case of SemEval are seen."
        ]
    },
    "S3.T5": {
        "caption": "Table 5: Centralized and Federated Learning Results for the Twitter multilingual dataset for Bert-Base model.",
        "table": "<table id=\"S3.T5.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T5.1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Data</td>\n<td id=\"S3.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Baseline</td>\n<td id=\"S3.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">IID</td>\n<td id=\"S3.T5.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Non-IID</td>\n<td id=\"S3.T5.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Finetuned</td>\n</tr>\n<tr id=\"S3.T5.1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Spanish</td>\n<td id=\"S3.T5.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">27.7%</td>\n<td id=\"S3.T5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">28.5%</td>\n<td id=\"S3.T5.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">28.8%</td>\n<td id=\"S3.T5.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">27.3%</td>\n</tr>\n<tr id=\"S3.T5.1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">French</td>\n<td id=\"S3.T5.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.6%</td>\n<td id=\"S3.T5.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">31.3%</td>\n<td id=\"S3.T5.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">31.1%</td>\n<td id=\"S3.T5.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">30.9%</td>\n</tr>\n<tr id=\"S3.T5.1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T5.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Italian</td>\n<td id=\"S3.T5.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">30.2%</td>\n<td id=\"S3.T5.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">32.2%</td>\n<td id=\"S3.T5.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">32.3%</td>\n<td id=\"S3.T5.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">32.4%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In Table 5, we show the Macro-F1 score per language of the best-performing multilingual model Bert-Base in the centralized and federated setups. As shown, the accuracy does not significantly vary per language, which shows the effectiveness of our trained models."
        ]
    },
    "S3.T6": {
        "caption": "Table 6: Comparison between our models’ performance and models from the literature on emoji prediction task using the Micro-F1 and Macro-F1 metrics.",
        "table": "<table id=\"S3.T6.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T6.1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Model</td>\n<td id=\"S3.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Micro-F1</td>\n<td id=\"S3.T6.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Macro-F1</td>\n</tr>\n<tr id=\"S3.T6.1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">BiLSTM</td>\n<td id=\"S3.T6.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.6%</td>\n<td id=\"S3.T6.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">21.3%</td>\n</tr>\n<tr id=\"S3.T6.1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Venkit et al., <a href=\"#bib.bib35\" title=\"\" class=\"ltx_ref\">2021</a>)</cite></td>\n<td id=\"S3.T6.1.3.3.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.3.3.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Proposed LSTM IID</td>\n<td id=\"S3.T6.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">45.1%</td>\n<td id=\"S3.T6.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.2%</td>\n</tr>\n<tr id=\"S3.T6.1.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Multilingual</td>\n<td id=\"S3.T6.1.5.5.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.5.5.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">XLM-Tw</td>\n<td id=\"S3.T6.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S3.T6.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">30.9%</td>\n</tr>\n<tr id=\"S3.T6.1.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Barbieri et al., <a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">2022</a>)</cite></td>\n<td id=\"S3.T6.1.7.7.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.7.7.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">TweetNLP</td>\n<td id=\"S3.T6.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S3.T6.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">34.0%</td>\n</tr>\n<tr id=\"S3.T6.1.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Camacho-Collados et al., <a href=\"#bib.bib12\" title=\"\" class=\"ltx_ref\">2022</a>)</cite></td>\n<td id=\"S3.T6.1.9.9.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.9.9.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">SemEval first team</td>\n<td id=\"S3.T6.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">47.1%</td>\n<td id=\"S3.T6.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35.9%</td>\n</tr>\n<tr id=\"S3.T6.1.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Barbieri et al., <a href=\"#bib.bib5\" title=\"\" class=\"ltx_ref\">2018a</a>)</cite></td>\n<td id=\"S3.T6.1.11.11.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.11.11.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.12.12\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.12.12.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">BERT (Twitter)</td>\n<td id=\"S3.T6.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">40.0%</td>\n<td id=\"S3.T6.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">38.0%</td>\n</tr>\n<tr id=\"S3.T6.1.13.13\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\"><cite class=\"ltx_cite ltx_citemacro_citep\">(Edwards et al., <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2020</a>)</cite></td>\n<td id=\"S3.T6.1.13.13.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.13.13.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.14.14\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.14.14.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Proposed Bert-Base Fintuned</td>\n<td id=\"S3.T6.1.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">49.4%</td>\n<td id=\"S3.T6.1.14.14.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">35.7%</td>\n</tr>\n<tr id=\"S3.T6.1.15.15\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Multilingual</td>\n<td id=\"S3.T6.1.15.15.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.15.15.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.16.16\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.16.16.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Proposed Bert-Base FL Non-IID</td>\n<td id=\"S3.T6.1.16.16.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">49.5%</td>\n<td id=\"S3.T6.1.16.16.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.3%</td>\n</tr>\n<tr id=\"S3.T6.1.17.17\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.17.17.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Multilingual</td>\n<td id=\"S3.T6.1.17.17.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.17.17.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.18.18\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.18.18.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Proposed Bert-Base FL IID</td>\n<td id=\"S3.T6.1.18.18.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.3%</td>\n<td id=\"S3.T6.1.18.18.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.8%</td>\n</tr>\n<tr id=\"S3.T6.1.19.19\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.19.19.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Multilingual</td>\n<td id=\"S3.T6.1.19.19.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.19.19.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.20.20\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.20.20.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Proposed Bert-Base FL IID</td>\n<td id=\"S3.T6.1.20.20.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.9%</td>\n<td id=\"S3.T6.1.20.20.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">37.4%</td>\n</tr>\n<tr id=\"S3.T6.1.21.21\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.21.21.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Unilingual</td>\n<td id=\"S3.T6.1.21.21.2\" class=\"ltx_td ltx_border_r\"></td>\n<td id=\"S3.T6.1.21.21.3\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S3.T6.1.22.22\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.22.22.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Proposed Bert-Base Fintuned</td>\n<td id=\"S3.T6.1.22.22.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50.1%</td>\n<td id=\"S3.T6.1.22.22.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">38.1%</td>\n</tr>\n<tr id=\"S3.T6.1.23.23\" class=\"ltx_tr\">\n<td id=\"S3.T6.1.23.23.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">Unilingual</td>\n<td id=\"S3.T6.1.23.23.2\" class=\"ltx_td ltx_border_b ltx_border_r\"></td>\n<td id=\"S3.T6.1.23.23.3\" class=\"ltx_td ltx_border_b ltx_border_r\"></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Looking at the results in Table 6, we can observe that most of our multilingual models perform better than the majority of models that were also trained on the SemEval training dataset from the literature in terms of both Micro-F1 and Macro-F1. Specifically, our multilingual models in most cases achieved more than 36% Macro-F1 or more than 49% Micro-F1, whereas the best-performing model from the literature, BERT(Twitter) (Edwards et al., 2020), achieved 40% Micro-F1. Furthermore, our unilingual model with Bert-Base achieved 38.1% Macro-F1, which is comparable to the performance of BERT(Twitter), which achieved 38% Macro-F1. Moreover, our federated models achieved more than 50% Micro-F1, which is better than the performance of BERT(Twitter). Following the Large Language Models interest, we carried out an experiment using the Davinci-003 model Brown et al. (2020) on the SemEval set in zero-shot. Davinci-003 achieved a Macro-F1 score of 16%, which also shows the promise of our trained FL models."
        ]
    },
    "S3.T7": {
        "caption": "Table 7: The zero-shot inference results for Centralized and Federated Learning ",
        "table": "<table id=\"S3.T7.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T7.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T7.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S3.T7.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Baseline</th>\n<th id=\"S3.T7.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">IID</th>\n<th id=\"S3.T7.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Non-IID</th>\n<th id=\"S3.T7.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Finetuned</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T7.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T7.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Bert-Base</td>\n<td id=\"S3.T7.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">21.9%</td>\n<td id=\"S3.T7.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">23.1%</td>\n<td id=\"S3.T7.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">23.1%</td>\n<td id=\"S3.T7.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">21.5%</td>\n</tr>\n<tr id=\"S3.T7.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T7.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">XLM-R</td>\n<td id=\"S3.T7.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">20.04%</td>\n<td id=\"S3.T7.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">20.9%</td>\n<td id=\"S3.T7.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">21.07%</td>\n<td id=\"S3.T7.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">19.2%</td>\n</tr>\n<tr id=\"S3.T7.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T7.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">M-MiniLM</td>\n<td id=\"S3.T7.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">15.4%</td>\n<td id=\"S3.T7.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">16.5%</td>\n<td id=\"S3.T7.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">16.4%</td>\n<td id=\"S3.T7.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">17.1%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To investigate the trained models’ performance on unseen languages (i.e, zero-shot), we run inference on an unseen German dataset in Baseline, FL, and Finetune settings. Table 7 shows that there is some drop in performance due to the zero-shot setting. However, this experiment still shows that FL performs at least similarly to centralized settings even in unseen languages."
        ]
    },
    "S3.T8": {
        "caption": "Table 8: Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the SemEval English test dataset.",
        "table": "<table id=\"S3.T8.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T8.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S3.T8.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Setting</th>\n<td id=\"S3.T8.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"S3.T8.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"S3.T8.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T8.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Fed-IID</th>\n<td id=\"S3.T8.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">26.2%</td>\n<td id=\"S3.T8.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.5%</td>\n</tr>\n<tr id=\"S3.T8.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Bert-Base</th>\n<th id=\"S3.T8.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T8.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">28.1%</td>\n<td id=\"S3.T8.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">35.2%</td>\n</tr>\n<tr id=\"S3.T8.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.4.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T8.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"S3.T8.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">24.4%</td>\n</tr>\n<tr id=\"S3.T8.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.5.5.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T8.1.5.5.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"S3.T8.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"S3.T8.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"S3.T8.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.6.6.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T8.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"S3.T8.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">27.6%</td>\n<td id=\"S3.T8.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">34.8%</td>\n</tr>\n<tr id=\"S3.T8.1.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">XLM-R</th>\n<th id=\"S3.T8.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T8.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">27.5%</td>\n<td id=\"S3.T8.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">32.9%</td>\n</tr>\n<tr id=\"S3.T8.1.8.8\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.8.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T8.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"S3.T8.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">23.3%</td>\n</tr>\n<tr id=\"S3.T8.1.9.9\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.9.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T8.1.9.9.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"S3.T8.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"S3.T8.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"S3.T8.1.10.10\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.10.10.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T8.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"S3.T8.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">25.3%</td>\n<td id=\"S3.T8.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.6%</td>\n</tr>\n<tr id=\"S3.T8.1.11.11\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">M-MiniLM</th>\n<th id=\"S3.T8.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T8.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">26.7%</td>\n<td id=\"S3.T8.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">30.5%</td>\n</tr>\n<tr id=\"S3.T8.1.12.12\" class=\"ltx_tr\">\n<th id=\"S3.T8.1.12.12.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T8.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">Finetuned</th>\n<td id=\"S3.T8.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" colspan=\"2\">23.9%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 8 and 9 depict the results of label flipping experiments when 50% clients are attacked (i.e 50% of the data is attacked) using the SemEval and Twitter datasets, respectively. The tables compare the centralized and FL results for FedAVG and Krum. The results demonstrate that Krum performed better than FedAVG in both datasets, with higher accuracy rates. Krum was able to handle the label-flipping attack scenario and produced scores that were very close to the results obtained with the Finetune setting.",
            "Table 8 presents the experiment results for three different models, namely Bert-Base, XLM-R, and M-MiniLM. However, we will focus on the results of the Bert-Base model in Figure 4, where Bert achieved 36.8% Macro-F1 in the clean IID scenario but dropped to 26.2% under FedAVG with Fed-IID due to label-flipping attacks. Traditional training and FL with FedAVG had low Macro-F1 of 24.4%, while Krum aggregation function achieved 36.5% Macro-F1, showing superior handling of label-flipping attacks and improving FL model performance. Appendix A shows similar results for 25% Toxic clients experiments.",
            "Similar to the experiment in Table 8. Table 9 shows that Krum outperforms FedAVG in both the Non-IID and IID settings as well. Also in Table 10 reports the results of applying label flipping to the German Zero-shot scenario,"
        ]
    },
    "S3.T9": {
        "caption": "Table 9: Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the average results for the Twitter multilingual dataset.",
        "table": "<table id=\"S3.T9.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T9.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S3.T9.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Setting</th>\n<td id=\"S3.T9.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"S3.T9.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"S3.T9.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T9.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Fed-IID</th>\n<td id=\"S3.T9.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">21.8%</td>\n<td id=\"S3.T9.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">30.1%</td>\n</tr>\n<tr id=\"S3.T9.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Bert-Base</th>\n<th id=\"S3.T9.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T9.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">23.6%</td>\n<td id=\"S3.T9.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">29.4%</td>\n</tr>\n<tr id=\"S3.T9.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.4.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T9.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"S3.T9.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">21.8%</td>\n</tr>\n<tr id=\"S3.T9.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.5.5.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T9.1.5.5.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"S3.T9.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"S3.T9.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"S3.T9.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.6.6.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T9.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"S3.T9.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20.4%</td>\n<td id=\"S3.T9.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">27.3%</td>\n</tr>\n<tr id=\"S3.T9.1.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">XLM-R</th>\n<th id=\"S3.T9.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T9.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">21.5%</td>\n<td id=\"S3.T9.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">26.6%</td>\n</tr>\n<tr id=\"S3.T9.1.8.8\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.8.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T9.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"S3.T9.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">20.8%</td>\n</tr>\n<tr id=\"S3.T9.1.9.9\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.9.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T9.1.9.9.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"S3.T9.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"S3.T9.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"S3.T9.1.10.10\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.10.10.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T9.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"S3.T9.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">19.02%</td>\n<td id=\"S3.T9.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">24.9%</td>\n</tr>\n<tr id=\"S3.T9.1.11.11\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">M-MiniLM</th>\n<th id=\"S3.T9.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T9.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">20.9%</td>\n<td id=\"S3.T9.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">23.9%</td>\n</tr>\n<tr id=\"S3.T9.1.12.12\" class=\"ltx_tr\">\n<th id=\"S3.T9.1.12.12.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T9.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">Finetuned</th>\n<td id=\"S3.T9.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" colspan=\"2\">20.3%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Similar to the experiment in Table 8. Table 9 shows that Krum outperforms FedAVG in both the Non-IID and IID settings as well. Also in Table 10 reports the results of applying label flipping to the German Zero-shot scenario,"
        ]
    },
    "S3.T10": {
        "caption": "Table 10: Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the German Zero-shot.",
        "table": "<table id=\"S3.T10.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T10.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S3.T10.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Setting</th>\n<td id=\"S3.T10.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"S3.T10.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"S3.T10.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T10.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Fed-IID</th>\n<td id=\"S3.T10.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">15.3%</td>\n<td id=\"S3.T10.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">23.5%</td>\n</tr>\n<tr id=\"S3.T10.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Bert-Base</th>\n<th id=\"S3.T10.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T10.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">15.7%</td>\n<td id=\"S3.T10.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">21.3%</td>\n</tr>\n<tr id=\"S3.T10.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.4.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T10.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"S3.T10.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">16.3%</td>\n</tr>\n<tr id=\"S3.T10.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.5.5.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T10.1.5.5.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"S3.T10.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"S3.T10.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"S3.T10.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.6.6.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T10.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"S3.T10.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">14.1%</td>\n<td id=\"S3.T10.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20.9%</td>\n</tr>\n<tr id=\"S3.T10.1.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">XLM-R</th>\n<th id=\"S3.T10.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T10.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">14.04%</td>\n<td id=\"S3.T10.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">19.8%</td>\n</tr>\n<tr id=\"S3.T10.1.8.8\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.8.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T10.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"S3.T10.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">14.3%</td>\n</tr>\n<tr id=\"S3.T10.1.9.9\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.9.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T10.1.9.9.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"S3.T10.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"S3.T10.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"S3.T10.1.10.10\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.10.10.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T10.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"S3.T10.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">11.3%</td>\n<td id=\"S3.T10.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.2%</td>\n</tr>\n<tr id=\"S3.T10.1.11.11\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">M-MiniLM</th>\n<th id=\"S3.T10.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T10.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">11.9%</td>\n<td id=\"S3.T10.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">14.3%</td>\n</tr>\n<tr id=\"S3.T10.1.12.12\" class=\"ltx_tr\">\n<th id=\"S3.T10.1.12.12.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T10.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">Finetuned</th>\n<td id=\"S3.T10.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" colspan=\"2\">11.7%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Similar to the experiment in Table 8. Table 9 shows that Krum outperforms FedAVG in both the Non-IID and IID settings as well. Also in Table 10 reports the results of applying label flipping to the German Zero-shot scenario,"
        ]
    },
    "S3.T11": {
        "caption": "Table 11: Macro-F1 Results of Bert-Base Model in Federated Multilingual Setting under Clean and Attack Scenarios for the SemEval English test dataset.",
        "table": "<table id=\"S3.T11.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T11.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T11.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">Setting</th>\n<td id=\"S3.T11.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">Results</td>\n</tr>\n<tr id=\"S3.T11.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T11.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Cleaned</th>\n<th id=\"S3.T11.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Fed-IID</th>\n<td id=\"S3.T11.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">36.8</td>\n</tr>\n<tr id=\"S3.T11.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T11.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">FedAVG</th>\n<th id=\"S3.T11.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T11.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"2\">36.3</td>\n</tr>\n<tr id=\"S3.T11.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T11.1.4.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T11.1.4.4.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"S3.T11.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">25% Toxic</td>\n<td id=\"S3.T11.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50% Toxic</td>\n</tr>\n<tr id=\"S3.T11.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T11.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">FedAVG</th>\n<th id=\"S3.T11.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"S3.T11.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.3</td>\n<td id=\"S3.T11.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">26.2</td>\n</tr>\n<tr id=\"S3.T11.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T11.1.6.6.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T11.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T11.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">33.5</td>\n<td id=\"S3.T11.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">28.1</td>\n</tr>\n<tr id=\"S3.T11.1.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T11.1.7.7.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"S3.T11.1.7.7.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"S3.T11.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">25% Toxic</td>\n<td id=\"S3.T11.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">50% Toxic</td>\n</tr>\n<tr id=\"S3.T11.1.8.8\" class=\"ltx_tr\">\n<th id=\"S3.T11.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Krum</th>\n<th id=\"S3.T11.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"S3.T11.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.5</td>\n<td id=\"S3.T11.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.5</td>\n</tr>\n<tr id=\"S3.T11.1.9.9\" class=\"ltx_tr\">\n<th id=\"S3.T11.1.9.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"></th>\n<th id=\"S3.T11.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">Fed-Non-IID</th>\n<td id=\"S3.T11.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">35.3</td>\n<td id=\"S3.T11.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">35.2</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 11 provides valuable insights into the performance of a Bert-Base model in a federated multilingual setting under both clean and attack scenarios. The results highlight the impact of toxic clients on the federated setting, showing a decrease in performance for FedAVG, Fed-IID, and Fed-Non-IID. This decrease in performance is particularly evident when the toxic data constitutes 50% of the overall data. However, the use of the Krum aggregation function can mitigate this drop in performance, as previously observed in literature."
        ]
    },
    "S3.T12": {
        "caption": "Table 12: Comparison of computational overhead between FedAVG and Krum algorithms in terms of training time for each federated learning experiment.",
        "table": "<table id=\"S3.T12.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T12.4.5.1\" class=\"ltx_tr\">\n<th id=\"S3.T12.4.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" colspan=\"2\">Setting</th>\n<th id=\"S3.T12.4.5.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Time (Hours)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T12.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T12.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S3.T12.1.1.2.1\" class=\"ltx_text\">FedAVG</span></td>\n<td id=\"S3.T12.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Fed-IID</td>\n<td id=\"S3.T12.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<math id=\"S3.T12.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S3.T12.1.1.1.m1.1a\"><mo id=\"S3.T12.1.1.1.m1.1.1\" xref=\"S3.T12.1.1.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T12.1.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S3.T12.1.1.1.m1.1.1.cmml\" xref=\"S3.T12.1.1.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T12.1.1.1.m1.1c\">\\sim</annotation></semantics></math>18</td>\n</tr>\n<tr id=\"S3.T12.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T12.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">Fed-Non-IID</td>\n<td id=\"S3.T12.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">\n<math id=\"S3.T12.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S3.T12.2.2.1.m1.1a\"><mo id=\"S3.T12.2.2.1.m1.1.1\" xref=\"S3.T12.2.2.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T12.2.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S3.T12.2.2.1.m1.1.1.cmml\" xref=\"S3.T12.2.2.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T12.2.2.1.m1.1c\">\\sim</annotation></semantics></math>22</td>\n</tr>\n<tr id=\"S3.T12.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T12.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S3.T12.3.3.2.1\" class=\"ltx_text\">Krum</span></td>\n<td id=\"S3.T12.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Fed-IID</td>\n<td id=\"S3.T12.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<math id=\"S3.T12.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S3.T12.3.3.1.m1.1a\"><mo id=\"S3.T12.3.3.1.m1.1.1\" xref=\"S3.T12.3.3.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T12.3.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S3.T12.3.3.1.m1.1.1.cmml\" xref=\"S3.T12.3.3.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T12.3.3.1.m1.1c\">\\sim</annotation></semantics></math>20</td>\n</tr>\n<tr id=\"S3.T12.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T12.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">Fed-Non-IID</td>\n<td id=\"S3.T12.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">\n<math id=\"S3.T12.4.4.1.m1.1\" class=\"ltx_Math\" alttext=\"\\sim\" display=\"inline\"><semantics id=\"S3.T12.4.4.1.m1.1a\"><mo id=\"S3.T12.4.4.1.m1.1.1\" xref=\"S3.T12.4.4.1.m1.1.1.cmml\">∼</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T12.4.4.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S3.T12.4.4.1.m1.1.1.cmml\" xref=\"S3.T12.4.4.1.m1.1.1\">similar-to</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T12.4.4.1.m1.1c\">\\sim</annotation></semantics></math>24</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The exact computational overhead of the Krum algorithm compared to FedAvg or other aggregation algorithms depends on several factors, such as the number of participating clients, the size of the models, and the specific implementation of the algorithms. In some cases, the computational overhead of the Krum algorithm may be similar to that of FedAvg or even lower, depending on the specific scenario. Table",
                "12",
                " shows that the Fed-Non-IID algorithm takes longer than the Fed-IID algorithm due to the varying sizes of language datasets among clients. The Krum algorithm takes ",
                "∼",
                "similar-to",
                "\\sim",
                "10% more time than FedAVG in our experiments.",
                "To estimate the communicated payload for each client in our federated learning model, the total size of model parameters needs to be calculated. Assuming that the local model is the M-MiniLM model of size 0.47 GB, and there are 4 clients participating in each round, then each client needs to transmit 0.47 GB of data to the server during each round of training. Since there are 5 rounds in total, the total amount of data transmitted per client for one epoch would be 2.35 GB (0.47 GB x 5). This is an approximate estimate and does not take into account factors such as compression techniques or network latency. The actual resource constraints may vary depending on these factors.",
                "Table ",
                "13",
                " shows the estimated amount of data transmitted during one epoch of federated learning using different models. For the M-MiniLM model, the estimated amount of data transmitted per client for one epoch would be 2.35 GB, which is within the resource constraints for modern devices."
            ]
        ]
    },
    "S3.T13": {
        "caption": "Table 13: Estimated amount of data transmitted per client during one round of FL.",
        "table": "<table id=\"S3.T13.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T13.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T13.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model Name</th>\n<th id=\"S3.T13.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Model Size</th>\n<th id=\"S3.T13.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Data Transmitted</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T13.1.2.1\" class=\"ltx_tr\">\n<td id=\"S3.T13.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Switch-Base-8</td>\n<td id=\"S3.T13.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.24 GB</td>\n<td id=\"S3.T13.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1.24*5 = 6.2 GB</td>\n</tr>\n<tr id=\"S3.T13.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T13.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">Bert-Base</td>\n<td id=\"S3.T13.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">1.12 GB</td>\n<td id=\"S3.T13.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">1.12*5 = 5.6 GB</td>\n</tr>\n<tr id=\"S3.T13.1.4.3\" class=\"ltx_tr\">\n<td id=\"S3.T13.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">XLM-R</td>\n<td id=\"S3.T13.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">1.11 GB</td>\n<td id=\"S3.T13.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">1.11*5 = 5.55 GB</td>\n</tr>\n<tr id=\"S3.T13.1.5.4\" class=\"ltx_tr\">\n<td id=\"S3.T13.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">M-MiniLM</td>\n<td id=\"S3.T13.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.47 GB</td>\n<td id=\"S3.T13.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">0.47*5 = 2.35 GB</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 13 shows the estimated amount of data transmitted during one epoch of federated learning using different models. For the M-MiniLM model, the estimated amount of data transmitted per client for one epoch would be 2.35 GB, which is within the resource constraints for modern devices."
        ]
    },
    "A1.T14": {
        "caption": "Table 14: Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the SemEval English test dataset.",
        "table": "<table id=\"A1.T14.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T14.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"A1.T14.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Setting</th>\n<td id=\"A1.T14.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"A1.T14.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"A1.T14.1.2.2\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"A1.T14.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Fed-IID</th>\n<td id=\"A1.T14.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.3%</td>\n<td id=\"A1.T14.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">36.5%</td>\n</tr>\n<tr id=\"A1.T14.1.3.3\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Bert-Base</th>\n<th id=\"A1.T14.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"A1.T14.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">33.5%</td>\n<td id=\"A1.T14.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">35.3%</td>\n</tr>\n<tr id=\"A1.T14.1.4.4\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.4.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T14.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"A1.T14.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">35.6%</td>\n</tr>\n<tr id=\"A1.T14.1.5.5\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.5.5.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"A1.T14.1.5.5.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"A1.T14.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"A1.T14.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"A1.T14.1.6.6\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.6.6.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T14.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"A1.T14.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">34.7%</td>\n<td id=\"A1.T14.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">34.9%</td>\n</tr>\n<tr id=\"A1.T14.1.7.7\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">XLM-R</th>\n<th id=\"A1.T14.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"A1.T14.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">32.7%</td>\n<td id=\"A1.T14.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">33.4%</td>\n</tr>\n<tr id=\"A1.T14.1.8.8\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.8.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T14.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"A1.T14.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">33.9%</td>\n</tr>\n<tr id=\"A1.T14.1.9.9\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.9.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"A1.T14.1.9.9.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"A1.T14.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"A1.T14.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"A1.T14.1.10.10\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.10.10.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T14.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"A1.T14.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.3%</td>\n<td id=\"A1.T14.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">32.2%</td>\n</tr>\n<tr id=\"A1.T14.1.11.11\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">M-MiniLM</th>\n<th id=\"A1.T14.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"A1.T14.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">30.4%</td>\n<td id=\"A1.T14.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">30.7%</td>\n</tr>\n<tr id=\"A1.T14.1.12.12\" class=\"ltx_tr\">\n<th id=\"A1.T14.1.12.12.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T14.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">Finetuned</th>\n<td id=\"A1.T14.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" colspan=\"2\">32.7%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Tables ",
                "14",
                " and ",
                "15",
                " present a comparison of centralized and federated learning for different multilingual models in a label-flipping attack scenarios with an Attack Ratio of 25%. The models’ performance was evaluated using two different settings, Fed-IID and Fed-Non-IID, and two federated learning algorithms, FedAVG and Krum. Both tables show that the models’ performance decreases in the label-flipping attack scenario, and is worse for the Fed-Non-IID setting than for the Fed-IID setting. Furthermore, Krum outperforms the FedAVG aggregator in all cases, and the Bert-Base model generally performs better than the other models.",
                "Table ",
                "16",
                " presents a comparison between centralized and federated learning approaches using FedAVG and Krum aggregation functions in a label-flipping attack scenario for three different models. We observed that Krum outperforms FedAVG in all cases, indicating that the choice of aggregation function has a significant impact on the model’s performance. However, we also found that the model’s performance is heavily dependent on the architecture and the type of aggregation function used.",
                "The results of our experiments highlight the vulnerability of federated learning to label-flipping attacks. This vulnerability emphasizes the importance of carefully selecting the federated learning algorithm and aggregation function to mitigate such attacks. Additionally, our experiments revealed that the Fed-IID setting is less vulnerable to label-flipping attacks, which suggests that data distribution plays a critical role in the performance of federated learning models.",
                "To further investigate the impact of label-flipping attacks on federated learning models, future experiments can explore the impact of different attack ratios and other attack scenarios. Moreover, it would be interesting to study the impact of other factors, such as the heterogeneity of data sources and the distribution of data samples, on the vulnerability of federated learning models to attacks. By gaining a better understanding of the vulnerabilities of federated learning, we can develop more robust and secure models that are better suited for real-world applications."
            ]
        ]
    },
    "A1.T15": {
        "caption": "Table 15: Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the average results for the Twitter multilingual dataset.",
        "table": "<table id=\"A1.T15.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T15.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"A1.T15.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Setting</th>\n<td id=\"A1.T15.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"A1.T15.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"A1.T15.1.2.2\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"A1.T15.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Fed-IID</th>\n<td id=\"A1.T15.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">28.2%</td>\n<td id=\"A1.T15.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">29.1%</td>\n</tr>\n<tr id=\"A1.T15.1.3.3\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Bert-Base</th>\n<th id=\"A1.T15.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"A1.T15.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">30.5%</td>\n<td id=\"A1.T15.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">29.7%</td>\n</tr>\n<tr id=\"A1.T15.1.4.4\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.4.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T15.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"A1.T15.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">28.9%</td>\n</tr>\n<tr id=\"A1.T15.1.5.5\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.5.5.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"A1.T15.1.5.5.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"A1.T15.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"A1.T15.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"A1.T15.1.6.6\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.6.6.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T15.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"A1.T15.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">26.1%</td>\n<td id=\"A1.T15.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">27.7%</td>\n</tr>\n<tr id=\"A1.T15.1.7.7\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">XLM-R</th>\n<th id=\"A1.T15.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"A1.T15.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">27.1%</td>\n<td id=\"A1.T15.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">27.3%</td>\n</tr>\n<tr id=\"A1.T15.1.8.8\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.8.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T15.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"A1.T15.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">26.7%</td>\n</tr>\n<tr id=\"A1.T15.1.9.9\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.9.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"A1.T15.1.9.9.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"A1.T15.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"A1.T15.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"A1.T15.1.10.10\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.10.10.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T15.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"A1.T15.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">22.9%</td>\n<td id=\"A1.T15.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">25.3%</td>\n</tr>\n<tr id=\"A1.T15.1.11.11\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">M-MiniLM</th>\n<th id=\"A1.T15.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"A1.T15.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">24.8%</td>\n<td id=\"A1.T15.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">24.6%</td>\n</tr>\n<tr id=\"A1.T15.1.12.12\" class=\"ltx_tr\">\n<th id=\"A1.T15.1.12.12.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T15.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">Finetuned</th>\n<td id=\"A1.T15.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" colspan=\"2\">24.6%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Tables ",
                "14",
                " and ",
                "15",
                " present a comparison of centralized and federated learning for different multilingual models in a label-flipping attack scenarios with an Attack Ratio of 25%. The models’ performance was evaluated using two different settings, Fed-IID and Fed-Non-IID, and two federated learning algorithms, FedAVG and Krum. Both tables show that the models’ performance decreases in the label-flipping attack scenario, and is worse for the Fed-Non-IID setting than for the Fed-IID setting. Furthermore, Krum outperforms the FedAVG aggregator in all cases, and the Bert-Base model generally performs better than the other models.",
                "Table ",
                "16",
                " presents a comparison between centralized and federated learning approaches using FedAVG and Krum aggregation functions in a label-flipping attack scenario for three different models. We observed that Krum outperforms FedAVG in all cases, indicating that the choice of aggregation function has a significant impact on the model’s performance. However, we also found that the model’s performance is heavily dependent on the architecture and the type of aggregation function used.",
                "The results of our experiments highlight the vulnerability of federated learning to label-flipping attacks. This vulnerability emphasizes the importance of carefully selecting the federated learning algorithm and aggregation function to mitigate such attacks. Additionally, our experiments revealed that the Fed-IID setting is less vulnerable to label-flipping attacks, which suggests that data distribution plays a critical role in the performance of federated learning models.",
                "To further investigate the impact of label-flipping attacks on federated learning models, future experiments can explore the impact of different attack ratios and other attack scenarios. Moreover, it would be interesting to study the impact of other factors, such as the heterogeneity of data sources and the distribution of data samples, on the vulnerability of federated learning models to attacks. By gaining a better understanding of the vulnerabilities of federated learning, we can develop more robust and secure models that are better suited for real-world applications."
            ]
        ]
    },
    "A1.T16": {
        "caption": "Table 16: Centralized and Federated Learning Results in Label-Flipping Attack Scenario for the German Zero-shot.",
        "table": "<table id=\"A1.T16.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A1.T16.1.1.1\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"A1.T16.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Setting</th>\n<td id=\"A1.T16.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"A1.T16.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"A1.T16.1.2.2\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.2.2.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"A1.T16.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Fed-IID</th>\n<td id=\"A1.T16.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">20.67%</td>\n<td id=\"A1.T16.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">23.48%</td>\n</tr>\n<tr id=\"A1.T16.1.3.3\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">Bert-Base</th>\n<th id=\"A1.T16.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"A1.T16.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">20.60%</td>\n<td id=\"A1.T16.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">21.62%</td>\n</tr>\n<tr id=\"A1.T16.1.4.4\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.4.4.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T16.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"A1.T16.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">21.6%</td>\n</tr>\n<tr id=\"A1.T16.1.5.5\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.5.5.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"A1.T16.1.5.5.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"A1.T16.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"A1.T16.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"A1.T16.1.6.6\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.6.6.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T16.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"A1.T16.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">18.60%</td>\n<td id=\"A1.T16.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">21.25%</td>\n</tr>\n<tr id=\"A1.T16.1.7.7\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">XLM-R</th>\n<th id=\"A1.T16.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"A1.T16.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">18.81%</td>\n<td id=\"A1.T16.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">19.93%</td>\n</tr>\n<tr id=\"A1.T16.1.8.8\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.8.8.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T16.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Finetuned</th>\n<td id=\"A1.T16.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"2\">18.2%</td>\n</tr>\n<tr id=\"A1.T16.1.9.9\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.9.9.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"></th>\n<th id=\"A1.T16.1.9.9.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_r ltx_border_t\"></th>\n<td id=\"A1.T16.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAVG</td>\n<td id=\"A1.T16.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Krum</td>\n</tr>\n<tr id=\"A1.T16.1.10.10\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.10.10.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T16.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-IID</th>\n<td id=\"A1.T16.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">13.78%</td>\n<td id=\"A1.T16.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16.47%</td>\n</tr>\n<tr id=\"A1.T16.1.11.11\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r\">M-MiniLM</th>\n<th id=\"A1.T16.1.11.11.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fed-Non-IID</th>\n<td id=\"A1.T16.1.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\">14.62%</td>\n<td id=\"A1.T16.1.11.11.4\" class=\"ltx_td ltx_align_center ltx_border_r\">15.38%</td>\n</tr>\n<tr id=\"A1.T16.1.12.12\" class=\"ltx_tr\">\n<th id=\"A1.T16.1.12.12.1\" class=\"ltx_td ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r\"></th>\n<th id=\"A1.T16.1.12.12.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r\">Finetuned</th>\n<td id=\"A1.T16.1.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" colspan=\"2\">14.7%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table 16 presents a comparison between centralized and federated learning approaches using FedAVG and Krum aggregation functions in a label-flipping attack scenario for three different models. We observed that Krum outperforms FedAVG in all cases, indicating that the choice of aggregation function has a significant impact on the model’s performance. However, we also found that the model’s performance is heavily dependent on the architecture and the type of aggregation function used."
        ]
    }
}