{
    "S4.T1.1": {
        "caption": "Performance over Chinese-to-English zero-shot prompting with different template and different template language on our IT domain-specific test dataset. [src], [tgt] and [input] represent the source language name, the target language name and the input sentences to be translated;   and   represent the template language.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S4.T1.1.1.1.1\">ID</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.2.1\">Templates (in English)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T1.1.1.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.3.1\">Chinese</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\" id=\"S4.T1.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.1.4.1\">English</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.1.1\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.2.1\">chrF++</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.3.1\">COMET</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.4.1\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.5.1\">chrF++</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.2.6.1\">COMET</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.1\">A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.2\">Translate the following[src]sentence into [tgt]:[input]</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.3.3.1\">34.11</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.4\">58.85</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.3.5.1\">55.92</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.3.6.1\">35.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.3.7.1\">59.72</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T1.1.3.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.3.8.1\">57.61</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.1\">B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.2\">Provide [tgt] translation for the [src] sentence:[input]</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.3\">33.77</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T1.1.4.4.1\">58.91</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.5\">55.84</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.6\">35.13</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.7\">59.35</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.4.8\">55.81</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.5.1\">C</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.5.2\">How to express [input] in [tgt] ?</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.5.3\">33.46</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.5.4\">58.40</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.5.5\">55.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.5.6\">34.90</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.5.7\">59.45</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.5.8\">56.36</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.6.1\">D</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.6.2\">What&#8217;s the [tgt] translation of [input]</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.6.3\">33.95</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.6.4\">58.70</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.6.5\">55.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.6.6\">34.51</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.6.7\">59.19</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.6.8\">56.22</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.7.1\">F</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.7.2\">Generate [tgt] translation for [input]</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.7.3\">32.53</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.7.4\">57.61</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.7.5\">54.30</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.7.6\">34.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.7.7\">59.49</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.7.8\">57.01</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.8.1\">G</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.8.2\">Express the following [src] sentence:[input] in [tgt]</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.8.3\">33.16</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.8.4\">58.39</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.8.5\">55.52</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.8.6\">34.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.8.7\">59.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T1.1.8.8\">56.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.9\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.9.1\">H</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.9.2\">How to say [input] in [tgt] ?</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.9.3\">33.48</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.9.4\">58.77</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.9.5\">55.74</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.9.6\">35.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.9.7\">59.43</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T1.1.9.8\">57.26</td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "In-context learning feeds a language model with extra contexts to improve its MT capabilities, without fine-tuning Brown et al. (2020). Existing works adopted similar methods to explore different settings for the contexts, such as zero-shot and few-shot examples. Several works focused on testing the MT capabilities of LLMs with simple prompts (zero-shot examples) via natural language descriptions of the MT task Brown et al. (2020); Winata et al. (2021); Reynolds and McDonell (2021); Lin et al. (2022); Scao et al. (2022); Zhang et al. (2022); Garcia and Firat (2022). For instance, Reynolds et al. Reynolds and McDonell (2021) tested different prompt templates for GPT-3 Brown et al. (2020), while Garcia et al. Garcia and Firat (2022) used prompts with mT5 Xue et al. (2021) to control the outputs. Other works investigated prompting strategies for identifying appropriate examples to enhance the MT capabilities of LLMs Chowdhery et al. (2023); Vilar et al. (2023); Moslem et al. (2023a); Agrawal et al. (2023); Zhang et al. (2023); Jiao et al. (2023b); Hendy et al. (2023). Although moderate progress has been made, these methods suffer from several drawbacks: (iùëñiitalic_i) their effectiveness is highly sensitive to the quality of examples (See Table 1); (i‚Å¢iùëñùëñiiitalic_i italic_i) extra post-processing is required due to over-generation (See Figure 2); (i‚Å¢i‚Å¢iùëñùëñùëñiiiitalic_i italic_i italic_i) processing examples can increase inference costs Alves et al. (2023).\n",
            "It is difficult to manually annotate large-scale task-specific data, as it requires expertise for writing the solutions to each task. Accordingly, we develop a self-constructed pipeline for generating high-quality bilingual translation pairs. We select the IT domain as a case study in our work, while our method can be easily applied to other domains. We first collect about 100,000100000100,000100 , 000 IT product documentations from official websites of some well-known IT companies, half in Chinese and half in English. Then, we semantically segment the documents into textual sentences and sequentially align each pair of bilingual sentences. To ensure the high quality of translation data, we adopt the LLM with zero-shot capabilities for validating the translation pairs. For each pair, we use the Chinese sentences to create a translation instructions using our designed prompting templates (See Table 1), and directly feed them to the Baichuan-13B Yang et al. (2023a) for translation inference. The returned results are then used to compute BLEU scores with the English sentences of our generated pairs. For a low BLEU score that is not exceeding a given threshold value (This value is set as 10101010 without loss of generality), our generated pair would require manual verification from expertise. As manual annotations are costly, we directly discard such pairs in this work. Consequently, low-quality and duplicate pairs are removed, resulting in 200,000200000200,000200 , 000 high-quality Chinese-English translation pairs in the IT domain. We name this self-constructed domain-specific dataset as XFIT24-ORI.\n",
            "Fine-tuning an LLM directly on the XFIT24-ORI dataset can adapt its MT capabilities to the specific domain. However, its effectiveness is reported to be limited for MT tasks as it can struggle to respond to translation instructions without instruction-tuning Wei et al. (2022a). Thus, the need arises to construct a task-specific dataset, that is described via natural language instructions. We conduct extensive experiments to explore 7 widely used MT instruction templates, as shown in Table 1. We observed that prompting performance varies across templates, and task-specific templates mainly work when translating into languages for which LLMs are pre-trained on. An English template in a simple form like ‚ÄúTranslate the following [src] sentence into [tgt]: [input]‚Äù, works best for MT. Particularly, we adopt this template to re-write the source sentences for all translation pairs in XFIT24-ORI, and name the resulting task-specific dataset as XFIT24-TSI. Fine-tuning an LLM on the XFIT24-TSI dataset can greatly enhance its MT capabilities by zero-shot prompting.\n\n",
            "We compare 7 templates and evaluate them on the XFIT24 datasets covering two language\npairs (English‚Üí‚Üí\\rightarrow‚ÜíChinese and Chinese‚Üí‚Üí\\rightarrow‚ÜíEnglish). Table 1 shows the results. The template affects zero-shot prompting quality substantially, and the simple template A in English specifying just the source and target language name achieves the best overall results. Thus, we use the template A in the following experiments. Table 1 also shows the prompting results of Chinese templates, which often under-perform their English counterparts. Overall, an English template works best on average.\n"
        ]
    },
    "S4.T2.1": {
        "caption": "Performance of Supervised Baseline Models and LLM base models in our XFIT24 test dataset. The * represents Supervised Baseline Models.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T2.1\">\n<tr class=\"ltx_tr\" id=\"S4.T2.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.1.1\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.2.1\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.3.1\">chrF++</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T2.1.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.1.4.1\">COMET</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.1\">NLLB-600M*</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.2\">23.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.3\">53.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.2.4\">53.65</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.1\">Google*</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.2.1\">44.00</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.3.1\">66.14</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.3.4.1\">80.22</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.1\">Aquila2-7B</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.2\">15.02</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.3\">34.78</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T2.1.4.4\">17.23</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.1\">InternLM-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.2\">15.88</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.3\">37.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.5.4\">18.98</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.6\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.1\">Baichuan2-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.2\">20.56</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.3\">45.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.6.4\">28.42</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.1\">Llama2-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.2\">22.72</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.3\">49.20</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.7.4\">32.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.8\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.1\">BLOOM-7B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.2\">24.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.3\">51.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.8.4\">36.74</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.1\">BigTranslate</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.2\">25.02</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.3\">50.78</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.9.4\">37.34</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.10\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.1\">PolyLM-13B</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.2\">28.33</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.3\">46.14</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T2.1.10.4\">45.28</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T2.1.11\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.1\">ChatGPT 3.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.11.2.1\">34.35</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.11.3.1\">55.78</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T2.1.11.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T2.1.11.4.1\">57.80</span></td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Table 2 shows the results. we observed that: (iùëñiitalic_i) The performance of LLMs in domain-specific MT tasks is less impressive, compared with the traditional supervision methods like Google. Among them, ChatGPT 3.5 demonstrates the best performance; (i‚Å¢iùëñùëñiiitalic_i italic_i) The MT capability of traditional supervised methods surpass those of LLMs. With just 600 million parameters, the NLLB outperforms most 7B models. Similarly, the Google based on the supervised model, achieves the best results; (i‚Å¢i‚Å¢iùëñùëñùëñiiiitalic_i italic_i italic_i) The scale of model parameters in LLMs has a significant impact on the translation quality. As an example, the PolyLM with 13B parameters outperforms other 7B models by a considerable margin in all metrics.\n"
        ]
    },
    "S4.T3.1": {
        "caption": "Performance of different dictionary-based prompt methods on XFIT24 test dataset.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T3.1\">\n<tr class=\"ltx_tr\" id=\"S4.T3.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T3.1.1.1\" rowspan=\"2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1\" style=\"font-size:90%;\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T3.1.1.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.2.1\" style=\"font-size:90%;\">Chinese &#8594; English</span><span class=\"ltx_text\" id=\"S4.T3.1.1.2.2\" style=\"font-size:90%;\"/>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T3.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.3.1\" style=\"font-size:90%;\">English &#8594; Chinese</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.1.1\" style=\"font-size:90%;\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.2.1\" style=\"font-size:90%;\">COMET</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.3.1\" style=\"font-size:90%;\">BLEU</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.2.4.1\" style=\"font-size:90%;\">COMET</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.1\"><span class=\"ltx_text\" id=\"S4.T3.1.3.1.1\" style=\"font-size:90%;\">No-Dict</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.2\"><span class=\"ltx_text\" id=\"S4.T3.1.3.2.1\" style=\"font-size:90%;\">35.69</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.3\"><span class=\"ltx_text\" id=\"S4.T3.1.3.3.1\" style=\"font-size:90%;\">58.16</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.4\"><span class=\"ltx_text\" id=\"S4.T3.1.3.4.1\" style=\"font-size:90%;\">50.62</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.1.3.5\"><span class=\"ltx_text\" id=\"S4.T3.1.3.5.1\" style=\"font-size:90%;\">80.78</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.1\"><span class=\"ltx_text\" id=\"S4.T3.1.4.1.1\" style=\"font-size:90%;\">Chain-of-Dict</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.2\"><span class=\"ltx_text\" id=\"S4.T3.1.4.2.1\" style=\"font-size:90%;\">36.23</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.3\"><span class=\"ltx_text\" id=\"S4.T3.1.4.3.1\" style=\"font-size:90%;\">58.91</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.4\"><span class=\"ltx_text\" id=\"S4.T3.1.4.4.1\" style=\"font-size:90%;\">52.07</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.1.4.5\"><span class=\"ltx_text\" id=\"S4.T3.1.4.5.1\" style=\"font-size:90%;\">81.26</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.1.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.5.1\"><span class=\"ltx_text\" id=\"S4.T3.1.5.1.1\" style=\"font-size:90%;\">Dict-Rephrasing</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.5.2.1\" style=\"font-size:90%;\">37.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.5.3.1\" style=\"font-size:90%;\">60.20</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.5.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.5.4.1\" style=\"font-size:90%;\">52.98</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T3.1.5.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.5.5.1\" style=\"font-size:90%;\">82.36</span></td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We construct extra translation prompts by incorporating domain-specific bilingual vocabulary into the input source sentences, which highly improve the MT capability of the LLM for rare words (See Table 3).\n",
            "We construct a domain-specific dictionary containing 324,785324785324,785324 , 785 bilingual vocabulary terms in the IT domain. For incorporating the vocabulary, we explore two methods: The Chain-of-Dictionary uses a few chain-of-thought demonstrations as exemplars in prompting Wei et al. (2022b), while the Dictionary-Rephrasing directly incorporates the vocabulary into zero-shot examples without model training Ghazvininejad et al. (2023). Taking the Chinese‚Üí‚Üí\\rightarrow‚ÜíEnglish translation task as an example, the input Chinese sentences are rephrased by simply replacing the occurrences of specific terms that exist in the dictionary, with their target English translations. This also results in a mixed-language input for the LLM. As shown in Figure 4, one source sentence on the XFIT24-TSI dataset as ‚ÄúTranslate the following Chinese sentence into English: Â∑¶ÊåÇËÄ≥ÊùøÂà∞‰∏ªÊùøÁöÑÂ∑¶ÊåÇËÄ≥ËøûÊé•Âô®ÔºàJ6081ÔºâÁöÑ‰ΩéÈÄü‰ø°Âè∑Á∫øÁºÜ‚Äù, is rephrased into a new source sentence as ‚ÄúÂ∑¶ mounting ear plate Âà∞ mainboard ÁöÑÂ∑¶ÊåÇËÄ≥ connector ÔºàJ6081ÔºâÁöÑ‰ΩéÈÄü‰ø°Âè∑Á∫øÁºÜ‚Äù. Particularly, we rephrase the source sentences for all translation pairs in XFIT24-TSI, and name the resulting dataset as XFIT24-TSID. Fine-tuning an LLM on the XFIT24-TSID dataset can enhance its MT capabilities for rare words, by dictionary-based prompting (See Table 3).\n",
            "We apply two dictionary-based prompting methods, as detailed in Section 3.3, respectively to augment the XFIT24-TSI with vocabulary information. Noted that we remove mix-domain data from our dataset for purely evaluating the effects of dictionary-based prompting strategies. We fine-tune the Llama2-7B model using both methods and evaluate their performance on the XFIT24 test set. Table 3 shows the results. The Dictionary-Rephrasing method outperforms the Chain-of-Dictionary method on two translation directions. The results also highlight the positive impact of dictionary-based prompting on the capability of LLMs in domain-specific MT tasks.\n"
        ]
    },
    "S4.T4.4.4": {
        "caption": "The translation performance of LlamaIT on general domain and specific domain. The   represents the translation direction is from Chinese to English, the   represents the translation direction from English to Chinese.",
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T4.4.4\">\n<tr class=\"ltx_tr\" id=\"S4.T4.4.4.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.4.4.5.1\" rowspan=\"2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\"><span class=\"ltx_text\" id=\"S4.T4.4.4.5.1.1\">Models</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T4.4.4.5.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.4.5.2.1\">Flores-101</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T4.4.4.5.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.4.5.3.1\">OPUS-100</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T4.4.4.5.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.4.5.4.1\">IT</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\" id=\"S4.T4.4.4.5.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.4.4.5.5.1\">XFIT24</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.4.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.4.6.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.4.6.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">COMET</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.4.6.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.4.6.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">COMET</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.4.6.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.4.6.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">COMET</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.4.6.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">BLEU</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.4.4.6.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">COMET</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">Llama2-7B<math alttext=\"{}^{\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.1.1.1.1.m1.1\"><semantics id=\"S4.T4.1.1.1.1.m1.1a\"><msup id=\"S4.T4.1.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T4.1.1.1.1.m1.1.1a\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\"/><mo id=\"S4.T4.1.1.1.1.m1.1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.1.cmml\">&#8224;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.m1.1b\"><apply id=\"S4.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1\"><ci id=\"S4.T4.1.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1.1\">&#8224;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.m1.1c\">{}^{\\dagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.1.1.1.1.m1.1d\">start_FLOATSUPERSCRIPT &#8224; end_FLOATSUPERSCRIPT</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">25.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">49.90</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">24.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">8.42</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">17.19</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">-28.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">22.72</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.1.1.1.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">32.79</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.2.2.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">LlamaIT<math alttext=\"{}^{\\dagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.2.2.2.1.m1.1\"><semantics id=\"S4.T4.2.2.2.1.m1.1a\"><msup id=\"S4.T4.2.2.2.1.m1.1.1\" xref=\"S4.T4.2.2.2.1.m1.1.1.cmml\"><mi id=\"S4.T4.2.2.2.1.m1.1.1a\" xref=\"S4.T4.2.2.2.1.m1.1.1.cmml\"/><mo id=\"S4.T4.2.2.2.1.m1.1.1.1\" xref=\"S4.T4.2.2.2.1.m1.1.1.1.cmml\">&#8224;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.1.m1.1b\"><apply id=\"S4.T4.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.1.m1.1.1\"><ci id=\"S4.T4.2.2.2.1.m1.1.1.1.cmml\" xref=\"S4.T4.2.2.2.1.m1.1.1.1\">&#8224;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.1.m1.1c\">{}^{\\dagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.2.2.2.1.m1.1d\">start_FLOATSUPERSCRIPT &#8224; end_FLOATSUPERSCRIPT</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">22.04 <span class=\"ltx_text\" id=\"S4.T4.2.2.2.2.1\" style=\"color:#FF0000;\">(&#8595;3.34)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">45.49 <span class=\"ltx_text\" id=\"S4.T4.2.2.2.3.1\" style=\"color:#FF0000;\">(&#8595;4.41)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">35.91 <span class=\"ltx_text\" id=\"S4.T4.2.2.2.4.1\" style=\"color:#0000FF;\">(&#8593;10.92)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">31.27 <span class=\"ltx_text\" id=\"S4.T4.2.2.2.5.1\" style=\"color:#0000FF;\">(&#8593;22.85)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">36.24 <span class=\"ltx_text\" id=\"S4.T4.2.2.2.6.1\" style=\"color:#0000FF;\">(&#8593;19.05)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">61.97 <span class=\"ltx_text\" id=\"S4.T4.2.2.2.7.1\" style=\"color:#0000FF;\">(&#8593;90.90)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">55.16 <span class=\"ltx_text\" id=\"S4.T4.2.2.2.8.1\" style=\"color:#0000FF;\">(&#8593;32.44)</span>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.2.2.2.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">89.24 <span class=\"ltx_text\" id=\"S4.T4.2.2.2.9.1\" style=\"color:#0000FF;\">(&#8593;56.45)</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">Llama2-7B<math alttext=\"{}^{\\ddagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.3.3.3.1.m1.1\"><semantics id=\"S4.T4.3.3.3.1.m1.1a\"><msup id=\"S4.T4.3.3.3.1.m1.1.1\" xref=\"S4.T4.3.3.3.1.m1.1.1.cmml\"><mi id=\"S4.T4.3.3.3.1.m1.1.1a\" xref=\"S4.T4.3.3.3.1.m1.1.1.cmml\"/><mo id=\"S4.T4.3.3.3.1.m1.1.1.1\" xref=\"S4.T4.3.3.3.1.m1.1.1.1.cmml\">&#8225;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.3.3.3.1.m1.1b\"><apply id=\"S4.T4.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T4.3.3.3.1.m1.1.1\"><ci id=\"S4.T4.3.3.3.1.m1.1.1.1.cmml\" xref=\"S4.T4.3.3.3.1.m1.1.1.1\">&#8225;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.3.3.3.1.m1.1c\">{}^{\\ddagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.3.3.3.1.m1.1d\">start_FLOATSUPERSCRIPT &#8225; end_FLOATSUPERSCRIPT</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">33.49</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">55.18</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">27.54</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">34.58</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">20.65</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">5.36</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">33.26</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T4.3.3.3.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">64.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.4.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.4.4.4.1\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">LlamaIT<math alttext=\"{}^{\\ddagger}\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.4.4.4.1.m1.1\"><semantics id=\"S4.T4.4.4.4.1.m1.1a\"><msup id=\"S4.T4.4.4.4.1.m1.1.1\" xref=\"S4.T4.4.4.4.1.m1.1.1.cmml\"><mi id=\"S4.T4.4.4.4.1.m1.1.1a\" xref=\"S4.T4.4.4.4.1.m1.1.1.cmml\"/><mo id=\"S4.T4.4.4.4.1.m1.1.1.1\" xref=\"S4.T4.4.4.4.1.m1.1.1.1.cmml\">&#8225;</mo></msup><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.4.4.4.1.m1.1b\"><apply id=\"S4.T4.4.4.4.1.m1.1.1.cmml\" xref=\"S4.T4.4.4.4.1.m1.1.1\"><ci id=\"S4.T4.4.4.4.1.m1.1.1.1.cmml\" xref=\"S4.T4.4.4.4.1.m1.1.1.1\">&#8225;</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.4.4.4.1.m1.1c\">{}^{\\ddagger}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.4.4.4.1.m1.1d\">start_FLOATSUPERSCRIPT &#8225; end_FLOATSUPERSCRIPT</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.4.4.4.2\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">32.60 <span class=\"ltx_text\" id=\"S4.T4.4.4.4.2.1\" style=\"color:#FF0000;\">(&#8595;0.89)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.4.4.4.3\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">53.98 <span class=\"ltx_text\" id=\"S4.T4.4.4.4.3.1\" style=\"color:#FF0000;\">(&#8595;1.2)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.4.4.4.4\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">37.79 <span class=\"ltx_text\" id=\"S4.T4.4.4.4.4.1\" style=\"color:#0000FF;\">(&#8593;10.25)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.4.4.4.5\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">15.63 <span class=\"ltx_text\" id=\"S4.T4.4.4.4.5.1\" style=\"color:#FF0000;\">(&#8595;18.95)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.4.4.4.6\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">40.41 <span class=\"ltx_text\" id=\"S4.T4.4.4.4.6.1\" style=\"color:#0000FF;\">(&#8593;19.76)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.4.4.4.7\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">72.98 <span class=\"ltx_text\" id=\"S4.T4.4.4.4.7.1\" style=\"color:#0000FF;\">(&#8593;67.62)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.4.4.4.8\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">63.76 <span class=\"ltx_text\" id=\"S4.T4.4.4.4.8.1\" style=\"color:#0000FF;\">(&#8593;30.50)</span>\n</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T4.4.4.4.9\" style=\"padding-top:2.5pt;padding-bottom:2.5pt;\">101.71 <span class=\"ltx_text\" id=\"S4.T4.4.4.4.9.1\" style=\"color:#0000FF;\">(&#8593;37.04)</span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "To eliminate the need for in-context examples or post-processing (See Figure 5), we fine-tune a general-purpose LLM on the task-specific mix-domain data, which are described via translation instructions, to enhance the MT capabilities of the LLM for domain adaptation at inference time. The training on mix-domain data can repair the zero-shot MT capability of LLMs for general-purpose (See Table 4). This also naturally avoids the inference costs increased by processing input translation examples.\n\n",
            "Through comprehensive experiments, the results show that our LlamaIT can significantly enhance the MT abilities of the LLM, when applied to the targeted domain with rare words, for both Chinese‚Üí‚Üí\\rightarrow‚ÜíEnglish and English‚Üí‚Üí\\rightarrow‚ÜíChinese MT tasks (See Table 4).\n",
            "So far, we have constructed a task-specific and domain-specific dataset for fine-tuning LLMs, to enhance their capabilities for domain-specific MT tasks with rare words. However, this method might weaken their general-purpose MT capabilities due to over-specialization. To solve this problem, we further augment our dataset by integrating several publicly available mix-domain datasets, including UM-Corpus Tian et al. (2014) and OPUS-100 Zhang et al. (2020). We name the final task-specific mix-domain dataset as XFIT24. We observe that the training on mix-domain data can repair the zero-shot MT capability of LLMs for general-purpose (See Table 4).\n",
            "As detailed in Section 3.1, we select the instruction template A described in English to generate instructions for inputs, and employ the Dictionary-Rephrasing strategy to re-write the inputs in our dataset. To repair the zero-shot capabilities of LLMs, we further integrate several publicly available general-domain datasets, to generate the final XFIT24 dataset for evaluation. We use test datasets from two general-domain datasets of Flores-101 and OPUS-100, and two domain-specific datasets of IT and XFIT24. Table 4 shows the results that our LlamaIT outperforms the base Llama2-7B by a significant margin on the domain-specific MT tasks. The results also show that fine-tuning can degrade the MT capability of LLMs in general domains as shown in the results on Flores-101, while mixing the dataset with general-domain data can repair the capability as shown in the results on OPUS-100.\n\n"
        ]
    },
    "S4.T5.1.1": {
        "caption": "Comparisons of trainable parameters and training time differences during the fine-tuning of Llama2-7B using Llama2IT and Full Parameter Fine-Tuning.",
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S4.T5.1.1\">\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.1.1\">Methods</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.2.1\">Parameters</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T5.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T5.1.1.1.3.1\">Training Time(hours/epoch)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.2.1\">LlamaIT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.2.2\">20M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T5.1.1.2.3\">4.20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T5.1.1.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.3.1\">Fine-Tuning</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.3.2\">6,758M</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T5.1.1.3.3\">48.40</td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We perform the prompt-oriented fine-tuning with LoRA on a pre-trained LLM (like Llama2-7B), which can reduce the number of training parameters and thus reduces the cost of model fine-tuning (See Table 5 and 6).\n",
            "Our LlamaIT method demonstrates time efficiency benefits through experiments on fine-tuning. In our LlamaIT, LoRA is employed to accelerate training by reducing parameters. We compared LoRA with full parameter fine-tuning on a 20,000-scale translation dataset, recording parameters and average epoch training time. Results in Table 5 show that LoRA achieving a roughly 10-fold acceleration, significantly reducing the validation time.\n"
        ]
    },
    "S4.T6.1": {
        "caption": "Training time variations during the fine-tuning of Llama2-7B with 0-shot (LlamaIT), 1-shot, and 3-shot prompting.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S4.T6.1\">\n<tr class=\"ltx_tr\" id=\"S4.T6.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.1.1\">Method</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S4.T6.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T6.1.1.2.1\">Training Time(hours/epoch)</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.2.1\">LlamaIT</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T6.1.2.2\">4.20</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.3.1\">1-shot Prompting</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T6.1.3.2\">5.46</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T6.1.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.1.4.1\">3-shot Prompting</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S4.T6.1.4.2\">7.66</td>\n</tr>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We perform the prompt-oriented fine-tuning with LoRA on a pre-trained LLM (like Llama2-7B), which can reduce the number of training parameters and thus reduces the cost of model fine-tuning (See Table 5 and 6).\n",
            "We further verify that our LlamaIT with zero-shot prompting saves more training time compared to 1-shot and few-shot prompting methods (Table 6). It suggests that adopting LlamaIT effectively manages training time costs, offering an economical and viable option for experimental validation and model training while maintaining efficiency.\n"
        ]
    }
}