{
    "id_table_1": {
        "caption": "Table 1:  Comparison of  FRAMES  against other datasets.  FRAMES  provides a combination of evaluation samples to test the factuality, retrieval, and reasoning of RAG systems. The dataset also covers multi-hop/step questions along with temporal disambiguation.",
        "table": "S1.T1.1",
        "footnotes": [],
        "references": [
            "Given these findings, we decided to use the core instruction for generating questions that combine information from multiple articles as a guide for human annotation, shown in Figure  1 . This approach aimed to leverage the challenging nature of the synthetic questions while also mitigating the issues of hallucination present in LLM-generated content. Human annotators were tasked with creating questions that required information from multiple Wikipedia articles, following a similar structure to the synthetic prompts but with greater reliability and accuracy. The outcome of this human annotation resulted in 824 questions with their correct responses along with the list of Wikipedia articles needed to answer the questions. We also ask the human annotators to label each question based on five reasoning types, i.e, Numerical Reasoning, Tabular Reasoning, Multiple Constraints, Temporal Reasoning, and Post-Processing, described in more details in Table  2 . Please note that a question can belong to multiple reasoning types. To ensure the highest quality annotations, we engaged a team of carefully vetted experts with extensive experience in question generation and complex reasoning tasks.",
            "After obtaining a high-quality test set, we evaluate state-of-the-art LLMs on their ability to answer questions that require proficiency in factuality, retrieval, and reasoning. Our analysis is divided into two sections: (1) Single-step Evaluations (Section  3.1 ): Here, we evaluate the LLMs based on a single-shot inference, where the idea is to ask the question and assess the response after a single inference call. This evaluation is further divided into cases with and without retrieval to analyze the impact of retrieval on performance. (2) Multi-Step Evaluations (Section  3.2 ): In this case, we evaluate the models after making more than a single inference step, focusing on scenarios where retrieval is explicitly required. The motivation for multi-step evaluations is to determine whether forcing the model to retrieve and reason across multiple steps could lead to performance improvements. Next, we describe the details of the two sets of experiments.",
            "Based on the findings from the previous experiment with single-step evaluations, where we observed an increase in performance when related articles are added to the context, wew were led to explore a setting where the model is compelled to plan its search for relevant Wikipedia articles in order to find answers. More specifically, we design a pipeline where the model is asked a question along with the instruction to generate  k k k italic_k  search queries which are then used to extract the top-n_docs Wikipedia articles with the highest BM25 scores. These documents are then appended to the context. This process of query generation and retrieved article augmentation is carried forward for  n n n italic_n  steps. Once the  n n n italic_n  steps of retrieval are completed, the model is asked to answer the question based on the articles appended in the context, as shown in Algorithm  1 . We conduct two sets of experiments here: (1) Vanilla with no explicit planning instructions, and (2) With search planning instructions to help the model navigate the search process efficiently. To implement this pipeline, we used the simplest document retrieval component which is essentially an index of Wikipedia pages, where the articles with the highest BM25 scores for each query are returned to the LLM and added to the context. This retrieval component is used instead of making direct calls to an online search engine for two reasons: (1) We would like to keep the retrieval system constant to clearly evaluate the search planning capability of the LLMs instead of the retrieval systems capability in returning the most relevant articles, and (2) The BM25-based retrieval system makes our pipeline reproducible and limits the search space to Wikipedia pages only, as the questions were generated from Wikipedia articles only.",
            "Evaluating Retrieval-Augmented Generation (RAG) systems has become increasingly important as these models integrate retrieval mechanisms with generative capabilities to enhance factual accuracy and reasoning (Yu et al.,  2024b ) . Existing benchmarks, such as NaturalQuestions  (Kwiatkowski et al.,  2019 ) , TriviaQA  (Joshi et al.,  2017 ) , and ELI5  (Fan et al.,  2019 ) , have been used to evaluate RAG models, but they often focus on specific aspects like retrieval accuracy or single-turn question answering without considering the full complexity of real-world applications. For instance, NaturalQuestions primarily tests retrieval precision, while TriviaQA emphasizes factual correctness in trivia-style questions. ELI5, on the other hand, is designed for explainability but does not rigorously assess the multi-hop reasoning necessary for synthesizing information from multiple sources. These benchmarks, while valuable, tend to evaluate RAG systems in a piecemeal fashion, missing the comprehensive assessment needed to truly measure their end-to-end capabilities. We provide additional comparisons against other datasets in Table  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  This table provides descriptions of the different reasoning types to which each question in  FRAMES  belongs. The distribution of samples belonging to each reasoning type is shown in Figure  2 .",
        "table": "S2.T2.1",
        "footnotes": [],
        "references": [
            "Given these findings, we decided to use the core instruction for generating questions that combine information from multiple articles as a guide for human annotation, shown in Figure  1 . This approach aimed to leverage the challenging nature of the synthetic questions while also mitigating the issues of hallucination present in LLM-generated content. Human annotators were tasked with creating questions that required information from multiple Wikipedia articles, following a similar structure to the synthetic prompts but with greater reliability and accuracy. The outcome of this human annotation resulted in 824 questions with their correct responses along with the list of Wikipedia articles needed to answer the questions. We also ask the human annotators to label each question based on five reasoning types, i.e, Numerical Reasoning, Tabular Reasoning, Multiple Constraints, Temporal Reasoning, and Post-Processing, described in more details in Table  2 . Please note that a question can belong to multiple reasoning types. To ensure the highest quality annotations, we engaged a team of carefully vetted experts with extensive experience in question generation and complex reasoning tasks.",
            "The dataset comprises questions related to a diverse set of topics from Wikipedia, involving subjects such as history, sports, science, animals, health, etc. Each question in our dataset require 2-15 Wikipedia articles to answer, with the distribution of the percentage of dataset requiring different numbers of Wikipedia articles shown in Figure  2  (left). Approximately 36% of questions require two articles to answer,   similar-to \\sim  35% require three articles,   similar-to \\sim  16% require four articles, and so on. This distribution also represents the general trend of queries asked from LLMs in the real world  (Liu et al.,  2009 ) , since the proportion of questions requiring two articles is higher than more complicated questions requiring a greater number of articles. Additionally, we have a healthy distribution of questions belonging to different reasoning types, shown in Figure  2  (right). Questions requiring reasoning over multiple constraints hold the highest percentage of data samples in the test (  similar-to \\sim  36%), followed by questions requiring numerical reasoning (  similar-to \\sim  20%). Please note that many questions in the dataset also require a combination of different reasoning abilities to find an answer.",
            "After obtaining a high-quality test set, we evaluate state-of-the-art LLMs on their ability to answer questions that require proficiency in factuality, retrieval, and reasoning. Our analysis is divided into two sections: (1) Single-step Evaluations (Section  3.1 ): Here, we evaluate the LLMs based on a single-shot inference, where the idea is to ask the question and assess the response after a single inference call. This evaluation is further divided into cases with and without retrieval to analyze the impact of retrieval on performance. (2) Multi-Step Evaluations (Section  3.2 ): In this case, we evaluate the models after making more than a single inference step, focusing on scenarios where retrieval is explicitly required. The motivation for multi-step evaluations is to determine whether forcing the model to retrieve and reason across multiple steps could lead to performance improvements. Next, we describe the details of the two sets of experiments."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  This table presents the accuracy performance of  Gemini-Pro-1.5-0514  (G-Pro-1.5),  Gemini-Flash-1.5-0514  (G-Flash-1.5),  Gemma2-27b , and  Gemma2-9b  on our proposed evaluation dataset. Please note that the performance of Gemma models is not reported for cases requiring longer context due to the small maximum context length of the model.",
        "table": "S3.T3.2",
        "footnotes": [],
        "references": [
            "After obtaining a high-quality test set, we evaluate state-of-the-art LLMs on their ability to answer questions that require proficiency in factuality, retrieval, and reasoning. Our analysis is divided into two sections: (1) Single-step Evaluations (Section  3.1 ): Here, we evaluate the LLMs based on a single-shot inference, where the idea is to ask the question and assess the response after a single inference call. This evaluation is further divided into cases with and without retrieval to analyze the impact of retrieval on performance. (2) Multi-Step Evaluations (Section  3.2 ): In this case, we evaluate the models after making more than a single inference step, focusing on scenarios where retrieval is explicitly required. The motivation for multi-step evaluations is to determine whether forcing the model to retrieve and reason across multiple steps could lead to performance improvements. Next, we describe the details of the two sets of experiments.",
            "Based on results shown in Table  3 , we observe that naive prompting attains a performance of   similar-to \\sim  40% with gradual increases when including BM25 retrieved articles for  Gemini-Pro-1.5-0514 . The model achieves an accuracy of   similar-to \\sim  45% when the number of documents in the context is 2, and   similar-to \\sim  47% when double the number of articles are added to the context. These improvements demonstrate the room for enhancement when the model is able to retrieve relevant articles required to answer the question. The core reason behind these improvements is the improvement in recall in the articles present in context which increased from 0.12 (BM25-R(n_docs = 2) to 0.15 (BM25-R (n_docs = 4)). In addition to these approaches, we observe an accuracy of   similar-to \\sim  72% for  Gemini-Pro-1.5-0514  when all the gold Wikipedia articles are provided in the context, which we call Oracle Prompt. Out of   similar-to \\sim  28% samples where the model made errors,   similar-to \\sim  80% of those misclassifications belong to numerical, tabular, and post-processing categories. Hence, these misclassifications show the reasoning gaps in model performance where even after providing all the relevant facts, the model failed to reason through the different facts to provide a correct answer to the question. The accuracies obtained by the Naive Prompt and Oracle Prompt can be considered as the lower bound (when no relevant articles were provided to the model) and upper bound (when all relevant articles were provided to the model) of model performances on  FRAMES . This pattern can also be seen in Figure  3  where we plotted accuracy for each reasoning type and observe that the model performed the lowest in numerical, post-processing, and tabular reasoning tasks. We also observe that adding BM25 retrieved articles primarily helped with questions requiring reasoning through multiple constraints (  similar-to \\sim  8% improvement) and post-processing (  similar-to \\sim  10% improvement). This aligns well with the fact that providing more relevant articles helps in obtaining facts for each constraint, leading to improvements in performance. We take these learnings and experiment with a more complicated setup where the model is asked to find answer to questions through multiple iterations instead of a single step."
        ]
    },
    "global_footnotes": [
        "Dataset link :"
    ]
}