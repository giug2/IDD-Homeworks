{
    "PAPER'S NUMBER OF TABLES": 6,
    "S3.SS1.33": {
        "caption": "Table 1: Test accuracies and barriers of two trained models w/ and w/o connectivity loss. “Ind. Acc.” refers to 0.5∗𝒜​(𝐰1)+0.5∗𝒜​(𝐰2)0.5𝒜subscript𝐰10.5𝒜subscript𝐰20.5*\\mathcal{A}(\\mathbf{w}_{1})+0.5*\\mathcal{A}(\\mathbf{w}_{2}), and “Fused Acc.” refers to 𝒜​(0.5∗𝐰1+0.5∗𝐰2)𝒜0.5subscript𝐰10.5subscript𝐰2\\mathcal{A}(0.5*\\mathbf{w}_{1}+0.5*\\mathbf{w}_{2}). It validates the transitivity of LMC, stating that by leveraging the anchor model, the barriers of LMC are largely reduced. CIFAR-10.",
        "table": "<table id=\"S3.SS1.22.22.18\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.SS1.22.22.18.19.1\" class=\"ltx_tr\">\n<th id=\"S3.SS1.22.22.18.19.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.SS1.22.22.18.19.1.1.1\" class=\"ltx_text ltx_font_bold\">Models</span></th>\n<th id=\"S3.SS1.22.22.18.19.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.SS1.22.22.18.19.1.2.1\" class=\"ltx_text ltx_font_bold\">Metrics</span></th>\n<td id=\"S3.SS1.22.22.18.19.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S3.SS1.22.22.18.19.1.3.1\" class=\"ltx_text ltx_font_bold\">Vanilla CE Loss</span></td>\n<td id=\"S3.SS1.22.22.18.19.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.SS1.22.22.18.19.1.4.1\" class=\"ltx_text ltx_font_bold\">w/ Connectivity Loss</span></td>\n</tr>\n<tr id=\"S3.SS1.6.6.2.2\" class=\"ltx_tr\">\n<th id=\"S3.SS1.6.6.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S3.SS1.6.6.2.2.3.1\" class=\"ltx_text\">CNN</span></th>\n<th id=\"S3.SS1.6.6.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ind. Acc.</th>\n<td id=\"S3.SS1.5.5.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.5.5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"64.0\\pm 0.5\" display=\"inline\"><semantics id=\"S3.SS1.5.5.1.1.1.m1.1a\"><mrow id=\"S3.SS1.5.5.1.1.1.m1.1.1\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.cmml\"><mn id=\"S3.SS1.5.5.1.1.1.m1.1.1.2\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.2.cmml\">64.0</mn><mo id=\"S3.SS1.5.5.1.1.1.m1.1.1.1\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.5.5.1.1.1.m1.1.1.3\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.5.5.1.1.1.m1.1b\"><apply id=\"S3.SS1.5.5.1.1.1.m1.1.1.cmml\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.5.5.1.1.1.m1.1.1.1.cmml\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.5.5.1.1.1.m1.1.1.2.cmml\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.2\">64.0</cn><cn type=\"float\" id=\"S3.SS1.5.5.1.1.1.m1.1.1.3.cmml\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.5.5.1.1.1.m1.1c\">64.0\\pm 0.5</annotation></semantics></math></td>\n<td id=\"S3.SS1.6.6.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.6.6.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"63.9\\pm 1.4\" display=\"inline\"><semantics id=\"S3.SS1.6.6.2.2.2.m1.1a\"><mrow id=\"S3.SS1.6.6.2.2.2.m1.1.1\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.cmml\"><mn id=\"S3.SS1.6.6.2.2.2.m1.1.1.2\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.2.cmml\">63.9</mn><mo id=\"S3.SS1.6.6.2.2.2.m1.1.1.1\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.6.6.2.2.2.m1.1.1.3\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.3.cmml\">1.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.6.6.2.2.2.m1.1b\"><apply id=\"S3.SS1.6.6.2.2.2.m1.1.1.cmml\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.6.6.2.2.2.m1.1.1.1.cmml\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.6.6.2.2.2.m1.1.1.2.cmml\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.2\">63.9</cn><cn type=\"float\" id=\"S3.SS1.6.6.2.2.2.m1.1.1.3.cmml\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.3\">1.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.6.6.2.2.2.m1.1c\">63.9\\pm 1.4</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.8.8.4.4\" class=\"ltx_tr\">\n<th id=\"S3.SS1.8.8.4.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fused Acc.</th>\n<td id=\"S3.SS1.7.7.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><math id=\"S3.SS1.7.7.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"11.5\\pm 0.9\" display=\"inline\"><semantics id=\"S3.SS1.7.7.3.3.1.m1.1a\"><mrow id=\"S3.SS1.7.7.3.3.1.m1.1.1\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.cmml\"><mn id=\"S3.SS1.7.7.3.3.1.m1.1.1.2\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.2.cmml\">11.5</mn><mo id=\"S3.SS1.7.7.3.3.1.m1.1.1.1\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.7.7.3.3.1.m1.1.1.3\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.3.cmml\">0.9</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.7.7.3.3.1.m1.1b\"><apply id=\"S3.SS1.7.7.3.3.1.m1.1.1.cmml\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.7.7.3.3.1.m1.1.1.1.cmml\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.7.7.3.3.1.m1.1.1.2.cmml\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.2\">11.5</cn><cn type=\"float\" id=\"S3.SS1.7.7.3.3.1.m1.1.1.3.cmml\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.3\">0.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.7.7.3.3.1.m1.1c\">11.5\\pm 0.9</annotation></semantics></math></td>\n<td id=\"S3.SS1.8.8.4.4.2\" class=\"ltx_td ltx_align_center\"><math id=\"S3.SS1.8.8.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"32.1\\pm 9.0\" display=\"inline\"><semantics id=\"S3.SS1.8.8.4.4.2.m1.1a\"><mrow id=\"S3.SS1.8.8.4.4.2.m1.1.1\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.cmml\"><mn id=\"S3.SS1.8.8.4.4.2.m1.1.1.2\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.2.cmml\">32.1</mn><mo id=\"S3.SS1.8.8.4.4.2.m1.1.1.1\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.8.8.4.4.2.m1.1.1.3\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.3.cmml\">9.0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.8.8.4.4.2.m1.1b\"><apply id=\"S3.SS1.8.8.4.4.2.m1.1.1.cmml\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.8.8.4.4.2.m1.1.1.1.cmml\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.8.8.4.4.2.m1.1.1.2.cmml\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.2\">32.1</cn><cn type=\"float\" id=\"S3.SS1.8.8.4.4.2.m1.1.1.3.cmml\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.3\">9.0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.8.8.4.4.2.m1.1c\">32.1\\pm 9.0</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.10.10.6.6\" class=\"ltx_tr\">\n<th id=\"S3.SS1.10.10.6.6.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Acc. Barrier</th>\n<td id=\"S3.SS1.9.9.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.9.9.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"0.821\" display=\"inline\"><semantics id=\"S3.SS1.9.9.5.5.1.m1.1a\"><mn id=\"S3.SS1.9.9.5.5.1.m1.1.1\" xref=\"S3.SS1.9.9.5.5.1.m1.1.1.cmml\">0.821</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.9.9.5.5.1.m1.1b\"><cn type=\"float\" id=\"S3.SS1.9.9.5.5.1.m1.1.1.cmml\" xref=\"S3.SS1.9.9.5.5.1.m1.1.1\">0.821</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.9.9.5.5.1.m1.1c\">0.821</annotation></semantics></math></td>\n<td id=\"S3.SS1.10.10.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.10.10.6.6.2.m1.1\" class=\"ltx_math_unparsed\" alttext=\"0.495\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(39.7\\%\\downarrow)}\" display=\"inline\"><semantics id=\"S3.SS1.10.10.6.6.2.m1.1a\"><mrow id=\"S3.SS1.10.10.6.6.2.m1.1b\"><mn id=\"S3.SS1.10.10.6.6.2.m1.1.1\">0.495</mn><mrow id=\"S3.SS1.10.10.6.6.2.m1.1.2\"><mo lspace=\"0.170em\" mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.1\">(</mo><mn mathcolor=\"#33B333\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.2\">39.7</mn><mo mathcolor=\"#33B333\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.3\">%</mo><mo mathcolor=\"#33B333\" rspace=\"0em\" stretchy=\"false\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.4\">↓</mo><mo mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.5\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S3.SS1.10.10.6.6.2.m1.1c\">0.495\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(39.7\\%\\downarrow)}</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.12.12.8.8\" class=\"ltx_tr\">\n<th id=\"S3.SS1.12.12.8.8.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S3.SS1.12.12.8.8.3.1\" class=\"ltx_text\">ResNet 20</span></th>\n<th id=\"S3.SS1.12.12.8.8.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ind. Acc.</th>\n<td id=\"S3.SS1.11.11.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.11.11.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"66.7\\pm 0.9\" display=\"inline\"><semantics id=\"S3.SS1.11.11.7.7.1.m1.1a\"><mrow id=\"S3.SS1.11.11.7.7.1.m1.1.1\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.cmml\"><mn id=\"S3.SS1.11.11.7.7.1.m1.1.1.2\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.2.cmml\">66.7</mn><mo id=\"S3.SS1.11.11.7.7.1.m1.1.1.1\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.11.11.7.7.1.m1.1.1.3\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.3.cmml\">0.9</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.11.11.7.7.1.m1.1b\"><apply id=\"S3.SS1.11.11.7.7.1.m1.1.1.cmml\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.11.11.7.7.1.m1.1.1.1.cmml\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.11.11.7.7.1.m1.1.1.2.cmml\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.2\">66.7</cn><cn type=\"float\" id=\"S3.SS1.11.11.7.7.1.m1.1.1.3.cmml\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.3\">0.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.11.11.7.7.1.m1.1c\">66.7\\pm 0.9</annotation></semantics></math></td>\n<td id=\"S3.SS1.12.12.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.12.12.8.8.2.m1.1\" class=\"ltx_Math\" alttext=\"69.1\\pm 2.4\" display=\"inline\"><semantics id=\"S3.SS1.12.12.8.8.2.m1.1a\"><mrow id=\"S3.SS1.12.12.8.8.2.m1.1.1\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.cmml\"><mn id=\"S3.SS1.12.12.8.8.2.m1.1.1.2\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.2.cmml\">69.1</mn><mo id=\"S3.SS1.12.12.8.8.2.m1.1.1.1\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.12.12.8.8.2.m1.1.1.3\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.3.cmml\">2.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.12.12.8.8.2.m1.1b\"><apply id=\"S3.SS1.12.12.8.8.2.m1.1.1.cmml\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.12.12.8.8.2.m1.1.1.1.cmml\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.12.12.8.8.2.m1.1.1.2.cmml\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.2\">69.1</cn><cn type=\"float\" id=\"S3.SS1.12.12.8.8.2.m1.1.1.3.cmml\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.3\">2.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.12.12.8.8.2.m1.1c\">69.1\\pm 2.4</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.14.14.10.10\" class=\"ltx_tr\">\n<th id=\"S3.SS1.14.14.10.10.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fused Acc.</th>\n<td id=\"S3.SS1.13.13.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><math id=\"S3.SS1.13.13.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"13.0\\pm 3.8\" display=\"inline\"><semantics id=\"S3.SS1.13.13.9.9.1.m1.1a\"><mrow id=\"S3.SS1.13.13.9.9.1.m1.1.1\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.cmml\"><mn id=\"S3.SS1.13.13.9.9.1.m1.1.1.2\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.2.cmml\">13.0</mn><mo id=\"S3.SS1.13.13.9.9.1.m1.1.1.1\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.13.13.9.9.1.m1.1.1.3\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.3.cmml\">3.8</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.13.13.9.9.1.m1.1b\"><apply id=\"S3.SS1.13.13.9.9.1.m1.1.1.cmml\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.13.13.9.9.1.m1.1.1.1.cmml\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.13.13.9.9.1.m1.1.1.2.cmml\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.2\">13.0</cn><cn type=\"float\" id=\"S3.SS1.13.13.9.9.1.m1.1.1.3.cmml\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.3\">3.8</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.13.13.9.9.1.m1.1c\">13.0\\pm 3.8</annotation></semantics></math></td>\n<td id=\"S3.SS1.14.14.10.10.2\" class=\"ltx_td ltx_align_center\"><math id=\"S3.SS1.14.14.10.10.2.m1.1\" class=\"ltx_Math\" alttext=\"40.5\\pm 3.5\" display=\"inline\"><semantics id=\"S3.SS1.14.14.10.10.2.m1.1a\"><mrow id=\"S3.SS1.14.14.10.10.2.m1.1.1\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.cmml\"><mn id=\"S3.SS1.14.14.10.10.2.m1.1.1.2\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.2.cmml\">40.5</mn><mo id=\"S3.SS1.14.14.10.10.2.m1.1.1.1\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.14.14.10.10.2.m1.1.1.3\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.3.cmml\">3.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.14.14.10.10.2.m1.1b\"><apply id=\"S3.SS1.14.14.10.10.2.m1.1.1.cmml\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.14.14.10.10.2.m1.1.1.1.cmml\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.14.14.10.10.2.m1.1.1.2.cmml\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.2\">40.5</cn><cn type=\"float\" id=\"S3.SS1.14.14.10.10.2.m1.1.1.3.cmml\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.3\">3.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.14.14.10.10.2.m1.1c\">40.5\\pm 3.5</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.16.16.12.12\" class=\"ltx_tr\">\n<th id=\"S3.SS1.16.16.12.12.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Acc. Barrier</th>\n<td id=\"S3.SS1.15.15.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.15.15.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"0.805\" display=\"inline\"><semantics id=\"S3.SS1.15.15.11.11.1.m1.1a\"><mn id=\"S3.SS1.15.15.11.11.1.m1.1.1\" xref=\"S3.SS1.15.15.11.11.1.m1.1.1.cmml\">0.805</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.15.15.11.11.1.m1.1b\"><cn type=\"float\" id=\"S3.SS1.15.15.11.11.1.m1.1.1.cmml\" xref=\"S3.SS1.15.15.11.11.1.m1.1.1\">0.805</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.15.15.11.11.1.m1.1c\">0.805</annotation></semantics></math></td>\n<td id=\"S3.SS1.16.16.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.16.16.12.12.2.m1.1\" class=\"ltx_math_unparsed\" alttext=\"0.415\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(44.1\\%\\downarrow)}\" display=\"inline\"><semantics id=\"S3.SS1.16.16.12.12.2.m1.1a\"><mrow id=\"S3.SS1.16.16.12.12.2.m1.1b\"><mn id=\"S3.SS1.16.16.12.12.2.m1.1.1\">0.415</mn><mrow id=\"S3.SS1.16.16.12.12.2.m1.1.2\"><mo lspace=\"0.170em\" mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.1\">(</mo><mn mathcolor=\"#33B333\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.2\">44.1</mn><mo mathcolor=\"#33B333\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.3\">%</mo><mo mathcolor=\"#33B333\" rspace=\"0em\" stretchy=\"false\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.4\">↓</mo><mo mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.5\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S3.SS1.16.16.12.12.2.m1.1c\">0.415\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(44.1\\%\\downarrow)}</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.18.18.14.14\" class=\"ltx_tr\">\n<th id=\"S3.SS1.18.18.14.14.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"4\">\n<span id=\"S3.SS1.18.18.14.14.3.1\" class=\"ltx_text\">Pretrained </span>\nResNet18</th>\n<th id=\"S3.SS1.18.18.14.14.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ind. Acc.</th>\n<td id=\"S3.SS1.17.17.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.17.17.13.13.1.m1.1\" class=\"ltx_Math\" alttext=\"55.8\\pm 6.6\" display=\"inline\"><semantics id=\"S3.SS1.17.17.13.13.1.m1.1a\"><mrow id=\"S3.SS1.17.17.13.13.1.m1.1.1\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.cmml\"><mn id=\"S3.SS1.17.17.13.13.1.m1.1.1.2\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.2.cmml\">55.8</mn><mo id=\"S3.SS1.17.17.13.13.1.m1.1.1.1\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.17.17.13.13.1.m1.1.1.3\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.3.cmml\">6.6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.17.17.13.13.1.m1.1b\"><apply id=\"S3.SS1.17.17.13.13.1.m1.1.1.cmml\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.17.17.13.13.1.m1.1.1.1.cmml\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.17.17.13.13.1.m1.1.1.2.cmml\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.2\">55.8</cn><cn type=\"float\" id=\"S3.SS1.17.17.13.13.1.m1.1.1.3.cmml\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.3\">6.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.17.17.13.13.1.m1.1c\">55.8\\pm 6.6</annotation></semantics></math></td>\n<td id=\"S3.SS1.18.18.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.18.18.14.14.2.m1.1\" class=\"ltx_Math\" alttext=\"64.5\\pm 0.3\" display=\"inline\"><semantics id=\"S3.SS1.18.18.14.14.2.m1.1a\"><mrow id=\"S3.SS1.18.18.14.14.2.m1.1.1\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.cmml\"><mn id=\"S3.SS1.18.18.14.14.2.m1.1.1.2\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.2.cmml\">64.5</mn><mo id=\"S3.SS1.18.18.14.14.2.m1.1.1.1\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.18.18.14.14.2.m1.1.1.3\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.3.cmml\">0.3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.18.18.14.14.2.m1.1b\"><apply id=\"S3.SS1.18.18.14.14.2.m1.1.1.cmml\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.18.18.14.14.2.m1.1.1.1.cmml\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.18.18.14.14.2.m1.1.1.2.cmml\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.2\">64.5</cn><cn type=\"float\" id=\"S3.SS1.18.18.14.14.2.m1.1.1.3.cmml\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.3\">0.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.18.18.14.14.2.m1.1c\">64.5\\pm 0.3</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.20.20.16.16\" class=\"ltx_tr\">\n<th id=\"S3.SS1.20.20.16.16.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fused Acc.</th>\n<td id=\"S3.SS1.19.19.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><math id=\"S3.SS1.19.19.15.15.1.m1.1\" class=\"ltx_Math\" alttext=\"10.0\\pm 0.0\" display=\"inline\"><semantics id=\"S3.SS1.19.19.15.15.1.m1.1a\"><mrow id=\"S3.SS1.19.19.15.15.1.m1.1.1\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.cmml\"><mn id=\"S3.SS1.19.19.15.15.1.m1.1.1.2\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.2.cmml\">10.0</mn><mo id=\"S3.SS1.19.19.15.15.1.m1.1.1.1\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.19.19.15.15.1.m1.1.1.3\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.3.cmml\">0.0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.19.19.15.15.1.m1.1b\"><apply id=\"S3.SS1.19.19.15.15.1.m1.1.1.cmml\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.19.19.15.15.1.m1.1.1.1.cmml\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.19.19.15.15.1.m1.1.1.2.cmml\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.2\">10.0</cn><cn type=\"float\" id=\"S3.SS1.19.19.15.15.1.m1.1.1.3.cmml\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.3\">0.0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.19.19.15.15.1.m1.1c\">10.0\\pm 0.0</annotation></semantics></math></td>\n<td id=\"S3.SS1.20.20.16.16.2\" class=\"ltx_td ltx_align_center\"><math id=\"S3.SS1.20.20.16.16.2.m1.1\" class=\"ltx_Math\" alttext=\"62.1\\pm 0.4\" display=\"inline\"><semantics id=\"S3.SS1.20.20.16.16.2.m1.1a\"><mrow id=\"S3.SS1.20.20.16.16.2.m1.1.1\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.cmml\"><mn id=\"S3.SS1.20.20.16.16.2.m1.1.1.2\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.2.cmml\">62.1</mn><mo id=\"S3.SS1.20.20.16.16.2.m1.1.1.1\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.1.cmml\">±</mo><mn id=\"S3.SS1.20.20.16.16.2.m1.1.1.3\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.3.cmml\">0.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.20.20.16.16.2.m1.1b\"><apply id=\"S3.SS1.20.20.16.16.2.m1.1.1.cmml\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.20.20.16.16.2.m1.1.1.1.cmml\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.20.20.16.16.2.m1.1.1.2.cmml\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.2\">62.1</cn><cn type=\"float\" id=\"S3.SS1.20.20.16.16.2.m1.1.1.3.cmml\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.3\">0.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.20.20.16.16.2.m1.1c\">62.1\\pm 0.4</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.22.22.18.18\" class=\"ltx_tr\">\n<th id=\"S3.SS1.22.22.18.18.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">Acc. Barrier</th>\n<td id=\"S3.SS1.21.21.17.17.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><math id=\"S3.SS1.21.21.17.17.1.m1.1\" class=\"ltx_Math\" alttext=\"0.819\" display=\"inline\"><semantics id=\"S3.SS1.21.21.17.17.1.m1.1a\"><mn id=\"S3.SS1.21.21.17.17.1.m1.1.1\" xref=\"S3.SS1.21.21.17.17.1.m1.1.1.cmml\">0.819</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.21.21.17.17.1.m1.1b\"><cn type=\"float\" id=\"S3.SS1.21.21.17.17.1.m1.1.1.cmml\" xref=\"S3.SS1.21.21.17.17.1.m1.1.1\">0.819</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.21.21.17.17.1.m1.1c\">0.819</annotation></semantics></math></td>\n<td id=\"S3.SS1.22.22.18.18.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math id=\"S3.SS1.22.22.18.18.2.m1.1\" class=\"ltx_math_unparsed\" alttext=\"0.038\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(95.4\\%\\downarrow)}\" display=\"inline\"><semantics id=\"S3.SS1.22.22.18.18.2.m1.1a\"><mrow id=\"S3.SS1.22.22.18.18.2.m1.1b\"><mn id=\"S3.SS1.22.22.18.18.2.m1.1.1\">0.038</mn><mrow id=\"S3.SS1.22.22.18.18.2.m1.1.2\"><mo lspace=\"0.170em\" mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.1\">(</mo><mn mathcolor=\"#33B333\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.2\">95.4</mn><mo mathcolor=\"#33B333\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.3\">%</mo><mo mathcolor=\"#33B333\" rspace=\"0em\" stretchy=\"false\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.4\">↓</mo><mo mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.5\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S3.SS1.22.22.18.18.2.m1.1c\">0.038\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(95.4\\%\\downarrow)}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "We first present the connectivity loss given the anchor model, which is similar to previous literature (Wortsman et al., 2021; Garipov et al., 2018). The connectivity loss is as follows,where 𝐰anc∗superscriptsubscript𝐰anc\\mathbf{w}_{\\text{anc}}^{*} is the fixed anchor model and 𝐰𝐰\\mathbf{w} is the model for training. Then, we incorporate the connectivity loss into the vanilla cross entropy (CE) loss, formulated into the following overall learning objective,where ℒ​(𝐰)ℒ𝐰\\mathcal{L}(\\mathbf{w}) is the vanilla CE loss and β𝛽\\beta is the hyperparameter controlling the strength of the connectivity loss.We let 𝐰anc∗superscriptsubscript𝐰anc\\mathbf{w}_{\\text{anc}}^{*} be the fixed trained anchor model and independently train two models 𝐰1∗superscriptsubscript𝐰1\\mathbf{w}_{1}^{*} and 𝐰2∗superscriptsubscript𝐰2\\mathbf{w}_{2}^{*} according to Equation 8. According to Theorem 3.5, the 𝐰1∗superscriptsubscript𝐰1\\mathbf{w}_{1}^{*} and 𝐰2∗superscriptsubscript𝐰2\\mathbf{w}_{2}^{*}’s LMC barriers will be reduced if the transitivity holds. Note that 𝐰1subscript𝐰1\\mathbf{w}_{1} and 𝐰2subscript𝐰2\\mathbf{w}_{2} can have the same or different initializations, and the transitivity still holds; in the experiments, we make stricter verifications by setting different initializations.Empirical results. We conduct experiments in subsection 3.1 and Figure 2. The anchor model is a mode independently trained with vanilla CE loss using a different random seed. In subsection 3.1, training with the connectivity loss can largely reduce the barriers of LMC by utilizing the anchor model, even if two models have different initializations and never communicate with each other. More intuitive landscape visualizations are in Figure 2. It can be seen that the connectivity loss can eliminate the barrier between the anchor model and the trained model, and due to the transitivity of LMC, the barrier between the two independent models is also reduced. The experiments verify the transitivity of LMC between two models, and we will show that this transitivity can be extended to the connectivity of multiple models.We study the group connectivity among multiple models and propose the barrier of group connectivity akin to Definition 2.1 of LMC. For brevity, we only present the definition of accuracy barriers.Group connectivity. The group connectivity of model set {𝐰i}i=1Ksuperscriptsubscriptsubscript𝐰𝑖𝑖1𝐾\\{\\mathbf{w}_{i}\\}_{i=1}^{K} is depicted by the loss and accuracy barrier defined as:where ℒℒ\\mathcal{L} is the loss and 𝒜𝒜\\mathcal{A} is the accuracy function. A lower barrier refers to better group connectivity.We prove the transitivity of group connectivity that individually training several models and improving the LMC between one common anchor model will result in better group connectivity among the trained ones. In addition, we consider the data heterogeneity of practical FL in group connectivity by giving the following definition.Data heterogeneity. Similar to Li et al. (2019), we use the minimum to measure the degree of heterogeneity among the group of individual workers (e.g., clients in FL and modes in LMC). Let 𝐰∗superscript𝐰\\mathbf{w}^{*} be a global minimum of all workers and 𝐰i∗superscriptsubscript𝐰𝑖\\mathbf{w}_{i}^{*} is the minimum value of worker i𝑖i closest to 𝐰∗superscript𝐰\\mathbf{w}^{*}. We use the term Γ=maxi⁡‖𝐰i∗−𝐰∗‖2,i∈[K]formulae-sequenceΓsubscript𝑖subscriptnormsuperscriptsubscript𝐰𝑖superscript𝐰2𝑖delimited-[]𝐾\\Gamma=\\max_{i}\\|\\mathbf{w}_{i}^{*}-\\mathbf{w}^{*}\\|_{2},\\,i\\in[K] for quantifying the degree of data heterogeneity.We define a two-layer neural network with ReLU activation, and the function is f𝐯,𝐔​(𝐱)=𝐯⊤​σ​(𝐔𝐱)subscript𝑓𝐯𝐔𝐱superscript𝐯top𝜎𝐔𝐱f_{\\mathbf{v},\\mathbf{U}}(\\mathbf{x})=\\mathbf{v}^{\\top}\\sigma(\\mathbf{U}\\mathbf{x}) where σ​(⋅)𝜎⋅\\sigma(\\cdot) is the ReLU activation function. 𝐯∈ℝh𝐯superscriptℝℎ\\mathbf{v}\\in\\mathbb{R}^{h} and 𝐔∈ℝh×l𝐔superscriptℝℎ𝑙\\mathbf{U}\\in\\mathbb{R}^{h\\times l} are parameters and 𝐱∈ℝl𝐱superscriptℝ𝑙\\mathbf{x}\\in\\mathbb{R}^{l} is the input which is taken from 𝕏={𝐱∈ℝl|‖𝐱‖2<b}𝕏conditional-set𝐱superscriptℝ𝑙subscriptnorm𝐱2𝑏\\mathbb{X}=\\{\\mathbf{x}\\in\\mathbb{R}^{l}|\\|\\mathbf{x}\\|_{2}<b\\} uniformly. Denote the deterministic anchor model as 𝐰anc∗={𝐔anc∗,𝐯anc∗}superscriptsubscript𝐰ancsuperscriptsubscript𝐔ancsuperscriptsubscript𝐯anc\\mathbf{w}_{\\text{anc}}^{*}=\\{\\mathbf{U}_{\\text{anc}}^{*},\\mathbf{v}_{\\text{anc}}^{*}\\}, with ‖𝐯anc∗‖2<dancsubscriptnormsuperscriptsubscript𝐯anc2subscript𝑑anc\\|\\mathbf{v}_{\\text{anc}}^{*}\\|_{2}<d_{\\text{anc}} and consider K𝐾K different networks 𝐰isubscript𝐰𝑖\\mathbf{w}_{i} parameterized with {𝐔i,𝐯i}subscript𝐔𝑖subscript𝐯𝑖\\{\\mathbf{U}_{i},\\mathbf{v}_{i}\\} located on K𝐾K clients respectively. Each element of 𝐔isubscript𝐔𝑖\\mathbf{U}_{i} and 𝐯isubscript𝐯𝑖\\mathbf{v}_{i} is sampled from a uniform distribution centered at 𝐔anc∗superscriptsubscript𝐔anc\\mathbf{U}_{\\text{anc}}^{*} and 𝐯anc∗superscriptsubscript𝐯anc\\mathbf{v}_{\\text{anc}}^{*} with an interval length of d𝑑d. If with probability 1−δ1𝛿1-\\delta, supαℒi​(α​𝐰anc∗+(1−α)​𝐰i)<ϵsubscriptsupremum𝛼subscriptℒ𝑖𝛼superscriptsubscript𝐰anc1𝛼subscript𝐰𝑖italic-ϵ\\sup_{\\alpha}\\mathcal{L}_{i}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{i})<\\epsilon, then with probability 1−δ1𝛿1-\\delta, it has,Landscape visualization. We empirically study whether the transitivity of LMC can be generalized to group connectivity of multiple models.\nWe let 𝐰anc∗superscriptsubscript𝐰anc\\mathbf{w}_{\\text{anc}}^{*} be the anchor model and independently train three models 𝐰1∗,𝐰2∗,𝐰3∗superscriptsubscript𝐰1superscriptsubscript𝐰2superscriptsubscript𝐰3\\mathbf{w}_{1}^{*},\\mathbf{w}_{2}^{*},\\mathbf{w}_{3}^{*} according to Equation 8. Also, training the three models without connectivity loss is conducted for comparison. Then, we visualize the loss landscapes of 𝐰1∗,𝐰2∗,𝐰3∗superscriptsubscript𝐰1superscriptsubscript𝐰2superscriptsubscript𝐰3\\mathbf{w}_{1}^{*},\\mathbf{w}_{2}^{*},\\mathbf{w}_{3}^{*} in Figure 3. For vanilla CE loss, the trained models are scattered in different loss basins with high barriers between them. However, with the connectivity loss, the LMC between each model and the anchor model is improved, and as a result of transitivity, the three models fall into a more connected low-loss region, and the barriers are largely eliminated.Group connectivity when vary K𝐾K. We study the transitivity of group connectivity by scaling up the number of trained models K𝐾K, which is critical for federated learning with numerous clients. The results are in Figure 4; note that the number of anchor models is still one. We observe that by increasing K𝐾K for the connectivity loss, the barrier in group connectivity will go up but still lower than the vanilla training. Also, the increase of barriers may converge to a point lower than vanilla training. It indicates that the transitivity of group connectivity may be weakened for larger K𝐾K but still effective, and when K𝐾K is relatively large (e.g., >8), increasing K𝐾K will cause little loss of group connectivity. Furthermore, we will show in Table 4 that our FedGuCci, which incorporates the connectivity loss, can improve the generalization under different large numbers of clients.In section 3, we have verified the transitivity of group connectivity by using an anchor model. In this section, we will present FedGuCci, incorporating this property in FL to improve generalization.Global models as anchor models. We refer to subsection 2.1 for the settings and notations. In our FedGuCci, we use the global models as the anchor models for connectivity loss with local clients. Instead of solely using the current round global model as the anchor, we find using several previous rounds’ global models can form the clients into a more connected region, so we use N𝑁N previous global models as the anchors. Specifically, in round t∈[T]𝑡delimited-[]𝑇t\\in[T], the set of anchor models 𝐖anc∗tsuperscriptsubscript𝐖superscriptanc𝑡\\mathbf{W}_{\\text{anc}^{*}}^{t} is as follows:where 𝐰gjsuperscriptsubscript𝐰𝑔𝑗\\mathbf{w}_{g}^{j} refers to the global model at round j𝑗j.FedGuCci local updates. FedGuCci is a client-side algorithm that utilizes the global models as the anchor and improves the group connectivity of clients, without additional communication overhead. FedGuCci has the following update rules. In each round t𝑡t, client i∈[M]𝑖delimited-[]𝑀i\\in[M] conducts local training according to the following objective:where 𝐖anc∗,jtsuperscriptsubscript𝐖superscriptanc𝑗𝑡\\mathbf{W}_{\\text{anc}^{*},j}^{t} refers to the j𝑗j-th model in the anchor model set, β𝛽\\beta is the hyperparameter for connectivity loss, ℒisubscriptℒ𝑖\\mathcal{L}_{i} is the client’s local CE loss, and ℒconnectisubscriptℒsubscriptconnect𝑖\\mathcal{L}_{\\text{connect}_{i}} is the connectivity loss regarding subsection 3.1. Clients conduct SGD as Equation 1 to update the local models.By learning to connect with the global anchor models, FedGuCci will improve the group connectivity and achieve better generalization as we will elaborate in section 5.In the study of LMC, different modes are trained on the same dataset but with different random seeds or initializations (Entezari et al., 2022). However, in FL, clients have heterogeneous data, and it is found that data heterogeneity of clients will cause different curvatures of local loss landscapes (Zhou et al., 2023), making the connectivity worse. Therefore, aligning local loss landscapes is essential for better performances of the connectivity loss. In this subsection, we incorporate previous techniques in FedGuCci to align local loss landscapes and propose FedGuCci+.Bias reduction. In FL, class imbalance (a.k.a. label skew) is a main cause of data heterogeneity, and previous works propose logit calibration (Zhang et al., 2022), balanced softmax (Chen & Chao, 2022), and other techniques (Li et al., 2023b; Acar et al., 2020) for reducing the bias caused by class imbalance. Here, we introduce the logit calibration technique used in FedLC (Zhang et al., 2022) for bias reduction. The main idea of logit calibration is to add additional terms to the logits to balance the overall class distributions. From Figure 5 (b), it demonstrates that logit calibration and other bias reduction methods can align the landscapes by making the local objectives more consistent.Flatter minima. Sharpness-aware minimization (Foret et al., 2021; Kwon et al., 2021) (SAM) find flatter minima to improve generalization. SAM has also been introduced in FL for better generalization (Caldarola et al., 2022; Qu et al., 2022). In our paper, we find SAM can be used to align local loss landscapes by making the landscapes flatter, so we also incorporate it in FedGuCci+. From Figure 5 (c), it can be seen that if the landscapes are flatter, the overlap regions between two clients will increase, therefore, it will have more aligned landscapes. Also, for FedGuCci, SAM makes the connectivity loss to learn a cylinder connected with the anchor model instead of a line (Wen et al., 2023), improving connectivity robustness and generalization.FedGuCci+ incorporates logit calibration and SAM into FedGuCci, achieving better generalization. We note that FedGuCci+ is a showcase of how FedGucCci is compatible with other existing techniques for better results, and more techniques can be integrated.In this section, we conduct extensive experiments to validate how FedGuCci and FedGuCci+ improve the generalization of FL under various settings and datasets.Datasets and models. Following previous works (Li et al., 2023b; Lin et al., 2020; Li et al., 2023a), we use 4 vision datasets to conduct experiments: Fashion-MNIST (Xiao et al., 2017), CIFAR-10 (Krizhevsky et al., 2009), CIFAR-100 (Krizhevsky et al., 2009), and Tiny-ImageNet (Le & Yang, 2015). Tiny-ImageNet is a subset of ImageNet (Deng et al., 2009) with 100k samples of 200 classes. We use different models for the datasets as follows: {Fashion-MNIST: VGG11 (Simonyan & Zisserman, 2015), CIFAR-10: SimpleCNN (Li et al., 2023a), CIFAR-100: ResNet20 (Li et al., 2018; He et al., 2016), Tiny-ImageNet: ResNet18 (He et al., 2016)}. We also conduct experiments of pretrained language models on 6 datasets are from GLUE (Wang et al., 2019), and the model is RoBERTa-base (Liu et al., 2019). For the detailed settings, please refer to Appendix A.Compared methods. We take the most relevant and the most state-of-the-art FL algorithms as the baselines. (1) FedAvg (McMahan et al., 2017) with vanilla local training, a simple but strong baseline; (2) FedProx (Li et al., 2020a), which uses the current round’s global model as local regularization term; (3) FedDyn (Acar et al., 2020), FL based on dynamic regularization; (4) SCAFFOLD (Karimireddy et al., 2020), using control variates for variance reduction; (5) MOON (Li et al., 2021) with model-contrastive learning; (6) FedRoD (Chen & Chao, 2022), generalization through decoupling and balanced softmax loss; (7) FedLC (Zhang et al., 2022), FL with logit calibration for bias reduction; (8) FedSAM (Qu et al., 2022; Caldarola et al., 2022), incorporating sharpness-aware minimization into FL.Client Settings. We adopt the Dirichlet sampling to craft IID and heterogeneous data for clients, which is widely used in FL literature (Lin et al., 2020; Chen & Chao, 2022; Li et al., 2023b). It considers a class-imbalanced data heterogeneity, controlled by non-IID hyperparameter, and smaller value refers to more heterogeneous data of clients. We vary the hyperparameter ∈{100,10,1.0,0.5,0.4}absent100101.00.50.4\\in\\{100,10,1.0,0.5,0.4\\} with a spectrum from IID to non-IID (heterogeneous).\nThe hyperparameters are shown in the captions or in Appendix A. Except from Table 4, we use full client participation.Evaluation and implementation. We test the generalization performance, which is validated on the balanced testset after the global model is generated on the server. For all the experiments, we conduct three trials for each setting and present the mean accuracy and the standard deviation in the tables. More implementation details in Appendix A.Results under various datasets and models. In Table 2, our methods can reach state-of-the-art results across four datasets under both IID (α=100𝛼100\\alpha=100) and heterogeneous (α=0.5𝛼0.5\\alpha=0.5) settings222It’s important to mention that certain methods might fail in specific settings, exhibiting accuracy levels close to random guessing, e.g., FedProx in Fashion-MNIST..\nGenerally, FedGuCci can reach the best performances over current FL methods, and FedGuCci+ can strengthen FedGuCci in most cases. Also, the performance gains of our approaches are more dominant under more complicated datasets, like Tiny-ImageNet. While FedSAM stands as the most robust baseline for generalization, our connectivity loss not only yields better results but is also compatible with it (FedGuCci+).Results on different M𝑀M and ρ𝜌\\rho. We conduct experiments by varying the number of clients M𝑀M and participation ratios of clients ρ𝜌\\rho in Table 4. It demonstrates that FedGuCci and FedGuCci+ can also excel when the number of clients is large and partial participation exists, indicating their great potential under cross-device settings (Charles et al., 2021).Results of different local epochs E𝐸E. In Figure 6, FedGuCci is consistently leading under different E𝐸E, while FedGuCci+ is not robust on CIFAR-10. For CIFAR-100, FedGuCci has a more obvious advantage when E𝐸E is large, and this is rationale since the connectivity and model drift issues are more severe under large local updates.In this section, we conduct experiments under pretrain-finetune paradigm for both vision and language tasks.Results under pretrained language models. We use 6 datasets from GLUE (Wang et al., 2019) benchmark for finetuning pretrained language models. For each dataset, we randomly split the data into several clients and conduct finetuning using low-rank adaption (LoRA), and the pretrained model is RoBERTa-base (Liu et al., 2019). It is notable that some language tasks are not classifications, so FedRoD, FedLC, and FedGuCci+, which rely on classification loss, are not applicable. The results are in Table 3, where our FedGuCci reaches promising performances over existing methods. It is observed that some methods that are superior in Table 2 have worse performances in pretrained language models, e.g., FedDyn, while our FedGuCci keeps steady advantages.Results under pretrained vision models. We conduct experiments under pretrained vision models, namely, ResNet18 (He et al., 2016) pretrained on ImageNet (Deng et al., 2009) and Vision Transformer (ViT-B/32) (Dosovitskiy et al., 2021) pretrained on CLIP (Radford et al., 2021). Table 5 presents the finetuning results of FL methods on CIFAR-10 and CIFAR-100. It seems that FedAvg is a strong baseline when it comes to pretrained vision backbones, especially for the ViT. However, it is illustrated that FedGuCci is also improving generalization over FedAvg, whereas some methods may have unsatisfactory results, e.g., FedSAM. As a consequence of SAM’s negative effect, FedGuCci+ has inferior performance.In this subsection, we showcase the applicability of FedGuCci under the pretrain-finetune paradigm, and it reveals FedGuCci’s great potential in collaboratively finetuning foundation models, such as large language models (Radford et al., 2018; Touvron et al., 2023).We conduct sensitivity analyses of FedGuCci(+)’s hyperparameters and their ablation study.Sensitivity analyses of hyperparameters. As illustrated in Figure 7, we vary the FedGuCci(+)’s hyperparameters N𝑁N and β𝛽\\beta of Equation 12 and subsection 4.1. It reveals that FedGuCci and FedGuCci+ have a wide range of effective hyperparameters, outperforming FedAvg. We find FedGuCci+ is more sensitive than FedGuCci, that high N𝑁N and β𝛽\\beta may degrade the performances. For β𝛽\\beta, there may exist an optimization-connectivity tradeoff at the clients. If β𝛽\\beta is too high, the connectivity loss may hurt the local optimization steps, causing generalization declines of local models, further detrimental to the fused global model.Ablation study. Table 6 shows that FedGuCci already has obvious generalization gains over FedAvg; further, SAM and the bias reduction method (logit calibration) can reach higher generalization on FedGuCci. SAM has a more dominant improvement on FedGuCci. We note that FedGuCci is general and flexible and may be compatible with more existing FL algorithms, and FedGuCci+ is just one showcase.In this paper, we study the transitivity of linear mode connectivity (LMC) and use this property to improve the generalization of federated learning (FL). We first empirically and theoretically verify the transitivity of LMC between two models by leveraging a fixed anchor model, and we extend it to group connectivity among multiple models. Then, we propose FedGuCci and FedGuCci+ in FL. Extensive experiments demonstrate our proposed methods can improve the generalization of FL under various settings.This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.AppendixIn this appendix, we provide the details omitted in the main paper and more analyses and discussions.Appendix A: details of experimental setups (cf. section 3 and section 5 of the main paper).Appendix B: detailed proofs of Lemma 3.3, Theorem 3.5, and Theorem 3.8 (cf. section 3 of the main paper).\nAppendix C: more discussions about the related works (cf. section 2 of the main paper).In this section, we present the implementation details omitted from the main paper.CIFAR-10 (Krizhevsky et al., 2009) consists of 60,000 32x32 color images, evenly distributed among 10 different classes, including airplanes, automobiles, birds, cats, etc., each represented by 6,000 images. The dataset is split into 50,000 training images and 10,000 test images.\nFashionMNIST (Xiao et al., 2017) is designed as an advanced replacement for the MNIST dataset, suitable for benchmarking machine learning models. It comprises 70,000 images divided into 60,000 training samples and 10,000 test samples. Each image is a 28x28 grayscale representation of fashion items from 10 different classes, such as shirts, trousers, sneakers, etc.\nThe CIFAR-100 dataset (Krizhevsky et al., 2009) is similar to CIFAR-10 but more challenging, containing 100 different classes grouped into 20 superclasses. It includes 60,000 32x32 color images, with 600 images per class, divided into 50,000 training images and 10,000 test images. This dataset is primarily used for developing and evaluating more sophisticated image classification models.\nTinyImageNet TinyImageNet is a reduced-scale version of the renowned ImageNet dataset, which comprises a total of 200 classes. The dataset is structured into training, validation, and test sets, with 200,000 training images, 20,000 validation images, and 20,000 test images.\nThe GLUE benchmark is a compilation of 9 datasets for evaluating natural language understanding systems. Tasks are framed as either single-sentence classification or sentence-pair classification tasks. GLUE includes MNLI (inference, (Williams et al., 2017)), MRPC (paraphrase detection, (Socher et al., 2013)), MRPC (paraphrase detection, (Dolan & Brockett, 2005)), CoLA (linguistic acceptability, (Warstadt et al., 2019)), QNLI (inference, (Rajpurkar et al., 2018)), QQP (question-answering), RTE (inference), WNLI (inference), and STS-B (textual similarity, (Cer et al., 2017)). Due to high computation costs, we only used SST2, MRPC, CoLA, QNLI, RTE, and STS-B for evaluation. For the replication in Table 3, we report results on the development sets after fine-tuning the pretrained models on the corresponding single-task training data. Our fine-tuning approach is LoRA(Hu et al., 2021).SimpleCNN. The simple CNN for CIFAR-10 is a convolutional neural network model with ReLU activations, consisting of 3 convolutional layers followed by 2 fully connected layers. The first convolutional layer has a size of (3, 32, 3), followed by a max-pooling layer of size (2, 2). The second and third convolutional layers have sizes of (32, 64, 3) and (64, 64, 3), respectively. The last two fully connected layers have sizes of (6444, 64) and (64, num_classes), respectively.ResNets. We followed the model architectures used in (Li et al., 2018). The number in the model names indicates the number of layers in the models, where a larger number indicates a deeper network. We used ResNet18 and ResNet20 for CIFAR-10 and CIFAR-100, respectively. Notably, to mitigate abnormal effects introduced by batch normalization layers (Li et al., 2020b; Lin et al., 2020), following by (Adilova et al., 2023), we removed all batch normalization layers from the ResNets.VGG. VGG (Simonyan & Zisserman, 2015) is a convolutional neural network (CNN) architecture that gained prominence in the field of computer vision. Among its variants, we used VGG11.RoBERTa. RoBERTa is a natural language processing (NLP) model that builds upon the foundation laid by BERT, which was introduced by (Liu et al., 2019) to address some limitations and improve the performance of BERT on various NLP tasks. It comes in various sizes, and we used RoBERTa-base considering to high computational costs.In all experiments, we conducted each experiment three times with different random seeds and reported the averaged results along with standard deviations.We ensured consistency by setting torch, numpy, and random functions with the same random seed, thereby making the data partitions and other settings identical. To ensure all algorithms started with the same initial model, we saved an initial model for each architecture and loaded it at the beginning of each experiment. Additionally, for experiments involving partial participation, the selection of participating clients in each round significantly influenced the model’s performance. To maintain fairness, we saved the sequences of participating clients in each round and loaded these sequences for all experiments. This procedure guaranteed that, given a random seed and participation ratio, every algorithm had the same set of sampled clients in each round.CIFAR-10, CIFAR-100, FashionMNIST and Tiny-ImageNet. We evaluate the global model performance on the test dataset of each dataset. The test dataset is mostly class-balanced and can reflect the global learning objective of a federated learning system. Therefore, the performance of the model on the test set can indicate the generalization performance of global models (Li et al., 2023a; Lin et al., 2020). In each experiment, we take the average test accuracy of the last 5 rounds as the final test accuracy.GLUE. For GLUE, we used the validation dataset for evaluation. Following by Hu et al. (2021), we chose the best accuracy as the final test accuracy.Table 2: For Fashion-MNIST, T𝑇T is 400, batch size is 64 and learning rate is 0.08. For CIFAR-10, T𝑇T is 150, batch size is 64 and learning rate is 0.04. For CIFAR-100, T𝑇T is 200, batch size is 64 and learning rate is 0.03. For Tiny-ImageNet, learning rate is 0.01 and T𝑇T is 50. Optimzier is ADAM for Fashion-MNIST and others are SGD.Table 3: Optimizer is Adam for all datasets. For CoLA and STSB, T𝑇T is 25, batch size is 16 and learning rate is 2e-5. For SST-2, T𝑇T is 50, batch size is 16, and learning rate is 2e-6. For QNLI, T𝑇T is 20, batch size is 32 and learning rate is 2e-6. For RTE and MRPC, T𝑇T is 80, batch size is 16 and learning rate is 2e-5.Table 4: T𝑇T is 150, E is 3, batch size is 64 and learning rate is 0.04.Table 5: ResNet-18 and MobileViT are pretrained on ImageNet. E is 3 for both models. For ViT, T𝑇T is 15, batch size is 16 and learning rate is 0.001. For ResNet, T𝑇T is 50, batch size is 64 and learning rate 1e-4.Table 6: For CIFAR-10, T𝑇T is 150, batch size is 64 and learning rate is 0.04. For CIFAR-100, T is 200, batch size is 64, and learning rate is 0.03.Figure 6: M=60𝑀60M=60 for CIFAR-10, and M=20𝑀20M=20 for CIFAR-100. T𝑇T is 200 for both datasets. Learning rate is 0.03 for CIFAR-10, and 0.04 for CIFAR-100.Figure 7: T𝑇T is 150, E𝐸E is 3, M𝑀M is 60, learning rate is 0.02, and batch size is 64.In this section, we give the proofs of the lemma and theorem in section 3.(Lemma 3.3)\nSet the uniform and bounded domain for network 𝐰𝐰\\mathbf{w} as ℰϵ={𝐰∈Ω|ℒ​(𝐰)<ϵ}subscriptℰitalic-ϵconditional-set𝐰Ωℒ𝐰italic-ϵ\\mathcal{E}_{\\epsilon}=\\{\\mathbf{w}\\in\\Omega|\\mathcal{L}(\\mathbf{w})<\\epsilon\\}. Define a random event Dϵ​(𝐰anc∗)subscript𝐷italic-ϵsuperscriptsubscript𝐰ancD_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}) as Dϵ​(𝐰anc∗)={𝐰∈ℰϵ|∀α∈[0,1],ℒ​(α​𝐰anc∗+(1−α)​𝐰)≤ϵ}subscript𝐷italic-ϵsuperscriptsubscript𝐰ancconditional-set𝐰subscriptℰitalic-ϵformulae-sequencefor-all𝛼01ℒ𝛼superscriptsubscript𝐰anc1𝛼𝐰italic-ϵD_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})=\\{\\mathbf{w}\\in\\mathcal{E}_{\\epsilon}|\\forall\\alpha\\in[0,1],\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w})\\leq\\epsilon\\}. Consider an anchor model 𝐰anc∗superscriptsubscript𝐰anc{\\mathbf{w}_{\\text{anc}}^{*}} and an arbitrary network 𝐰𝐰\\mathbf{w} and for ϵ>0italic-ϵ0\\epsilon>0. Then for ‖𝐰−𝐰anc∗‖∞≤d2subscriptnorm𝐰superscriptsubscript𝐰anc𝑑2\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq\\frac{d}{2},where dϵ=|ℰϵ|1Ssubscript𝑑italic-ϵsuperscriptsubscriptℰitalic-ϵ1𝑆d_{\\epsilon}=\\left|\\mathcal{E}_{\\epsilon}\\right|^{\\frac{1}{S}} represents the average diameter of region ℰϵsubscriptℰitalic-ϵ\\mathcal{E}_{\\epsilon}, S𝑆S represents the number of parameters of the neural network and the equality holds if and only if ℰϵ⊂{𝐰|‖𝐰−𝐰anc∗‖∞≤d}subscriptℰitalic-ϵconditional-set𝐰subscriptnorm𝐰superscriptsubscript𝐰anc𝑑\\mathcal{E}_{\\epsilon}\\subset\\{\\mathbf{w}|\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq d\\} is a star domain centered at 𝐰anc∗superscriptsubscript𝐰anc{\\mathbf{w}_{\\text{anc}}^{*}}. Thus, when P(Dϵ(𝐰anc∗)))>1−δP(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})))>1-\\delta, it holds d<dϵ(1−δ)1S𝑑subscript𝑑italic-ϵsuperscript1𝛿1𝑆d<{\\frac{d_{\\epsilon}}{(1-\\delta)^{\\frac{1}{S}}}}.In the following proof, we denote the region as 𝒱d={𝐰|‖𝐰−𝐰anc∗‖∞≤d2}subscript𝒱𝑑conditional-set𝐰subscriptnorm𝐰superscriptsubscript𝐰anc𝑑2\\mathcal{V}_{d}=\\{\\mathbf{w}|\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq\\frac{d}{2}\\} with volume |𝒱d|=dSsubscript𝒱𝑑superscript𝑑𝑆|\\mathcal{V}_{d}|=d^{S} and denote the segment between 𝐰𝐰\\mathbf{w} and 𝐰anc∗superscriptsubscript𝐰anc{\\mathbf{w}_{\\text{anc}}^{*}} as l​(𝐰anc∗,𝐰)={α​𝐰anc∗+(1−α)​𝐰,α∈[0,1]}𝑙superscriptsubscript𝐰anc𝐰𝛼superscriptsubscript𝐰anc1𝛼𝐰𝛼01l({\\mathbf{w}_{\\text{anc}}^{*}},\\mathbf{w})=\\{\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w},\\alpha\\in[0,1]\\}.First we prove if ℰϵ⊂𝒱dsubscriptℰitalic-ϵsubscript𝒱𝑑\\mathcal{E}_{\\epsilon}\\subset\\mathcal{V}_{d} is a star domain centered at 𝐰anc∗superscriptsubscript𝐰anc{\\mathbf{w}_{\\text{anc}}^{*}}, P​(Dϵ​(𝐰anc∗))=|ℰϵ|dS𝑃subscript𝐷italic-ϵsuperscriptsubscript𝐰ancsubscriptℰitalic-ϵsuperscript𝑑𝑆P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))=\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{d^{S}}. Select a parameter point 𝐰0subscript𝐰0\\mathbf{w}_{0} in 𝒱dsubscript𝒱𝑑\\mathcal{V}_{d} arbitrarily. If 𝐰0∈ℰϵsubscript𝐰0subscriptℰitalic-ϵ\\mathbf{w}_{0}\\in\\mathcal{E}_{\\epsilon}, then because ℰϵsubscriptℰitalic-ϵ\\mathcal{E}_{\\epsilon} is a star domain centered at 𝐰anc∗superscriptsubscript𝐰anc{\\mathbf{w}_{\\text{anc}}^{*}}, l​(𝐰anc∗,𝐰)⊂ℰϵ𝑙superscriptsubscript𝐰anc𝐰subscriptℰitalic-ϵl({\\mathbf{w}_{\\text{anc}}^{*}},\\mathbf{w})\\subset\\mathcal{E}_{\\epsilon} and thus 𝐰0∈Dϵ​(𝐰anc∗)subscript𝐰0subscript𝐷italic-ϵsuperscriptsubscript𝐰anc\\mathbf{w}_{0}\\in D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}). If 𝐰0∉ℰϵsubscript𝐰0subscriptℰitalic-ϵ\\mathbf{w}_{0}\\notin\\mathcal{E}_{\\epsilon}, then 𝐰0∉Dϵ​(𝐰anc∗)subscript𝐰0subscript𝐷italic-ϵsuperscriptsubscript𝐰anc\\mathbf{w}_{0}\\notin D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}) by the definition of Dϵ​(𝐰anc∗)subscript𝐷italic-ϵsuperscriptsubscript𝐰ancD_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}). Therefore, ℰϵ=Dϵ​(𝐰anc∗)subscriptℰitalic-ϵsubscript𝐷italic-ϵsuperscriptsubscript𝐰anc\\mathcal{E}_{\\epsilon}=D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}) and we have P​(Dϵ​(𝐰anc∗))=P​(ℰϵ)=|ℰϵ||𝒱d|=|ℰϵ|dS𝑃subscript𝐷italic-ϵsuperscriptsubscript𝐰anc𝑃subscriptℰitalic-ϵsubscriptℰitalic-ϵsubscript𝒱𝑑subscriptℰitalic-ϵsuperscript𝑑𝑆P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))=P(\\mathcal{E}_{\\epsilon})=\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{\\left|\\mathcal{V}_{d}\\right|}=\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{d^{S}}.The next step we prove that if ℰϵ⊄𝒱dnot-subset-ofsubscriptℰitalic-ϵsubscript𝒱𝑑\\mathcal{E}_{\\epsilon}\\not\\subset\\mathcal{V}_{d}, or ℰϵsubscriptℰitalic-ϵ\\mathcal{E}_{\\epsilon} is not a star domain centered at 𝐰anc∗superscriptsubscript𝐰anc{\\mathbf{w}_{\\text{anc}}^{*}}, then P​(Dϵ​(𝐰anc∗))<|ℰϵ|dS𝑃subscript𝐷italic-ϵsuperscriptsubscript𝐰ancsubscriptℰitalic-ϵsuperscript𝑑𝑆P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))<\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{d^{S}}.If ℰϵ⊄𝒱dnot-subset-ofsubscriptℰitalic-ϵsubscript𝒱𝑑\\mathcal{E}_{\\epsilon}\\not\\subset\\mathcal{V}_{d}, then |Dϵ​(𝐰anc∗)|≤|ℰϵ∩𝒱d|<|ℰϵ|subscript𝐷italic-ϵsuperscriptsubscript𝐰ancsubscriptℰitalic-ϵsubscript𝒱𝑑subscriptℰitalic-ϵ\\left|D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\right|\\leq\\left|\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d}\\right|<\\left|\\mathcal{E}_{\\epsilon}\\right| and P​(Dϵ​(𝐰anc∗))=|Dϵ​(𝐰anc∗)||𝒱d|<|ℰϵ||𝒱d|𝑃subscript𝐷italic-ϵsuperscriptsubscript𝐰ancsubscript𝐷italic-ϵsuperscriptsubscript𝐰ancsubscript𝒱𝑑subscriptℰitalic-ϵsubscript𝒱𝑑P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))=\\frac{\\left|D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\right|}{\\left|\\mathcal{V}_{d}\\right|}<\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{\\left|\\mathcal{V}_{d}\\right|}. Here, the first inequality |Dϵ​(𝐰anc∗)|≤|ℰϵ∩𝒱d|subscript𝐷italic-ϵsuperscriptsubscript𝐰ancsubscriptℰitalic-ϵsubscript𝒱𝑑\\left|D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\right|\\leq\\left|\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d}\\right| holds, because Dϵ​(𝐰anc∗)⊂ℰϵ∩𝒱dsubscript𝐷italic-ϵsuperscriptsubscript𝐰ancsubscriptℰitalic-ϵsubscript𝒱𝑑D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\subset\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d} and the second inequality |ℰϵ∩𝒱d|<|ℰϵ|subscriptℰitalic-ϵsubscript𝒱𝑑subscriptℰitalic-ϵ\\left|\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d}\\right|<|\\mathcal{E}_{\\epsilon}| holds, because ∃𝐰0∈ℰϵ/𝒱d,ϵ0>0formulae-sequencesubscript𝐰0subscriptℰitalic-ϵsubscript𝒱𝑑subscriptitalic-ϵ00\\exists\\mathbf{w}_{0}\\in\\mathcal{E}_{\\epsilon}/\\mathcal{V}_{d},\\epsilon_{0}>0 st. {𝐰|‖𝐰−𝐰0‖<ϵ0}⊂Ω/𝒱d∩ℰϵconditional-set𝐰norm𝐰subscript𝐰0subscriptitalic-ϵ0Ωsubscript𝒱𝑑subscriptℰitalic-ϵ\\{\\mathbf{w}|\\|\\mathbf{w}-\\mathbf{w}_{0}\\|<\\epsilon_{0}\\}\\subset\\Omega/\\mathcal{V}_{d}\\cap\\mathcal{E}_{\\epsilon} for Ω/𝒱dΩsubscript𝒱𝑑\\Omega/\\mathcal{V}_{d} and ℰϵsubscriptℰitalic-ϵ\\mathcal{E}_{\\epsilon} are open sets and |ℰϵ∩𝒱d|≤|ℰϵ|−|{𝐰|‖𝐰−𝐰0‖<ϵ0}|<|ℰϵ|subscriptℰitalic-ϵsubscript𝒱𝑑subscriptℰitalic-ϵconditional-set𝐰norm𝐰subscript𝐰0subscriptitalic-ϵ0subscriptℰitalic-ϵ\\left|\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d}\\right|\\leq|\\mathcal{E}_{\\epsilon}|-\\left|\\{\\mathbf{w}|\\|\\mathbf{w}-\\mathbf{w}_{0}\\|<\\epsilon_{0}\\}\\right|<\\left|\\mathcal{E}_{\\epsilon}\\right|.If ℰϵsubscriptℰitalic-ϵ\\mathcal{E}_{\\epsilon} is not a star domain centered at 𝐰anc∗superscriptsubscript𝐰anc{\\mathbf{w}_{\\text{anc}}^{*}}, then there exists 𝐰0∈ℰϵsubscript𝐰0subscriptℰitalic-ϵ\\mathbf{w}_{0}\\in\\mathcal{E}_{\\epsilon} such that l​(𝐰anc∗,𝐰0)⊄ℰϵnot-subset-of𝑙superscriptsubscript𝐰ancsubscript𝐰0subscriptℰitalic-ϵl({\\mathbf{w}_{\\text{anc}}^{*}},\\mathbf{w}_{0})\\not\\subset\\mathcal{E}_{\\epsilon}. Then ∃α1∈(0,1)subscript𝛼101\\exists\\alpha_{1}\\in(0,1) st. 𝐰1​=Δ​α1​𝐰anc∗+(1−α1)​𝐰0subscript𝐰1Δsubscript𝛼1superscriptsubscript𝐰anc1subscript𝛼1subscript𝐰0\\mathbf{w}_{1}\\overset{\\Delta}{=}\\alpha_{1}{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha_{1})\\mathbf{w}_{0} satisfies ℒ​(𝐰1)>ϵℒsubscript𝐰1italic-ϵ\\mathcal{L}(\\mathbf{w}_{1})>\\epsilon. For ℒ​(⋅)ℒ⋅\\mathcal{L}(\\cdot) is smooth, there exists ϵ1>0subscriptitalic-ϵ10\\epsilon_{1}>0 st. ∀𝐰∈Uϵ1​(𝐰1)​=Δ​{𝐰|‖𝐰1−𝐰‖2<ϵ1}for-all𝐰subscript𝑈subscriptitalic-ϵ1subscript𝐰1Δconditional-set𝐰subscriptnormsubscript𝐰1𝐰2subscriptitalic-ϵ1\\forall\\mathbf{w}\\in U_{\\epsilon_{1}}(\\mathbf{w}_{1})\\overset{\\Delta}{=}\\{\\mathbf{w}|\\|\\mathbf{w}_{1}-\\mathbf{w}\\|_{2}<\\epsilon_{1}\\}, ℒ​(𝐰)≥ϵ+ℒ​(𝐰1)−ϵ2>ϵℒ𝐰italic-ϵℒsubscript𝐰1italic-ϵ2italic-ϵ\\mathcal{L}(\\mathbf{w})\\geq\\epsilon+\\frac{\\mathcal{L}(\\mathbf{w}_{1})-\\epsilon}{2}>\\epsilon. Then for ℰϵsubscriptℰitalic-ϵ\\mathcal{E}_{\\epsilon} is an open set, choose ϵ2<ϵ1subscriptitalic-ϵ2subscriptitalic-ϵ1\\epsilon_{2}<\\epsilon_{1} st. Uϵ2​(𝐰0)⊂ℰϵsubscript𝑈subscriptitalic-ϵ2subscript𝐰0subscriptℰitalic-ϵU_{\\epsilon_{2}}(\\mathbf{w}_{0})\\subset\\mathcal{E}_{\\epsilon}. ∀𝐰2∈Uϵ2​(𝐰0)for-allsubscript𝐰2subscript𝑈subscriptitalic-ϵ2subscript𝐰0\\forall\\mathbf{w}_{2}\\in U_{\\epsilon_{2}}(\\mathbf{w}_{0}), 𝐰3=α1​𝐰anc∗+(1−α1)​𝐰2subscript𝐰3subscript𝛼1superscriptsubscript𝐰anc1subscript𝛼1subscript𝐰2\\mathbf{w}_{3}=\\alpha_{1}{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha_{1})\\mathbf{w}_{2} satisfies ‖𝐰3−𝐰1‖2=(1−α1)​‖𝐰0−𝐰2‖2<(1−α1)​ϵ2<ϵ1subscriptnormsubscript𝐰3subscript𝐰121subscript𝛼1subscriptnormsubscript𝐰0subscript𝐰221subscript𝛼1subscriptitalic-ϵ2subscriptitalic-ϵ1\\|\\mathbf{w}_{3}-\\mathbf{w}_{1}\\|_{2}=(1-\\alpha_{1})\\|\\mathbf{w}_{0}-\\mathbf{w}_{2}\\|_{2}<(1-\\alpha_{1})\\epsilon_{2}<\\epsilon_{1}. Thus 𝐰3∈Uϵ1​(𝐰1)subscript𝐰3subscript𝑈subscriptitalic-ϵ1subscript𝐰1\\mathbf{w}_{3}\\in U_{\\epsilon_{1}}(\\mathbf{w}_{1}), which leads to ℒ​(𝐰3)>ϵℒsubscript𝐰3italic-ϵ\\mathcal{L}(\\mathbf{w}_{3})>\\epsilon. Therefore, Uϵ2​(𝐰0)∩Dϵ​(𝐰anc∗)=∅subscript𝑈subscriptitalic-ϵ2subscript𝐰0subscript𝐷italic-ϵsuperscriptsubscript𝐰ancU_{\\epsilon_{2}}(\\mathbf{w}_{0})\\cap D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})=\\emptyset and P​(Dϵ​(𝐰anc∗))=|Dϵ​(𝐰anc∗)|dS≤|ℰϵ|−|Uϵ2​(𝐰0)|dS<|ℰϵ|dS𝑃subscript𝐷italic-ϵsuperscriptsubscript𝐰ancsubscript𝐷italic-ϵsuperscriptsubscript𝐰ancsuperscript𝑑𝑆subscriptℰitalic-ϵsubscript𝑈subscriptitalic-ϵ2subscript𝐰0superscript𝑑𝑆subscriptℰitalic-ϵsuperscript𝑑𝑆P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))=\\frac{\\left|D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\right|}{d^{S}}\\leq\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|-\\left|U_{\\epsilon_{2}}(\\mathbf{w}_{0})\\right|}{d^{S}}<\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{d^{S}}.∎(Theorem 3.5)\nWe define a two-layer neural network with ReLU activation, and the function is f𝐯,𝐔​(𝐱)=𝐯⊤​σ​(𝐔𝐱)subscript𝑓𝐯𝐔𝐱superscript𝐯top𝜎𝐔𝐱f_{\\mathbf{v},\\mathbf{U}}(\\mathbf{x})=\\mathbf{v}^{\\top}\\sigma(\\mathbf{U}\\mathbf{x}) where σ​(⋅)𝜎⋅\\sigma(\\cdot) is the ReLU activation function. 𝐯∈ℝh𝐯superscriptℝℎ\\mathbf{v}\\in\\mathbb{R}^{h} and 𝐔∈ℝh×l𝐔superscriptℝℎ𝑙\\mathbf{U}\\in\\mathbb{R}^{h\\times l} are parameters333For simplicity and without loss of generality, we omit the bias terms. and 𝐱∈ℝl𝐱superscriptℝ𝑙\\mathbf{x}\\in\\mathbb{R}^{l} is the input which is taken from 𝕏={𝐱∈ℝl|‖𝐱‖2<b}𝕏conditional-set𝐱superscriptℝ𝑙subscriptnorm𝐱2𝑏\\mathbb{X}=\\{\\mathbf{x}\\in\\mathbb{R}^{l}|\\|\\mathbf{x}\\|_{2}<b\\} uniformly. Denote the deterministic anchor model as 𝐰anc∗={𝐔anc∗,𝐯anc∗}superscriptsubscript𝐰ancsuperscriptsubscript𝐔ancsuperscriptsubscript𝐯anc{\\mathbf{w}_{\\text{anc}}^{*}}=\\{\\mathbf{U}_{\\text{anc}}^{*},\\mathbf{v}_{\\text{anc}}^{*}\\}, with ‖𝐯anc∗‖2<dancsubscriptnormsuperscriptsubscript𝐯anc2subscript𝑑anc\\|\\mathbf{v}_{\\text{anc}}^{*}\\|_{2}<d_{\\text{anc}} and consider two different networks 𝐰1,𝐰2subscript𝐰1subscript𝐰2\\mathbf{w}_{1},\\mathbf{w}_{2} parameterized with {𝐔1,𝐯1}subscript𝐔1subscript𝐯1\\{\\mathbf{U}_{1},\\mathbf{v}_{1}\\} and {𝐔2,𝐯2}subscript𝐔2subscript𝐯2\\{\\mathbf{U}_{2},\\mathbf{v}_{2}\\} respectively. Each element of 𝐔1subscript𝐔1\\mathbf{U}_{1} and 𝐔2subscript𝐔2\\mathbf{U}_{2}, 𝐯1subscript𝐯1\\mathbf{v}_{1} and 𝐯2subscript𝐯2\\mathbf{v}_{2} is sampled from a uniform distribution centered at 𝐔anc∗superscriptsubscript𝐔anc\\mathbf{U}_{\\text{anc}}^{*} and 𝐯ancsubscript𝐯anc\\mathbf{v}_{\\text{anc}} with an interval length of d𝑑d. If with probability 1−δ1𝛿1-\\delta, supαℒ​(α​𝐰anc∗+(1−α)​𝐰1)<ϵsubscriptsupremum𝛼ℒ𝛼superscriptsubscript𝐰anc1𝛼subscript𝐰1italic-ϵ\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{1})<\\epsilon and supαℒ​(α​𝐰anc∗+(1−α)​𝐰2)<ϵsubscriptsupremum𝛼ℒ𝛼superscriptsubscript𝐰anc1𝛼subscript𝐰2italic-ϵ\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{2})<\\epsilon, then with probability 1−δ1𝛿1-\\delta, it has,where Bl​o​s​s​(𝐰1,𝐰2)subscript𝐵𝑙𝑜𝑠𝑠subscript𝐰1subscript𝐰2B_{loss}(\\mathbf{w}_{1},\\mathbf{w}_{2}) is the loss barrier as Equation 3.Let’s first define gα​(𝐱)=(α​𝐔1+(1−α)​𝐔2)​𝐱subscript𝑔𝛼𝐱𝛼subscript𝐔11𝛼subscript𝐔2𝐱g_{\\alpha}(\\mathbf{x})=(\\alpha\\mathbf{U}_{1}+(1-\\alpha)\\mathbf{U}_{2})\\mathbf{x} and z𝐱​(α)=(α​𝐯1+(1−α)​𝐯2)⊤​σ​((α​𝐔1+(1−α)​𝐔2)​𝐱)−α​𝐯1⊤​σ​(𝐔1​𝐱)−(1−α)​𝐯2⊤​σ​(𝐔2​𝐱)subscript𝑧𝐱𝛼superscript𝛼subscript𝐯11𝛼subscript𝐯2top𝜎𝛼subscript𝐔11𝛼subscript𝐔2𝐱𝛼superscriptsubscript𝐯1top𝜎subscript𝐔1𝐱1𝛼superscriptsubscript𝐯2top𝜎subscript𝐔2𝐱z_{\\mathbf{x}}(\\alpha)=(\\alpha\\mathbf{v}_{1}+(1-\\alpha)\\mathbf{v}_{2})^{\\top}\\sigma((\\alpha\\mathbf{U}_{1}+(1-\\alpha)\\mathbf{U}_{2})\\mathbf{x})-\\alpha\\mathbf{v}_{1}^{\\top}\\sigma(\\mathbf{U}_{1}\\mathbf{x})-(1-\\alpha){\\mathbf{v}_{2}}^{\\top}\\sigma(\\mathbf{U}_{2}\\mathbf{x}), α∈[0,1]𝛼01\\alpha\\in[0,1]. Then we can express z𝐱​(α)subscript𝑧𝐱𝛼z_{\\mathbf{x}}(\\alpha) as:For each element of 𝐔1subscript𝐔1\\mathbf{U}_{1} and 𝐔2subscript𝐔2\\mathbf{U}_{2}, 𝐯1subscript𝐯1\\mathbf{v}_{1} and 𝐯2subscript𝐯2\\mathbf{v}_{2} is sampled from a uniform distribution centered at 𝐔anc∗superscriptsubscript𝐔anc\\mathbf{U}_{\\text{anc}}^{*} and 𝐯anc∗superscriptsubscript𝐯anc\\mathbf{v}_{\\text{anc}}^{*} with an interval length of d𝑑d, 𝐔1subscript𝐔1\\mathbf{U}_{1}, 𝐔2subscript𝐔2\\mathbf{U}_{2}, 𝐯1subscript𝐯1\\mathbf{v}_{1} and 𝐯2subscript𝐯2\\mathbf{v}_{2} can be represented as 𝐔1=𝐔anc∗+𝐔~1subscript𝐔1superscriptsubscript𝐔ancsubscript~𝐔1\\mathbf{U}_{1}=\\mathbf{U}_{\\text{anc}}^{*}+\\tilde{\\mathbf{U}}_{1}, 𝐔2=𝐔anc∗+𝐔~2subscript𝐔2superscriptsubscript𝐔ancsubscript~𝐔2\\mathbf{U}_{2}=\\mathbf{U}_{\\text{anc}}^{*}+\\tilde{\\mathbf{U}}_{2}, 𝐯1=𝐯anc∗+𝐯~1subscript𝐯1superscriptsubscript𝐯ancsubscript~𝐯1\\mathbf{v}_{1}=\\mathbf{v}_{\\text{anc}}^{*}+\\tilde{\\mathbf{v}}_{1} and 𝐯2=𝐯anc∗+𝐯~2subscript𝐯2superscriptsubscript𝐯ancsubscript~𝐯2\\mathbf{v}_{2}=\\mathbf{v}_{\\text{anc}}^{*}+\\tilde{\\mathbf{v}}_{2} respectively, where each element of 𝐔~1subscript~𝐔1\\tilde{\\mathbf{U}}_{1}, 𝐔~2subscript~𝐔2\\tilde{\\mathbf{U}}_{2}, 𝐯~1subscript~𝐯1\\tilde{\\mathbf{v}}_{1} and 𝐯~2subscript~𝐯2\\tilde{\\mathbf{v}}_{2} follows distribution U​[−d2,d2]𝑈𝑑2𝑑2U[-\\frac{d}{2},\\frac{d}{2}]. Using 𝐯~1subscript~𝐯1\\tilde{\\mathbf{v}}_{1} and 𝐯~2subscript~𝐯2\\tilde{\\mathbf{v}}_{2}, z𝐱​(α)subscript𝑧𝐱𝛼z_{\\mathbf{x}}(\\alpha) can be represented asWe also assume that the number of hidden neurons hℎh is sufficiently large for the convenience of analysis as Entezari et al. (2022). In the following proof, we will make use of Hoeffding’s inequality for sub-Gaussian distributions (especially, uniform distribution). Here, we state it for reference: Let X1,…,Xnsubscript𝑋1…subscript𝑋𝑛X_{1},\\ldots,X_{n} be n𝑛n independent random variables such that Xi∼U​(−d2,−d2)similar-tosubscript𝑋𝑖𝑈𝑑2𝑑2X_{i}\\sim U(-\\frac{d}{2},-\\frac{d}{2}). Then for any 𝐚=(a1,…,an)∈ℝn𝐚subscript𝑎1…subscript𝑎𝑛superscriptℝ𝑛\\mathbf{a}=(a_{1},...,a_{n})\\in\\mathbb{R}^{n}, we have\n\nP[|∑_i=1^n a_i X_i| >t] ≤2exp(-2t2d2∥a ∥22).\n\nTo bound z𝐱​(α)subscript𝑧𝐱𝛼z_{\\mathbf{x}}(\\alpha), we haveThen we bound the first term and the third term, and the second term and the fourth term are bounded similarly due to symmetry. For the concentration upper bound of the first term of Equation 18, we use the Hoeffding’s inequality for elements of 𝐯~1subscript~𝐯1\\tilde{\\mathbf{v}}_{1}, with probability 1−δk1𝛿𝑘1-\\frac{\\delta}{k}Equation 20 is due to the fact that the ReLU activation function satisfies the Lipschitz continuous condition with constant 111. For the bound of the third term of Equation 18, we haveEquation 23 is due to the fact that the ReLU activation function satisfies the Lipschitz continuous condition with constant 111. For the term ‖(𝐔2−𝐔1)​𝐱‖2subscriptnormsubscript𝐔2subscript𝐔1𝐱2\\|(\\mathbf{U}_{2}-\\mathbf{U}_{1})\\mathbf{x}\\|_{2} in Equation 21 and Equation 24, taking a union bound, with probability 1−δk1𝛿𝑘1-\\frac{\\delta}{k}, we haveThen take a union bound choosing k=6𝑘6k=6 (because the union bound is taken for 666 equations, Equation 21 and Equation 28 for the first and the second terms in Equation 18 respectively, and Equation 28 for the third and the fourth terms in Equation 18 respectively.), with probability 1−δ1𝛿1-\\delta we haveFor supαℒ​(α​𝐰anc∗+(1−α)​𝐰)<ϵsubscriptsupremum𝛼ℒ𝛼superscriptsubscript𝐰anc1𝛼𝐰italic-ϵ\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w})<\\epsilon holds with probability 1−δ1𝛿1-\\delta, by Lemma 3.3, we have d<dϵ(1−δ)1S𝑑subscript𝑑italic-ϵsuperscript1𝛿1𝑆d<\\frac{d_{\\epsilon}}{(1-\\delta)^{\\frac{1}{S}}} with S=h​l+h𝑆ℎ𝑙ℎS=hl+h. Then |z𝐱​(α)|subscript𝑧𝐱𝛼|z_{\\mathbf{x}}(\\alpha)| can be bounded asNow we turn to calculate the bound of the loss barrier Bl​o​s​s​(𝐰1,𝐰2)subscript𝐵𝑙𝑜𝑠𝑠subscript𝐰1subscript𝐰2B_{loss}(\\mathbf{w}_{1},\\mathbf{w}_{2}). For the loss function L​(⋅,y)𝐿⋅𝑦L(\\cdot,y) is convex and 1-Lipschitz, we have:where the expectation is with respect to the dataset. Equation 36 is due to the convexity of L​(⋅,y)𝐿⋅𝑦L(\\cdot,y), while Equation 37 is due to the assumption that L​(⋅,y)𝐿⋅𝑦L(\\cdot,y) is 1-Lipschitz. Then use the bound of zx​(α)subscript𝑧x𝛼z_{\\textbf{x}}(\\alpha), with probability 1−δ1𝛿1-\\delta, we have∎(Theorem 3.8)\nWe define a two-layer neural network with ReLU activation, and the function is f𝐯,𝐔​(𝐱)=𝐯⊤​σ​(𝐔𝐱)subscript𝑓𝐯𝐔𝐱superscript𝐯top𝜎𝐔𝐱f_{\\mathbf{v},\\mathbf{U}}(\\mathbf{x})=\\mathbf{v}^{\\top}\\sigma(\\mathbf{U}\\mathbf{x}) where σ​(⋅)𝜎⋅\\sigma(\\cdot) is the ReLU activation function. 𝐯∈ℝh𝐯superscriptℝℎ\\mathbf{v}\\in\\mathbb{R}^{h} and 𝐔∈ℝh×l𝐔superscriptℝℎ𝑙\\mathbf{U}\\in\\mathbb{R}^{h\\times l} are parameters and 𝐱∈ℝl𝐱superscriptℝ𝑙\\mathbf{x}\\in\\mathbb{R}^{l} is the input which is taken from 𝕏={𝐱∈ℝl|‖𝐱‖2<b}𝕏conditional-set𝐱superscriptℝ𝑙subscriptnorm𝐱2𝑏\\mathbb{X}=\\{\\mathbf{x}\\in\\mathbb{R}^{l}|\\|\\mathbf{x}\\|_{2}<b\\} uniformly. Denote the deterministic anchor model as 𝐰anc∗={𝐔anc∗,𝐯anc∗}superscriptsubscript𝐰ancsuperscriptsubscript𝐔ancsuperscriptsubscript𝐯anc\\mathbf{w}_{\\text{anc}}^{*}=\\{\\mathbf{U}_{\\text{anc}}^{*},\\mathbf{v}_{\\text{anc}}^{*}\\}, with ‖𝐯anc∗‖2<dancsubscriptnormsuperscriptsubscript𝐯anc2subscript𝑑anc\\|\\mathbf{v}_{\\text{anc}}^{*}\\|_{2}<d_{\\text{anc}} and consider K𝐾K different networks 𝐰isubscript𝐰𝑖\\mathbf{w}_{i} parameterized with {𝐔i,𝐯i}subscript𝐔𝑖subscript𝐯𝑖\\{\\mathbf{U}_{i},\\mathbf{v}_{i}\\} located on K𝐾K clients respectively. Each element of 𝐔isubscript𝐔𝑖\\mathbf{U}_{i} and 𝐯isubscript𝐯𝑖\\mathbf{v}_{i} is sampled from a uniform distribution centered at 𝐔anc∗superscriptsubscript𝐔anc\\mathbf{U}_{\\text{anc}}^{*} and 𝐯anc∗superscriptsubscript𝐯anc\\mathbf{v}_{\\text{anc}}^{*} with an interval length of d𝑑d. If with probability 1−δ1𝛿1-\\delta, supαℒi​(α​𝐰anc∗+(1−α)​𝐰i)<ϵsubscriptsupremum𝛼subscriptℒ𝑖𝛼superscriptsubscript𝐰anc1𝛼subscript𝐰𝑖italic-ϵ\\sup_{\\alpha}\\mathcal{L}_{i}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{i})<\\epsilon, then with probability 1−δ1𝛿1-\\delta, it has,Similar to Theorem 3.5, we first define g​(𝐱)=(1K​∑i=1K𝐔i)​𝐱𝑔𝐱1𝐾superscriptsubscript𝑖1𝐾subscript𝐔𝑖𝐱g(\\mathbf{x})=(\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{U}_{i})\\mathbf{x} and z​(𝐱)=(1K​∑i=1K𝐯i)⊤​σ​((1K​∑i=1K𝐔i)​𝐱)−1K​∑i=1K𝐯i​σ​(𝐔i​𝐱)𝑧𝐱superscript1𝐾superscriptsubscript𝑖1𝐾subscript𝐯𝑖top𝜎1𝐾superscriptsubscript𝑖1𝐾subscript𝐔𝑖𝐱1𝐾superscriptsubscript𝑖1𝐾subscript𝐯𝑖𝜎subscript𝐔𝑖𝐱z(\\mathbf{x})=(\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{v}_{i})^{\\top}\\sigma((\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{U}_{i})\\mathbf{x})-\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{v}_{i}\\sigma(\\mathbf{U}_{i}\\mathbf{x}). Then we can express z​(𝐱)𝑧𝐱z(\\mathbf{x}) as:For each element of 𝐔isubscript𝐔𝑖\\mathbf{U}_{i} and 𝐯isubscript𝐯𝑖\\mathbf{v}_{i} is sampled from a uniform distribution centered at 𝐔anc∗superscriptsubscript𝐔anc\\mathbf{U}_{\\text{anc}}^{*} and 𝐯anc∗superscriptsubscript𝐯anc\\mathbf{v}_{\\text{anc}}^{*} with an interval length of d𝑑d, 𝐔isubscript𝐔𝑖\\mathbf{U}_{i} and 𝐯isubscript𝐯𝑖\\mathbf{v}_{i} can be represented as 𝐔i=𝐔anc∗+𝐔~isubscript𝐔𝑖superscriptsubscript𝐔ancsubscript~𝐔𝑖\\mathbf{U}_{i}=\\mathbf{U}_{\\text{anc}}^{*}+\\tilde{\\mathbf{U}}_{i} and 𝐯i=𝐯anc∗+𝐯~isubscript𝐯𝑖superscriptsubscript𝐯ancsubscript~𝐯𝑖\\mathbf{v}_{i}=\\mathbf{v}_{\\text{anc}}^{*}+\\tilde{\\mathbf{v}}_{i} respectively, where each element of 𝐔~isubscript~𝐔𝑖\\tilde{\\mathbf{U}}_{i} and 𝐯~isubscript~𝐯𝑖\\tilde{\\mathbf{v}}_{i} follows distribution U​[−d2,d2]𝑈𝑑2𝑑2U[-\\frac{d}{2},\\frac{d}{2}]. Using 𝐯~isubscript~𝐯𝑖\\tilde{\\mathbf{v}}_{i}, z𝐱​(α)subscript𝑧𝐱𝛼z_{\\mathbf{x}}(\\alpha) can be represented asSimilar to Equation 18 and Equation 21, with probability 1−δ21𝛿21-\\frac{\\delta}{2}, Equation 43 can be bound withThen similar to Equation 28, with probability 1−δ21𝛿21-\\frac{\\delta}{2}, Equation 47 can be bound withSet the minimum of ℒisubscriptℒ𝑖\\mathcal{L}_{i} closest to 𝐰anc∗superscriptsubscript𝐰anc\\mathbf{w}_{\\text{anc}}^{*} is 𝐰anc,i∗superscriptsubscript𝐰anc𝑖\\mathbf{w}_{\\text{anc},i}^{*}. For supαℒi​(α​𝐰i+(1−α)​𝐰anc∗)<ϵsubscriptsupremum𝛼subscriptℒ𝑖𝛼subscript𝐰𝑖1𝛼superscriptsubscript𝐰ancitalic-ϵ\\sup_{\\alpha}\\mathcal{L}_{i}(\\alpha\\mathbf{w}_{i}+(1-\\alpha)\\mathbf{w}_{\\text{anc}}^{*})<\\epsilon holds with probability 1−δ1𝛿1-\\delta, then with probability 1−δ1𝛿1-\\delta we have,Equation 52 is due to the assumption that ℒ​(⋅)ℒ⋅\\mathcal{L}(\\cdot) is γ𝛾\\gamma-smooth. By Lemma 3.3, we have d<dϵ+γ​Γ2(1−δ)1S𝑑subscript𝑑italic-ϵ𝛾superscriptΓ2superscript1𝛿1𝑆d<\\frac{d_{\\epsilon+\\gamma\\Gamma^{2}}}{(1-\\delta)^{\\frac{1}{S}}} with S=h​l+h𝑆ℎ𝑙ℎS=hl+h. Then |z𝐱​(α)|subscript𝑧𝐱𝛼|z_{\\mathbf{x}}(\\alpha)| can be bounded asNow we turn to calculate the bound of the loss barrier Bl​o​s​s​({𝐰i}i=1K)subscript𝐵𝑙𝑜𝑠𝑠superscriptsubscriptsubscript𝐰𝑖𝑖1𝐾B_{loss}(\\{\\mathbf{w}_{i}\\}_{i=1}^{K}). For the loss function L​(⋅,y)𝐿⋅𝑦L(\\cdot,y) is convex and 1-Lipschitz, similar to Equation 37, we have:where the expectation is with respect to the server dataset. Then use the bound of z​(α)𝑧𝛼z(\\alpha), with probability 1−δ1𝛿1-\\delta, we have∎Linear Mode Connectivity. Linear mode connectivity (LMC) refers to the phenomenon that there exists a loss (energy) barrier along the linear interpolation path of two networks, in the cases where i) the two networks have the same initialization and are trained on the same dataset but with different random seeds (data shuffles) or augmentations (Ainsworth et al., 2022); ii) the two networks are with different initializations but are trained on the same dataset (Entezari et al., 2022); iii) the two networks are the initial network and the final trained network (Vlaar & Frankle, 2022).\nIn our paper, the transitivity of LMC can be applied to i), ii), and iii), and especially, the two trained models can have different initializations.\nSpecifically, Adilova et al. (2023) examines layer-wise LMC, and finds that there may be no barriers in the layer-wise manner. Frankle et al. (2020) connects linear mode connectivity with the lottery ticket hypothesis and finds better connectivity can result in better pruning performances. Vlaar & Frankle (2022) studies the relationship between generalization and the initial-to-final linear mode connectivity. Zhao et al. (2020) bridges mode connectivity and adversarial robustness. Some works try to extend mode connectivity beyond “linear”, e.g., searching for a non-linear low-loss path (Draxler et al., 2018) or studying mode connectivity under spurious attributes (Lubana et al., 2023).Studying the barriers in LMC is an important direction of LMC. Previous works find that there may be no barriers between different modes, but the connected regions may be non-linear (Draxler et al., 2018; Garipov et al., 2018). In Garipov et al. (2018), the authors propose to find paths along modes by learning Polygonal chain and Bezier curve. Also, Nudged Elastic Band can also be used to find that connected paths (Draxler et al., 2018). In Wortsman et al. (2021), the authors propose to learn connected but diverse low-loss subspaces for efficient ensembling. Our work about the transitivity of LMC is inspired by the previous works of learning connected paths. However, instead of learning diverse modes for ensembling, we aim to use the anchor model to improve the linear connectivity between two independent modes.Generalization of Federated Learning. Generalization and personalization are two important goals of federated learning systems (Chen & Chao, 2022; Li et al., 2023a, b; Yuan et al., 2022). Previous works study and understand the property and nature of generalization in FL. In Yuan et al. (2022), the authors rethink the previous definition of generalization by considering the data distributions of non-participated clients as the participation gap and propose a new data split method based on the insight. In the paper of FedRoD (Chen & Chao, 2022), the authors claim that generalization and personalization are not conflicted; instead, improving generalization is the basis for better personalization.Some works aim to improve generalization from both the server and client sides. For the clients, sharpness-aware minimization methods are introduced at the local to find a flatter minimum of local solvers for better generalization (Caldarola et al., 2022; Qu et al., 2022). Global sharpness-aware minimization is also considered (Dai et al., 2023). In addition, previous literature seeks to tackle local heterogeneity to improve generalization, and methods like proximal terms (Li et al., 2020a), dynamic regularization (Acar et al., 2020), variance reduction (Karimireddy et al., 2020), logit calibration (Zhang et al., 2022), fixed classifier (Li et al., 2023b), and balanced loss (Chen & Chao, 2022) are devised. For the server, weighted aggregation approaches to de-bias local updates (Wang et al., 2020) or heterogeneity (Ye et al., 2023) can improve generalization. Recently, global weight shrinking that sets smaller aggregation weights has been studied for unleashing the potential of weight regularization in boosting the generalization of FL (Li et al., 2023a).",
        "references": [
            [
                "We first give the hypothesis on the transitivity of LMC.",
                "Transitivity of linear mode connectivity (informal).",
                " There are three models ",
                "{",
                "𝐰",
                "1",
                ",",
                "𝐰",
                "2",
                ",",
                "𝐰",
                "anc",
                "∗",
                "}",
                "subscript",
                "𝐰",
                "1",
                "subscript",
                "𝐰",
                "2",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "\\{\\mathbf{w}_{1},\\mathbf{w}_{2},{\\mathbf{w}_{\\text{anc}}^{*}}\\}",
                ". If the linear mode connectivity between ",
                "𝐰",
                "1",
                "subscript",
                "𝐰",
                "1",
                "\\mathbf{w}_{1}",
                " and ",
                "𝐰",
                "anc",
                "∗",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ", as well as the one between ",
                "𝐰",
                "2",
                "subscript",
                "𝐰",
                "2",
                "\\mathbf{w}_{2}",
                " and ",
                "𝐰",
                "anc",
                "∗",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ", are independently improved, then, the linear mode connectivity between ",
                "𝐰",
                "1",
                "subscript",
                "𝐰",
                "1",
                "\\mathbf{w}_{1}",
                " and ",
                "𝐰",
                "2",
                "subscript",
                "𝐰",
                "2",
                "\\mathbf{w}_{2}",
                " is also improved.",
                "We make a theoretical analysis to prove the transitivity of LMC. We make the assumption below, following the Assumption 7 in ",
                "Ferbach et al. (",
                "2023",
                ")",
                " and the Assumption 1 in ",
                "Li et al. (",
                "2019",
                ")",
                ".",
                "∀",
                "y",
                "∈",
                "𝕐",
                "for-all",
                "𝑦",
                "𝕐",
                "\\forall y\\in\\mathbb{Y}",
                ", the loss function ",
                "L",
                "​",
                "(",
                "⋅",
                ",",
                "y",
                ")",
                "𝐿",
                "⋅",
                "𝑦",
                "L\\mathbb{(}\\cdot,y)",
                " is convex and 1-Lipschitz for each ",
                "y",
                "𝑦",
                "y",
                " and the loss ",
                "ℒ",
                "​",
                "(",
                "⋅",
                ")",
                "ℒ",
                "⋅",
                "\\mathcal{L}(\\cdot)",
                " is ",
                "γ",
                "𝛾",
                "\\gamma",
                "-smooth, where ",
                "ℒ",
                "​",
                "(",
                "𝐰",
                ")",
                "=",
                "𝔼",
                "​",
                "[",
                "L",
                "​",
                "(",
                "f",
                "𝐰",
                "​",
                "(",
                "x",
                ")",
                ",",
                "y",
                ")",
                "]",
                "ℒ",
                "𝐰",
                "𝔼",
                "delimited-[]",
                "𝐿",
                "subscript",
                "𝑓",
                "𝐰",
                "𝑥",
                "𝑦",
                "\\mathcal{L}(\\mathbf{w})=\\mathbb{E}[L(f_{\\mathbf{w}}(x),y)]",
                " and the expectation ",
                "𝔼",
                "𝔼",
                "\\mathbb{E}",
                " is taken over the dataset.",
                "Set the uniform and bounded domain for network ",
                "𝐰",
                "𝐰",
                "\\mathbf{w}",
                " as ",
                "ℰ",
                "ϵ",
                "=",
                "{",
                "𝐰",
                "∈",
                "Ω",
                "|",
                "ℒ",
                "​",
                "(",
                "𝐰",
                ")",
                "<",
                "ϵ",
                "}",
                "subscript",
                "ℰ",
                "italic-ϵ",
                "conditional-set",
                "𝐰",
                "Ω",
                "ℒ",
                "𝐰",
                "italic-ϵ",
                "\\mathcal{E}_{\\epsilon}=\\{\\mathbf{w}\\in\\Omega|\\mathcal{L}(\\mathbf{w})<\\epsilon\\}",
                ". Define a random event ",
                "D",
                "ϵ",
                "​",
                "(",
                "𝐰",
                "anc",
                "∗",
                ")",
                "subscript",
                "𝐷",
                "italic-ϵ",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})",
                " as ",
                "D",
                "ϵ",
                "​",
                "(",
                "𝐰",
                "anc",
                "∗",
                ")",
                "=",
                "{",
                "𝐰",
                "∈",
                "ℰ",
                "ϵ",
                "|",
                "∀",
                "α",
                "∈",
                "[",
                "0",
                ",",
                "1",
                "]",
                ",",
                "ℒ",
                "​",
                "(",
                "α",
                "​",
                "𝐰",
                "anc",
                "∗",
                "+",
                "(",
                "1",
                "−",
                "α",
                ")",
                "​",
                "𝐰",
                ")",
                "≤",
                "ϵ",
                "}",
                "subscript",
                "𝐷",
                "italic-ϵ",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "conditional-set",
                "𝐰",
                "subscript",
                "ℰ",
                "italic-ϵ",
                "formulae-sequence",
                "for-all",
                "𝛼",
                "0",
                "1",
                "ℒ",
                "𝛼",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "1",
                "𝛼",
                "𝐰",
                "italic-ϵ",
                "D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})=\\{\\mathbf{w}\\in\\mathcal{E}_{\\epsilon}|\\forall\\alpha\\in[0,1],\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w})\\leq\\epsilon\\}",
                ". Consider an anchor model ",
                "𝐰",
                "anc",
                "∗",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                " and an arbitrary network ",
                "𝐰",
                "𝐰",
                "\\mathbf{w}",
                " and for ",
                "ϵ",
                ">",
                "0",
                "italic-ϵ",
                "0",
                "\\epsilon>0",
                ". Then for ",
                "‖",
                "𝐰",
                "−",
                "𝐰",
                "anc",
                "∗",
                "‖",
                "∞",
                "≤",
                "d",
                "2",
                "subscript",
                "norm",
                "𝐰",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "𝑑",
                "2",
                "\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq\\frac{d}{2}",
                ",",
                "where ",
                "d",
                "ϵ",
                "=",
                "|",
                "ℰ",
                "ϵ",
                "|",
                "1",
                "S",
                "subscript",
                "𝑑",
                "italic-ϵ",
                "superscript",
                "subscript",
                "ℰ",
                "italic-ϵ",
                "1",
                "𝑆",
                "d_{\\epsilon}=\\left|\\mathcal{E}_{\\epsilon}\\right|^{\\frac{1}{S}}",
                " represents the average diameter of region ",
                "ℰ",
                "ϵ",
                "subscript",
                "ℰ",
                "italic-ϵ",
                "\\mathcal{E}_{\\epsilon}",
                ", ",
                "S",
                "𝑆",
                "S",
                " represents the number of parameters of the neural network and the equality holds if and only if ",
                "ℰ",
                "ϵ",
                "⊂",
                "{",
                "𝐰",
                "|",
                "‖",
                "𝐰",
                "−",
                "𝐰",
                "anc",
                "∗",
                "‖",
                "∞",
                "≤",
                "d",
                "}",
                "subscript",
                "ℰ",
                "italic-ϵ",
                "conditional-set",
                "𝐰",
                "subscript",
                "norm",
                "𝐰",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "𝑑",
                "\\mathcal{E}_{\\epsilon}\\subset\\{\\mathbf{w}|\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq d\\}",
                " is a star domain centered at ",
                "𝐰",
                "anc",
                "∗",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ". Thus, when ",
                "P",
                "(",
                "D",
                "ϵ",
                "(",
                "𝐰",
                "anc",
                "∗",
                ")",
                ")",
                ")",
                ">",
                "1",
                "−",
                "δ",
                "P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})))>1-\\delta",
                ", it holds ",
                "d",
                "<",
                "d",
                "ϵ",
                "(",
                "1",
                "−",
                "δ",
                ")",
                "1",
                "S",
                "𝑑",
                "subscript",
                "𝑑",
                "italic-ϵ",
                "superscript",
                "1",
                "𝛿",
                "1",
                "𝑆",
                "d<{\\frac{d_{\\epsilon}}{(1-\\delta)^{\\frac{1}{S}}}}",
                ".",
                "This lemma links the distance between parameters to LMC, describing that the greater the probability of LMC (i.e., a small loss barrier) existing between the network ",
                "𝐰",
                "𝐰",
                "\\mathbf{w}",
                " and the anchor model ",
                "𝐰",
                "anc",
                "∗",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ", the smaller the distance should be between ",
                "𝐰",
                "𝐰",
                "\\mathbf{w}",
                " and ",
                "𝐰",
                "anc",
                "∗",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ".",
                "Then, we provide the following theorem.",
                "We define a two-layer neural network with ReLU activation, and the function is ",
                "f",
                "𝐯",
                ",",
                "𝐔",
                "​",
                "(",
                "𝐱",
                ")",
                "=",
                "𝐯",
                "⊤",
                "​",
                "σ",
                "​",
                "(",
                "𝐔𝐱",
                ")",
                "subscript",
                "𝑓",
                "𝐯",
                "𝐔",
                "𝐱",
                "superscript",
                "𝐯",
                "top",
                "𝜎",
                "𝐔𝐱",
                "f_{\\mathbf{v},\\mathbf{U}}(\\mathbf{x})=\\mathbf{v}^{\\top}\\sigma(\\mathbf{U}\\mathbf{x})",
                " where ",
                "σ",
                "​",
                "(",
                "⋅",
                ")",
                "𝜎",
                "⋅",
                "\\sigma(\\cdot)",
                " is the ReLU activation function. ",
                "𝐯",
                "∈",
                "ℝ",
                "h",
                "𝐯",
                "superscript",
                "ℝ",
                "ℎ",
                "\\mathbf{v}\\in\\mathbb{R}^{h}",
                " and ",
                "𝐔",
                "∈",
                "ℝ",
                "h",
                "×",
                "l",
                "𝐔",
                "superscript",
                "ℝ",
                "ℎ",
                "𝑙",
                "\\mathbf{U}\\in\\mathbb{R}^{h\\times l}",
                " are parameters",
                "1",
                "1",
                "1",
                "For simplicity and without loss of generality, we omit the bias terms.",
                " and ",
                "𝐱",
                "∈",
                "ℝ",
                "l",
                "𝐱",
                "superscript",
                "ℝ",
                "𝑙",
                "\\mathbf{x}\\in\\mathbb{R}^{l}",
                " is the input which is taken from ",
                "𝕏",
                "=",
                "{",
                "𝐱",
                "∈",
                "ℝ",
                "l",
                "|",
                "‖",
                "𝐱",
                "‖",
                "2",
                "<",
                "b",
                "}",
                "𝕏",
                "conditional-set",
                "𝐱",
                "superscript",
                "ℝ",
                "𝑙",
                "subscript",
                "norm",
                "𝐱",
                "2",
                "𝑏",
                "\\mathbb{X}=\\{\\mathbf{x}\\in\\mathbb{R}^{l}|\\|\\mathbf{x}\\|_{2}<b\\}",
                " uniformly. Denote the deterministic anchor model as ",
                "𝐰",
                "anc",
                "∗",
                "=",
                "{",
                "𝐔",
                "anc",
                "∗",
                ",",
                "𝐯",
                "anc",
                "∗",
                "}",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "superscript",
                "subscript",
                "𝐔",
                "anc",
                "superscript",
                "subscript",
                "𝐯",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}=\\{\\mathbf{U}_{\\text{anc}}^{*},\\mathbf{v}_{\\text{anc}}^{*}\\}",
                ", with ",
                "‖",
                "𝐯",
                "anc",
                "∗",
                "‖",
                "2",
                "<",
                "d",
                "anc",
                "subscript",
                "norm",
                "superscript",
                "subscript",
                "𝐯",
                "anc",
                "2",
                "subscript",
                "𝑑",
                "anc",
                "\\|\\mathbf{v}_{\\text{anc}}^{*}\\|_{2}<d_{\\text{anc}}",
                " and consider two different networks ",
                "𝐰",
                "1",
                ",",
                "𝐰",
                "2",
                "subscript",
                "𝐰",
                "1",
                "subscript",
                "𝐰",
                "2",
                "\\mathbf{w}_{1},\\mathbf{w}_{2}",
                " parameterized with ",
                "{",
                "𝐔",
                "1",
                ",",
                "𝐯",
                "1",
                "}",
                "subscript",
                "𝐔",
                "1",
                "subscript",
                "𝐯",
                "1",
                "\\{\\mathbf{U}_{1},\\mathbf{v}_{1}\\}",
                " and ",
                "{",
                "𝐔",
                "2",
                ",",
                "𝐯",
                "2",
                "}",
                "subscript",
                "𝐔",
                "2",
                "subscript",
                "𝐯",
                "2",
                "\\{\\mathbf{U}_{2},\\mathbf{v}_{2}\\}",
                " respectively. Each element of ",
                "𝐔",
                "1",
                "subscript",
                "𝐔",
                "1",
                "\\mathbf{U}_{1}",
                " and ",
                "𝐔",
                "2",
                "subscript",
                "𝐔",
                "2",
                "\\mathbf{U}_{2}",
                ", ",
                "𝐯",
                "1",
                "subscript",
                "𝐯",
                "1",
                "\\mathbf{v}_{1}",
                " and ",
                "𝐯",
                "2",
                "subscript",
                "𝐯",
                "2",
                "\\mathbf{v}_{2}",
                " is sampled from a uniform distribution centered at ",
                "𝐔",
                "anc",
                "∗",
                "superscript",
                "subscript",
                "𝐔",
                "anc",
                "\\mathbf{U}_{\\text{anc}}^{*}",
                " and ",
                "𝐯",
                "anc",
                "∗",
                "superscript",
                "subscript",
                "𝐯",
                "anc",
                "\\mathbf{v}_{\\text{anc}}^{*}",
                " with an interval length of ",
                "d",
                "𝑑",
                "d",
                ". If with probability ",
                "1",
                "−",
                "δ",
                "1",
                "𝛿",
                "1-\\delta",
                ", ",
                "sup",
                "α",
                "ℒ",
                "​",
                "(",
                "α",
                "​",
                "𝐰",
                "anc",
                "∗",
                "+",
                "(",
                "1",
                "−",
                "α",
                ")",
                "​",
                "𝐰",
                "1",
                ")",
                "<",
                "ϵ",
                "subscript",
                "supremum",
                "𝛼",
                "ℒ",
                "𝛼",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "1",
                "𝛼",
                "subscript",
                "𝐰",
                "1",
                "italic-ϵ",
                "\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{1})<\\epsilon",
                " and ",
                "sup",
                "α",
                "ℒ",
                "​",
                "(",
                "α",
                "​",
                "𝐰",
                "anc",
                "∗",
                "+",
                "(",
                "1",
                "−",
                "α",
                ")",
                "​",
                "𝐰",
                "2",
                ")",
                "<",
                "ϵ",
                "subscript",
                "supremum",
                "𝛼",
                "ℒ",
                "𝛼",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "1",
                "𝛼",
                "subscript",
                "𝐰",
                "2",
                "italic-ϵ",
                "\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{2})<\\epsilon",
                ", then with probability ",
                "1",
                "−",
                "δ",
                "1",
                "𝛿",
                "1-\\delta",
                ", it has,",
                "where ",
                "B",
                "l",
                "​",
                "o",
                "​",
                "s",
                "​",
                "s",
                "​",
                "(",
                "𝐰",
                "1",
                ",",
                "𝐰",
                "2",
                ")",
                "subscript",
                "𝐵",
                "𝑙",
                "𝑜",
                "𝑠",
                "𝑠",
                "subscript",
                "𝐰",
                "1",
                "subscript",
                "𝐰",
                "2",
                "B_{loss}(\\mathbf{w}_{1},\\mathbf{w}_{2})",
                " is the loss barrier as ",
                "Equation 3",
                ".",
                "The proofs are in ",
                "Appendix B",
                ". ",
                "Theorem 3.5",
                " proves the transitivity of LMC that when ",
                "𝐰",
                "1",
                "subscript",
                "𝐰",
                "1",
                "\\mathbf{w}_{1}",
                " and ",
                "𝐰",
                "2",
                "subscript",
                "𝐰",
                "2",
                "\\mathbf{w}_{2}",
                " have lower LMC barrier with ",
                "𝐰",
                "anc",
                "∗",
                "superscript",
                "subscript",
                "𝐰",
                "anc",
                "\\mathbf{w}_{\\text{anc}}^{*}",
                " (the barrier proxy is ",
                "ϵ",
                "italic-ϵ",
                "\\epsilon",
                ") then the barrier between ",
                "𝐰",
                "1",
                "subscript",
                "𝐰",
                "1",
                "\\mathbf{w}_{1}",
                " and ",
                "𝐰",
                "2",
                "subscript",
                "𝐰",
                "2",
                "\\mathbf{w}_{2}",
                " is also reduced and bounded.",
                "Then, we will empirically validate the transitivity."
            ]
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Results in terms of generalization accuracy (%) of global models on four datasets under different data heterogeneity. The best two methods in each setting are highlighted in bold fonts. M=50,E=3formulae-sequence𝑀50𝐸3M=50,E=3. ",
        "table": "<table id=\"S3.T2.11\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.11.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.T2.11.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dataset</span></th>\n<td id=\"S3.T2.11.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S3.T2.11.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Fashion-MNIST</span></td>\n<td id=\"S3.T2.11.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S3.T2.11.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-10</span></td>\n<td id=\"S3.T2.11.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S3.T2.11.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-100</span></td>\n<td id=\"S3.T2.11.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S3.T2.11.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Tiny-ImageNet</span></td>\n</tr>\n<tr id=\"S3.T2.11.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Non-IID hyper.</span></th>\n<td id=\"S3.T2.11.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S3.T2.11.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.5</span></td>\n<td id=\"S3.T2.11.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S3.T2.11.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.5</span></td>\n<td id=\"S3.T2.11.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S3.T2.11.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.5</span></td>\n<td id=\"S3.T2.11.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S3.T2.11.2.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.5</span></td>\n</tr>\n<tr id=\"S3.T2.11.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Local</span></th>\n<td id=\"S3.T2.11.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">76.22±0.16</span></td>\n<td id=\"S3.T2.11.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">62.24±0.35</span></td>\n<td id=\"S3.T2.11.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">36.69±0.10</span></td>\n<td id=\"S3.T2.11.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">29.73±0.36</span></td>\n<td id=\"S3.T2.11.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.19±0.08</span></td>\n<td id=\"S3.T2.11.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.3.3.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.29±0.04</span></td>\n<td id=\"S3.T2.11.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">6.47±0.12</span></td>\n<td id=\"S3.T2.11.3.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">6.09±0.02</span></td>\n</tr>\n<tr id=\"S3.T2.11.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S3.T2.11.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.94±0.34</span></td>\n<td id=\"S3.T2.11.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.99±0.04</span></td>\n<td id=\"S3.T2.11.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.55±0.16</span></td>\n<td id=\"S3.T2.11.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.99±0.32</span></td>\n<td id=\"S3.T2.11.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.78±1.17</span></td>\n<td id=\"S3.T2.11.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.4.4.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">33.49±0.90</span></td>\n<td id=\"S3.T2.11.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.43±1.39</span></td>\n<td id=\"S3.T2.11.4.4.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.11±1.82</span></td>\n</tr>\n<tr id=\"S3.T2.11.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedProx</span></th>\n<td id=\"S3.T2.11.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">10.00±0.00</span></td>\n<td id=\"S3.T2.11.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">10.00±0.00</span></td>\n<td id=\"S3.T2.11.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.81±0.47</span></td>\n<td id=\"S3.T2.11.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.5.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.45±0.43</span></td>\n<td id=\"S3.T2.11.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">13.18±0.82</span></td>\n<td id=\"S3.T2.11.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.5.5.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">15.62±0.40</span></td>\n<td id=\"S3.T2.11.5.5.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.58±0.28</span></td>\n<td id=\"S3.T2.11.5.5.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.02±0.19</span></td>\n</tr>\n<tr id=\"S3.T2.11.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedDyn</span></th>\n<td id=\"S3.T2.11.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.26±0.17</span></td>\n<td id=\"S3.T2.11.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.18±0.36</span></td>\n<td id=\"S3.T2.11.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.99±0.64</span></td>\n<td id=\"S3.T2.11.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.6.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.73±0.31</span></td>\n<td id=\"S3.T2.11.6.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.24±0.86</span></td>\n<td id=\"S3.T2.11.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.6.6.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">33.20±1.81</span></td>\n<td id=\"S3.T2.11.6.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">30.89±0.03</span></td>\n<td id=\"S3.T2.11.6.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.63±2.68</span></td>\n</tr>\n<tr id=\"S3.T2.11.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SCAFFOLD</span></th>\n<td id=\"S3.T2.11.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.95±0.31</span></td>\n<td id=\"S3.T2.11.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.47±0.14</span></td>\n<td id=\"S3.T2.11.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.20±0.32</span></td>\n<td id=\"S3.T2.11.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.96±0.41</span></td>\n<td id=\"S3.T2.11.7.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.06±0.94</span></td>\n<td id=\"S3.T2.11.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.7.7.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">33.84±0.81</span></td>\n<td id=\"S3.T2.11.7.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.529±0.05</span></td>\n<td id=\"S3.T2.11.7.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.517±0.02</span></td>\n</tr>\n<tr id=\"S3.T2.11.8.8\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.8.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MOON</span></th>\n<td id=\"S3.T2.11.8.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.95±0.09</span></td>\n<td id=\"S3.T2.11.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.8.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.02±0.29</span></td>\n<td id=\"S3.T2.11.8.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.24±0.65</span></td>\n<td id=\"S3.T2.11.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.8.8.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.41±0.31</span></td>\n<td id=\"S3.T2.11.8.8.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.90±3.16</span></td>\n<td id=\"S3.T2.11.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.8.8.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">32.64±0.08</span></td>\n<td id=\"S3.T2.11.8.8.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.88±1.08</span></td>\n<td id=\"S3.T2.11.8.8.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.34±0.66</span></td>\n</tr>\n<tr id=\"S3.T2.11.9.9\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.9.9.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedRoD</span></th>\n<td id=\"S3.T2.11.9.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.97±0.40</span></td>\n<td id=\"S3.T2.11.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.9.9.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.56±0.60</span></td>\n<td id=\"S3.T2.11.9.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">62.64±0.20</span></td>\n<td id=\"S3.T2.11.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.9.9.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">62.56±0.46</span></td>\n<td id=\"S3.T2.11.9.9.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.77±1.13</span></td>\n<td id=\"S3.T2.11.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.9.9.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.04±0.89</span></td>\n<td id=\"S3.T2.11.9.9.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.67±1.64</span></td>\n<td id=\"S3.T2.11.9.9.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.55±1.56</span></td>\n</tr>\n<tr id=\"S3.T2.11.10.10\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.10.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedLC</span></th>\n<td id=\"S3.T2.11.10.10.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.90±0.36</span></td>\n<td id=\"S3.T2.11.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.10.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.79±0.29</span></td>\n<td id=\"S3.T2.11.10.10.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.49±0.17</span></td>\n<td id=\"S3.T2.11.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.10.10.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.97±0.35</span></td>\n<td id=\"S3.T2.11.10.10.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.93±1.07</span></td>\n<td id=\"S3.T2.11.10.10.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.10.10.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">31.79±0.78</span></td>\n<td id=\"S3.T2.11.10.10.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.63±1.62</span></td>\n<td id=\"S3.T2.11.10.10.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.47±1.84</span></td>\n</tr>\n<tr id=\"S3.T2.11.11.11\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.11.11.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedSAM</span></th>\n<td id=\"S3.T2.11.11.11.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.41±0.49</span></td>\n<td id=\"S3.T2.11.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.11.11.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.62±0.30</span></td>\n<td id=\"S3.T2.11.11.11.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.10±0.41</span></td>\n<td id=\"S3.T2.11.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.11.11.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.02±0.15</span></td>\n<td id=\"S3.T2.11.11.11.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.24±1.58</span></td>\n<td id=\"S3.T2.11.11.11.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.11.11.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">33.64±1.31</span></td>\n<td id=\"S3.T2.11.11.11.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">31.23±0.16</span></td>\n<td id=\"S3.T2.11.11.11.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">30.44±0.97</span></td>\n</tr>\n<tr id=\"S3.T2.11.12.12\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S3.T2.11.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.12.12.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci</span></th>\n<td id=\"S3.T2.11.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">88.85±0.11</span></td>\n<td id=\"S3.T2.11.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.12.12.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">88.30±0.39</span></td>\n<td id=\"S3.T2.11.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.11±0.11</span></td>\n<td id=\"S3.T2.11.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.12.12.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.80±0.22</span></td>\n<td id=\"S3.T2.11.12.12.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">28.98±1.33</span></td>\n<td id=\"S3.T2.11.12.12.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.12.12.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">33.96±1.18</span></td>\n<td id=\"S3.T2.11.12.12.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">36.46±0.40</span></td>\n<td id=\"S3.T2.11.12.12.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">33.61±0.60</span></td>\n</tr>\n<tr id=\"S3.T2.11.13.13\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S3.T2.11.13.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S3.T2.11.13.13.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci+</span></th>\n<td id=\"S3.T2.11.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">89.38±0.14</span></td>\n<td id=\"S3.T2.11.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S3.T2.11.13.13.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">88.61±0.40</span></td>\n<td id=\"S3.T2.11.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">68.11±0.27</span></td>\n<td id=\"S3.T2.11.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S3.T2.11.13.13.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">66.44±0.69</span></td>\n<td id=\"S3.T2.11.13.13.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">28.86±1.09</span></td>\n<td id=\"S3.T2.11.13.13.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S3.T2.11.13.13.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">34.25±1.12</span></td>\n<td id=\"S3.T2.11.13.13.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">37.42±0.52</span></td>\n<td id=\"S3.T2.11.13.13.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">34.80±0.35</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Results under pretrained language models. We use 6 datasets from GLUE (Wang et al., 2019) benchmark for finetuning pretrained language models. For each dataset, we randomly split the data into several clients and conduct finetuning using low-rank adaption (LoRA), and the pretrained model is RoBERTa-base (Liu et al., 2019). It is notable that some language tasks are not classifications, so FedRoD, FedLC, and FedGuCci+, which rely on classification loss, are not applicable. The results are in Table 3, where our FedGuCci reaches promising performances over existing methods. It is observed that some methods that are superior in Table 2 have worse performances in pretrained language models, e.g., FedDyn, while our FedGuCci keeps steady advantages."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Results of pretrained language models on natural language processing (GLUE benchmark).",
        "table": "<table id=\"S4.T3.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T3.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Methods/Tasks</span></th>\n<td id=\"S4.T3.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">SST-2</span></td>\n<td id=\"S4.T3.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">MRPC</span></td>\n<td id=\"S4.T3.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">CoLA</span></td>\n<td id=\"S4.T3.4.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">QNLI</span></td>\n<td id=\"S4.T3.4.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">RTE</span></td>\n<td id=\"S4.T3.4.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">STS-B</span></td>\n<td id=\"S4.T3.4.1.1.8\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">AVG</span></td>\n</tr>\n<tr id=\"S4.T3.4.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Local</span></th>\n<td id=\"S4.T3.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.55±0.19</span></td>\n<td id=\"S4.T3.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.38±0.37</span></td>\n<td id=\"S4.T3.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">47.98±1.01</span></td>\n<td id=\"S4.T3.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.66±0.10</span></td>\n<td id=\"S4.T3.4.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">55.69±1.03</span></td>\n<td id=\"S4.T3.4.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.2.2.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.11±0.36</span></td>\n<td id=\"S4.T3.4.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">75.40±0.51</span></td>\n</tr>\n<tr id=\"S4.T3.4.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S4.T3.4.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.79±0.24</span></td>\n<td id=\"S4.T3.4.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.17±0.38</span></td>\n<td id=\"S4.T3.4.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">53.86±0.70</span></td>\n<td id=\"S4.T3.4.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.52±0.14</span></td>\n<td id=\"S4.T3.4.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">68.63±1.53</span></td>\n<td id=\"S4.T3.4.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.3.3.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">88.61±0.34</span></td>\n<td id=\"S4.T3.4.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.76±0.56</span></td>\n</tr>\n<tr id=\"S4.T3.4.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedProx</span></th>\n<td id=\"S4.T3.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">50.88±0.00</span></td>\n<td id=\"S4.T3.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">67.26±0.75</span></td>\n<td id=\"S4.T3.4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">00.00±0.00</span></td>\n<td id=\"S4.T3.4.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">50.55±0.98</span></td>\n<td id=\"S4.T3.4.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">49.39±3.42</span></td>\n<td id=\"S4.T3.4.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.4.4.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">00.00±0.00</span></td>\n<td id=\"S4.T3.4.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">54.52±1.71</span></td>\n</tr>\n<tr id=\"S4.T3.4.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T3.4.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedDyn</span></th>\n<td id=\"S4.T3.4.5.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">91.19±0.85</span></td>\n<td id=\"S4.T3.4.5.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.80±0.41</span></td>\n<td id=\"S4.T3.4.5.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">55.49±1.02</span></td>\n<td id=\"S4.T3.4.5.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">85.51±0.54</span></td>\n<td id=\"S4.T3.4.5.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.40±3.89</span></td>\n<td id=\"S4.T3.4.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.5.5.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.75±9.38</span></td>\n<td id=\"S4.T3.4.5.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">67.19±2.68</span></td>\n</tr>\n<tr id=\"S4.T3.4.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T3.4.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SCAFFOLD</span></th>\n<td id=\"S4.T3.4.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.75±0.12</span></td>\n<td id=\"S4.T3.4.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.11±0.65</span></td>\n<td id=\"S4.T3.4.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">54.28±0.31</span></td>\n<td id=\"S4.T3.4.6.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.73±0.16</span></td>\n<td id=\"S4.T3.4.6.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">69.24±2.76</span></td>\n<td id=\"S4.T3.4.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.6.6.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.31±0.31</span></td>\n<td id=\"S4.T3.4.6.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">78.90±0.72</span></td>\n</tr>\n<tr id=\"S4.T3.4.7.7\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T3.4.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedSAM</span></th>\n<td id=\"S4.T3.4.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">92.79±0.14</span></td>\n<td id=\"S4.T3.4.7.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">84.81±0.08</span></td>\n<td id=\"S4.T3.4.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">53.25±0.43</span></td>\n<td id=\"S4.T3.4.7.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">82.13±0.34</span></td>\n<td id=\"S4.T3.4.7.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">68.14±2.09</span></td>\n<td id=\"S4.T3.4.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.7.7.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.71±0.42</span></td>\n<td id=\"S4.T3.4.7.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.14±0.58</span></td>\n</tr>\n<tr id=\"S4.T3.4.8.8\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T3.4.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.8.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci</span></th>\n<td id=\"S4.T3.4.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">93.22±0.20</span></td>\n<td id=\"S4.T3.4.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">85.77±0.44</span></td>\n<td id=\"S4.T3.4.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">55.38±0.44</span></td>\n<td id=\"S4.T3.4.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">89.40±0.40</span></td>\n<td id=\"S4.T3.4.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">70.96±1.60</span></td>\n<td id=\"S4.T3.4.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.8.8.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">89.25±0.44</span></td>\n<td id=\"S4.T3.4.8.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">80.66±0.59</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Results under pretrained language models. We use 6 datasets from GLUE (Wang et al., 2019) benchmark for finetuning pretrained language models. For each dataset, we randomly split the data into several clients and conduct finetuning using low-rank adaption (LoRA), and the pretrained model is RoBERTa-base (Liu et al., 2019). It is notable that some language tasks are not classifications, so FedRoD, FedLC, and FedGuCci+, which rely on classification loss, are not applicable. The results are in Table 3, where our FedGuCci reaches promising performances over existing methods. It is observed that some methods that are superior in Table 2 have worse performances in pretrained language models, e.g., FedDyn, while our FedGuCci keeps steady advantages.",
            "CIFAR-10 (Krizhevsky et al., 2009) consists of 60,000 32x32 color images, evenly distributed among 10 different classes, including airplanes, automobiles, birds, cats, etc., each represented by 6,000 images. The dataset is split into 50,000 training images and 10,000 test images.\nFashionMNIST (Xiao et al., 2017) is designed as an advanced replacement for the MNIST dataset, suitable for benchmarking machine learning models. It comprises 70,000 images divided into 60,000 training samples and 10,000 test samples. Each image is a 28x28 grayscale representation of fashion items from 10 different classes, such as shirts, trousers, sneakers, etc.\nThe CIFAR-100 dataset (Krizhevsky et al., 2009) is similar to CIFAR-10 but more challenging, containing 100 different classes grouped into 20 superclasses. It includes 60,000 32x32 color images, with 600 images per class, divided into 50,000 training images and 10,000 test images. This dataset is primarily used for developing and evaluating more sophisticated image classification models.\nTinyImageNet TinyImageNet is a reduced-scale version of the renowned ImageNet dataset, which comprises a total of 200 classes. The dataset is structured into training, validation, and test sets, with 200,000 training images, 20,000 validation images, and 20,000 test images.\nThe GLUE benchmark is a compilation of 9 datasets for evaluating natural language understanding systems. Tasks are framed as either single-sentence classification or sentence-pair classification tasks. GLUE includes MNLI (inference, (Williams et al., 2017)), MRPC (paraphrase detection, (Socher et al., 2013)), MRPC (paraphrase detection, (Dolan & Brockett, 2005)), CoLA (linguistic acceptability, (Warstadt et al., 2019)), QNLI (inference, (Rajpurkar et al., 2018)), QQP (question-answering), RTE (inference), WNLI (inference), and STS-B (textual similarity, (Cer et al., 2017)). Due to high computation costs, we only used SST2, MRPC, CoLA, QNLI, RTE, and STS-B for evaluation. For the replication in Table 3, we report results on the development sets after fine-tuning the pretrained models on the corresponding single-task training data. Our fine-tuning approach is LoRA(Hu et al., 2021)."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Results on different numbers of clients and participation ratios. Non-IID hyper. is 1.0, and the dataset is CIFAR-10.",
        "table": "<table id=\"S4.T4.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S4.T4.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S4.T4.1.1.1.1.m1.1a\"><mi mathsize=\"80%\" id=\"S4.T4.1.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.m1.1b\"><ci id=\"S4.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.m1.1c\">M</annotation></semantics></math></th>\n<td id=\"S4.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S4.T4.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S4.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S4.T4.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">200</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><math id=\"S4.T4.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rho\" display=\"inline\"><semantics id=\"S4.T4.2.2.2.1.m1.1a\"><mi mathsize=\"80%\" id=\"S4.T4.2.2.2.1.m1.1.1\" xref=\"S4.T4.2.2.2.1.m1.1.1.cmml\">ρ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.1.m1.1b\"><ci id=\"S4.T4.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.1.m1.1.1\">𝜌</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.1.m1.1c\">\\rho</annotation></semantics></math></th>\n<td id=\"S4.T4.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.3</span></td>\n<td id=\"S4.T4.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.6</span></td>\n<td id=\"S4.T4.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.3</span></td>\n<td id=\"S4.T4.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.6</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Local</span></th>\n<td id=\"S4.T4.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.91±0.24</span></td>\n<td id=\"S4.T4.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.53±0.10</span></td>\n<td id=\"S4.T4.2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.39±0.18</span></td>\n<td id=\"S4.T4.2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.20±0.22</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S4.T4.2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.98±0.84</span></td>\n<td id=\"S4.T4.2.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.41±0.55</span></td>\n<td id=\"S4.T4.2.2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.37±0.79</span></td>\n<td id=\"S4.T4.2.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.15±1.01</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedProx</span></th>\n<td id=\"S4.T4.2.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">52.43±0.66</span></td>\n<td id=\"S4.T4.2.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">52.79±0.73</span></td>\n<td id=\"S4.T4.2.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">44.63±0.95</span></td>\n<td id=\"S4.T4.2.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">44.96±0.78</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T4.2.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedRoD</span></th>\n<td id=\"S4.T4.2.2.6.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.15±0.05</span></td>\n<td id=\"S4.T4.2.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">60.30±0.02</span></td>\n<td id=\"S4.T4.2.2.6.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">58.01±0.92</span></td>\n<td id=\"S4.T4.2.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">57.63±1.44</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T4.2.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedLC</span></th>\n<td id=\"S4.T4.2.2.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.70±0.69</span></td>\n<td id=\"S4.T4.2.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.7.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.24±0.70</span></td>\n<td id=\"S4.T4.2.2.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.7.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">60.99±0.66</span></td>\n<td id=\"S4.T4.2.2.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.7.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">60.67±0.81</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.8.6\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T4.2.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedSAM</span></th>\n<td id=\"S4.T4.2.2.8.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.87±0.58</span></td>\n<td id=\"S4.T4.2.2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.45±0.22</span></td>\n<td id=\"S4.T4.2.2.8.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">62.33±0.56</span></td>\n<td id=\"S4.T4.2.2.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.8.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.93±0.90</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.9.7\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T4.2.2.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.9.7.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci</span></th>\n<td id=\"S4.T4.2.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.9.7.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.02±0.41</span></td>\n<td id=\"S4.T4.2.2.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.9.7.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">64.54±0.41</span></td>\n<td id=\"S4.T4.2.2.9.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.9.7.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">62.37±0.83</span></td>\n<td id=\"S4.T4.2.2.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.9.7.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">62.13±0.63</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.10.8\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T4.2.2.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S4.T4.2.2.10.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci+</span></th>\n<td id=\"S4.T4.2.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.2.2.10.8.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.34±0.21</span></td>\n<td id=\"S4.T4.2.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T4.2.2.10.8.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.50±0.35</span></td>\n<td id=\"S4.T4.2.2.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.2.2.10.8.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">63.29±0.71</span></td>\n<td id=\"S4.T4.2.2.10.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T4.2.2.10.8.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">63.93±0.81</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In the study of LMC, different modes are trained on the ",
                "same",
                " dataset but with different random seeds or initializations ",
                "(Entezari et al., ",
                "2022",
                ")",
                ". However, in FL, clients have ",
                "heterogeneous",
                " data, and it is found that data heterogeneity of clients will cause different curvatures of local loss landscapes ",
                "(Zhou et al., ",
                "2023",
                ")",
                ", making the connectivity worse. Therefore, aligning local loss landscapes is essential for better performances of the connectivity loss. In this subsection, we incorporate previous techniques in FedGuCci to align local loss landscapes and propose FedGuCci+.",
                "Bias reduction.",
                " In FL, class imbalance (a.k.a. label skew) is a main cause of data heterogeneity, and previous works propose logit calibration ",
                "(Zhang et al., ",
                "2022",
                ")",
                ", balanced softmax ",
                "(Chen & Chao, ",
                "2022",
                ")",
                ", and other techniques ",
                "(Li et al., ",
                "2023b",
                "; Acar et al., ",
                "2020",
                ")",
                " for reducing the bias caused by class imbalance. Here, we introduce the logit calibration technique used in FedLC ",
                "(Zhang et al., ",
                "2022",
                ")",
                " for bias reduction. The main idea of logit calibration is to add additional terms to the logits to balance the overall class distributions. From ",
                "Figure 5",
                " (b), it demonstrates that logit calibration and other bias reduction methods can align the landscapes by making the local objectives more consistent.",
                "Flatter minima.",
                " Sharpness-aware minimization ",
                "(Foret et al., ",
                "2021",
                "; Kwon et al., ",
                "2021",
                ")",
                " (SAM) find flatter minima to improve generalization. SAM has also been introduced in FL for better generalization ",
                "(Caldarola et al., ",
                "2022",
                "; Qu et al., ",
                "2022",
                ")",
                ". In our paper, we find SAM can be used to align local loss landscapes by making the landscapes flatter, so we also incorporate it in FedGuCci+. From ",
                "Figure 5",
                " (c), it can be seen that if the landscapes are flatter, the overlap regions between two clients will increase, therefore, it will have more aligned landscapes. Also, for FedGuCci, SAM makes the connectivity loss to learn a cylinder connected with the anchor model instead of a line ",
                "(Wen et al., ",
                "2023",
                ")",
                ", improving connectivity robustness and generalization.",
                "FedGuCci+ incorporates logit calibration and SAM into FedGuCci, achieving better generalization. We note that FedGuCci+ is a showcase of how FedGucCci is compatible with other existing techniques for better results, and more techniques can be integrated."
            ]
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Results of global models under pretrain-finetune vision models. Non-IID hyper. is 10.",
        "table": "<table id=\"S5.T5.5.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S5.T5.5.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dataset</span></th>\n<td id=\"S5.T5.5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S5.T5.5.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-10</span></td>\n<td id=\"S5.T5.5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S5.T5.5.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-100</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Models</span></th>\n<td id=\"S5.T5.5.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet-18</span></td>\n<td id=\"S5.T5.5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">ViT</span></td>\n<td id=\"S5.T5.5.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet-18</span></td>\n<td id=\"S5.T5.5.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">ViT</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Local</span></th>\n<td id=\"S5.T5.5.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.33±0.35</span></td>\n<td id=\"S5.T5.5.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.04±0.43</span></td>\n<td id=\"S5.T5.5.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">31.01±0.34</span></td>\n<td id=\"S5.T5.5.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.38±0.47</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S5.T5.5.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">74.89±0.16</span></td>\n<td id=\"S5.T5.5.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">96.16±0.19</span></td>\n<td id=\"S5.T5.5.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.4.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">45.24±0.57</span></td>\n<td id=\"S5.T5.5.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">83.61±0.69</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedProx</span></th>\n<td id=\"S5.T5.5.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">50.61±0.81</span></td>\n<td id=\"S5.T5.5.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.5.5.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">96.32±0.21</span></td>\n<td id=\"S5.T5.5.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.5.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">4.29±0.38</span></td>\n<td id=\"S5.T5.5.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.5.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.49±1.92</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.6.6\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T5.5.1.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedRoD</span></th>\n<td id=\"S5.T5.5.1.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">74.91±0.17</span></td>\n<td id=\"S5.T5.5.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">96.18±0.18</span></td>\n<td id=\"S5.T5.5.1.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">45.19±0.76</span></td>\n<td id=\"S5.T5.5.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.6.6.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">83.64±0.35</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.7.7\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T5.5.1.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedLC</span></th>\n<td id=\"S5.T5.5.1.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.7.7.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">74.94±0.13</span></td>\n<td id=\"S5.T5.5.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">96.21±0.17</span></td>\n<td id=\"S5.T5.5.1.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">45.18±0.65</span></td>\n<td id=\"S5.T5.5.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">83.38±0.64</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.8.8\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T5.5.1.8.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedSAM</span></th>\n<td id=\"S5.T5.5.1.8.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.8.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">69.92±0.51</span></td>\n<td id=\"S5.T5.5.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.8.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">56.88±6.79</span></td>\n<td id=\"S5.T5.5.1.8.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.8.8.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.92±0.64</span></td>\n<td id=\"S5.T5.5.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.8.8.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">38.36±13.5</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.9.9\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S5.T5.5.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.9.9.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci</span></th>\n<td id=\"S5.T5.5.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.9.9.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">75.22±0.12</span></td>\n<td id=\"S5.T5.5.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.9.9.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">96.38±0.11</span></td>\n<td id=\"S5.T5.5.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.9.9.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">45.62±0.61</span></td>\n<td id=\"S5.T5.5.1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.9.9.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">83.71±0.48</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.10.10\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S5.T5.5.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S5.T5.5.1.10.10.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci+</span></th>\n<td id=\"S5.T5.5.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T5.5.1.10.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">71.40±0.45</span></td>\n<td id=\"S5.T5.5.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T5.5.1.10.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">64.05±26.3</span></td>\n<td id=\"S5.T5.5.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T5.5.1.10.10.4.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">34.16±0.72</span></td>\n<td id=\"S5.T5.5.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T5.5.1.10.10.5.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">28.52±5.56</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Results under various datasets and models.",
                " In ",
                "Table 2",
                ", our methods can reach state-of-the-art results across four datasets under both IID (",
                "α",
                "=",
                "100",
                "𝛼",
                "100",
                "\\alpha=100",
                ") and heterogeneous (",
                "α",
                "=",
                "0.5",
                "𝛼",
                "0.5",
                "\\alpha=0.5",
                ") settings",
                "2",
                "2",
                "2",
                "It’s important to mention that certain methods might fail in specific settings, exhibiting accuracy levels close to random guessing, e.g., FedProx in Fashion-MNIST.",
                ".\nGenerally, FedGuCci can reach the best performances over current FL methods, and FedGuCci+ can strengthen FedGuCci in most cases. Also, the performance gains of our approaches are more dominant under more complicated datasets, like Tiny-ImageNet. While FedSAM stands as the most robust baseline for generalization, our connectivity loss not only yields better results but is also compatible with it (FedGuCci+).",
                "Results on different ",
                "M",
                "𝑀",
                "M",
                " and ",
                "ρ",
                "𝜌",
                "\\rho",
                ".",
                " We conduct experiments by varying the number of clients ",
                "M",
                "𝑀",
                "M",
                " and participation ratios of clients ",
                "ρ",
                "𝜌",
                "\\rho",
                " in ",
                "Table 4",
                ". It demonstrates that FedGuCci and FedGuCci+ can also excel when the number of clients is large and partial participation exists, indicating their great potential under cross-device settings ",
                "(Charles et al., ",
                "2021",
                ")",
                ".",
                "Results of different local epochs ",
                "E",
                "𝐸",
                "E",
                ".",
                " In ",
                "Figure 6",
                ", FedGuCci is consistently leading under different ",
                "E",
                "𝐸",
                "E",
                ", while FedGuCci+ is not robust on CIFAR-10. For CIFAR-100, FedGuCci has a more obvious advantage when ",
                "E",
                "𝐸",
                "E",
                " is large, and this is rationale since the connectivity and model drift issues are more severe under large local updates."
            ]
        ]
    },
    "S5.T6": {
        "caption": "Table 6: Ablation study of FedGuCci+. M=50𝑀50M=50, non-IID: 1.0. ",
        "table": "<table id=\"S5.T6.12\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.12.11.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.12.11.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S5.T6.12.11.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Methods/Datasets</span></th>\n<th id=\"S5.T6.12.11.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T6.12.11.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-10</span></th>\n<th id=\"S5.T6.12.11.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T6.12.11.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-100</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T6.4.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S5.T6.4.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S5.T6.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.14</span><math id=\"S5.T6.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.3.1.1.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.3.1.1.m1.1.1\" xref=\"S5.T6.3.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.3.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.3.1.1.m1.1.1.cmml\" xref=\"S5.T6.3.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.3.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.3.1.1.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.38</span>\n</td>\n<td id=\"S5.T6.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">20.81</span><math id=\"S5.T6.4.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.4.2.2.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.4.2.2.m1.1.1\" xref=\"S5.T6.4.2.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.4.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.4.2.2.m1.1.1.cmml\" xref=\"S5.T6.4.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.4.2.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.4.2.2.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.52</span>\n</td>\n</tr>\n<tr id=\"S5.T6.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T6.6.4.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T6.6.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedGuCci</span></th>\n<td id=\"S5.T6.5.3.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S5.T6.5.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.45</span><math id=\"S5.T6.5.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.5.3.1.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.5.3.1.m1.1.1\" xref=\"S5.T6.5.3.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.5.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.5.3.1.m1.1.1.cmml\" xref=\"S5.T6.5.3.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.5.3.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.5.3.1.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.19</span>\n</td>\n<td id=\"S5.T6.6.4.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S5.T6.6.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">22.74</span><math id=\"S5.T6.6.4.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.6.4.2.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.6.4.2.m1.1.1\" xref=\"S5.T6.6.4.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.6.4.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.6.4.2.m1.1.1.cmml\" xref=\"S5.T6.6.4.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.6.4.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.6.4.2.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.42</span>\n</td>\n</tr>\n<tr id=\"S5.T6.8.6\" class=\"ltx_tr\">\n<th id=\"S5.T6.8.6.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S5.T6.8.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedGuCci + only logit calibration</span></th>\n<td id=\"S5.T6.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.7.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.51</span><math id=\"S5.T6.7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.7.5.1.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.7.5.1.m1.1.1\" xref=\"S5.T6.7.5.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.7.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.7.5.1.m1.1.1.cmml\" xref=\"S5.T6.7.5.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.7.5.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.7.5.1.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.15</span>\n</td>\n<td id=\"S5.T6.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.8.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">22.32</span><math id=\"S5.T6.8.6.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.8.6.2.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.8.6.2.m1.1.1\" xref=\"S5.T6.8.6.2.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.8.6.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.8.6.2.m1.1.1.cmml\" xref=\"S5.T6.8.6.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.8.6.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.8.6.2.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.67</span>\n</td>\n</tr>\n<tr id=\"S5.T6.10.8\" class=\"ltx_tr\">\n<th id=\"S5.T6.10.8.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T6.10.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedGuCci + only SAM</span></th>\n<td id=\"S5.T6.9.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.9.7.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">65.93<math id=\"S5.T6.9.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.9.7.1.1.m1.1a\"><mo mathsize=\"63%\" id=\"S5.T6.9.7.1.1.m1.1.1\" xref=\"S5.T6.9.7.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.9.7.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.9.7.1.1.m1.1.1.cmml\" xref=\"S5.T6.9.7.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.9.7.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.9.7.1.1.1\" class=\"ltx_text\" style=\"font-size:63%;\">0.38</span></span></td>\n<td id=\"S5.T6.10.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.10.8.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">25.81<math id=\"S5.T6.10.8.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.10.8.2.1.m1.1a\"><mo mathsize=\"63%\" id=\"S5.T6.10.8.2.1.m1.1.1\" xref=\"S5.T6.10.8.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.10.8.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.10.8.2.1.m1.1.1.cmml\" xref=\"S5.T6.10.8.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.10.8.2.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.10.8.2.1.1\" class=\"ltx_text\" style=\"font-size:63%;\">1.02</span></span></td>\n</tr>\n<tr id=\"S5.T6.12.10\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S5.T6.12.10.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span id=\"S5.T6.12.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci+ (with both)</span></th>\n<td id=\"S5.T6.11.9.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T6.11.9.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">66.05<math id=\"S5.T6.11.9.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.11.9.1.1.m1.1a\"><mo mathbackground=\"#E6E6E6\" mathsize=\"63%\" id=\"S5.T6.11.9.1.1.m1.1.1\" xref=\"S5.T6.11.9.1.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.11.9.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.11.9.1.1.m1.1.1.cmml\" xref=\"S5.T6.11.9.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.11.9.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.11.9.1.1.1\" class=\"ltx_text\" style=\"font-size:63%;\">0.35</span></span></td>\n<td id=\"S5.T6.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T6.12.10.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">25.30<math id=\"S5.T6.12.10.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.12.10.2.1.m1.1a\"><mo mathbackground=\"#E6E6E6\" mathsize=\"63%\" id=\"S5.T6.12.10.2.1.m1.1.1\" xref=\"S5.T6.12.10.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.12.10.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.12.10.2.1.m1.1.1.cmml\" xref=\"S5.T6.12.10.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.12.10.2.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.12.10.2.1.1\" class=\"ltx_text\" style=\"font-size:63%;\">0.65</span></span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Results under various datasets and models.",
                " In ",
                "Table 2",
                ", our methods can reach state-of-the-art results across four datasets under both IID (",
                "α",
                "=",
                "100",
                "𝛼",
                "100",
                "\\alpha=100",
                ") and heterogeneous (",
                "α",
                "=",
                "0.5",
                "𝛼",
                "0.5",
                "\\alpha=0.5",
                ") settings",
                "2",
                "2",
                "2",
                "It’s important to mention that certain methods might fail in specific settings, exhibiting accuracy levels close to random guessing, e.g., FedProx in Fashion-MNIST.",
                ".\nGenerally, FedGuCci can reach the best performances over current FL methods, and FedGuCci+ can strengthen FedGuCci in most cases. Also, the performance gains of our approaches are more dominant under more complicated datasets, like Tiny-ImageNet. While FedSAM stands as the most robust baseline for generalization, our connectivity loss not only yields better results but is also compatible with it (FedGuCci+).",
                "Results on different ",
                "M",
                "𝑀",
                "M",
                " and ",
                "ρ",
                "𝜌",
                "\\rho",
                ".",
                " We conduct experiments by varying the number of clients ",
                "M",
                "𝑀",
                "M",
                " and participation ratios of clients ",
                "ρ",
                "𝜌",
                "\\rho",
                " in ",
                "Table 4",
                ". It demonstrates that FedGuCci and FedGuCci+ can also excel when the number of clients is large and partial participation exists, indicating their great potential under cross-device settings ",
                "(Charles et al., ",
                "2021",
                ")",
                ".",
                "Results of different local epochs ",
                "E",
                "𝐸",
                "E",
                ".",
                " In ",
                "Figure 6",
                ", FedGuCci is consistently leading under different ",
                "E",
                "𝐸",
                "E",
                ", while FedGuCci+ is not robust on CIFAR-10. For CIFAR-100, FedGuCci has a more obvious advantage when ",
                "E",
                "𝐸",
                "E",
                " is large, and this is rationale since the connectivity and model drift issues are more severe under large local updates."
            ]
        ]
    }
}