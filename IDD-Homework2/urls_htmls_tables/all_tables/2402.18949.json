{
    "PAPER'S NUMBER OF TABLES": 6,
    "S3.SS1.33": {
        "caption": "Table 1: Test accuracies and barriers of two trained models w/ and w/o connectivity loss. â€œInd. Acc.â€ refers to 0.5âˆ—ğ’œâ€‹(ğ°1)+0.5âˆ—ğ’œâ€‹(ğ°2)0.5ğ’œsubscriptğ°10.5ğ’œsubscriptğ°20.5*\\mathcal{A}(\\mathbf{w}_{1})+0.5*\\mathcal{A}(\\mathbf{w}_{2}), and â€œFused Acc.â€ refers to ğ’œâ€‹(0.5âˆ—ğ°1+0.5âˆ—ğ°2)ğ’œ0.5subscriptğ°10.5subscriptğ°2\\mathcal{A}(0.5*\\mathbf{w}_{1}+0.5*\\mathbf{w}_{2}). It validates the transitivity of LMC, stating that by leveraging the anchor model, the barriers of LMC are largely reduced. CIFAR-10.",
        "table": "<table id=\"S3.SS1.22.22.18\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.SS1.22.22.18.19.1\" class=\"ltx_tr\">\n<th id=\"S3.SS1.22.22.18.19.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.SS1.22.22.18.19.1.1.1\" class=\"ltx_text ltx_font_bold\">Models</span></th>\n<th id=\"S3.SS1.22.22.18.19.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.SS1.22.22.18.19.1.2.1\" class=\"ltx_text ltx_font_bold\">Metrics</span></th>\n<td id=\"S3.SS1.22.22.18.19.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\"><span id=\"S3.SS1.22.22.18.19.1.3.1\" class=\"ltx_text ltx_font_bold\">Vanilla CE Loss</span></td>\n<td id=\"S3.SS1.22.22.18.19.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.SS1.22.22.18.19.1.4.1\" class=\"ltx_text ltx_font_bold\">w/ Connectivity Loss</span></td>\n</tr>\n<tr id=\"S3.SS1.6.6.2.2\" class=\"ltx_tr\">\n<th id=\"S3.SS1.6.6.2.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S3.SS1.6.6.2.2.3.1\" class=\"ltx_text\">CNN</span></th>\n<th id=\"S3.SS1.6.6.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ind. Acc.</th>\n<td id=\"S3.SS1.5.5.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.5.5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"64.0\\pm 0.5\" display=\"inline\"><semantics id=\"S3.SS1.5.5.1.1.1.m1.1a\"><mrow id=\"S3.SS1.5.5.1.1.1.m1.1.1\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.cmml\"><mn id=\"S3.SS1.5.5.1.1.1.m1.1.1.2\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.2.cmml\">64.0</mn><mo id=\"S3.SS1.5.5.1.1.1.m1.1.1.1\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.5.5.1.1.1.m1.1.1.3\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.5.5.1.1.1.m1.1b\"><apply id=\"S3.SS1.5.5.1.1.1.m1.1.1.cmml\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.5.5.1.1.1.m1.1.1.1.cmml\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.5.5.1.1.1.m1.1.1.2.cmml\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.2\">64.0</cn><cn type=\"float\" id=\"S3.SS1.5.5.1.1.1.m1.1.1.3.cmml\" xref=\"S3.SS1.5.5.1.1.1.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.5.5.1.1.1.m1.1c\">64.0\\pm 0.5</annotation></semantics></math></td>\n<td id=\"S3.SS1.6.6.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.6.6.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"63.9\\pm 1.4\" display=\"inline\"><semantics id=\"S3.SS1.6.6.2.2.2.m1.1a\"><mrow id=\"S3.SS1.6.6.2.2.2.m1.1.1\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.cmml\"><mn id=\"S3.SS1.6.6.2.2.2.m1.1.1.2\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.2.cmml\">63.9</mn><mo id=\"S3.SS1.6.6.2.2.2.m1.1.1.1\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.6.6.2.2.2.m1.1.1.3\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.3.cmml\">1.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.6.6.2.2.2.m1.1b\"><apply id=\"S3.SS1.6.6.2.2.2.m1.1.1.cmml\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.6.6.2.2.2.m1.1.1.1.cmml\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.6.6.2.2.2.m1.1.1.2.cmml\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.2\">63.9</cn><cn type=\"float\" id=\"S3.SS1.6.6.2.2.2.m1.1.1.3.cmml\" xref=\"S3.SS1.6.6.2.2.2.m1.1.1.3\">1.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.6.6.2.2.2.m1.1c\">63.9\\pm 1.4</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.8.8.4.4\" class=\"ltx_tr\">\n<th id=\"S3.SS1.8.8.4.4.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fused Acc.</th>\n<td id=\"S3.SS1.7.7.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><math id=\"S3.SS1.7.7.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"11.5\\pm 0.9\" display=\"inline\"><semantics id=\"S3.SS1.7.7.3.3.1.m1.1a\"><mrow id=\"S3.SS1.7.7.3.3.1.m1.1.1\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.cmml\"><mn id=\"S3.SS1.7.7.3.3.1.m1.1.1.2\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.2.cmml\">11.5</mn><mo id=\"S3.SS1.7.7.3.3.1.m1.1.1.1\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.7.7.3.3.1.m1.1.1.3\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.3.cmml\">0.9</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.7.7.3.3.1.m1.1b\"><apply id=\"S3.SS1.7.7.3.3.1.m1.1.1.cmml\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.7.7.3.3.1.m1.1.1.1.cmml\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.7.7.3.3.1.m1.1.1.2.cmml\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.2\">11.5</cn><cn type=\"float\" id=\"S3.SS1.7.7.3.3.1.m1.1.1.3.cmml\" xref=\"S3.SS1.7.7.3.3.1.m1.1.1.3\">0.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.7.7.3.3.1.m1.1c\">11.5\\pm 0.9</annotation></semantics></math></td>\n<td id=\"S3.SS1.8.8.4.4.2\" class=\"ltx_td ltx_align_center\"><math id=\"S3.SS1.8.8.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"32.1\\pm 9.0\" display=\"inline\"><semantics id=\"S3.SS1.8.8.4.4.2.m1.1a\"><mrow id=\"S3.SS1.8.8.4.4.2.m1.1.1\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.cmml\"><mn id=\"S3.SS1.8.8.4.4.2.m1.1.1.2\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.2.cmml\">32.1</mn><mo id=\"S3.SS1.8.8.4.4.2.m1.1.1.1\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.8.8.4.4.2.m1.1.1.3\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.3.cmml\">9.0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.8.8.4.4.2.m1.1b\"><apply id=\"S3.SS1.8.8.4.4.2.m1.1.1.cmml\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.8.8.4.4.2.m1.1.1.1.cmml\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.8.8.4.4.2.m1.1.1.2.cmml\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.2\">32.1</cn><cn type=\"float\" id=\"S3.SS1.8.8.4.4.2.m1.1.1.3.cmml\" xref=\"S3.SS1.8.8.4.4.2.m1.1.1.3\">9.0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.8.8.4.4.2.m1.1c\">32.1\\pm 9.0</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.10.10.6.6\" class=\"ltx_tr\">\n<th id=\"S3.SS1.10.10.6.6.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Acc. Barrier</th>\n<td id=\"S3.SS1.9.9.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.9.9.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"0.821\" display=\"inline\"><semantics id=\"S3.SS1.9.9.5.5.1.m1.1a\"><mn id=\"S3.SS1.9.9.5.5.1.m1.1.1\" xref=\"S3.SS1.9.9.5.5.1.m1.1.1.cmml\">0.821</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.9.9.5.5.1.m1.1b\"><cn type=\"float\" id=\"S3.SS1.9.9.5.5.1.m1.1.1.cmml\" xref=\"S3.SS1.9.9.5.5.1.m1.1.1\">0.821</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.9.9.5.5.1.m1.1c\">0.821</annotation></semantics></math></td>\n<td id=\"S3.SS1.10.10.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.10.10.6.6.2.m1.1\" class=\"ltx_math_unparsed\" alttext=\"0.495\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(39.7\\%\\downarrow)}\" display=\"inline\"><semantics id=\"S3.SS1.10.10.6.6.2.m1.1a\"><mrow id=\"S3.SS1.10.10.6.6.2.m1.1b\"><mn id=\"S3.SS1.10.10.6.6.2.m1.1.1\">0.495</mn><mrow id=\"S3.SS1.10.10.6.6.2.m1.1.2\"><mo lspace=\"0.170em\" mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.1\">(</mo><mn mathcolor=\"#33B333\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.2\">39.7</mn><mo mathcolor=\"#33B333\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.3\">%</mo><mo mathcolor=\"#33B333\" rspace=\"0em\" stretchy=\"false\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.4\">â†“</mo><mo mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.10.10.6.6.2.m1.1.2.5\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S3.SS1.10.10.6.6.2.m1.1c\">0.495\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(39.7\\%\\downarrow)}</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.12.12.8.8\" class=\"ltx_tr\">\n<th id=\"S3.SS1.12.12.8.8.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S3.SS1.12.12.8.8.3.1\" class=\"ltx_text\">ResNet 20</span></th>\n<th id=\"S3.SS1.12.12.8.8.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ind. Acc.</th>\n<td id=\"S3.SS1.11.11.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.11.11.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"66.7\\pm 0.9\" display=\"inline\"><semantics id=\"S3.SS1.11.11.7.7.1.m1.1a\"><mrow id=\"S3.SS1.11.11.7.7.1.m1.1.1\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.cmml\"><mn id=\"S3.SS1.11.11.7.7.1.m1.1.1.2\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.2.cmml\">66.7</mn><mo id=\"S3.SS1.11.11.7.7.1.m1.1.1.1\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.11.11.7.7.1.m1.1.1.3\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.3.cmml\">0.9</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.11.11.7.7.1.m1.1b\"><apply id=\"S3.SS1.11.11.7.7.1.m1.1.1.cmml\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.11.11.7.7.1.m1.1.1.1.cmml\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.11.11.7.7.1.m1.1.1.2.cmml\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.2\">66.7</cn><cn type=\"float\" id=\"S3.SS1.11.11.7.7.1.m1.1.1.3.cmml\" xref=\"S3.SS1.11.11.7.7.1.m1.1.1.3\">0.9</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.11.11.7.7.1.m1.1c\">66.7\\pm 0.9</annotation></semantics></math></td>\n<td id=\"S3.SS1.12.12.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.12.12.8.8.2.m1.1\" class=\"ltx_Math\" alttext=\"69.1\\pm 2.4\" display=\"inline\"><semantics id=\"S3.SS1.12.12.8.8.2.m1.1a\"><mrow id=\"S3.SS1.12.12.8.8.2.m1.1.1\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.cmml\"><mn id=\"S3.SS1.12.12.8.8.2.m1.1.1.2\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.2.cmml\">69.1</mn><mo id=\"S3.SS1.12.12.8.8.2.m1.1.1.1\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.12.12.8.8.2.m1.1.1.3\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.3.cmml\">2.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.12.12.8.8.2.m1.1b\"><apply id=\"S3.SS1.12.12.8.8.2.m1.1.1.cmml\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.12.12.8.8.2.m1.1.1.1.cmml\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.12.12.8.8.2.m1.1.1.2.cmml\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.2\">69.1</cn><cn type=\"float\" id=\"S3.SS1.12.12.8.8.2.m1.1.1.3.cmml\" xref=\"S3.SS1.12.12.8.8.2.m1.1.1.3\">2.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.12.12.8.8.2.m1.1c\">69.1\\pm 2.4</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.14.14.10.10\" class=\"ltx_tr\">\n<th id=\"S3.SS1.14.14.10.10.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fused Acc.</th>\n<td id=\"S3.SS1.13.13.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><math id=\"S3.SS1.13.13.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"13.0\\pm 3.8\" display=\"inline\"><semantics id=\"S3.SS1.13.13.9.9.1.m1.1a\"><mrow id=\"S3.SS1.13.13.9.9.1.m1.1.1\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.cmml\"><mn id=\"S3.SS1.13.13.9.9.1.m1.1.1.2\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.2.cmml\">13.0</mn><mo id=\"S3.SS1.13.13.9.9.1.m1.1.1.1\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.13.13.9.9.1.m1.1.1.3\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.3.cmml\">3.8</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.13.13.9.9.1.m1.1b\"><apply id=\"S3.SS1.13.13.9.9.1.m1.1.1.cmml\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.13.13.9.9.1.m1.1.1.1.cmml\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.13.13.9.9.1.m1.1.1.2.cmml\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.2\">13.0</cn><cn type=\"float\" id=\"S3.SS1.13.13.9.9.1.m1.1.1.3.cmml\" xref=\"S3.SS1.13.13.9.9.1.m1.1.1.3\">3.8</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.13.13.9.9.1.m1.1c\">13.0\\pm 3.8</annotation></semantics></math></td>\n<td id=\"S3.SS1.14.14.10.10.2\" class=\"ltx_td ltx_align_center\"><math id=\"S3.SS1.14.14.10.10.2.m1.1\" class=\"ltx_Math\" alttext=\"40.5\\pm 3.5\" display=\"inline\"><semantics id=\"S3.SS1.14.14.10.10.2.m1.1a\"><mrow id=\"S3.SS1.14.14.10.10.2.m1.1.1\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.cmml\"><mn id=\"S3.SS1.14.14.10.10.2.m1.1.1.2\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.2.cmml\">40.5</mn><mo id=\"S3.SS1.14.14.10.10.2.m1.1.1.1\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.14.14.10.10.2.m1.1.1.3\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.3.cmml\">3.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.14.14.10.10.2.m1.1b\"><apply id=\"S3.SS1.14.14.10.10.2.m1.1.1.cmml\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.14.14.10.10.2.m1.1.1.1.cmml\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.14.14.10.10.2.m1.1.1.2.cmml\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.2\">40.5</cn><cn type=\"float\" id=\"S3.SS1.14.14.10.10.2.m1.1.1.3.cmml\" xref=\"S3.SS1.14.14.10.10.2.m1.1.1.3\">3.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.14.14.10.10.2.m1.1c\">40.5\\pm 3.5</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.16.16.12.12\" class=\"ltx_tr\">\n<th id=\"S3.SS1.16.16.12.12.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Acc. Barrier</th>\n<td id=\"S3.SS1.15.15.11.11.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.15.15.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"0.805\" display=\"inline\"><semantics id=\"S3.SS1.15.15.11.11.1.m1.1a\"><mn id=\"S3.SS1.15.15.11.11.1.m1.1.1\" xref=\"S3.SS1.15.15.11.11.1.m1.1.1.cmml\">0.805</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.15.15.11.11.1.m1.1b\"><cn type=\"float\" id=\"S3.SS1.15.15.11.11.1.m1.1.1.cmml\" xref=\"S3.SS1.15.15.11.11.1.m1.1.1\">0.805</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.15.15.11.11.1.m1.1c\">0.805</annotation></semantics></math></td>\n<td id=\"S3.SS1.16.16.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.16.16.12.12.2.m1.1\" class=\"ltx_math_unparsed\" alttext=\"0.415\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(44.1\\%\\downarrow)}\" display=\"inline\"><semantics id=\"S3.SS1.16.16.12.12.2.m1.1a\"><mrow id=\"S3.SS1.16.16.12.12.2.m1.1b\"><mn id=\"S3.SS1.16.16.12.12.2.m1.1.1\">0.415</mn><mrow id=\"S3.SS1.16.16.12.12.2.m1.1.2\"><mo lspace=\"0.170em\" mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.1\">(</mo><mn mathcolor=\"#33B333\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.2\">44.1</mn><mo mathcolor=\"#33B333\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.3\">%</mo><mo mathcolor=\"#33B333\" rspace=\"0em\" stretchy=\"false\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.4\">â†“</mo><mo mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.16.16.12.12.2.m1.1.2.5\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S3.SS1.16.16.12.12.2.m1.1c\">0.415\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(44.1\\%\\downarrow)}</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.18.18.14.14\" class=\"ltx_tr\">\n<th id=\"S3.SS1.18.18.14.14.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" rowspan=\"4\">\n<span id=\"S3.SS1.18.18.14.14.3.1\" class=\"ltx_text\">Pretrained </span>\nResNet18</th>\n<th id=\"S3.SS1.18.18.14.14.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">Ind. Acc.</th>\n<td id=\"S3.SS1.17.17.13.13.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S3.SS1.17.17.13.13.1.m1.1\" class=\"ltx_Math\" alttext=\"55.8\\pm 6.6\" display=\"inline\"><semantics id=\"S3.SS1.17.17.13.13.1.m1.1a\"><mrow id=\"S3.SS1.17.17.13.13.1.m1.1.1\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.cmml\"><mn id=\"S3.SS1.17.17.13.13.1.m1.1.1.2\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.2.cmml\">55.8</mn><mo id=\"S3.SS1.17.17.13.13.1.m1.1.1.1\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.17.17.13.13.1.m1.1.1.3\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.3.cmml\">6.6</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.17.17.13.13.1.m1.1b\"><apply id=\"S3.SS1.17.17.13.13.1.m1.1.1.cmml\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.17.17.13.13.1.m1.1.1.1.cmml\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.17.17.13.13.1.m1.1.1.2.cmml\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.2\">55.8</cn><cn type=\"float\" id=\"S3.SS1.17.17.13.13.1.m1.1.1.3.cmml\" xref=\"S3.SS1.17.17.13.13.1.m1.1.1.3\">6.6</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.17.17.13.13.1.m1.1c\">55.8\\pm 6.6</annotation></semantics></math></td>\n<td id=\"S3.SS1.18.18.14.14.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><math id=\"S3.SS1.18.18.14.14.2.m1.1\" class=\"ltx_Math\" alttext=\"64.5\\pm 0.3\" display=\"inline\"><semantics id=\"S3.SS1.18.18.14.14.2.m1.1a\"><mrow id=\"S3.SS1.18.18.14.14.2.m1.1.1\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.cmml\"><mn id=\"S3.SS1.18.18.14.14.2.m1.1.1.2\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.2.cmml\">64.5</mn><mo id=\"S3.SS1.18.18.14.14.2.m1.1.1.1\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.18.18.14.14.2.m1.1.1.3\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.3.cmml\">0.3</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.18.18.14.14.2.m1.1b\"><apply id=\"S3.SS1.18.18.14.14.2.m1.1.1.cmml\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.18.18.14.14.2.m1.1.1.1.cmml\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.18.18.14.14.2.m1.1.1.2.cmml\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.2\">64.5</cn><cn type=\"float\" id=\"S3.SS1.18.18.14.14.2.m1.1.1.3.cmml\" xref=\"S3.SS1.18.18.14.14.2.m1.1.1.3\">0.3</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.18.18.14.14.2.m1.1c\">64.5\\pm 0.3</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.20.20.16.16\" class=\"ltx_tr\">\n<th id=\"S3.SS1.20.20.16.16.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">Fused Acc.</th>\n<td id=\"S3.SS1.19.19.15.15.1\" class=\"ltx_td ltx_align_center ltx_border_r\"><math id=\"S3.SS1.19.19.15.15.1.m1.1\" class=\"ltx_Math\" alttext=\"10.0\\pm 0.0\" display=\"inline\"><semantics id=\"S3.SS1.19.19.15.15.1.m1.1a\"><mrow id=\"S3.SS1.19.19.15.15.1.m1.1.1\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.cmml\"><mn id=\"S3.SS1.19.19.15.15.1.m1.1.1.2\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.2.cmml\">10.0</mn><mo id=\"S3.SS1.19.19.15.15.1.m1.1.1.1\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.19.19.15.15.1.m1.1.1.3\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.3.cmml\">0.0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.19.19.15.15.1.m1.1b\"><apply id=\"S3.SS1.19.19.15.15.1.m1.1.1.cmml\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.19.19.15.15.1.m1.1.1.1.cmml\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.19.19.15.15.1.m1.1.1.2.cmml\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.2\">10.0</cn><cn type=\"float\" id=\"S3.SS1.19.19.15.15.1.m1.1.1.3.cmml\" xref=\"S3.SS1.19.19.15.15.1.m1.1.1.3\">0.0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.19.19.15.15.1.m1.1c\">10.0\\pm 0.0</annotation></semantics></math></td>\n<td id=\"S3.SS1.20.20.16.16.2\" class=\"ltx_td ltx_align_center\"><math id=\"S3.SS1.20.20.16.16.2.m1.1\" class=\"ltx_Math\" alttext=\"62.1\\pm 0.4\" display=\"inline\"><semantics id=\"S3.SS1.20.20.16.16.2.m1.1a\"><mrow id=\"S3.SS1.20.20.16.16.2.m1.1.1\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.cmml\"><mn id=\"S3.SS1.20.20.16.16.2.m1.1.1.2\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.2.cmml\">62.1</mn><mo id=\"S3.SS1.20.20.16.16.2.m1.1.1.1\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.1.cmml\">Â±</mo><mn id=\"S3.SS1.20.20.16.16.2.m1.1.1.3\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.3.cmml\">0.4</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.20.20.16.16.2.m1.1b\"><apply id=\"S3.SS1.20.20.16.16.2.m1.1.1.cmml\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1\"><csymbol cd=\"latexml\" id=\"S3.SS1.20.20.16.16.2.m1.1.1.1.cmml\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"S3.SS1.20.20.16.16.2.m1.1.1.2.cmml\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.2\">62.1</cn><cn type=\"float\" id=\"S3.SS1.20.20.16.16.2.m1.1.1.3.cmml\" xref=\"S3.SS1.20.20.16.16.2.m1.1.1.3\">0.4</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.20.20.16.16.2.m1.1c\">62.1\\pm 0.4</annotation></semantics></math></td>\n</tr>\n<tr id=\"S3.SS1.22.22.18.18\" class=\"ltx_tr\">\n<th id=\"S3.SS1.22.22.18.18.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\">Acc. Barrier</th>\n<td id=\"S3.SS1.21.21.17.17.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><math id=\"S3.SS1.21.21.17.17.1.m1.1\" class=\"ltx_Math\" alttext=\"0.819\" display=\"inline\"><semantics id=\"S3.SS1.21.21.17.17.1.m1.1a\"><mn id=\"S3.SS1.21.21.17.17.1.m1.1.1\" xref=\"S3.SS1.21.21.17.17.1.m1.1.1.cmml\">0.819</mn><annotation-xml encoding=\"MathML-Content\" id=\"S3.SS1.21.21.17.17.1.m1.1b\"><cn type=\"float\" id=\"S3.SS1.21.21.17.17.1.m1.1.1.cmml\" xref=\"S3.SS1.21.21.17.17.1.m1.1.1\">0.819</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.SS1.21.21.17.17.1.m1.1c\">0.819</annotation></semantics></math></td>\n<td id=\"S3.SS1.22.22.18.18.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><math id=\"S3.SS1.22.22.18.18.2.m1.1\" class=\"ltx_math_unparsed\" alttext=\"0.038\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(95.4\\%\\downarrow)}\" display=\"inline\"><semantics id=\"S3.SS1.22.22.18.18.2.m1.1a\"><mrow id=\"S3.SS1.22.22.18.18.2.m1.1b\"><mn id=\"S3.SS1.22.22.18.18.2.m1.1.1\">0.038</mn><mrow id=\"S3.SS1.22.22.18.18.2.m1.1.2\"><mo lspace=\"0.170em\" mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.1\">(</mo><mn mathcolor=\"#33B333\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.2\">95.4</mn><mo mathcolor=\"#33B333\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.3\">%</mo><mo mathcolor=\"#33B333\" rspace=\"0em\" stretchy=\"false\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.4\">â†“</mo><mo mathcolor=\"#33B333\" stretchy=\"false\" id=\"S3.SS1.22.22.18.18.2.m1.1.2.5\">)</mo></mrow></mrow><annotation encoding=\"application/x-tex\" id=\"S3.SS1.22.22.18.18.2.m1.1c\">0.038\\,{\\color[rgb]{0.2,0.7,0.2}\\definecolor[named]{pgfstrokecolor}{rgb}{0.2,0.7,0.2}(95.4\\%\\downarrow)}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "We first present the connectivity loss given the anchor model, which is similar to previous literatureÂ (Wortsman etÂ al., 2021; Garipov etÂ al., 2018). The connectivity loss is as follows,where ğ°ancâˆ—superscriptsubscriptğ°anc\\mathbf{w}_{\\text{anc}}^{*} is the fixed anchor model and ğ°ğ°\\mathbf{w} is the model for training. Then, we incorporate the connectivity loss into the vanilla cross entropy (CE) loss, formulated into the following overall learning objective,where â„’â€‹(ğ°)â„’ğ°\\mathcal{L}(\\mathbf{w}) is the vanilla CE loss and Î²ğ›½\\beta is the hyperparameter controlling the strength of the connectivity loss.We let ğ°ancâˆ—superscriptsubscriptğ°anc\\mathbf{w}_{\\text{anc}}^{*} be the fixed trained anchor model and independently train two models ğ°1âˆ—superscriptsubscriptğ°1\\mathbf{w}_{1}^{*} and ğ°2âˆ—superscriptsubscriptğ°2\\mathbf{w}_{2}^{*} according to EquationÂ 8. According to TheoremÂ 3.5, the ğ°1âˆ—superscriptsubscriptğ°1\\mathbf{w}_{1}^{*} and ğ°2âˆ—superscriptsubscriptğ°2\\mathbf{w}_{2}^{*}â€™s LMC barriers will be reduced if the transitivity holds. Note that ğ°1subscriptğ°1\\mathbf{w}_{1} and ğ°2subscriptğ°2\\mathbf{w}_{2} can have the same or different initializations, and the transitivity still holds; in the experiments, we make stricter verifications by setting different initializations.Empirical results. We conduct experiments in subsectionÂ 3.1 and FigureÂ 2. The anchor model is a mode independently trained with vanilla CE loss using a different random seed. In subsectionÂ 3.1, training with the connectivity loss can largely reduce the barriers of LMC by utilizing the anchor model, even if two models have different initializations and never communicate with each other. More intuitive landscape visualizations are in FigureÂ 2. It can be seen that the connectivity loss can eliminate the barrier between the anchor model and the trained model, and due to the transitivity of LMC, the barrier between the two independent models is also reduced. The experiments verify the transitivity of LMC between two models, and we will show that this transitivity can be extended to the connectivity of multiple models.We study the group connectivity among multiple models and propose the barrier of group connectivity akin to DefinitionÂ 2.1 of LMC. For brevity, we only present the definition of accuracy barriers.Group connectivity. The group connectivity of model set {ğ°i}i=1Ksuperscriptsubscriptsubscriptğ°ğ‘–ğ‘–1ğ¾\\{\\mathbf{w}_{i}\\}_{i=1}^{K} is depicted by the loss and accuracy barrier defined as:where â„’â„’\\mathcal{L} is the loss and ğ’œğ’œ\\mathcal{A} is the accuracy function. A lower barrier refers to better group connectivity.We prove the transitivity of group connectivity that individually training several models and improving the LMC between one common anchor model will result in better group connectivity among the trained ones. In addition, we consider the data heterogeneity of practical FL in group connectivity by giving the following definition.Data heterogeneity. Similar to Li etÂ al. (2019), we use the minimum to measure the degree of heterogeneity among the group of individual workers (e.g., clients in FL and modes in LMC). Let ğ°âˆ—superscriptğ°\\mathbf{w}^{*} be a global minimum of all workers and ğ°iâˆ—superscriptsubscriptğ°ğ‘–\\mathbf{w}_{i}^{*} is the minimum value of worker iğ‘–i closest to ğ°âˆ—superscriptğ°\\mathbf{w}^{*}. We use the term Î“=maxiâ¡â€–ğ°iâˆ—âˆ’ğ°âˆ—â€–2,iâˆˆ[K]formulae-sequenceÎ“subscriptğ‘–subscriptnormsuperscriptsubscriptğ°ğ‘–superscriptğ°2ğ‘–delimited-[]ğ¾\\Gamma=\\max_{i}\\|\\mathbf{w}_{i}^{*}-\\mathbf{w}^{*}\\|_{2},\\,i\\in[K] for quantifying the degree of data heterogeneity.We define a two-layer neural network with ReLU activation, and the function is fğ¯,ğ”â€‹(ğ±)=ğ¯âŠ¤â€‹Ïƒâ€‹(ğ”ğ±)subscriptğ‘“ğ¯ğ”ğ±superscriptğ¯topğœğ”ğ±f_{\\mathbf{v},\\mathbf{U}}(\\mathbf{x})=\\mathbf{v}^{\\top}\\sigma(\\mathbf{U}\\mathbf{x}) where Ïƒâ€‹(â‹…)ğœâ‹…\\sigma(\\cdot) is the ReLU activation function. ğ¯âˆˆâ„hğ¯superscriptâ„â„\\mathbf{v}\\in\\mathbb{R}^{h} and ğ”âˆˆâ„hÃ—lğ”superscriptâ„â„ğ‘™\\mathbf{U}\\in\\mathbb{R}^{h\\times l} are parameters and ğ±âˆˆâ„lğ±superscriptâ„ğ‘™\\mathbf{x}\\in\\mathbb{R}^{l} is the input which is taken from ğ•={ğ±âˆˆâ„l|â€–ğ±â€–2<b}ğ•conditional-setğ±superscriptâ„ğ‘™subscriptnormğ±2ğ‘\\mathbb{X}=\\{\\mathbf{x}\\in\\mathbb{R}^{l}|\\|\\mathbf{x}\\|_{2}<b\\} uniformly. Denote the deterministic anchor model as ğ°ancâˆ—={ğ”ancâˆ—,ğ¯ancâˆ—}superscriptsubscriptğ°ancsuperscriptsubscriptğ”ancsuperscriptsubscriptğ¯anc\\mathbf{w}_{\\text{anc}}^{*}=\\{\\mathbf{U}_{\\text{anc}}^{*},\\mathbf{v}_{\\text{anc}}^{*}\\}, with â€–ğ¯ancâˆ—â€–2<dancsubscriptnormsuperscriptsubscriptğ¯anc2subscriptğ‘‘anc\\|\\mathbf{v}_{\\text{anc}}^{*}\\|_{2}<d_{\\text{anc}} and consider Kğ¾K different networks ğ°isubscriptğ°ğ‘–\\mathbf{w}_{i} parameterized with {ğ”i,ğ¯i}subscriptğ”ğ‘–subscriptğ¯ğ‘–\\{\\mathbf{U}_{i},\\mathbf{v}_{i}\\} located on Kğ¾K clients respectively. Each element of ğ”isubscriptğ”ğ‘–\\mathbf{U}_{i} and ğ¯isubscriptğ¯ğ‘–\\mathbf{v}_{i} is sampled from a uniform distribution centered at ğ”ancâˆ—superscriptsubscriptğ”anc\\mathbf{U}_{\\text{anc}}^{*} and ğ¯ancâˆ—superscriptsubscriptğ¯anc\\mathbf{v}_{\\text{anc}}^{*} with an interval length of dğ‘‘d. If with probability 1âˆ’Î´1ğ›¿1-\\delta, supÎ±â„’iâ€‹(Î±â€‹ğ°ancâˆ—+(1âˆ’Î±)â€‹ğ°i)<Ïµsubscriptsupremumğ›¼subscriptâ„’ğ‘–ğ›¼superscriptsubscriptğ°anc1ğ›¼subscriptğ°ğ‘–italic-Ïµ\\sup_{\\alpha}\\mathcal{L}_{i}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{i})<\\epsilon, then with probability 1âˆ’Î´1ğ›¿1-\\delta, it has,Landscape visualization. We empirically study whether the transitivity of LMC can be generalized to group connectivity of multiple models.\nWe let ğ°ancâˆ—superscriptsubscriptğ°anc\\mathbf{w}_{\\text{anc}}^{*} be the anchor model and independently train three models ğ°1âˆ—,ğ°2âˆ—,ğ°3âˆ—superscriptsubscriptğ°1superscriptsubscriptğ°2superscriptsubscriptğ°3\\mathbf{w}_{1}^{*},\\mathbf{w}_{2}^{*},\\mathbf{w}_{3}^{*} according to EquationÂ 8. Also, training the three models without connectivity loss is conducted for comparison. Then, we visualize the loss landscapes of ğ°1âˆ—,ğ°2âˆ—,ğ°3âˆ—superscriptsubscriptğ°1superscriptsubscriptğ°2superscriptsubscriptğ°3\\mathbf{w}_{1}^{*},\\mathbf{w}_{2}^{*},\\mathbf{w}_{3}^{*} in FigureÂ 3. For vanilla CE loss, the trained models are scattered in different loss basins with high barriers between them. However, with the connectivity loss, the LMC between each model and the anchor model is improved, and as a result of transitivity, the three models fall into a more connected low-loss region, and the barriers are largely eliminated.Group connectivity when vary Kğ¾K. We study the transitivity of group connectivity by scaling up the number of trained models Kğ¾K, which is critical for federated learning with numerous clients. The results are in FigureÂ 4; note that the number of anchor models is still one. We observe that by increasing Kğ¾K for the connectivity loss, the barrier in group connectivity will go up but still lower than the vanilla training. Also, the increase of barriers may converge to a point lower than vanilla training. It indicates that the transitivity of group connectivity may be weakened for larger Kğ¾K but still effective, and when Kğ¾K is relatively large (e.g., >8), increasing Kğ¾K will cause little loss of group connectivity. Furthermore, we will show in TableÂ 4 that our FedGuCci, which incorporates the connectivity loss, can improve the generalization under different large numbers of clients.In sectionÂ 3, we have verified the transitivity of group connectivity by using an anchor model. In this section, we will present FedGuCci, incorporating this property in FL to improve generalization.Global models as anchor models. We refer to subsectionÂ 2.1 for the settings and notations. In our FedGuCci, we use the global models as the anchor models for connectivity loss with local clients. Instead of solely using the current round global model as the anchor, we find using several previous roundsâ€™ global models can form the clients into a more connected region, so we use Nğ‘N previous global models as the anchors. Specifically, in round tâˆˆ[T]ğ‘¡delimited-[]ğ‘‡t\\in[T], the set of anchor models ğ–ancâˆ—tsuperscriptsubscriptğ–superscriptancğ‘¡\\mathbf{W}_{\\text{anc}^{*}}^{t} is as follows:where ğ°gjsuperscriptsubscriptğ°ğ‘”ğ‘—\\mathbf{w}_{g}^{j} refers to the global model at round jğ‘—j.FedGuCci local updates. FedGuCci is a client-side algorithm that utilizes the global models as the anchor and improves the group connectivity of clients, without additional communication overhead. FedGuCci has the following update rules. In each round tğ‘¡t, client iâˆˆ[M]ğ‘–delimited-[]ğ‘€i\\in[M] conducts local training according to the following objective:where ğ–ancâˆ—,jtsuperscriptsubscriptğ–superscriptancğ‘—ğ‘¡\\mathbf{W}_{\\text{anc}^{*},j}^{t} refers to the jğ‘—j-th model in the anchor model set, Î²ğ›½\\beta is the hyperparameter for connectivity loss, â„’isubscriptâ„’ğ‘–\\mathcal{L}_{i} is the clientâ€™s local CE loss, and â„’connectisubscriptâ„’subscriptconnectğ‘–\\mathcal{L}_{\\text{connect}_{i}} is the connectivity loss regarding subsectionÂ 3.1. Clients conduct SGD as EquationÂ 1 to update the local models.By learning to connect with the global anchor models, FedGuCci will improve the group connectivity and achieve better generalization as we will elaborate in sectionÂ 5.In the study of LMC, different modes are trained on the same dataset but with different random seeds or initializationsÂ (Entezari etÂ al., 2022). However, in FL, clients have heterogeneous data, and it is found that data heterogeneity of clients will cause different curvatures of local loss landscapesÂ (Zhou etÂ al., 2023), making the connectivity worse. Therefore, aligning local loss landscapes is essential for better performances of the connectivity loss. In this subsection, we incorporate previous techniques in FedGuCci to align local loss landscapes and propose FedGuCci+.Bias reduction. In FL, class imbalance (a.k.a. label skew) is a main cause of data heterogeneity, and previous works propose logit calibrationÂ (Zhang etÂ al., 2022), balanced softmaxÂ (Chen & Chao, 2022), and other techniquesÂ (Li etÂ al., 2023b; Acar etÂ al., 2020) for reducing the bias caused by class imbalance. Here, we introduce the logit calibration technique used in FedLCÂ (Zhang etÂ al., 2022) for bias reduction. The main idea of logit calibration is to add additional terms to the logits to balance the overall class distributions. From FigureÂ 5Â (b), it demonstrates that logit calibration and other bias reduction methods can align the landscapes by making the local objectives more consistent.Flatter minima. Sharpness-aware minimizationÂ (Foret etÂ al., 2021; Kwon etÂ al., 2021) (SAM) find flatter minima to improve generalization. SAM has also been introduced in FL for better generalizationÂ (Caldarola etÂ al., 2022; Qu etÂ al., 2022). In our paper, we find SAM can be used to align local loss landscapes by making the landscapes flatter, so we also incorporate it in FedGuCci+. From FigureÂ 5Â (c), it can be seen that if the landscapes are flatter, the overlap regions between two clients will increase, therefore, it will have more aligned landscapes. Also, for FedGuCci, SAM makes the connectivity loss to learn a cylinder connected with the anchor model instead of a lineÂ (Wen etÂ al., 2023), improving connectivity robustness and generalization.FedGuCci+ incorporates logit calibration and SAM into FedGuCci, achieving better generalization. We note that FedGuCci+ is a showcase of how FedGucCci is compatible with other existing techniques for better results, and more techniques can be integrated.In this section, we conduct extensive experiments to validate how FedGuCci and FedGuCci+ improve the generalization of FL under various settings and datasets.Datasets and models. Following previous works (Li etÂ al., 2023b; Lin etÂ al., 2020; Li etÂ al., 2023a), we use 4 vision datasets to conduct experiments: Fashion-MNISTÂ (Xiao etÂ al., 2017), CIFAR-10 (Krizhevsky etÂ al., 2009), CIFAR-100 (Krizhevsky etÂ al., 2009), and Tiny-ImageNet (Le & Yang, 2015). Tiny-ImageNet is a subset of ImageNetÂ (Deng etÂ al., 2009) with 100k samples of 200 classes. We use different models for the datasets as follows: {Fashion-MNIST: VGG11Â (Simonyan & Zisserman, 2015), CIFAR-10: SimpleCNNÂ (Li etÂ al., 2023a), CIFAR-100: ResNet20Â (Li etÂ al., 2018; He etÂ al., 2016), Tiny-ImageNet: ResNet18Â (He etÂ al., 2016)}. We also conduct experiments of pretrained language models on 6 datasets are from GLUEÂ (Wang etÂ al., 2019), and the model is RoBERTa-baseÂ (Liu etÂ al., 2019). For the detailed settings, please refer to AppendixÂ A.Compared methods. We take the most relevant and the most state-of-the-art FL algorithms as the baselines. (1)Â FedAvg (McMahan etÂ al., 2017) with vanilla local training, a simple but strong baseline; (2)Â FedProx (Li etÂ al., 2020a), which uses the current roundâ€™s global model as local regularization term; (3)Â FedDyn (Acar etÂ al., 2020), FL based on dynamic regularization; (4)Â SCAFFOLDÂ (Karimireddy etÂ al., 2020), using control variates for variance reduction; (5)Â MOONÂ (Li etÂ al., 2021) with model-contrastive learning; (6)Â FedRoD (Chen & Chao, 2022), generalization through decoupling and balanced softmax loss; (7)Â FedLCÂ (Zhang etÂ al., 2022), FL with logit calibration for bias reduction; (8)Â FedSAMÂ (Qu etÂ al., 2022; Caldarola etÂ al., 2022), incorporating sharpness-aware minimization into FL.Client Settings. We adopt the Dirichlet sampling to craft IID and heterogeneous data for clients, which is widely used in FL literature (Lin etÂ al., 2020; Chen & Chao, 2022; Li etÂ al., 2023b). It considers a class-imbalanced data heterogeneity, controlled by non-IID hyperparameter, and smaller value refers to more heterogeneous data of clients. We vary the hyperparameter âˆˆ{100,10,1.0,0.5,0.4}absent100101.00.50.4\\in\\{100,10,1.0,0.5,0.4\\} with a spectrum from IID to non-IID (heterogeneous).\nThe hyperparameters are shown in the captions or in AppendixÂ A. Except from TableÂ 4, we use full client participation.Evaluation and implementation. We test the generalization performance, which is validated on the balanced testset after the global model is generated on the server. For all the experiments, we conduct three trials for each setting and present the mean accuracy and the standard deviation in the tables. More implementation details in AppendixÂ A.Results under various datasets and models. In TableÂ 2, our methods can reach state-of-the-art results across four datasets under both IID (Î±=100ğ›¼100\\alpha=100) and heterogeneous (Î±=0.5ğ›¼0.5\\alpha=0.5) settings222Itâ€™s important to mention that certain methods might fail in specific settings, exhibiting accuracy levels close to random guessing, e.g., FedProx in Fashion-MNIST..\nGenerally, FedGuCci can reach the best performances over current FL methods, and FedGuCci+ can strengthen FedGuCci in most cases. Also, the performance gains of our approaches are more dominant under more complicated datasets, like Tiny-ImageNet. While FedSAM stands as the most robust baseline for generalization, our connectivity loss not only yields better results but is also compatible with it (FedGuCci+).Results on different Mğ‘€M and ÏğœŒ\\rho. We conduct experiments by varying the number of clients Mğ‘€M and participation ratios of clients ÏğœŒ\\rho in TableÂ 4. It demonstrates that FedGuCci and FedGuCci+ can also excel when the number of clients is large and partial participation exists, indicating their great potential under cross-device settingsÂ (Charles etÂ al., 2021).Results of different local epochs Eğ¸E. In FigureÂ 6, FedGuCci is consistently leading under different Eğ¸E, while FedGuCci+ is not robust on CIFAR-10. For CIFAR-100, FedGuCci has a more obvious advantage when Eğ¸E is large, and this is rationale since the connectivity and model drift issues are more severe under large local updates.In this section, we conduct experiments under pretrain-finetune paradigm for both vision and language tasks.Results under pretrained language models. We use 6 datasets from GLUEÂ (Wang etÂ al., 2019) benchmark for finetuning pretrained language models. For each dataset, we randomly split the data into several clients and conduct finetuning using low-rank adaption (LoRA), and the pretrained model is RoBERTa-baseÂ (Liu etÂ al., 2019). It is notable that some language tasks are not classifications, so FedRoD, FedLC, and FedGuCci+, which rely on classification loss, are not applicable. The results are in TableÂ 3, where our FedGuCci reaches promising performances over existing methods. It is observed that some methods that are superior in TableÂ 2 have worse performances in pretrained language models, e.g., FedDyn, while our FedGuCci keeps steady advantages.Results under pretrained vision models. We conduct experiments under pretrained vision models, namely, ResNet18Â (He etÂ al., 2016) pretrained on ImageNetÂ (Deng etÂ al., 2009) and Vision Transformer (ViT-B/32)Â (Dosovitskiy etÂ al., 2021) pretrained on CLIPÂ (Radford etÂ al., 2021). TableÂ 5 presents the finetuning results of FL methods on CIFAR-10 and CIFAR-100. It seems that FedAvg is a strong baseline when it comes to pretrained vision backbones, especially for the ViT. However, it is illustrated that FedGuCci is also improving generalization over FedAvg, whereas some methods may have unsatisfactory results, e.g., FedSAM. As a consequence of SAMâ€™s negative effect, FedGuCci+ has inferior performance.In this subsection, we showcase the applicability of FedGuCci under the pretrain-finetune paradigm, and it reveals FedGuCciâ€™s great potential in collaboratively finetuning foundation models, such as large language modelsÂ (Radford etÂ al., 2018; Touvron etÂ al., 2023).We conduct sensitivity analyses of FedGuCci(+)â€™s hyperparameters and their ablation study.Sensitivity analyses of hyperparameters. As illustrated in FigureÂ 7, we vary the FedGuCci(+)â€™s hyperparameters Nğ‘N and Î²ğ›½\\beta of EquationÂ 12 and subsectionÂ 4.1. It reveals that FedGuCci and FedGuCci+ have a wide range of effective hyperparameters, outperforming FedAvg. We find FedGuCci+ is more sensitive than FedGuCci, that high Nğ‘N and Î²ğ›½\\beta may degrade the performances. For Î²ğ›½\\beta, there may exist an optimization-connectivity tradeoff at the clients. If Î²ğ›½\\beta is too high, the connectivity loss may hurt the local optimization steps, causing generalization declines of local models, further detrimental to the fused global model.Ablation study. TableÂ 6 shows that FedGuCci already has obvious generalization gains over FedAvg; further, SAM and the bias reduction method (logit calibration) can reach higher generalization on FedGuCci. SAM has a more dominant improvement on FedGuCci. We note that FedGuCci is general and flexible and may be compatible with more existing FL algorithms, and FedGuCci+ is just one showcase.In this paper, we study the transitivity of linear mode connectivity (LMC) and use this property to improve the generalization of federated learning (FL). We first empirically and theoretically verify the transitivity of LMC between two models by leveraging a fixed anchor model, and we extend it to group connectivity among multiple models. Then, we propose FedGuCci and FedGuCci+ in FL. Extensive experiments demonstrate our proposed methods can improve the generalization of FL under various settings.This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.AppendixIn this appendix, we provide the details omitted in the main paper and more analyses and discussions.AppendixÂ A: details of experimental setups (cf. sectionÂ 3 and sectionÂ 5 of the main paper).AppendixÂ B: detailed proofs of Lemma 3.3, TheoremÂ 3.5, and TheoremÂ 3.8 (cf. sectionÂ 3 of the main paper).\nAppendixÂ C: more discussions about the related works (cf. sectionÂ 2 of the main paper).In this section, we present the implementation details omitted from the main paper.CIFAR-10Â (Krizhevsky etÂ al., 2009) consists of 60,000 32x32 color images, evenly distributed among 10 different classes, including airplanes, automobiles, birds, cats, etc., each represented by 6,000 images. The dataset is split into 50,000 training images and 10,000 test images.\nFashionMNISTÂ (Xiao etÂ al., 2017) is designed as an advanced replacement for the MNIST dataset, suitable for benchmarking machine learning models. It comprises 70,000 images divided into 60,000 training samples and 10,000 test samples. Each image is a 28x28 grayscale representation of fashion items from 10 different classes, such as shirts, trousers, sneakers, etc.\nThe CIFAR-100 datasetÂ (Krizhevsky etÂ al., 2009) is similar to CIFAR-10 but more challenging, containing 100 different classes grouped into 20 superclasses. It includes 60,000 32x32 color images, with 600 images per class, divided into 50,000 training images and 10,000 test images. This dataset is primarily used for developing and evaluating more sophisticated image classification models.\nTinyImageNet TinyImageNet is a reduced-scale version of the renowned ImageNet dataset, which comprises a total of 200 classes. The dataset is structured into training, validation, and test sets, with 200,000 training images, 20,000 validation images, and 20,000 test images.\nThe GLUE benchmark is a compilation of 9 datasets for evaluating natural language understanding systems. Tasks are framed as either single-sentence classification or sentence-pair classification tasks. GLUE includes MNLI (inference, (Williams etÂ al., 2017)), MRPC (paraphrase detection, (Socher etÂ al., 2013)), MRPC (paraphrase detection, (Dolan & Brockett, 2005)), CoLA (linguistic acceptability, (Warstadt etÂ al., 2019)), QNLI (inference, (Rajpurkar etÂ al., 2018)), QQP (question-answering), RTE (inference), WNLI (inference), and STS-B (textual similarity, (Cer etÂ al., 2017)). Due to high computation costs, we only used SST2, MRPC, CoLA, QNLI, RTE, and STS-B for evaluation. For the replication in TableÂ 3, we report results on the development sets after fine-tuning the pretrained models on the corresponding single-task training data. Our fine-tuning approach is LoRA(Hu etÂ al., 2021).SimpleCNN. The simple CNN for CIFAR-10 is a convolutional neural network model with ReLU activations, consisting of 3 convolutional layers followed by 2 fully connected layers. The first convolutional layer has a size of (3, 32, 3), followed by a max-pooling layer of size (2, 2). The second and third convolutional layers have sizes of (32, 64, 3) and (64, 64, 3), respectively. The last two fully connected layers have sizes of (6444, 64) and (64, num_classes), respectively.ResNets. We followed the model architectures used in (Li etÂ al., 2018). The number in the model names indicates the number of layers in the models, where a larger number indicates a deeper network. We used ResNet18 and ResNet20 for CIFAR-10 and CIFAR-100, respectively. Notably, to mitigate abnormal effects introduced by batch normalization layers (Li etÂ al., 2020b; Lin etÂ al., 2020), following by (Adilova etÂ al., 2023), we removed all batch normalization layers from the ResNets.VGG. VGG (Simonyan & Zisserman, 2015) is a convolutional neural network (CNN) architecture that gained prominence in the field of computer vision. Among its variants, we used VGG11.RoBERTa. RoBERTa is a natural language processing (NLP) model that builds upon the foundation laid by BERT, which was introduced by (Liu etÂ al., 2019) to address some limitations and improve the performance of BERT on various NLP tasks. It comes in various sizes, and we used RoBERTa-base considering to high computational costs.In all experiments, we conducted each experiment three times with different random seeds and reported the averaged results along with standard deviations.We ensured consistency by setting torch, numpy, and random functions with the same random seed, thereby making the data partitions and other settings identical. To ensure all algorithms started with the same initial model, we saved an initial model for each architecture and loaded it at the beginning of each experiment. Additionally, for experiments involving partial participation, the selection of participating clients in each round significantly influenced the modelâ€™s performance. To maintain fairness, we saved the sequences of participating clients in each round and loaded these sequences for all experiments. This procedure guaranteed that, given a random seed and participation ratio, every algorithm had the same set of sampled clients in each round.CIFAR-10, CIFAR-100, FashionMNIST and Tiny-ImageNet. We evaluate the global model performance on the test dataset of each dataset. The test dataset is mostly class-balanced and can reflect the global learning objective of a federated learning system. Therefore, the performance of the model on the test set can indicate the generalization performance of global modelsÂ (Li etÂ al., 2023a; Lin etÂ al., 2020). In each experiment, we take the average test accuracy of the last 5 rounds as the final test accuracy.GLUE. For GLUE, we used the validation dataset for evaluation. Following by Hu etÂ al. (2021), we chose the best accuracy as the final test accuracy.TableÂ 2: For Fashion-MNIST, Tğ‘‡T is 400, batch size is 64 and learning rate is 0.08. For CIFAR-10, Tğ‘‡T is 150, batch size is 64 and learning rate is 0.04. For CIFAR-100, Tğ‘‡T is 200, batch size is 64 and learning rate is 0.03. For Tiny-ImageNet, learning rate is 0.01 and Tğ‘‡T is 50. Optimzier is ADAM for Fashion-MNIST and others are SGD.TableÂ 3: Optimizer is Adam for all datasets. For CoLA and STSB, Tğ‘‡T is 25, batch size is 16 and learning rate is 2e-5. For SST-2, Tğ‘‡T is 50, batch size is 16, and learning rate is 2e-6. For QNLI, Tğ‘‡T is 20, batch size is 32 and learning rate is 2e-6. For RTE and MRPC, Tğ‘‡T is 80, batch size is 16 and learning rate is 2e-5.TableÂ 4: Tğ‘‡T is 150, E is 3, batch size is 64 and learning rate is 0.04.TableÂ 5: ResNet-18 and MobileViT are pretrained on ImageNet. E is 3 for both models. For ViT, Tğ‘‡T is 15, batch size is 16 and learning rate is 0.001. For ResNet, Tğ‘‡T is 50, batch size is 64 and learning rate 1e-4.TableÂ 6: For CIFAR-10, Tğ‘‡T is 150, batch size is 64 and learning rate is 0.04. For CIFAR-100, T is 200, batch size is 64, and learning rate is 0.03.FigureÂ 6: M=60ğ‘€60M=60 for CIFAR-10, and M=20ğ‘€20M=20 for CIFAR-100. Tğ‘‡T is 200 for both datasets. Learning rate is 0.03 for CIFAR-10, and 0.04 for CIFAR-100.FigureÂ 7: Tğ‘‡T is 150, Eğ¸E is 3, Mğ‘€M is 60, learning rate is 0.02, and batch size is 64.In this section, we give the proofs of the lemma and theorem in sectionÂ 3.(Lemma 3.3)\nSet the uniform and bounded domain for network ğ°ğ°\\mathbf{w} as â„°Ïµ={ğ°âˆˆÎ©|â„’â€‹(ğ°)<Ïµ}subscriptâ„°italic-Ïµconditional-setğ°Î©â„’ğ°italic-Ïµ\\mathcal{E}_{\\epsilon}=\\{\\mathbf{w}\\in\\Omega|\\mathcal{L}(\\mathbf{w})<\\epsilon\\}. Define a random event DÏµâ€‹(ğ°ancâˆ—)subscriptğ·italic-Ïµsuperscriptsubscriptğ°ancD_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}) as DÏµâ€‹(ğ°ancâˆ—)={ğ°âˆˆâ„°Ïµ|âˆ€Î±âˆˆ[0,1],â„’â€‹(Î±â€‹ğ°ancâˆ—+(1âˆ’Î±)â€‹ğ°)â‰¤Ïµ}subscriptğ·italic-Ïµsuperscriptsubscriptğ°ancconditional-setğ°subscriptâ„°italic-Ïµformulae-sequencefor-allğ›¼01â„’ğ›¼superscriptsubscriptğ°anc1ğ›¼ğ°italic-ÏµD_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})=\\{\\mathbf{w}\\in\\mathcal{E}_{\\epsilon}|\\forall\\alpha\\in[0,1],\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w})\\leq\\epsilon\\}. Consider an anchor model ğ°ancâˆ—superscriptsubscriptğ°anc{\\mathbf{w}_{\\text{anc}}^{*}} and an arbitrary network ğ°ğ°\\mathbf{w} and for Ïµ>0italic-Ïµ0\\epsilon>0. Then for â€–ğ°âˆ’ğ°ancâˆ—â€–âˆâ‰¤d2subscriptnormğ°superscriptsubscriptğ°ancğ‘‘2\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq\\frac{d}{2},where dÏµ=|â„°Ïµ|1Ssubscriptğ‘‘italic-Ïµsuperscriptsubscriptâ„°italic-Ïµ1ğ‘†d_{\\epsilon}=\\left|\\mathcal{E}_{\\epsilon}\\right|^{\\frac{1}{S}} represents the average diameter of region â„°Ïµsubscriptâ„°italic-Ïµ\\mathcal{E}_{\\epsilon}, Sğ‘†S represents the number of parameters of the neural network and the equality holds if and only if â„°ÏµâŠ‚{ğ°|â€–ğ°âˆ’ğ°ancâˆ—â€–âˆâ‰¤d}subscriptâ„°italic-Ïµconditional-setğ°subscriptnormğ°superscriptsubscriptğ°ancğ‘‘\\mathcal{E}_{\\epsilon}\\subset\\{\\mathbf{w}|\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq d\\} is a star domain centered at ğ°ancâˆ—superscriptsubscriptğ°anc{\\mathbf{w}_{\\text{anc}}^{*}}. Thus, when P(DÏµ(ğ°ancâˆ—)))>1âˆ’Î´P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})))>1-\\delta, it holds d<dÏµ(1âˆ’Î´)1Sğ‘‘subscriptğ‘‘italic-Ïµsuperscript1ğ›¿1ğ‘†d<{\\frac{d_{\\epsilon}}{(1-\\delta)^{\\frac{1}{S}}}}.In the following proof, we denote the region as ğ’±d={ğ°|â€–ğ°âˆ’ğ°ancâˆ—â€–âˆâ‰¤d2}subscriptğ’±ğ‘‘conditional-setğ°subscriptnormğ°superscriptsubscriptğ°ancğ‘‘2\\mathcal{V}_{d}=\\{\\mathbf{w}|\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq\\frac{d}{2}\\} with volume |ğ’±d|=dSsubscriptğ’±ğ‘‘superscriptğ‘‘ğ‘†|\\mathcal{V}_{d}|=d^{S} and denote the segment between ğ°ğ°\\mathbf{w} and ğ°ancâˆ—superscriptsubscriptğ°anc{\\mathbf{w}_{\\text{anc}}^{*}} as lâ€‹(ğ°ancâˆ—,ğ°)={Î±â€‹ğ°ancâˆ—+(1âˆ’Î±)â€‹ğ°,Î±âˆˆ[0,1]}ğ‘™superscriptsubscriptğ°ancğ°ğ›¼superscriptsubscriptğ°anc1ğ›¼ğ°ğ›¼01l({\\mathbf{w}_{\\text{anc}}^{*}},\\mathbf{w})=\\{\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w},\\alpha\\in[0,1]\\}.First we prove if â„°ÏµâŠ‚ğ’±dsubscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘\\mathcal{E}_{\\epsilon}\\subset\\mathcal{V}_{d} is a star domain centered at ğ°ancâˆ—superscriptsubscriptğ°anc{\\mathbf{w}_{\\text{anc}}^{*}}, Pâ€‹(DÏµâ€‹(ğ°ancâˆ—))=|â„°Ïµ|dSğ‘ƒsubscriptğ·italic-Ïµsuperscriptsubscriptğ°ancsubscriptâ„°italic-Ïµsuperscriptğ‘‘ğ‘†P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))=\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{d^{S}}. Select a parameter point ğ°0subscriptğ°0\\mathbf{w}_{0} in ğ’±dsubscriptğ’±ğ‘‘\\mathcal{V}_{d} arbitrarily. If ğ°0âˆˆâ„°Ïµsubscriptğ°0subscriptâ„°italic-Ïµ\\mathbf{w}_{0}\\in\\mathcal{E}_{\\epsilon}, then because â„°Ïµsubscriptâ„°italic-Ïµ\\mathcal{E}_{\\epsilon} is a star domain centered at ğ°ancâˆ—superscriptsubscriptğ°anc{\\mathbf{w}_{\\text{anc}}^{*}}, lâ€‹(ğ°ancâˆ—,ğ°)âŠ‚â„°Ïµğ‘™superscriptsubscriptğ°ancğ°subscriptâ„°italic-Ïµl({\\mathbf{w}_{\\text{anc}}^{*}},\\mathbf{w})\\subset\\mathcal{E}_{\\epsilon} and thus ğ°0âˆˆDÏµâ€‹(ğ°ancâˆ—)subscriptğ°0subscriptğ·italic-Ïµsuperscriptsubscriptğ°anc\\mathbf{w}_{0}\\in D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}). If ğ°0âˆ‰â„°Ïµsubscriptğ°0subscriptâ„°italic-Ïµ\\mathbf{w}_{0}\\notin\\mathcal{E}_{\\epsilon}, then ğ°0âˆ‰DÏµâ€‹(ğ°ancâˆ—)subscriptğ°0subscriptğ·italic-Ïµsuperscriptsubscriptğ°anc\\mathbf{w}_{0}\\notin D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}) by the definition of DÏµâ€‹(ğ°ancâˆ—)subscriptğ·italic-Ïµsuperscriptsubscriptğ°ancD_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}). Therefore, â„°Ïµ=DÏµâ€‹(ğ°ancâˆ—)subscriptâ„°italic-Ïµsubscriptğ·italic-Ïµsuperscriptsubscriptğ°anc\\mathcal{E}_{\\epsilon}=D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}) and we have Pâ€‹(DÏµâ€‹(ğ°ancâˆ—))=Pâ€‹(â„°Ïµ)=|â„°Ïµ||ğ’±d|=|â„°Ïµ|dSğ‘ƒsubscriptğ·italic-Ïµsuperscriptsubscriptğ°ancğ‘ƒsubscriptâ„°italic-Ïµsubscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘subscriptâ„°italic-Ïµsuperscriptğ‘‘ğ‘†P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))=P(\\mathcal{E}_{\\epsilon})=\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{\\left|\\mathcal{V}_{d}\\right|}=\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{d^{S}}.The next step we prove that if â„°ÏµâŠ„ğ’±dnot-subset-ofsubscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘\\mathcal{E}_{\\epsilon}\\not\\subset\\mathcal{V}_{d}, or â„°Ïµsubscriptâ„°italic-Ïµ\\mathcal{E}_{\\epsilon} is not a star domain centered at ğ°ancâˆ—superscriptsubscriptğ°anc{\\mathbf{w}_{\\text{anc}}^{*}}, then Pâ€‹(DÏµâ€‹(ğ°ancâˆ—))<|â„°Ïµ|dSğ‘ƒsubscriptğ·italic-Ïµsuperscriptsubscriptğ°ancsubscriptâ„°italic-Ïµsuperscriptğ‘‘ğ‘†P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))<\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{d^{S}}.If â„°ÏµâŠ„ğ’±dnot-subset-ofsubscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘\\mathcal{E}_{\\epsilon}\\not\\subset\\mathcal{V}_{d}, then |DÏµâ€‹(ğ°ancâˆ—)|â‰¤|â„°Ïµâˆ©ğ’±d|<|â„°Ïµ|subscriptğ·italic-Ïµsuperscriptsubscriptğ°ancsubscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘subscriptâ„°italic-Ïµ\\left|D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\right|\\leq\\left|\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d}\\right|<\\left|\\mathcal{E}_{\\epsilon}\\right| and Pâ€‹(DÏµâ€‹(ğ°ancâˆ—))=|DÏµâ€‹(ğ°ancâˆ—)||ğ’±d|<|â„°Ïµ||ğ’±d|ğ‘ƒsubscriptğ·italic-Ïµsuperscriptsubscriptğ°ancsubscriptğ·italic-Ïµsuperscriptsubscriptğ°ancsubscriptğ’±ğ‘‘subscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))=\\frac{\\left|D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\right|}{\\left|\\mathcal{V}_{d}\\right|}<\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{\\left|\\mathcal{V}_{d}\\right|}. Here, the first inequality |DÏµâ€‹(ğ°ancâˆ—)|â‰¤|â„°Ïµâˆ©ğ’±d|subscriptğ·italic-Ïµsuperscriptsubscriptğ°ancsubscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘\\left|D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\right|\\leq\\left|\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d}\\right| holds, because DÏµâ€‹(ğ°ancâˆ—)âŠ‚â„°Ïµâˆ©ğ’±dsubscriptğ·italic-Ïµsuperscriptsubscriptğ°ancsubscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\subset\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d} and the second inequality |â„°Ïµâˆ©ğ’±d|<|â„°Ïµ|subscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘subscriptâ„°italic-Ïµ\\left|\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d}\\right|<|\\mathcal{E}_{\\epsilon}| holds, because âˆƒğ°0âˆˆâ„°Ïµ/ğ’±d,Ïµ0>0formulae-sequencesubscriptğ°0subscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘subscriptitalic-Ïµ00\\exists\\mathbf{w}_{0}\\in\\mathcal{E}_{\\epsilon}/\\mathcal{V}_{d},\\epsilon_{0}>0 st. {ğ°|â€–ğ°âˆ’ğ°0â€–<Ïµ0}âŠ‚Î©/ğ’±dâˆ©â„°Ïµconditional-setğ°normğ°subscriptğ°0subscriptitalic-Ïµ0Î©subscriptğ’±ğ‘‘subscriptâ„°italic-Ïµ\\{\\mathbf{w}|\\|\\mathbf{w}-\\mathbf{w}_{0}\\|<\\epsilon_{0}\\}\\subset\\Omega/\\mathcal{V}_{d}\\cap\\mathcal{E}_{\\epsilon} for Î©/ğ’±dÎ©subscriptğ’±ğ‘‘\\Omega/\\mathcal{V}_{d} and â„°Ïµsubscriptâ„°italic-Ïµ\\mathcal{E}_{\\epsilon} are open sets and |â„°Ïµâˆ©ğ’±d|â‰¤|â„°Ïµ|âˆ’|{ğ°|â€–ğ°âˆ’ğ°0â€–<Ïµ0}|<|â„°Ïµ|subscriptâ„°italic-Ïµsubscriptğ’±ğ‘‘subscriptâ„°italic-Ïµconditional-setğ°normğ°subscriptğ°0subscriptitalic-Ïµ0subscriptâ„°italic-Ïµ\\left|\\mathcal{E}_{\\epsilon}\\cap\\mathcal{V}_{d}\\right|\\leq|\\mathcal{E}_{\\epsilon}|-\\left|\\{\\mathbf{w}|\\|\\mathbf{w}-\\mathbf{w}_{0}\\|<\\epsilon_{0}\\}\\right|<\\left|\\mathcal{E}_{\\epsilon}\\right|.If â„°Ïµsubscriptâ„°italic-Ïµ\\mathcal{E}_{\\epsilon} is not a star domain centered at ğ°ancâˆ—superscriptsubscriptğ°anc{\\mathbf{w}_{\\text{anc}}^{*}}, then there exists ğ°0âˆˆâ„°Ïµsubscriptğ°0subscriptâ„°italic-Ïµ\\mathbf{w}_{0}\\in\\mathcal{E}_{\\epsilon} such that lâ€‹(ğ°ancâˆ—,ğ°0)âŠ„â„°Ïµnot-subset-ofğ‘™superscriptsubscriptğ°ancsubscriptğ°0subscriptâ„°italic-Ïµl({\\mathbf{w}_{\\text{anc}}^{*}},\\mathbf{w}_{0})\\not\\subset\\mathcal{E}_{\\epsilon}. Then âˆƒÎ±1âˆˆ(0,1)subscriptğ›¼101\\exists\\alpha_{1}\\in(0,1) st. ğ°1â€‹=Î”â€‹Î±1â€‹ğ°ancâˆ—+(1âˆ’Î±1)â€‹ğ°0subscriptğ°1Î”subscriptğ›¼1superscriptsubscriptğ°anc1subscriptğ›¼1subscriptğ°0\\mathbf{w}_{1}\\overset{\\Delta}{=}\\alpha_{1}{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha_{1})\\mathbf{w}_{0} satisfies â„’â€‹(ğ°1)>Ïµâ„’subscriptğ°1italic-Ïµ\\mathcal{L}(\\mathbf{w}_{1})>\\epsilon. For â„’â€‹(â‹…)â„’â‹…\\mathcal{L}(\\cdot) is smooth, there exists Ïµ1>0subscriptitalic-Ïµ10\\epsilon_{1}>0 st. âˆ€ğ°âˆˆUÏµ1â€‹(ğ°1)â€‹=Î”â€‹{ğ°|â€–ğ°1âˆ’ğ°â€–2<Ïµ1}for-allğ°subscriptğ‘ˆsubscriptitalic-Ïµ1subscriptğ°1Î”conditional-setğ°subscriptnormsubscriptğ°1ğ°2subscriptitalic-Ïµ1\\forall\\mathbf{w}\\in U_{\\epsilon_{1}}(\\mathbf{w}_{1})\\overset{\\Delta}{=}\\{\\mathbf{w}|\\|\\mathbf{w}_{1}-\\mathbf{w}\\|_{2}<\\epsilon_{1}\\}, â„’â€‹(ğ°)â‰¥Ïµ+â„’â€‹(ğ°1)âˆ’Ïµ2>Ïµâ„’ğ°italic-Ïµâ„’subscriptğ°1italic-Ïµ2italic-Ïµ\\mathcal{L}(\\mathbf{w})\\geq\\epsilon+\\frac{\\mathcal{L}(\\mathbf{w}_{1})-\\epsilon}{2}>\\epsilon. Then for â„°Ïµsubscriptâ„°italic-Ïµ\\mathcal{E}_{\\epsilon} is an open set, choose Ïµ2<Ïµ1subscriptitalic-Ïµ2subscriptitalic-Ïµ1\\epsilon_{2}<\\epsilon_{1} st. UÏµ2â€‹(ğ°0)âŠ‚â„°Ïµsubscriptğ‘ˆsubscriptitalic-Ïµ2subscriptğ°0subscriptâ„°italic-ÏµU_{\\epsilon_{2}}(\\mathbf{w}_{0})\\subset\\mathcal{E}_{\\epsilon}. âˆ€ğ°2âˆˆUÏµ2â€‹(ğ°0)for-allsubscriptğ°2subscriptğ‘ˆsubscriptitalic-Ïµ2subscriptğ°0\\forall\\mathbf{w}_{2}\\in U_{\\epsilon_{2}}(\\mathbf{w}_{0}), ğ°3=Î±1â€‹ğ°ancâˆ—+(1âˆ’Î±1)â€‹ğ°2subscriptğ°3subscriptğ›¼1superscriptsubscriptğ°anc1subscriptğ›¼1subscriptğ°2\\mathbf{w}_{3}=\\alpha_{1}{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha_{1})\\mathbf{w}_{2} satisfies â€–ğ°3âˆ’ğ°1â€–2=(1âˆ’Î±1)â€‹â€–ğ°0âˆ’ğ°2â€–2<(1âˆ’Î±1)â€‹Ïµ2<Ïµ1subscriptnormsubscriptğ°3subscriptğ°121subscriptğ›¼1subscriptnormsubscriptğ°0subscriptğ°221subscriptğ›¼1subscriptitalic-Ïµ2subscriptitalic-Ïµ1\\|\\mathbf{w}_{3}-\\mathbf{w}_{1}\\|_{2}=(1-\\alpha_{1})\\|\\mathbf{w}_{0}-\\mathbf{w}_{2}\\|_{2}<(1-\\alpha_{1})\\epsilon_{2}<\\epsilon_{1}. Thus ğ°3âˆˆUÏµ1â€‹(ğ°1)subscriptğ°3subscriptğ‘ˆsubscriptitalic-Ïµ1subscriptğ°1\\mathbf{w}_{3}\\in U_{\\epsilon_{1}}(\\mathbf{w}_{1}), which leads to â„’â€‹(ğ°3)>Ïµâ„’subscriptğ°3italic-Ïµ\\mathcal{L}(\\mathbf{w}_{3})>\\epsilon. Therefore, UÏµ2â€‹(ğ°0)âˆ©DÏµâ€‹(ğ°ancâˆ—)=âˆ…subscriptğ‘ˆsubscriptitalic-Ïµ2subscriptğ°0subscriptğ·italic-Ïµsuperscriptsubscriptğ°ancU_{\\epsilon_{2}}(\\mathbf{w}_{0})\\cap D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})=\\emptyset and Pâ€‹(DÏµâ€‹(ğ°ancâˆ—))=|DÏµâ€‹(ğ°ancâˆ—)|dSâ‰¤|â„°Ïµ|âˆ’|UÏµ2â€‹(ğ°0)|dS<|â„°Ïµ|dSğ‘ƒsubscriptğ·italic-Ïµsuperscriptsubscriptğ°ancsubscriptğ·italic-Ïµsuperscriptsubscriptğ°ancsuperscriptğ‘‘ğ‘†subscriptâ„°italic-Ïµsubscriptğ‘ˆsubscriptitalic-Ïµ2subscriptğ°0superscriptğ‘‘ğ‘†subscriptâ„°italic-Ïµsuperscriptğ‘‘ğ‘†P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}}))=\\frac{\\left|D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})\\right|}{d^{S}}\\leq\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|-\\left|U_{\\epsilon_{2}}(\\mathbf{w}_{0})\\right|}{d^{S}}<\\frac{\\left|\\mathcal{E}_{\\epsilon}\\right|}{d^{S}}.âˆ(TheoremÂ 3.5)\nWe define a two-layer neural network with ReLU activation, and the function is fğ¯,ğ”â€‹(ğ±)=ğ¯âŠ¤â€‹Ïƒâ€‹(ğ”ğ±)subscriptğ‘“ğ¯ğ”ğ±superscriptğ¯topğœğ”ğ±f_{\\mathbf{v},\\mathbf{U}}(\\mathbf{x})=\\mathbf{v}^{\\top}\\sigma(\\mathbf{U}\\mathbf{x}) where Ïƒâ€‹(â‹…)ğœâ‹…\\sigma(\\cdot) is the ReLU activation function. ğ¯âˆˆâ„hğ¯superscriptâ„â„\\mathbf{v}\\in\\mathbb{R}^{h} and ğ”âˆˆâ„hÃ—lğ”superscriptâ„â„ğ‘™\\mathbf{U}\\in\\mathbb{R}^{h\\times l} are parameters333For simplicity and without loss of generality, we omit the bias terms. and ğ±âˆˆâ„lğ±superscriptâ„ğ‘™\\mathbf{x}\\in\\mathbb{R}^{l} is the input which is taken from ğ•={ğ±âˆˆâ„l|â€–ğ±â€–2<b}ğ•conditional-setğ±superscriptâ„ğ‘™subscriptnormğ±2ğ‘\\mathbb{X}=\\{\\mathbf{x}\\in\\mathbb{R}^{l}|\\|\\mathbf{x}\\|_{2}<b\\} uniformly. Denote the deterministic anchor model as ğ°ancâˆ—={ğ”ancâˆ—,ğ¯ancâˆ—}superscriptsubscriptğ°ancsuperscriptsubscriptğ”ancsuperscriptsubscriptğ¯anc{\\mathbf{w}_{\\text{anc}}^{*}}=\\{\\mathbf{U}_{\\text{anc}}^{*},\\mathbf{v}_{\\text{anc}}^{*}\\}, with â€–ğ¯ancâˆ—â€–2<dancsubscriptnormsuperscriptsubscriptğ¯anc2subscriptğ‘‘anc\\|\\mathbf{v}_{\\text{anc}}^{*}\\|_{2}<d_{\\text{anc}} and consider two different networks ğ°1,ğ°2subscriptğ°1subscriptğ°2\\mathbf{w}_{1},\\mathbf{w}_{2} parameterized with {ğ”1,ğ¯1}subscriptğ”1subscriptğ¯1\\{\\mathbf{U}_{1},\\mathbf{v}_{1}\\} and {ğ”2,ğ¯2}subscriptğ”2subscriptğ¯2\\{\\mathbf{U}_{2},\\mathbf{v}_{2}\\} respectively. Each element of ğ”1subscriptğ”1\\mathbf{U}_{1} and ğ”2subscriptğ”2\\mathbf{U}_{2}, ğ¯1subscriptğ¯1\\mathbf{v}_{1} and ğ¯2subscriptğ¯2\\mathbf{v}_{2} is sampled from a uniform distribution centered at ğ”ancâˆ—superscriptsubscriptğ”anc\\mathbf{U}_{\\text{anc}}^{*} and ğ¯ancsubscriptğ¯anc\\mathbf{v}_{\\text{anc}} with an interval length of dğ‘‘d. If with probability 1âˆ’Î´1ğ›¿1-\\delta, supÎ±â„’â€‹(Î±â€‹ğ°ancâˆ—+(1âˆ’Î±)â€‹ğ°1)<Ïµsubscriptsupremumğ›¼â„’ğ›¼superscriptsubscriptğ°anc1ğ›¼subscriptğ°1italic-Ïµ\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{1})<\\epsilon and supÎ±â„’â€‹(Î±â€‹ğ°ancâˆ—+(1âˆ’Î±)â€‹ğ°2)<Ïµsubscriptsupremumğ›¼â„’ğ›¼superscriptsubscriptğ°anc1ğ›¼subscriptğ°2italic-Ïµ\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{2})<\\epsilon, then with probability 1âˆ’Î´1ğ›¿1-\\delta, it has,where Blâ€‹oâ€‹sâ€‹sâ€‹(ğ°1,ğ°2)subscriptğµğ‘™ğ‘œğ‘ ğ‘ subscriptğ°1subscriptğ°2B_{loss}(\\mathbf{w}_{1},\\mathbf{w}_{2}) is the loss barrier as EquationÂ 3.Letâ€™s first define gÎ±â€‹(ğ±)=(Î±â€‹ğ”1+(1âˆ’Î±)â€‹ğ”2)â€‹ğ±subscriptğ‘”ğ›¼ğ±ğ›¼subscriptğ”11ğ›¼subscriptğ”2ğ±g_{\\alpha}(\\mathbf{x})=(\\alpha\\mathbf{U}_{1}+(1-\\alpha)\\mathbf{U}_{2})\\mathbf{x} and zğ±â€‹(Î±)=(Î±â€‹ğ¯1+(1âˆ’Î±)â€‹ğ¯2)âŠ¤â€‹Ïƒâ€‹((Î±â€‹ğ”1+(1âˆ’Î±)â€‹ğ”2)â€‹ğ±)âˆ’Î±â€‹ğ¯1âŠ¤â€‹Ïƒâ€‹(ğ”1â€‹ğ±)âˆ’(1âˆ’Î±)â€‹ğ¯2âŠ¤â€‹Ïƒâ€‹(ğ”2â€‹ğ±)subscriptğ‘§ğ±ğ›¼superscriptğ›¼subscriptğ¯11ğ›¼subscriptğ¯2topğœğ›¼subscriptğ”11ğ›¼subscriptğ”2ğ±ğ›¼superscriptsubscriptğ¯1topğœsubscriptğ”1ğ±1ğ›¼superscriptsubscriptğ¯2topğœsubscriptğ”2ğ±z_{\\mathbf{x}}(\\alpha)=(\\alpha\\mathbf{v}_{1}+(1-\\alpha)\\mathbf{v}_{2})^{\\top}\\sigma((\\alpha\\mathbf{U}_{1}+(1-\\alpha)\\mathbf{U}_{2})\\mathbf{x})-\\alpha\\mathbf{v}_{1}^{\\top}\\sigma(\\mathbf{U}_{1}\\mathbf{x})-(1-\\alpha){\\mathbf{v}_{2}}^{\\top}\\sigma(\\mathbf{U}_{2}\\mathbf{x}), Î±âˆˆ[0,1]ğ›¼01\\alpha\\in[0,1]. Then we can express zğ±â€‹(Î±)subscriptğ‘§ğ±ğ›¼z_{\\mathbf{x}}(\\alpha) as:For each element of ğ”1subscriptğ”1\\mathbf{U}_{1} and ğ”2subscriptğ”2\\mathbf{U}_{2}, ğ¯1subscriptğ¯1\\mathbf{v}_{1} and ğ¯2subscriptğ¯2\\mathbf{v}_{2} is sampled from a uniform distribution centered at ğ”ancâˆ—superscriptsubscriptğ”anc\\mathbf{U}_{\\text{anc}}^{*} and ğ¯ancâˆ—superscriptsubscriptğ¯anc\\mathbf{v}_{\\text{anc}}^{*} with an interval length of dğ‘‘d, ğ”1subscriptğ”1\\mathbf{U}_{1}, ğ”2subscriptğ”2\\mathbf{U}_{2}, ğ¯1subscriptğ¯1\\mathbf{v}_{1} and ğ¯2subscriptğ¯2\\mathbf{v}_{2} can be represented as ğ”1=ğ”ancâˆ—+ğ”~1subscriptğ”1superscriptsubscriptğ”ancsubscript~ğ”1\\mathbf{U}_{1}=\\mathbf{U}_{\\text{anc}}^{*}+\\tilde{\\mathbf{U}}_{1}, ğ”2=ğ”ancâˆ—+ğ”~2subscriptğ”2superscriptsubscriptğ”ancsubscript~ğ”2\\mathbf{U}_{2}=\\mathbf{U}_{\\text{anc}}^{*}+\\tilde{\\mathbf{U}}_{2}, ğ¯1=ğ¯ancâˆ—+ğ¯~1subscriptğ¯1superscriptsubscriptğ¯ancsubscript~ğ¯1\\mathbf{v}_{1}=\\mathbf{v}_{\\text{anc}}^{*}+\\tilde{\\mathbf{v}}_{1} and ğ¯2=ğ¯ancâˆ—+ğ¯~2subscriptğ¯2superscriptsubscriptğ¯ancsubscript~ğ¯2\\mathbf{v}_{2}=\\mathbf{v}_{\\text{anc}}^{*}+\\tilde{\\mathbf{v}}_{2} respectively, where each element of ğ”~1subscript~ğ”1\\tilde{\\mathbf{U}}_{1}, ğ”~2subscript~ğ”2\\tilde{\\mathbf{U}}_{2}, ğ¯~1subscript~ğ¯1\\tilde{\\mathbf{v}}_{1} and ğ¯~2subscript~ğ¯2\\tilde{\\mathbf{v}}_{2} follows distribution Uâ€‹[âˆ’d2,d2]ğ‘ˆğ‘‘2ğ‘‘2U[-\\frac{d}{2},\\frac{d}{2}]. Using ğ¯~1subscript~ğ¯1\\tilde{\\mathbf{v}}_{1} and ğ¯~2subscript~ğ¯2\\tilde{\\mathbf{v}}_{2}, zğ±â€‹(Î±)subscriptğ‘§ğ±ğ›¼z_{\\mathbf{x}}(\\alpha) can be represented asWe also assume that the number of hidden neurons hâ„h is sufficiently large for the convenience of analysis as Entezari etÂ al. (2022). In the following proof, we will make use of Hoeffdingâ€™s inequality for sub-Gaussian distributions (especially, uniform distribution). Here, we state it for reference: Let X1,â€¦,Xnsubscriptğ‘‹1â€¦subscriptğ‘‹ğ‘›X_{1},\\ldots,X_{n} be nğ‘›n independent random variables such that Xiâˆ¼Uâ€‹(âˆ’d2,âˆ’d2)similar-tosubscriptğ‘‹ğ‘–ğ‘ˆğ‘‘2ğ‘‘2X_{i}\\sim U(-\\frac{d}{2},-\\frac{d}{2}). Then for any ğš=(a1,â€¦,an)âˆˆâ„nğšsubscriptğ‘1â€¦subscriptğ‘ğ‘›superscriptâ„ğ‘›\\mathbf{a}=(a_{1},...,a_{n})\\in\\mathbb{R}^{n}, we have\n\nP[|âˆ‘_i=1^n a_i X_i| >t] â‰¤2exp(-2t2d2âˆ¥a âˆ¥22).\n\nTo bound zğ±â€‹(Î±)subscriptğ‘§ğ±ğ›¼z_{\\mathbf{x}}(\\alpha), we haveThen we bound the first term and the third term, and the second term and the fourth term are bounded similarly due to symmetry. For the concentration upper bound of the first term of EquationÂ 18, we use the Hoeffdingâ€™s inequality for elements of ğ¯~1subscript~ğ¯1\\tilde{\\mathbf{v}}_{1}, with probability 1âˆ’Î´k1ğ›¿ğ‘˜1-\\frac{\\delta}{k}EquationÂ 20 is due to the fact that the ReLU activation function satisfies the Lipschitz continuous condition with constant 111. For the bound of the third term of EquationÂ 18, we haveEquationÂ 23 is due to the fact that the ReLU activation function satisfies the Lipschitz continuous condition with constant 111. For the term â€–(ğ”2âˆ’ğ”1)â€‹ğ±â€–2subscriptnormsubscriptğ”2subscriptğ”1ğ±2\\|(\\mathbf{U}_{2}-\\mathbf{U}_{1})\\mathbf{x}\\|_{2} in EquationÂ 21 and EquationÂ 24, taking a union bound, with probability 1âˆ’Î´k1ğ›¿ğ‘˜1-\\frac{\\delta}{k}, we haveThen take a union bound choosing k=6ğ‘˜6k=6 (because the union bound is taken for 666 equations, EquationÂ 21 and EquationÂ 28 for the first and the second terms in EquationÂ 18 respectively, and EquationÂ 28 for the third and the fourth terms in EquationÂ 18 respectively.), with probability 1âˆ’Î´1ğ›¿1-\\delta we haveFor supÎ±â„’â€‹(Î±â€‹ğ°ancâˆ—+(1âˆ’Î±)â€‹ğ°)<Ïµsubscriptsupremumğ›¼â„’ğ›¼superscriptsubscriptğ°anc1ğ›¼ğ°italic-Ïµ\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w})<\\epsilon holds with probability 1âˆ’Î´1ğ›¿1-\\delta, by Lemma 3.3, we have d<dÏµ(1âˆ’Î´)1Sğ‘‘subscriptğ‘‘italic-Ïµsuperscript1ğ›¿1ğ‘†d<\\frac{d_{\\epsilon}}{(1-\\delta)^{\\frac{1}{S}}} with S=hâ€‹l+hğ‘†â„ğ‘™â„S=hl+h. Then |zğ±â€‹(Î±)|subscriptğ‘§ğ±ğ›¼|z_{\\mathbf{x}}(\\alpha)| can be bounded asNow we turn to calculate the bound of the loss barrier Blâ€‹oâ€‹sâ€‹sâ€‹(ğ°1,ğ°2)subscriptğµğ‘™ğ‘œğ‘ ğ‘ subscriptğ°1subscriptğ°2B_{loss}(\\mathbf{w}_{1},\\mathbf{w}_{2}). For the loss function Lâ€‹(â‹…,y)ğ¿â‹…ğ‘¦L(\\cdot,y) is convex and 1-Lipschitz, we have:where the expectation is with respect to the dataset. EquationÂ 36 is due to the convexity of Lâ€‹(â‹…,y)ğ¿â‹…ğ‘¦L(\\cdot,y), while EquationÂ 37 is due to the assumption that Lâ€‹(â‹…,y)ğ¿â‹…ğ‘¦L(\\cdot,y) is 1-Lipschitz. Then use the bound of zxâ€‹(Î±)subscriptğ‘§xğ›¼z_{\\textbf{x}}(\\alpha), with probability 1âˆ’Î´1ğ›¿1-\\delta, we haveâˆ(TheoremÂ 3.8)\nWe define a two-layer neural network with ReLU activation, and the function is fğ¯,ğ”â€‹(ğ±)=ğ¯âŠ¤â€‹Ïƒâ€‹(ğ”ğ±)subscriptğ‘“ğ¯ğ”ğ±superscriptğ¯topğœğ”ğ±f_{\\mathbf{v},\\mathbf{U}}(\\mathbf{x})=\\mathbf{v}^{\\top}\\sigma(\\mathbf{U}\\mathbf{x}) where Ïƒâ€‹(â‹…)ğœâ‹…\\sigma(\\cdot) is the ReLU activation function. ğ¯âˆˆâ„hğ¯superscriptâ„â„\\mathbf{v}\\in\\mathbb{R}^{h} and ğ”âˆˆâ„hÃ—lğ”superscriptâ„â„ğ‘™\\mathbf{U}\\in\\mathbb{R}^{h\\times l} are parameters and ğ±âˆˆâ„lğ±superscriptâ„ğ‘™\\mathbf{x}\\in\\mathbb{R}^{l} is the input which is taken from ğ•={ğ±âˆˆâ„l|â€–ğ±â€–2<b}ğ•conditional-setğ±superscriptâ„ğ‘™subscriptnormğ±2ğ‘\\mathbb{X}=\\{\\mathbf{x}\\in\\mathbb{R}^{l}|\\|\\mathbf{x}\\|_{2}<b\\} uniformly. Denote the deterministic anchor model as ğ°ancâˆ—={ğ”ancâˆ—,ğ¯ancâˆ—}superscriptsubscriptğ°ancsuperscriptsubscriptğ”ancsuperscriptsubscriptğ¯anc\\mathbf{w}_{\\text{anc}}^{*}=\\{\\mathbf{U}_{\\text{anc}}^{*},\\mathbf{v}_{\\text{anc}}^{*}\\}, with â€–ğ¯ancâˆ—â€–2<dancsubscriptnormsuperscriptsubscriptğ¯anc2subscriptğ‘‘anc\\|\\mathbf{v}_{\\text{anc}}^{*}\\|_{2}<d_{\\text{anc}} and consider Kğ¾K different networks ğ°isubscriptğ°ğ‘–\\mathbf{w}_{i} parameterized with {ğ”i,ğ¯i}subscriptğ”ğ‘–subscriptğ¯ğ‘–\\{\\mathbf{U}_{i},\\mathbf{v}_{i}\\} located on Kğ¾K clients respectively. Each element of ğ”isubscriptğ”ğ‘–\\mathbf{U}_{i} and ğ¯isubscriptğ¯ğ‘–\\mathbf{v}_{i} is sampled from a uniform distribution centered at ğ”ancâˆ—superscriptsubscriptğ”anc\\mathbf{U}_{\\text{anc}}^{*} and ğ¯ancâˆ—superscriptsubscriptğ¯anc\\mathbf{v}_{\\text{anc}}^{*} with an interval length of dğ‘‘d. If with probability 1âˆ’Î´1ğ›¿1-\\delta, supÎ±â„’iâ€‹(Î±â€‹ğ°ancâˆ—+(1âˆ’Î±)â€‹ğ°i)<Ïµsubscriptsupremumğ›¼subscriptâ„’ğ‘–ğ›¼superscriptsubscriptğ°anc1ğ›¼subscriptğ°ğ‘–italic-Ïµ\\sup_{\\alpha}\\mathcal{L}_{i}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{i})<\\epsilon, then with probability 1âˆ’Î´1ğ›¿1-\\delta, it has,Similar to TheoremÂ 3.5, we first define gâ€‹(ğ±)=(1Kâ€‹âˆ‘i=1Kğ”i)â€‹ğ±ğ‘”ğ±1ğ¾superscriptsubscriptğ‘–1ğ¾subscriptğ”ğ‘–ğ±g(\\mathbf{x})=(\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{U}_{i})\\mathbf{x} and zâ€‹(ğ±)=(1Kâ€‹âˆ‘i=1Kğ¯i)âŠ¤â€‹Ïƒâ€‹((1Kâ€‹âˆ‘i=1Kğ”i)â€‹ğ±)âˆ’1Kâ€‹âˆ‘i=1Kğ¯iâ€‹Ïƒâ€‹(ğ”iâ€‹ğ±)ğ‘§ğ±superscript1ğ¾superscriptsubscriptğ‘–1ğ¾subscriptğ¯ğ‘–topğœ1ğ¾superscriptsubscriptğ‘–1ğ¾subscriptğ”ğ‘–ğ±1ğ¾superscriptsubscriptğ‘–1ğ¾subscriptğ¯ğ‘–ğœsubscriptğ”ğ‘–ğ±z(\\mathbf{x})=(\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{v}_{i})^{\\top}\\sigma((\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{U}_{i})\\mathbf{x})-\\frac{1}{K}\\sum_{i=1}^{K}\\mathbf{v}_{i}\\sigma(\\mathbf{U}_{i}\\mathbf{x}). Then we can express zâ€‹(ğ±)ğ‘§ğ±z(\\mathbf{x}) as:For each element of ğ”isubscriptğ”ğ‘–\\mathbf{U}_{i} and ğ¯isubscriptğ¯ğ‘–\\mathbf{v}_{i} is sampled from a uniform distribution centered at ğ”ancâˆ—superscriptsubscriptğ”anc\\mathbf{U}_{\\text{anc}}^{*} and ğ¯ancâˆ—superscriptsubscriptğ¯anc\\mathbf{v}_{\\text{anc}}^{*} with an interval length of dğ‘‘d, ğ”isubscriptğ”ğ‘–\\mathbf{U}_{i} and ğ¯isubscriptğ¯ğ‘–\\mathbf{v}_{i} can be represented as ğ”i=ğ”ancâˆ—+ğ”~isubscriptğ”ğ‘–superscriptsubscriptğ”ancsubscript~ğ”ğ‘–\\mathbf{U}_{i}=\\mathbf{U}_{\\text{anc}}^{*}+\\tilde{\\mathbf{U}}_{i} and ğ¯i=ğ¯ancâˆ—+ğ¯~isubscriptğ¯ğ‘–superscriptsubscriptğ¯ancsubscript~ğ¯ğ‘–\\mathbf{v}_{i}=\\mathbf{v}_{\\text{anc}}^{*}+\\tilde{\\mathbf{v}}_{i} respectively, where each element of ğ”~isubscript~ğ”ğ‘–\\tilde{\\mathbf{U}}_{i} and ğ¯~isubscript~ğ¯ğ‘–\\tilde{\\mathbf{v}}_{i} follows distribution Uâ€‹[âˆ’d2,d2]ğ‘ˆğ‘‘2ğ‘‘2U[-\\frac{d}{2},\\frac{d}{2}]. Using ğ¯~isubscript~ğ¯ğ‘–\\tilde{\\mathbf{v}}_{i}, zğ±â€‹(Î±)subscriptğ‘§ğ±ğ›¼z_{\\mathbf{x}}(\\alpha) can be represented asSimilar to EquationÂ 18 and EquationÂ 21, with probability 1âˆ’Î´21ğ›¿21-\\frac{\\delta}{2}, EquationÂ 43 can be bound withThen similar to EquationÂ 28, with probability 1âˆ’Î´21ğ›¿21-\\frac{\\delta}{2}, EquationÂ 47 can be bound withSet the minimum of â„’isubscriptâ„’ğ‘–\\mathcal{L}_{i} closest to ğ°ancâˆ—superscriptsubscriptğ°anc\\mathbf{w}_{\\text{anc}}^{*} is ğ°anc,iâˆ—superscriptsubscriptğ°ancğ‘–\\mathbf{w}_{\\text{anc},i}^{*}. For supÎ±â„’iâ€‹(Î±â€‹ğ°i+(1âˆ’Î±)â€‹ğ°ancâˆ—)<Ïµsubscriptsupremumğ›¼subscriptâ„’ğ‘–ğ›¼subscriptğ°ğ‘–1ğ›¼superscriptsubscriptğ°ancitalic-Ïµ\\sup_{\\alpha}\\mathcal{L}_{i}(\\alpha\\mathbf{w}_{i}+(1-\\alpha)\\mathbf{w}_{\\text{anc}}^{*})<\\epsilon holds with probability 1âˆ’Î´1ğ›¿1-\\delta, then with probability 1âˆ’Î´1ğ›¿1-\\delta we have,EquationÂ 52 is due to the assumption that â„’â€‹(â‹…)â„’â‹…\\mathcal{L}(\\cdot) is Î³ğ›¾\\gamma-smooth. By Lemma 3.3, we have d<dÏµ+Î³â€‹Î“2(1âˆ’Î´)1Sğ‘‘subscriptğ‘‘italic-Ïµğ›¾superscriptÎ“2superscript1ğ›¿1ğ‘†d<\\frac{d_{\\epsilon+\\gamma\\Gamma^{2}}}{(1-\\delta)^{\\frac{1}{S}}} with S=hâ€‹l+hğ‘†â„ğ‘™â„S=hl+h. Then |zğ±â€‹(Î±)|subscriptğ‘§ğ±ğ›¼|z_{\\mathbf{x}}(\\alpha)| can be bounded asNow we turn to calculate the bound of the loss barrier Blâ€‹oâ€‹sâ€‹sâ€‹({ğ°i}i=1K)subscriptğµğ‘™ğ‘œğ‘ ğ‘ superscriptsubscriptsubscriptğ°ğ‘–ğ‘–1ğ¾B_{loss}(\\{\\mathbf{w}_{i}\\}_{i=1}^{K}). For the loss function Lâ€‹(â‹…,y)ğ¿â‹…ğ‘¦L(\\cdot,y) is convex and 1-Lipschitz, similar to EquationÂ 37, we have:where the expectation is with respect to the server dataset. Then use the bound of zâ€‹(Î±)ğ‘§ğ›¼z(\\alpha), with probability 1âˆ’Î´1ğ›¿1-\\delta, we haveâˆLinear Mode Connectivity. Linear mode connectivity (LMC) refers to the phenomenon that there exists a loss (energy) barrier along the linear interpolation path of two networks, in the cases where i) the two networks have the same initialization and are trained on the same dataset but with different random seeds (data shuffles) or augmentationsÂ (Ainsworth etÂ al., 2022); ii) the two networks are with different initializations but are trained on the same datasetÂ (Entezari etÂ al., 2022); iii) the two networks are the initial network and the final trained networkÂ (Vlaar & Frankle, 2022).\nIn our paper, the transitivity of LMC can be applied to i), ii), and iii), and especially, the two trained models can have different initializations.\nSpecifically, Adilova etÂ al. (2023) examines layer-wise LMC, and finds that there may be no barriers in the layer-wise manner. Frankle etÂ al. (2020) connects linear mode connectivity with the lottery ticket hypothesis and finds better connectivity can result in better pruning performances. Vlaar & Frankle (2022) studies the relationship between generalization and the initial-to-final linear mode connectivity. Zhao etÂ al. (2020) bridges mode connectivity and adversarial robustness. Some works try to extend mode connectivity beyond â€œlinearâ€, e.g., searching for a non-linear low-loss pathÂ (Draxler etÂ al., 2018) or studying mode connectivity under spurious attributesÂ (Lubana etÂ al., 2023).Studying the barriers in LMC is an important direction of LMC. Previous works find that there may be no barriers between different modes, but the connected regions may be non-linearÂ (Draxler etÂ al., 2018; Garipov etÂ al., 2018). In Garipov etÂ al. (2018), the authors propose to find paths along modes by learning Polygonal chain and Bezier curve. Also, Nudged Elastic Band can also be used to find that connected pathsÂ (Draxler etÂ al., 2018). In Wortsman etÂ al. (2021), the authors propose to learn connected but diverse low-loss subspaces for efficient ensembling. Our work about the transitivity of LMC is inspired by the previous works of learning connected paths. However, instead of learning diverse modes for ensembling, we aim to use the anchor model to improve the linear connectivity between two independent modes.Generalization of Federated Learning. Generalization and personalization are two important goals of federated learning systemsÂ (Chen & Chao, 2022; Li etÂ al., 2023a, b; Yuan etÂ al., 2022). Previous works study and understand the property and nature of generalization in FL. In Yuan etÂ al. (2022), the authors rethink the previous definition of generalization by considering the data distributions of non-participated clients as the participation gap and propose a new data split method based on the insight. In the paper of FedRoDÂ (Chen & Chao, 2022), the authors claim that generalization and personalization are not conflicted; instead, improving generalization is the basis for better personalization.Some works aim to improve generalization from both the server and client sides. For the clients, sharpness-aware minimization methods are introduced at the local to find a flatter minimum of local solvers for better generalizationÂ (Caldarola etÂ al., 2022; Qu etÂ al., 2022). Global sharpness-aware minimization is also consideredÂ (Dai etÂ al., 2023). In addition, previous literature seeks to tackle local heterogeneity to improve generalization, and methods like proximal termsÂ (Li etÂ al., 2020a), dynamic regularizationÂ (Acar etÂ al., 2020), variance reductionÂ (Karimireddy etÂ al., 2020), logit calibrationÂ (Zhang etÂ al., 2022), fixed classifierÂ (Li etÂ al., 2023b), and balanced lossÂ (Chen & Chao, 2022) are devised. For the server, weighted aggregation approaches to de-bias local updatesÂ (Wang etÂ al., 2020) or heterogeneityÂ (Ye etÂ al., 2023) can improve generalization. Recently, global weight shrinking that sets smaller aggregation weights has been studied for unleashing the potential of weight regularization in boosting the generalization of FLÂ (Li etÂ al., 2023a).",
        "references": [
            [
                "We first give the hypothesis on the transitivity of LMC.",
                "Transitivity of linear mode connectivity (informal).",
                " There are three models ",
                "{",
                "ğ°",
                "1",
                ",",
                "ğ°",
                "2",
                ",",
                "ğ°",
                "anc",
                "âˆ—",
                "}",
                "subscript",
                "ğ°",
                "1",
                "subscript",
                "ğ°",
                "2",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "\\{\\mathbf{w}_{1},\\mathbf{w}_{2},{\\mathbf{w}_{\\text{anc}}^{*}}\\}",
                ". If the linear mode connectivity between ",
                "ğ°",
                "1",
                "subscript",
                "ğ°",
                "1",
                "\\mathbf{w}_{1}",
                " and ",
                "ğ°",
                "anc",
                "âˆ—",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ", as well as the one between ",
                "ğ°",
                "2",
                "subscript",
                "ğ°",
                "2",
                "\\mathbf{w}_{2}",
                " and ",
                "ğ°",
                "anc",
                "âˆ—",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ", are independently improved, then, the linear mode connectivity between ",
                "ğ°",
                "1",
                "subscript",
                "ğ°",
                "1",
                "\\mathbf{w}_{1}",
                " and ",
                "ğ°",
                "2",
                "subscript",
                "ğ°",
                "2",
                "\\mathbf{w}_{2}",
                " is also improved.",
                "We make a theoretical analysis to prove the transitivity of LMC. We make the assumption below, following the Assumption 7 in ",
                "Ferbach etÂ al. (",
                "2023",
                ")",
                " and the Assumption 1 in ",
                "Li etÂ al. (",
                "2019",
                ")",
                ".",
                "âˆ€",
                "y",
                "âˆˆ",
                "ğ•",
                "for-all",
                "ğ‘¦",
                "ğ•",
                "\\forall y\\in\\mathbb{Y}",
                ", the loss function ",
                "L",
                "â€‹",
                "(",
                "â‹…",
                ",",
                "y",
                ")",
                "ğ¿",
                "â‹…",
                "ğ‘¦",
                "L\\mathbb{(}\\cdot,y)",
                " is convex and 1-Lipschitz for each ",
                "y",
                "ğ‘¦",
                "y",
                " and the loss ",
                "â„’",
                "â€‹",
                "(",
                "â‹…",
                ")",
                "â„’",
                "â‹…",
                "\\mathcal{L}(\\cdot)",
                " is ",
                "Î³",
                "ğ›¾",
                "\\gamma",
                "-smooth, where ",
                "â„’",
                "â€‹",
                "(",
                "ğ°",
                ")",
                "=",
                "ğ”¼",
                "â€‹",
                "[",
                "L",
                "â€‹",
                "(",
                "f",
                "ğ°",
                "â€‹",
                "(",
                "x",
                ")",
                ",",
                "y",
                ")",
                "]",
                "â„’",
                "ğ°",
                "ğ”¼",
                "delimited-[]",
                "ğ¿",
                "subscript",
                "ğ‘“",
                "ğ°",
                "ğ‘¥",
                "ğ‘¦",
                "\\mathcal{L}(\\mathbf{w})=\\mathbb{E}[L(f_{\\mathbf{w}}(x),y)]",
                " and the expectation ",
                "ğ”¼",
                "ğ”¼",
                "\\mathbb{E}",
                " is taken over the dataset.",
                "Set the uniform and bounded domain for network ",
                "ğ°",
                "ğ°",
                "\\mathbf{w}",
                " as ",
                "â„°",
                "Ïµ",
                "=",
                "{",
                "ğ°",
                "âˆˆ",
                "Î©",
                "|",
                "â„’",
                "â€‹",
                "(",
                "ğ°",
                ")",
                "<",
                "Ïµ",
                "}",
                "subscript",
                "â„°",
                "italic-Ïµ",
                "conditional-set",
                "ğ°",
                "Î©",
                "â„’",
                "ğ°",
                "italic-Ïµ",
                "\\mathcal{E}_{\\epsilon}=\\{\\mathbf{w}\\in\\Omega|\\mathcal{L}(\\mathbf{w})<\\epsilon\\}",
                ". Define a random event ",
                "D",
                "Ïµ",
                "â€‹",
                "(",
                "ğ°",
                "anc",
                "âˆ—",
                ")",
                "subscript",
                "ğ·",
                "italic-Ïµ",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})",
                " as ",
                "D",
                "Ïµ",
                "â€‹",
                "(",
                "ğ°",
                "anc",
                "âˆ—",
                ")",
                "=",
                "{",
                "ğ°",
                "âˆˆ",
                "â„°",
                "Ïµ",
                "|",
                "âˆ€",
                "Î±",
                "âˆˆ",
                "[",
                "0",
                ",",
                "1",
                "]",
                ",",
                "â„’",
                "â€‹",
                "(",
                "Î±",
                "â€‹",
                "ğ°",
                "anc",
                "âˆ—",
                "+",
                "(",
                "1",
                "âˆ’",
                "Î±",
                ")",
                "â€‹",
                "ğ°",
                ")",
                "â‰¤",
                "Ïµ",
                "}",
                "subscript",
                "ğ·",
                "italic-Ïµ",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "conditional-set",
                "ğ°",
                "subscript",
                "â„°",
                "italic-Ïµ",
                "formulae-sequence",
                "for-all",
                "ğ›¼",
                "0",
                "1",
                "â„’",
                "ğ›¼",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "1",
                "ğ›¼",
                "ğ°",
                "italic-Ïµ",
                "D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})=\\{\\mathbf{w}\\in\\mathcal{E}_{\\epsilon}|\\forall\\alpha\\in[0,1],\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w})\\leq\\epsilon\\}",
                ". Consider an anchor model ",
                "ğ°",
                "anc",
                "âˆ—",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                " and an arbitrary network ",
                "ğ°",
                "ğ°",
                "\\mathbf{w}",
                " and for ",
                "Ïµ",
                ">",
                "0",
                "italic-Ïµ",
                "0",
                "\\epsilon>0",
                ". Then for ",
                "â€–",
                "ğ°",
                "âˆ’",
                "ğ°",
                "anc",
                "âˆ—",
                "â€–",
                "âˆ",
                "â‰¤",
                "d",
                "2",
                "subscript",
                "norm",
                "ğ°",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "ğ‘‘",
                "2",
                "\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq\\frac{d}{2}",
                ",",
                "where ",
                "d",
                "Ïµ",
                "=",
                "|",
                "â„°",
                "Ïµ",
                "|",
                "1",
                "S",
                "subscript",
                "ğ‘‘",
                "italic-Ïµ",
                "superscript",
                "subscript",
                "â„°",
                "italic-Ïµ",
                "1",
                "ğ‘†",
                "d_{\\epsilon}=\\left|\\mathcal{E}_{\\epsilon}\\right|^{\\frac{1}{S}}",
                " represents the average diameter of region ",
                "â„°",
                "Ïµ",
                "subscript",
                "â„°",
                "italic-Ïµ",
                "\\mathcal{E}_{\\epsilon}",
                ", ",
                "S",
                "ğ‘†",
                "S",
                " represents the number of parameters of the neural network and the equality holds if and only if ",
                "â„°",
                "Ïµ",
                "âŠ‚",
                "{",
                "ğ°",
                "|",
                "â€–",
                "ğ°",
                "âˆ’",
                "ğ°",
                "anc",
                "âˆ—",
                "â€–",
                "âˆ",
                "â‰¤",
                "d",
                "}",
                "subscript",
                "â„°",
                "italic-Ïµ",
                "conditional-set",
                "ğ°",
                "subscript",
                "norm",
                "ğ°",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "ğ‘‘",
                "\\mathcal{E}_{\\epsilon}\\subset\\{\\mathbf{w}|\\|\\mathbf{w}-{\\mathbf{w}_{\\text{anc}}^{*}}\\|_{\\infty}\\leq d\\}",
                " is a star domain centered at ",
                "ğ°",
                "anc",
                "âˆ—",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ". Thus, when ",
                "P",
                "(",
                "D",
                "Ïµ",
                "(",
                "ğ°",
                "anc",
                "âˆ—",
                ")",
                ")",
                ")",
                ">",
                "1",
                "âˆ’",
                "Î´",
                "P(D_{\\epsilon}({\\mathbf{w}_{\\text{anc}}^{*}})))>1-\\delta",
                ", it holds ",
                "d",
                "<",
                "d",
                "Ïµ",
                "(",
                "1",
                "âˆ’",
                "Î´",
                ")",
                "1",
                "S",
                "ğ‘‘",
                "subscript",
                "ğ‘‘",
                "italic-Ïµ",
                "superscript",
                "1",
                "ğ›¿",
                "1",
                "ğ‘†",
                "d<{\\frac{d_{\\epsilon}}{(1-\\delta)^{\\frac{1}{S}}}}",
                ".",
                "This lemma links the distance between parameters to LMC, describing that the greater the probability of LMC (i.e., a small loss barrier) existing between the network ",
                "ğ°",
                "ğ°",
                "\\mathbf{w}",
                " and the anchor model ",
                "ğ°",
                "anc",
                "âˆ—",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ", the smaller the distance should be between ",
                "ğ°",
                "ğ°",
                "\\mathbf{w}",
                " and ",
                "ğ°",
                "anc",
                "âˆ—",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}",
                ".",
                "Then, we provide the following theorem.",
                "We define a two-layer neural network with ReLU activation, and the function is ",
                "f",
                "ğ¯",
                ",",
                "ğ”",
                "â€‹",
                "(",
                "ğ±",
                ")",
                "=",
                "ğ¯",
                "âŠ¤",
                "â€‹",
                "Ïƒ",
                "â€‹",
                "(",
                "ğ”ğ±",
                ")",
                "subscript",
                "ğ‘“",
                "ğ¯",
                "ğ”",
                "ğ±",
                "superscript",
                "ğ¯",
                "top",
                "ğœ",
                "ğ”ğ±",
                "f_{\\mathbf{v},\\mathbf{U}}(\\mathbf{x})=\\mathbf{v}^{\\top}\\sigma(\\mathbf{U}\\mathbf{x})",
                " where ",
                "Ïƒ",
                "â€‹",
                "(",
                "â‹…",
                ")",
                "ğœ",
                "â‹…",
                "\\sigma(\\cdot)",
                " is the ReLU activation function. ",
                "ğ¯",
                "âˆˆ",
                "â„",
                "h",
                "ğ¯",
                "superscript",
                "â„",
                "â„",
                "\\mathbf{v}\\in\\mathbb{R}^{h}",
                " and ",
                "ğ”",
                "âˆˆ",
                "â„",
                "h",
                "Ã—",
                "l",
                "ğ”",
                "superscript",
                "â„",
                "â„",
                "ğ‘™",
                "\\mathbf{U}\\in\\mathbb{R}^{h\\times l}",
                " are parameters",
                "1",
                "1",
                "1",
                "For simplicity and without loss of generality, we omit the bias terms.",
                " and ",
                "ğ±",
                "âˆˆ",
                "â„",
                "l",
                "ğ±",
                "superscript",
                "â„",
                "ğ‘™",
                "\\mathbf{x}\\in\\mathbb{R}^{l}",
                " is the input which is taken from ",
                "ğ•",
                "=",
                "{",
                "ğ±",
                "âˆˆ",
                "â„",
                "l",
                "|",
                "â€–",
                "ğ±",
                "â€–",
                "2",
                "<",
                "b",
                "}",
                "ğ•",
                "conditional-set",
                "ğ±",
                "superscript",
                "â„",
                "ğ‘™",
                "subscript",
                "norm",
                "ğ±",
                "2",
                "ğ‘",
                "\\mathbb{X}=\\{\\mathbf{x}\\in\\mathbb{R}^{l}|\\|\\mathbf{x}\\|_{2}<b\\}",
                " uniformly. Denote the deterministic anchor model as ",
                "ğ°",
                "anc",
                "âˆ—",
                "=",
                "{",
                "ğ”",
                "anc",
                "âˆ—",
                ",",
                "ğ¯",
                "anc",
                "âˆ—",
                "}",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "superscript",
                "subscript",
                "ğ”",
                "anc",
                "superscript",
                "subscript",
                "ğ¯",
                "anc",
                "{\\mathbf{w}_{\\text{anc}}^{*}}=\\{\\mathbf{U}_{\\text{anc}}^{*},\\mathbf{v}_{\\text{anc}}^{*}\\}",
                ", with ",
                "â€–",
                "ğ¯",
                "anc",
                "âˆ—",
                "â€–",
                "2",
                "<",
                "d",
                "anc",
                "subscript",
                "norm",
                "superscript",
                "subscript",
                "ğ¯",
                "anc",
                "2",
                "subscript",
                "ğ‘‘",
                "anc",
                "\\|\\mathbf{v}_{\\text{anc}}^{*}\\|_{2}<d_{\\text{anc}}",
                " and consider two different networks ",
                "ğ°",
                "1",
                ",",
                "ğ°",
                "2",
                "subscript",
                "ğ°",
                "1",
                "subscript",
                "ğ°",
                "2",
                "\\mathbf{w}_{1},\\mathbf{w}_{2}",
                " parameterized with ",
                "{",
                "ğ”",
                "1",
                ",",
                "ğ¯",
                "1",
                "}",
                "subscript",
                "ğ”",
                "1",
                "subscript",
                "ğ¯",
                "1",
                "\\{\\mathbf{U}_{1},\\mathbf{v}_{1}\\}",
                " and ",
                "{",
                "ğ”",
                "2",
                ",",
                "ğ¯",
                "2",
                "}",
                "subscript",
                "ğ”",
                "2",
                "subscript",
                "ğ¯",
                "2",
                "\\{\\mathbf{U}_{2},\\mathbf{v}_{2}\\}",
                " respectively. Each element of ",
                "ğ”",
                "1",
                "subscript",
                "ğ”",
                "1",
                "\\mathbf{U}_{1}",
                " and ",
                "ğ”",
                "2",
                "subscript",
                "ğ”",
                "2",
                "\\mathbf{U}_{2}",
                ", ",
                "ğ¯",
                "1",
                "subscript",
                "ğ¯",
                "1",
                "\\mathbf{v}_{1}",
                " and ",
                "ğ¯",
                "2",
                "subscript",
                "ğ¯",
                "2",
                "\\mathbf{v}_{2}",
                " is sampled from a uniform distribution centered at ",
                "ğ”",
                "anc",
                "âˆ—",
                "superscript",
                "subscript",
                "ğ”",
                "anc",
                "\\mathbf{U}_{\\text{anc}}^{*}",
                " and ",
                "ğ¯",
                "anc",
                "âˆ—",
                "superscript",
                "subscript",
                "ğ¯",
                "anc",
                "\\mathbf{v}_{\\text{anc}}^{*}",
                " with an interval length of ",
                "d",
                "ğ‘‘",
                "d",
                ". If with probability ",
                "1",
                "âˆ’",
                "Î´",
                "1",
                "ğ›¿",
                "1-\\delta",
                ", ",
                "sup",
                "Î±",
                "â„’",
                "â€‹",
                "(",
                "Î±",
                "â€‹",
                "ğ°",
                "anc",
                "âˆ—",
                "+",
                "(",
                "1",
                "âˆ’",
                "Î±",
                ")",
                "â€‹",
                "ğ°",
                "1",
                ")",
                "<",
                "Ïµ",
                "subscript",
                "supremum",
                "ğ›¼",
                "â„’",
                "ğ›¼",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "1",
                "ğ›¼",
                "subscript",
                "ğ°",
                "1",
                "italic-Ïµ",
                "\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{1})<\\epsilon",
                " and ",
                "sup",
                "Î±",
                "â„’",
                "â€‹",
                "(",
                "Î±",
                "â€‹",
                "ğ°",
                "anc",
                "âˆ—",
                "+",
                "(",
                "1",
                "âˆ’",
                "Î±",
                ")",
                "â€‹",
                "ğ°",
                "2",
                ")",
                "<",
                "Ïµ",
                "subscript",
                "supremum",
                "ğ›¼",
                "â„’",
                "ğ›¼",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "1",
                "ğ›¼",
                "subscript",
                "ğ°",
                "2",
                "italic-Ïµ",
                "\\sup_{\\alpha}\\mathcal{L}(\\alpha{\\mathbf{w}_{\\text{anc}}^{*}}+(1-\\alpha)\\mathbf{w}_{2})<\\epsilon",
                ", then with probability ",
                "1",
                "âˆ’",
                "Î´",
                "1",
                "ğ›¿",
                "1-\\delta",
                ", it has,",
                "where ",
                "B",
                "l",
                "â€‹",
                "o",
                "â€‹",
                "s",
                "â€‹",
                "s",
                "â€‹",
                "(",
                "ğ°",
                "1",
                ",",
                "ğ°",
                "2",
                ")",
                "subscript",
                "ğµ",
                "ğ‘™",
                "ğ‘œ",
                "ğ‘ ",
                "ğ‘ ",
                "subscript",
                "ğ°",
                "1",
                "subscript",
                "ğ°",
                "2",
                "B_{loss}(\\mathbf{w}_{1},\\mathbf{w}_{2})",
                " is the loss barrier as ",
                "EquationÂ 3",
                ".",
                "The proofs are in ",
                "AppendixÂ B",
                ". ",
                "TheoremÂ 3.5",
                " proves the transitivity of LMC that when ",
                "ğ°",
                "1",
                "subscript",
                "ğ°",
                "1",
                "\\mathbf{w}_{1}",
                " and ",
                "ğ°",
                "2",
                "subscript",
                "ğ°",
                "2",
                "\\mathbf{w}_{2}",
                " have lower LMC barrier with ",
                "ğ°",
                "anc",
                "âˆ—",
                "superscript",
                "subscript",
                "ğ°",
                "anc",
                "\\mathbf{w}_{\\text{anc}}^{*}",
                " (the barrier proxy is ",
                "Ïµ",
                "italic-Ïµ",
                "\\epsilon",
                ") then the barrier between ",
                "ğ°",
                "1",
                "subscript",
                "ğ°",
                "1",
                "\\mathbf{w}_{1}",
                " and ",
                "ğ°",
                "2",
                "subscript",
                "ğ°",
                "2",
                "\\mathbf{w}_{2}",
                " is also reduced and bounded.",
                "Then, we will empirically validate the transitivity."
            ]
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Results in terms of generalization accuracy (%) of global models on four datasets under different data heterogeneity. The best two methods in each setting are highlighted in bold fonts. M=50,E=3formulae-sequenceğ‘€50ğ¸3M=50,E=3. ",
        "table": "<table id=\"S3.T2.11\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.11.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.T2.11.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dataset</span></th>\n<td id=\"S3.T2.11.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S3.T2.11.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Fashion-MNIST</span></td>\n<td id=\"S3.T2.11.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S3.T2.11.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-10</span></td>\n<td id=\"S3.T2.11.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S3.T2.11.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-100</span></td>\n<td id=\"S3.T2.11.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S3.T2.11.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">Tiny-ImageNet</span></td>\n</tr>\n<tr id=\"S3.T2.11.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Non-IID hyper.</span></th>\n<td id=\"S3.T2.11.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S3.T2.11.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.5</span></td>\n<td id=\"S3.T2.11.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S3.T2.11.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.5</span></td>\n<td id=\"S3.T2.11.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S3.T2.11.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.5</span></td>\n<td id=\"S3.T2.11.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S3.T2.11.2.2.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.2.2.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.5</span></td>\n</tr>\n<tr id=\"S3.T2.11.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Local</span></th>\n<td id=\"S3.T2.11.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">76.22Â±0.16</span></td>\n<td id=\"S3.T2.11.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">62.24Â±0.35</span></td>\n<td id=\"S3.T2.11.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">36.69Â±0.10</span></td>\n<td id=\"S3.T2.11.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">29.73Â±0.36</span></td>\n<td id=\"S3.T2.11.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.19Â±0.08</span></td>\n<td id=\"S3.T2.11.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.3.3.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">7.29Â±0.04</span></td>\n<td id=\"S3.T2.11.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">6.47Â±0.12</span></td>\n<td id=\"S3.T2.11.3.3.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.3.3.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">6.09Â±0.02</span></td>\n</tr>\n<tr id=\"S3.T2.11.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S3.T2.11.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.94Â±0.34</span></td>\n<td id=\"S3.T2.11.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.99Â±0.04</span></td>\n<td id=\"S3.T2.11.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.55Â±0.16</span></td>\n<td id=\"S3.T2.11.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.99Â±0.32</span></td>\n<td id=\"S3.T2.11.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.78Â±1.17</span></td>\n<td id=\"S3.T2.11.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.4.4.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">33.49Â±0.90</span></td>\n<td id=\"S3.T2.11.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.43Â±1.39</span></td>\n<td id=\"S3.T2.11.4.4.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.4.4.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.11Â±1.82</span></td>\n</tr>\n<tr id=\"S3.T2.11.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedProx</span></th>\n<td id=\"S3.T2.11.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">10.00Â±0.00</span></td>\n<td id=\"S3.T2.11.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">10.00Â±0.00</span></td>\n<td id=\"S3.T2.11.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.81Â±0.47</span></td>\n<td id=\"S3.T2.11.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.5.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.45Â±0.43</span></td>\n<td id=\"S3.T2.11.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">13.18Â±0.82</span></td>\n<td id=\"S3.T2.11.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.5.5.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">15.62Â±0.40</span></td>\n<td id=\"S3.T2.11.5.5.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.58Â±0.28</span></td>\n<td id=\"S3.T2.11.5.5.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.5.5.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.02Â±0.19</span></td>\n</tr>\n<tr id=\"S3.T2.11.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedDyn</span></th>\n<td id=\"S3.T2.11.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.26Â±0.17</span></td>\n<td id=\"S3.T2.11.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.18Â±0.36</span></td>\n<td id=\"S3.T2.11.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.99Â±0.64</span></td>\n<td id=\"S3.T2.11.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.6.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.73Â±0.31</span></td>\n<td id=\"S3.T2.11.6.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.24Â±0.86</span></td>\n<td id=\"S3.T2.11.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.6.6.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">33.20Â±1.81</span></td>\n<td id=\"S3.T2.11.6.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">30.89Â±0.03</span></td>\n<td id=\"S3.T2.11.6.6.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.6.6.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.63Â±2.68</span></td>\n</tr>\n<tr id=\"S3.T2.11.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SCAFFOLD</span></th>\n<td id=\"S3.T2.11.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.95Â±0.31</span></td>\n<td id=\"S3.T2.11.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.47Â±0.14</span></td>\n<td id=\"S3.T2.11.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.20Â±0.32</span></td>\n<td id=\"S3.T2.11.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.96Â±0.41</span></td>\n<td id=\"S3.T2.11.7.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.06Â±0.94</span></td>\n<td id=\"S3.T2.11.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.7.7.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">33.84Â±0.81</span></td>\n<td id=\"S3.T2.11.7.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.529Â±0.05</span></td>\n<td id=\"S3.T2.11.7.7.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.7.7.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.517Â±0.02</span></td>\n</tr>\n<tr id=\"S3.T2.11.8.8\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.8.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">MOON</span></th>\n<td id=\"S3.T2.11.8.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.95Â±0.09</span></td>\n<td id=\"S3.T2.11.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.8.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.02Â±0.29</span></td>\n<td id=\"S3.T2.11.8.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.24Â±0.65</span></td>\n<td id=\"S3.T2.11.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.8.8.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.41Â±0.31</span></td>\n<td id=\"S3.T2.11.8.8.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.90Â±3.16</span></td>\n<td id=\"S3.T2.11.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.8.8.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">32.64Â±0.08</span></td>\n<td id=\"S3.T2.11.8.8.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.88Â±1.08</span></td>\n<td id=\"S3.T2.11.8.8.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.8.8.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.34Â±0.66</span></td>\n</tr>\n<tr id=\"S3.T2.11.9.9\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.9.9.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedRoD</span></th>\n<td id=\"S3.T2.11.9.9.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.97Â±0.40</span></td>\n<td id=\"S3.T2.11.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.9.9.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.56Â±0.60</span></td>\n<td id=\"S3.T2.11.9.9.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">62.64Â±0.20</span></td>\n<td id=\"S3.T2.11.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.9.9.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">62.56Â±0.46</span></td>\n<td id=\"S3.T2.11.9.9.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.77Â±1.13</span></td>\n<td id=\"S3.T2.11.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.9.9.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.04Â±0.89</span></td>\n<td id=\"S3.T2.11.9.9.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.67Â±1.64</span></td>\n<td id=\"S3.T2.11.9.9.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.9.9.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.55Â±1.56</span></td>\n</tr>\n<tr id=\"S3.T2.11.10.10\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.10.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedLC</span></th>\n<td id=\"S3.T2.11.10.10.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.90Â±0.36</span></td>\n<td id=\"S3.T2.11.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.10.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">86.79Â±0.29</span></td>\n<td id=\"S3.T2.11.10.10.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.49Â±0.17</span></td>\n<td id=\"S3.T2.11.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.10.10.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.97Â±0.35</span></td>\n<td id=\"S3.T2.11.10.10.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.93Â±1.07</span></td>\n<td id=\"S3.T2.11.10.10.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.10.10.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">31.79Â±0.78</span></td>\n<td id=\"S3.T2.11.10.10.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.63Â±1.62</span></td>\n<td id=\"S3.T2.11.10.10.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.10.10.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">25.47Â±1.84</span></td>\n</tr>\n<tr id=\"S3.T2.11.11.11\" class=\"ltx_tr\">\n<th id=\"S3.T2.11.11.11.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.11.11.11.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedSAM</span></th>\n<td id=\"S3.T2.11.11.11.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.41Â±0.49</span></td>\n<td id=\"S3.T2.11.11.11.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.11.11.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.62Â±0.30</span></td>\n<td id=\"S3.T2.11.11.11.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.10Â±0.41</span></td>\n<td id=\"S3.T2.11.11.11.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.11.11.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.02Â±0.15</span></td>\n<td id=\"S3.T2.11.11.11.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.24Â±1.58</span></td>\n<td id=\"S3.T2.11.11.11.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S3.T2.11.11.11.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">33.64Â±1.31</span></td>\n<td id=\"S3.T2.11.11.11.8\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">31.23Â±0.16</span></td>\n<td id=\"S3.T2.11.11.11.9\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.11.11.11.9.1\" class=\"ltx_text\" style=\"font-size:80%;\">30.44Â±0.97</span></td>\n</tr>\n<tr id=\"S3.T2.11.12.12\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S3.T2.11.12.12.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.12.12.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci</span></th>\n<td id=\"S3.T2.11.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">88.85Â±0.11</span></td>\n<td id=\"S3.T2.11.12.12.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.12.12.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">88.30Â±0.39</span></td>\n<td id=\"S3.T2.11.12.12.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.11Â±0.11</span></td>\n<td id=\"S3.T2.11.12.12.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.12.12.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.80Â±0.22</span></td>\n<td id=\"S3.T2.11.12.12.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">28.98Â±1.33</span></td>\n<td id=\"S3.T2.11.12.12.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S3.T2.11.12.12.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">33.96Â±1.18</span></td>\n<td id=\"S3.T2.11.12.12.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">36.46Â±0.40</span></td>\n<td id=\"S3.T2.11.12.12.9\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.11.12.12.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">33.61Â±0.60</span></td>\n</tr>\n<tr id=\"S3.T2.11.13.13\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S3.T2.11.13.13.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S3.T2.11.13.13.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci+</span></th>\n<td id=\"S3.T2.11.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">89.38Â±0.14</span></td>\n<td id=\"S3.T2.11.13.13.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S3.T2.11.13.13.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">88.61Â±0.40</span></td>\n<td id=\"S3.T2.11.13.13.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">68.11Â±0.27</span></td>\n<td id=\"S3.T2.11.13.13.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S3.T2.11.13.13.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">66.44Â±0.69</span></td>\n<td id=\"S3.T2.11.13.13.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">28.86Â±1.09</span></td>\n<td id=\"S3.T2.11.13.13.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S3.T2.11.13.13.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">34.25Â±1.12</span></td>\n<td id=\"S3.T2.11.13.13.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">37.42Â±0.52</span></td>\n<td id=\"S3.T2.11.13.13.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.11.13.13.9.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">34.80Â±0.35</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Results under pretrained language models. We use 6 datasets from GLUEÂ (Wang etÂ al., 2019) benchmark for finetuning pretrained language models. For each dataset, we randomly split the data into several clients and conduct finetuning using low-rank adaption (LoRA), and the pretrained model is RoBERTa-baseÂ (Liu etÂ al., 2019). It is notable that some language tasks are not classifications, so FedRoD, FedLC, and FedGuCci+, which rely on classification loss, are not applicable. The results are in TableÂ 3, where our FedGuCci reaches promising performances over existing methods. It is observed that some methods that are superior in TableÂ 2 have worse performances in pretrained language models, e.g., FedDyn, while our FedGuCci keeps steady advantages."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Results of pretrained language models on natural language processing (GLUE benchmark).",
        "table": "<table id=\"S4.T3.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.4.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S4.T3.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Methods/Tasks</span></th>\n<td id=\"S4.T3.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">SST-2</span></td>\n<td id=\"S4.T3.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">MRPC</span></td>\n<td id=\"S4.T3.4.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">CoLA</span></td>\n<td id=\"S4.T3.4.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">QNLI</span></td>\n<td id=\"S4.T3.4.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">RTE</span></td>\n<td id=\"S4.T3.4.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">STS-B</span></td>\n<td id=\"S4.T3.4.1.1.8\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S4.T3.4.1.1.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">AVG</span></td>\n</tr>\n<tr id=\"S4.T3.4.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Local</span></th>\n<td id=\"S4.T3.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.55Â±0.19</span></td>\n<td id=\"S4.T3.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.38Â±0.37</span></td>\n<td id=\"S4.T3.4.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">47.98Â±1.01</span></td>\n<td id=\"S4.T3.4.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.66Â±0.10</span></td>\n<td id=\"S4.T3.4.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">55.69Â±1.03</span></td>\n<td id=\"S4.T3.4.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.2.2.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.11Â±0.36</span></td>\n<td id=\"S4.T3.4.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.2.2.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">75.40Â±0.51</span></td>\n</tr>\n<tr id=\"S4.T3.4.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S4.T3.4.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.79Â±0.24</span></td>\n<td id=\"S4.T3.4.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.17Â±0.38</span></td>\n<td id=\"S4.T3.4.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">53.86Â±0.70</span></td>\n<td id=\"S4.T3.4.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.52Â±0.14</span></td>\n<td id=\"S4.T3.4.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">68.63Â±1.53</span></td>\n<td id=\"S4.T3.4.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.3.3.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">88.61Â±0.34</span></td>\n<td id=\"S4.T3.4.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.3.3.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.76Â±0.56</span></td>\n</tr>\n<tr id=\"S4.T3.4.4.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedProx</span></th>\n<td id=\"S4.T3.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">50.88Â±0.00</span></td>\n<td id=\"S4.T3.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">67.26Â±0.75</span></td>\n<td id=\"S4.T3.4.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">00.00Â±0.00</span></td>\n<td id=\"S4.T3.4.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">50.55Â±0.98</span></td>\n<td id=\"S4.T3.4.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">49.39Â±3.42</span></td>\n<td id=\"S4.T3.4.4.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.4.4.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">00.00Â±0.00</span></td>\n<td id=\"S4.T3.4.4.4.8\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T3.4.4.4.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">54.52Â±1.71</span></td>\n</tr>\n<tr id=\"S4.T3.4.5.5\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T3.4.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedDyn</span></th>\n<td id=\"S4.T3.4.5.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">91.19Â±0.85</span></td>\n<td id=\"S4.T3.4.5.5.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.80Â±0.41</span></td>\n<td id=\"S4.T3.4.5.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">55.49Â±1.02</span></td>\n<td id=\"S4.T3.4.5.5.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">85.51Â±0.54</span></td>\n<td id=\"S4.T3.4.5.5.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.40Â±3.89</span></td>\n<td id=\"S4.T3.4.5.5.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.5.5.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">24.75Â±9.38</span></td>\n<td id=\"S4.T3.4.5.5.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.5.5.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">67.19Â±2.68</span></td>\n</tr>\n<tr id=\"S4.T3.4.6.6\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T3.4.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SCAFFOLD</span></th>\n<td id=\"S4.T3.4.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">92.75Â±0.12</span></td>\n<td id=\"S4.T3.4.6.6.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.11Â±0.65</span></td>\n<td id=\"S4.T3.4.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">54.28Â±0.31</span></td>\n<td id=\"S4.T3.4.6.6.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">84.73Â±0.16</span></td>\n<td id=\"S4.T3.4.6.6.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">69.24Â±2.76</span></td>\n<td id=\"S4.T3.4.6.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.6.6.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">88.31Â±0.31</span></td>\n<td id=\"S4.T3.4.6.6.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.6.6.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">78.90Â±0.72</span></td>\n</tr>\n<tr id=\"S4.T3.4.7.7\" class=\"ltx_tr\">\n<th id=\"S4.T3.4.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T3.4.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedSAM</span></th>\n<td id=\"S4.T3.4.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">92.79Â±0.14</span></td>\n<td id=\"S4.T3.4.7.7.3\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">84.81Â±0.08</span></td>\n<td id=\"S4.T3.4.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">53.25Â±0.43</span></td>\n<td id=\"S4.T3.4.7.7.5\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">82.13Â±0.34</span></td>\n<td id=\"S4.T3.4.7.7.6\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.6.1\" class=\"ltx_text\" style=\"font-size:80%;\">68.14Â±2.09</span></td>\n<td id=\"S4.T3.4.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T3.4.7.7.7.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.71Â±0.42</span></td>\n<td id=\"S4.T3.4.7.7.8\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T3.4.7.7.8.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.14Â±0.58</span></td>\n</tr>\n<tr id=\"S4.T3.4.8.8\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T3.4.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.8.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci</span></th>\n<td id=\"S4.T3.4.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">93.22Â±0.20</span></td>\n<td id=\"S4.T3.4.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">85.77Â±0.44</span></td>\n<td id=\"S4.T3.4.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">55.38Â±0.44</span></td>\n<td id=\"S4.T3.4.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">89.40Â±0.40</span></td>\n<td id=\"S4.T3.4.8.8.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.6.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">70.96Â±1.60</span></td>\n<td id=\"S4.T3.4.8.8.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S4.T3.4.8.8.7.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">89.25Â±0.44</span></td>\n<td id=\"S4.T3.4.8.8.8\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.4.8.8.8.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">80.66Â±0.59</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Results under pretrained language models. We use 6 datasets from GLUEÂ (Wang etÂ al., 2019) benchmark for finetuning pretrained language models. For each dataset, we randomly split the data into several clients and conduct finetuning using low-rank adaption (LoRA), and the pretrained model is RoBERTa-baseÂ (Liu etÂ al., 2019). It is notable that some language tasks are not classifications, so FedRoD, FedLC, and FedGuCci+, which rely on classification loss, are not applicable. The results are in TableÂ 3, where our FedGuCci reaches promising performances over existing methods. It is observed that some methods that are superior in TableÂ 2 have worse performances in pretrained language models, e.g., FedDyn, while our FedGuCci keeps steady advantages.",
            "CIFAR-10Â (Krizhevsky etÂ al., 2009) consists of 60,000 32x32 color images, evenly distributed among 10 different classes, including airplanes, automobiles, birds, cats, etc., each represented by 6,000 images. The dataset is split into 50,000 training images and 10,000 test images.\nFashionMNISTÂ (Xiao etÂ al., 2017) is designed as an advanced replacement for the MNIST dataset, suitable for benchmarking machine learning models. It comprises 70,000 images divided into 60,000 training samples and 10,000 test samples. Each image is a 28x28 grayscale representation of fashion items from 10 different classes, such as shirts, trousers, sneakers, etc.\nThe CIFAR-100 datasetÂ (Krizhevsky etÂ al., 2009) is similar to CIFAR-10 but more challenging, containing 100 different classes grouped into 20 superclasses. It includes 60,000 32x32 color images, with 600 images per class, divided into 50,000 training images and 10,000 test images. This dataset is primarily used for developing and evaluating more sophisticated image classification models.\nTinyImageNet TinyImageNet is a reduced-scale version of the renowned ImageNet dataset, which comprises a total of 200 classes. The dataset is structured into training, validation, and test sets, with 200,000 training images, 20,000 validation images, and 20,000 test images.\nThe GLUE benchmark is a compilation of 9 datasets for evaluating natural language understanding systems. Tasks are framed as either single-sentence classification or sentence-pair classification tasks. GLUE includes MNLI (inference, (Williams etÂ al., 2017)), MRPC (paraphrase detection, (Socher etÂ al., 2013)), MRPC (paraphrase detection, (Dolan & Brockett, 2005)), CoLA (linguistic acceptability, (Warstadt etÂ al., 2019)), QNLI (inference, (Rajpurkar etÂ al., 2018)), QQP (question-answering), RTE (inference), WNLI (inference), and STS-B (textual similarity, (Cer etÂ al., 2017)). Due to high computation costs, we only used SST2, MRPC, CoLA, QNLI, RTE, and STS-B for evaluation. For the replication in TableÂ 3, we report results on the development sets after fine-tuning the pretrained models on the corresponding single-task training data. Our fine-tuning approach is LoRA(Hu etÂ al., 2021)."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Results on different numbers of clients and participation ratios. Non-IID hyper. is 1.0, and the dataset is CIFAR-10.",
        "table": "<table id=\"S4.T4.2.2\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><math id=\"S4.T4.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S4.T4.1.1.1.1.m1.1a\"><mi mathsize=\"80%\" id=\"S4.T4.1.1.1.1.m1.1.1\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.m1.1b\"><ci id=\"S4.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1\">ğ‘€</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.m1.1c\">M</annotation></semantics></math></th>\n<td id=\"S4.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S4.T4.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">100</span></td>\n<td id=\"S4.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S4.T4.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">200</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><math id=\"S4.T4.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\rho\" display=\"inline\"><semantics id=\"S4.T4.2.2.2.1.m1.1a\"><mi mathsize=\"80%\" id=\"S4.T4.2.2.2.1.m1.1.1\" xref=\"S4.T4.2.2.2.1.m1.1.1.cmml\">Ï</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.1.m1.1b\"><ci id=\"S4.T4.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.1.m1.1.1\">ğœŒ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.1.m1.1c\">\\rho</annotation></semantics></math></th>\n<td id=\"S4.T4.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.3</span></td>\n<td id=\"S4.T4.2.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.6</span></td>\n<td id=\"S4.T4.2.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.3</span></td>\n<td id=\"S4.T4.2.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">0.6</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.3.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Local</span></th>\n<td id=\"S4.T4.2.2.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.3.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.91Â±0.24</span></td>\n<td id=\"S4.T4.2.2.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.3.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">27.53Â±0.10</span></td>\n<td id=\"S4.T4.2.2.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.3.1.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.39Â±0.18</span></td>\n<td id=\"S4.T4.2.2.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.3.1.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">23.20Â±0.22</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.4.2\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.4.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S4.T4.2.2.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.4.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.98Â±0.84</span></td>\n<td id=\"S4.T4.2.2.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.4.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.41Â±0.55</span></td>\n<td id=\"S4.T4.2.2.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.4.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.37Â±0.79</span></td>\n<td id=\"S4.T4.2.2.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.4.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.15Â±1.01</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.5.3\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.5.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.5.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedProx</span></th>\n<td id=\"S4.T4.2.2.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.5.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">52.43Â±0.66</span></td>\n<td id=\"S4.T4.2.2.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.5.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">52.79Â±0.73</span></td>\n<td id=\"S4.T4.2.2.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.5.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">44.63Â±0.95</span></td>\n<td id=\"S4.T4.2.2.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.5.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">44.96Â±0.78</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.6.4\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.6.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T4.2.2.6.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedRoD</span></th>\n<td id=\"S4.T4.2.2.6.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.6.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.15Â±0.05</span></td>\n<td id=\"S4.T4.2.2.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.6.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">60.30Â±0.02</span></td>\n<td id=\"S4.T4.2.2.6.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.6.4.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">58.01Â±0.92</span></td>\n<td id=\"S4.T4.2.2.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.6.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">57.63Â±1.44</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.7.5\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.7.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T4.2.2.7.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedLC</span></th>\n<td id=\"S4.T4.2.2.7.5.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.7.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.70Â±0.69</span></td>\n<td id=\"S4.T4.2.2.7.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.7.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">63.24Â±0.70</span></td>\n<td id=\"S4.T4.2.2.7.5.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.7.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">60.99Â±0.66</span></td>\n<td id=\"S4.T4.2.2.7.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.7.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">60.67Â±0.81</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.8.6\" class=\"ltx_tr\">\n<th id=\"S4.T4.2.2.8.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S4.T4.2.2.8.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedSAM</span></th>\n<td id=\"S4.T4.2.2.8.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.8.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.87Â±0.58</span></td>\n<td id=\"S4.T4.2.2.8.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.8.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.45Â±0.22</span></td>\n<td id=\"S4.T4.2.2.8.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T4.2.2.8.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">62.33Â±0.56</span></td>\n<td id=\"S4.T4.2.2.8.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S4.T4.2.2.8.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">61.93Â±0.90</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.9.7\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T4.2.2.9.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.9.7.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci</span></th>\n<td id=\"S4.T4.2.2.9.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.9.7.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.02Â±0.41</span></td>\n<td id=\"S4.T4.2.2.9.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.9.7.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">64.54Â±0.41</span></td>\n<td id=\"S4.T4.2.2.9.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S4.T4.2.2.9.7.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">62.37Â±0.83</span></td>\n<td id=\"S4.T4.2.2.9.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T4.2.2.9.7.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">62.13Â±0.63</span></td>\n</tr>\n<tr id=\"S4.T4.2.2.10.8\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S4.T4.2.2.10.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S4.T4.2.2.10.8.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci+</span></th>\n<td id=\"S4.T4.2.2.10.8.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.2.2.10.8.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.34Â±0.21</span></td>\n<td id=\"S4.T4.2.2.10.8.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T4.2.2.10.8.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">65.50Â±0.35</span></td>\n<td id=\"S4.T4.2.2.10.8.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T4.2.2.10.8.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">63.29Â±0.71</span></td>\n<td id=\"S4.T4.2.2.10.8.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S4.T4.2.2.10.8.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">63.93Â±0.81</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In the study of LMC, different modes are trained on the ",
                "same",
                " dataset but with different random seeds or initializationsÂ ",
                "(Entezari etÂ al., ",
                "2022",
                ")",
                ". However, in FL, clients have ",
                "heterogeneous",
                " data, and it is found that data heterogeneity of clients will cause different curvatures of local loss landscapesÂ ",
                "(Zhou etÂ al., ",
                "2023",
                ")",
                ", making the connectivity worse. Therefore, aligning local loss landscapes is essential for better performances of the connectivity loss. In this subsection, we incorporate previous techniques in FedGuCci to align local loss landscapes and propose FedGuCci+.",
                "Bias reduction.",
                " In FL, class imbalance (a.k.a. label skew) is a main cause of data heterogeneity, and previous works propose logit calibrationÂ ",
                "(Zhang etÂ al., ",
                "2022",
                ")",
                ", balanced softmaxÂ ",
                "(Chen & Chao, ",
                "2022",
                ")",
                ", and other techniquesÂ ",
                "(Li etÂ al., ",
                "2023b",
                "; Acar etÂ al., ",
                "2020",
                ")",
                " for reducing the bias caused by class imbalance. Here, we introduce the logit calibration technique used in FedLCÂ ",
                "(Zhang etÂ al., ",
                "2022",
                ")",
                " for bias reduction. The main idea of logit calibration is to add additional terms to the logits to balance the overall class distributions. From ",
                "FigureÂ 5",
                "Â (b), it demonstrates that logit calibration and other bias reduction methods can align the landscapes by making the local objectives more consistent.",
                "Flatter minima.",
                " Sharpness-aware minimizationÂ ",
                "(Foret etÂ al., ",
                "2021",
                "; Kwon etÂ al., ",
                "2021",
                ")",
                " (SAM) find flatter minima to improve generalization. SAM has also been introduced in FL for better generalizationÂ ",
                "(Caldarola etÂ al., ",
                "2022",
                "; Qu etÂ al., ",
                "2022",
                ")",
                ". In our paper, we find SAM can be used to align local loss landscapes by making the landscapes flatter, so we also incorporate it in FedGuCci+. From ",
                "FigureÂ 5",
                "Â (c), it can be seen that if the landscapes are flatter, the overlap regions between two clients will increase, therefore, it will have more aligned landscapes. Also, for FedGuCci, SAM makes the connectivity loss to learn a cylinder connected with the anchor model instead of a lineÂ ",
                "(Wen etÂ al., ",
                "2023",
                ")",
                ", improving connectivity robustness and generalization.",
                "FedGuCci+ incorporates logit calibration and SAM into FedGuCci, achieving better generalization. We note that FedGuCci+ is a showcase of how FedGucCci is compatible with other existing techniques for better results, and more techniques can be integrated."
            ]
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Results of global models under pretrain-finetune vision models. Non-IID hyper. is 10.",
        "table": "<table id=\"S5.T5.5.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S5.T5.5.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dataset</span></th>\n<td id=\"S5.T5.5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S5.T5.5.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-10</span></td>\n<td id=\"S5.T5.5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"2\"><span id=\"S5.T5.5.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-100</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.2.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Models</span></th>\n<td id=\"S5.T5.5.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet-18</span></td>\n<td id=\"S5.T5.5.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">ViT</span></td>\n<td id=\"S5.T5.5.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.2.2.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">ResNet-18</span></td>\n<td id=\"S5.T5.5.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.2.2.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">ViT</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.3.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Local</span></th>\n<td id=\"S5.T5.5.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.3.3.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.33Â±0.35</span></td>\n<td id=\"S5.T5.5.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.3.3.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">87.04Â±0.43</span></td>\n<td id=\"S5.T5.5.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">31.01Â±0.34</span></td>\n<td id=\"S5.T5.5.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.38Â±0.47</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.4.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S5.T5.5.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.4.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">74.89Â±0.16</span></td>\n<td id=\"S5.T5.5.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.4.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">96.16Â±0.19</span></td>\n<td id=\"S5.T5.5.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.4.4.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">45.24Â±0.57</span></td>\n<td id=\"S5.T5.5.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.4.4.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">83.61Â±0.69</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.5.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedProx</span></th>\n<td id=\"S5.T5.5.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.5.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">50.61Â±0.81</span></td>\n<td id=\"S5.T5.5.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.5.5.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">96.32Â±0.21</span></td>\n<td id=\"S5.T5.5.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.5.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">4.29Â±0.38</span></td>\n<td id=\"S5.T5.5.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.5.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">78.49Â±1.92</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.6.6\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T5.5.1.6.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedRoD</span></th>\n<td id=\"S5.T5.5.1.6.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.6.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">74.91Â±0.17</span></td>\n<td id=\"S5.T5.5.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.6.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">96.18Â±0.18</span></td>\n<td id=\"S5.T5.5.1.6.6.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.6.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">45.19Â±0.76</span></td>\n<td id=\"S5.T5.5.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.6.6.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">83.64Â±0.35</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.7.7\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T5.5.1.7.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedLC</span></th>\n<td id=\"S5.T5.5.1.7.7.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.7.7.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">74.94Â±0.13</span></td>\n<td id=\"S5.T5.5.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.7.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">96.21Â±0.17</span></td>\n<td id=\"S5.T5.5.1.7.7.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.7.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">45.18Â±0.65</span></td>\n<td id=\"S5.T5.5.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.7.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">83.38Â±0.64</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.8.8\" class=\"ltx_tr\">\n<th id=\"S5.T5.5.1.8.8.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r\"><span id=\"S5.T5.5.1.8.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedSAM</span></th>\n<td id=\"S5.T5.5.1.8.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.8.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">69.92Â±0.51</span></td>\n<td id=\"S5.T5.5.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.8.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">56.88Â±6.79</span></td>\n<td id=\"S5.T5.5.1.8.8.4\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T5.5.1.8.8.4.1\" class=\"ltx_text\" style=\"font-size:80%;\">28.92Â±0.64</span></td>\n<td id=\"S5.T5.5.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T5.5.1.8.8.5.1\" class=\"ltx_text\" style=\"font-size:80%;\">38.36Â±13.5</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.9.9\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S5.T5.5.1.9.9.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.9.9.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci</span></th>\n<td id=\"S5.T5.5.1.9.9.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.9.9.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">75.22Â±0.12</span></td>\n<td id=\"S5.T5.5.1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.9.9.3.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">96.38Â±0.11</span></td>\n<td id=\"S5.T5.5.1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S5.T5.5.1.9.9.4.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">45.62Â±0.61</span></td>\n<td id=\"S5.T5.5.1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T5.5.1.9.9.5.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">83.71Â±0.48</span></td>\n</tr>\n<tr id=\"S5.T5.5.1.10.10\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S5.T5.5.1.10.10.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S5.T5.5.1.10.10.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci+</span></th>\n<td id=\"S5.T5.5.1.10.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T5.5.1.10.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">71.40Â±0.45</span></td>\n<td id=\"S5.T5.5.1.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T5.5.1.10.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">64.05Â±26.3</span></td>\n<td id=\"S5.T5.5.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S5.T5.5.1.10.10.4.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">34.16Â±0.72</span></td>\n<td id=\"S5.T5.5.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S5.T5.5.1.10.10.5.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">28.52Â±5.56</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Results under various datasets and models.",
                " In ",
                "TableÂ 2",
                ", our methods can reach state-of-the-art results across four datasets under both IID (",
                "Î±",
                "=",
                "100",
                "ğ›¼",
                "100",
                "\\alpha=100",
                ") and heterogeneous (",
                "Î±",
                "=",
                "0.5",
                "ğ›¼",
                "0.5",
                "\\alpha=0.5",
                ") settings",
                "2",
                "2",
                "2",
                "Itâ€™s important to mention that certain methods might fail in specific settings, exhibiting accuracy levels close to random guessing, e.g., FedProx in Fashion-MNIST.",
                ".\nGenerally, FedGuCci can reach the best performances over current FL methods, and FedGuCci+ can strengthen FedGuCci in most cases. Also, the performance gains of our approaches are more dominant under more complicated datasets, like Tiny-ImageNet. While FedSAM stands as the most robust baseline for generalization, our connectivity loss not only yields better results but is also compatible with it (FedGuCci+).",
                "Results on different ",
                "M",
                "ğ‘€",
                "M",
                " and ",
                "Ï",
                "ğœŒ",
                "\\rho",
                ".",
                " We conduct experiments by varying the number of clients ",
                "M",
                "ğ‘€",
                "M",
                " and participation ratios of clients ",
                "Ï",
                "ğœŒ",
                "\\rho",
                " in ",
                "TableÂ 4",
                ". It demonstrates that FedGuCci and FedGuCci+ can also excel when the number of clients is large and partial participation exists, indicating their great potential under cross-device settingsÂ ",
                "(Charles etÂ al., ",
                "2021",
                ")",
                ".",
                "Results of different local epochs ",
                "E",
                "ğ¸",
                "E",
                ".",
                " In ",
                "FigureÂ 6",
                ", FedGuCci is consistently leading under different ",
                "E",
                "ğ¸",
                "E",
                ", while FedGuCci+ is not robust on CIFAR-10. For CIFAR-100, FedGuCci has a more obvious advantage when ",
                "E",
                "ğ¸",
                "E",
                " is large, and this is rationale since the connectivity and model drift issues are more severe under large local updates."
            ]
        ]
    },
    "S5.T6": {
        "caption": "Table 6: Ablation study of FedGuCci+. M=50ğ‘€50M=50, non-IID: 1.0. ",
        "table": "<table id=\"S5.T6.12\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T6.12.11.1\" class=\"ltx_tr\">\n<th id=\"S5.T6.12.11.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S5.T6.12.11.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Methods/Datasets</span></th>\n<th id=\"S5.T6.12.11.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T6.12.11.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-10</span></th>\n<th id=\"S5.T6.12.11.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T6.12.11.1.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">CIFAR-100</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T6.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T6.4.2.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S5.T6.4.2.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedAvg</span></th>\n<td id=\"S5.T6.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">64.14</span><math id=\"S5.T6.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.3.1.1.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.3.1.1.m1.1.1\" xref=\"S5.T6.3.1.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.3.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.3.1.1.m1.1.1.cmml\" xref=\"S5.T6.3.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.3.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.3.1.1.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.38</span>\n</td>\n<td id=\"S5.T6.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.4.2.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">20.81</span><math id=\"S5.T6.4.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.4.2.2.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.4.2.2.m1.1.1\" xref=\"S5.T6.4.2.2.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.4.2.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.4.2.2.m1.1.1.cmml\" xref=\"S5.T6.4.2.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.4.2.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.4.2.2.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.52</span>\n</td>\n</tr>\n<tr id=\"S5.T6.6.4\" class=\"ltx_tr\">\n<th id=\"S5.T6.6.4.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T6.6.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedGuCci</span></th>\n<td id=\"S5.T6.5.3.1\" class=\"ltx_td ltx_align_center\">\n<span id=\"S5.T6.5.3.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.45</span><math id=\"S5.T6.5.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.5.3.1.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.5.3.1.m1.1.1\" xref=\"S5.T6.5.3.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.5.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.5.3.1.m1.1.1.cmml\" xref=\"S5.T6.5.3.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.5.3.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.5.3.1.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.19</span>\n</td>\n<td id=\"S5.T6.6.4.2\" class=\"ltx_td ltx_align_center\">\n<span id=\"S5.T6.6.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">22.74</span><math id=\"S5.T6.6.4.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.6.4.2.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.6.4.2.m1.1.1\" xref=\"S5.T6.6.4.2.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.6.4.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.6.4.2.m1.1.1.cmml\" xref=\"S5.T6.6.4.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.6.4.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.6.4.2.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.42</span>\n</td>\n</tr>\n<tr id=\"S5.T6.8.6\" class=\"ltx_tr\">\n<th id=\"S5.T6.8.6.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\"><span id=\"S5.T6.8.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedGuCci + only logit calibration</span></th>\n<td id=\"S5.T6.7.5.1\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.7.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">65.51</span><math id=\"S5.T6.7.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.7.5.1.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.7.5.1.m1.1.1\" xref=\"S5.T6.7.5.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.7.5.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.7.5.1.m1.1.1.cmml\" xref=\"S5.T6.7.5.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.7.5.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.7.5.1.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.15</span>\n</td>\n<td id=\"S5.T6.8.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<span id=\"S5.T6.8.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">22.32</span><math id=\"S5.T6.8.6.2.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.8.6.2.m1.1a\"><mo mathsize=\"50%\" id=\"S5.T6.8.6.2.m1.1.1\" xref=\"S5.T6.8.6.2.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.8.6.2.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.8.6.2.m1.1.1.cmml\" xref=\"S5.T6.8.6.2.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.8.6.2.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.8.6.2.2\" class=\"ltx_text\" style=\"font-size:50%;\">0.67</span>\n</td>\n</tr>\n<tr id=\"S5.T6.10.8\" class=\"ltx_tr\">\n<th id=\"S5.T6.10.8.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\"><span id=\"S5.T6.10.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;\">FedGuCci + only SAM</span></th>\n<td id=\"S5.T6.9.7.1\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.9.7.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">65.93<math id=\"S5.T6.9.7.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.9.7.1.1.m1.1a\"><mo mathsize=\"63%\" id=\"S5.T6.9.7.1.1.m1.1.1\" xref=\"S5.T6.9.7.1.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.9.7.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.9.7.1.1.m1.1.1.cmml\" xref=\"S5.T6.9.7.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.9.7.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.9.7.1.1.1\" class=\"ltx_text\" style=\"font-size:63%;\">0.38</span></span></td>\n<td id=\"S5.T6.10.8.2\" class=\"ltx_td ltx_align_center\"><span id=\"S5.T6.10.8.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">25.81<math id=\"S5.T6.10.8.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.10.8.2.1.m1.1a\"><mo mathsize=\"63%\" id=\"S5.T6.10.8.2.1.m1.1.1\" xref=\"S5.T6.10.8.2.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.10.8.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.10.8.2.1.m1.1.1.cmml\" xref=\"S5.T6.10.8.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.10.8.2.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.10.8.2.1.1\" class=\"ltx_text\" style=\"font-size:63%;\">1.02</span></span></td>\n</tr>\n<tr id=\"S5.T6.12.10\" class=\"ltx_tr\" style=\"background-color:#E6E6E6;\">\n<th id=\"S5.T6.12.10.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\"><span id=\"S5.T6.12.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;background-color:#E6E6E6;\">FedGuCci+ (with both)</span></th>\n<td id=\"S5.T6.11.9.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T6.11.9.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">66.05<math id=\"S5.T6.11.9.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.11.9.1.1.m1.1a\"><mo mathbackground=\"#E6E6E6\" mathsize=\"63%\" id=\"S5.T6.11.9.1.1.m1.1.1\" xref=\"S5.T6.11.9.1.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.11.9.1.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.11.9.1.1.m1.1.1.cmml\" xref=\"S5.T6.11.9.1.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.11.9.1.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.11.9.1.1.1\" class=\"ltx_text\" style=\"font-size:63%;\">0.35</span></span></td>\n<td id=\"S5.T6.12.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S5.T6.12.10.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;background-color:#E6E6E6;\">25.30<math id=\"S5.T6.12.10.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T6.12.10.2.1.m1.1a\"><mo mathbackground=\"#E6E6E6\" mathsize=\"63%\" id=\"S5.T6.12.10.2.1.m1.1.1\" xref=\"S5.T6.12.10.2.1.m1.1.1.cmml\">Â±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T6.12.10.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T6.12.10.2.1.m1.1.1.cmml\" xref=\"S5.T6.12.10.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T6.12.10.2.1.m1.1c\">\\pm</annotation></semantics></math><span id=\"S5.T6.12.10.2.1.1\" class=\"ltx_text\" style=\"font-size:63%;\">0.65</span></span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Results under various datasets and models.",
                " In ",
                "TableÂ 2",
                ", our methods can reach state-of-the-art results across four datasets under both IID (",
                "Î±",
                "=",
                "100",
                "ğ›¼",
                "100",
                "\\alpha=100",
                ") and heterogeneous (",
                "Î±",
                "=",
                "0.5",
                "ğ›¼",
                "0.5",
                "\\alpha=0.5",
                ") settings",
                "2",
                "2",
                "2",
                "Itâ€™s important to mention that certain methods might fail in specific settings, exhibiting accuracy levels close to random guessing, e.g., FedProx in Fashion-MNIST.",
                ".\nGenerally, FedGuCci can reach the best performances over current FL methods, and FedGuCci+ can strengthen FedGuCci in most cases. Also, the performance gains of our approaches are more dominant under more complicated datasets, like Tiny-ImageNet. While FedSAM stands as the most robust baseline for generalization, our connectivity loss not only yields better results but is also compatible with it (FedGuCci+).",
                "Results on different ",
                "M",
                "ğ‘€",
                "M",
                " and ",
                "Ï",
                "ğœŒ",
                "\\rho",
                ".",
                " We conduct experiments by varying the number of clients ",
                "M",
                "ğ‘€",
                "M",
                " and participation ratios of clients ",
                "Ï",
                "ğœŒ",
                "\\rho",
                " in ",
                "TableÂ 4",
                ". It demonstrates that FedGuCci and FedGuCci+ can also excel when the number of clients is large and partial participation exists, indicating their great potential under cross-device settingsÂ ",
                "(Charles etÂ al., ",
                "2021",
                ")",
                ".",
                "Results of different local epochs ",
                "E",
                "ğ¸",
                "E",
                ".",
                " In ",
                "FigureÂ 6",
                ", FedGuCci is consistently leading under different ",
                "E",
                "ğ¸",
                "E",
                ", while FedGuCci+ is not robust on CIFAR-10. For CIFAR-100, FedGuCci has a more obvious advantage when ",
                "E",
                "ğ¸",
                "E",
                " is large, and this is rationale since the connectivity and model drift issues are more severe under large local updates."
            ]
        ]
    }
}