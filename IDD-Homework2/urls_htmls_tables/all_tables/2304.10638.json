{
    "PAPER'S NUMBER OF TABLES": 1,
    "S5.T1": {
        "caption": "TABLE I: Overview of the backdoor insertion settings considered in the evaluation.",
        "table": "<table id=\"S5.T1.4.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">ID</span></th>\n<th id=\"S5.T1.4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"S5.T1.4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S5.T1.4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Trigger Pattern</span></th>\n<th id=\"S5.T1.4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.4.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Attack Method</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T1.4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.4.1.2.1.2.1\" class=\"ltx_text\">CIFAR-10</span></td>\n<td id=\"S5.T1.4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.4.1.2.1.3.1\" class=\"ltx_text\">VGG-11</span></td>\n<td id=\"S5.T1.4.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"S5.T1.4.1.2.1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.2.1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Green Cars</td>\n</tr>\n<tr id=\"S5.T1.4.1.2.1.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">to Bird</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T1.4.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"S5.T1.4.1.2.1.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.2.1.5.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Constraint</td>\n</tr>\n<tr id=\"S5.T1.4.1.2.1.5.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">and ScaleÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"S5.T1.4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S5.T1.4.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"S5.T1.4.1.3.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.3.2.2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.3.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Label 5</td>\n</tr>\n<tr id=\"S5.T1.4.1.3.2.2.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.3.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">to Label 9</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T1.4.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">NeurotoxinÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n</tr>\n<tr id=\"S5.T1.4.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3</td>\n<td id=\"S5.T1.4.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"S5.T1.4.1.4.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.4.3.2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.4.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Edge</td>\n</tr>\n<tr id=\"S5.T1.4.1.4.3.2.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.4.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Case</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T1.4.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">NeurotoxinÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n</tr>\n<tr id=\"S5.T1.4.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">4</td>\n<td id=\"S5.T1.4.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">CIFAR-100</td>\n<td id=\"S5.T1.4.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">ResNet-18</td>\n<td id=\"S5.T1.4.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">\n<table id=\"S5.T1.4.1.5.4.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.5.4.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.5.4.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Label 5</td>\n</tr>\n<tr id=\"S5.T1.4.1.5.4.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.5.4.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">to Label 9</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T1.4.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">NeurotoxinÂ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In our evaluation, we consider one out of the total 100100100 participants to be malicious. The malicious participant is selected based on the three poisoning mechanisms outlined in SectionÂ 3.2.3. We examine two open-sourced state-of-the-art backdoor insertion attacks in FL - Constraint and ScaleÂ [11], and NeurotoxinÂ [13] - well-established for their ability to embed stealthy backdoors and maintain persistence across multiple FL training rounds. We consider three distinct categories of commonly used semantic targeted variants for backdoor triggers: (1) misclassifying all â€œgreen carsâ€ as â€œbirdsâ€Â [11, 12, 13], (2) misclassifying a subset of images from class label â€œ5â€ as class label â€œ9â€Â [13], and (3) edge case samples (e.g., misclassifying images of Southwest airplanes as â€œtruckâ€ for a CIFAR-10 classifier)Â [12]. The proposed method remains equally applicable to other trigger patterns and target labels. TableÂ I provides an overview of the prevalent attack settings found in the literature, which we have considered in our evaluation.",
            "We evaluate the performance of the proposed backdoor removal approach within the context of FL, taking into account the backdoor insertion techniques outlined in TableÂ I. We adhere to the default attack configurations specified in the respective papers to ensure a fair evaluation, including hyperparameters, the number of backdoor samples, and other relevant settings. We also consider the three poisoning strategies - continuous selection, fixed-frequency selection, and random selection - for this evaluation. We consider that the adversary starts backdoor insertion into the global model once it stabilizes from the initial learning fluctuations, as inserting backdoors at the initial rounds is unlikely to persist due to the dynamics of FL training iterations. We observe that the global model stabilizes roughly after 100 FL training rounds for both the datasets. So for each of the configurations mentioned in TableÂ I, we initiate backdoor insertion starting from round 100. We continue the process until the backdoor accuracy in the global model reaches 90%. Without loss of generality, we begin removing backdoors after 5 FL rounds when the backdoor accuracy attains 90%. We proceed with backdoor removal until the backdoor patterns reach random guess accuracy, i.e., 10% for CIFAR-10 and 1% for CIFAR-100. We consider 6 local epochs for the unlearning step with an initial learning rate of 10âˆ’5superscript10510^{-5}. The learning rate is subsequently reduced by a factor of 101010 after every 2 epochs. We use the stochastic gradient descent algorithm for optimizing the loss mentioned in Equation(Â 10). We consider Î³=3ğ›¾3\\gamma=3 for this analysis. We also provide an ablation study on the choice of Î³ğ›¾\\gamma and its impact on the results later in SectionÂ 6.3.2. In this analysis, we assume that the adversary employs a continuous poisoning strategy only during the backdoor removal phase, with the aim of rapidly eliminating the traces of backdoors from the global model, irrespective of the strategy implemented in the backdoor insertion phase. We later conduct an ablation study to analyze the impact of a non-continuous poisoning strategy for backdoor removal in SectionÂ 6.3.3. The results of unlearning the backdoor patterns using the aforementioned configurations for all the backdoor insertion methods are presented in FigureÂ 3. Without loss of generality the plots are shown till 300 rounds for continuous selection and 500 rounds for both fixed-frequency and random selection.",
            "The plots displayed in FigureÂ 3 are generated by considering ten distinct, independent runs with varying random seeds. The solid lines represent the mean values of these runs, while the shaded regions surrounding the lines illustrate the corresponding standard deviations. In the figure, Aâ€‹câ€‹câ„³ğ´ğ‘subscriptğ‘â„³Acc_{\\mathcal{M}} represents the main task accuracy. Aâ€‹câ€‹câ„¬nâ€‹oâ€‹râ€‹mâ€‹aâ€‹lğ´ğ‘subscriptsuperscriptğ‘ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™â„¬Acc^{normal}_{\\mathcal{B}} refers to the backdoor accuracy in cases where no backdoor unlearning is applied, and the adversary stops contributing to FL training after backdoor insertion is completed. Aâ€‹câ€‹câ„¬uâ€‹nâ€‹lâ€‹eâ€‹aâ€‹râ€‹nâ€‹iâ€‹nâ€‹gğ´ğ‘subscriptsuperscriptğ‘ğ‘¢ğ‘›ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘–ğ‘›ğ‘”â„¬Acc^{unlearning}_{\\mathcal{B}}, on the other hand, represents the backdoor accuracy when the proposed backdoor removal method is implemented. As the figure demonstrates, each attack setting effectively injects stealthy backdoors into the global model without impacting the main task accuracy Aâ€‹câ€‹câ„³ğ´ğ‘subscriptğ‘â„³Acc_{\\mathcal{M}}, consistent with the state-of-the-art results. This demonstrates the effectiveness of the attack strategies in achieving their intended goals. It is to be noted that the continuous selection strategy attains the desired backdoor accuracy earlier than the other two strategies because of the continuous accumulation of malicious updates in the global model. Furthermore, we observe that the backdoor accuracy Aâ€‹câ€‹câ„¬nâ€‹oâ€‹râ€‹mâ€‹aâ€‹lğ´ğ‘subscriptsuperscriptğ‘ğ‘›ğ‘œğ‘Ÿğ‘šğ‘ğ‘™â„¬Acc^{normal}_{\\mathcal{B}} progressively diminishes over subsequent FL training rounds, aligning with expectations, as backdoors are not intrinsically persistent within the global model due to the inherent training dynamics of the FL framework. However, upon applying the proposed backdoor removal method, we notice that the final backdoor accuracy of the global model is reduced by 47.87% for ID1, 41.93% for ID2, 48.93% for ID3, and 46.05% for ID4 on average over all poisoning strategies\nin comparison to scenarios where no unlearning is employed, where ID1, ID2, ID3, and ID4 refers to the respective configurations mentioned in TableÂ I. It is to be noted that different combinations of datasets, neural network architectures, trigger patterns, backdoor insertion techniques, and poisoning strategies create backdoor models with varying impacts and complexities. The observation mentioned above highlights the effectiveness of the proposed approach in removing the influence of backdoors for all these combinations.",
            "We assess the stealthiness of the proposed backdoor removal method within the context of FL by examining the L2subscriptğ¿2L_{2}-norm of the difference between local model updates and the global model. We take into account all participants involved in the process, considering the various backdoor insertion settings outlined in TableÂ I, as well as three distinct poisoning strategies, as discussed previously. We analyze the distributions of these norms during both the backdoor insertion and backdoor removal phases, with the results shown in FigureÂ 4.",
            "A key observation from the figure is that the malicious updates overlap with their benign counterparts across all configurations mentioned in TableÂ I. As mentioned earlier, different combinations of attack configurations like datasets, neural network architectures, trigger patterns, backdoor insertion techniques, and poisoning strategies create backdoor models with varying impacts and complexities. The observation mentioned in FigureÂ 4 indicates that the proposed method effectively maintains stealth throughout the process of unlearning for all these combinations. Consequently, this approach does not raise any suspicions based on the norm of updates, thereby ensuring the successful implementation of backdoor removal for diverse scenarios.",
            "In SectionÂ 4.1.2, we introduced a dynamic penalization approach, which involves assigning weights of varying magnitudes to penalty terms. In this section, we evaluate the performance of the proposed backdoor removal method in the absence of the weighted penalty (which is analogous to the machine unlearning-based defense against backdoor attacks in a conventional machine learning framework as discussed inÂ [45]). We adopt EquationÂ (8) as the unlearning loss function and set Î³=3ğ›¾3\\gamma=3 for this analysis. Our assessment consists of all the backdoor insertion settings outlined in TableÂ I, along with the three poisoning strategies discussed earlier. FigureÂ 5 presents the results of this analysis, where solid bars indicate the final backdoor accuracy after 300 rounds for continuous selection and after 500 rounds for both fixed-frequency and random selection, taking into account the proposed weighted penalty parameters (i.e., for the loss function mentioned in EquationÂ (10)).",
            "In SectionÂ 4.1.2, we presented the dynamic penalization approach, where we utilized the hyperparameter Î³ğ›¾\\gamma to modulate the influence of the penalty term on the loss function. In this section, we assess the performance of the proposed backdoor removal method for varying gamma values. We adopt the weighted penalization strategy described in EquationÂ (10) as the unlearn loss function. Without loss of generality, we consider Î³=2ğ›¾2\\gamma=2, Î³=3ğ›¾3\\gamma=3, and Î³=4ğ›¾4\\gamma=4 for this analysis, taking into account all backdoor insertion settings outlined in TableÂ I and the three poisoning strategies discussed previously. The results of our analysis are illustrated in FigureÂ 6. In the figure, the black bars represent the final backdoor accuracy for different Î³ğ›¾\\gamma values after 300 rounds for continuous selection and after 500 rounds for both fixed-frequency and random selection. The axes for these bars are located on the left side of each figure. The yellow bars correspond to the average deviation of parameters (in terms of the L2subscriptğ¿2L_{2}-norm) for the local model of the compromised participant from the parameters of the local model of other benign participants, with their axes displayed on the right side of each figure. Each bar in the plot is computed by averaging the results of ten independent runs with distinct random seeds. Each plot also illustrates the variation trend for both backdoor accuracy and the average deviation as the Î³ğ›¾\\gamma value increases using dashed lines with respective colors. From the figure, we can observe the trade-off between backdoor accuracy and the deviation of the malicious model updates from the benign model updates as Î³ğ›¾\\gamma varies. As Î³ğ›¾\\gamma increases, the final backdoor accuracy is minimized, as desired, but it also results in a model that deviates significantly from the models of other benign participants, making it detectable by the central server. This is expected, as higher Î³ğ›¾\\gamma values penalize over-unlearning more, but in doing so, they considerably impact the loss function. The average deviation is close to or more than 3 for almost all attack settings in the figure, which is notably different when compared to the L2subscriptğ¿2L_{2}-norms depicted in FigureÂ 4, where an average deviation of 3 in the L2subscriptğ¿2L_{2}-norm can cause the model updates of the compromised participant to be markedly distinct from those of other participants. In our analysis, we consider Î³=3ğ›¾3\\gamma=3 as it provides the optimal trade-off between backdoor accuracy and deviation of the compromised model from other benignÂ participants.",
            "In the previous discussions, we present our analyses based on the assumption that the adversary employs a continuous selection strategy only during the backdoor removal phase, with the aim of rapidly eliminating the traces of backdoors from the global model, independent of the strategy utilized in the backdoor insertion phase. In this section, we assess the performance of the proposed backdoor removal method while considering identical poisoning strategies for both the backdoor insertion and removal phases. This analysis considers all backdoor insertion settings as outlined in TableÂ I and focuses on fixed-frequency and random selection strategies, as the results pertaining to continuous selection have already been covered in FigureÂ 3. The results of this analysis are depicted in FigureÂ 7, where the line plot labeled Non Continuous represents backdoor accuracy when the same poisoning strategy is applied during both insertion and removal phases, while the plot labeled Continuous represents backdoor accuracy considering the continuous selection strategy for backdoor removal.",
            "One of the useful techniques to prevent any malicious modification to the global model in any FL framework is to add noise to the model updates when a participant transmits the local model to the central server. Given that both backdoor attacks and removal influence various parameters within the global model with varying magnitudes, it is crucial to investigate the impact of noise addition on the proposed backdoor removal strategy. In this section, we evaluate the performance of the proposed backdoor removal method, considering that the central server introduces Gaussian noise to the model updates during aggregation. Without loss of generality, we assume Gaussian noise with a zero mean and investigate varying standard deviations. The analysis includes all backdoor insertion settings outlined in TableÂ I and all poisoning strategies discussed previously. FigureÂ 8 illustrates the results for different standard deviations of the noise distribution. Solid bars represent the final backdoor accuracy after 300 rounds for continuous selection and after 500 rounds for both fixed-frequency and random selection. Hatched bars, on the other hand, represent the main task accuracy for corresponding poisoning strategies. As shown in FigureÂ 8, increasing the noise level diminishes the efficacy of the proposed method in removing backdoors, as backdoor accuracy rises with an increase in noise standard deviation. Simultaneously, a higher noise level also impacts the main task accuracy of the global model, which, as demonstrated in the figure, declines with an increase in noise standard deviation. Consequently, the addition of noise presents an interesting trade-off between model performance and the impact of the proposed backdoor removal method."
        ]
    }
}