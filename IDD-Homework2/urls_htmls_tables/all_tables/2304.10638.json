{
    "PAPER'S NUMBER OF TABLES": 1,
    "S5.T1": {
        "caption": "TABLE I: Overview of the backdoor insertion settings considered in the evaluation.",
        "table": "<table id=\"S5.T1.4.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.4.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">ID</span></th>\n<th id=\"S5.T1.4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"S5.T1.4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Model</span></th>\n<th id=\"S5.T1.4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Trigger Pattern</span></th>\n<th id=\"S5.T1.4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S5.T1.4.1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Attack Method</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.4.1.2.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"S5.T1.4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.4.1.2.1.2.1\" class=\"ltx_text\">CIFAR-10</span></td>\n<td id=\"S5.T1.4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S5.T1.4.1.2.1.3.1\" class=\"ltx_text\">VGG-11</span></td>\n<td id=\"S5.T1.4.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"S5.T1.4.1.2.1.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.2.1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Green Cars</td>\n</tr>\n<tr id=\"S5.T1.4.1.2.1.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">to Bird</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T1.4.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"S5.T1.4.1.2.1.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.2.1.5.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Constraint</td>\n</tr>\n<tr id=\"S5.T1.4.1.2.1.5.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.2.1.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">and Scale <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">11</a>]</cite>\n</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"S5.T1.4.1.3.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S5.T1.4.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"S5.T1.4.1.3.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.3.2.2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.3.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Label 5</td>\n</tr>\n<tr id=\"S5.T1.4.1.3.2.2.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.3.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">to Label 9</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T1.4.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Neurotoxin <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n</tr>\n<tr id=\"S5.T1.4.1.4.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3</td>\n<td id=\"S5.T1.4.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"S5.T1.4.1.4.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.4.3.2.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.4.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Edge</td>\n</tr>\n<tr id=\"S5.T1.4.1.4.3.2.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.4.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Case</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T1.4.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Neurotoxin <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n</tr>\n<tr id=\"S5.T1.4.1.5.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">4</td>\n<td id=\"S5.T1.4.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">CIFAR-100</td>\n<td id=\"S5.T1.4.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">ResNet-18</td>\n<td id=\"S5.T1.4.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">\n<table id=\"S5.T1.4.1.5.4.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S5.T1.4.1.5.4.4.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.5.4.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Label 5</td>\n</tr>\n<tr id=\"S5.T1.4.1.5.4.4.1.2\" class=\"ltx_tr\">\n<td id=\"S5.T1.4.1.5.4.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">to Label 9</td>\n</tr>\n</table>\n</td>\n<td id=\"S5.T1.4.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\">Neurotoxin <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">13</a>]</cite>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In our evaluation, we consider one out of the total 100100100 participants to be malicious. The malicious participant is selected based on the three poisoning mechanisms outlined in Section 3.2.3. We examine two open-sourced state-of-the-art backdoor insertion attacks in FL - Constraint and Scale [11], and Neurotoxin [13] - well-established for their ability to embed stealthy backdoors and maintain persistence across multiple FL training rounds. We consider three distinct categories of commonly used semantic targeted variants for backdoor triggers: (1) misclassifying all “green cars” as “birds” [11, 12, 13], (2) misclassifying a subset of images from class label “5” as class label “9” [13], and (3) edge case samples (e.g., misclassifying images of Southwest airplanes as “truck” for a CIFAR-10 classifier) [12]. The proposed method remains equally applicable to other trigger patterns and target labels. Table I provides an overview of the prevalent attack settings found in the literature, which we have considered in our evaluation.",
            "We evaluate the performance of the proposed backdoor removal approach within the context of FL, taking into account the backdoor insertion techniques outlined in Table I. We adhere to the default attack configurations specified in the respective papers to ensure a fair evaluation, including hyperparameters, the number of backdoor samples, and other relevant settings. We also consider the three poisoning strategies - continuous selection, fixed-frequency selection, and random selection - for this evaluation. We consider that the adversary starts backdoor insertion into the global model once it stabilizes from the initial learning fluctuations, as inserting backdoors at the initial rounds is unlikely to persist due to the dynamics of FL training iterations. We observe that the global model stabilizes roughly after 100 FL training rounds for both the datasets. So for each of the configurations mentioned in Table I, we initiate backdoor insertion starting from round 100. We continue the process until the backdoor accuracy in the global model reaches 90%. Without loss of generality, we begin removing backdoors after 5 FL rounds when the backdoor accuracy attains 90%. We proceed with backdoor removal until the backdoor patterns reach random guess accuracy, i.e., 10% for CIFAR-10 and 1% for CIFAR-100. We consider 6 local epochs for the unlearning step with an initial learning rate of 10−5superscript10510^{-5}. The learning rate is subsequently reduced by a factor of 101010 after every 2 epochs. We use the stochastic gradient descent algorithm for optimizing the loss mentioned in Equation( 10). We consider γ=3𝛾3\\gamma=3 for this analysis. We also provide an ablation study on the choice of γ𝛾\\gamma and its impact on the results later in Section 6.3.2. In this analysis, we assume that the adversary employs a continuous poisoning strategy only during the backdoor removal phase, with the aim of rapidly eliminating the traces of backdoors from the global model, irrespective of the strategy implemented in the backdoor insertion phase. We later conduct an ablation study to analyze the impact of a non-continuous poisoning strategy for backdoor removal in Section 6.3.3. The results of unlearning the backdoor patterns using the aforementioned configurations for all the backdoor insertion methods are presented in Figure 3. Without loss of generality the plots are shown till 300 rounds for continuous selection and 500 rounds for both fixed-frequency and random selection.",
            "The plots displayed in Figure 3 are generated by considering ten distinct, independent runs with varying random seeds. The solid lines represent the mean values of these runs, while the shaded regions surrounding the lines illustrate the corresponding standard deviations. In the figure, A​c​cℳ𝐴𝑐subscript𝑐ℳAcc_{\\mathcal{M}} represents the main task accuracy. A​c​cℬn​o​r​m​a​l𝐴𝑐subscriptsuperscript𝑐𝑛𝑜𝑟𝑚𝑎𝑙ℬAcc^{normal}_{\\mathcal{B}} refers to the backdoor accuracy in cases where no backdoor unlearning is applied, and the adversary stops contributing to FL training after backdoor insertion is completed. A​c​cℬu​n​l​e​a​r​n​i​n​g𝐴𝑐subscriptsuperscript𝑐𝑢𝑛𝑙𝑒𝑎𝑟𝑛𝑖𝑛𝑔ℬAcc^{unlearning}_{\\mathcal{B}}, on the other hand, represents the backdoor accuracy when the proposed backdoor removal method is implemented. As the figure demonstrates, each attack setting effectively injects stealthy backdoors into the global model without impacting the main task accuracy A​c​cℳ𝐴𝑐subscript𝑐ℳAcc_{\\mathcal{M}}, consistent with the state-of-the-art results. This demonstrates the effectiveness of the attack strategies in achieving their intended goals. It is to be noted that the continuous selection strategy attains the desired backdoor accuracy earlier than the other two strategies because of the continuous accumulation of malicious updates in the global model. Furthermore, we observe that the backdoor accuracy A​c​cℬn​o​r​m​a​l𝐴𝑐subscriptsuperscript𝑐𝑛𝑜𝑟𝑚𝑎𝑙ℬAcc^{normal}_{\\mathcal{B}} progressively diminishes over subsequent FL training rounds, aligning with expectations, as backdoors are not intrinsically persistent within the global model due to the inherent training dynamics of the FL framework. However, upon applying the proposed backdoor removal method, we notice that the final backdoor accuracy of the global model is reduced by 47.87% for ID1, 41.93% for ID2, 48.93% for ID3, and 46.05% for ID4 on average over all poisoning strategies\nin comparison to scenarios where no unlearning is employed, where ID1, ID2, ID3, and ID4 refers to the respective configurations mentioned in Table I. It is to be noted that different combinations of datasets, neural network architectures, trigger patterns, backdoor insertion techniques, and poisoning strategies create backdoor models with varying impacts and complexities. The observation mentioned above highlights the effectiveness of the proposed approach in removing the influence of backdoors for all these combinations.",
            "We assess the stealthiness of the proposed backdoor removal method within the context of FL by examining the L2subscript𝐿2L_{2}-norm of the difference between local model updates and the global model. We take into account all participants involved in the process, considering the various backdoor insertion settings outlined in Table I, as well as three distinct poisoning strategies, as discussed previously. We analyze the distributions of these norms during both the backdoor insertion and backdoor removal phases, with the results shown in Figure 4.",
            "A key observation from the figure is that the malicious updates overlap with their benign counterparts across all configurations mentioned in Table I. As mentioned earlier, different combinations of attack configurations like datasets, neural network architectures, trigger patterns, backdoor insertion techniques, and poisoning strategies create backdoor models with varying impacts and complexities. The observation mentioned in Figure 4 indicates that the proposed method effectively maintains stealth throughout the process of unlearning for all these combinations. Consequently, this approach does not raise any suspicions based on the norm of updates, thereby ensuring the successful implementation of backdoor removal for diverse scenarios.",
            "In Section 4.1.2, we introduced a dynamic penalization approach, which involves assigning weights of varying magnitudes to penalty terms. In this section, we evaluate the performance of the proposed backdoor removal method in the absence of the weighted penalty (which is analogous to the machine unlearning-based defense against backdoor attacks in a conventional machine learning framework as discussed in [45]). We adopt Equation (8) as the unlearning loss function and set γ=3𝛾3\\gamma=3 for this analysis. Our assessment consists of all the backdoor insertion settings outlined in Table I, along with the three poisoning strategies discussed earlier. Figure 5 presents the results of this analysis, where solid bars indicate the final backdoor accuracy after 300 rounds for continuous selection and after 500 rounds for both fixed-frequency and random selection, taking into account the proposed weighted penalty parameters (i.e., for the loss function mentioned in Equation (10)).",
            "In Section 4.1.2, we presented the dynamic penalization approach, where we utilized the hyperparameter γ𝛾\\gamma to modulate the influence of the penalty term on the loss function. In this section, we assess the performance of the proposed backdoor removal method for varying gamma values. We adopt the weighted penalization strategy described in Equation (10) as the unlearn loss function. Without loss of generality, we consider γ=2𝛾2\\gamma=2, γ=3𝛾3\\gamma=3, and γ=4𝛾4\\gamma=4 for this analysis, taking into account all backdoor insertion settings outlined in Table I and the three poisoning strategies discussed previously. The results of our analysis are illustrated in Figure 6. In the figure, the black bars represent the final backdoor accuracy for different γ𝛾\\gamma values after 300 rounds for continuous selection and after 500 rounds for both fixed-frequency and random selection. The axes for these bars are located on the left side of each figure. The yellow bars correspond to the average deviation of parameters (in terms of the L2subscript𝐿2L_{2}-norm) for the local model of the compromised participant from the parameters of the local model of other benign participants, with their axes displayed on the right side of each figure. Each bar in the plot is computed by averaging the results of ten independent runs with distinct random seeds. Each plot also illustrates the variation trend for both backdoor accuracy and the average deviation as the γ𝛾\\gamma value increases using dashed lines with respective colors. From the figure, we can observe the trade-off between backdoor accuracy and the deviation of the malicious model updates from the benign model updates as γ𝛾\\gamma varies. As γ𝛾\\gamma increases, the final backdoor accuracy is minimized, as desired, but it also results in a model that deviates significantly from the models of other benign participants, making it detectable by the central server. This is expected, as higher γ𝛾\\gamma values penalize over-unlearning more, but in doing so, they considerably impact the loss function. The average deviation is close to or more than 3 for almost all attack settings in the figure, which is notably different when compared to the L2subscript𝐿2L_{2}-norms depicted in Figure 4, where an average deviation of 3 in the L2subscript𝐿2L_{2}-norm can cause the model updates of the compromised participant to be markedly distinct from those of other participants. In our analysis, we consider γ=3𝛾3\\gamma=3 as it provides the optimal trade-off between backdoor accuracy and deviation of the compromised model from other benign participants.",
            "In the previous discussions, we present our analyses based on the assumption that the adversary employs a continuous selection strategy only during the backdoor removal phase, with the aim of rapidly eliminating the traces of backdoors from the global model, independent of the strategy utilized in the backdoor insertion phase. In this section, we assess the performance of the proposed backdoor removal method while considering identical poisoning strategies for both the backdoor insertion and removal phases. This analysis considers all backdoor insertion settings as outlined in Table I and focuses on fixed-frequency and random selection strategies, as the results pertaining to continuous selection have already been covered in Figure 3. The results of this analysis are depicted in Figure 7, where the line plot labeled Non Continuous represents backdoor accuracy when the same poisoning strategy is applied during both insertion and removal phases, while the plot labeled Continuous represents backdoor accuracy considering the continuous selection strategy for backdoor removal.",
            "One of the useful techniques to prevent any malicious modification to the global model in any FL framework is to add noise to the model updates when a participant transmits the local model to the central server. Given that both backdoor attacks and removal influence various parameters within the global model with varying magnitudes, it is crucial to investigate the impact of noise addition on the proposed backdoor removal strategy. In this section, we evaluate the performance of the proposed backdoor removal method, considering that the central server introduces Gaussian noise to the model updates during aggregation. Without loss of generality, we assume Gaussian noise with a zero mean and investigate varying standard deviations. The analysis includes all backdoor insertion settings outlined in Table I and all poisoning strategies discussed previously. Figure 8 illustrates the results for different standard deviations of the noise distribution. Solid bars represent the final backdoor accuracy after 300 rounds for continuous selection and after 500 rounds for both fixed-frequency and random selection. Hatched bars, on the other hand, represent the main task accuracy for corresponding poisoning strategies. As shown in Figure 8, increasing the noise level diminishes the efficacy of the proposed method in removing backdoors, as backdoor accuracy rises with an increase in noise standard deviation. Simultaneously, a higher noise level also impacts the main task accuracy of the global model, which, as demonstrated in the figure, declines with an increase in noise standard deviation. Consequently, the addition of noise presents an interesting trade-off between model performance and the impact of the proposed backdoor removal method."
        ]
    }
}