{
    "id_table_1": {
        "caption": "Table 2 :  Generalization performance comparison between  STACKFEED  and baseline models across multiple datasets, reported as accuracy percentages (higher is better).",
        "table": "S3.F1.1.1.1.pic1.16.16.16.1.1.1",
        "footnotes": [],
        "references": [
            "Figure  1  illustrates our technique applied to the ARKS Pony domain  ( Su et al. ,  2024a ) , where a knowledge base (KB) for the low-resource programming language Pony supports a natural language-to-code task. Due to Ponys rarity, language models often generate code that fails to compile. To address this, we use the Pony compiler as an expert to provide feedback in the form of compile errors.",
            "Optimization Objective:  Following Equation  1 , our objective then is to find the optimal state  s  superscript s s^{*} italic_s start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  of the KB that maximizes the overall performance of the RAG system, as measured by a global reward function  R R R italic_R . The optimization problem is formulated as:",
            "We adapt  two  code generation datasets from ARKS  ( Su et al. ,  2024a ) , namely  ARKS-Pony  and  ARKS-Ring . The dataset consists of LeetCode problems and their solutions in low-resource languages Pony and Ring respectively. Each datapoint is supplemented with a corresponding language documentation, with execution accuracy as the success metric and execution failures as feedback to the system. Given that these language dont appear prominently in LLM pre-training data, the performance of code generation RAG agents on these datasets depends significantly on the quality of the Knowledge Base. However, given that these languages have smaller communities, their documentation isnt as well maintained and often lack critical information. . For the purpose of evaluation on these datasets, we split them into train, eval, test splits as specified in Table   1 . To ensure that we have a good representation of failure cases during training, we first execute the RAG pipeline on the entire dataset and divide the failures at random in a 1:1:2 ratio for train, eval and test respectively. All the datapoints with successful execution match are put in the test split. We use the compiler feedback from the executions as the expert feedback to the  STACKFEED  system.",
            "While fact retrieval is one of the most popular use cases of RAG systems, evolving nature of information requires us to keep the knowledge bases up to date. To simulate these dynamic factual knowledge updates we use the CLARKS-news dataset from Erase  ( Li et al. ,  2024 )  which contains questions and their respective answers extracted from Wikidata at different timestamps. Each timestamp is characterized by a set of articles that were added in the data at that time. For our evaluation, we pool all the questions whose answers changed for the  first  time at a given timestamp and split them across train, eval and test splits in a 1:1:2 ratio (Table  1 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 3 :  Completeness and coherence comparison between  STACKFEED  and baseline models across multiple datasets. Completeness is reported as accuracy percentages (higher is better), while coherence is measured on a scale of 1-5 (higher is better).",
        "table": "S6.T1.1.1",
        "footnotes": [],
        "references": [
            "This paper is organized as follows: Section  2  reviews relevant prior work, while Section  3  presents an illustrative example to introduce and explain our approach. Section  4  details the proposed methodology, and Section  5  outlines the desired characteristics for the edited KB along with metrics for evaluation. Section  6  describes the experimental setup, and finally, Section  7  reports the results.",
            "For a given query  q i subscript q i q_{i} italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and the generated answer  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , the expert provides feedback  ( c i , f i ) subscript c i subscript f i (c_{i},f_{i}) ( italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  that includesa ground truth answer  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and qualitative expert feedback  f i subscript f i f_{i} italic_f start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  on any errors. The global reward signal is derived from  c i subscript c i c_{i} italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  as per the scoring function  s s s italic_s  (Refer Equation  2 ).",
            "Figure  2  illustrates the environmental interaction of the actor-critic model. Following methodologies in prior works  ( Pryzant et al. ,  2023 ;  Juneja et al. ,  2024 ;  Gupta et al. ,  2024 ) , we use LLMs to generate an overall text gradient    \\nabla   over each failing example. The critic first identifies and select which documents in    ( q i , s )  subscript q i s \\Gamma(q_{i},s) roman_ ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_s )  are responsible for any inaccuracies in  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Reflections are then generated for these documents based on the correct answer, expert feedback and the text gradient. However, as shown in Equation  4 , we need to aggregate these reflections across all queries. Instead of a simple concatenation, we adopt the clustering approach similar to  Juneja et al.   ( 2024 ) , producing generalized reflections that effectively capture the core insights from multiple queries. These aggregated reflections can be effectively considered as the partial textual gradient   \\partial   with respect to the document. These partial gradients are provided as feedback to the document-specific actor  A j subscript A j A_{j} italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , which then perform the actions to edit the specific documents."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "S6.T2.7.7",
        "footnotes": [],
        "references": [
            "This paper is organized as follows: Section  2  reviews relevant prior work, while Section  3  presents an illustrative example to introduce and explain our approach. Section  4  details the proposed methodology, and Section  5  outlines the desired characteristics for the edited KB along with metrics for evaluation. Section  6  describes the experimental setup, and finally, Section  7  reports the results.",
            "As seen in Table 3,  STACKFEED  produces edits with a coherence score of 4 or higher for most datasets. For KBs which need long term maintenance (like language and code documentation as seen in the ARKS datasets),  STACKFEED  makes more coherent edits compared to the baseline. This is especially true for long documents as seen in the ARKS Pony dataset. For news-article like dataset like CLARK-news with factual edits. Incoherency is naturally induced when the facts of the article are changed. For instance, an article on the coronation of a king will lose coherency when the article is updated to add information about the coronation of a new king."
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "S7.T3.2.2",
        "footnotes": [],
        "references": [
            "This paper is organized as follows: Section  2  reviews relevant prior work, while Section  3  presents an illustrative example to introduce and explain our approach. Section  4  details the proposed methodology, and Section  5  outlines the desired characteristics for the edited KB along with metrics for evaluation. Section  6  describes the experimental setup, and finally, Section  7  reports the results.",
            "Figure  2  illustrates the environmental interaction of the actor-critic model. Following methodologies in prior works  ( Pryzant et al. ,  2023 ;  Juneja et al. ,  2024 ;  Gupta et al. ,  2024 ) , we use LLMs to generate an overall text gradient    \\nabla   over each failing example. The critic first identifies and select which documents in    ( q i , s )  subscript q i s \\Gamma(q_{i},s) roman_ ( italic_q start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_s )  are responsible for any inaccuracies in  o i subscript o i o_{i} italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . Reflections are then generated for these documents based on the correct answer, expert feedback and the text gradient. However, as shown in Equation  4 , we need to aggregate these reflections across all queries. Instead of a simple concatenation, we adopt the clustering approach similar to  Juneja et al.   ( 2024 ) , producing generalized reflections that effectively capture the core insights from multiple queries. These aggregated reflections can be effectively considered as the partial textual gradient   \\partial   with respect to the document. These partial gradients are provided as feedback to the document-specific actor  A j subscript A j A_{j} italic_A start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT , which then perform the actions to edit the specific documents."
        ]
    },
    "global_footnotes": []
}