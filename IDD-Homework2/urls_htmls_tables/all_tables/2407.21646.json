{
    "id_table_1": {
        "caption": "Table 1 :  Illustration of different configurations of ST task in Multi-task Continual Training.",
        "table": "S3.T1.2.1",
        "footnotes": [],
        "references": [
            "Simultaneous speech translation (SiST) is recognized as one of the most challenging tasks in the translation domain  jones2014conference  .  Machine-assisted automatic interpretation has been receiving much attention in the natural language processing (NLP) community  iwslt-2021-international  ;  iwslt-2020-international  ;  iwslt-2022-international  ;  iwslt-2023-international  .  Traditional simultaneous translation approaches  cho2016can  ;  gu2017learning  ;  zhao2021volctrans    usually employs a cascaded system, involving a streaming Automatic Speech Recognition (ASR) model, a punctuation model and a Machine Translation (MT) model.  However, such cascaded systems often suffer error propagation and latency from the ASR module.  Despite these advancements in both academic SiST models  barrault2023seamless  ;  fukuda-etal-2023-naist  ;  liu2024recentadvancesendtoendsimultaneous  ;  papi-etal-2023-direct  ;  ren2020simulspeech  ;  zeng-etal-2021-realtrans  ;  zhang2023end   and commercial SiST engines, the translation quality is still far from satisfactory.  As shown in  Figure   1 , we conduct a human assessment of the current accessible SiST systems. From the user-centered perspective, these systems only deliver less than 42% of the valid information to listeners, which heavily affects communication effectiveness.  In contrast, professional human interpreters usually deliver more than 70% of the necessary information  chmiel2021effects   and 95% ideally. Thus in this paper, we use 80% to indicate high-level human interpreters.",
            "In addition, we would like to highlight that the conventional automatic evaluation metrics  papi2022over  ;  papineni2002bleu  ;  rei2020comet  ;  sellam2020bleurt   of simultaneous interpretation might not be good indicators for reflecting the performance of SiST, which often contains compaction, abstraction, and paraphrasing.  Aligned with human interpreters  moores2024nerle  ;  wu2010assessing  , we propose a new evaluation metric named Valid Information Proportion (VIP) 2 2 2 Detailed guidelines of our proposed VIP metric can be found in Appendix  A . . VIP represents the percentage of information that can be precisely delivered, reflecting the central objective of SiST: communication in real-time.  Through thorough human evaluation on diverse and challenging real-world long speech datasets, our approach outperforms other currently accessible systems by a large margin.  As shown in Figure  1 , taking the Chinese-to-English direction as an example, CLASI achieves a VIP score of 81.3%, significantly narrowing the gap between machine-assisted systems and human interpreters.",
            "Training Method.   We follow the work of  ( huang2023speechtranslationlargelanguage,  )  for multi-task training. Specifically, for streaming and higher-quality translation, we mainly focus on three tasks for training CLASI: Automatic Speech Recognition (ASR), Speech Translation (ST), and Text Translation (MT).  To align the modalities of the pretrained LLM and audio encoder,  CLASI is continually trained on various tasks with a substantial volume of paired data.  We further strengthen the in-context learning ability of our approach by incorporating translation in the memory and knowledge from external databases.  As a result, we expand the ST tasks to different configurations as shown in  Table   1 . An ST translation can either be streaming or offline, direct or COT, with or without context, which leads to 8 different tasks.",
            "Training Method.   Even though CLASI possesses a good translation quality on the SiST tasks after the previous multi-task continual training stage, we further boost the performance by fine-tuning on human-annotated streaming ST data with diverse tasks listed in  Table   1 .  Such high-quality data enables our model to better align with the segmentation methodologies of professional human interpreters.  Furthermore, this process enhances our models robustness to speech disfluencies such as stuttering, ensuring smoother communication in real-world scenarios.",
            "We present case studies to show the ability of CLASI in translating complicated speech for zh-en and en-zh in  Table   8  and  Table   9 . We choose one of the most-performed cascaded systems Commerical 4 for comparison. The Commerical 4 adopted a cascaded approach for SiST and it is shown in  Section   4.3  to be one of the best previous SiST systems. Detailed explanations are described in the tables. For zh-en direction, we present cases regarding robustness to recognition errors, reasoning ability, and trending words translation. For the en-zh direction, we present cases regarding native, expressive, and accurate terminology translations. More cases are shown in  Table   11 .",
            "We illustrate the evaluation criteria with two examples in Table  10 . Although these translations achieve high accuracy in automatic evaluations, we still categorize them as invalid. Its important to note that our standard aims to emulate human interpreters, presenting a significant challenge to both human evaluators and translation systems.",
            "We provide more case studies in  Table   11 . In terms of informal, disfluent, code-mixing, and named-entity translation, CLASI could achieve much better results than the commercial products. Benefits from the end-to-end approach, CLASI could also understand the original speech tone and generate better translations."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Statistics of our proposed RealSI benchmark.",
        "table": "S4.T2.2.1",
        "footnotes": [],
        "references": [
            "To address these challenges, we introduce our end-to-end approach, CLASI, a  C ross- L ingual  A gent that accomplishes  S imultaneous  I nterpretation by iteratively performing multiple actions, as illustrated in Figure  2 .  Regarding the first challenge, we imitate professional human interpreters to learn their policy of segmenting a complete sentence into several semantic chunks through syntactic boundaries (pauses, commas, conjunctions, etc.) and contextual meaning.  To enable CLASI to learn such a policy, we follow a data-driven policy learning process and invite human interpreters to annotate real-world speech, which includes the read-write timing for segmentation. From the data, CLASI learns the robust read-write policy for SiST from humans.",
            "Figure   2  presents a flow of operation of CLASI. To perform the SiST task, we design 5 operations:  <INPUT> ,  <OUTPUT> ,  <RETRIEVE> ,  <LOAD_MEM> , and  <UPDATE_MEM> . The following sections describe the details of each operation.  As further illustrated in  Figure   3 , CLASI is an LLM agent that can take input speech, instruction, relevant information retrieved from external knowledge, and last round memory as context. The memory stores previous transcriptions (optional) and translations. At round  r r r italic_r , it first reads speech  x t r  1 : T r subscript x : superscript t r 1 superscript T r {\\mathbf{x}}_{t^{r-1}:T^{r}} bold_x start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT : italic_T start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , where  t r  1 superscript t r 1 t^{r-1} italic_t start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT  is the predicted cut-off time of round  r  1 r 1 r-1 italic_r - 1  and  T r superscript T r T^{r} italic_T start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT  is the end time for audio stream at round  r r r italic_r .  Then the agent retrieves relevant information  k r subscript k r {\\mathbf{k}}_{r} bold_k start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  from the external knowledge and loads context  y 1 : r  1 subscript y : 1 r 1 {\\mathbf{y}}_{1:r-1} bold_y start_POSTSUBSCRIPT 1 : italic_r - 1 end_POSTSUBSCRIPT  from the last round memory.  Once CLASI think sufficient context is loaded, it generates the transcription (optional), translation, and cut-off timestamp  t r superscript t r t^{r} italic_t start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT :",
            "where  U U \\mathcal{U} caligraphic_U  indicates uniform distribution over time of speech.  Trained with  Equation   2 , CLASI learns to generate the cut-off time for the input speech. Additionally, the objective function makes the CLASI wait for appropriate time before starting translation as the LLM will output nothing when it think current speech stream does not contain a complete speech chunk.",
            "As a preliminary attempt to address the shortcomings as mentioned earlier, we propose a new benchmark RealSI for Chinese-to-English (zh-en) and English-to-Chinese (en-zh). RealSI is collected from diverse sources, and most speakers talk naturally and casually without careful preparation.  We choose 10 popular domains: technology, healthcare, education, finance, law, environment, entertainment, science, sports, and art.  One video clip is selected for each domain from a well-known online video platform for both zh-en and en-zh settings. 4 4 4 RealSI is available at  https://github.com/byteresearchcla/RealSI . We do not own the copyright of the videos and only release our annotations together with the publicly available website links of the corresponding videos. If anyone believes that the content constitutes infringement, please contact us. We will remove the relevant content as soon as it is confirmed.   Each sample in RealSI is a nearly 5-minute speech to mock SiST without manual segmentation.  For systems that cannot take long-form audio as input, we also provide sentence-level timestamps for segmentation.   Table   2  presents the detailed statistics of our RealSI.",
            "Therefore, besides the automatic evaluation, we collaborated with senior professional human simultaneous interpreters to standardize the guidelines for a more realistic human evaluation.  Our proposed human evaluation metric focuses on whether the output of the translation model can accurately convey the speakers original intention for each semantic fragment.  This is also the key objective of human interpreters in real-time translation. Note that a single semantic fragment indicates a complete piece of source speech. Typically, a single semantic fragment is a complete sentence.  Detailed definition can be found in  Section   A.2 .  The percentage of valid information fragments within a complete speech session is defined as VIP, which is consistent with real-world criteria for human simultaneous interpretation  wu2010assessing  .",
            "Quantitative Analysis.   As shown in  Section   4.3 , we compare CLASI with the baseline methods on RealSI dataset.  In terms of the reliable human evaluation metrics, VIP, CLASI achieves scores of 81.3% and 78.0% for zh-en and en-zh, respectively. While all other models VIP scores are lower than 42%. For more references, we use 3 widely-used automatic evaluation metrics: BLEU 5 5 5 We use SacreBLEU  post-2018-call   for all the BLEU calculations in this paper. , BLEURT, COMET. Under the automatic evaluation metrics, CLASI also surpasses baselines by a large margin. The detailed human evaluation results of CLASI can be found in  Section   B.2 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Experiment results of translation quality. Automatic evaluations were calculated on both document level and sentence level. In document level evaluations, each translation of a nearly 5-minute audio was considered one instance. For sentence level, automatic scores are calculated on human-segmented translations. VIP refers to the human-evaluated Valid Information Proportion that reflects the translation quality of these systems.     Due to the limitations in human evaluation capacity, the VIP scores are calculated on 4 randomly selected samples out of 10 in RealSI across all systems for fair comparison, while automatic metrics are evaluated on 10 samples.     Additionally, we evaluate the performance of CLASI on all 10 samples, achieving VIP scores of 78.0% for zh-en and 74.9% for en-zh.",
        "table": "S4.T4.2.1",
        "footnotes": [],
        "references": [
            "Figure   2  presents a flow of operation of CLASI. To perform the SiST task, we design 5 operations:  <INPUT> ,  <OUTPUT> ,  <RETRIEVE> ,  <LOAD_MEM> , and  <UPDATE_MEM> . The following sections describe the details of each operation.  As further illustrated in  Figure   3 , CLASI is an LLM agent that can take input speech, instruction, relevant information retrieved from external knowledge, and last round memory as context. The memory stores previous transcriptions (optional) and translations. At round  r r r italic_r , it first reads speech  x t r  1 : T r subscript x : superscript t r 1 superscript T r {\\mathbf{x}}_{t^{r-1}:T^{r}} bold_x start_POSTSUBSCRIPT italic_t start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT : italic_T start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , where  t r  1 superscript t r 1 t^{r-1} italic_t start_POSTSUPERSCRIPT italic_r - 1 end_POSTSUPERSCRIPT  is the predicted cut-off time of round  r  1 r 1 r-1 italic_r - 1  and  T r superscript T r T^{r} italic_T start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT  is the end time for audio stream at round  r r r italic_r .  Then the agent retrieves relevant information  k r subscript k r {\\mathbf{k}}_{r} bold_k start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT  from the external knowledge and loads context  y 1 : r  1 subscript y : 1 r 1 {\\mathbf{y}}_{1:r-1} bold_y start_POSTSUBSCRIPT 1 : italic_r - 1 end_POSTSUBSCRIPT  from the last round memory.  Once CLASI think sufficient context is loaded, it generates the transcription (optional), translation, and cut-off timestamp  t r superscript t r t^{r} italic_t start_POSTSUPERSCRIPT italic_r end_POSTSUPERSCRIPT :",
            "CLASI employs an Encoder-Conditioned LLM architecture. As shown in  Figure   3 , the audio encoder transforms input speech stream  x x {\\mathbf{x}} bold_x  to a series of continuous representations  s s {\\mathbf{s}} bold_s .  Then, the LLM takes the speech representation  s s {\\mathbf{s}} bold_s , retrieved knowledge  k k {\\mathbf{k}} bold_k , historical translation  y y {\\mathbf{y}} bold_y  and instruction  I I {\\mathbf{I}} bold_I  as a sequence of prompt  ( y , s , k , I ) y s k I ({\\mathbf{y}},{\\mathbf{s}},{\\mathbf{k}},{\\mathbf{I}}) ( bold_y , bold_s , bold_k , bold_I )  to generate the translation result  y y {\\mathbf{y}} bold_y .",
            "As shown in  Figure   3 , at round  r r r italic_r ,  <LOAD_MEM>  forwards relevant translations  y 1 : r  1 subscript y : 1 r 1 {\\mathbf{y}}_{1:r-1} bold_y start_POSTSUBSCRIPT 1 : italic_r - 1 end_POSTSUBSCRIPT  to the LLM as a prompt. After CLASI agent generates the translation  y r subscript y r {\\mathbf{y}}_{r} bold_y start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT ,  <UPDATE_MEM>  stores it to the memory and obtains  y 1 : r subscript y : 1 r {\\mathbf{y}}_{1:r} bold_y start_POSTSUBSCRIPT 1 : italic_r end_POSTSUBSCRIPT .",
            "Theoretically, all items in the external database might be added into the prompt to provide information for the translation. However, the external knowledge database often contains tremendous items.  Simply prompting LLM with all the terms not only increases the  inference time but may also hurt the performance of CLASI because of noisy intervention.  Therefore, we design a novel Multi-Modal Retrieval Augmented Generation (MM-RAG) process.  Our multi-modal retriever first retrieves the relevant terminologies from the database based on the input speech.  A small number of filtered items are incorporated into the prompt of CLASI agent for in-context learning as shown in  Figure   3 .",
            "Quantitative Analysis.   As shown in  Section   4.3 , we compare CLASI with the baseline methods on RealSI dataset.  In terms of the reliable human evaluation metrics, VIP, CLASI achieves scores of 81.3% and 78.0% for zh-en and en-zh, respectively. While all other models VIP scores are lower than 42%. For more references, we use 3 widely-used automatic evaluation metrics: BLEU 5 5 5 We use SacreBLEU  post-2018-call   for all the BLEU calculations in this paper. , BLEURT, COMET. Under the automatic evaluation metrics, CLASI also surpasses baselines by a large margin. The detailed human evaluation results of CLASI can be found in  Section   B.2 .",
            "We present case studies to show the ability of CLASI in translating complicated speech for zh-en and en-zh in  Table   8  and  Table   9 . We choose one of the most-performed cascaded systems Commerical 4 for comparison. The Commerical 4 adopted a cascaded approach for SiST and it is shown in  Section   4.3  to be one of the best previous SiST systems. Detailed explanations are described in the tables. For zh-en direction, we present cases regarding robustness to recognition errors, reasoning ability, and trending words translation. For the en-zh direction, we present cases regarding native, expressive, and accurate terminology translations. More cases are shown in  Table   11 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Comparison of latency between CLASI and baselines. AL and LAAL are standard metrics for measuring latency in sentence-level datasets. Even though AL and LAAL yield reliable results on the sentence-level CoVoST2 dataset, we argue that they are less effective for long speeches due to the complexity of long-speech translation.  Therefore, we propose First Letter Appearance Lagging (FLAL), representing the time that each system outputs the first determined translation.",
        "table": "S4.T5.2.1",
        "footnotes": [],
        "references": [
            "We compare CLASI with the open-sourced SiST model,  SeamlessStreaming   barrault2023seamless  . In addition, because of the limited number of available SiST models, we choose to compare CLASI with several commercial systems. We denote the commercial systems as  Commercial  1-4. It is worth noting that unlike CLASI, most of the commercial SiST systems will first generate a temporary translation as soon as possible, then rewrite the temporary translation with a potentially better translation after getting more context.  Notwithstanding, we evaluate the finalized translation of these systems in all our experiments.  Although this re-writing strategy could improve translation quality, continually revising existing translations might affect the user experience, potentially leading to additional confusion.  We would also like to highlight that human interpreters usually do  not employ such a rewriting strategy during translation.  During the entire evaluation, we employ a general external knowledge database that maintains the same for all the evaluation in this paper. It does not contain domain-specific external knowledge to form unfair comparisons. The improvement of external knowledge is independently reported in  Section   4.6 .",
            "Quantitative Analysis.   As shown in  Section   4.3 , we compare CLASI with the baseline methods on RealSI dataset.  In terms of the reliable human evaluation metrics, VIP, CLASI achieves scores of 81.3% and 78.0% for zh-en and en-zh, respectively. While all other models VIP scores are lower than 42%. For more references, we use 3 widely-used automatic evaluation metrics: BLEU 5 5 5 We use SacreBLEU  post-2018-call   for all the BLEU calculations in this paper. , BLEURT, COMET. Under the automatic evaluation metrics, CLASI also surpasses baselines by a large margin. The detailed human evaluation results of CLASI can be found in  Section   B.2 .",
            "Quantitative Results.   Table   4  compares the latency of our model with various systems in terms of AL, LAAL, and our proposed FLAL on the RealSI and CoVoST.  We find that the existing metrics AL and LAAL are not suitable latency measurements of paragraph-level SiST on RealSI.  When the results are significantly shorter or longer than the reference translation, AL and LAAL may be largely exaggerated, leading to unreliable high latency.  In these scenarios, FLAL is a more reliable and stable metric for all the systems.",
            "Besides the paragraph-level latency evaluation, we compare our approach with other systems on the sentence-level dataset CoVoST2 zh-en, where both AL and LAAL produce reasonable values and the results are shown on the right side of  Table   4 .  Since the commercial systems usually rewrite the translation, their latency is higher than the CLASI.  Compared with the fastest approach  SeamlessStreaming , CLASI achieves comparable latency but much better translation quality.",
            "Consequently, we perform user studies and argue that the latency is less important than the translation quality for a practical SiST system. In the recent IWSLT 2023 simultaneous track  iwslt-2023-international  , the ranking of models is also evaluated by the translation quality within certain latency constraints. We verify whether the latency of CLASI is acceptable to users through real-world user surveys. To the publication date of this paper, we collected 14 user surveys on zh-en direction, each user using CLASI for at least 30 minutes. Under the current latency performance shown in  Table   4 , only 1/14 == 7% of them suggest that the latency significantly affects their user experiences while the rest think the improvement of translation quality outweighs the latency and overall output of CLASI largely helps them to understand the speech. Considering that the latency of CLASI is even lower than most of the commercial systems, We believe the latency of CLASI can be acceptable on most cases.",
            "Current latency metrics are proposed on sentence-level SiST. As shown in  Table   4 , such metrics may not be suitable latency measurements for paragraph-level.  As the importance of end-to-end evaluation for long speech keeps increasing, more refined metrics are required to measure the latency and provide a deeper insight into the systems.",
            "We present case studies to show the ability of CLASI in translating complicated speech for zh-en and en-zh in  Table   8  and  Table   9 . We choose one of the most-performed cascaded systems Commerical 4 for comparison. The Commerical 4 adopted a cascaded approach for SiST and it is shown in  Section   4.3  to be one of the best previous SiST systems. Detailed explanations are described in the tables. For zh-en direction, we present cases regarding robustness to recognition errors, reasoning ability, and trending words translation. For the en-zh direction, we present cases regarding native, expressive, and accurate terminology translations. More cases are shown in  Table   11 .",
            "Figure   4  shows the distribution and regression curve for VIP with regard to BLEU, BLEURT, and COMET, respectively. From the scatter points in  Figure   4  we may observe that as VIP score increases, the growth of the automatic metric curves slows down and becomes less significant, making it hard to reflect the real changes in translation quality. The correlation curves in  Figure   4  also demonstrate the finding. Here, we calculate Kendalls Tau correlation coefficient  [ 1 ] , which measures the monotonic correlation between two ordered variables. In low VIP ranges, the correlation between VIP and automatic metrics is observable; as the score increases, the correlation harshly drops, which indicates a significant distortion of the automatic metrics. A possible reason is that the translations may differ from the groundtruths by only a few words in the mediocre ranges. However, in a real simultaneous interpretation scenario, these words are likely to be keywords that play important roles in conveying precise information, which may significantly impact VIP scores."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Comparisons of CLASI and baselines on paragraph-level (BSTC) and sentence-level (CoVoST2, MuST-C, and GigaST) zh-en and en-zh datasets in terms of automatic evaluation metrics. We would like to emphasize that sentence-level evaluation schemes by automatic metrics cannot truly reflect the models performance. VIP in  Section   4.3  is a better metrics for comparing different systems.",
        "table": "S4.T6.4.1",
        "footnotes": [],
        "references": [
            "To ensure a comprehensive evaluation of CLASI, our model is further evaluated on four additional datasets, including BSTC (zh-en)  zhang-etal-2021-bstc  , CoVoST2 (zh-en)  wang2020covost  , MuST-C (en-zh)  di2019must  , and GigaST (en-zh)  ye23b_interspeech  6 6 6 We use the subset from in  GigaS2S  for evaluation .   Table   5  presents the results of the automatic evaluation metrics for both zh-en and en-zh.  Due to the high cost of human evaluation, we are not able to provide VIP for these four datasets.  We observe that our model achieves consistently better performance than the baseline models.  Even though our system achieves the best automatic evaluation results among all the compared systems,  we still would like to emphasize that such a sentence-level evaluation scheme might overestimate the performance of SiST systems.",
            "We provide a detailed human evaluation result for CLASI in Figure  5 , where we provide golden source transcription, CLASI output, human evaluation results, and reference translation. We randomly choose one of the test samples in RealSI. We share the full detailed evaluation results at online sheets 7 7 7 We provide the full evaluation results of CLASI at  https://bit.ly/clasi-eval   for academic reference. Note that to ensure fair comparison, when evaluating multiple systems, we randomly shuffle the ordering between systems for each semantic fragment so that human evaluators cannot identify the specific system."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Top- k k k italic_k  retrieve accuracy (%).",
        "table": "S4.T7.2",
        "footnotes": [],
        "references": [
            "We compare CLASI with the open-sourced SiST model,  SeamlessStreaming   barrault2023seamless  . In addition, because of the limited number of available SiST models, we choose to compare CLASI with several commercial systems. We denote the commercial systems as  Commercial  1-4. It is worth noting that unlike CLASI, most of the commercial SiST systems will first generate a temporary translation as soon as possible, then rewrite the temporary translation with a potentially better translation after getting more context.  Notwithstanding, we evaluate the finalized translation of these systems in all our experiments.  Although this re-writing strategy could improve translation quality, continually revising existing translations might affect the user experience, potentially leading to additional confusion.  We would also like to highlight that human interpreters usually do  not employ such a rewriting strategy during translation.  During the entire evaluation, we employ a general external knowledge database that maintains the same for all the evaluation in this paper. It does not contain domain-specific external knowledge to form unfair comparisons. The improvement of external knowledge is independently reported in  Section   4.6 .",
            "Table  6  presents the performance of various retrieve models on the development set of our proprietary dataset. Each sample in the test set includes a short audio chunk and the mentioned terms in the audio.  Our MM-RAG retriever outperforms other open-source models by a large margin, achieving 91.3 % vs. 26.0% for Top-10 retrieve accuracy.  We compare two types of methodologies: audio-to-audio and audio-to-text.  In the audio-to-audio approach, a Text-to-Speech (TTS) model is utilized to convert the text keys from the external knowledge database into audio format,  forming a database with audio-based keys.  The audio keys and the user-input audio are then encoded with the ASR model to produce the corresponding representations.  The Top- k k k italic_k  retrieved items are subsequently determined using the Maximum Inner Product Search (MIPS) algorithm.  For audio-to-text approach, we compare MM-RAG with CLAP  Elizalde2023CLAPLA  . As indicated in Table  6 , the effectiveness of these models remains significantly below acceptable standards, and MM-RAG significantly outperforms them."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Recall and Precision of ICL and shallow-fusion for the intervention of the keywords. When calculating the Recall, we input 1 ground-truth keyword with 9 similar negative words as context. When calculating the false positive rate for precision, we input 10 similar negative words as context.",
        "table": "S4.T8.2.1",
        "footnotes": [],
        "references": [
            "By incorporating the retrieved terms from the external knowledge database as contextual information, our models in-context learning ability significantly improves the performance of speech translation for in-domain terminologies.  Table   7  compares our method with the widely-used shallow fusion for intervention in the generated conclusion. When calculating the Recall, we input 1 ground-truth keyword with 9 similar negative words as context. When calculating the false positive rate for precision, we input 10 similar negative words as context. ICL is able to achieve a high recall rate with good precision, obtaining the highest F1 while shallow fusion only gets a recall rate that is only half of ICL."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Comparison between CLASI and Commerical 4 for zh-en direction.",
        "table": "S4.T9.2",
        "footnotes": [],
        "references": [
            "We present case studies to show the ability of CLASI in translating complicated speech for zh-en and en-zh in  Table   8  and  Table   9 . We choose one of the most-performed cascaded systems Commerical 4 for comparison. The Commerical 4 adopted a cascaded approach for SiST and it is shown in  Section   4.3  to be one of the best previous SiST systems. Detailed explanations are described in the tables. For zh-en direction, we present cases regarding robustness to recognition errors, reasoning ability, and trending words translation. For the en-zh direction, we present cases regarding native, expressive, and accurate terminology translations. More cases are shown in  Table   11 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Comparison between CLASI and Commerical 4 for en-zh direction.",
        "table": "A1.T10.2",
        "footnotes": [],
        "references": [
            "We present case studies to show the ability of CLASI in translating complicated speech for zh-en and en-zh in  Table   8  and  Table   9 . We choose one of the most-performed cascaded systems Commerical 4 for comparison. The Commerical 4 adopted a cascaded approach for SiST and it is shown in  Section   4.3  to be one of the best previous SiST systems. Detailed explanations are described in the tables. For zh-en direction, we present cases regarding robustness to recognition errors, reasoning ability, and trending words translation. For the en-zh direction, we present cases regarding native, expressive, and accurate terminology translations. More cases are shown in  Table   11 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Human evaluation examples of Chinese-to-English Translation task.",
        "table": "A2.T11.2",
        "footnotes": [],
        "references": [
            "We illustrate the evaluation criteria with two examples in Table  10 . Although these translations achieve high accuracy in automatic evaluations, we still categorize them as invalid. Its important to note that our standard aims to emulate human interpreters, presenting a significant challenge to both human evaluators and translation systems."
        ]
    },
    "global_footnotes": [
        "In this paper, we use Simultaneous Interpretation and Simultaneous Speech Translation interchangeably.",
        "Detailed guidelines of our proposed VIP metric can be found in Appendix",
        ".",
        "We use Doubao LLM as our foundation model.",
        "RealSI is available at",
        ". We do not own the copyright of the videos and only release our annotations together with the publicly available website links of the corresponding videos. If anyone believes that the content constitutes infringement, please contact us. We will remove the relevant content as soon as it is confirmed.",
        "We use SacreBLEU",
        "for all the BLEU calculations in this paper.",
        "We use the subset from in",
        "for evaluation",
        "We provide the full evaluation results of CLASI at"
    ]
}