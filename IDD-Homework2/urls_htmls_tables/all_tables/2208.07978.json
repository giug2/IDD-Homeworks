{
    "PAPER'S NUMBER OF TABLES": 1,
    "S4.T1": {
        "caption": "Table 1. Models distribution at local clients in the multi-model setting.",
        "table": "<table id=\"S4.T1.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.4.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.1.1.1\" class=\"ltx_td ltx_border_t\"></td>\n<th id=\"S4.T1.4.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.4.1.1.2.1\" class=\"ltx_text ltx_font_bold\">knowledge network</span></th>\n<th id=\"S4.T1.4.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S4.T1.4.1.1.3.1\" class=\"ltx_text ltx_font_bold\"># of {ResNet-20, ResNet-32, ResNet-44, VGG-11}</span></th>\n</tr>\n<tr id=\"S4.T1.4.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.4.2.2.1.1\" class=\"ltx_text\">30 clients</span></td>\n<td id=\"S4.T1.4.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">ResNet-20</td>\n<td id=\"S4.T1.4.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">{6, 5, 10, 9}</td>\n</tr>\n<tr id=\"S4.T1.4.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.3.3.1\" class=\"ltx_td ltx_align_center\">ResNet-32</td>\n<td id=\"S4.T1.4.3.3.2\" class=\"ltx_td ltx_align_center\">{0, 8, 13, 9}</td>\n</tr>\n<tr id=\"S4.T1.4.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.4.4.4.1.1\" class=\"ltx_text\">50 clients</span></td>\n<td id=\"S4.T1.4.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">ResNet-20</td>\n<td id=\"S4.T1.4.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">{13, 10, 14, 13}</td>\n</tr>\n<tr id=\"S4.T1.4.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.5.5.1\" class=\"ltx_td ltx_align_center\">ResNet-32</td>\n<td id=\"S4.T1.4.5.5.2\" class=\"ltx_td ltx_align_center\">{0, 13, 20, 17}</td>\n</tr>\n<tr id=\"S4.T1.4.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" rowspan=\"2\"><span id=\"S4.T1.4.6.6.1.1\" class=\"ltx_text\">100 clients</span></td>\n<td id=\"S4.T1.4.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">ResNet-20</td>\n<td id=\"S4.T1.4.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_t\">{26, 24, 28, 22}</td>\n</tr>\n<tr id=\"S4.T1.4.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T1.4.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_b\">ResNet-32</td>\n<td id=\"S4.T1.4.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_b\">{0, 31, 34, 35}</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In our evaluation, we assess FedKEM’s learning efficiency and optimization, particularly analyzing the correlation between communication rounds and the target model’s accuracy. The universal models trained for the baselines include VGG-11, ResNet-20, and ResNet-32, while ResNet-18 is used as the knowledge network for FedKEM, due to its status as a commonly utilized, minimal model in the ResNet family.",
                "Figure ",
                "4",
                " reveals that FedKEM delivers superior results in most benchmark settings compared to robust FL baselines, exhibiting a stable training process. It significantly surpasses baselines in handling over-parameterized networks like VGG-11 and is particularly adept at managing heterogeneous settings. For example, with 30 clients, FedKEM achieves 70% accuracy after 110 rounds, while all baselines fail to reach this accuracy even after 200 rounds. This gap widens with 50 clients, where baselines fail to attain 50% accuracy after 20000 total local updates.",
                "These results demonstrate FedKEM’s advantages in stability and consistency, particularly in high heterogeneity FL environments. However, SPATL, although performing well with 30 and 50 clients, does not compete effectively with other baselines when the number of clients increases to 100.",
                "We observe that the knowledge model used in FedKEM doesn’t significantly impact its performance. It consistently maintains stable training processes and high final convergence (over 70%) across all settings. The final converged accuracies for 30 and 50 clients are documented in Figure ",
                "5",
                ". Notably, FedKEM avoids gradient explosions during training, an issue observed in other methods ",
                "(yu20spatl, ",
                "45",
                ")",
                ".",
                "Baseline methods primarily concentrate on parameter and gradient aggregation for model fusion in the cloud. However, their aggregation approach, such as FedAvg’s weighted averaging, can introduce biases. This is primarily due to the contribution of individual edge models to the FL system being a black box.",
                "On the contrary, FedKEM uses ensemble distillation for model fusion. This approach generalizes heterogeneous edge models effectively, guiding the model towards an optimal direction, resulting in a much more stable optimization process across various non-IID FL settings."
            ]
        ]
    }
}