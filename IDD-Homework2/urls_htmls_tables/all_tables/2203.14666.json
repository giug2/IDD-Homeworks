{
    "PAPER'S NUMBER OF TABLES": 3,
    "S5.T1": {
        "caption": "Table 1: Comparison results with other popular FL algorithms on Cifar10 with VGG9. The left shows settings. The middle shows the cited results from FedMA [43], Fed2 [48], and FedDF [31]. The last two columns show the results we implement.",
        "table": "<table id=\"S5.T1.9\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.4.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">Settings (<math id=\"S5.T1.1.1.1.m1.4\" class=\"ltx_Math\" alttext=\"K,R,\\alpha,E\" display=\"inline\"><semantics id=\"S5.T1.1.1.1.m1.4a\"><mrow id=\"S5.T1.1.1.1.m1.4.5.2\" xref=\"S5.T1.1.1.1.m1.4.5.1.cmml\"><mi id=\"S5.T1.1.1.1.m1.1.1\" xref=\"S5.T1.1.1.1.m1.1.1.cmml\">K</mi><mo id=\"S5.T1.1.1.1.m1.4.5.2.1\" xref=\"S5.T1.1.1.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.T1.1.1.1.m1.2.2\" xref=\"S5.T1.1.1.1.m1.2.2.cmml\">R</mi><mo id=\"S5.T1.1.1.1.m1.4.5.2.2\" xref=\"S5.T1.1.1.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.T1.1.1.1.m1.3.3\" xref=\"S5.T1.1.1.1.m1.3.3.cmml\">α</mi><mo id=\"S5.T1.1.1.1.m1.4.5.2.3\" xref=\"S5.T1.1.1.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.T1.1.1.1.m1.4.4\" xref=\"S5.T1.1.1.1.m1.4.4.cmml\">E</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.1.1.1.m1.4b\"><list id=\"S5.T1.1.1.1.m1.4.5.1.cmml\" xref=\"S5.T1.1.1.1.m1.4.5.2\"><ci id=\"S5.T1.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.1.1.1.m1.1.1\">𝐾</ci><ci id=\"S5.T1.1.1.1.m1.2.2.cmml\" xref=\"S5.T1.1.1.1.m1.2.2\">𝑅</ci><ci id=\"S5.T1.1.1.1.m1.3.3.cmml\" xref=\"S5.T1.1.1.1.m1.3.3\">𝛼</ci><ci id=\"S5.T1.1.1.1.m1.4.4.cmml\" xref=\"S5.T1.1.1.1.m1.4.4\">𝐸</ci></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.1.1.1.m1.4c\">K,R,\\alpha,E</annotation></semantics></math>)</th>\n<th id=\"S5.T1.4.4.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FedAvg</th>\n<th id=\"S5.T1.4.4.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FedProx</th>\n<th id=\"S5.T1.4.4.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FedMA</th>\n<th id=\"S5.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Fed<sup id=\"S5.T1.2.2.2.1\" class=\"ltx_sup\">2</sup>\n</th>\n<th id=\"S5.T1.4.4.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">FedDF</th>\n<th id=\"S5.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FedAvg<sup id=\"S5.T1.3.3.3.1\" class=\"ltx_sup\">⋆</sup>\n</th>\n<th id=\"S5.T1.4.4.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FedAvg<sup id=\"S5.T1.4.4.4.1\" class=\"ltx_sup\">⋆</sup>+PANs</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">(<math id=\"S5.T1.5.5.1.m1.4\" class=\"ltx_Math\" alttext=\"16,1.0,0.5,20\" display=\"inline\"><semantics id=\"S5.T1.5.5.1.m1.4a\"><mrow id=\"S5.T1.5.5.1.m1.4.5.2\" xref=\"S5.T1.5.5.1.m1.4.5.1.cmml\"><mn id=\"S5.T1.5.5.1.m1.1.1\" xref=\"S5.T1.5.5.1.m1.1.1.cmml\">16</mn><mo id=\"S5.T1.5.5.1.m1.4.5.2.1\" xref=\"S5.T1.5.5.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T1.5.5.1.m1.2.2\" xref=\"S5.T1.5.5.1.m1.2.2.cmml\">1.0</mn><mo id=\"S5.T1.5.5.1.m1.4.5.2.2\" xref=\"S5.T1.5.5.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T1.5.5.1.m1.3.3\" xref=\"S5.T1.5.5.1.m1.3.3.cmml\">0.5</mn><mo id=\"S5.T1.5.5.1.m1.4.5.2.3\" xref=\"S5.T1.5.5.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T1.5.5.1.m1.4.4\" xref=\"S5.T1.5.5.1.m1.4.4.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.5.5.1.m1.4b\"><list id=\"S5.T1.5.5.1.m1.4.5.1.cmml\" xref=\"S5.T1.5.5.1.m1.4.5.2\"><cn type=\"integer\" id=\"S5.T1.5.5.1.m1.1.1.cmml\" xref=\"S5.T1.5.5.1.m1.1.1\">16</cn><cn type=\"float\" id=\"S5.T1.5.5.1.m1.2.2.cmml\" xref=\"S5.T1.5.5.1.m1.2.2\">1.0</cn><cn type=\"float\" id=\"S5.T1.5.5.1.m1.3.3.cmml\" xref=\"S5.T1.5.5.1.m1.3.3\">0.5</cn><cn type=\"integer\" id=\"S5.T1.5.5.1.m1.4.4.cmml\" xref=\"S5.T1.5.5.1.m1.4.4\">20</cn></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.5.5.1.m1.4c\">16,1.0,0.5,20</annotation></semantics></math>)</td>\n<td id=\"S5.T1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">86.29</td>\n<td id=\"S5.T1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\">85.32</td>\n<td id=\"S5.T1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">84.0 (<span id=\"S5.T1.6.6.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">87.53, <math id=\"S5.T1.6.6.2.1.m1.1\" class=\"ltx_Math\" alttext=\"E=150\" display=\"inline\"><semantics id=\"S5.T1.6.6.2.1.m1.1a\"><mrow id=\"S5.T1.6.6.2.1.m1.1.1\" xref=\"S5.T1.6.6.2.1.m1.1.1.cmml\"><mi id=\"S5.T1.6.6.2.1.m1.1.1.2\" xref=\"S5.T1.6.6.2.1.m1.1.1.2.cmml\">E</mi><mo id=\"S5.T1.6.6.2.1.m1.1.1.1\" xref=\"S5.T1.6.6.2.1.m1.1.1.1.cmml\">=</mo><mn id=\"S5.T1.6.6.2.1.m1.1.1.3\" xref=\"S5.T1.6.6.2.1.m1.1.1.3.cmml\">150</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.6.6.2.1.m1.1b\"><apply id=\"S5.T1.6.6.2.1.m1.1.1.cmml\" xref=\"S5.T1.6.6.2.1.m1.1.1\"><eq id=\"S5.T1.6.6.2.1.m1.1.1.1.cmml\" xref=\"S5.T1.6.6.2.1.m1.1.1.1\"></eq><ci id=\"S5.T1.6.6.2.1.m1.1.1.2.cmml\" xref=\"S5.T1.6.6.2.1.m1.1.1.2\">𝐸</ci><cn type=\"integer\" id=\"S5.T1.6.6.2.1.m1.1.1.3.cmml\" xref=\"S5.T1.6.6.2.1.m1.1.1.3\">150</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.6.6.2.1.m1.1c\">E=150</annotation></semantics></math>)</span>\n</td>\n<td id=\"S5.T1.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_t\">88.29</td>\n<td id=\"S5.T1.7.7.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">-</td>\n<td id=\"S5.T1.7.7.8\" class=\"ltx_td ltx_align_center ltx_border_t\">86.83</td>\n<td id=\"S5.T1.7.7.3\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S5.T1.7.7.3.1\" class=\"ltx_text ltx_font_bold\">88.49<math id=\"S5.T1.7.7.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T1.7.7.3.1.m1.1a\"><mo id=\"S5.T1.7.7.3.1.m1.1.1\" xref=\"S5.T1.7.7.3.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.7.7.3.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T1.7.7.3.1.m1.1.1.cmml\" xref=\"S5.T1.7.7.3.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.7.7.3.1.m1.1c\">\\pm</annotation></semantics></math>0.07</span></td>\n</tr>\n<tr id=\"S5.T1.9.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">(<math id=\"S5.T1.8.8.1.m1.4\" class=\"ltx_Math\" alttext=\"20,0.4,1.0,40\" display=\"inline\"><semantics id=\"S5.T1.8.8.1.m1.4a\"><mrow id=\"S5.T1.8.8.1.m1.4.5.2\" xref=\"S5.T1.8.8.1.m1.4.5.1.cmml\"><mn id=\"S5.T1.8.8.1.m1.1.1\" xref=\"S5.T1.8.8.1.m1.1.1.cmml\">20</mn><mo id=\"S5.T1.8.8.1.m1.4.5.2.1\" xref=\"S5.T1.8.8.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T1.8.8.1.m1.2.2\" xref=\"S5.T1.8.8.1.m1.2.2.cmml\">0.4</mn><mo id=\"S5.T1.8.8.1.m1.4.5.2.2\" xref=\"S5.T1.8.8.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T1.8.8.1.m1.3.3\" xref=\"S5.T1.8.8.1.m1.3.3.cmml\">1.0</mn><mo id=\"S5.T1.8.8.1.m1.4.5.2.3\" xref=\"S5.T1.8.8.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T1.8.8.1.m1.4.4\" xref=\"S5.T1.8.8.1.m1.4.4.cmml\">40</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.8.8.1.m1.4b\"><list id=\"S5.T1.8.8.1.m1.4.5.1.cmml\" xref=\"S5.T1.8.8.1.m1.4.5.2\"><cn type=\"integer\" id=\"S5.T1.8.8.1.m1.1.1.cmml\" xref=\"S5.T1.8.8.1.m1.1.1\">20</cn><cn type=\"float\" id=\"S5.T1.8.8.1.m1.2.2.cmml\" xref=\"S5.T1.8.8.1.m1.2.2\">0.4</cn><cn type=\"float\" id=\"S5.T1.8.8.1.m1.3.3.cmml\" xref=\"S5.T1.8.8.1.m1.3.3\">1.0</cn><cn type=\"integer\" id=\"S5.T1.8.8.1.m1.4.4.cmml\" xref=\"S5.T1.8.8.1.m1.4.4\">40</cn></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.8.8.1.m1.4c\">20,0.4,1.0,40</annotation></semantics></math>)</td>\n<td id=\"S5.T1.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">78.34</td>\n<td id=\"S5.T1.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">78.60</td>\n<td id=\"S5.T1.9.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">65.0</td>\n<td id=\"S5.T1.9.9.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">-</td>\n<td id=\"S5.T1.9.9.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">80.36</td>\n<td id=\"S5.T1.9.9.8\" class=\"ltx_td ltx_align_center ltx_border_bb\">79.76</td>\n<td id=\"S5.T1.9.9.2\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S5.T1.9.9.2.1\" class=\"ltx_text ltx_font_bold\">81.94<math id=\"S5.T1.9.9.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T1.9.9.2.1.m1.1a\"><mo id=\"S5.T1.9.9.2.1.m1.1.1\" xref=\"S5.T1.9.9.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.9.9.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T1.9.9.2.1.m1.1.1.cmml\" xref=\"S5.T1.9.9.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.9.9.2.1.m1.1c\">\\pm</annotation></semantics></math>0.09</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We propose several strategies from aspects of parameters, activations, and preference vectors to compare the neuron correspondences in FL with PANs off/on. For PANs turned on, we use multiplicative PANs with ",
                "T",
                "=",
                "1.0",
                "𝑇",
                "1.0",
                "T=1.0",
                " and ",
                "A",
                "=",
                "0.1",
                "𝐴",
                "0.1",
                "A=0.1",
                " by default.",
                "I. Weight Divergence:",
                " Weight divergence ",
                "[",
                "52",
                "]",
                " measures the variances of local parameters. Specifically, we calculate ",
                "1",
                "|",
                "S",
                "t",
                "|",
                "​",
                "∑",
                "k",
                "∈",
                "S",
                "t",
                "∥",
                "W",
                "l",
                "(",
                "k",
                ")",
                "−",
                "W",
                "l",
                "∥",
                "2",
                "1",
                "subscript",
                "𝑆",
                "𝑡",
                "subscript",
                "𝑘",
                "subscript",
                "𝑆",
                "𝑡",
                "subscript",
                "delimited-∥∥",
                "superscript",
                "subscript",
                "𝑊",
                "𝑙",
                "𝑘",
                "subscript",
                "𝑊",
                "𝑙",
                "2",
                "\\frac{1}{|S_{t}|}\\sum_{k\\in S_{t}}\\lVert W_{l}^{(k)}-W_{l}\\rVert_{2}",
                " for each layer ",
                "l",
                "𝑙",
                "l",
                ". ",
                "W",
                "l",
                "=",
                "1",
                "|",
                "S",
                "t",
                "|",
                "​",
                "∑",
                "k",
                "∈",
                "S",
                "t",
                "W",
                "l",
                "(",
                "k",
                ")",
                "subscript",
                "𝑊",
                "𝑙",
                "1",
                "subscript",
                "𝑆",
                "𝑡",
                "subscript",
                "𝑘",
                "subscript",
                "𝑆",
                "𝑡",
                "superscript",
                "subscript",
                "𝑊",
                "𝑙",
                "𝑘",
                "W_{l}=\\frac{1}{|S_{t}|}\\sum_{k\\in S_{t}}W_{l}^{(k)}",
                " denotes the averaged parameters. The weight divergences of MLP on Mnist with ",
                "α",
                "∈",
                "{",
                "1.0",
                ",",
                "0.1",
                "}",
                "𝛼",
                "1.0",
                "0.1",
                "\\alpha\\in\\{1.0,0.1\\}",
                " are in Fig. ",
                "6",
                ", where PANs could reduce the divergences a lot (the red bars). This corresponds to the explanation in Sect. ",
                "4.2",
                " that clients’ parameters are partially updated towards the same direction.",
                "II. Matching via Optimal Assignment:",
                " We feed 500 test samples into the network and obtain the activations of each neuron as its representation. Neurons’ representations of global and local model are denoted as ",
                "h",
                "l",
                "∈",
                "ℛ",
                "J",
                "l",
                "×",
                "m",
                "subscript",
                "ℎ",
                "𝑙",
                "superscript",
                "ℛ",
                "subscript",
                "𝐽",
                "𝑙",
                "𝑚",
                "h_{l}\\in\\mathcal{R}^{J_{l}\\times m}",
                " and ",
                "h",
                "l",
                "(",
                "k",
                ")",
                "∈",
                "ℛ",
                "J",
                "l",
                "×",
                "m",
                "superscript",
                "subscript",
                "ℎ",
                "𝑙",
                "𝑘",
                "superscript",
                "ℛ",
                "subscript",
                "𝐽",
                "𝑙",
                "𝑚",
                "h_{l}^{(k)}\\in\\mathcal{R}^{J_{l}\\times m}",
                ", where ",
                "m",
                "=",
                "500",
                "𝑚",
                "500",
                "m=500",
                ". Then we search for the optimal assignment matrix ",
                "Q",
                "∈",
                "{",
                "0",
                ",",
                "1",
                "}",
                "J",
                "l",
                "×",
                "J",
                "l",
                "𝑄",
                "superscript",
                "0",
                "1",
                "subscript",
                "𝐽",
                "𝑙",
                "subscript",
                "𝐽",
                "𝑙",
                "Q\\in\\{0,1\\}^{J_{l}\\times J_{l}}",
                " that minimizes ",
                "∑",
                "i",
                "=",
                "1",
                "J",
                "l",
                "∑",
                "j",
                "=",
                "1",
                "J",
                "l",
                "Q",
                "i",
                "​",
                "j",
                "​",
                "∥",
                "h",
                "l",
                ",",
                "i",
                "−",
                "h",
                "l",
                ",",
                "j",
                "(",
                "k",
                ")",
                "∥",
                "2",
                "superscript",
                "subscript",
                "𝑖",
                "1",
                "subscript",
                "𝐽",
                "𝑙",
                "superscript",
                "subscript",
                "𝑗",
                "1",
                "subscript",
                "𝐽",
                "𝑙",
                "subscript",
                "𝑄",
                "𝑖",
                "𝑗",
                "subscript",
                "delimited-∥∥",
                "subscript",
                "ℎ",
                "𝑙",
                "𝑖",
                "superscript",
                "subscript",
                "ℎ",
                "𝑙",
                "𝑗",
                "𝑘",
                "2",
                "\\sum_{i=1}^{J_{l}}\\sum_{j=1}^{J_{l}}Q_{ij}\\lVert h_{l,i}-h_{l,j}^{(k)}\\rVert_{2}",
                " and satisfies ",
                "∑",
                "i",
                "Q",
                "i",
                ",",
                "⋅",
                "=",
                "1",
                "subscript",
                "𝑖",
                "subscript",
                "𝑄",
                "𝑖",
                "⋅",
                "1",
                "\\sum_{i}Q_{i,\\cdot}=1",
                ", ",
                "∑",
                "j",
                "Q",
                "⋅",
                ",",
                "j",
                "=",
                "1",
                "subscript",
                "𝑗",
                "subscript",
                "𝑄",
                "⋅",
                "𝑗",
                "1",
                "\\sum_{j}Q_{\\cdot,j}=1",
                ". In fact, ",
                "Q",
                "𝑄",
                "Q",
                " is a permutation matrix that could approximately reflect the disturbance of neurons, and it could match neurons with similar outputs. We plot the solved matching matrix in Fig. ",
                "7",
                ", where the number in “[]” shows the ratio of the diagonal ones. Using PANs could make the diagonal denser, implying that neurons at the same coordinates output similarly.",
                "III. Visualizing Neurons via Preference Vectors:",
                " Then, we correspond neurons to classes via calculating preference vectors as done in ",
                "[",
                "48",
                "]",
                ". Specifically, we calculate ",
                "p",
                "c",
                "=",
                "∑",
                "b",
                "=",
                "1",
                "B",
                "Acti",
                "​",
                "(",
                "x",
                "c",
                ",",
                "b",
                ")",
                "​",
                "∂",
                "Z",
                "c",
                "∂",
                "Acti",
                "​",
                "(",
                "x",
                "c",
                ",",
                "b",
                ")",
                "subscript",
                "𝑝",
                "𝑐",
                "superscript",
                "subscript",
                "𝑏",
                "1",
                "𝐵",
                "Acti",
                "subscript",
                "𝑥",
                "𝑐",
                "𝑏",
                "subscript",
                "𝑍",
                "𝑐",
                "Acti",
                "subscript",
                "𝑥",
                "𝑐",
                "𝑏",
                "p_{c}=\\sum_{b=1}^{B}\\text{Acti}(x_{c,b})\\frac{\\partial Z_{c}}{\\partial\\text{Acti}(x_{c,b})}",
                " for each class ",
                "c",
                "𝑐",
                "c",
                ", and then concatenate all classes as the preference vector ",
                "[",
                "p",
                "1",
                ",",
                "p",
                "2",
                ",",
                "⋯",
                ",",
                "p",
                "C",
                "]",
                "subscript",
                "𝑝",
                "1",
                "subscript",
                "𝑝",
                "2",
                "⋯",
                "subscript",
                "𝑝",
                "𝐶",
                "[p_{1},p_{2},\\cdots,p_{C}]",
                ". ",
                "Acti",
                "​",
                "(",
                "⋅",
                ")",
                "Acti",
                "⋅",
                "\\text{Acti}(\\cdot)",
                " denotes the activation value and ",
                "Z",
                "c",
                "subscript",
                "𝑍",
                "𝑐",
                "Z_{c}",
                " is the prediction score of the ",
                "c",
                "𝑐",
                "c",
                "th class. Then, ",
                "arg",
                "⁡",
                "max",
                "c",
                "⁡",
                "p",
                "c",
                "subscript",
                "𝑐",
                "subscript",
                "𝑝",
                "𝑐",
                "\\arg\\max_{c}p_{c}",
                " implies which class the neuron contributes to more. The results are shown in Fig. ",
                "8",
                ", where each vertical line represents a neuron/channel. The number in “[]” shows how much neurons/channels correspond to the same class between global and local models. With PANs, the coordinate matching results are better. These empirical results verify the pre-alignment effects brought by PANs."
            ]
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Comparison results with SOTA on more scenes. The results are all implemented by our reproduced code.",
        "table": "<table id=\"S5.T2.9\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.5.5\" class=\"ltx_tr\">\n<th id=\"S5.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">(<math id=\"S5.T2.1.1.1.m1.4\" class=\"ltx_Math\" alttext=\"K,R,\\alpha,E\" display=\"inline\"><semantics id=\"S5.T2.1.1.1.m1.4a\"><mrow id=\"S5.T2.1.1.1.m1.4.5.2\" xref=\"S5.T2.1.1.1.m1.4.5.1.cmml\"><mi id=\"S5.T2.1.1.1.m1.1.1\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\">K</mi><mo id=\"S5.T2.1.1.1.m1.4.5.2.1\" xref=\"S5.T2.1.1.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.T2.1.1.1.m1.2.2\" xref=\"S5.T2.1.1.1.m1.2.2.cmml\">R</mi><mo id=\"S5.T2.1.1.1.m1.4.5.2.2\" xref=\"S5.T2.1.1.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.T2.1.1.1.m1.3.3\" xref=\"S5.T2.1.1.1.m1.3.3.cmml\">α</mi><mo id=\"S5.T2.1.1.1.m1.4.5.2.3\" xref=\"S5.T2.1.1.1.m1.4.5.1.cmml\">,</mo><mi id=\"S5.T2.1.1.1.m1.4.4\" xref=\"S5.T2.1.1.1.m1.4.4.cmml\">E</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.4b\"><list id=\"S5.T2.1.1.1.m1.4.5.1.cmml\" xref=\"S5.T2.1.1.1.m1.4.5.2\"><ci id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\">𝐾</ci><ci id=\"S5.T2.1.1.1.m1.2.2.cmml\" xref=\"S5.T2.1.1.1.m1.2.2\">𝑅</ci><ci id=\"S5.T2.1.1.1.m1.3.3.cmml\" xref=\"S5.T2.1.1.1.m1.3.3\">𝛼</ci><ci id=\"S5.T2.1.1.1.m1.4.4.cmml\" xref=\"S5.T2.1.1.1.m1.4.4\">𝐸</ci></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.4c\">K,R,\\alpha,E</annotation></semantics></math>)</th>\n<th id=\"S5.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FedMA<sup id=\"S5.T2.2.2.2.1\" class=\"ltx_sup\">⋆</sup>\n</th>\n<th id=\"S5.T2.4.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">Fed<sup id=\"S5.T2.4.4.4.1\" class=\"ltx_sup\"><span id=\"S5.T2.4.4.4.1.1\" class=\"ltx_text ltx_font_italic\">2</span></sup><sup id=\"S5.T2.4.4.4.2\" class=\"ltx_sup\">⋆</sup>\n</th>\n<th id=\"S5.T2.5.5.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FedAvg<sup id=\"S5.T2.5.5.5.1\" class=\"ltx_sup\">⋆</sup>+PANs</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.7.7\" class=\"ltx_tr\">\n<th id=\"S5.T2.6.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">(<math id=\"S5.T2.6.6.1.m1.4\" class=\"ltx_Math\" alttext=\"16,1.0,\\underline{0.1},20\" display=\"inline\"><semantics id=\"S5.T2.6.6.1.m1.4a\"><mrow id=\"S5.T2.6.6.1.m1.4.5.2\" xref=\"S5.T2.6.6.1.m1.4.5.1.cmml\"><mn id=\"S5.T2.6.6.1.m1.1.1\" xref=\"S5.T2.6.6.1.m1.1.1.cmml\">16</mn><mo id=\"S5.T2.6.6.1.m1.4.5.2.1\" xref=\"S5.T2.6.6.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T2.6.6.1.m1.2.2\" xref=\"S5.T2.6.6.1.m1.2.2.cmml\">1.0</mn><mo id=\"S5.T2.6.6.1.m1.4.5.2.2\" xref=\"S5.T2.6.6.1.m1.4.5.1.cmml\">,</mo><munder accentunder=\"true\" id=\"S5.T2.6.6.1.m1.3.3\" xref=\"S5.T2.6.6.1.m1.3.3.cmml\"><mn id=\"S5.T2.6.6.1.m1.3.3.2\" xref=\"S5.T2.6.6.1.m1.3.3.2.cmml\">0.1</mn><mo id=\"S5.T2.6.6.1.m1.3.3.1\" xref=\"S5.T2.6.6.1.m1.3.3.1.cmml\">¯</mo></munder><mo id=\"S5.T2.6.6.1.m1.4.5.2.3\" xref=\"S5.T2.6.6.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T2.6.6.1.m1.4.4\" xref=\"S5.T2.6.6.1.m1.4.4.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.6.6.1.m1.4b\"><list id=\"S5.T2.6.6.1.m1.4.5.1.cmml\" xref=\"S5.T2.6.6.1.m1.4.5.2\"><cn type=\"integer\" id=\"S5.T2.6.6.1.m1.1.1.cmml\" xref=\"S5.T2.6.6.1.m1.1.1\">16</cn><cn type=\"float\" id=\"S5.T2.6.6.1.m1.2.2.cmml\" xref=\"S5.T2.6.6.1.m1.2.2\">1.0</cn><apply id=\"S5.T2.6.6.1.m1.3.3.cmml\" xref=\"S5.T2.6.6.1.m1.3.3\"><ci id=\"S5.T2.6.6.1.m1.3.3.1.cmml\" xref=\"S5.T2.6.6.1.m1.3.3.1\">¯</ci><cn type=\"float\" id=\"S5.T2.6.6.1.m1.3.3.2.cmml\" xref=\"S5.T2.6.6.1.m1.3.3.2\">0.1</cn></apply><cn type=\"integer\" id=\"S5.T2.6.6.1.m1.4.4.cmml\" xref=\"S5.T2.6.6.1.m1.4.4\">20</cn></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.6.6.1.m1.4c\">16,1.0,\\underline{0.1},20</annotation></semantics></math>)</th>\n<td id=\"S5.T2.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_t\">83.91</td>\n<td id=\"S5.T2.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">82.26</td>\n<td id=\"S5.T2.7.7.2\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"><span id=\"S5.T2.7.7.2.1\" class=\"ltx_text ltx_font_bold\">85.82 <math id=\"S5.T2.7.7.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T2.7.7.2.1.m1.1a\"><mo id=\"S5.T2.7.7.2.1.m1.1.1\" xref=\"S5.T2.7.7.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.7.7.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T2.7.7.2.1.m1.1.1.cmml\" xref=\"S5.T2.7.7.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.7.7.2.1.m1.1c\">\\pm</annotation></semantics></math>0.16</span></td>\n</tr>\n<tr id=\"S5.T2.9.9\" class=\"ltx_tr\">\n<th id=\"S5.T2.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">(<math id=\"S5.T2.8.8.1.m1.4\" class=\"ltx_Math\" alttext=\"16,\\underline{0.4},0.5,20\" display=\"inline\"><semantics id=\"S5.T2.8.8.1.m1.4a\"><mrow id=\"S5.T2.8.8.1.m1.4.5.2\" xref=\"S5.T2.8.8.1.m1.4.5.1.cmml\"><mn id=\"S5.T2.8.8.1.m1.1.1\" xref=\"S5.T2.8.8.1.m1.1.1.cmml\">16</mn><mo id=\"S5.T2.8.8.1.m1.4.5.2.1\" xref=\"S5.T2.8.8.1.m1.4.5.1.cmml\">,</mo><munder accentunder=\"true\" id=\"S5.T2.8.8.1.m1.2.2\" xref=\"S5.T2.8.8.1.m1.2.2.cmml\"><mn id=\"S5.T2.8.8.1.m1.2.2.2\" xref=\"S5.T2.8.8.1.m1.2.2.2.cmml\">0.4</mn><mo id=\"S5.T2.8.8.1.m1.2.2.1\" xref=\"S5.T2.8.8.1.m1.2.2.1.cmml\">¯</mo></munder><mo id=\"S5.T2.8.8.1.m1.4.5.2.2\" xref=\"S5.T2.8.8.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T2.8.8.1.m1.3.3\" xref=\"S5.T2.8.8.1.m1.3.3.cmml\">0.5</mn><mo id=\"S5.T2.8.8.1.m1.4.5.2.3\" xref=\"S5.T2.8.8.1.m1.4.5.1.cmml\">,</mo><mn id=\"S5.T2.8.8.1.m1.4.4\" xref=\"S5.T2.8.8.1.m1.4.4.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.8.8.1.m1.4b\"><list id=\"S5.T2.8.8.1.m1.4.5.1.cmml\" xref=\"S5.T2.8.8.1.m1.4.5.2\"><cn type=\"integer\" id=\"S5.T2.8.8.1.m1.1.1.cmml\" xref=\"S5.T2.8.8.1.m1.1.1\">16</cn><apply id=\"S5.T2.8.8.1.m1.2.2.cmml\" xref=\"S5.T2.8.8.1.m1.2.2\"><ci id=\"S5.T2.8.8.1.m1.2.2.1.cmml\" xref=\"S5.T2.8.8.1.m1.2.2.1\">¯</ci><cn type=\"float\" id=\"S5.T2.8.8.1.m1.2.2.2.cmml\" xref=\"S5.T2.8.8.1.m1.2.2.2\">0.4</cn></apply><cn type=\"float\" id=\"S5.T2.8.8.1.m1.3.3.cmml\" xref=\"S5.T2.8.8.1.m1.3.3\">0.5</cn><cn type=\"integer\" id=\"S5.T2.8.8.1.m1.4.4.cmml\" xref=\"S5.T2.8.8.1.m1.4.4\">20</cn></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.8.8.1.m1.4c\">16,\\underline{0.4},0.5,20</annotation></semantics></math>)</th>\n<td id=\"S5.T2.9.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">48.25</td>\n<td id=\"S5.T2.9.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">81.23</td>\n<td id=\"S5.T2.9.9.2\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S5.T2.9.9.2.1\" class=\"ltx_text ltx_font_bold\">82.87 <math id=\"S5.T2.9.9.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\pm\" display=\"inline\"><semantics id=\"S5.T2.9.9.2.1.m1.1a\"><mo id=\"S5.T2.9.9.2.1.m1.1.1\" xref=\"S5.T2.9.9.2.1.m1.1.1.cmml\">±</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.9.9.2.1.m1.1b\"><csymbol cd=\"latexml\" id=\"S5.T2.9.9.2.1.m1.1.1.cmml\" xref=\"S5.T2.9.9.2.1.m1.1.1\">plus-or-minus</csymbol></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.9.9.2.1.m1.1c\">\\pm</annotation></semantics></math>0.21</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We propose several strategies from aspects of parameters, activations, and preference vectors to compare the neuron correspondences in FL with PANs off/on. For PANs turned on, we use multiplicative PANs with ",
                "T",
                "=",
                "1.0",
                "𝑇",
                "1.0",
                "T=1.0",
                " and ",
                "A",
                "=",
                "0.1",
                "𝐴",
                "0.1",
                "A=0.1",
                " by default.",
                "I. Weight Divergence:",
                " Weight divergence ",
                "[",
                "52",
                "]",
                " measures the variances of local parameters. Specifically, we calculate ",
                "1",
                "|",
                "S",
                "t",
                "|",
                "​",
                "∑",
                "k",
                "∈",
                "S",
                "t",
                "∥",
                "W",
                "l",
                "(",
                "k",
                ")",
                "−",
                "W",
                "l",
                "∥",
                "2",
                "1",
                "subscript",
                "𝑆",
                "𝑡",
                "subscript",
                "𝑘",
                "subscript",
                "𝑆",
                "𝑡",
                "subscript",
                "delimited-∥∥",
                "superscript",
                "subscript",
                "𝑊",
                "𝑙",
                "𝑘",
                "subscript",
                "𝑊",
                "𝑙",
                "2",
                "\\frac{1}{|S_{t}|}\\sum_{k\\in S_{t}}\\lVert W_{l}^{(k)}-W_{l}\\rVert_{2}",
                " for each layer ",
                "l",
                "𝑙",
                "l",
                ". ",
                "W",
                "l",
                "=",
                "1",
                "|",
                "S",
                "t",
                "|",
                "​",
                "∑",
                "k",
                "∈",
                "S",
                "t",
                "W",
                "l",
                "(",
                "k",
                ")",
                "subscript",
                "𝑊",
                "𝑙",
                "1",
                "subscript",
                "𝑆",
                "𝑡",
                "subscript",
                "𝑘",
                "subscript",
                "𝑆",
                "𝑡",
                "superscript",
                "subscript",
                "𝑊",
                "𝑙",
                "𝑘",
                "W_{l}=\\frac{1}{|S_{t}|}\\sum_{k\\in S_{t}}W_{l}^{(k)}",
                " denotes the averaged parameters. The weight divergences of MLP on Mnist with ",
                "α",
                "∈",
                "{",
                "1.0",
                ",",
                "0.1",
                "}",
                "𝛼",
                "1.0",
                "0.1",
                "\\alpha\\in\\{1.0,0.1\\}",
                " are in Fig. ",
                "6",
                ", where PANs could reduce the divergences a lot (the red bars). This corresponds to the explanation in Sect. ",
                "4.2",
                " that clients’ parameters are partially updated towards the same direction.",
                "II. Matching via Optimal Assignment:",
                " We feed 500 test samples into the network and obtain the activations of each neuron as its representation. Neurons’ representations of global and local model are denoted as ",
                "h",
                "l",
                "∈",
                "ℛ",
                "J",
                "l",
                "×",
                "m",
                "subscript",
                "ℎ",
                "𝑙",
                "superscript",
                "ℛ",
                "subscript",
                "𝐽",
                "𝑙",
                "𝑚",
                "h_{l}\\in\\mathcal{R}^{J_{l}\\times m}",
                " and ",
                "h",
                "l",
                "(",
                "k",
                ")",
                "∈",
                "ℛ",
                "J",
                "l",
                "×",
                "m",
                "superscript",
                "subscript",
                "ℎ",
                "𝑙",
                "𝑘",
                "superscript",
                "ℛ",
                "subscript",
                "𝐽",
                "𝑙",
                "𝑚",
                "h_{l}^{(k)}\\in\\mathcal{R}^{J_{l}\\times m}",
                ", where ",
                "m",
                "=",
                "500",
                "𝑚",
                "500",
                "m=500",
                ". Then we search for the optimal assignment matrix ",
                "Q",
                "∈",
                "{",
                "0",
                ",",
                "1",
                "}",
                "J",
                "l",
                "×",
                "J",
                "l",
                "𝑄",
                "superscript",
                "0",
                "1",
                "subscript",
                "𝐽",
                "𝑙",
                "subscript",
                "𝐽",
                "𝑙",
                "Q\\in\\{0,1\\}^{J_{l}\\times J_{l}}",
                " that minimizes ",
                "∑",
                "i",
                "=",
                "1",
                "J",
                "l",
                "∑",
                "j",
                "=",
                "1",
                "J",
                "l",
                "Q",
                "i",
                "​",
                "j",
                "​",
                "∥",
                "h",
                "l",
                ",",
                "i",
                "−",
                "h",
                "l",
                ",",
                "j",
                "(",
                "k",
                ")",
                "∥",
                "2",
                "superscript",
                "subscript",
                "𝑖",
                "1",
                "subscript",
                "𝐽",
                "𝑙",
                "superscript",
                "subscript",
                "𝑗",
                "1",
                "subscript",
                "𝐽",
                "𝑙",
                "subscript",
                "𝑄",
                "𝑖",
                "𝑗",
                "subscript",
                "delimited-∥∥",
                "subscript",
                "ℎ",
                "𝑙",
                "𝑖",
                "superscript",
                "subscript",
                "ℎ",
                "𝑙",
                "𝑗",
                "𝑘",
                "2",
                "\\sum_{i=1}^{J_{l}}\\sum_{j=1}^{J_{l}}Q_{ij}\\lVert h_{l,i}-h_{l,j}^{(k)}\\rVert_{2}",
                " and satisfies ",
                "∑",
                "i",
                "Q",
                "i",
                ",",
                "⋅",
                "=",
                "1",
                "subscript",
                "𝑖",
                "subscript",
                "𝑄",
                "𝑖",
                "⋅",
                "1",
                "\\sum_{i}Q_{i,\\cdot}=1",
                ", ",
                "∑",
                "j",
                "Q",
                "⋅",
                ",",
                "j",
                "=",
                "1",
                "subscript",
                "𝑗",
                "subscript",
                "𝑄",
                "⋅",
                "𝑗",
                "1",
                "\\sum_{j}Q_{\\cdot,j}=1",
                ". In fact, ",
                "Q",
                "𝑄",
                "Q",
                " is a permutation matrix that could approximately reflect the disturbance of neurons, and it could match neurons with similar outputs. We plot the solved matching matrix in Fig. ",
                "7",
                ", where the number in “[]” shows the ratio of the diagonal ones. Using PANs could make the diagonal denser, implying that neurons at the same coordinates output similarly.",
                "III. Visualizing Neurons via Preference Vectors:",
                " Then, we correspond neurons to classes via calculating preference vectors as done in ",
                "[",
                "48",
                "]",
                ". Specifically, we calculate ",
                "p",
                "c",
                "=",
                "∑",
                "b",
                "=",
                "1",
                "B",
                "Acti",
                "​",
                "(",
                "x",
                "c",
                ",",
                "b",
                ")",
                "​",
                "∂",
                "Z",
                "c",
                "∂",
                "Acti",
                "​",
                "(",
                "x",
                "c",
                ",",
                "b",
                ")",
                "subscript",
                "𝑝",
                "𝑐",
                "superscript",
                "subscript",
                "𝑏",
                "1",
                "𝐵",
                "Acti",
                "subscript",
                "𝑥",
                "𝑐",
                "𝑏",
                "subscript",
                "𝑍",
                "𝑐",
                "Acti",
                "subscript",
                "𝑥",
                "𝑐",
                "𝑏",
                "p_{c}=\\sum_{b=1}^{B}\\text{Acti}(x_{c,b})\\frac{\\partial Z_{c}}{\\partial\\text{Acti}(x_{c,b})}",
                " for each class ",
                "c",
                "𝑐",
                "c",
                ", and then concatenate all classes as the preference vector ",
                "[",
                "p",
                "1",
                ",",
                "p",
                "2",
                ",",
                "⋯",
                ",",
                "p",
                "C",
                "]",
                "subscript",
                "𝑝",
                "1",
                "subscript",
                "𝑝",
                "2",
                "⋯",
                "subscript",
                "𝑝",
                "𝐶",
                "[p_{1},p_{2},\\cdots,p_{C}]",
                ". ",
                "Acti",
                "​",
                "(",
                "⋅",
                ")",
                "Acti",
                "⋅",
                "\\text{Acti}(\\cdot)",
                " denotes the activation value and ",
                "Z",
                "c",
                "subscript",
                "𝑍",
                "𝑐",
                "Z_{c}",
                " is the prediction score of the ",
                "c",
                "𝑐",
                "c",
                "th class. Then, ",
                "arg",
                "⁡",
                "max",
                "c",
                "⁡",
                "p",
                "c",
                "subscript",
                "𝑐",
                "subscript",
                "𝑝",
                "𝑐",
                "\\arg\\max_{c}p_{c}",
                " implies which class the neuron contributes to more. The results are shown in Fig. ",
                "8",
                ", where each vertical line represents a neuron/channel. The number in “[]” shows how much neurons/channels correspond to the same class between global and local models. With PANs, the coordinate matching results are better. These empirical results verify the pre-alignment effects brought by PANs."
            ]
        ]
    },
    "A3.T3": {
        "caption": "Table 3: The performances of centralized training with corresponding networks (without PANs), i.e., the upper bound of decentralized training (FL).",
        "table": "<table id=\"A3.T3.2\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A3.T3.2.1.1\" class=\"ltx_tr\">\n<td id=\"A3.T3.2.1.1.1\" class=\"ltx_td ltx_border_tt\"></td>\n<th id=\"A3.T3.2.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">FeMnist</th>\n<th id=\"A3.T3.2.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">GTSRB</th>\n<th id=\"A3.T3.2.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">SVHN</th>\n<th id=\"A3.T3.2.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Cifar10</th>\n<th id=\"A3.T3.2.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Cifar100</th>\n<th id=\"A3.T3.2.1.1.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Cinic10</th>\n</tr>\n<tr id=\"A3.T3.2.2.2\" class=\"ltx_tr\">\n<td id=\"A3.T3.2.2.2.1\" class=\"ltx_td\"></td>\n<td id=\"A3.T3.2.2.2.2\" class=\"ltx_td ltx_align_center\">MLP</td>\n<td id=\"A3.T3.2.2.2.3\" class=\"ltx_td ltx_align_center\">VGG9</td>\n<td id=\"A3.T3.2.2.2.4\" class=\"ltx_td ltx_align_center\">VGG9</td>\n<td id=\"A3.T3.2.2.2.5\" class=\"ltx_td ltx_align_center\">VGG11</td>\n<td id=\"A3.T3.2.2.2.6\" class=\"ltx_td ltx_align_center\">ResNet20</td>\n<td id=\"A3.T3.2.2.2.7\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ResNet20</td>\n</tr>\n<tr id=\"A3.T3.2.3.3\" class=\"ltx_tr\">\n<th id=\"A3.T3.2.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">SGD + Momentum=0.9 (LR in {0.05,0.1})</th>\n<th id=\"A3.T3.2.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">53.39</th>\n<th id=\"A3.T3.2.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">86.96</th>\n<th id=\"A3.T3.2.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">89.93</th>\n<th id=\"A3.T3.2.3.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">84.57</th>\n<th id=\"A3.T3.2.3.3.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\">70.82</th>\n<th id=\"A3.T3.2.3.3.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\">82.76</th>\n</tr>\n<tr id=\"A3.T3.2.4.4\" class=\"ltx_tr\">\n<td id=\"A3.T3.2.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_bb\">Adam (LR=3e-4)</td>\n<td id=\"A3.T3.2.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">54.25</td>\n<td id=\"A3.T3.2.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">90.84</td>\n<td id=\"A3.T3.2.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">91.13</td>\n<td id=\"A3.T3.2.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\">87.13</td>\n<td id=\"A3.T3.2.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_bb\">67.22</td>\n<td id=\"A3.T3.2.4.4.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">81.99</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "For both centralized training and decentralized training (i.e., FL), we take a constant learning rate without scheduling, although some works have pointed out decaying the learning rate will help in FL ",
                "[",
                "5",
                "]",
                ". We take SGD with momentum 0.9 as the optimizer by default if without more declaration. For MLP and VGG networks, we set the learning rate as 0.05; for ResNet, we use 0.1. We respectively use a warm start with 100 training steps and 10 training steps for centralized training and decentralized training (during local training). We use batch size 10 for FeMnist and 64 for other datasets.",
                "We use FedAvg ",
                "[",
                "32",
                "]",
                ", FedProx ",
                "[",
                "28",
                "]",
                ", FedOpt ",
                "[",
                "35",
                "]",
                ", Scaffold ",
                "[",
                "19",
                "]",
                ", and MOON ",
                "[",
                "27",
                "]",
                " as base FL algorithms. For all of these algorithms, we take ",
                "H",
                "𝐻",
                "H",
                " communication rounds, and select ",
                "R",
                "∗",
                "100.0",
                "%",
                "𝑅",
                "percent",
                "100.0",
                "R*100.0\\%",
                " clients during each round. Each client updates the global model on their private data for ",
                "E",
                "𝐸",
                "E",
                " epochs. For FedProx, the regularization coefficient of the proximal term is tuned in ",
                "{",
                "1",
                "​",
                "e",
                "−",
                "4",
                ",",
                "1",
                "​",
                "e",
                "−",
                "3",
                "}",
                "1",
                "𝑒",
                "4",
                "1",
                "𝑒",
                "3",
                "\\{1e-4,1e-3\\}",
                " and the best one is reported. For FedOpt, we take SGD with momentum 0.9 as the global optimizer, and tune the global learning rate in ",
                "{",
                "0.1",
                ",",
                "0.5",
                ",",
                "0.9",
                "}",
                "0.1",
                "0.5",
                "0.9",
                "\\{0.1,0.5,0.9\\}",
                ", which is similar to FedAvgM ",
                "[",
                "16",
                "]",
                ". We also try using Adam as the global optimizer and find the performances are not stable. For Scaffold, we use the implementation from the online page ",
                "2",
                "2",
                "2",
                "https://github.com/ramshi236/Accelerated-Federated-Learning-Over-MAC-in-Heterogeneous-Networks",
                ". For MOON, we set the coefficient of the contrastive loss as ",
                "1.0",
                "1.0",
                "1.0",
                ", which is recommended by the authors. We then replace the normal neurons with the proposed PANs to improve these algorithms. We keep ",
                "T",
                "=",
                "1",
                "𝑇",
                "1",
                "T=1",
                " by default and tune hyper-parameters from: PAN",
                "+",
                " with ",
                "A",
                "=",
                "0.05",
                "𝐴",
                "0.05",
                "A=0.05",
                ", PAN",
                "∘",
                " with ",
                "A",
                "=",
                "0.05",
                "𝐴",
                "0.05",
                "A=0.05",
                ", PAN",
                "∘",
                " with ",
                "A",
                "=",
                "0.1",
                "𝐴",
                "0.1",
                "A=0.1",
                "."
            ]
        ]
    }
}