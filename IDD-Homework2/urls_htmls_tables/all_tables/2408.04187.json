{
    "id_table_1": {
        "caption": "Table 1:  The improvement of MedGraphRAG on various LLMs.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "First, we conducted experiments to assess the impact of our Medical Graph RAG on various large language models, with the results presented in Table  1 . The data reveals that our MedGraphRAG significantly enhances the performance of LLMs on medical benchmarks. This improvement is attributed to the implementation of zero-shot RAG, which is more cost-effective, faster, and more convenient than fine-tuning or using adapters. Notably, MedGraphRAG yields more substantial improvements in smaller LLMs, such as LLaMA2-13B and LLaMA3-8B, which typically underperform on these benchmarks, thus broadening its applicability across a wider user base. MedGraphRAG also significantly boosts the performance of more powerful, closed-source LLMs like GPT and LLaMA3-70B, helping them achieve state-of-the-art (SOTA) results on multiple benchmarks. These outcomes surpass the accuracy of human experts, demonstrating the strong potential of AI to enhance clinical workflows."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  An ablation study on MedGraphRAG.",
        "table": "S3.T2.1.1",
        "footnotes": [],
        "references": [
            "We also evaluated MedGraphRAG against a range of previous state-of-the-art (SOTA) models on these benchmarks, including both intensively fine-tuned models  Gu et al. ( 2022 ) Yasunaga et al. ( 2022a ) Yasunaga et al. ( 2022b ) Bolton et al. ( 2022 ) Singhal et al. ( 2022 ) Singhal et al. ( 2023 ) Wu et al. ( 2023 )  and non-fine-tuned models  Nori et al. ( 2023 ) OpenAI ( 2023a ) OpenAI ( 2023b )  on the MedQA benchmark. The results, depicted in Figure  2 , show that when applied to a powerful GPT-4 LLM, our MedGraphRAG surpasses the previous SOTA prompted model, Medprompt  Nori et al. ( 2023 ) , by a significant 1.1%. Even when compared with intensive fine-tuning methods on these medical datasets, MedGraphRAG outperforms all and achieves the SOTA. This superior performance stems from leveraging the inherent capabilities of the robust GPT-4 model. This further underscores the advantages of our non-fine-tuned MedGraphRAG approach: it inherits the strong capabilities of a closed-source model and outperforms many models that require costly and exhaustive fine-tuning.",
            "We conducted a comprehensive ablation study to validate the effectiveness of our proposed modules, the results of which are presented in Table  2 . This study compares various methods for document chunking, hierarchy graph construction, and information retrieval. Specifically, for document chunking, we evaluated our hybrid static-semantic method against a purely static approach. For hierarchy graph construction, we contrasted our method with the basic construction approach used in LangChain. For information retrieval, we compared the summarized-based retrieval  Edge et al. ( 2024 )  with our U-retrieve method. These methods were assessed across the three Q&A benchmarks previously mentioned."
        ]
    },
    "global_footnotes": []
}