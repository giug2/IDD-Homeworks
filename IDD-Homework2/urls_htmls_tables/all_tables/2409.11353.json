{
    "id_table_1": {
        "caption": "Table 1:  Comparison of models on Hallucination Generation and Identification. The data is split into two hallucination criteriaHallucination Generation and Hallucination Identificationwith each model undergoing several mitigation experiments: Original, ICL, RAG and PEFT (applied only to Llama3.1). The metrics  A F subscript A F A_{F} italic_A start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT ,  A R subscript A R A_{R} italic_A start_POSTSUBSCRIPT italic_R end_POSTSUBSCRIPT ,  A C subscript A C A_{C} italic_A start_POSTSUBSCRIPT italic_C end_POSTSUBSCRIPT , and  A S subscript A S A_{S} italic_A start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT  represent Answer Faithfulness, Answer Relevancy, Answer Correctness, and Answer Similarity, respectively. The metrics  A  c  c . A c c Acc. italic_A italic_c italic_c . ,  P  r  e  c . P r e c Prec. italic_P italic_r italic_e italic_c . ,  R  e  c . R e c Rec. italic_R italic_e italic_c . , and  F  1 F 1 F1 italic_F 1  represent Accuracy, Precision, Recall, and F1-Score, respectively.",
        "table": "S3.T1.24",
        "footnotes": [],
        "references": [
            "The THaMES framework is divided into three main components as shown in figure  1 :  (1)  Testset generation from a user-provided corpus.  (2)  Baseline metric evaluation based on testset. and  (3)  Mitigation strategy evaluation based on baseline metrics. In integrating these components, we present an end-to-end solution that transforms raw corpora into specialized LLM benchmarks, revealing optimal hallucination mitigation strategies.",
            "We chose  GPT-4o-mini   (OpenAI,  2024 )  for question generation to minimize human validation, due to its high performance and relatively low cost. The HaluEval testset had simple questions with short answers, so we created six question types:  [simple, reasoning, multi-context, situational, distracting, and double]  based on RAGAS  (Es et al.,  2023 )  and Giskard  (Giskard AI,  2024 ) . Unlike these frameworks, our type definitions were designed not for retrieval-augmented generation (RAG) systems but for general LLM hallucination evaluation. We also introduced rules to ensure high generated question quality, such as avoiding ambiguous references, numerical estimates, ensuring self-containment, etc. (detailed in Appendix  A.3.1 ). To generate  simple -type questions, we used few-shot prompting, instructing  GPT-4o-mini  to output batches of JSON objects (Appendix  A.2.1 ). Batch generation was chosen for its cost-effectiveness and ability to minimize repeated questions, ensuring testset diversity. The questions were then evolved into one of the six predefined types through a filtering process (Appendix  A.2.3 ) to ensure quality and compliance with our criteria. Next, the model generated correct answers for each question, using the original context from the knowledge base to ensure factual accuracy.",
            "A key feature of the THaMES testset is its inclusion of hallucinated answers. HaluEval  (Li et al.,  2023 )  used other models including  GPT-3.5   (Brown et al.,  2020 )  to generate and select hallucinated answers, but this lacked interpretability and did not guarantee the selection of the most distracting answer. THaMES improves this by using fine-tuned NLI ( deberta-v3-base-tasksource-nli )  (Sileo,  2023 )  and hallucination evaluation models ( HHEM-2.1-Open )  (Tang et al.,  2023 ; Niu et al.,  2024 ; Luo et al.,  2023 )  to assess the Entailment and Factual Consistency Scores of generated answers. These scores are combined to form an Ensemble Score:  Ensemble Score = Entailment Score + Factual Consistency Score Ensemble Score Entailment Score Factual Consistency Score \\texttt{Ensemble Score}=\\texttt{Entailment Score}+\\texttt{Factual Consistency Score} Ensemble Score = Entailment Score + Factual Consistency Score . Lower scores indicate stronger hallucinations, helping to select the most hallucinated answers. Figure  1  provides more details on the generation process with a system flowchart. Finally, we used the selected corpus to generate a total of 2,100 sets of data (300 sets for each type of question). Each set of data includes a question, a correct answer, and the best hallucinated answer.",
            "We conducted comprehensive experiments under two hallucination evaluation tasks, comparing the effects of three different mitigation strategies on model performance. The results, as shown in table  1 , indicate that while overall performance improved after applying mitigation strategies, no optimal strategy works across all models. For example, we found that for GPT-4o, the prompt-based In-Context Learning (ICL) strategy had a limited effect on improving the model, suggesting that its reasoning capabilities are already high and have reached a performance bottleneck. In contrast, using RAG, which introduces external knowledge, significantly enhanced the models ability to prevent hallucinations. Open-weight models like Llama-3.1 showed different behavior with these strategies. While RAG helped reduce hallucination generation in  Llama-3.1-8B-Instruct , ICL notably improved its accuracy in detecting hallucinations. With these evaluations in mind, the system we designed allows for the application of different mitigation strategies tailored to the specific model and task, enabling the models to achieve optimal performance in detecting or producing less hallucination. Due to computational restraints, we were only able to conduct fine-tuning mitigation (PEFT) evaluations on the  Llama-3.1-8B-Instruct  model. Despite these limitations, results show significant improvement over the original model in text generation, specifically in regard to Answer Relevancy, Answer Correctness, and Answer Similarity. Additionally, in Hallucination Identification, we saw improvement over the baseline model in Recall and overall F-1 score. This highlights the potential of fine-tuning in reducing hallucinated output."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Quantization and LoRA Configuration",
        "table": "A1.T2.1.1",
        "footnotes": [],
        "references": [
            "We chose  GPT-4o-mini   (OpenAI,  2024 )  for question generation to minimize human validation, due to its high performance and relatively low cost. The HaluEval testset had simple questions with short answers, so we created six question types:  [simple, reasoning, multi-context, situational, distracting, and double]  based on RAGAS  (Es et al.,  2023 )  and Giskard  (Giskard AI,  2024 ) . Unlike these frameworks, our type definitions were designed not for retrieval-augmented generation (RAG) systems but for general LLM hallucination evaluation. We also introduced rules to ensure high generated question quality, such as avoiding ambiguous references, numerical estimates, ensuring self-containment, etc. (detailed in Appendix  A.3.1 ). To generate  simple -type questions, we used few-shot prompting, instructing  GPT-4o-mini  to output batches of JSON objects (Appendix  A.2.1 ). Batch generation was chosen for its cost-effectiveness and ability to minimize repeated questions, ensuring testset diversity. The questions were then evolved into one of the six predefined types through a filtering process (Appendix  A.2.3 ) to ensure quality and compliance with our criteria. Next, the model generated correct answers for each question, using the original context from the knowledge base to ensure factual accuracy.",
            "Table  2  shows the LoRA training configuration for fine-tuning in hallucination mitigation.",
            "Both figure  2  and  3  shows the text node retrieval distribution comparison of different sampling methods. The results show that the weighted sampling method we designed has a significant improvement over random sampling, and node retrieval distribution is closer to a uniform distribution, indicating that our dataset covers the original corpus more evenly."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparison of models on Hallucination Generation and Detection for \"Simple\" Questions",
        "table": "A1.T3.4",
        "footnotes": [],
        "references": [
            "We chose  GPT-4o-mini   (OpenAI,  2024 )  for question generation to minimize human validation, due to its high performance and relatively low cost. The HaluEval testset had simple questions with short answers, so we created six question types:  [simple, reasoning, multi-context, situational, distracting, and double]  based on RAGAS  (Es et al.,  2023 )  and Giskard  (Giskard AI,  2024 ) . Unlike these frameworks, our type definitions were designed not for retrieval-augmented generation (RAG) systems but for general LLM hallucination evaluation. We also introduced rules to ensure high generated question quality, such as avoiding ambiguous references, numerical estimates, ensuring self-containment, etc. (detailed in Appendix  A.3.1 ). To generate  simple -type questions, we used few-shot prompting, instructing  GPT-4o-mini  to output batches of JSON objects (Appendix  A.2.1 ). Batch generation was chosen for its cost-effectiveness and ability to minimize repeated questions, ensuring testset diversity. The questions were then evolved into one of the six predefined types through a filtering process (Appendix  A.2.3 ) to ensure quality and compliance with our criteria. Next, the model generated correct answers for each question, using the original context from the knowledge base to ensure factual accuracy.",
            "Both figure  2  and  3  shows the text node retrieval distribution comparison of different sampling methods. The results show that the weighted sampling method we designed has a significant improvement over random sampling, and node retrieval distribution is closer to a uniform distribution, indicating that our dataset covers the original corpus more evenly."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Comparison of models on Hallucination Generation and Detection for \"Reasoning\" Questions",
        "table": "A1.T4.4",
        "footnotes": [],
        "references": []
    },
    "id_table_5": {
        "caption": "Table 5:  Comparison of models on Hallucination Generation and Detection for \"Multi Context\" Questions",
        "table": "A1.T5.4",
        "footnotes": [],
        "references": []
    },
    "id_table_6": {
        "caption": "Table 6:  Comparison of models on Hallucination Generation and Detection for \"Distracting\" Questions",
        "table": "A1.T6.4",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "Table 7:  Comparison of models on Hallucination Generation and Detection for \"Situational\" Questions",
        "table": "A1.T7.4",
        "footnotes": [],
        "references": [
            "In this section, we conduct a comprehensive analysis of a benchmark designed to evaluate the ability of LLMs to handle hallucinations in QA tasks. We selected a range of mainstream LLMs for testing, including  GPT-4o (05-13-2024)  and  GPT-4o-mini (07-18-2024)  from OpenAIs GPT series  (OpenAI,  2024 )  [deployed through Azure Cloud Platform], the open-source Llama series  (MetaAI,  2024 ) , and Mistral-Nemo  (MistralAI,  2024 ) . Due to computational resource constraints, we were unable to test the full-parameter versions of these models, so we opted to use smaller, quantized versions of the open-source models with Ollama  (Ollama,  2024 ) . Additionally, for the fine-tuned model, we loaded  Llama-3.1-8B-Instruct  from HuggingFace. For details on the LoRA fine-tuning configuration, see Appendix  A.7 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Comparison of models on Hallucination Generation and Detection for \"Double\" Questions",
        "table": "A1.T8.4",
        "footnotes": [],
        "references": [
            "We employed two sets of metrics for evaluation. The first set includes metrics based on those defined by RAGAS  (Es et al.,  2023 ) : answer faithfulness, relevancy, semantic similarity, and correctness. Here, we present the formula used to calculate answer relevancy:   answer relevancy = 1 N   i = 1 N E g i  E o  E g i    E o  answer relevancy 1 N superscript subscript i 1 N  subscript E subscript g i subscript E o norm subscript E subscript g i norm subscript E o \\texttt{answer relevancy}=\\frac{1}{N}\\sum_{i=1}^{N}\\frac{E_{g_{i}}\\cdot E_{o}}%  {\\|E_{g_{i}}\\|\\|E_{o}\\|} answer relevancy = divide start_ARG 1 end_ARG start_ARG italic_N end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT divide start_ARG italic_E start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT  italic_E start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT end_ARG start_ARG  italic_E start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT   italic_E start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT  end_ARG   where  E g i subscript E subscript g i E_{g_{i}} italic_E start_POSTSUBSCRIPT italic_g start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUBSCRIPT  is the embedding of the generated question  i i i italic_i ,  E o subscript E o E_{o} italic_E start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT  is the embedding of the original question,  N N N italic_N  is the number of generated questions. Formulas for other metrics are provided in the Appendix  A.8 . We calculated these metrics by comparing the model-generated answers with the ground truth answers in our testset. We derived our second set of metrics from HaluEval  (Li et al.,  2023 ) , where the LLM is randomly presented with either the correct or hallucinated answer, each with a  50 % percent 50 50\\% 50 %  probability, to assess the models ability to identify hallucinations. The models performance is evaluated using accuracy, calculated as follows:  Accuracy = # Correct Predictions # Total Predictions Accuracy # Correct Predictions # Total Predictions \\texttt{Accuracy}=\\frac{\\texttt{\\# Correct Predictions}}{\\texttt{\\# Total %  Predictions}} Accuracy = divide start_ARG # Correct Predictions end_ARG start_ARG # Total Predictions end_ARG . This helps determine how well the model distinguishes between correct and hallucinated answers given a question."
        ]
    },
    "global_footnotes": [
        "The library and codebase will be made publicly available upon acceptance."
    ]
}