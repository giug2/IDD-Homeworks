{
    "PAPER'S NUMBER OF TABLES": 16,
    "Ch2.T1": {
        "caption": "Table 2.1: Criteria for classification of data fusion techniques [11] [81].",
        "table": "<table id=\"Ch2.T1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch2.T1.2.1\" class=\"ltx_tr\">\n<td id=\"Ch2.T1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch2.T1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Criterion</span></td>\n<td id=\"Ch2.T1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch2.T1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Details</span></td>\n</tr>\n<tr id=\"Ch2.T1.2.2\" class=\"ltx_tr\">\n<td id=\"Ch2.T1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T1.2.2.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@</td>\n<td id=\"Ch2.T1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">(1) complementary, (2) redundant, (3) cooperative data</td>\n</tr>\n<tr id=\"Ch2.T1.2.3\" class=\"ltx_tr\">\n<td id=\"Ch2.T1.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T1.2.3.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>ii@</td>\n<td id=\"Ch2.T1.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">(1) raw measurement/signal, (2) pixel, (3) characteristic or decision</td>\n</tr>\n<tr id=\"Ch2.T1.2.4\" class=\"ltx_tr\">\n<td id=\"Ch2.T1.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T1.2.4.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iii@</td>\n<td id=\"Ch2.T1.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">(1) early, (2) late</td>\n</tr>\n<tr id=\"Ch2.T1.2.5\" class=\"ltx_tr\">\n<td id=\"Ch2.T1.2.5.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T1.2.5.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iv@</td>\n<td id=\"Ch2.T1.2.5.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">(1) centralized, (2) decentralized, (3) distributed, (4) hierarchical</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Parcalabescu ",
                "et al",
                ".Â ",
                "[",
                "60",
                "]",
                " presented a survey about multi-modality in our environment. This survey highlights that a machine processes the input and acts as a (multimodal) agent that decides how to perceive the input. In principle, a machine uses multiple sensors in a combination way to perceive the environment. This method is formalized as Data Fusion.\n",
                "â€Data fusion techniques combine data from multiple sensors and related information from associated databases to achieve improved accuracy and more specific inferences than could be achieved by the use of a single sensor alone.â€",
                "Different modalities represent variants of data. The diversity, which the particular natural processes and phenomena can describe themselves under totally various physical characters, is the motivation for multimodal data fusion ",
                "[",
                "11",
                "]",
                ". However, very little is understood about the potential association among the modalities. The main task of any multimodal analysis is to identify the connections among the modalities, their mutual properties, their complementarity, and shared modality-specific information ",
                "[",
                "11",
                "]",
                ".",
                "Data fusion relates to many fields. Although it is difficult to set up an explicit, generic and rigorous classification of the techniques, the authors of ",
                "[",
                "11",
                "]",
                " ",
                "[",
                "81",
                "]",
                " suggested that we use for classification the following four criteria.",
                "Criterion ",
                "\\@slowromancap",
                "i@.",
                " The first criterion was proposed by Durrant-Whyte ",
                "[",
                "18",
                "]",
                ", where the links within source datasets should be considered. The links can be defined as (1) ",
                "complementary",
                ", (2) ",
                "redundant",
                ", (3) ",
                "cooperative data",
                ". The links within the source datasets are complementary, if the input source data represent various scenes and can be used to acquire global information, e.g. the information obtained by two cameras observing the same object from different fields of view is considered complementary. The relations in the source datasets are redundant, if large equal than two input source data supply details about the identical object and can be merged to increase the confidence, e.g. data from overlapping regions are conclude as redundant. The relationship between the source datasets is cooperative, if the supplied features are merged as a new feature which is often more complicated than the original features ",
                "[",
                "11",
                "]",
                ".",
                "Criterion ",
                "\\@slowromancap",
                "ii@.",
                " The second criterion is to consider the abstraction level of source data:(1) ",
                "raw measurement/signal",
                ", (2) ",
                "pixel",
                ", (3) ",
                "characteristic or decision",
                ". When the signals obtained from the sensor can be processed directly, the abstraction level is raw measurement/signal. When the fusion happen at image and can be utilized to increase image clarity performance, the abstraction level is pixel level. When the fusion uses features extracted from images or signals, the abstract fusion level is ",
                "characteristic",
                ". At ",
                "characteristic",
                " level, fused information is represented as symbols, which is also called decision level ",
                "[",
                "11",
                "]",
                ".",
                "Criterion ",
                "\\@slowromancap",
                "iii@.",
                " The third criterion is consider when to perform fusion during the associated procedures: (1) ",
                "early fusion",
                ", (2) ",
                "late fusion",
                " ",
                "[",
                "81",
                "]",
                ". Early fusion performs fusion at early training stage, while late fusion performs fusion at almost the end of training stage.",
                "Criterion ",
                "\\@slowromancap",
                "iv@.",
                " The fourth criterion is considered as different architecture variants:(1) ",
                "centralized",
                ", (2) ",
                "decentralized",
                ", (3) ",
                "distributed",
                ", (4) ",
                "hierarchical",
                ". In the ",
                "centralized",
                " architecture, the fusion nodes locate in the central processor where the information from all of the inputs are received, measured and transmitted. A ",
                "centralized",
                " approach is theoretically optimal, if it is assumed that data alignment and data association are performed correctly, and that the required transfer time is not significant. However, there is drawback that a large of bandwidth is required to send raw data over the architecture. This drawback can have a greater impact on the results than other architectures ",
                "[",
                "11",
                "]",
                ".\nA ",
                "decentralized",
                " architecture consists of a network of nodes, where each node has its own processing power, there exists no individual point of data fusion. Thus, the information that each node receives from its peers is fused autonomously with its local information. However, this fusion schema has a disadvantage, which is ",
                "O",
                "â€‹",
                "(",
                "n",
                "2",
                ")",
                "ğ‘‚",
                "superscript",
                "ğ‘›",
                "2",
                "O(n^{2})",
                " for cost at each communication step (",
                "n",
                "ğ‘›",
                "n",
                " is the number of nodes). Furthermore, this schema may suffer from scale expansion issues when the number of nodes increases ",
                "[",
                "11",
                "]",
                ".\nIn a ",
                "distributed",
                " architecture, each source node performs its data association and state estimation individually before the raw information conveys to the fusion node. This means that each source node contributes an estimation of the object state from its local perspective, and then the fusion node fuses estimations based on the global perspective. Therefore, this schema provides various range of options, from just one fusion node to many intermediate fusion nodes. In a ",
                "hierarchical",
                " architecture, ",
                "decentralized",
                " and ",
                "distributed",
                " architectures are combined to generate a hierarchical schema, where the data fusion can be achieved at different levels ",
                "[",
                "11",
                "]",
                ".",
                "Classification of Data Fusion of Our Framework.",
                " Our framework is ",
                "complementary",
                ", because our framework applies for the multimodal data with multi modalities (views). Our framework is ",
                "characteristic",
                " level, because the features are fused after they are extracted from images and audio. This framework is also a ",
                "late fusion",
                " approach, because the fusion operation occurs at the end of training process. Besides, this approach can be seen as a centralized model and is used in Federated Learning, because it can be used as."
            ]
        ]
    },
    "Ch2.T2": {
        "caption": "Table 2.2: Classes of Transfer Learning [61].",
        "table": "<table id=\"Ch2.T2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch2.T2.2.1\" class=\"ltx_tr\">\n<td id=\"Ch2.T2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch2.T2.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Classes of Transfer Learning</span></td>\n<td id=\"Ch2.T2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch2.T2.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Details</span></td>\n</tr>\n<tr id=\"Ch2.T2.2.2\" class=\"ltx_tr\">\n<td id=\"Ch2.T2.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Inductive Transfer Learning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib61\" title=\"\" class=\"ltx_ref\">61</a>]</cite>\n</td>\n<td id=\"Ch2.T2.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T2.2.2.2.1\" class=\"ltx_text\"></span><span id=\"Ch2.T2.2.2.2.2\" class=\"ltx_text\">\n<span id=\"Ch2.T2.2.2.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch2.T2.2.2.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch2.T2.2.2.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Which part of the knowledge can be</span></span>\n<span id=\"Ch2.T2.2.2.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch2.T2.2.2.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">transferred across domains or tasks</span></span>\n<span id=\"Ch2.T2.2.2.2.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch2.T2.2.2.2.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(What to transfer)?</span></span>\n</span></span><span id=\"Ch2.T2.2.2.2.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch2.T2.2.3\" class=\"ltx_tr\">\n<td id=\"Ch2.T2.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Transductive Transfer Learning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib61\" title=\"\" class=\"ltx_ref\">61</a>]</cite>\n</td>\n<td id=\"Ch2.T2.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T2.2.3.2.1\" class=\"ltx_text\"></span><span id=\"Ch2.T2.2.3.2.2\" class=\"ltx_text\">\n<span id=\"Ch2.T2.2.3.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch2.T2.2.3.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch2.T2.2.3.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Which machine learning should be chosen</span></span>\n<span id=\"Ch2.T2.2.3.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch2.T2.2.3.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">to Transfer Learning, after pointing out</span></span>\n<span id=\"Ch2.T2.2.3.2.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch2.T2.2.3.2.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">which knowledge can be transferred</span></span>\n<span id=\"Ch2.T2.2.3.2.2.1.4\" class=\"ltx_tr\">\n<span id=\"Ch2.T2.2.3.2.2.1.4.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(how to transfer)?</span></span>\n</span></span><span id=\"Ch2.T2.2.3.2.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch2.T2.2.4\" class=\"ltx_tr\">\n<td id=\"Ch2.T2.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Unsupervised Transfer Learning <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib61\" title=\"\" class=\"ltx_ref\">61</a>]</cite>\n</td>\n<td id=\"Ch2.T2.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T2.2.4.2.1\" class=\"ltx_text\"></span><span id=\"Ch2.T2.2.4.2.2\" class=\"ltx_text\">\n<span id=\"Ch2.T2.2.4.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch2.T2.2.4.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch2.T2.2.4.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Which scenario can transferred be applied</span></span>\n<span id=\"Ch2.T2.2.4.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch2.T2.2.4.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(when to transfer)?</span></span>\n</span></span><span id=\"Ch2.T2.2.4.2.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Transfer Learning is a machine learning technique which aims at enhancing the performance of target models developed on target domains by reusing the knowledge contained in diverse but related models developed on source domains ",
                "[",
                "10",
                "]",
                ".",
                "The definition of Transfer Learning is given by Pan ",
                "et al",
                ".Â in the survey ",
                "[",
                "61",
                "]",
                ". In this survey, they use binary document classification as a description example. The definitions of a ",
                "domain",
                " and a ",
                "task",
                " should be explained.\nA domain ",
                "D",
                "ğ·",
                "D",
                " has two parts, i.e. a feature space ",
                "X",
                "ğ‘‹",
                "X",
                " and a marginal probability distribution ",
                "P",
                "â€‹",
                "(",
                "X",
                ")",
                "ğ‘ƒ",
                "ğ‘‹",
                "P(X)",
                " with ",
                "X",
                "=",
                "{",
                "x",
                "1",
                ",",
                "x",
                "2",
                ",",
                "â€¦",
                ",",
                "x",
                "n",
                "}",
                "âˆˆ",
                "X",
                "ğ‘‹",
                "subscript",
                "ğ‘¥",
                "1",
                "subscript",
                "ğ‘¥",
                "2",
                "â€¦",
                "subscript",
                "ğ‘¥",
                "ğ‘›",
                "ğ‘‹",
                "X=\\{x_{1},x_{2},...,x_{n}\\}\\in X",
                ". In the example of binary document classification, ",
                "X",
                "ğ‘‹",
                "X",
                " consists of vectors and is the space of all representations, ",
                "x",
                "i",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "x_{i}",
                " is the ",
                "i",
                "ğ‘–",
                "i",
                "th term vector according to some document, and ",
                "X",
                "ğ‘‹",
                "X",
                " is the sample for training.\nA task consists two parts corresponding to a give domain, ",
                "D",
                "=",
                "{",
                "X",
                ",",
                "P",
                "â€‹",
                "(",
                "X",
                ")",
                "}",
                "ğ·",
                "ğ‘‹",
                "ğ‘ƒ",
                "ğ‘‹",
                "D=\\{X,P(X)\\}",
                ", i.e. a label space ",
                "Y",
                "ğ‘Œ",
                "Y",
                " and a conditional probability distribution ",
                "P",
                "â€‹",
                "(",
                "Y",
                "|",
                "X",
                ")",
                "ğ‘ƒ",
                "conditional",
                "ğ‘Œ",
                "ğ‘‹",
                "P(Y|X)",
                " which is obtained from training data, which is in the form of pairs ",
                "{",
                "x",
                "i",
                ",",
                "y",
                "i",
                "}",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "subscript",
                "ğ‘¦",
                "ğ‘–",
                "\\{x_{i},y_{i}\\}",
                ". For the set of labels ",
                "Y",
                "ğ‘Œ",
                "Y",
                ", a element in the binary classification is either True or False ",
                "[",
                "61",
                "]",
                ".\nConsidering a source domain ",
                "D",
                "S",
                "subscript",
                "ğ·",
                "ğ‘†",
                "D_{S}",
                " and a source task ",
                "T",
                "S",
                "subscript",
                "ğ‘‡",
                "ğ‘†",
                "T_{S}",
                ", and a pair of corresponding target domain ",
                "D",
                "T",
                "subscript",
                "ğ·",
                "ğ‘‡",
                "D_{T}",
                " and target task ",
                "T",
                "T",
                "subscript",
                "ğ‘‡",
                "ğ‘‡",
                "T_{T}",
                ", the goal of Transfer Learning is to learn a conditional probability distribution ",
                "P",
                "â€‹",
                "(",
                "Y",
                "T",
                "|",
                "X",
                "T",
                ")",
                "ğ‘ƒ",
                "conditional",
                "subscript",
                "ğ‘Œ",
                "ğ‘‡",
                "subscript",
                "ğ‘‹",
                "ğ‘‡",
                "P(Y_{T}|X_{T})",
                " in ",
                "D",
                "T",
                "subscript",
                "ğ·",
                "ğ‘‡",
                "D_{T}",
                " with the samples obtained from ",
                "D",
                "S",
                "subscript",
                "ğ·",
                "ğ‘†",
                "D_{S}",
                " and ",
                "T",
                "S",
                "subscript",
                "ğ‘‡",
                "ğ‘†",
                "T_{S}",
                " where ",
                "D",
                "S",
                "â‰ ",
                "D",
                "T",
                "subscript",
                "ğ·",
                "ğ‘†",
                "subscript",
                "ğ·",
                "ğ‘‡",
                "D_{S}\\neq D_{T}",
                " and ",
                "T",
                "S",
                "â‰ ",
                "T",
                "T",
                "subscript",
                "ğ‘‡",
                "ğ‘†",
                "subscript",
                "ğ‘‡",
                "ğ‘‡",
                "T_{S}\\neq T_{T}",
                ". It is generally assumed that the number of available labeled target examples is limited, which is exponentially smaller than the number of labeled source examples. In addition, there are explicit or implicit between the feature spaces of target and source domains, when it exists some relationship ",
                "[",
                "61",
                "]",
                ".",
                "Moreover, Pan ",
                "et al",
                ".Â give the specific variants of Transfer Learning ",
                "[",
                "61",
                "]",
                ". According to the questions: which part of the knowledge can be transferred across domains or tasks (What to transfer); which machine learning should be chosen to Transfer Learning, after pointing out which knowledge can be transferred(how to transfer); in which scenario can transferred be applied (when to transfer), it is divided into three categories:inductive Transfer Learning, transductive Transfer Learning, and unsupervised Transfer Learning, respectively ",
                "[",
                "61",
                "]",
                ".",
                "The specific explanations of three categories are following:",
                "Inductive Transfer Learning.",
                " In the Inductive Transfer Learning, no matter if the target and source domains are the same or not, the target task differs from the source task. This inductive Transfer Learning tries to utilize the inductive bias of the source domain to help improve the performance of target tasks. Corresponding to whether the data from the source domain is labeled, this type of Transfer Learning can be further classified into two subdivisions, i.e. multitask learning and self-supervised (self-taught) learning ",
                "[",
                "61",
                "]",
                ".",
                "Transductive Transfer Learning.",
                " In the Transductive Transfer Learning, there are similarities between the source and target tasks, but the differences exist in corresponding domains. Besides, there is no labeled data in the target domain, while there are a large amount of labeled data available in the source domain. This can be further divided into two categories, corresponding to scenario with different feature space or different marginal probabilities ",
                "[",
                "61",
                "]",
                ".",
                "Unsupervised Transfer Learning.",
                " In the Unsupervised Transfer Learning, the source and target domains are similar, but the source and target tasks are different from each other. This unsupervised Transfer Learning is similar to inductive Transfer Learning, with the focal point on solving Unsupervised Learning tasks in the target domain. In addition, no labeled data exists in both source and target domains ",
                "[",
                "61",
                "]",
                ".",
                "There are four different ways to use Transfer Learning, i.e. instance transfer ",
                "[",
                "19",
                "]",
                ", feature representation transfer ",
                "[",
                "64",
                "]",
                ", parameter transfer ",
                "[",
                "44",
                "]",
                " and relational knowledge transfer ",
                "[",
                "51",
                "]",
                " ",
                "[",
                "61",
                "]",
                ".",
                "Approach ",
                "\\@slowromancap",
                "i@.",
                " For instance transfer, the main purpose is to reuse knowledge from the source domain to the target domain. Although the source domain cannot be reused directly, particular parts of the data can be extended along with target data in the target domain for training by re-weighting ",
                "[",
                "61",
                "]",
                ".",
                "Approach ",
                "\\@slowromancap",
                "ii@.",
                " For feature-representation transfer, the main idea is to scale up the convergence and performance through recognizing useful feature representations, which are able to be used from source to target domains. According to whether data is labeled, feature-representation transfer can be used based on supervised or Unsupervised Learning ",
                "[",
                "61",
                "]",
                ".",
                "Approach ",
                "\\@slowromancap",
                "iii@.",
                " The main intuition is to share prior probability distribution of parameters or even some parameters, which is based on the assumption of related tasks ",
                "[",
                "61",
                "]",
                ".",
                "Approach ",
                "\\@slowromancap",
                "iv@.",
                " For relational-knowledge-transfer, the main goal is to deal with data that is not independent and identically distributed. That is to say, there is a relationship between data nodes. For example, the web information of social network can use relational-knowledge-transfer methods ",
                "[",
                "61",
                "]",
                ".",
                "Transfer Learning has a big success in the machine learning area. In image classification problems, the paper from Wu ",
                "et al",
                ".Â has shown that additional data extracted from a different distribution can help the main classification learners greatly improve the performance ",
                "[",
                "74",
                "]",
                ". In natural language processing, Raina ",
                "et al",
                ".Â present that an approach for learning a mapping covariance matrix from additional labeled text can help original classifier improve the classification accuracy ",
                "[",
                "65",
                "]",
                ". Zhuo ",
                "et al",
                ".Â present that problems not using the existing domain for transfer are worse than the action model learned by Transfer Learning ",
                "[",
                "82",
                "]",
                ".",
                "Especially, Transfer Learning is beneficial to image field. Digital medical image is a helpful technique for computer-aided diagnosis. Medical images are difficult to collect and their quantity is limited because medical data are generated by special techniques (e.g. X-ray radiography). Therefore, Transfer Learning can be used as an auxiliary diagnostic tool. There are many successful applications. Maqsood ",
                "et al",
                ".Â use Transfer Learning technique to detect Alzheimerâ€™s disease by fine-tuning AlexNet ",
                "[",
                "39",
                "]",
                ". The proposed approach gets the highest accuracy rate in the experiments of Alzheimerâ€™s stage detection. For this Alzheimerâ€™s stage, the medical images (MRI) is preprocessed by using a contrast stretching operation in the target domain. Next, the AlexNet architecture is pre-trained (source domain) as the start of learning new tasks. Then, the last three layers (one softmax layer, one fully connected layer, and one output layer) of AlexNet is replaced, but the previous convolutional layers are reserved. At last, the fine-tuned training on Alzheimerâ€™s dataset ",
                "[",
                "54",
                "]",
                " in the target domain is carried out using the modified AlexNet.",
                "According to the same physical natures between Electromyographic (EMG) signals from the muscles and Electroencephalographic (EEG) brainwaves, Bird ",
                "et al",
                ".Â ",
                "[",
                "8",
                "]",
                " utilize Transfer Learning from the gesture recognition domain to the mental state recognition domain. It also shows that EEG brainwaves can be transferred to classify EMG signals. The experimental results show that Transfer Learning is helpful to improve the performance of neural network classifiers ",
                "[",
                "8",
                "]",
                ".",
                "Negative Transfer.",
                " However, negative transfer may happen. By negative transfer we mean that the learned knowledge contributes to the decreased performance of learning in the new knowledge. That is, the performance of learning from target domain could decrease due to the source domain data and task ",
                "[",
                "61",
                "]",
                ". The experimental results have shown that if two tasks are extremely different, naively applying transfer technique may cause the accuracy loss of target task ",
                "[",
                "67",
                "]",
                ".\nThe paper ",
                "[",
                "5",
                "]",
                " can provide us direction to avoid negative transfer, and the most important tool is task clustering.\nThe similar tasks should be gathered using task clustering techniques, when data is clustered regarding to task models. Moreover, the learning tasks can be split into different groups. Each group of tasks is relevant to a low-dimensional feature, and different groups hold different low-dimensional features. Finally, efficient knowledge transfer is done within each group ",
                "[",
                "61",
                "]",
                ".",
                "Transfer Learning in Our framework.",
                " The Transfer Learning in our framework belongs to inductive Transfer Learning, because Contrastive Learning is one of the most representative approach of Self-Supervised Learning. Besides, Contrastive Learning also tries to minimize the contrastive loss of the source domain (multi modalities) to help improve the performance of target domain (one single modality). The transfer approach in our framework applies for feature-representation transferring, because the Contrastive Learning tasks aim to learn the feature representation from unlabeled data."
            ]
        ]
    },
    "Ch2.T3": {
        "caption": "Table 2.3: Criterion for classification of data fusion techniques [11][81].",
        "table": "<table id=\"Ch2.T3.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch2.T3.2.1\" class=\"ltx_tr\">\n<td id=\"Ch2.T3.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch2.T3.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Approach Index</span></td>\n<td id=\"Ch2.T3.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch2.T3.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Approaches to Use Transfer Learning</span></td>\n</tr>\n<tr id=\"Ch2.T3.2.2\" class=\"ltx_tr\">\n<td id=\"Ch2.T3.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T3.2.2.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@</td>\n<td id=\"Ch2.T3.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Instance transfer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">19</a>]</cite>\n</td>\n</tr>\n<tr id=\"Ch2.T3.2.3\" class=\"ltx_tr\">\n<td id=\"Ch2.T3.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T3.2.3.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>ii@</td>\n<td id=\"Ch2.T3.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Feature-representation transfer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib64\" title=\"\" class=\"ltx_ref\">64</a>]</cite>\n</td>\n</tr>\n<tr id=\"Ch2.T3.2.4\" class=\"ltx_tr\">\n<td id=\"Ch2.T3.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T3.2.4.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iii@</td>\n<td id=\"Ch2.T3.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Parameter transfer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">44</a>]</cite>\n</td>\n</tr>\n<tr id=\"Ch2.T3.2.5\" class=\"ltx_tr\">\n<td id=\"Ch2.T3.2.5.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch2.T3.2.5.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iv@</td>\n<td id=\"Ch2.T3.2.5.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">Relational knowledge transfer <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Transfer Learning is a machine learning technique which aims at enhancing the performance of target models developed on target domains by reusing the knowledge contained in diverse but related models developed on source domains ",
                "[",
                "10",
                "]",
                ".",
                "The definition of Transfer Learning is given by Pan ",
                "et al",
                ".Â in the survey ",
                "[",
                "61",
                "]",
                ". In this survey, they use binary document classification as a description example. The definitions of a ",
                "domain",
                " and a ",
                "task",
                " should be explained.\nA domain ",
                "D",
                "ğ·",
                "D",
                " has two parts, i.e. a feature space ",
                "X",
                "ğ‘‹",
                "X",
                " and a marginal probability distribution ",
                "P",
                "â€‹",
                "(",
                "X",
                ")",
                "ğ‘ƒ",
                "ğ‘‹",
                "P(X)",
                " with ",
                "X",
                "=",
                "{",
                "x",
                "1",
                ",",
                "x",
                "2",
                ",",
                "â€¦",
                ",",
                "x",
                "n",
                "}",
                "âˆˆ",
                "X",
                "ğ‘‹",
                "subscript",
                "ğ‘¥",
                "1",
                "subscript",
                "ğ‘¥",
                "2",
                "â€¦",
                "subscript",
                "ğ‘¥",
                "ğ‘›",
                "ğ‘‹",
                "X=\\{x_{1},x_{2},...,x_{n}\\}\\in X",
                ". In the example of binary document classification, ",
                "X",
                "ğ‘‹",
                "X",
                " consists of vectors and is the space of all representations, ",
                "x",
                "i",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "x_{i}",
                " is the ",
                "i",
                "ğ‘–",
                "i",
                "th term vector according to some document, and ",
                "X",
                "ğ‘‹",
                "X",
                " is the sample for training.\nA task consists two parts corresponding to a give domain, ",
                "D",
                "=",
                "{",
                "X",
                ",",
                "P",
                "â€‹",
                "(",
                "X",
                ")",
                "}",
                "ğ·",
                "ğ‘‹",
                "ğ‘ƒ",
                "ğ‘‹",
                "D=\\{X,P(X)\\}",
                ", i.e. a label space ",
                "Y",
                "ğ‘Œ",
                "Y",
                " and a conditional probability distribution ",
                "P",
                "â€‹",
                "(",
                "Y",
                "|",
                "X",
                ")",
                "ğ‘ƒ",
                "conditional",
                "ğ‘Œ",
                "ğ‘‹",
                "P(Y|X)",
                " which is obtained from training data, which is in the form of pairs ",
                "{",
                "x",
                "i",
                ",",
                "y",
                "i",
                "}",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "subscript",
                "ğ‘¦",
                "ğ‘–",
                "\\{x_{i},y_{i}\\}",
                ". For the set of labels ",
                "Y",
                "ğ‘Œ",
                "Y",
                ", a element in the binary classification is either True or False ",
                "[",
                "61",
                "]",
                ".\nConsidering a source domain ",
                "D",
                "S",
                "subscript",
                "ğ·",
                "ğ‘†",
                "D_{S}",
                " and a source task ",
                "T",
                "S",
                "subscript",
                "ğ‘‡",
                "ğ‘†",
                "T_{S}",
                ", and a pair of corresponding target domain ",
                "D",
                "T",
                "subscript",
                "ğ·",
                "ğ‘‡",
                "D_{T}",
                " and target task ",
                "T",
                "T",
                "subscript",
                "ğ‘‡",
                "ğ‘‡",
                "T_{T}",
                ", the goal of Transfer Learning is to learn a conditional probability distribution ",
                "P",
                "â€‹",
                "(",
                "Y",
                "T",
                "|",
                "X",
                "T",
                ")",
                "ğ‘ƒ",
                "conditional",
                "subscript",
                "ğ‘Œ",
                "ğ‘‡",
                "subscript",
                "ğ‘‹",
                "ğ‘‡",
                "P(Y_{T}|X_{T})",
                " in ",
                "D",
                "T",
                "subscript",
                "ğ·",
                "ğ‘‡",
                "D_{T}",
                " with the samples obtained from ",
                "D",
                "S",
                "subscript",
                "ğ·",
                "ğ‘†",
                "D_{S}",
                " and ",
                "T",
                "S",
                "subscript",
                "ğ‘‡",
                "ğ‘†",
                "T_{S}",
                " where ",
                "D",
                "S",
                "â‰ ",
                "D",
                "T",
                "subscript",
                "ğ·",
                "ğ‘†",
                "subscript",
                "ğ·",
                "ğ‘‡",
                "D_{S}\\neq D_{T}",
                " and ",
                "T",
                "S",
                "â‰ ",
                "T",
                "T",
                "subscript",
                "ğ‘‡",
                "ğ‘†",
                "subscript",
                "ğ‘‡",
                "ğ‘‡",
                "T_{S}\\neq T_{T}",
                ". It is generally assumed that the number of available labeled target examples is limited, which is exponentially smaller than the number of labeled source examples. In addition, there are explicit or implicit between the feature spaces of target and source domains, when it exists some relationship ",
                "[",
                "61",
                "]",
                ".",
                "Moreover, Pan ",
                "et al",
                ".Â give the specific variants of Transfer Learning ",
                "[",
                "61",
                "]",
                ". According to the questions: which part of the knowledge can be transferred across domains or tasks (What to transfer); which machine learning should be chosen to Transfer Learning, after pointing out which knowledge can be transferred(how to transfer); in which scenario can transferred be applied (when to transfer), it is divided into three categories:inductive Transfer Learning, transductive Transfer Learning, and unsupervised Transfer Learning, respectively ",
                "[",
                "61",
                "]",
                ".",
                "The specific explanations of three categories are following:",
                "Inductive Transfer Learning.",
                " In the Inductive Transfer Learning, no matter if the target and source domains are the same or not, the target task differs from the source task. This inductive Transfer Learning tries to utilize the inductive bias of the source domain to help improve the performance of target tasks. Corresponding to whether the data from the source domain is labeled, this type of Transfer Learning can be further classified into two subdivisions, i.e. multitask learning and self-supervised (self-taught) learning ",
                "[",
                "61",
                "]",
                ".",
                "Transductive Transfer Learning.",
                " In the Transductive Transfer Learning, there are similarities between the source and target tasks, but the differences exist in corresponding domains. Besides, there is no labeled data in the target domain, while there are a large amount of labeled data available in the source domain. This can be further divided into two categories, corresponding to scenario with different feature space or different marginal probabilities ",
                "[",
                "61",
                "]",
                ".",
                "Unsupervised Transfer Learning.",
                " In the Unsupervised Transfer Learning, the source and target domains are similar, but the source and target tasks are different from each other. This unsupervised Transfer Learning is similar to inductive Transfer Learning, with the focal point on solving Unsupervised Learning tasks in the target domain. In addition, no labeled data exists in both source and target domains ",
                "[",
                "61",
                "]",
                ".",
                "There are four different ways to use Transfer Learning, i.e. instance transfer ",
                "[",
                "19",
                "]",
                ", feature representation transfer ",
                "[",
                "64",
                "]",
                ", parameter transfer ",
                "[",
                "44",
                "]",
                " and relational knowledge transfer ",
                "[",
                "51",
                "]",
                " ",
                "[",
                "61",
                "]",
                ".",
                "Approach ",
                "\\@slowromancap",
                "i@.",
                " For instance transfer, the main purpose is to reuse knowledge from the source domain to the target domain. Although the source domain cannot be reused directly, particular parts of the data can be extended along with target data in the target domain for training by re-weighting ",
                "[",
                "61",
                "]",
                ".",
                "Approach ",
                "\\@slowromancap",
                "ii@.",
                " For feature-representation transfer, the main idea is to scale up the convergence and performance through recognizing useful feature representations, which are able to be used from source to target domains. According to whether data is labeled, feature-representation transfer can be used based on supervised or Unsupervised Learning ",
                "[",
                "61",
                "]",
                ".",
                "Approach ",
                "\\@slowromancap",
                "iii@.",
                " The main intuition is to share prior probability distribution of parameters or even some parameters, which is based on the assumption of related tasks ",
                "[",
                "61",
                "]",
                ".",
                "Approach ",
                "\\@slowromancap",
                "iv@.",
                " For relational-knowledge-transfer, the main goal is to deal with data that is not independent and identically distributed. That is to say, there is a relationship between data nodes. For example, the web information of social network can use relational-knowledge-transfer methods ",
                "[",
                "61",
                "]",
                ".",
                "Transfer Learning has a big success in the machine learning area. In image classification problems, the paper from Wu ",
                "et al",
                ".Â has shown that additional data extracted from a different distribution can help the main classification learners greatly improve the performance ",
                "[",
                "74",
                "]",
                ". In natural language processing, Raina ",
                "et al",
                ".Â present that an approach for learning a mapping covariance matrix from additional labeled text can help original classifier improve the classification accuracy ",
                "[",
                "65",
                "]",
                ". Zhuo ",
                "et al",
                ".Â present that problems not using the existing domain for transfer are worse than the action model learned by Transfer Learning ",
                "[",
                "82",
                "]",
                ".",
                "Especially, Transfer Learning is beneficial to image field. Digital medical image is a helpful technique for computer-aided diagnosis. Medical images are difficult to collect and their quantity is limited because medical data are generated by special techniques (e.g. X-ray radiography). Therefore, Transfer Learning can be used as an auxiliary diagnostic tool. There are many successful applications. Maqsood ",
                "et al",
                ".Â use Transfer Learning technique to detect Alzheimerâ€™s disease by fine-tuning AlexNet ",
                "[",
                "39",
                "]",
                ". The proposed approach gets the highest accuracy rate in the experiments of Alzheimerâ€™s stage detection. For this Alzheimerâ€™s stage, the medical images (MRI) is preprocessed by using a contrast stretching operation in the target domain. Next, the AlexNet architecture is pre-trained (source domain) as the start of learning new tasks. Then, the last three layers (one softmax layer, one fully connected layer, and one output layer) of AlexNet is replaced, but the previous convolutional layers are reserved. At last, the fine-tuned training on Alzheimerâ€™s dataset ",
                "[",
                "54",
                "]",
                " in the target domain is carried out using the modified AlexNet.",
                "According to the same physical natures between Electromyographic (EMG) signals from the muscles and Electroencephalographic (EEG) brainwaves, Bird ",
                "et al",
                ".Â ",
                "[",
                "8",
                "]",
                " utilize Transfer Learning from the gesture recognition domain to the mental state recognition domain. It also shows that EEG brainwaves can be transferred to classify EMG signals. The experimental results show that Transfer Learning is helpful to improve the performance of neural network classifiers ",
                "[",
                "8",
                "]",
                ".",
                "Negative Transfer.",
                " However, negative transfer may happen. By negative transfer we mean that the learned knowledge contributes to the decreased performance of learning in the new knowledge. That is, the performance of learning from target domain could decrease due to the source domain data and task ",
                "[",
                "61",
                "]",
                ". The experimental results have shown that if two tasks are extremely different, naively applying transfer technique may cause the accuracy loss of target task ",
                "[",
                "67",
                "]",
                ".\nThe paper ",
                "[",
                "5",
                "]",
                " can provide us direction to avoid negative transfer, and the most important tool is task clustering.\nThe similar tasks should be gathered using task clustering techniques, when data is clustered regarding to task models. Moreover, the learning tasks can be split into different groups. Each group of tasks is relevant to a low-dimensional feature, and different groups hold different low-dimensional features. Finally, efficient knowledge transfer is done within each group ",
                "[",
                "61",
                "]",
                ".",
                "Transfer Learning in Our framework.",
                " The Transfer Learning in our framework belongs to inductive Transfer Learning, because Contrastive Learning is one of the most representative approach of Self-Supervised Learning. Besides, Contrastive Learning also tries to minimize the contrastive loss of the source domain (multi modalities) to help improve the performance of target domain (one single modality). The transfer approach in our framework applies for feature-representation transferring, because the Contrastive Learning tasks aim to learn the feature representation from unlabeled data."
            ]
        ]
    },
    "Ch3.T1": {
        "caption": "Table 3.1: Security solutions for Federated Transfer Learning.",
        "table": "<table id=\"Ch3.T1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch3.T1.2.1\" class=\"ltx_tr\">\n<td id=\"Ch3.T1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch3.T1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Security Solution Index</span></td>\n<td id=\"Ch3.T1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch3.T1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Methods</span></td>\n</tr>\n<tr id=\"Ch3.T1.2.2\" class=\"ltx_tr\">\n<td id=\"Ch3.T1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T1.2.2.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@</td>\n<td id=\"Ch3.T1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">MPC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib70\" title=\"\" class=\"ltx_ref\">70</a>]</cite>, SPDZ <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib17\" title=\"\" class=\"ltx_ref\">17</a>]</cite>\n</td>\n</tr>\n<tr id=\"Ch3.T1.2.3\" class=\"ltx_tr\">\n<td id=\"Ch3.T1.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T1.2.3.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>ii@</td>\n<td id=\"Ch3.T1.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">DDPG (S-TD3) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>\n</td>\n</tr>\n<tr id=\"Ch3.T1.2.4\" class=\"ltx_tr\">\n<td id=\"Ch3.T1.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T1.2.4.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iii@</td>\n<td id=\"Ch3.T1.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">HFTL <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">27</a>]</cite>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The initial goal of Federated Learning is to carry out useful machine learning methods in multiple devices, and build an aggregated model based on dataset across multiple devices, while ensuring usersâ€™ privacy and security [37]. Thus, we should obey the original goal of federated (transfer) learning. Although they are not chosen by us, we briefly review some other alternative for security and privacy in machine learning (see Table 3.1) for completeness."
        ]
    },
    "Ch3.T2": {
        "caption": "Table 3.2: Applications with pre-trained models in Federated Transfer Learning.",
        "table": "<table id=\"Ch3.T2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch3.T2.2.1\" class=\"ltx_tr\">\n<td id=\"Ch3.T2.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch3.T2.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Index of Applications for Pre-trained Models</span></td>\n<td id=\"Ch3.T2.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"Ch3.T2.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Applications</span></td>\n</tr>\n<tr id=\"Ch3.T2.2.2\" class=\"ltx_tr\">\n<td id=\"Ch3.T2.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T2.2.2.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@</td>\n<td id=\"Ch3.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedHealth2 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">15</a>]</cite>\n</td>\n</tr>\n<tr id=\"Ch3.T2.2.3\" class=\"ltx_tr\">\n<td id=\"Ch3.T2.2.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T2.2.3.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>ii@</td>\n<td id=\"Ch3.T2.2.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedURR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">26</a>]</cite>\n</td>\n</tr>\n<tr id=\"Ch3.T2.2.4\" class=\"ltx_tr\">\n<td id=\"Ch3.T2.2.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T2.2.4.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iii@</td>\n<td id=\"Ch3.T2.2.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">FedDCSCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib79\" title=\"\" class=\"ltx_ref\">79</a>]</cite>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated Transfer Learning is a variant of Federated Learning ",
                "[",
                "77",
                "]",
                ". Federated Transfer Learning utilizes different datasets which are neither identical in sample space nor in feature space.",
                "The initial goal of Federated Learning is to carry out useful machine learning methods in multiple devices, and build an aggregated model based on dataset across multiple devices, while ensuring usersâ€™ privacy and security ",
                "[",
                "37",
                "]",
                ". Thus, we should obey the original goal of federated (transfer) learning. Although they are not chosen by us, we briefly review some other alternative for security and privacy in machine learning (see Table 3.1) for completeness.",
                "Security Solution ",
                "\\@slowromancap",
                "i@.",
                " The paper ",
                "[",
                "41",
                "]",
                " is a start in federated machine learning, and this paper is focused on data security in a multi-party privacy-preserving setting. The similar focus is also in the paper ",
                "[",
                "70",
                "]",
                ", which emphasizes the use of multi-party computation (MPC) can improve efficiency by an order of magnitude in semi-honest security settings. In the paper ",
                "[",
                "70",
                "]",
                ", the authors use the SPDZ ",
                "[",
                "17",
                "]",
                " protocol, which is an implementation of MPC, and experimental results proved that the protocol outperforms homomorphic encryption (HE) in terms of communication and time under the same Federated Transfer Learning framework.",
                "Security Solution ",
                "\\@slowromancap",
                "ii@.",
                " Further, the authors in ",
                "[",
                "52",
                "]",
                " proposed a new method of authentication and key exchange protocol to provide an efficient authentication mechanism for Federated Transfer Learning blockchain (FTL-Block), which uses the Novel Supportive Twin Delayed DDPG (S-TD3) algorithm ",
                "[",
                "52",
                "]",
                ". The participants implement the authentication in each part, which completely relies on the credit of the participants. This authentication mechanism can integrate the usersâ€™ credit with both local credit and cross-region credit. With the help of S-TD3 algorithm, the training of the local authentication model achieves the highest accuracy ",
                "[",
                "52",
                "]",
                ". Transfer Learning method is applied to reduce the extra time cost in the authentication model, and domain the authentication model can be accurately and successfully migrated from local to foreign users ",
                "[",
                "52",
                "]",
                ". Moreover, Majeed ",
                "et al",
                ".Â ",
                "[",
                "50",
                "]",
                " proposed the cross-silo secure aggregation technique based on MPC for secure Federated Transfer Learning.",
                "Security Solution ",
                "\\@slowromancap",
                "iii@.",
                " In addition, the existing approaches mainly focus on homogeneous feature spaces, which will leak privacy when dealing with the problems of covariate shift and feature heterogeneity. Thus, Gao ",
                "et al",
                ".Â ",
                "[",
                "27",
                "]",
                " provide a privacy preserving Federated Transfer Learning framework for homogeneous feature spaces called heterogeneous Federated Transfer Learning (HFTL). Specifically, Gao ",
                "et al",
                ".Â designed privacy-preserving Transfer Learning method to remove covariate shifts in homogeneous feature spaces, and connect heterogeneous feature spaces from different participants ",
                "[",
                "27",
                "]",
                ". Besides, two variants based on HE and secret sharing techniques are applied in the HFTL. Experimental results have demonstrated that the framework performs general feature-based Federated Learning methods and self-learning methods under the same challenge constraints, and HFTL also is shown to have practical efficiency and scalability ",
                "[",
                "27",
                "]",
                ".",
                "Our Framework without Additional Security Solutions.",
                " To conclusion, all the these security solutions ",
                "[",
                "70",
                "]",
                " ",
                "[",
                "17",
                "]",
                " ",
                "[",
                "52",
                "]",
                " ",
                "[",
                "27",
                "]",
                " added to add additional security protection mechanisms. Since our primary goal is to investigate Federated Learning itself, we do not incorporate any of the mechanisms above, although we believe our solution and framework can benefit from them in security, too.",
                "There are some successful applications based on different Transfer Learning methods. For edge devices, tiny devices like mobile phones, medical instruments, smart manufacturing, etc. ",
                "[",
                "15",
                "]",
                " ",
                "[",
                "76",
                "]",
                " ",
                "[",
                "32",
                "]",
                ". There are two strategies are used in Federated Transfer Learning: ",
                "Pre-trained models",
                " can be reused in related tasks, while ",
                "domain adaptation",
                " can be used from a source domain to a related target domain. A ",
                "pre-trained",
                " model can be used directly in some Federated Transfer Learning frameworks. Domain adaptation is also known as the knowledge transferring from the source domain to the target domain ",
                "[",
                "10",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "i@ with Pre-trained Models.",
                " After FedHealth ",
                "[",
                "15",
                "]",
                ", Chen ",
                "et al",
                ".Â ",
                "[",
                "14",
                "]",
                " proposed FedHealth2, a weighted Federated Transfer Learning framework through batch normalization . In FedHealth2, all participants aggregate the features without compromising privacy security, and obtain local models for participants via weighting and protecting local batch normalization ",
                "[",
                "14",
                "]",
                ". More specifically, FedHealth2 achieves the similarities in participants with the support of a pre-trained model. The similarities are confirmed by the metrics of the data distributions, and the metrics can be determined by outputs values of the pre-trained model. Then, with the achieved similarities, the server can do the averaging of the weighted modelsâ€™ parameters in a localized approach and produce a unique model for each participant ",
                "[",
                "14",
                "]",
                ". Experimental results have shown that FedHealth2 enables the local participantsâ€™ models to do the recognition with higher accuracy. Besides, FedHealth2 can achieve similarity in several epochs even if no pre-trained model exists ",
                "[",
                "14",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "ii@ with Pre-trained Models.",
                " Moreover, FedURR ",
                "[",
                "26",
                "]",
                " also benefits from the pre-trained model. In Urban risk recognition (URR) task, the urban management usually has multiple departments, each of which stores a large amount of data locally. When data is uploaded to a central database, it means huge cost and a lot of time consumption, and there exists a risk of data leakage ",
                "[",
                "26",
                "]",
                ". Thus, the proposed framework FedURR integrates two types of Transfer Learning into the Federated Learning framework, i.e. fine-tuning based and parameter sharing based Transfer Learning methods. With the help of fine-tuning and parameter sharing, they are connected into different stages of Federated Learning with an precisely design. The experimental results are shown that FedURR can improve multi-department collaborative URR accuracy ",
                "[",
                "26",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "iii@ with Pre-trained Models.",
                " Furthermore, Zhang ",
                "et al",
                ".Â ",
                "[",
                "79",
                "]",
                " proposed the first Federated Transfer Learning framework, to solve problems in Disaster Classification in Social Computing Networks (FedDCSCN). The authors want to eliminate shortcomings of the local models of the participants, which are deep learning models. The local models need a large number of high-quality samples, and fast computation speed is required to accelerate the training process ",
                "[",
                "79",
                "]",
                ". In addition, the data labeling process is time consuming in the field of social computing, which hinders the use of deep learning networks ",
                "[",
                "79",
                "]",
                ". Thus, Federated Learning and Transfer Learning are combined to address the problems. Pre-trained model based Transfer Learning is used as to reduce communication and computation costs ",
                "[",
                "79",
                "]",
                ". Besides, homomorphic encryption approach is applied as a additional to preserve the local data privacy of social computing participants ",
                "[",
                "79",
                "]",
                ". Experimental results are shown that a feasible but not ideal performance is obtained by the framework in the social computing field ",
                "[",
                "79",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "i@ with Domain Adaptation.",
                " FedSteg ",
                "[",
                "76",
                "]",
                " provides an example for using domain adaptation.\nImage steganography is the method of concealing secret information within images. Conversely, image steganalysis is a counter method to image steganography. This method intends to detect the secret information within images. Through this detection technique, the steganographic features which are generated by image steganographic methods can be extracted. However, there are still problems that exist in image steganalysis. Image steganalysis algorithms train on machine learning models which rely on a large amount of data. However, it is hard to aggregate all the steganographic images to a global cloud server.\nMoreover, the users do not want unrelated people to snoop on confidential information. To solve the problems, Yang ",
                "et al",
                ".Â propose the framework called FedSteg. FedSteg trains a machine learning model with a privacy-protecting technique through domain adaptation. Domain adaptation is used to train the local model by decreasing the domain discrepancy between the global server and local data. Compared with traditional non-federated steganalysis techniques, the experiment results show that FedSteg achieves certain improvements ",
                "[",
                "76",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "ii@ with Domain Adaptation.",
                " Fedhealth ",
                "[",
                "15",
                "]",
                " benefits from domain adaptation. Wearable devices allow people to get access to and record healthcare information. Additionally, smart wearable devices use a large amount of personal data to train machine learning models. Different wearable devices have diverse characteristics and domains. However, the healthcare data from different people with diverse monitoring patterns are difficult to aggregate together to generate robust results. Each personal data is an island. Besides, the machine models using personal data are hard to train on cloud servers. To solve data isolation and locally training problem, Chen ",
                "et al",
                ".Â proposed a Federated Learning framework called FedHealth ",
                "[",
                "15",
                "]",
                ". In this paper, the authors used a neural network (NN), which has two convolutional layers, two pooling layers, and three fully-connected layers ",
                "[",
                "15",
                "]",
                ". NN aims at extracting low-level features. Domain adaptation is applied to transfer the extracted features from server to clients by minimizing the feature distance between server and clients. Compared to the approaches without Federated Learning and traditional methods (KNN, SVM, and RF), FedHealth achieves better performance ",
                "[",
                "15",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "iii@ with Domain Adaptation.",
                " One more example that uses domain adaptation is the electroencephalographic (EEG) signal classification ",
                "[",
                "32",
                "]",
                ". Brain-Computer Interface (BCI) systems are mainly to identify the usersâ€™ consciousness from the brain states. Deep learning methods achieve success in the BCI field for classification of EEG signals. However, the success is restricted to the lack of a large amount of data. Besides, according to the privacy of personal EEG data, it is constrained to build a collection of big BCI dataset. In order to solve the lack of data and the private privacy problems, Ju ",
                "et al",
                ".Â proposed a Federated Transfer Learning method for EEG Signal classification. They propose an method which use Transfer Learning technique with domain adaptation to extract the common discriminative information, and map the common discriminative information into a spatial covariance matrix, then subsequently fed the spatial covariance matrix to a deep learning based Federated Transfer Learning architecture ",
                "[",
                "32",
                "]",
                ". The proposed architecture based on deep learning has 4 layers, namely Manifold reduction layer (M), Common embedded space (C), Tangent projection layer (T) and Federated layer (F), the middle two layers (M and T) provide the functionality of Transfer Learning ",
                "[",
                "32",
                "]",
                ". The experimental result shows that this method using domain adaption in Federated Learning architecture has robust generation ability.",
                "There are two special cases of the problems to be solved in the heterogeneous Federated Transfer Learning setting, and one case for quantifying the performance of Federated Transfer Learning.",
                "Model Distillation.",
                " FedMD ",
                "[",
                "45",
                "]",
                " provides a way to solve statistical heterogeneity (the non-IID problem) in Federated Transfer Learning. Concretely, the authors in FedMD focus on the differences of local models ",
                "[",
                "45",
                "]",
                ". The authors in FedMD identify that communication is the key to fix model heterogeneity. Devices should have the ability to learn the communication protocol to leverage Transfer Learning and model distillation. The communication protocol aims to reuse the models, which are trained from a public dataset. Each client achieves a well-trained model, and applies the well-trained model on local data which is considered as Transfer Learning with model distillation. Thus, the proposed FedMD, which combines Federated Learning and Transfer Learning with knowledge distillation, allows participants to create their models locally, and a communication protocol that utilizes the power of Transfer Learning with model distillation ",
                "[",
                "45",
                "]",
                ". FedMD is demonstrated its efficiency to work on different tasks and datasets ",
                "[",
                "45",
                "]",
                ".",
                "Knowledge Distillation.",
                " Wang ",
                "et al",
                ".Â ",
                "[",
                "75",
                "]",
                " propose Federated Transfer Learning via Knowledge Distillation (FTLKD), which is a robust centralized prediction framework, and is used to solve data islands and data privacy. This framework helps participants to do heterogeneous defect prediction (HDP), predict the defect tendency regarding private models. Concretely, a pre-trained model of public datasets is transferred to the private model, and the model on the private data to converge by fine-tuning, and then the final output in each participantâ€™s private model is conveyed through knowledge distillation ",
                "[",
                "75",
                "]",
                ". Besides, HE is used to encrypt data without disturbing the processing results. Experimental results on 9 projects in 3 public databases (NASA, AEEEM and SOFTLAB) show that FTLKD outperforms the related competing methods ",
                "[",
                "75",
                "]",
                ".",
                "Quantifying Performance.",
                " In addition, the authors in ",
                "[",
                "35",
                "]",
                " analyze ",
                "three",
                " major bottlenecks in Federated Transfer Learning and their potential solutions. The main bottleneck is inter-process communication. Data exchange and memory copy in a device can cause extremely high latency. JVM native memory heap and UNIX domain sockets give us the opportunity to alleviate the type of bottlenecks ",
                "[",
                "35",
                "]",
                ". The second bottleneck is in the additional encryption tool that increases computational cost. The last is the traditional congestion control problem. Intensive data exchange causes heavy network traffic ",
                "[",
                "35",
                "]",
                ".",
                "Transfer Learning Strategies for Our Framework.",
                " The secure methods from papers ",
                "[",
                "70",
                "]",
                " ",
                "[",
                "17",
                "]",
                " ",
                "[",
                "52",
                "]",
                " ",
                "[",
                "27",
                "]",
                ", show that these are additionally add to Federated Transfer Learning meanwhile keep its original structure. The paper ",
                "[",
                "35",
                "]",
                " shows that additional secure methods bring a bottleneck to Federated Transfer Learning. Thus, we need no additional security methods but keep the original structure of Federated Transfer Learning. The methods with Pre-trained models ",
                "[",
                "15",
                "]",
                " ",
                "[",
                "26",
                "]",
                " ",
                "[",
                "79",
                "]",
                " can be considered in our framework. The applications ",
                "[",
                "76",
                "]",
                " ",
                "[",
                "15",
                "]",
                "[",
                "32",
                "]",
                " show that domain adaptation can be used to transfer features from server to participants. Methods ",
                "[",
                "45",
                "]",
                "[",
                "35",
                "]",
                " can be considered to solve problems where the data distributions of participants are different. However, these two methods require an additional public dataset. In conclusion, the methods of transferring from pre-trained models and strategies with domain adaptation can be considered in our framework."
            ]
        ]
    },
    "Ch3.T3": {
        "caption": "Table 3.3: Applications with domain adaptation in Federated Transfer Learning.",
        "table": "<table id=\"Ch3.T3.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch3.T3.2.1\" class=\"ltx_tr\">\n<td id=\"Ch3.T3.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch3.T3.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Index of Applications with Domain Adaptation</span></td>\n<td id=\"Ch3.T3.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch3.T3.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Applications</span></td>\n</tr>\n<tr id=\"Ch3.T3.2.2\" class=\"ltx_tr\">\n<td id=\"Ch3.T3.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T3.2.2.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@</td>\n<td id=\"Ch3.T3.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">FedSteg <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib76\" title=\"\" class=\"ltx_ref\">76</a>]</cite>\n</td>\n</tr>\n<tr id=\"Ch3.T3.2.3\" class=\"ltx_tr\">\n<td id=\"Ch3.T3.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T3.2.3.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>ii@</td>\n<td id=\"Ch3.T3.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Fedhealth <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib15\" title=\"\" class=\"ltx_ref\">15</a>]</cite>\n</td>\n</tr>\n<tr id=\"Ch3.T3.2.4\" class=\"ltx_tr\">\n<td id=\"Ch3.T3.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T3.2.4.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iii@</td>\n<td id=\"Ch3.T3.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">EEG signal classification <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite>\n</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Federated Transfer Learning is a variant of Federated Learning ",
                "[",
                "77",
                "]",
                ". Federated Transfer Learning utilizes different datasets which are neither identical in sample space nor in feature space.",
                "The initial goal of Federated Learning is to carry out useful machine learning methods in multiple devices, and build an aggregated model based on dataset across multiple devices, while ensuring usersâ€™ privacy and security ",
                "[",
                "37",
                "]",
                ". Thus, we should obey the original goal of federated (transfer) learning. Although they are not chosen by us, we briefly review some other alternative for security and privacy in machine learning (see Table 3.1) for completeness.",
                "Security Solution ",
                "\\@slowromancap",
                "i@.",
                " The paper ",
                "[",
                "41",
                "]",
                " is a start in federated machine learning, and this paper is focused on data security in a multi-party privacy-preserving setting. The similar focus is also in the paper ",
                "[",
                "70",
                "]",
                ", which emphasizes the use of multi-party computation (MPC) can improve efficiency by an order of magnitude in semi-honest security settings. In the paper ",
                "[",
                "70",
                "]",
                ", the authors use the SPDZ ",
                "[",
                "17",
                "]",
                " protocol, which is an implementation of MPC, and experimental results proved that the protocol outperforms homomorphic encryption (HE) in terms of communication and time under the same Federated Transfer Learning framework.",
                "Security Solution ",
                "\\@slowromancap",
                "ii@.",
                " Further, the authors in ",
                "[",
                "52",
                "]",
                " proposed a new method of authentication and key exchange protocol to provide an efficient authentication mechanism for Federated Transfer Learning blockchain (FTL-Block), which uses the Novel Supportive Twin Delayed DDPG (S-TD3) algorithm ",
                "[",
                "52",
                "]",
                ". The participants implement the authentication in each part, which completely relies on the credit of the participants. This authentication mechanism can integrate the usersâ€™ credit with both local credit and cross-region credit. With the help of S-TD3 algorithm, the training of the local authentication model achieves the highest accuracy ",
                "[",
                "52",
                "]",
                ". Transfer Learning method is applied to reduce the extra time cost in the authentication model, and domain the authentication model can be accurately and successfully migrated from local to foreign users ",
                "[",
                "52",
                "]",
                ". Moreover, Majeed ",
                "et al",
                ".Â ",
                "[",
                "50",
                "]",
                " proposed the cross-silo secure aggregation technique based on MPC for secure Federated Transfer Learning.",
                "Security Solution ",
                "\\@slowromancap",
                "iii@.",
                " In addition, the existing approaches mainly focus on homogeneous feature spaces, which will leak privacy when dealing with the problems of covariate shift and feature heterogeneity. Thus, Gao ",
                "et al",
                ".Â ",
                "[",
                "27",
                "]",
                " provide a privacy preserving Federated Transfer Learning framework for homogeneous feature spaces called heterogeneous Federated Transfer Learning (HFTL). Specifically, Gao ",
                "et al",
                ".Â designed privacy-preserving Transfer Learning method to remove covariate shifts in homogeneous feature spaces, and connect heterogeneous feature spaces from different participants ",
                "[",
                "27",
                "]",
                ". Besides, two variants based on HE and secret sharing techniques are applied in the HFTL. Experimental results have demonstrated that the framework performs general feature-based Federated Learning methods and self-learning methods under the same challenge constraints, and HFTL also is shown to have practical efficiency and scalability ",
                "[",
                "27",
                "]",
                ".",
                "Our Framework without Additional Security Solutions.",
                " To conclusion, all the these security solutions ",
                "[",
                "70",
                "]",
                " ",
                "[",
                "17",
                "]",
                " ",
                "[",
                "52",
                "]",
                " ",
                "[",
                "27",
                "]",
                " added to add additional security protection mechanisms. Since our primary goal is to investigate Federated Learning itself, we do not incorporate any of the mechanisms above, although we believe our solution and framework can benefit from them in security, too.",
                "There are some successful applications based on different Transfer Learning methods. For edge devices, tiny devices like mobile phones, medical instruments, smart manufacturing, etc. ",
                "[",
                "15",
                "]",
                " ",
                "[",
                "76",
                "]",
                " ",
                "[",
                "32",
                "]",
                ". There are two strategies are used in Federated Transfer Learning: ",
                "Pre-trained models",
                " can be reused in related tasks, while ",
                "domain adaptation",
                " can be used from a source domain to a related target domain. A ",
                "pre-trained",
                " model can be used directly in some Federated Transfer Learning frameworks. Domain adaptation is also known as the knowledge transferring from the source domain to the target domain ",
                "[",
                "10",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "i@ with Pre-trained Models.",
                " After FedHealth ",
                "[",
                "15",
                "]",
                ", Chen ",
                "et al",
                ".Â ",
                "[",
                "14",
                "]",
                " proposed FedHealth2, a weighted Federated Transfer Learning framework through batch normalization . In FedHealth2, all participants aggregate the features without compromising privacy security, and obtain local models for participants via weighting and protecting local batch normalization ",
                "[",
                "14",
                "]",
                ". More specifically, FedHealth2 achieves the similarities in participants with the support of a pre-trained model. The similarities are confirmed by the metrics of the data distributions, and the metrics can be determined by outputs values of the pre-trained model. Then, with the achieved similarities, the server can do the averaging of the weighted modelsâ€™ parameters in a localized approach and produce a unique model for each participant ",
                "[",
                "14",
                "]",
                ". Experimental results have shown that FedHealth2 enables the local participantsâ€™ models to do the recognition with higher accuracy. Besides, FedHealth2 can achieve similarity in several epochs even if no pre-trained model exists ",
                "[",
                "14",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "ii@ with Pre-trained Models.",
                " Moreover, FedURR ",
                "[",
                "26",
                "]",
                " also benefits from the pre-trained model. In Urban risk recognition (URR) task, the urban management usually has multiple departments, each of which stores a large amount of data locally. When data is uploaded to a central database, it means huge cost and a lot of time consumption, and there exists a risk of data leakage ",
                "[",
                "26",
                "]",
                ". Thus, the proposed framework FedURR integrates two types of Transfer Learning into the Federated Learning framework, i.e. fine-tuning based and parameter sharing based Transfer Learning methods. With the help of fine-tuning and parameter sharing, they are connected into different stages of Federated Learning with an precisely design. The experimental results are shown that FedURR can improve multi-department collaborative URR accuracy ",
                "[",
                "26",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "iii@ with Pre-trained Models.",
                " Furthermore, Zhang ",
                "et al",
                ".Â ",
                "[",
                "79",
                "]",
                " proposed the first Federated Transfer Learning framework, to solve problems in Disaster Classification in Social Computing Networks (FedDCSCN). The authors want to eliminate shortcomings of the local models of the participants, which are deep learning models. The local models need a large number of high-quality samples, and fast computation speed is required to accelerate the training process ",
                "[",
                "79",
                "]",
                ". In addition, the data labeling process is time consuming in the field of social computing, which hinders the use of deep learning networks ",
                "[",
                "79",
                "]",
                ". Thus, Federated Learning and Transfer Learning are combined to address the problems. Pre-trained model based Transfer Learning is used as to reduce communication and computation costs ",
                "[",
                "79",
                "]",
                ". Besides, homomorphic encryption approach is applied as a additional to preserve the local data privacy of social computing participants ",
                "[",
                "79",
                "]",
                ". Experimental results are shown that a feasible but not ideal performance is obtained by the framework in the social computing field ",
                "[",
                "79",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "i@ with Domain Adaptation.",
                " FedSteg ",
                "[",
                "76",
                "]",
                " provides an example for using domain adaptation.\nImage steganography is the method of concealing secret information within images. Conversely, image steganalysis is a counter method to image steganography. This method intends to detect the secret information within images. Through this detection technique, the steganographic features which are generated by image steganographic methods can be extracted. However, there are still problems that exist in image steganalysis. Image steganalysis algorithms train on machine learning models which rely on a large amount of data. However, it is hard to aggregate all the steganographic images to a global cloud server.\nMoreover, the users do not want unrelated people to snoop on confidential information. To solve the problems, Yang ",
                "et al",
                ".Â propose the framework called FedSteg. FedSteg trains a machine learning model with a privacy-protecting technique through domain adaptation. Domain adaptation is used to train the local model by decreasing the domain discrepancy between the global server and local data. Compared with traditional non-federated steganalysis techniques, the experiment results show that FedSteg achieves certain improvements ",
                "[",
                "76",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "ii@ with Domain Adaptation.",
                " Fedhealth ",
                "[",
                "15",
                "]",
                " benefits from domain adaptation. Wearable devices allow people to get access to and record healthcare information. Additionally, smart wearable devices use a large amount of personal data to train machine learning models. Different wearable devices have diverse characteristics and domains. However, the healthcare data from different people with diverse monitoring patterns are difficult to aggregate together to generate robust results. Each personal data is an island. Besides, the machine models using personal data are hard to train on cloud servers. To solve data isolation and locally training problem, Chen ",
                "et al",
                ".Â proposed a Federated Learning framework called FedHealth ",
                "[",
                "15",
                "]",
                ". In this paper, the authors used a neural network (NN), which has two convolutional layers, two pooling layers, and three fully-connected layers ",
                "[",
                "15",
                "]",
                ". NN aims at extracting low-level features. Domain adaptation is applied to transfer the extracted features from server to clients by minimizing the feature distance between server and clients. Compared to the approaches without Federated Learning and traditional methods (KNN, SVM, and RF), FedHealth achieves better performance ",
                "[",
                "15",
                "]",
                ".",
                "Application ",
                "\\@slowromancap",
                "iii@ with Domain Adaptation.",
                " One more example that uses domain adaptation is the electroencephalographic (EEG) signal classification ",
                "[",
                "32",
                "]",
                ". Brain-Computer Interface (BCI) systems are mainly to identify the usersâ€™ consciousness from the brain states. Deep learning methods achieve success in the BCI field for classification of EEG signals. However, the success is restricted to the lack of a large amount of data. Besides, according to the privacy of personal EEG data, it is constrained to build a collection of big BCI dataset. In order to solve the lack of data and the private privacy problems, Ju ",
                "et al",
                ".Â proposed a Federated Transfer Learning method for EEG Signal classification. They propose an method which use Transfer Learning technique with domain adaptation to extract the common discriminative information, and map the common discriminative information into a spatial covariance matrix, then subsequently fed the spatial covariance matrix to a deep learning based Federated Transfer Learning architecture ",
                "[",
                "32",
                "]",
                ". The proposed architecture based on deep learning has 4 layers, namely Manifold reduction layer (M), Common embedded space (C), Tangent projection layer (T) and Federated layer (F), the middle two layers (M and T) provide the functionality of Transfer Learning ",
                "[",
                "32",
                "]",
                ". The experimental result shows that this method using domain adaption in Federated Learning architecture has robust generation ability.",
                "There are two special cases of the problems to be solved in the heterogeneous Federated Transfer Learning setting, and one case for quantifying the performance of Federated Transfer Learning.",
                "Model Distillation.",
                " FedMD ",
                "[",
                "45",
                "]",
                " provides a way to solve statistical heterogeneity (the non-IID problem) in Federated Transfer Learning. Concretely, the authors in FedMD focus on the differences of local models ",
                "[",
                "45",
                "]",
                ". The authors in FedMD identify that communication is the key to fix model heterogeneity. Devices should have the ability to learn the communication protocol to leverage Transfer Learning and model distillation. The communication protocol aims to reuse the models, which are trained from a public dataset. Each client achieves a well-trained model, and applies the well-trained model on local data which is considered as Transfer Learning with model distillation. Thus, the proposed FedMD, which combines Federated Learning and Transfer Learning with knowledge distillation, allows participants to create their models locally, and a communication protocol that utilizes the power of Transfer Learning with model distillation ",
                "[",
                "45",
                "]",
                ". FedMD is demonstrated its efficiency to work on different tasks and datasets ",
                "[",
                "45",
                "]",
                ".",
                "Knowledge Distillation.",
                " Wang ",
                "et al",
                ".Â ",
                "[",
                "75",
                "]",
                " propose Federated Transfer Learning via Knowledge Distillation (FTLKD), which is a robust centralized prediction framework, and is used to solve data islands and data privacy. This framework helps participants to do heterogeneous defect prediction (HDP), predict the defect tendency regarding private models. Concretely, a pre-trained model of public datasets is transferred to the private model, and the model on the private data to converge by fine-tuning, and then the final output in each participantâ€™s private model is conveyed through knowledge distillation ",
                "[",
                "75",
                "]",
                ". Besides, HE is used to encrypt data without disturbing the processing results. Experimental results on 9 projects in 3 public databases (NASA, AEEEM and SOFTLAB) show that FTLKD outperforms the related competing methods ",
                "[",
                "75",
                "]",
                ".",
                "Quantifying Performance.",
                " In addition, the authors in ",
                "[",
                "35",
                "]",
                " analyze ",
                "three",
                " major bottlenecks in Federated Transfer Learning and their potential solutions. The main bottleneck is inter-process communication. Data exchange and memory copy in a device can cause extremely high latency. JVM native memory heap and UNIX domain sockets give us the opportunity to alleviate the type of bottlenecks ",
                "[",
                "35",
                "]",
                ". The second bottleneck is in the additional encryption tool that increases computational cost. The last is the traditional congestion control problem. Intensive data exchange causes heavy network traffic ",
                "[",
                "35",
                "]",
                ".",
                "Transfer Learning Strategies for Our Framework.",
                " The secure methods from papers ",
                "[",
                "70",
                "]",
                " ",
                "[",
                "17",
                "]",
                " ",
                "[",
                "52",
                "]",
                " ",
                "[",
                "27",
                "]",
                ", show that these are additionally add to Federated Transfer Learning meanwhile keep its original structure. The paper ",
                "[",
                "35",
                "]",
                " shows that additional secure methods bring a bottleneck to Federated Transfer Learning. Thus, we need no additional security methods but keep the original structure of Federated Transfer Learning. The methods with Pre-trained models ",
                "[",
                "15",
                "]",
                " ",
                "[",
                "26",
                "]",
                " ",
                "[",
                "79",
                "]",
                " can be considered in our framework. The applications ",
                "[",
                "76",
                "]",
                " ",
                "[",
                "15",
                "]",
                "[",
                "32",
                "]",
                " show that domain adaptation can be used to transfer features from server to participants. Methods ",
                "[",
                "45",
                "]",
                "[",
                "35",
                "]",
                " can be considered to solve problems where the data distributions of participants are different. However, these two methods require an additional public dataset. In conclusion, the methods of transferring from pre-trained models and strategies with domain adaptation can be considered in our framework."
            ]
        ]
    },
    "Ch3.S2.tab1": {
        "caption": "",
        "table": "<table id=\"Ch3.S2.tab1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch3.S2.tab1.1.1\" class=\"ltx_tr\">\n<td id=\"Ch3.S2.tab1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch3.S2.tab1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Strategies</span></td>\n<td id=\"Ch3.S2.tab1.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch3.S2.tab1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Index of Methods</span></td>\n<td id=\"Ch3.S2.tab1.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch3.S2.tab1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Methods</span></td>\n<td id=\"Ch3.S2.tab1.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch3.S2.tab1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Scenarios</span></td>\n</tr>\n<tr id=\"Ch3.S2.tab1.1.2\" class=\"ltx_tr\">\n<td id=\"Ch3.S2.tab1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch3.S2.tab1.1.2.1.1\" class=\"ltx_text\">Multimodal Transfer Module</span></td>\n<td id=\"Ch3.S2.tab1.1.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.2.2.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@</td>\n<td id=\"Ch3.S2.tab1.1.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">MMTM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib33\" title=\"\" class=\"ltx_ref\">33</a>]</cite>\n</td>\n<td id=\"Ch3.S2.tab1.1.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.2.4.1\" class=\"ltx_text\"></span><span id=\"Ch3.S2.tab1.1.2.4.2\" class=\"ltx_text\">\n<span id=\"Ch3.S2.tab1.1.2.4.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch3.S2.tab1.1.2.4.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.2.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Sign Language Recognition</span></span>\n<span id=\"Ch3.S2.tab1.1.2.4.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.2.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Action Recognition</span></span>\n<span id=\"Ch3.S2.tab1.1.2.4.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.2.4.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Speech Enhancement</span></span>\n</span></span><span id=\"Ch3.S2.tab1.1.2.4.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch3.S2.tab1.1.3\" class=\"ltx_tr\">\n<td id=\"Ch3.S2.tab1.1.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"6\"><span id=\"Ch3.S2.tab1.1.3.1.1\" class=\"ltx_text\"><span id=\"Ch3.S2.tab1.1.3.1.1.1\" class=\"ltx_text\"></span><span id=\"Ch3.S2.tab1.1.3.1.1.2\" class=\"ltx_text\">\n<span id=\"Ch3.S2.tab1.1.3.1.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch3.S2.tab1.1.3.1.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.3.1.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Multimodal</span></span>\n<span id=\"Ch3.S2.tab1.1.3.1.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.3.1.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Domain</span></span>\n<span id=\"Ch3.S2.tab1.1.3.1.1.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.3.1.1.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Adaptation</span></span>\n</span></span> <span id=\"Ch3.S2.tab1.1.3.1.1.3\" class=\"ltx_text\"></span></span></td>\n<td id=\"Ch3.S2.tab1.1.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.3.2.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@.<span id=\"Ch3.S2.tab1.1.3.2.2\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@</td>\n<td id=\"Ch3.S2.tab1.1.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">MDANN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib63\" title=\"\" class=\"ltx_ref\">63</a>]</cite>\n</td>\n<td id=\"Ch3.S2.tab1.1.3.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.3.4.1\" class=\"ltx_text\"></span><span id=\"Ch3.S2.tab1.1.3.4.2\" class=\"ltx_text\">\n<span id=\"Ch3.S2.tab1.1.3.4.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch3.S2.tab1.1.3.4.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.3.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Emotion Recognition</span></span>\n<span id=\"Ch3.S2.tab1.1.3.4.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.3.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Cross-Media Retrieval</span></span>\n</span></span><span id=\"Ch3.S2.tab1.1.3.4.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch3.S2.tab1.1.4\" class=\"ltx_tr\">\n<td id=\"Ch3.S2.tab1.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.4.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@.<span id=\"Ch3.S2.tab1.1.4.1.2\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>ii@</td>\n<td id=\"Ch3.S2.tab1.1.4.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">ADA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a>]</cite>\n</td>\n<td id=\"Ch3.S2.tab1.1.4.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Vigilance Estimation</td>\n</tr>\n<tr id=\"Ch3.S2.tab1.1.5\" class=\"ltx_tr\">\n<td id=\"Ch3.S2.tab1.1.5.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.5.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@.<span id=\"Ch3.S2.tab1.1.5.1.2\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iii@</td>\n<td id=\"Ch3.S2.tab1.1.5.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">MM-SADA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite>\n</td>\n<td id=\"Ch3.S2.tab1.1.5.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Vigilance Estimation</td>\n</tr>\n<tr id=\"Ch3.S2.tab1.1.6\" class=\"ltx_tr\">\n<td id=\"Ch3.S2.tab1.1.6.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.6.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@.<span id=\"Ch3.S2.tab1.1.6.1.2\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iv@</td>\n<td id=\"Ch3.S2.tab1.1.6.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">DLMM <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib42\" title=\"\" class=\"ltx_ref\">42</a>]</cite>\n</td>\n<td id=\"Ch3.S2.tab1.1.6.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.6.3.1\" class=\"ltx_text\"></span><span id=\"Ch3.S2.tab1.1.6.3.2\" class=\"ltx_text\">\n<span id=\"Ch3.S2.tab1.1.6.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch3.S2.tab1.1.6.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.6.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Event Recognition</span></span>\n<span id=\"Ch3.S2.tab1.1.6.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch3.S2.tab1.1.6.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Action Recognition</span></span>\n</span></span><span id=\"Ch3.S2.tab1.1.6.3.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch3.S2.tab1.1.7\" class=\"ltx_tr\">\n<td id=\"Ch3.S2.tab1.1.7.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.7.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@.<span id=\"Ch3.S2.tab1.1.7.1.2\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>v@</td>\n<td id=\"Ch3.S2.tab1.1.7.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">PMC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib80\" title=\"\" class=\"ltx_ref\">80</a>]</cite>\n</td>\n<td id=\"Ch3.S2.tab1.1.7.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Visual Recognition</td>\n</tr>\n<tr id=\"Ch3.S2.tab1.1.8\" class=\"ltx_tr\">\n<td id=\"Ch3.S2.tab1.1.8.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">\n<span id=\"Ch3.S2.tab1.1.8.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@.<span id=\"Ch3.S2.tab1.1.8.1.2\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>vi@</td>\n<td id=\"Ch3.S2.tab1.1.8.2\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">JADA <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib43\" title=\"\" class=\"ltx_ref\">43</a>]</cite>\n</td>\n<td id=\"Ch3.S2.tab1.1.8.3\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">Image Classification</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The initial goal of Federated Learning is to carry out useful machine learning methods in multiple devices, and build an aggregated model based on dataset across multiple devices, while ensuring usersâ€™ privacy and security [37]. Thus, we should obey the original goal of federated (transfer) learning. Although they are not chosen by us, we briefly review some other alternative for security and privacy in machine learning (see Table 3.1) for completeness.",
            "MM-SADA. Similar to MDANN, fine-grained action recognition is also an application of multimodal domain adaptation. Munro et al.Â [49] propose Multi-Modal Self-Supervised Adversarial Domain Adaptation (MM-SADA) to address the inevitable domain shift problem, which training a model in one environment and then deploying it in another leads to performance degradation. In addition to domain adaptation, i.e. adversarial alignment, Munro et al.Â also leverage the correspondence of modalities as a self-supervised alignment method for Unsupervised Learning [49]. More specifically, they use self-supervision technique between different modalities in one sample, and domain adaptation (adversarial alignment) is used in the same modality [49]. Experiment results show MM-SADA outperforms other unsupervised domain adaptation methods.",
            "Effectiveness Analysis of Self-Supervision. Ericsson et al.Â [22] analyze the effectiveness of Self-Supervised Learning with comparison between Supervised Learning and Self-Supervised Learning. There is no best pre-trained Self-Supervised model is suitable for all downstream tasks, the best self-supervised methods can surpass supervised differs from Supervised Learning in terms of the information representations. If the downstream tasks perform on recognition tasks, the performance is seriously correlated. However, when the datasets are irrelevant to recognition, the correlation exists little.",
            "Combination \\@slowromancapiii@. The third combination approach is that a fusion approach is used to connect all the modalities N1subscriptğ‘1N_{1} and N2subscriptğ‘2N_{2} in P1subscriptğ‘ƒ1P_{1}, namely co-training sub-networks of their modalities, and then transfer N1subscriptğ‘1N_{1} in P2subscriptğ‘ƒ2P_{2}. The representative approaches of multimodal domain adaptation, including MDANN [63], MM-SADA [49], ADA [46], DLMM [42], PMC [80], and JADA [43], give us a hint to transfer after co-training sub-networks. However, multimodal domain adaptation approaches apply transferring between two multimodal datasets to solve domain shift problem. Domain adaptation approach in Federated Transfer Learning [32] give us a direction that all participants can learn a common discriminative information to do better transferring. Besides domain adaptation, Contrastive Learning (Self-Supervised Learning) have the advantages of both co-training sub-networks and learning discriminative information. AVC [2], AVE-Net [3], CrossCLR [83], AVID [52], LWTNet [1], CBT [66], CSTNet [36] give us support to learn fused feature representations and keep the matching information at same time (see Table 3.4).",
            "We choose Contrastive Learning as our Transfer Learning strategy. From the related work of Contrastive Learning, AVC [2] is the first paper using Contrastive Learning with an image-audio dataset. AVC applies contrastive loss to learn matching between different modalities. The contrastive loss of AVC is similar as it in SimCLR [13], which is calculated after fusing different modalities. AVE-Net [3] goes one step further based on AVC. This step is embedding [3], which is used to extract features after fusing. However, the authors in CrossCLR [83] analyzed that the embeddings ignore the inter-similarities between modalities. Thus, we choose the contrastive loss function in SimCLR to learn representative features from multimodal data. (refer to Table 3.4)\n",
            "Dataset.\nThe dataset consists of videos from 9 different environments. In total, 45 video sources have been processed, and they are at 29.97 frames per second and then reduced to 2000 seconds. Each frame (at 0.5s, 1.5s, 2.5s â€¦ and so on) of a video is extracted as a image, at the same time, each MFCC [48] audio statistic is also extracted from corresponding second of video. These 9 environmental classes are in the table 5.1. All the videos are captured in the nature environment. In order to make better distinction between different classes and obtain better recognition results, it is necessary to crop the image at each second to obtain the initial data objects. As it is shown in Figure 5.1, each frame of video and each piece of audio are extracted in pairs. Thus, 32,000 recognizable objects and 17,252 RGB images pairwise with 17,252 seconds of audio [7].",
            "We implement baseline models in [7], VGG16+MLP for image, and MFCC+DMLP for audio and VGG16+MLP and MFCC+DMLP for image-audio are trained from scratch.\nThe accuracy is about 7% higher than the baseline in image modality. The implementation of auditory brings about 1% lower accuracy than the baseline. The accuracy of image-audio multi-modality is close to the baseline (Table 6.1).",
            "The training losses of implemented and baseline models dramatically decrease in 10 global communication epochs, and the losses remain at 0.05 after ten global epochs. However, all validation losses decrease with jitters, and validation loss of auditory modality increases slowly after 20 global epochs. Besides, all training and validation accuracies increase dramatically in 10 global epochs and remain stable after 20 global epochs except for image modality. The validation accuracy of image modality remains unchanged with jitter (See Figures 6.2, 6.3 and 6.4).",
            "The accuracies of all the implemented baseline models with Federated Learning are lower than those without Federated Learning.\nMore specifically, the accuracies of baseline models in Federated Learning are about 3.5% lower than those without Federated Learning in two uni-modalities. In addition, the accuracy of the implemented baseline model of multi-modality with Federated Learning is about 1% lower than the same central model with no Federated Learning (Table 6.2).",
            "The training and validation losses of implemented baseline models with Federated Learning dramatically decrease in 10 global epochs, and they remain unchanged after 20 global epochs.\nThe validation losses of image and image-audio modalities remain unchanged with jitter at 0.25 after 20 global epochs, while it stays at 0.75 for audio modality.\nBesides, all training and validation accuracies increase dramatically in 10 global epochs and remain stable with slight jitter after 20 global epochs (Figures 6.8, 6.9 and 6.10).",
            "The accuracies of our framework (Federated Transfer Learning) are higher than the implemented baseline models with Federated Learning.\nMore specifically, the accuracy of image modality is about 0.5% higher, and that of audio modality is about 5% improved (Table 6.3).",
            "Our frameworkâ€™s training and validation losses for two uni-modalities dramatically decrease in 10 global epochs, and they remain stable after 20 global epochs.\nThe training losses of image and audio modalities remain unchanged with jitter at 0.25 after 20 global epochs, while the validation of those remains stable with jitter at 0.3. Besides, after 60 global epochs, the validation loss of image modality increases negligibly.\nFor image-audio multi-modality (Contrastive Learning), its training and validation losses dramatically decrease in 40 global epochs and remain unchanged after 60 global epochs.\nMoreover, all training and validation accuracies increase dramatically in 10 global epochs and remain stable with slight jitter after 40 global epochs. From 80 to 100 global epochs, the validation accuracies of image and audio overlap (Figures 6.14).",
            "The accuracies of our framework (Federated Transfer Learning) are higher than the implemented baseline models in Federated Learning.\nMore specifically, the accuracy of image modality is about 1% higher, and that of audio modality is about 4% improved (Table 6.4).",
            "Our frameworkâ€™s training and validation losses for two uni-modalities dramatically decrease in 10 global epochs, and they remain stable after 20 global epochs.\nThe training losses of image and audio modalities remain unchanged with a jitter at 0.15 after 20 global epochs, while the validation of those remains stable with small jitters at 0.4.\nFor image-audio multi-modality (Contrastive Learning), training loss dramatically decreases in 40 global epochs. Moreover, it remains unchanged with small jitters after 60 global epochs, while the validation is smoother after 40 global epochs.\nIn addition, all training and validation accuracies increase dramatically in 10 global epochs and remain stable with slight jitter after 40 global epochs. From 20 to 100 global epochs, the validation accuracy of the image is always higher than the audio (Figure 6.17).",
            "The accuracies of our framework (Federated Transfer Learning) are higher than the implemented baseline models in Federated Learning.\nMore specifically, the accuracy of image modality is about 0.5% higher, and that of audio modality is about 4.5% improved (Table 6.5).",
            "Our frameworkâ€™s training and validation losses for two uni-modalities dramatically decrease in 10 global epochs, and they remain stable after 20 global epochs.\nThe training losses of image and audio modalities remain unchanged with jitter at 0.1 after 20 global epochs, while the validation of those remains stable with jitter at 0.3. Besides, after 50 global epochs, the validation loss of image modality increases negligibly. Both uni-modalities of training loss overlap after 35 global epochs.\nFor image-audio multi-modality (self-supervision), its training and validation losses dramatically and smoothly decrease in 40 global epochs and remain unchanged after 60 global epochs.\nIn addition, all training and validation accuracies increase dramatically in 10 global epochs and remain stable with slight jitter after 40 global epochs. From 10 to 100 global epochs, the validation accuracy of the image is always higher than the audio (Figure 6.20)."
        ]
    },
    "Ch3.T4": {
        "caption": "Table 3.4: Applications for multimodal Transfer Learning. The matching column represents different approaches of connecting different modalities.",
        "table": "<table id=\"Ch3.T4.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch3.T4.2.1\" class=\"ltx_tr\">\n<td id=\"Ch3.T4.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch3.T4.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Index for Contrastive Learning</span></td>\n<td id=\"Ch3.T4.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch3.T4.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Methods</span></td>\n<td id=\"Ch3.T4.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch3.T4.2.1.3.1\" class=\"ltx_text ltx_font_bold\">Scenarios</span></td>\n<td id=\"Ch3.T4.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch3.T4.2.1.4.1\" class=\"ltx_text ltx_font_bold\">Matching Methods</span></td>\n</tr>\n<tr id=\"Ch3.T4.2.2\" class=\"ltx_tr\">\n<td id=\"Ch3.T4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.2.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@</td>\n<td id=\"Ch3.T4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">AVC <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a>]</cite>\n</td>\n<td id=\"Ch3.T4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Audio-Visual</td>\n<td id=\"Ch3.T4.2.2.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Late Fusion</td>\n</tr>\n<tr id=\"Ch3.T4.2.3\" class=\"ltx_tr\">\n<td id=\"Ch3.T4.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.3.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>ii@</td>\n<td id=\"Ch3.T4.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">AVE-Net <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>]</cite>\n</td>\n<td id=\"Ch3.T4.2.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Audio-Visual</td>\n<td id=\"Ch3.T4.2.3.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.3.4.1\" class=\"ltx_text\"></span><span id=\"Ch3.T4.2.3.4.2\" class=\"ltx_text\">\n<span id=\"Ch3.T4.2.3.4.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch3.T4.2.3.4.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.3.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Late Fusion</span></span>\n<span id=\"Ch3.T4.2.3.4.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.3.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Embedding</span></span>\n</span></span><span id=\"Ch3.T4.2.3.4.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch3.T4.2.4\" class=\"ltx_tr\">\n<td id=\"Ch3.T4.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.4.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iii@</td>\n<td id=\"Ch3.T4.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">CrossCLR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib83\" title=\"\" class=\"ltx_ref\">83</a>]</cite>\n</td>\n<td id=\"Ch3.T4.2.4.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Audio-Visual</td>\n<td id=\"Ch3.T4.2.4.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.4.4.1\" class=\"ltx_text\"></span><span id=\"Ch3.T4.2.4.4.2\" class=\"ltx_text\">\n<span id=\"Ch3.T4.2.4.4.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch3.T4.2.4.4.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.4.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Late Fusion</span></span>\n<span id=\"Ch3.T4.2.4.4.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.4.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Embedding</span></span>\n<span id=\"Ch3.T4.2.4.4.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.4.4.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Inter-Similarity</span></span>\n</span></span><span id=\"Ch3.T4.2.4.4.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch3.T4.2.5\" class=\"ltx_tr\">\n<td id=\"Ch3.T4.2.5.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.5.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iv@</td>\n<td id=\"Ch3.T4.2.5.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">AVID <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite>\n</td>\n<td id=\"Ch3.T4.2.5.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Audio-Visual</td>\n<td id=\"Ch3.T4.2.5.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Attention Map</td>\n</tr>\n<tr id=\"Ch3.T4.2.6\" class=\"ltx_tr\">\n<td id=\"Ch3.T4.2.6.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.6.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>v@</td>\n<td id=\"Ch3.T4.2.6.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">LWTNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib1\" title=\"\" class=\"ltx_ref\">1</a>]</cite>\n</td>\n<td id=\"Ch3.T4.2.6.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Audio-Visual</td>\n<td id=\"Ch3.T4.2.6.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.6.4.1\" class=\"ltx_text\"></span><span id=\"Ch3.T4.2.6.4.2\" class=\"ltx_text\">\n<span id=\"Ch3.T4.2.6.4.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch3.T4.2.6.4.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.6.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Late Fusion</span></span>\n<span id=\"Ch3.T4.2.6.4.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.6.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Embedding</span></span>\n<span id=\"Ch3.T4.2.6.4.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.6.4.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Agreement</span></span>\n</span></span><span id=\"Ch3.T4.2.6.4.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch3.T4.2.7\" class=\"ltx_tr\">\n<td id=\"Ch3.T4.2.7.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.7.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>vi@</td>\n<td id=\"Ch3.T4.2.7.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">CBT <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib66\" title=\"\" class=\"ltx_ref\">66</a>]</cite>\n</td>\n<td id=\"Ch3.T4.2.7.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Video-Text</td>\n<td id=\"Ch3.T4.2.7.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.7.4.1\" class=\"ltx_text\"></span><span id=\"Ch3.T4.2.7.4.2\" class=\"ltx_text\">\n<span id=\"Ch3.T4.2.7.4.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch3.T4.2.7.4.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.7.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Late Fusion with Pre-trained</span></span>\n<span id=\"Ch3.T4.2.7.4.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch3.T4.2.7.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">models</span></span>\n</span></span><span id=\"Ch3.T4.2.7.4.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch3.T4.2.8\" class=\"ltx_tr\">\n<td id=\"Ch3.T4.2.8.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch3.T4.2.8.1.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>vii@</td>\n<td id=\"Ch3.T4.2.8.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">CSTNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">36</a>]</cite>\n</td>\n<td id=\"Ch3.T4.2.8.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">Audio-Text</td>\n<td id=\"Ch3.T4.2.8.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">Complex Late Fusion</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Combination \\@slowromancapiii@. The third combination approach is that a fusion approach is used to connect all the modalities N1subscriptğ‘1N_{1} and N2subscriptğ‘2N_{2} in P1subscriptğ‘ƒ1P_{1}, namely co-training sub-networks of their modalities, and then transfer N1subscriptğ‘1N_{1} in P2subscriptğ‘ƒ2P_{2}. The representative approaches of multimodal domain adaptation, including MDANN [63], MM-SADA [49], ADA [46], DLMM [42], PMC [80], and JADA [43], give us a hint to transfer after co-training sub-networks. However, multimodal domain adaptation approaches apply transferring between two multimodal datasets to solve domain shift problem. Domain adaptation approach in Federated Transfer Learning [32] give us a direction that all participants can learn a common discriminative information to do better transferring. Besides domain adaptation, Contrastive Learning (Self-Supervised Learning) have the advantages of both co-training sub-networks and learning discriminative information. AVC [2], AVE-Net [3], CrossCLR [83], AVID [52], LWTNet [1], CBT [66], CSTNet [36] give us support to learn fused feature representations and keep the matching information at same time (see Table 3.4).",
            "We choose Contrastive Learning as our Transfer Learning strategy. From the related work of Contrastive Learning, AVC [2] is the first paper using Contrastive Learning with an image-audio dataset. AVC applies contrastive loss to learn matching between different modalities. The contrastive loss of AVC is similar as it in SimCLR [13], which is calculated after fusing different modalities. AVE-Net [3] goes one step further based on AVC. This step is embedding [3], which is used to extract features after fusing. However, the authors in CrossCLR [83] analyzed that the embeddings ignore the inter-similarities between modalities. Thus, we choose the contrastive loss function in SimCLR to learn representative features from multimodal data. (refer to Table 3.4)\n"
        ]
    },
    "Ch4.T1": {
        "caption": "Table 4.1: Different combinations of Transfer Learning for different modalities.",
        "table": "<table id=\"Ch4.T1.15\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch4.T1.15.16\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.15.16.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch4.T1.15.16.1.1\" class=\"ltx_text ltx_font_bold\">Combination Index</span></td>\n<td id=\"Ch4.T1.15.16.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch4.T1.15.16.2.1\" class=\"ltx_text ltx_font_bold\">Combinations after Grouped Federated Learning</span></td>\n</tr>\n<tr id=\"Ch4.T1.4.4\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.4.4.5\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch4.T1.4.4.5.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>i@</td>\n<td id=\"Ch4.T1.4.4.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">From (<math id=\"Ch4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"N_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.1.1.1.m1.1a\"><msub id=\"Ch4.T1.1.1.1.m1.1.1\" xref=\"Ch4.T1.1.1.1.m1.1.1.cmml\"><mi id=\"Ch4.T1.1.1.1.m1.1.1.2\" xref=\"Ch4.T1.1.1.1.m1.1.1.2.cmml\">N</mi><mn id=\"Ch4.T1.1.1.1.m1.1.1.3\" xref=\"Ch4.T1.1.1.1.m1.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.1.1.1.m1.1b\"><apply id=\"Ch4.T1.1.1.1.m1.1.1.cmml\" xref=\"Ch4.T1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.1.1.1.m1.1.1.1.cmml\" xref=\"Ch4.T1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"Ch4.T1.1.1.1.m1.1.1.2.cmml\" xref=\"Ch4.T1.1.1.1.m1.1.1.2\">ğ‘</ci><cn type=\"integer\" id=\"Ch4.T1.1.1.1.m1.1.1.3.cmml\" xref=\"Ch4.T1.1.1.1.m1.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.1.1.1.m1.1c\">N_{1}</annotation></semantics></math> in <math id=\"Ch4.T1.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"P_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.2.2.2.m2.1a\"><msub id=\"Ch4.T1.2.2.2.m2.1.1\" xref=\"Ch4.T1.2.2.2.m2.1.1.cmml\"><mi id=\"Ch4.T1.2.2.2.m2.1.1.2\" xref=\"Ch4.T1.2.2.2.m2.1.1.2.cmml\">P</mi><mn id=\"Ch4.T1.2.2.2.m2.1.1.3\" xref=\"Ch4.T1.2.2.2.m2.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.2.2.2.m2.1b\"><apply id=\"Ch4.T1.2.2.2.m2.1.1.cmml\" xref=\"Ch4.T1.2.2.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.2.2.2.m2.1.1.1.cmml\" xref=\"Ch4.T1.2.2.2.m2.1.1\">subscript</csymbol><ci id=\"Ch4.T1.2.2.2.m2.1.1.2.cmml\" xref=\"Ch4.T1.2.2.2.m2.1.1.2\">ğ‘ƒ</ci><cn type=\"integer\" id=\"Ch4.T1.2.2.2.m2.1.1.3.cmml\" xref=\"Ch4.T1.2.2.2.m2.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.2.2.2.m2.1c\">P_{1}</annotation></semantics></math>) transfer to (<math id=\"Ch4.T1.3.3.3.m3.1\" class=\"ltx_Math\" alttext=\"N_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.3.3.3.m3.1a\"><msub id=\"Ch4.T1.3.3.3.m3.1.1\" xref=\"Ch4.T1.3.3.3.m3.1.1.cmml\"><mi id=\"Ch4.T1.3.3.3.m3.1.1.2\" xref=\"Ch4.T1.3.3.3.m3.1.1.2.cmml\">N</mi><mn id=\"Ch4.T1.3.3.3.m3.1.1.3\" xref=\"Ch4.T1.3.3.3.m3.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.3.3.3.m3.1b\"><apply id=\"Ch4.T1.3.3.3.m3.1.1.cmml\" xref=\"Ch4.T1.3.3.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.3.3.3.m3.1.1.1.cmml\" xref=\"Ch4.T1.3.3.3.m3.1.1\">subscript</csymbol><ci id=\"Ch4.T1.3.3.3.m3.1.1.2.cmml\" xref=\"Ch4.T1.3.3.3.m3.1.1.2\">ğ‘</ci><cn type=\"integer\" id=\"Ch4.T1.3.3.3.m3.1.1.3.cmml\" xref=\"Ch4.T1.3.3.3.m3.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.3.3.3.m3.1c\">N_{1}</annotation></semantics></math> in <math id=\"Ch4.T1.4.4.4.m4.1\" class=\"ltx_Math\" alttext=\"P_{2}\" display=\"inline\"><semantics id=\"Ch4.T1.4.4.4.m4.1a\"><msub id=\"Ch4.T1.4.4.4.m4.1.1\" xref=\"Ch4.T1.4.4.4.m4.1.1.cmml\"><mi id=\"Ch4.T1.4.4.4.m4.1.1.2\" xref=\"Ch4.T1.4.4.4.m4.1.1.2.cmml\">P</mi><mn id=\"Ch4.T1.4.4.4.m4.1.1.3\" xref=\"Ch4.T1.4.4.4.m4.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.4.4.4.m4.1b\"><apply id=\"Ch4.T1.4.4.4.m4.1.1.cmml\" xref=\"Ch4.T1.4.4.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.4.4.4.m4.1.1.1.cmml\" xref=\"Ch4.T1.4.4.4.m4.1.1\">subscript</csymbol><ci id=\"Ch4.T1.4.4.4.m4.1.1.2.cmml\" xref=\"Ch4.T1.4.4.4.m4.1.1.2\">ğ‘ƒ</ci><cn type=\"integer\" id=\"Ch4.T1.4.4.4.m4.1.1.3.cmml\" xref=\"Ch4.T1.4.4.4.m4.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.4.4.4.m4.1c\">P_{2}</annotation></semantics></math>)</td>\n</tr>\n<tr id=\"Ch4.T1.10.10\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.10.10.7\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch4.T1.10.10.7.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>ii@</td>\n<td id=\"Ch4.T1.10.10.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">From (<math id=\"Ch4.T1.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"N_{2}\" display=\"inline\"><semantics id=\"Ch4.T1.5.5.1.m1.1a\"><msub id=\"Ch4.T1.5.5.1.m1.1.1\" xref=\"Ch4.T1.5.5.1.m1.1.1.cmml\"><mi id=\"Ch4.T1.5.5.1.m1.1.1.2\" xref=\"Ch4.T1.5.5.1.m1.1.1.2.cmml\">N</mi><mn id=\"Ch4.T1.5.5.1.m1.1.1.3\" xref=\"Ch4.T1.5.5.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.5.5.1.m1.1b\"><apply id=\"Ch4.T1.5.5.1.m1.1.1.cmml\" xref=\"Ch4.T1.5.5.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.5.5.1.m1.1.1.1.cmml\" xref=\"Ch4.T1.5.5.1.m1.1.1\">subscript</csymbol><ci id=\"Ch4.T1.5.5.1.m1.1.1.2.cmml\" xref=\"Ch4.T1.5.5.1.m1.1.1.2\">ğ‘</ci><cn type=\"integer\" id=\"Ch4.T1.5.5.1.m1.1.1.3.cmml\" xref=\"Ch4.T1.5.5.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.5.5.1.m1.1c\">N_{2}</annotation></semantics></math> in <math id=\"Ch4.T1.6.6.2.m2.1\" class=\"ltx_Math\" alttext=\"P_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.6.6.2.m2.1a\"><msub id=\"Ch4.T1.6.6.2.m2.1.1\" xref=\"Ch4.T1.6.6.2.m2.1.1.cmml\"><mi id=\"Ch4.T1.6.6.2.m2.1.1.2\" xref=\"Ch4.T1.6.6.2.m2.1.1.2.cmml\">P</mi><mn id=\"Ch4.T1.6.6.2.m2.1.1.3\" xref=\"Ch4.T1.6.6.2.m2.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.6.6.2.m2.1b\"><apply id=\"Ch4.T1.6.6.2.m2.1.1.cmml\" xref=\"Ch4.T1.6.6.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.6.6.2.m2.1.1.1.cmml\" xref=\"Ch4.T1.6.6.2.m2.1.1\">subscript</csymbol><ci id=\"Ch4.T1.6.6.2.m2.1.1.2.cmml\" xref=\"Ch4.T1.6.6.2.m2.1.1.2\">ğ‘ƒ</ci><cn type=\"integer\" id=\"Ch4.T1.6.6.2.m2.1.1.3.cmml\" xref=\"Ch4.T1.6.6.2.m2.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.6.6.2.m2.1c\">P_{1}</annotation></semantics></math>) transfer to (<math id=\"Ch4.T1.7.7.3.m3.1\" class=\"ltx_Math\" alttext=\"N_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.7.7.3.m3.1a\"><msub id=\"Ch4.T1.7.7.3.m3.1.1\" xref=\"Ch4.T1.7.7.3.m3.1.1.cmml\"><mi id=\"Ch4.T1.7.7.3.m3.1.1.2\" xref=\"Ch4.T1.7.7.3.m3.1.1.2.cmml\">N</mi><mn id=\"Ch4.T1.7.7.3.m3.1.1.3\" xref=\"Ch4.T1.7.7.3.m3.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.7.7.3.m3.1b\"><apply id=\"Ch4.T1.7.7.3.m3.1.1.cmml\" xref=\"Ch4.T1.7.7.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.7.7.3.m3.1.1.1.cmml\" xref=\"Ch4.T1.7.7.3.m3.1.1\">subscript</csymbol><ci id=\"Ch4.T1.7.7.3.m3.1.1.2.cmml\" xref=\"Ch4.T1.7.7.3.m3.1.1.2\">ğ‘</ci><cn type=\"integer\" id=\"Ch4.T1.7.7.3.m3.1.1.3.cmml\" xref=\"Ch4.T1.7.7.3.m3.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.7.7.3.m3.1c\">N_{1}</annotation></semantics></math> in <math id=\"Ch4.T1.8.8.4.m4.1\" class=\"ltx_Math\" alttext=\"P_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.8.8.4.m4.1a\"><msub id=\"Ch4.T1.8.8.4.m4.1.1\" xref=\"Ch4.T1.8.8.4.m4.1.1.cmml\"><mi id=\"Ch4.T1.8.8.4.m4.1.1.2\" xref=\"Ch4.T1.8.8.4.m4.1.1.2.cmml\">P</mi><mn id=\"Ch4.T1.8.8.4.m4.1.1.3\" xref=\"Ch4.T1.8.8.4.m4.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.8.8.4.m4.1b\"><apply id=\"Ch4.T1.8.8.4.m4.1.1.cmml\" xref=\"Ch4.T1.8.8.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.8.8.4.m4.1.1.1.cmml\" xref=\"Ch4.T1.8.8.4.m4.1.1\">subscript</csymbol><ci id=\"Ch4.T1.8.8.4.m4.1.1.2.cmml\" xref=\"Ch4.T1.8.8.4.m4.1.1.2\">ğ‘ƒ</ci><cn type=\"integer\" id=\"Ch4.T1.8.8.4.m4.1.1.3.cmml\" xref=\"Ch4.T1.8.8.4.m4.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.8.8.4.m4.1c\">P_{1}</annotation></semantics></math>), then transfer to (<math id=\"Ch4.T1.9.9.5.m5.1\" class=\"ltx_Math\" alttext=\"N_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.9.9.5.m5.1a\"><msub id=\"Ch4.T1.9.9.5.m5.1.1\" xref=\"Ch4.T1.9.9.5.m5.1.1.cmml\"><mi id=\"Ch4.T1.9.9.5.m5.1.1.2\" xref=\"Ch4.T1.9.9.5.m5.1.1.2.cmml\">N</mi><mn id=\"Ch4.T1.9.9.5.m5.1.1.3\" xref=\"Ch4.T1.9.9.5.m5.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.9.9.5.m5.1b\"><apply id=\"Ch4.T1.9.9.5.m5.1.1.cmml\" xref=\"Ch4.T1.9.9.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.9.9.5.m5.1.1.1.cmml\" xref=\"Ch4.T1.9.9.5.m5.1.1\">subscript</csymbol><ci id=\"Ch4.T1.9.9.5.m5.1.1.2.cmml\" xref=\"Ch4.T1.9.9.5.m5.1.1.2\">ğ‘</ci><cn type=\"integer\" id=\"Ch4.T1.9.9.5.m5.1.1.3.cmml\" xref=\"Ch4.T1.9.9.5.m5.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.9.9.5.m5.1c\">N_{1}</annotation></semantics></math> in <math id=\"Ch4.T1.10.10.6.m6.1\" class=\"ltx_Math\" alttext=\"P_{2}\" display=\"inline\"><semantics id=\"Ch4.T1.10.10.6.m6.1a\"><msub id=\"Ch4.T1.10.10.6.m6.1.1\" xref=\"Ch4.T1.10.10.6.m6.1.1.cmml\"><mi id=\"Ch4.T1.10.10.6.m6.1.1.2\" xref=\"Ch4.T1.10.10.6.m6.1.1.2.cmml\">P</mi><mn id=\"Ch4.T1.10.10.6.m6.1.1.3\" xref=\"Ch4.T1.10.10.6.m6.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.10.10.6.m6.1b\"><apply id=\"Ch4.T1.10.10.6.m6.1.1.cmml\" xref=\"Ch4.T1.10.10.6.m6.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.10.10.6.m6.1.1.1.cmml\" xref=\"Ch4.T1.10.10.6.m6.1.1\">subscript</csymbol><ci id=\"Ch4.T1.10.10.6.m6.1.1.2.cmml\" xref=\"Ch4.T1.10.10.6.m6.1.1.2\">ğ‘ƒ</ci><cn type=\"integer\" id=\"Ch4.T1.10.10.6.m6.1.1.3.cmml\" xref=\"Ch4.T1.10.10.6.m6.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.10.10.6.m6.1c\">P_{2}</annotation></semantics></math>)</td>\n</tr>\n<tr id=\"Ch4.T1.15.15\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.15.15.6\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch4.T1.15.15.6.1\" class=\"ltx_ERROR undefined\">\\@slowromancap</span>iii@</td>\n<td id=\"Ch4.T1.15.15.5\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">Fuse (<math id=\"Ch4.T1.11.11.1.m1.1\" class=\"ltx_Math\" alttext=\"N_{2}\" display=\"inline\"><semantics id=\"Ch4.T1.11.11.1.m1.1a\"><msub id=\"Ch4.T1.11.11.1.m1.1.1\" xref=\"Ch4.T1.11.11.1.m1.1.1.cmml\"><mi id=\"Ch4.T1.11.11.1.m1.1.1.2\" xref=\"Ch4.T1.11.11.1.m1.1.1.2.cmml\">N</mi><mn id=\"Ch4.T1.11.11.1.m1.1.1.3\" xref=\"Ch4.T1.11.11.1.m1.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.11.11.1.m1.1b\"><apply id=\"Ch4.T1.11.11.1.m1.1.1.cmml\" xref=\"Ch4.T1.11.11.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.11.11.1.m1.1.1.1.cmml\" xref=\"Ch4.T1.11.11.1.m1.1.1\">subscript</csymbol><ci id=\"Ch4.T1.11.11.1.m1.1.1.2.cmml\" xref=\"Ch4.T1.11.11.1.m1.1.1.2\">ğ‘</ci><cn type=\"integer\" id=\"Ch4.T1.11.11.1.m1.1.1.3.cmml\" xref=\"Ch4.T1.11.11.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.11.11.1.m1.1c\">N_{2}</annotation></semantics></math> and <math id=\"Ch4.T1.12.12.2.m2.1\" class=\"ltx_Math\" alttext=\"N_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.12.12.2.m2.1a\"><msub id=\"Ch4.T1.12.12.2.m2.1.1\" xref=\"Ch4.T1.12.12.2.m2.1.1.cmml\"><mi id=\"Ch4.T1.12.12.2.m2.1.1.2\" xref=\"Ch4.T1.12.12.2.m2.1.1.2.cmml\">N</mi><mn id=\"Ch4.T1.12.12.2.m2.1.1.3\" xref=\"Ch4.T1.12.12.2.m2.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.12.12.2.m2.1b\"><apply id=\"Ch4.T1.12.12.2.m2.1.1.cmml\" xref=\"Ch4.T1.12.12.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.12.12.2.m2.1.1.1.cmml\" xref=\"Ch4.T1.12.12.2.m2.1.1\">subscript</csymbol><ci id=\"Ch4.T1.12.12.2.m2.1.1.2.cmml\" xref=\"Ch4.T1.12.12.2.m2.1.1.2\">ğ‘</ci><cn type=\"integer\" id=\"Ch4.T1.12.12.2.m2.1.1.3.cmml\" xref=\"Ch4.T1.12.12.2.m2.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.12.12.2.m2.1c\">N_{1}</annotation></semantics></math> in <math id=\"Ch4.T1.13.13.3.m3.1\" class=\"ltx_Math\" alttext=\"P_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.13.13.3.m3.1a\"><msub id=\"Ch4.T1.13.13.3.m3.1.1\" xref=\"Ch4.T1.13.13.3.m3.1.1.cmml\"><mi id=\"Ch4.T1.13.13.3.m3.1.1.2\" xref=\"Ch4.T1.13.13.3.m3.1.1.2.cmml\">P</mi><mn id=\"Ch4.T1.13.13.3.m3.1.1.3\" xref=\"Ch4.T1.13.13.3.m3.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.13.13.3.m3.1b\"><apply id=\"Ch4.T1.13.13.3.m3.1.1.cmml\" xref=\"Ch4.T1.13.13.3.m3.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.13.13.3.m3.1.1.1.cmml\" xref=\"Ch4.T1.13.13.3.m3.1.1\">subscript</csymbol><ci id=\"Ch4.T1.13.13.3.m3.1.1.2.cmml\" xref=\"Ch4.T1.13.13.3.m3.1.1.2\">ğ‘ƒ</ci><cn type=\"integer\" id=\"Ch4.T1.13.13.3.m3.1.1.3.cmml\" xref=\"Ch4.T1.13.13.3.m3.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.13.13.3.m3.1c\">P_{1}</annotation></semantics></math>), then transfer to (<math id=\"Ch4.T1.14.14.4.m4.1\" class=\"ltx_Math\" alttext=\"N_{1}\" display=\"inline\"><semantics id=\"Ch4.T1.14.14.4.m4.1a\"><msub id=\"Ch4.T1.14.14.4.m4.1.1\" xref=\"Ch4.T1.14.14.4.m4.1.1.cmml\"><mi id=\"Ch4.T1.14.14.4.m4.1.1.2\" xref=\"Ch4.T1.14.14.4.m4.1.1.2.cmml\">N</mi><mn id=\"Ch4.T1.14.14.4.m4.1.1.3\" xref=\"Ch4.T1.14.14.4.m4.1.1.3.cmml\">1</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.14.14.4.m4.1b\"><apply id=\"Ch4.T1.14.14.4.m4.1.1.cmml\" xref=\"Ch4.T1.14.14.4.m4.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.14.14.4.m4.1.1.1.cmml\" xref=\"Ch4.T1.14.14.4.m4.1.1\">subscript</csymbol><ci id=\"Ch4.T1.14.14.4.m4.1.1.2.cmml\" xref=\"Ch4.T1.14.14.4.m4.1.1.2\">ğ‘</ci><cn type=\"integer\" id=\"Ch4.T1.14.14.4.m4.1.1.3.cmml\" xref=\"Ch4.T1.14.14.4.m4.1.1.3\">1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.14.14.4.m4.1c\">N_{1}</annotation></semantics></math> in <math id=\"Ch4.T1.15.15.5.m5.1\" class=\"ltx_Math\" alttext=\"P_{2}\" display=\"inline\"><semantics id=\"Ch4.T1.15.15.5.m5.1a\"><msub id=\"Ch4.T1.15.15.5.m5.1.1\" xref=\"Ch4.T1.15.15.5.m5.1.1.cmml\"><mi id=\"Ch4.T1.15.15.5.m5.1.1.2\" xref=\"Ch4.T1.15.15.5.m5.1.1.2.cmml\">P</mi><mn id=\"Ch4.T1.15.15.5.m5.1.1.3\" xref=\"Ch4.T1.15.15.5.m5.1.1.3.cmml\">2</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T1.15.15.5.m5.1b\"><apply id=\"Ch4.T1.15.15.5.m5.1.1.cmml\" xref=\"Ch4.T1.15.15.5.m5.1.1\"><csymbol cd=\"ambiguous\" id=\"Ch4.T1.15.15.5.m5.1.1.1.cmml\" xref=\"Ch4.T1.15.15.5.m5.1.1\">subscript</csymbol><ci id=\"Ch4.T1.15.15.5.m5.1.1.2.cmml\" xref=\"Ch4.T1.15.15.5.m5.1.1.2\">ğ‘ƒ</ci><cn type=\"integer\" id=\"Ch4.T1.15.15.5.m5.1.1.3.cmml\" xref=\"Ch4.T1.15.15.5.m5.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T1.15.15.5.m5.1c\">P_{2}</annotation></semantics></math>)</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In this thesis we design a new Federated Transfer Learning framework used for multimodal data with two or more modalities. We assume that some participants hold a part of data with multi-modality, and the others hold a part of data with uni-modality. The purpose of new designed framework is that participants have data with multi-modality help the others have data with uni-modality and protect their privacy.",
                "We have three possible approaches to combine Federated Learning ",
                "[",
                "37",
                "]",
                " and Transfer Learning ",
                "[",
                "10",
                "]",
                " methods (see Figures ",
                "4.1",
                "). We define a set of unimodal datasets ",
                "N",
                "=",
                "{",
                "n",
                "1",
                ",",
                "n",
                "2",
                ",",
                "â€¦",
                ",",
                "n",
                "m",
                "}",
                "ğ‘",
                "subscript",
                "ğ‘›",
                "1",
                "subscript",
                "ğ‘›",
                "2",
                "â€¦",
                "subscript",
                "ğ‘›",
                "ğ‘š",
                "N=\\{n_{1},n_{2},...,n_{m}\\}",
                ", where ",
                "m",
                "ğ‘š",
                "m",
                " is total number of modalities. The following gives us an overview of different combinations. To explain various combinations more concisely, we define ",
                "m",
                "ğ‘š",
                "m",
                " to 2. Besides, we define two participants, ",
                "P",
                "1",
                "subscript",
                "ğ‘ƒ",
                "1",
                "P_{1}",
                " and ",
                "P",
                "2",
                "subscript",
                "ğ‘ƒ",
                "2",
                "P_{2}",
                ", to clearly describe transferring between participants.",
                "The following three figures (Figure ",
                "4.1",
                ") illustrate the three combinations of Federated Learning and Transfer Learning.",
                "In all three combinations, models perform always grouped Federated Learning before the transferring process.\n",
                "Combination ",
                "\\@slowromancap",
                "i@.",
                " The first combination approach is that feature representations are directly transferred from modality ",
                "N",
                "1",
                "subscript",
                "ğ‘",
                "1",
                "N_{1}",
                " in ",
                "P",
                "1",
                "subscript",
                "ğ‘ƒ",
                "1",
                "P_{1}",
                " to modality ",
                "N",
                "2",
                "subscript",
                "ğ‘",
                "2",
                "N_{2}",
                " in ",
                "P",
                "2",
                "subscript",
                "ğ‘ƒ",
                "2",
                "P_{2}",
                " after grouped federation. The most representative approaches are to apply pre-trained models ",
                "[",
                "15",
                "]",
                " ",
                "[",
                "26",
                "]",
                " ",
                "[",
                "79",
                "]",
                ".",
                "Combination ",
                "\\@slowromancap",
                "ii@.",
                " The second combination approach is that feature representations are transferred ",
                "N",
                "2",
                "subscript",
                "ğ‘",
                "2",
                "N_{2}",
                " to ",
                "N",
                "1",
                "subscript",
                "ğ‘",
                "1",
                "N_{1}",
                " in ",
                "P",
                "1",
                "subscript",
                "ğ‘ƒ",
                "1",
                "P_{1}",
                ", then transfer ",
                "N",
                "1",
                "subscript",
                "ğ‘",
                "1",
                "N_{1}",
                " in ",
                "P",
                "2",
                "subscript",
                "ğ‘ƒ",
                "2",
                "P_{2}",
                ". The representative approach of transferring from ",
                "N",
                "2",
                "subscript",
                "ğ‘",
                "2",
                "N_{2}",
                " to ",
                "N",
                "1",
                "subscript",
                "ğ‘",
                "1",
                "N_{1}",
                " in ",
                "P",
                "1",
                "subscript",
                "ğ‘ƒ",
                "1",
                "P_{1}",
                " is MMTM ",
                "[",
                "33",
                "]",
                ", in which some connections are added between modalities.",
                "Combination ",
                "\\@slowromancap",
                "iii@.",
                " The third combination approach is that a fusion approach is used to connect all the modalities ",
                "N",
                "1",
                "subscript",
                "ğ‘",
                "1",
                "N_{1}",
                " and ",
                "N",
                "2",
                "subscript",
                "ğ‘",
                "2",
                "N_{2}",
                " in ",
                "P",
                "1",
                "subscript",
                "ğ‘ƒ",
                "1",
                "P_{1}",
                ", namely co-training sub-networks of their modalities, and then transfer ",
                "N",
                "1",
                "subscript",
                "ğ‘",
                "1",
                "N_{1}",
                " in ",
                "P",
                "2",
                "subscript",
                "ğ‘ƒ",
                "2",
                "P_{2}",
                ". The representative approaches of multimodal domain adaptation, including MDANN ",
                "[",
                "63",
                "]",
                ", MM-SADA ",
                "[",
                "49",
                "]",
                ", ADA ",
                "[",
                "46",
                "]",
                ", DLMM ",
                "[",
                "42",
                "]",
                ", PMC ",
                "[",
                "80",
                "]",
                ", and JADA ",
                "[",
                "43",
                "]",
                ", give us a hint to transfer after co-training sub-networks. However, multimodal domain adaptation approaches apply transferring between two multimodal datasets to solve domain shift problem. Domain adaptation approach in Federated Transfer Learning ",
                "[",
                "32",
                "]",
                " give us a direction that all participants can learn a common discriminative information to do better transferring. Besides domain adaptation, Contrastive Learning (Self-Supervised Learning) have the advantages of both co-training sub-networks and learning discriminative information. AVC ",
                "[",
                "2",
                "]",
                ", AVE-Net ",
                "[",
                "3",
                "]",
                ", CrossCLR ",
                "[",
                "83",
                "]",
                ", AVID ",
                "[",
                "52",
                "]",
                ", LWTNet ",
                "[",
                "1",
                "]",
                ", CBT ",
                "[",
                "66",
                "]",
                ", CSTNet ",
                "[",
                "36",
                "]",
                " give us support to learn fused feature representations and keep the ",
                "matching",
                " information at same time (see Table ",
                "3.4",
                ").",
                "We choose ",
                "Combination ",
                "\\@slowromancap",
                "iii@",
                ". The reasons are: If we use ",
                "Combination ",
                "\\@slowromancap",
                "i@",
                ", we can not guarantee that each participant holds a pre-trained model from other tasks. Moreover, FedMD ",
                "[",
                "45",
                "]",
                " and FTLKD ",
                "[",
                "75",
                "]",
                " are pre-trained over a public dataset in main sever and then distribute the pre-trained to local participants to their local data. We also can not guarantee that the main server holds a public dataset. If we choose ",
                "Combination ",
                "\\@slowromancap",
                "ii@",
                ", there are risks of negative transfer ",
                "[",
                "67",
                "]",
                ". Different modalities with different sub-networks lead to different feature representations and may lead to negative transferring ",
                "[",
                "67",
                "]",
                ". Besides, it is hard to transfer knowledge when the number of modalities is larger than two. If we choose ",
                "Combination ",
                "\\@slowromancap",
                "iii@",
                ", we can select Contrastive Learning to learn the ",
                "matching",
                " feature representations of each modality from multimodal data. Thus, the best choice is ",
                "Combination ",
                "\\@slowromancap",
                "iii@",
                " in all combinations."
            ]
        ]
    },
    "Ch5.T1": {
        "caption": "Table 5.1: Description of labels for scene classification [7].",
        "table": "<table id=\"Ch5.T1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch5.T1.2.1\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch5.T1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Labels for Scene Classification</span></td>\n<td id=\"Ch5.T1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch5.T1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Description</span></td>\n</tr>\n<tr id=\"Ch5.T1.2.2\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Beach</td>\n<td id=\"Ch5.T1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">2080 seconds, from 4 sources</td>\n</tr>\n<tr id=\"Ch5.T1.2.3\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">City</td>\n<td id=\"Ch5.T1.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">2432 seconds, from 5 sources</td>\n</tr>\n<tr id=\"Ch5.T1.2.4\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Classroom</td>\n<td id=\"Ch5.T1.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">2753 seconds, from 6 sources</td>\n</tr>\n<tr id=\"Ch5.T1.2.5\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.5.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Forest</td>\n<td id=\"Ch5.T1.2.5.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">2000 seconds, from 3 sources</td>\n</tr>\n<tr id=\"Ch5.T1.2.6\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.6.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Football Match</td>\n<td id=\"Ch5.T1.2.6.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">2300 seconds, from 4 sources</td>\n</tr>\n<tr id=\"Ch5.T1.2.7\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.7.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Grocery Store</td>\n<td id=\"Ch5.T1.2.7.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">2079 seconds, from 4 sources</td>\n</tr>\n<tr id=\"Ch5.T1.2.8\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.8.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Jungle</td>\n<td id=\"Ch5.T1.2.8.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">2000 seconds, from 3 sources</td>\n</tr>\n<tr id=\"Ch5.T1.2.9\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.9.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Restaurant</td>\n<td id=\"Ch5.T1.2.9.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">2300 seconds, from 8 sources</td>\n</tr>\n<tr id=\"Ch5.T1.2.10\" class=\"ltx_tr\">\n<td id=\"Ch5.T1.2.10.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">River</td>\n<td id=\"Ch5.T1.2.10.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">2500 seconds, from 8 sources</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Dataset.\nThe dataset consists of videos from 9 different environments. In total, 45 video sources have been processed, and they are at 29.97 frames per second and then reduced to 2000 seconds. Each frame (at 0.5s, 1.5s, 2.5s â€¦ and so on) of a video is extracted as a image, at the same time, each MFCC [48] audio statistic is also extracted from corresponding second of video. These 9 environmental classes are in the table 5.1. All the videos are captured in the nature environment. In order to make better distinction between different classes and obtain better recognition results, it is necessary to crop the image at each second to obtain the initial data objects. As it is shown in Figure 5.1, each frame of video and each piece of audio are extracted in pairs. Thus, 32,000 recognizable objects and 17,252 RGB images pairwise with 17,252 seconds of audio [7]."
        ]
    },
    "Ch5.T2": {
        "caption": "Table 5.2: Explanation of models to be used in different modalities in baseline models [7].",
        "table": "<table id=\"Ch5.T2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch5.T2.2.1\" class=\"ltx_tr\">\n<td id=\"Ch5.T2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch5.T2.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Names</span></td>\n<td id=\"Ch5.T2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch5.T2.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Explanations</span></td>\n</tr>\n<tr id=\"Ch5.T2.2.2\" class=\"ltx_tr\">\n<td id=\"Ch5.T2.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">VGG16</td>\n<td id=\"Ch5.T2.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">Visual Geometry Group <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib71\" title=\"\" class=\"ltx_ref\">71</a>]</cite>,a deep neural network model for image classification</td>\n</tr>\n<tr id=\"Ch5.T2.2.3\" class=\"ltx_tr\">\n<td id=\"Ch5.T2.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">MLP</td>\n<td id=\"Ch5.T2.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch5.T2.2.3.2.1\" class=\"ltx_text\"></span><span id=\"Ch5.T2.2.3.2.2\" class=\"ltx_text\">\n<span id=\"Ch5.T2.2.3.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch5.T2.2.3.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch5.T2.2.3.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">Multilayer Perceptron <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib30\" title=\"\" class=\"ltx_ref\">30</a>]</cite>, stacked several layers based on fully connected layers,</span></span>\n<span id=\"Ch5.T2.2.3.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch5.T2.2.3.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">can be used after VGG16.</span></span>\n</span></span><span id=\"Ch5.T2.2.3.2.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch5.T2.2.4\" class=\"ltx_tr\">\n<td id=\"Ch5.T2.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">DMLP</td>\n<td id=\"Ch5.T2.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">Deep Multilayer Perceptron, a deep neural network model for audio classification.</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We need to implement two parts: baseline models and the designed framework. \n",
                "Baseline models.",
                " There are three baseline models: Federated Learning for image, Federated Learning for audio, Federated Learning for image-audio. The centralized models of baseline models: VGG16+MLP for image, MFCC+DMLP for audio, and late fused VGG16+MLP and MFCC+DMLP for image-audio.\n",
                "Designed Framework.",
                " The centralized models of designed framework: VGG16+MLP for image, MFCC+DMLP for audio, and Contrastive Learning based model for image-audio. \n",
                "Following figures give us details about all the centralized models, including centralized baseline models and centralized models for new designed framework.\n",
                "Figure ",
                "5.4",
                " gives us details about VGG16+MLP. VGG16+MLP keeps the original VGG16 model, the input neurons of last layer is 4096, changes the last output neurons as 703, and follows 41 neurons. In MLP, we use Leaky ReLU as activation.\n",
                "Figure ",
                "5.5",
                " gives us details about DMLP. MFCC features are inputs. DMLP with hidden layers is ordered by 104, 977, 365, 703 and 41 neurons. Both VGG16+MLP and DMLP are introduced in ",
                "[",
                "7",
                "]",
                ". We add the relu activation function between the hidden layers of DMLP. \n",
                "Implementation of Baseline Models of Uni-modality.",
                " Following gives us details about baseline model of single modality, image or audio.",
                "Data Preprocessing.",
                " In our framework, we consider a video sample is ",
                "x",
                "ğ‘¥",
                "x",
                ", a RGB image frame and a audio sequence from this video are two different modalities (",
                "views",
                "): a RGB image frame is transformed by flipping, denoted as ",
                "x",
                "i",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "x_{i}",
                ", a audio sequence is extract by MFCC, denoted as ",
                "x",
                "a",
                "subscript",
                "ğ‘¥",
                "ğ‘",
                "x_{a}",
                ".",
                "Feature Representations ",
                "M",
                "i",
                "subscript",
                "ğ‘€",
                "ğ‘–",
                "M_{i}",
                " and ",
                "M",
                "a",
                "subscript",
                "ğ‘€",
                "ğ‘",
                "M_{a}",
                ".",
                " Deep neural networks are applied for feature representations. one sub-network VGG+MLP (",
                "M",
                "i",
                "subscript",
                "ğ‘€",
                "ğ‘–",
                "M_{i}",
                ") is applied for visual feature representation, and the other sub-network DMLP (",
                "M",
                "a",
                "subscript",
                "ğ‘€",
                "ğ‘",
                "M_{a}",
                ") is applied for auditory features. Here, the aim of feature representations are to enhance the performance of encoders, and align the represents to do late fusion. The outputs of feature representations are ",
                "r",
                "i",
                "=",
                "M",
                "â€‹",
                "(",
                "x",
                "i",
                ")",
                "subscript",
                "ğ‘Ÿ",
                "ğ‘–",
                "ğ‘€",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "r_{i}=M(x_{i})",
                " for ",
                "x",
                "i",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "x_{i}",
                " and ",
                "r",
                "a",
                "=",
                "M",
                "â€‹",
                "(",
                "x",
                "a",
                ")",
                "subscript",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘€",
                "subscript",
                "ğ‘¥",
                "ğ‘",
                "r_{a}=M(x_{a})",
                " for ",
                "x",
                "a",
                "subscript",
                "ğ‘¥",
                "ğ‘",
                "x_{a}",
                ".",
                "Output Layer ",
                "f",
                "f",
                "subscript",
                "ğ‘“",
                "ğ‘“",
                "f_{f}",
                ".",
                " We apply a fully connected layer ",
                "f",
                "f",
                "subscript",
                "ğ‘“",
                "ğ‘“",
                "f_{f}",
                " as the final output layer after concatenation. The predicted output is formed as ",
                "y",
                "~",
                "i",
                "=",
                "f",
                "f",
                "â€‹",
                "(",
                "r",
                "i",
                ")",
                "subscript",
                "~",
                "ğ‘¦",
                "ğ‘–",
                "subscript",
                "ğ‘“",
                "ğ‘“",
                "subscript",
                "ğ‘Ÿ",
                "ğ‘–",
                "\\widetilde{y}_{i}=f_{f}(r_{i})",
                " for image, and ",
                "y",
                "~",
                "a",
                "=",
                "f",
                "f",
                "â€‹",
                "(",
                "r",
                "a",
                ")",
                "subscript",
                "~",
                "ğ‘¦",
                "ğ‘",
                "subscript",
                "ğ‘“",
                "ğ‘“",
                "subscript",
                "ğ‘Ÿ",
                "ğ‘",
                "\\widetilde{y}_{a}=f_{f}(r_{a})",
                " for audio.",
                "Entropy Loss Function.",
                " The entropy loss function ",
                "[",
                "9",
                "]",
                " is make the late fusion two sub-networks perform Supervised Learning. Here, the loss function is to measure the distance the prediction and its labels.",
                "The output of ",
                "y",
                "~",
                "L",
                "â€‹",
                "F",
                "subscript",
                "~",
                "ğ‘¦",
                "ğ¿",
                "ğ¹",
                "\\widetilde{y}_{LF}",
                " is 9, because the dataset has 9 classes. We consider a dataset with augmented samples ",
                "X",
                "~",
                "~",
                "ğ‘‹",
                "\\widetilde{X}",
                " and labels ",
                "y",
                "âˆˆ",
                "Y",
                "ğ‘¦",
                "ğ‘Œ",
                "y\\in Y",
                ". Each sample ",
                "x",
                "ğ‘¥",
                "x",
                " has a positive pair ",
                "x",
                "~",
                "i",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘–",
                "\\widetilde{x}_{i}",
                " and ",
                "x",
                "~",
                "a",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘",
                "\\widetilde{x}_{a}",
                ". The entropy loss function is as follows.",
                "where ",
                "y",
                "k",
                "superscript",
                "ğ‘¦",
                "ğ‘˜",
                "{y^{k}}",
                " equals to 1 if the sample belongs to class ",
                "k",
                "ğ‘˜",
                "k",
                ", otherwise 0, and ",
                "y",
                "i",
                "k",
                "superscript",
                "subscript",
                "ğ‘¦",
                "ğ‘–",
                "ğ‘˜",
                "y_{i}^{k}",
                " or ",
                "y",
                "a",
                "k",
                "superscript",
                "subscript",
                "ğ‘¦",
                "ğ‘",
                "ğ‘˜",
                "y_{a}^{k}",
                " are the ",
                "k",
                "ğ‘˜",
                "k",
                "-th value of in ",
                "y",
                "i",
                "subscript",
                "ğ‘¦",
                "ğ‘–",
                "y_{i}",
                " or ",
                "y",
                "a",
                "subscript",
                "ğ‘¦",
                "ğ‘",
                "y_{a}",
                ".\n",
                "Implementation of Baseline Model of Multi-modality.",
                " Following gives us details about Contrastive Learning model for image-audio, which is similar in SimCLR ",
                "[",
                "13",
                "]",
                ".",
                "Figure ",
                "5.6",
                " gives us details about late fusion model for image-audio. The late fused model only concatenate two sub-networks together. Then, the concatenated outputs are fed into the last fully connect layer to get final outputs. Finally, the final outputs are used for classification.",
                "Data Preprocessing.",
                " In Our framework, we consider a video sample is ",
                "x",
                "ğ‘¥",
                "x",
                ", a RGB image frame and a audio sequence from this video are two different modalities (",
                "views",
                "): a RGB image frame is transformed by flipping, denoted as ",
                "x",
                "i",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "x_{i}",
                ", a audio sequence is extract by MFCC, denoted as ",
                "x",
                "a",
                "subscript",
                "ğ‘¥",
                "ğ‘",
                "x_{a}",
                ".",
                "Feature Representations ",
                "M",
                "i",
                "subscript",
                "ğ‘€",
                "ğ‘–",
                "M_{i}",
                " and ",
                "M",
                "a",
                "subscript",
                "ğ‘€",
                "ğ‘",
                "M_{a}",
                ".",
                " Deep neural networks are applied for feature representations. one sub-network VGG+MLP (",
                "M",
                "i",
                "subscript",
                "ğ‘€",
                "ğ‘–",
                "M_{i}",
                ") is applied for visual feature representation, and the other sub-network DMLP (",
                "M",
                "a",
                "subscript",
                "ğ‘€",
                "ğ‘",
                "M_{a}",
                ") is applied for auditory features. Here, the aim of feature representations are to enhance the performance of encoders, and align the represents to do late fusion. The outputs of project head are ",
                "r",
                "i",
                "=",
                "M",
                "â€‹",
                "(",
                "x",
                "i",
                ")",
                "subscript",
                "ğ‘Ÿ",
                "ğ‘–",
                "ğ‘€",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "r_{i}=M(x_{i})",
                " for ",
                "x",
                "i",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "x_{i}",
                " and ",
                "r",
                "a",
                "=",
                "M",
                "â€‹",
                "(",
                "x",
                "a",
                ")",
                "subscript",
                "ğ‘Ÿ",
                "ğ‘",
                "ğ‘€",
                "subscript",
                "ğ‘¥",
                "ğ‘",
                "r_{a}=M(x_{a})",
                " for ",
                "x",
                "a",
                "subscript",
                "ğ‘¥",
                "ğ‘",
                "x_{a}",
                ".",
                "Late Fusion ",
                "c",
                "ğ‘",
                "c",
                " and Output Layer ",
                "f",
                "f",
                "subscript",
                "ğ‘“",
                "ğ‘“",
                "f_{f}",
                ".",
                " We concatenate the outputs from ",
                "M",
                "i",
                "subscript",
                "ğ‘€",
                "ğ‘–",
                "M_{i}",
                " and ",
                "M",
                "a",
                "subscript",
                "ğ‘€",
                "ğ‘",
                "M_{a}",
                ". The late fusion result is ",
                "c",
                "L",
                "â€‹",
                "F",
                "=",
                "c",
                "â€‹",
                "(",
                "r",
                "i",
                ",",
                "r",
                "a",
                ")",
                "subscript",
                "ğ‘",
                "ğ¿",
                "ğ¹",
                "ğ‘",
                "subscript",
                "ğ‘Ÿ",
                "ğ‘–",
                "subscript",
                "ğ‘Ÿ",
                "ğ‘",
                "c_{LF}=c(r_{i},r_{a})",
                ". Then, we apply a fully connected layer ",
                "f",
                "f",
                "subscript",
                "ğ‘“",
                "ğ‘“",
                "f_{f}",
                " as the final output layer after concatenation. The predicted output is formed as ",
                "y",
                "~",
                "L",
                "â€‹",
                "F",
                "=",
                "f",
                "f",
                "â€‹",
                "(",
                "c",
                "L",
                "â€‹",
                "F",
                ")",
                "subscript",
                "~",
                "ğ‘¦",
                "ğ¿",
                "ğ¹",
                "subscript",
                "ğ‘“",
                "ğ‘“",
                "subscript",
                "ğ‘",
                "ğ¿",
                "ğ¹",
                "\\widetilde{y}_{LF}=f_{f}(c_{LF})",
                "Entropy Loss Function.",
                " The entropy loss function is make the late fusion two sub-networks perform Supervised Learning. Here, the loss function is to measure the distance the prediction and its labels. The output of ",
                "y",
                "~",
                "L",
                "â€‹",
                "F",
                "subscript",
                "~",
                "ğ‘¦",
                "ğ¿",
                "ğ¹",
                "\\widetilde{y}_{LF}",
                " is 9, because the dataset has 9 classes. We consider a dataset with augmented samples ",
                "X",
                "~",
                "~",
                "ğ‘‹",
                "\\widetilde{X}",
                " and labels/classes ",
                "y",
                "âˆˆ",
                "Y",
                "ğ‘¦",
                "ğ‘Œ",
                "y\\in Y",
                ", the entropy loss function is as follows.",
                "where ",
                "y",
                "k",
                "superscript",
                "ğ‘¦",
                "ğ‘˜",
                "{y^{k}}",
                " equals to 1 if the sample belongs to class ",
                "k",
                "ğ‘˜",
                "k",
                ", otherwise 0, and ",
                "y",
                "~",
                "L",
                "â€‹",
                "F",
                "k",
                "superscript",
                "subscript",
                "~",
                "ğ‘¦",
                "ğ¿",
                "ğ¹",
                "ğ‘˜",
                "\\widetilde{y}_{LF}^{k}",
                " is the ",
                "k",
                "ğ‘˜",
                "k",
                "-th value of in ",
                "y",
                "L",
                "â€‹",
                "F",
                "subscript",
                "ğ‘¦",
                "ğ¿",
                "ğ¹",
                "y_{LF}",
                ".",
                "Implementation of Contrastive Learning.",
                " Following gives us details about centralized model of our framework for multi-modality, which is applied from ",
                "[",
                "7",
                "]",
                ". Baseline model with multi-modality has similar neural architecture. \n",
                "Figure ",
                "5.7",
                " gives us details about Contrastive Learning for image-audio. The late fusion model only concatenate two sub-networks together. Then, the concatenated outputs are used to compute similarity. Finally, the final outputs are used for calculating contrastive loss.",
                "Data Preprocessing.",
                " In Our framework, we consider a video sample is ",
                "x",
                "ğ‘¥",
                "x",
                ", a RGB image frame and a audio sequence from this video are two different modalities (",
                "views",
                "): a RGB image frame is transformed by flipping, denoted as ",
                "x",
                "i",
                "subscript",
                "ğ‘¥",
                "ğ‘–",
                "x_{i}",
                ", a audio sequence is extract by MFCC, denoted as ",
                "x",
                "a",
                "subscript",
                "ğ‘¥",
                "ğ‘",
                "x_{a}",
                ".",
                "Encoders ",
                "f",
                "i",
                "subscript",
                "ğ‘“",
                "ğ‘–",
                "f_{i}",
                " and ",
                "f",
                "a",
                "subscript",
                "ğ‘“",
                "ğ‘",
                "f_{a}",
                ".",
                " Encoder (stacked convolutional layers) is the main components in Contrastive Learning. Encoders can be various neural networks. Here, we apply VGG16 for visual modality and the first three layers of DMLP for auditory modality. The representation are ",
                "h",
                "i",
                "=",
                "f",
                "i",
                "â€‹",
                "(",
                "x",
                "~",
                "i",
                ")",
                "subscript",
                "â„",
                "ğ‘–",
                "subscript",
                "ğ‘“",
                "ğ‘–",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘–",
                "h_{i}=f_{i}(\\widetilde{x}_{i})",
                " for ",
                "x",
                "~",
                "i",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘–",
                "\\widetilde{x}_{i}",
                " and ",
                "h",
                "a",
                "=",
                "f",
                "a",
                "â€‹",
                "(",
                "x",
                "~",
                "a",
                ")",
                "subscript",
                "â„",
                "ğ‘",
                "subscript",
                "ğ‘“",
                "ğ‘",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘",
                "h_{a}=f_{a}(\\widetilde{x}_{a})",
                " for ",
                "x",
                "~",
                "a",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘",
                "\\widetilde{x}_{a}",
                ".",
                "Project Head ",
                "p",
                "ğ‘",
                "p",
                ".",
                " Projection head ",
                "p",
                "ğ‘",
                "p",
                " is a shallow neural network, we apply a projection function with several two fully connected layers, projects the representations of two modalities from encoders to hidden space. The aim of project head is to enhance the performance of encoders, and align the represents with a same hidden space shape. The outputs of project head are ",
                "z",
                "i",
                "=",
                "p",
                "â€‹",
                "(",
                "h",
                "i",
                ")",
                "subscript",
                "ğ‘§",
                "ğ‘–",
                "ğ‘",
                "subscript",
                "â„",
                "ğ‘–",
                "z_{i}=p(h_{i})",
                " for ",
                "h",
                "i",
                "subscript",
                "â„",
                "ğ‘–",
                "h_{i}",
                " and ",
                "z",
                "a",
                "=",
                "p",
                "â€‹",
                "(",
                "h",
                "a",
                ")",
                "subscript",
                "ğ‘§",
                "ğ‘",
                "ğ‘",
                "subscript",
                "â„",
                "ğ‘",
                "z_{a}=p(h_{a})",
                " for ",
                "h",
                "a",
                "subscript",
                "â„",
                "ğ‘",
                "h_{a}",
                ".",
                "Contrastive Loss Function.",
                " The contrastive loss function ",
                "[",
                "25",
                "]",
                " is make the encoders to learn the feature representations by themselves. There is dataset with augmented samples ",
                "X",
                "~",
                "=",
                "{",
                "x",
                "~",
                "1",
                ",",
                "x",
                "~",
                "2",
                ",",
                "â€¦",
                ",",
                "x",
                "~",
                "k",
                "}",
                "~",
                "ğ‘‹",
                "subscript",
                "~",
                "ğ‘¥",
                "1",
                "subscript",
                "~",
                "ğ‘¥",
                "2",
                "â€¦",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘˜",
                "\\widetilde{X}=\\{\\widetilde{x}_{1},\\widetilde{x}_{2},...,\\widetilde{x}_{k}\\}",
                ". Each sample ",
                "x",
                "ğ‘¥",
                "x",
                " has a positive pair ",
                "x",
                "~",
                "i",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘–",
                "\\widetilde{x}_{i}",
                " and ",
                "x",
                "~",
                "a",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘",
                "\\widetilde{x}_{a}",
                ". The contrastive loss is to maximize the similarity between ",
                "x",
                "~",
                "i",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘–",
                "\\widetilde{x}_{i}",
                " and ",
                "x",
                "~",
                "a",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘",
                "\\widetilde{x}_{a}",
                ", and minimize the positive pair (",
                "x",
                "~",
                "i",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘–",
                "\\widetilde{x}_{i}",
                ", ",
                "x",
                "~",
                "a",
                "subscript",
                "~",
                "ğ‘¥",
                "ğ‘",
                "\\widetilde{x}_{a}",
                ") and other samples. With a batch of ",
                "B",
                "ğµ",
                "B",
                " samples, we have ",
                "2",
                "â€‹",
                "B",
                "2",
                "ğµ",
                "2B",
                " augmented samples. The contrastive loss function is as follows.",
                "we use cosine similarity ",
                "[",
                "9",
                "]",
                "At last, the final loss is calculated over all the positive pairs. In a batch with ",
                "B",
                "ğµ",
                "B",
                " samples, ",
                "L",
                "C",
                "â€‹",
                "L",
                "subscript",
                "L",
                "ğ¶",
                "ğ¿",
                "\\text{L}_{CL}",
                " is computed as:",
                "where ",
                "k",
                "ğ‘˜",
                "k",
                " is the index of samples, ",
                "(",
                "2",
                "â€‹",
                "k",
                "âˆ’",
                "1",
                ",",
                "2",
                "â€‹",
                "k",
                ")",
                "2",
                "ğ‘˜",
                "1",
                "2",
                "ğ‘˜",
                "(2k-1,2k)",
                " and ",
                "(",
                "2",
                "â€‹",
                "k",
                ",",
                "2",
                "â€‹",
                "k",
                "âˆ’",
                "1",
                ")",
                "2",
                "ğ‘˜",
                "2",
                "ğ‘˜",
                "1",
                "(2k,2k-1)",
                " are the indices of each positive pair.",
                "We build five comparable experiments. In all experiments, we set hyper-parameters: 100 epochs, 0.001 as learning rate, 10 as batch size, Stochastic Gradient Descent (SGD) as optimizer. Besides, hyperparameters of all federated Learning: 30 participants, 10 as local epochs, 10 as local batch size. The data distribution of each participant and the whole dataset have the same distribution, and sampling follows random sampling without replacement.",
                "We use one single GPU (on Colab) with python (version 3.7.3) and PyTorch ",
                "[",
                "62",
                "]",
                " (version 1.7.1). The training process of Federated Learning with only audio needs 10 minutes. The training process of Federated Learning with only images needs about 2 hours. However, our framework and late fusion model need about 7 hours. \n",
                "Code sources are as follows:",
                "It is an implement example of Federated Learning without socket communication with PyTorch.",
                "https://github.com/AshwinRJ/Federated-Learning-PyTorch\n",
                "It is an tutorial which uses Contrastive Learning to learn contrastive representations for the downstream task of music classification. The details are similar in SimCLR ",
                "[",
                "13",
                "]",
                ".",
                "https://music-classification.github.io/tutorial/part5_beyond/self-supervised-learning.html",
                "Because the weight of each sub-network is loaded as the data structure form of dictionary. So we aggregate the weights by keys in dictionaries. Following algorithm (see Algorithm ",
                "1",
                ") shows the aggregation process."
            ]
        ]
    },
    "Ch6.T1": {
        "caption": "Table 6.1: Average performance is about uni-modality and multi-modality without federated learning. The training process begins from scratch.",
        "table": "<table id=\"Ch6.T1.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch6.T1.2.1\" class=\"ltx_tr\">\n<td id=\"Ch6.T1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch6.T1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Modality</span></td>\n<td id=\"Ch6.T1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"Ch6.T1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Average Testing Accuracy</span></td>\n<td id=\"Ch6.T1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T1.2.1.3.1\" class=\"ltx_text\"></span> <span id=\"Ch6.T1.2.1.3.2\" class=\"ltx_text\">\n<span id=\"Ch6.T1.2.1.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T1.2.1.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T1.2.1.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T1.2.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Testing Accuracy</span></span></span>\n<span id=\"Ch6.T1.2.1.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T1.2.1.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T1.2.1.3.2.1.2.1.1\" class=\"ltx_text ltx_font_bold\">(Baselines <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib7\" title=\"\" class=\"ltx_ref\">7</a>]</cite>)</span></span></span>\n</span></span><span id=\"Ch6.T1.2.1.3.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch6.T1.2.2\" class=\"ltx_tr\">\n<td id=\"Ch6.T1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T1.2.2.1.1\" class=\"ltx_text\"></span><span id=\"Ch6.T1.2.2.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T1.2.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T1.2.2.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T1.2.2.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T1.2.2.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Image (Visual)</span></span></span>\n<span id=\"Ch6.T1.2.2.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T1.2.2.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">VGG16+MLP</span></span>\n</span></span><span id=\"Ch6.T1.2.2.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T1.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">96.31%</td>\n<td id=\"Ch6.T1.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">89.27%</td>\n</tr>\n<tr id=\"Ch6.T1.2.3\" class=\"ltx_tr\">\n<td id=\"Ch6.T1.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T1.2.3.1.1\" class=\"ltx_text\"></span><span id=\"Ch6.T1.2.3.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T1.2.3.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T1.2.3.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T1.2.3.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T1.2.3.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Audio (Auditory)</span></span></span>\n<span id=\"Ch6.T1.2.3.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T1.2.3.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">MFCC+DMLP</span></span>\n</span></span><span id=\"Ch6.T1.2.3.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T1.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">92.99%</td>\n<td id=\"Ch6.T1.2.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">93.72%</td>\n</tr>\n<tr id=\"Ch6.T1.2.4\" class=\"ltx_tr\">\n<td id=\"Ch6.T1.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T1.2.4.1.1\" class=\"ltx_text\"></span><span id=\"Ch6.T1.2.4.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T1.2.4.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T1.2.4.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T1.2.4.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T1.2.4.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Image-Audio (Multimodality)</span></span></span>\n<span id=\"Ch6.T1.2.4.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T1.2.4.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">VGG16+MLP and MFCC+DMLP</span></span>\n</span></span><span id=\"Ch6.T1.2.4.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T1.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">97.55%</td>\n<td id=\"Ch6.T1.2.4.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">96.81%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We implement baseline models in [7], VGG16+MLP for image, and MFCC+DMLP for audio and VGG16+MLP and MFCC+DMLP for image-audio are trained from scratch.\nThe accuracy is about 7% higher than the baseline in image modality. The implementation of auditory brings about 1% lower accuracy than the baseline. The accuracy of image-audio multi-modality is close to the baseline (Table 6.1)."
        ]
    },
    "Ch6.T2": {
        "caption": "Table 6.2: Average performance of un-modality and multi-modality with Federated Learning.",
        "table": "<table id=\"Ch6.T2.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch6.T2.2.1\" class=\"ltx_tr\">\n<td id=\"Ch6.T2.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch6.T2.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Modality</span></td>\n<td id=\"Ch6.T2.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T2.2.1.2.1\" class=\"ltx_text\"></span> <span id=\"Ch6.T2.2.1.2.2\" class=\"ltx_text\">\n<span id=\"Ch6.T2.2.1.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T2.2.1.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.1.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T2.2.1.2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Average Testing Accuracy</span></span></span>\n<span id=\"Ch6.T2.2.1.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.1.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(Implemented Baselines</span></span>\n<span id=\"Ch6.T2.2.1.2.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.1.2.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T2.2.1.2.2.1.3.1.1\" class=\"ltx_text ltx_font_bold\">with</span> Federated Learning)</span></span>\n</span></span><span id=\"Ch6.T2.2.1.2.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T2.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T2.2.1.3.1\" class=\"ltx_text\"></span> <span id=\"Ch6.T2.2.1.3.2\" class=\"ltx_text\">\n<span id=\"Ch6.T2.2.1.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T2.2.1.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.1.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T2.2.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Average Testing Accuracy</span></span></span>\n<span id=\"Ch6.T2.2.1.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.1.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(Implementd Baselines</span></span>\n<span id=\"Ch6.T2.2.1.3.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.1.3.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T2.2.1.3.2.1.3.1.1\" class=\"ltx_text ltx_font_bold\">without</span> Federated Learning)</span></span>\n</span></span><span id=\"Ch6.T2.2.1.3.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch6.T2.2.2\" class=\"ltx_tr\">\n<td id=\"Ch6.T2.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T2.2.2.1.1\" class=\"ltx_text\"></span> <span id=\"Ch6.T2.2.2.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T2.2.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T2.2.2.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.2.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T2.2.2.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Image (Visual)</span></span></span>\n<span id=\"Ch6.T2.2.2.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.2.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">VGG16+MLP</span></span>\n</span></span><span id=\"Ch6.T2.2.2.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T2.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">93.68%</td>\n<td id=\"Ch6.T2.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">96.31%</td>\n</tr>\n<tr id=\"Ch6.T2.2.3\" class=\"ltx_tr\">\n<td id=\"Ch6.T2.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T2.2.3.1.1\" class=\"ltx_text\"></span> <span id=\"Ch6.T2.2.3.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T2.2.3.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T2.2.3.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.3.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T2.2.3.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Audio (Auditory)</span></span></span>\n<span id=\"Ch6.T2.2.3.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.3.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">MFCC+DMLP</span></span>\n</span></span><span id=\"Ch6.T2.2.3.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T2.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">88.16%</td>\n<td id=\"Ch6.T2.2.3.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">92.99%</td>\n</tr>\n<tr id=\"Ch6.T2.2.4\" class=\"ltx_tr\">\n<td id=\"Ch6.T2.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T2.2.4.1.1\" class=\"ltx_text\"></span> <span id=\"Ch6.T2.2.4.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T2.2.4.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T2.2.4.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.4.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T2.2.4.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Image-Audio (Multimodality)</span></span></span>\n<span id=\"Ch6.T2.2.4.1.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T2.2.4.1.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">VGG16+MLP and MFCC+DMLP</span></span>\n</span></span><span id=\"Ch6.T2.2.4.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T2.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">96.41%</td>\n<td id=\"Ch6.T2.2.4.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">97.55%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The accuracies of all the implemented baseline models with Federated Learning are lower than those without Federated Learning.\nMore specifically, the accuracies of baseline models in Federated Learning are about 3.5% lower than those without Federated Learning in two uni-modalities. In addition, the accuracy of the implemented baseline model of multi-modality with Federated Learning is about 1% lower than the same central model with no Federated Learning (Table 6.2)."
        ]
    },
    "Ch6.T3": {
        "caption": "Table 6.3: Average accuracy of our Framework.",
        "table": "<table id=\"Ch6.T3.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch6.T3.2.1\" class=\"ltx_tr\">\n<td id=\"Ch6.T3.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch6.T3.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Modality</span></td>\n<td id=\"Ch6.T3.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T3.2.1.2.1\" class=\"ltx_text\"></span> <span id=\"Ch6.T3.2.1.2.2\" class=\"ltx_text\">\n<span id=\"Ch6.T3.2.1.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T3.2.1.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T3.2.1.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T3.2.1.2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Testing Accuracy</span></span></span>\n<span id=\"Ch6.T3.2.1.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T3.2.1.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Our Framework</span></span>\n</span></span><span id=\"Ch6.T3.2.1.2.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T3.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T3.2.1.3.1\" class=\"ltx_text\"></span><span id=\"Ch6.T3.2.1.3.2\" class=\"ltx_text\">\n<span id=\"Ch6.T3.2.1.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T3.2.1.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T3.2.1.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T3.2.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Testing Accuracy</span></span></span>\n<span id=\"Ch6.T3.2.1.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T3.2.1.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(Implemented Baselines Models</span></span>\n<span id=\"Ch6.T3.2.1.3.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch6.T3.2.1.3.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T3.2.1.3.2.1.3.1.1\" class=\"ltx_text ltx_font_bold\">with</span> Federated Learning)</span></span>\n</span></span><span id=\"Ch6.T3.2.1.3.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch6.T3.2.2\" class=\"ltx_tr\">\n<td id=\"Ch6.T3.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T3.2.2.1.1\" class=\"ltx_text\"></span><span id=\"Ch6.T3.2.2.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T3.2.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T3.2.2.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T3.2.2.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T3.2.2.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Image (Visual)</span></span></span>\n</span></span><span id=\"Ch6.T3.2.2.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T3.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">93.91%</td>\n<td id=\"Ch6.T3.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">93.68%</td>\n</tr>\n<tr id=\"Ch6.T3.2.3\" class=\"ltx_tr\">\n<td id=\"Ch6.T3.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T3.2.3.1.1\" class=\"ltx_text\"></span><span id=\"Ch6.T3.2.3.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T3.2.3.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T3.2.3.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T3.2.3.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T3.2.3.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Audio (Auditory)</span></span></span>\n</span></span><span id=\"Ch6.T3.2.3.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T3.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">92.87%</td>\n<td id=\"Ch6.T3.2.3.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">88.16%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The accuracies of our framework (Federated Transfer Learning) are higher than the implemented baseline models with Federated Learning.\nMore specifically, the accuracy of image modality is about 0.5% higher, and that of audio modality is about 5% improved (Table 6.3)."
        ]
    },
    "Ch6.T4": {
        "caption": "Table 6.4: Average accuracy of our framework.",
        "table": "<table id=\"Ch6.T4.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch6.T4.2.1\" class=\"ltx_tr\">\n<td id=\"Ch6.T4.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch6.T4.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Modality</span></td>\n<td id=\"Ch6.T4.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T4.2.1.2.1\" class=\"ltx_text\"></span> <span id=\"Ch6.T4.2.1.2.2\" class=\"ltx_text\">\n<span id=\"Ch6.T4.2.1.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T4.2.1.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T4.2.1.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T4.2.1.2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Average Testing Accuracy</span></span></span>\n<span id=\"Ch6.T4.2.1.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T4.2.1.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Our Framework</span></span>\n</span></span><span id=\"Ch6.T4.2.1.2.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T4.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T4.2.1.3.1\" class=\"ltx_text\"></span><span id=\"Ch6.T4.2.1.3.2\" class=\"ltx_text\">\n<span id=\"Ch6.T4.2.1.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T4.2.1.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T4.2.1.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T4.2.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Average Testing Accuracy</span></span></span>\n<span id=\"Ch6.T4.2.1.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T4.2.1.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(Implemented Baselines Models</span></span>\n<span id=\"Ch6.T4.2.1.3.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch6.T4.2.1.3.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T4.2.1.3.2.1.3.1.1\" class=\"ltx_text ltx_font_bold\">with</span> Federated Learning)</span></span>\n</span></span><span id=\"Ch6.T4.2.1.3.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch6.T4.2.2\" class=\"ltx_tr\">\n<td id=\"Ch6.T4.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T4.2.2.1.1\" class=\"ltx_text\"></span><span id=\"Ch6.T4.2.2.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T4.2.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T4.2.2.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T4.2.2.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T4.2.2.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Image (Visual)</span></span></span>\n</span></span><span id=\"Ch6.T4.2.2.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T4.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">94.42%</td>\n<td id=\"Ch6.T4.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">93.68%</td>\n</tr>\n<tr id=\"Ch6.T4.2.3\" class=\"ltx_tr\">\n<td id=\"Ch6.T4.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T4.2.3.1.1\" class=\"ltx_text\"></span><span id=\"Ch6.T4.2.3.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T4.2.3.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T4.2.3.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T4.2.3.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T4.2.3.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Audio (Auditory)</span></span></span>\n</span></span><span id=\"Ch6.T4.2.3.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T4.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">91.77%</td>\n<td id=\"Ch6.T4.2.3.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">88.16%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The accuracies of our framework (Federated Transfer Learning) are higher than the implemented baseline models in Federated Learning.\nMore specifically, the accuracy of image modality is about 1% higher, and that of audio modality is about 4% improved (Table 6.4)."
        ]
    },
    "Ch6.T5": {
        "caption": "Table 6.5: Average accuracy of our framework.",
        "table": "<table id=\"Ch6.T5.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"Ch6.T5.2.1\" class=\"ltx_tr\">\n<td id=\"Ch6.T5.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"Ch6.T5.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Modality</span></td>\n<td id=\"Ch6.T5.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T5.2.1.2.1\" class=\"ltx_text\"></span> <span id=\"Ch6.T5.2.1.2.2\" class=\"ltx_text\">\n<span id=\"Ch6.T5.2.1.2.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T5.2.1.2.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T5.2.1.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><span id=\"Ch6.T5.2.1.2.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Average Testing Accuracy</span></span></span>\n<span id=\"Ch6.T5.2.1.2.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T5.2.1.2.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Our Framework</span></span>\n</span></span><span id=\"Ch6.T5.2.1.2.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T5.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T5.2.1.3.1\" class=\"ltx_text\"></span><span id=\"Ch6.T5.2.1.3.2\" class=\"ltx_text\">\n<span id=\"Ch6.T5.2.1.3.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T5.2.1.3.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T5.2.1.3.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T5.2.1.3.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Average Testing Accuracy</span></span></span>\n<span id=\"Ch6.T5.2.1.3.2.1.2\" class=\"ltx_tr\">\n<span id=\"Ch6.T5.2.1.3.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\">(Implemented Baselines Models</span></span>\n<span id=\"Ch6.T5.2.1.3.2.1.3\" class=\"ltx_tr\">\n<span id=\"Ch6.T5.2.1.3.2.1.3.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T5.2.1.3.2.1.3.1.1\" class=\"ltx_text ltx_font_bold\">with</span> Federated Learning).</span></span>\n</span></span><span id=\"Ch6.T5.2.1.3.3\" class=\"ltx_text\"></span>\n</td>\n</tr>\n<tr id=\"Ch6.T5.2.2\" class=\"ltx_tr\">\n<td id=\"Ch6.T5.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T5.2.2.1.1\" class=\"ltx_text\"></span><span id=\"Ch6.T5.2.2.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T5.2.2.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T5.2.2.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T5.2.2.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T5.2.2.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Image (Visual)</span></span></span>\n</span></span><span id=\"Ch6.T5.2.2.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T5.2.2.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">93.91%</td>\n<td id=\"Ch6.T5.2.2.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">93.68%</td>\n</tr>\n<tr id=\"Ch6.T5.2.3\" class=\"ltx_tr\">\n<td id=\"Ch6.T5.2.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">\n<span id=\"Ch6.T5.2.3.1.1\" class=\"ltx_text\"></span><span id=\"Ch6.T5.2.3.1.2\" class=\"ltx_text\">\n<span id=\"Ch6.T5.2.3.1.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"Ch6.T5.2.3.1.2.1.1\" class=\"ltx_tr\">\n<span id=\"Ch6.T5.2.3.1.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_left\"><span id=\"Ch6.T5.2.3.1.2.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Audio (Auditory)</span></span></span>\n</span></span><span id=\"Ch6.T5.2.3.1.3\" class=\"ltx_text\"></span>\n</td>\n<td id=\"Ch6.T5.2.3.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">92.64%</td>\n<td id=\"Ch6.T5.2.3.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t\">88.16%</td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "The accuracies of our framework (Federated Transfer Learning) are higher than the implemented baseline models in Federated Learning.\nMore specifically, the accuracy of image modality is about 0.5% higher, and that of audio modality is about 4.5% improved (Table 6.5)."
        ]
    }
}