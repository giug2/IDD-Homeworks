{
    "id_table_1": {
        "caption": "Full fine-tuning vs LoRA benchmark",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T1.1.1.1\">\n       Training Configuration\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T1.1.1.2\">\n       Benchmark Accuracy\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.1.2.1\">\n       Full model training\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.2.2\">\n       98.1%\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.1.3.1\">\n       LoRA, rank = 64, lora_alpha = 256, target_modules\n       <span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T1.1.3.1.1\">\n        \"all-linear\"\n       </span>\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.3.2\">\n       85.1%\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T1.1.4.1\">\n       LoRA, rank = 16, lora_alpha = 32, target_modules\n       <span class=\"ltx_text ltx_font_typewriter\" id=\"S5.T1.1.4.1.1\">\n        \"all-linear\"\n       </span>\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.1.4.2\">\n       72.9%\n      </td>\n     \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "Table                  1                presents a detailed comparison of full fine-tuning and LoRA approaches for our planning model. Our experiments reveal significant differences in performance across these methods. Full fine-tuning achieves the highest accuracy at 98.1%, demonstrating superior performance. In contrast, LoRA performance depends on rank size. With rank 64 and alpha 256, LoRA achieves 85.1% accuracy, while reducing to rank 16 and alpha 32 drops accuracy to 72.9%.These results highlight the trade-off between model performance and computational efficiency when using LoRA. While full fine-tuning provides better accuracy, LoRA offers a more resource-efficient alternative, with performance varying based on rank configuration."
        ]
    },
    "id_table_2": {
        "caption": "Multi-LoRA Benchmark",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T2.1.1.1\">\n       Training Configuration\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.1.1.2\">\n       Benchmark Accuracy (%)\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.2.1\">\n       LoRA for Android\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.2.2\">\n       85.1\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.3.1\">\n       Merged for Android, E-Commerce\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.3.2\">\n       82.2\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.4.1\">\n       Merged for Android, E-Commerce, video streaming\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2\">\n       78.9\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.5.1\">\n       Merged for Android, E-Commerce, video streaming, travelling\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.5.2\">\n       69.7\n      </td>\n     \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "Table                  2                presents the performance results of our multi-LoRA merging technique. Each individual LoRA was trained with consistent hyperparameters: rank 64, lora_alpha 256, and target_modules set to \"all-linear\".The single-domain Android function set LoRA achieves 85.1% accuracy. When merging LoRAs from two domains (Android and E-Commerce), accuracy slightly decreases to 82.2%. Further merging yields lower accuracies: 78.9% for three domains (adding Video Streaming), and 69.7% for four domains (adding Travel). These results reveal a pattern of gradual accuracy decline as we integrate more function sets, with a steeper drop occurring after the third domain is added."
        ]
    },
    "id_table_3": {
        "caption": "Different base model benchmark",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.1.1.1\">\n       Base model\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.1.1.2\">\n       Benchmark Accuracy\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.2.1\">\n       Google Gemma 2b\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.2\">\n       85.6%\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.3.1\">\n       Google Gemma 7b\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.3.2\">\n       99.7%\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T3.1.4.1\">\n       Microsoft Phi-3 Mini\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.4.2\">\n       98.1%\n      </td>\n     \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "Table                  3                presents the benchmark accuracy for different base models after full fine-tuning. Google Gemma 2b achieved 85.6% accuracy, while the larger Gemma 7b excelled with 99.7%. Microsoft Phi-3 also performed strongly at 98.1%. These results indicate that our framework adapts well to various on-device LLMs, with larger models generally achieving higher accuracy."
        ]
    },
    "id_table_4": {
        "caption": "Different training dataset size benchmark",
        "table": [
            [
                "<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.1\">\n       Training Dataset Size\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.1.1.2\">\n       Benchmark Accuracy\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.1\">\n       1000\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.2\">\n       98.1%\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.3.1\">\n       500\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.3.2\">\n       92.5%\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.1\">\n       250\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.1.4.2\">\n       85.3 %\n      </td>\n     \n"
            ],
            [
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.5.1\">\n       100\n      </td>\n      \n",
                "<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.1.5.2\">\n       78.1%\n      </td>\n     \n"
            ]
        ],
        "footnotes": [],
        "references": [
            "Our default training dataset contains 1000 data points, evenly distributed across 1-5 step sequences (200 each) to represent varying task complexities. We investigated the impact of dataset size on model performance to optimize function set integration efficiency and address synthetic data generation costs. Table                  4                shows the benchmark accuracy for various training dataset sizes:"
        ]
    }
}