{
    "PAPER'S NUMBER OF TABLES": 1,
    "S2.T1": {
        "caption": "TABLE I: Key differences between VFL and HFL",
        "table": "<table id=\"S2.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Framework</th>\n<th id=\"S2.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Data characteristics of participants</th>\n<th id=\"S2.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Exchanged messages</th>\n<th id=\"S2.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Model structures</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\">VFL</td>\n<td id=\"S2.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Same sample space, but different attribute space</td>\n<td id=\"S2.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Intermediate outputs and its gradients</td>\n<td id=\"S2.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\">Flexible and local secret</td>\n</tr>\n<tr id=\"S2.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">HFL</td>\n<td id=\"S2.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Different sample space, but same attribute space</td>\n<td id=\"S2.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Global and local model parameters</td>\n<td id=\"S2.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">Fixed and consistent</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The key idea of VFL is to enhance a learning model by utilizing the distributed data with various attributes.\nHence, VFL accepts the vertically partitioned data where participants’ data share the same sample space with different attribute spaces.\nAs shown in Fig. ",
                "1",
                ", a general VFL process for each learning epoch includes the following seven key steps:",
                "Step 1: Private set intersection.",
                "\nBefore model training, the framework needs to find the common identifiers (IDs) served by all participants (i.e., guest organization and host organization) to align the training data samples, which is called private set intersection (PSI) or secure entity alignment ",
                "[",
                "8",
                "]",
                ".\nPSI is a secure multiparty protocol which allows multiple participants to find out the common IDs available across their data and nothing else.\nWidely adopted PSI techniques include naive hashing, oblivious polynomial evaluation, and oblivious transfer ",
                "[",
                "8",
                "]",
                ".",
                "Step 2: Bottom model forward propagation.",
                "\nAfter determining the aligned data samples among all participants, each participant will complete a forward propagation process using local data based on its bottom (local) model.\nThis forward propagation process is similar to the conventional training except calculating the loss value.",
                "Step 3: Forward output transmission.",
                "\nEach participant needs to transmit its forward output to the label owner.\nIntuitively speaking, the forward output contains intermediate results of local neural networks, which transforms the original attributes into features ",
                "[",
                "4",
                "]",
                ".\nSuch a transmission process may divulge participants’ privacy information.\nHence, advanced privacy preserving methods, such as differential privacy (DP), should be exploited to address potential privacy risks but will may incur additional communication costs and computation complexities.",
                "Step 4: Top model forward propagation.",
                "\nThe label owner uses the collected outputs from all participants to calculate the loss function value based on the top model and labels.",
                "Step 5: Top model backward propagation.",
                "\nThe label owner performs backward propagation and computes two gradients for: 1) model parameters of the top model; and 2) forward outputs from each participant.\nUsing the gradients of the top model, the label owner can calculate the average gradients for each batch of samples and update its model.",
                "Step 6: Backward output transmission.",
                "\nThe gradients of forward outputs are sent back to every guest participant.\nIt can be noticed the required communication cost (transmission bits) is usually much smaller than the ones in ",
                "Step 2",
                ", because they are gradients instead of intermediate outputs.",
                "Step 7: Bottom model backward propagation.",
                "\nEach participant calculates the gradients of its bottom model parameters based on the local data and gradients of the forward outputs from the label owner, and then updates its bottom model.",
                "We next discuss the differences between VFL and HFL in the following subsection."
            ]
        ]
    }
}