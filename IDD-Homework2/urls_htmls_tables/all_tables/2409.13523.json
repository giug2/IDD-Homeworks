{
    "S5.T1.2": {
        "caption": "Speech translation SacreBLEU scores on a subset of FLEURS. The baselines are SeamlessM4T-v2  , Canary-1B AST  , and Canary-1B ASR cascaded with pretrained T5 NMT. BESTOW-GPT and SALM-T5 are TinyLlama and T5 models finetuned exclusively on speech. Multimodal indicates joint finetuning on speech and text. ",
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T1.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T1.1.1.2.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S5.T1.1.1.1\">En <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.1.1.1.m1.1\"><semantics id=\"S5.T1.1.1.1.m1.1a\"><mo id=\"S5.T1.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T1.1.1.1.m1.1.1.cmml\">&#8594;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.1.1.1.m1.1b\"><ci id=\"S5.T1.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.1.1.1.m1.1.1\">&#8594;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.1.1.1.m1.1c\">\\rightarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T1.1.1.1.m1.1d\">&#8594;</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.1.1.3\">De</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.1.1.4\">Es</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.1.1.5\">Fr</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.1.1.6\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T1.1.1.6.1\">Avg</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.2.2\">De</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.2.3\">Es</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.2.4\">Fr</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"S5.T1.2.2.1\">\n<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.2.2.1.m1.1\"><semantics id=\"S5.T1.2.2.1.m1.1a\"><mo id=\"S5.T1.2.2.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T1.2.2.1.m1.1.1.cmml\">&#8594;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.2.2.1.m1.1b\"><ci id=\"S5.T1.2.2.1.m1.1.1.cmml\" xref=\"S5.T1.2.2.1.m1.1.1\">&#8594;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.2.2.1.m1.1c\">\\rightarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T1.2.2.1.m1.1d\">&#8594;</annotation></semantics></math> En</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T1.2.3.1.1\"><span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.2.3.1.1.1\">Baselines</span></td>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T1.2.3.1.2\"/>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T1.2.3.1.3\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S5.T1.2.3.1.4\"/>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T1.2.3.1.5\"/>\n<td class=\"ltx_td ltx_border_t\" id=\"S5.T1.2.3.1.6\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S5.T1.2.3.1.7\"/>\n<td class=\"ltx_td ltx_border_r ltx_border_t\" id=\"S5.T1.2.3.1.8\"/>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S5.T1.2.4.2.1\">SeamlessM4T-v2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.4.2.2\">33.2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.4.2.3\">23.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.4.2.4\">43.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.4.2.5\">37.1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.4.2.6\">25.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.4.2.7\">30.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.4.2.8\">32.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S5.T1.2.5.3.1\">Canary-1B</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.5.3.2\">32.1</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.5.3.3\">22.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.5.3.4\">40.8</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.5.3.5\">34.0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.5.3.6\">21.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.5.3.7\">31.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.5.3.8\">30.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S5.T1.2.6.4.1\">&#160;&#160;<span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.2.6.4.1.1\">&#8195;+T5 cascade</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.6.4.2\">32.6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.6.4.3\">23.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.6.4.4\">42.6</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.6.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.6.4.5.1\">39.4</span></td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.6.4.6\">27.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.6.4.7\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.6.4.7.1\">37.9</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.6.4.8\">34.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.7.5\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T1.2.7.5.1\">BESTOW-GPT</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.2.7.5.2\">32.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.2.7.5.3\">23.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.2.7.5.4\">41.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.2.7.5.5\">35.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.2.7.5.6\">23.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.2.7.5.7\">35.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.2.7.5.8\">32.0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.8.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S5.T1.2.8.6.1\">&#160;&#160;<span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.2.8.6.1.1\">&#8195;+multimodal</span>\n</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.8.6.2\">33.2</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.8.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.8.6.3.1\">24.5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.8.6.4\">42.7</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.8.6.5\">37.9</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T1.2.8.6.6\">24.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.8.6.7\">36.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T1.2.8.6.8\">33.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.9.7\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T1.2.9.7.1\">SALM-T5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.2.9.7.2\">34.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.2.9.7.3\">24.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.2.9.7.4\">43.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.2.9.7.5\">38.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T1.2.9.7.6\">25.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.2.9.7.7\">37.0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T1.2.9.7.8\">33.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.10.8\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r\" id=\"S5.T1.2.10.8.1\">&#160;&#160;<span class=\"ltx_text ltx_font_italic\" id=\"S5.T1.2.10.8.1.1\">&#8195;+multimodal</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T1.2.10.8.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.10.8.2.1\">34.6</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T1.2.10.8.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.10.8.3.1\">24.5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T1.2.10.8.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.10.8.4.1\">44.6</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T1.2.10.8.5\">37.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T1.2.10.8.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.10.8.6.1\">27.5</span></td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T1.2.10.8.7\">37.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T1.2.10.8.8\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.10.8.8.1\">34.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [
            [],
            []
        ],
        "references": [
            "Speech translation (Table I). Both BESTOW-GPT and SALM-T5 are trained purely for speech translation as baselines. We expect that these models ”catastrophically forget” their text translation skills while they learn speech translation. With speech-only training, SALM-T5 naturally achieves a better mean BLEU score of 33.9 compared to BESTOW-GPT’s 32.0 since the underlying foundation model was trained specifically for NMT.\nWe confront these results with multimodal training, where we observe 1.3 absolute BLEU score improvement for BESTOW-GPT and 0.5 absolute BLEU score improvement for SALM-T5. Larger improvement for BESTOW-GPT makes sense since TinyLlama model hasn’t had as much prior training on NMT data. In both cases, speech translation capability is improved by joint multimodal training, demonstrating the synergy between two modalities.\n",
            "Text machine translation (Table II). We compare the performance of the T5 NMT model in three scenarios: pretrained text-only model, after speech-only finetuning (SALM-T5) and after joint multimodal training (SALM-T5+multimodal). First, we notice that SALM-T5 completely lost its capability of translating text while acquiring AST skills (as seen in Table I). For multimodal SALM-T5, the mean BLEU score across 6 language pairs is preserved (we don’t consider 0.2 as a meaningful improvement), while we observe up to 0.6 BLEU score variation in some pairs. Therefore, the first goal of multimodal training is achieved: the T5 model did not lose its original performance on text after extension to audio modality.\n"
        ]
    },
    "S5.T2.2": {
        "caption": "Text translation SacreBLEU scores on a subset of FLORES. The T5 NMT baseline is trained exclusively on text, and extended to audio modality as SALM-T5. ",
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S5.T2.2\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T2.1.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T2.1.1.2.1\">Model</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\" id=\"S5.T2.1.1.1\">En <math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.1.1.1.m1.1\"><semantics id=\"S5.T2.1.1.1.m1.1a\"><mo id=\"S5.T2.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\">&#8594;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.1b\"><ci id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\">&#8594;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.1c\">\\rightarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T2.1.1.1.m1.1d\">&#8594;</annotation></semantics></math>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.1.3\">De</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.1.4\">Es</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T2.1.1.5\">Fr</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T2.1.1.6\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T2.1.1.6.1\">Avg</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.2.2.2\">De</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.2.2.3\">Es</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T2.2.2.4\">Fr</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\" id=\"S5.T2.2.2.1\">\n<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.2.2.1.m1.1\"><semantics id=\"S5.T2.2.2.1.m1.1a\"><mo id=\"S5.T2.2.2.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T2.2.2.1.m1.1.1.cmml\">&#8594;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.2.2.1.m1.1b\"><ci id=\"S5.T2.2.2.1.m1.1.1.cmml\" xref=\"S5.T2.2.2.1.m1.1.1\">&#8594;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.2.2.1.m1.1c\">\\rightarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T2.2.2.1.m1.1d\">&#8594;</annotation></semantics></math> En</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.1.1\">T5 NMT</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.3.1.2\">38.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.3.1.3\">27.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.1.4\">50.2</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.3.1.5\">44.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.2.3.1.6\">30.3</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.1.7\">45.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T2.2.3.1.8\">39.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S5.T2.2.4.2.1\">SALM-T5</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.2.4.2.2\">0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.2.4.2.3\">0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T2.2.4.2.4\">0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.2.4.2.5\">0</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.2.4.2.6\">0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T2.2.4.2.7\">0</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T2.2.4.2.8\">0</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.2.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r\" id=\"S5.T2.2.5.3.1\">&#160;&#160;<span class=\"ltx_text ltx_font_italic\" id=\"S5.T2.2.5.3.1.1\">&#8195;+multimodal</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T2.2.5.3.2\">38.7</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T2.2.5.3.3\">27.5</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T2.2.5.3.4\">50.4</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T2.2.5.3.5\">44.1</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T2.2.5.3.6\">30.9</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T2.2.5.3.7\">45.8</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T2.2.5.3.8\">39.6</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Text machine translation (Table II). We compare the performance of the T5 NMT model in three scenarios: pretrained text-only model, after speech-only finetuning (SALM-T5) and after joint multimodal training (SALM-T5+multimodal). First, we notice that SALM-T5 completely lost its capability of translating text while acquiring AST skills (as seen in Table I). For multimodal SALM-T5, the mean BLEU score across 6 language pairs is preserved (we don’t consider 0.2 as a meaningful improvement), while we observe up to 0.6 BLEU score variation in some pairs. Therefore, the first goal of multimodal training is achieved: the T5 model did not lose its original performance on text after extension to audio modality.\n"
        ]
    },
    "S5.T3.1": {
        "caption": "Multimodal training efficiency gains from 2D bucketing and OOMptimizer (opt). ",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T3.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T3.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.2.1\">Steps</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T3.1.1.1.3\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.3.1\">Runtime</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"2\" id=\"S5.T3.1.1.1.4\">Mean batch size</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T3.1.1.1.5\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T3.1.1.1.5.1\">Steps/sec</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column\" id=\"S5.T3.1.2.2.1\">Audio</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r\" id=\"S5.T3.1.2.2.2\">Text</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\" id=\"S5.T3.1.3.1.1\">BESTOW-GPT</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T3.1.3.1.2\">450k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T3.1.3.1.3\">7 days</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T3.1.3.1.4\">13</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T3.1.3.1.5\">100</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\" id=\"S5.T3.1.3.1.6\">1.15</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.4.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\" id=\"S5.T3.1.4.2.1\">&#160;&#160;<span class=\"ltx_text ltx_font_italic\" id=\"S5.T3.1.4.2.1.1\">&#8195;+opt</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.4.2.2\">100k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.4.2.3\">2.5 days</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.4.2.4\">55</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.4.2.5\">269</td>\n<td class=\"ltx_td ltx_align_left ltx_border_r\" id=\"S5.T3.1.4.2.6\">0.81</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.5.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r\" id=\"S5.T3.1.5.3.1\">SALM-T5<span class=\"ltx_text ltx_font_italic\" id=\"S5.T3.1.5.3.1.1\">+opt</span>\n</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T3.1.5.3.2\">7.5k</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T3.1.5.3.3\">5 hours</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S5.T3.1.5.3.4\">30</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T3.1.5.3.5\">251</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_r\" id=\"S5.T3.1.5.3.6\">0.7</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Efficient multimodal training with 2D bucketing and OOMptimizer.\nWe compare the training efficiency of BESTOW-GPT without and with our optimizations in Table III.\nThe baseline used 1D bucketing with a batch size heuristic on the total number of seconds (audio) or tokens (text) in a mini-batch, and a quadratic length penalty to offset the quadratic memory complexity of transformer.\nThe penalty and total length thresholds were tuned manually to observe close to 100% allocated GPU memory.\nSALM-T5 model was trained directly with OOMptimizer to save resources and converged in 5 hours, which is much faster than BESTOW-GPT likely due to being pretrained specifically for NMT. We noticed that without OOMptimizer, the bucket batch sizes are constrained either by sequence length outliers (especially for drastically different input and output sequence length examples) or the longest sequence lengths in the dataset.\n2D bucketing increased the batch sizes for most buckets by 1.5-2x for audio modality and 2-4x for text modality when training SALM-T5 model, since the large sequence length outliers have their own sub-buckets with a smaller batch size. With BESTOW-GPT, we applied 2D bucketing for audio modality with a modest 15-20% batch size increase.\n"
        ]
    }
}