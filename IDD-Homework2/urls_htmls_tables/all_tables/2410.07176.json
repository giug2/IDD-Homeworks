{
    "id_table_1": {
        "caption": "Table 1:  Main results on Claude under zero-shot setting, showing the accuracy of different benchmark methods vs.  Astute RAG , along with their prediction complexity, in number of prediction API calls. Best scores are in bold.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "These findings underscore the potential severity of the imperfect retrieval issue in real-world RAG and highlight the widespread existence of knowledge conflicts as the bottleneck to overcome it (Figure  1 ). Recent studies demonstrate that LLM-internal and external knowledge offer distinct advantages, but LLMs often struggle to consolidate conflicting information reliably, failing to respond based on collective knowledge  (Mallen et al.,  2023 ; Tan et al.,  2024 ; Xie et al.,  2024 ; Jin et al.,  2024 ) . This raises the following research question:  Is there an effective method to combine internal (from LLMs pretrained weights) and external (from specific corpora or knowledge bases) knowledge for more reliable RAG?  Previous work has widely explored using external knowledge to enhance LLMs through RAG. We seek to further leverage LLMs internal knowledge to recover from RAG failures",
            "Knowledge conflicts widely exist in RAG failures.   We provide an in-depth analyses of knowledge conflicts between LLMs internal knowledge and retrieved passages from external sources. With Claude 3.5 Sonnet as the LLM, Figure  1  shows that 19.2% of the overall data exhibit knowledge conflicts, where either the answer with or without RAG is correct. Among the conflicting cases, the internal knowledge is correct on 47.4% of them, while the external knowledge is correct on the remaining 52.6%. These results emphasize the importance of  effectively combining the internal and external knowledge to overcome the inherent limitation of relying solely on either source . However, previous work  (Tan et al.,  2024 ; Xie et al.,  2024 ; Jin et al.,  2024 )  show that LLMs might respond based on misleading information rather than comprehensive understanding of the conflicting knowledge in this context.",
            "We begin with formulating the problem of imperfect retrieval in RAG (Section  3.1 ). We then provide an overview of  Astute RAG , designed to overcome this problem (Section  3.2 ). Subsequently, we delve into the three major steps of  Astute RAG , including adaptive generation of internal knowledge (Section  3.3 ), source-aware knowledge consolidation (Section  3.4 ), and answer finalization (Section  3.5 ).",
            "Astute RAG  is designed to better leverage collective knowledge from both internal knowledge of LLMs and external corpus, for more reliable responses. As shown in Figure  3  and Algorithm  1 ,  Astute RAG  starts from acquiring the most accurate, relevant, and thorough passage set from the LLMs internal knowledge. Then, internal and external knowledge are consolidated in an iterative way, by comparing the generated and retrieved passages. Finally, the reliability of conflicting information is compared and the final output is generated according to the most reliable knowledge.",
            "We evaluate the effectiveness of  Astute RAG  on overcoming imperfect retrieval augmentation and addressing knowledge conflicts. In this section, we first introduce the experiment setting in detail (Section  4.1 ). Then, we compare the performance of  Astute RAG  with various baselines on diverse datasets (Section  4.2 ). Finally, we provide in-depth analyses (Section  4.3 ).",
            "Table  1  and Table  2  presents the results on data with realistic retrieval augmentation for each dataset. By comparing RAG and No RAG, we find that retrieved passages might not always bring benefits to downstream performance  on NQ and TriviaQA, RAG performance lags behind No RAG. We attribute this to that the questions being covered by the LLMs internal knowledge and the noise in retrieval results misleading the LLM. In contrast, on BioASQ and PopQA, which focus on domain-specific and long-tail questions, RAG significantly improves LLM performance. However, due to imperfect retrieval augmentation, the absolute performance still remains to be unsatisfactory.  Among all baselines, no single method consistently outperforms others across all datasets. This observation highlights that these baselines are tailored to distinct settings and may not be universally applicable.  For instance, InstructRAG is more effective on TriviaQA, achieving the best performance among all baselines with both Claude and Gemini.  In contrast, Self-Route performs better than InstructRAG on both NQ and BioASQ.  Moreover, RobustRAG achieves very different performance when applied to Gemini and Claude. Through in-depth analysis, we find that RobustRAG with Gemini exhibits a high refusal rate (refuse to answer) in responses.  We attribute this instability to the varying method designs of the baselines, which are tailored for different scenarios, resulting in inconsistent improvement across datasets.  Overall, InstructRAG and Self-Route demonstrates the best performance among all baselines when applied to Claude and Gemini respectively.  We also note that increasing the number of API calls does not necessarily correlate with improved performance."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Main results on Gemini under zero-shot setting, showing the accuracy of different benchmark methods vs.  Astute RAG , along with their prediction complexity, in number of prediction API calls. Best scores are in bold.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "While there have been independent analyses of information retrieval and RAG in the context of LLMs  (Su et al.,  2024 ; Mallen et al.,  2023 ) , previous studies have rarely connected the behaviors of retrieval and subsequent generation,  particularly regarding the propagation of information retrieval errors, which may lead to  knowledge conflicts   (Longpre et al.,  2021 ; Wang et al.,  2023a ; Xu et al.,  2024b )  between LLMs and context. To this end, we conduct comprehensive analyses on the occurrence of imperfect retrieval augmentation and its impact on LLM behavior under realistic conditions (Section  2 ). We conduct controlled experiments on a diverse range of general, domain-specific, and long-tail questions from NQ  (Kwiatkowski et al.,  2019 ) , TriviaQA  (Joshi et al.,  2017 ) , BioASQ  (Tsatsaronis et al.,  2015 ) , and PopQA  (Mallen et al.,  2023 ) .  We observe that imperfect retrieval augmentation is widespread even with adept real-world search engine (such as Google Search with Web as corpus)  roughly 70% retrieved passages do not directly contain true answers, leading to the impeded performance of LLM with RAG augmentation. 2 2 2 Note that some passages may contain information indirectly relevant to the answer, but may unintentionally mislead or distract LLMs.",
            "As shown in Figure  2 , although instances from different datasets exhibit different data distributions, imperfect retrieval is prevalent. Specifically,   similar-to \\sim  20% of the overall data have no mentions of the correct answer within any retrieved passage, including 34% on NQ, 18% on TriviaQA, 24% on BioASQ, and 50% on PopQA.  This finding also aligns with previous observation on information retrieval  (Thakur et al.,  2024 ) , that highlights that the number of positive passages can be very limited.",
            "We begin with formulating the problem of imperfect retrieval in RAG (Section  3.1 ). We then provide an overview of  Astute RAG , designed to overcome this problem (Section  3.2 ). Subsequently, we delve into the three major steps of  Astute RAG , including adaptive generation of internal knowledge (Section  3.3 ), source-aware knowledge consolidation (Section  3.4 ), and answer finalization (Section  3.5 ).",
            "We evaluate the effectiveness of  Astute RAG  on overcoming imperfect retrieval augmentation and addressing knowledge conflicts. In this section, we first introduce the experiment setting in detail (Section  4.1 ). Then, we compare the performance of  Astute RAG  with various baselines on diverse datasets (Section  4.2 ). Finally, we provide in-depth analyses (Section  4.3 ).",
            "Datasets and metrics.   We conduct experiments on the data collected in Section  2  consisting of data from NQ, TriviaQA, BioASQ, and PopQA. For each instance from these datasets, we provide 10 passages collected under a realistic retrieval setting: for each question in our benchmark, we query Google Search to retrieve the top 30 results and select the first 10 accessible websites. From each retrieved website, we extract the paragraph corresponding to the snippet provided in Google Search results as the retrieved passage.. Most of the retrieval results contains natural noise with irrelevant or misleading information. We do not consider enhancements to the retrieval side, such as query rewriting, as such enhancements are typically already incorporated into commercial information retrieval systems. Notably, we do not select questions or annotate answers based on the retrieval results. This setting allows us to analyze the severity of imperfect retrieval in real-world RAG. It distinguishes our benchmark from previous ones that employ synthetic retrieval corruptions or that unintentionally reduce the frequency of imperfect retrieval with biased construction protocols  (Chen et al.,  2024a ; Yang et al.,  2024 ) . We also evaluate our method on RGB  (Chen et al.,  2024a ) , a RAG diagnostic benchmark evaluating several crucial RAG abilities. Specifically, we choose the English subset of RGB focusing on noise robustness. The benchmark have positive and negative passage sets for each question. We select five negative documents per question as the context to form a worst-case scenario.  All the data in these datasets are short-form QA. Following previous work  (Xiang et al.,  2024 ; Wei et al.,  2024 ; Mallen et al.,  2023 ) , a model response is considered correct if it contains the ground-truth answer. To enhance evaluation reliability, we prompt LLMs to enclose the exact answer within special tokens, extracting them as the final responses.",
            "Table  1  and Table  2  presents the results on data with realistic retrieval augmentation for each dataset. By comparing RAG and No RAG, we find that retrieved passages might not always bring benefits to downstream performance  on NQ and TriviaQA, RAG performance lags behind No RAG. We attribute this to that the questions being covered by the LLMs internal knowledge and the noise in retrieval results misleading the LLM. In contrast, on BioASQ and PopQA, which focus on domain-specific and long-tail questions, RAG significantly improves LLM performance. However, due to imperfect retrieval augmentation, the absolute performance still remains to be unsatisfactory.  Among all baselines, no single method consistently outperforms others across all datasets. This observation highlights that these baselines are tailored to distinct settings and may not be universally applicable.  For instance, InstructRAG is more effective on TriviaQA, achieving the best performance among all baselines with both Claude and Gemini.  In contrast, Self-Route performs better than InstructRAG on both NQ and BioASQ.  Moreover, RobustRAG achieves very different performance when applied to Gemini and Claude. Through in-depth analysis, we find that RobustRAG with Gemini exhibits a high refusal rate (refuse to answer) in responses.  We attribute this instability to the varying method designs of the baselines, which are tailored for different scenarios, resulting in inconsistent improvement across datasets.  Overall, InstructRAG and Self-Route demonstrates the best performance among all baselines when applied to Claude and Gemini respectively.  We also note that increasing the number of API calls does not necessarily correlate with improved performance."
        ]
    },
    "global_footnotes": [
        ".",
        "Note that some passages may contain information indirectly relevant to the answer, but may unintentionally mislead or distract LLMs.",
        "https://deepmind.google/technologies/gemini/pro/",
        "https://www.anthropic.com/news/claude-3-5-sonnet",
        "The original Self-Route switches between RAG and long-context LLMs, while our implementation switches between LLMs with and without RAG to better align with the problem formulation in this paper."
    ]
}