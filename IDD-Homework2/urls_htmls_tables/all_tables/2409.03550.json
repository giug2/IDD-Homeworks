{
    "id_table_1": {
        "caption": "Table 1:  Performance of DMs with the same architecture as the teacher and those with  2  2\\times 2   faster architecture derived by baseline and our DKDM on three datasets.",
        "table": "A5.EGx1",
        "footnotes": [],
        "references": [
            "To accelerate diffusion models, as illustrated in Figure  1  (b)&(c), existing methods can be categorized into two pathways: reducing denoising steps and speeding up inference process of denoising networks for each step. Compared with the standard generation of DMs as shown in Figure  1  (a), existing methods of the first category  (Lu et al.,  2022a ,  b ; Luhman and Luhman,  2021 ; Salimans and Ho,  2022 ; Song et al.,  2023 ; Gu et al.,  2023 ; Sauer et al.,  2023 ,  2024 )  focus on reducing denoising steps of the lengthy sampling process. The second category focuses on reducing the inference time of each denoising step, through quantization  (Shang et al.,  2023 ; Li et al.,  2023 ; He et al.,  2024 ; Wang et al.,  2024 ) , pruning  (Fang et al.,  2024 ; Zhang et al.,  2024 ) , and so on. However, these studies often overlook the efficiency in denoising network architecture, a critical factor in the generation speed of DMs, which can be improved by efficient architecture design, i.e., Neural Architecture Search  (Elsken et al.,  2019 ; Cheng et al.,  2020 ) .",
            "The DKDM paradigm hinges on addressing two critical challenges. The first challenge involves optimizing a student model through the synthetic denoising data, instead of the source data. The second challenge involves flexibly organizing the synthesis of denoising data, preventing it from becoming the main bottleneck in slowing the optimization process, as generation of DMs is inherently slow. For the former, the optimization objective used in traditional DMs, as described by  Ho et al. ( 2020 ) , is inappropriate due to the absence of the data. To address this, we have especially designed a DKDM objective that aligns closely with the original DM optimization objective. For the latter challenge, the most straightforward approach is to utilize the teacher DMs to generate a comprehensive dataset, matching the size of the source dataset employed for training the teacher. This dataset is then used to train the student model following the standard training algorithm  (Ho et al.,  2020 ) . However, this method becomes impractical for extremely large datasets, like those utilized by models such as Stable Diffusion, due to excessive computational and storage demands. To overcome this, we introduce a dynamic iterative distillation method that efficiently collects denoising data with varying noise levels, rather than generating real-like samples. This method significantly reduces computation and storage requirements. Importantly, our approach is complementary to most previously established methods, as summarized in Figure  1  (b)&(c). Our experiment results validate that our DKDM is able to derive  2  2\\times 2   faster DMs while still able generate high-quality samples. Additionally, our method allows pretrained DMs to act as dataset for training new DMs, thereby reducing the storage demands for further research on DMs.",
            "Ho et al. ( 2020 )  found that predicting   bold-italic- \\boldsymbol{\\epsilon} bold_italic_  is a more efficient way when parameterizing     ( x t , t ) subscript   superscript x t t \\mu_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^{t},t) italic_ start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_t )  in practice, which can be derived by Equation ( 1 ) and Equation ( 2 ):",
            "In this section, we introduce a novel paradigm, termed  D ata-Free  K nowledge  D istillation for  D iffusion  Models  ( DKDM ). Section  3.1  details the DKDM paradigm, focusing on two principal challenges: the formulation of the optimization objective and the acquisition of denoising data for distillation. Section  3.2  describes our proposed optimization objective tailored for DKDM. Section  3.3  details our proposed method for effective collection of denoising data.",
            "In standard training of DMs, as depicted in Figure  2  (a), a training sample  x 0  D similar-to superscript x 0 D \\boldsymbol{x}^{0}\\sim\\mathcal{D} bold_italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  caligraphic_D  is selected along with a timestep  t  [ 1 , 1000 ] similar-to t 1 1000 t\\sim\\left[1,1000\\right] italic_t  [ 1 , 1000 ]  and random noise    N  ( 0 , I ) similar-to bold-italic- N 0 I \\boldsymbol{\\epsilon}\\sim\\mathcal{N}(0,\\bm{I}) bold_italic_  caligraphic_N ( 0 , bold_italic_I ) . The input  x t superscript x t \\boldsymbol{x}^{t} bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT  is computed using Equation ( 1 ), and the denoising network is optimized according to Equation ( 6 ) to generate outputs close to   bold-italic- \\boldsymbol{\\epsilon} bold_italic_ . However, without dataset access, DKDM cannot obtain training data  ( x t , t ,  ) superscript x t t bold-italic- (\\boldsymbol{x}^{t},t,\\boldsymbol{\\epsilon}) ( bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_t , bold_italic_ )  to employ this standard method. A straightforward approach for DKDM, termed the intuitive baseline and depicted in Figure  2  (b), involves using DMs pretrained on  D D \\mathcal{D} caligraphic_D  to generate a synthetic dataset  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , 4 4 4 The number of samples in the synthetic dataset  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is equal to those in original dataset  D D \\mathcal{D} caligraphic_D  which is then used to train new DMs with varying architectures. Despite its simplicity, creating  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is time-intensive and impractical for large datasets.",
            "However, the removal of the diffusion posterior and prior in the DKDM objective introduces a significant bottleneck, resulting in notably slow learning rates. As depicted in Figure  2  (a), standard training for DMs enable straightforward acquisition of noisy samples  x i t i superscript subscript x i subscript t i \\boldsymbol{x}_{i}^{t_{i}} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  at an arbitrary diffusion step  t  [ 1 , T ] similar-to t 1 T t\\sim[1,T] italic_t  [ 1 , italic_T ]  using Equation ( 1 ). These samples are compiled into a training data batch  B j = { x i t i } subscript B j superscript subscript x i subscript t i \\mathcal{B}_{j}=\\{\\boldsymbol{x}_{i}^{t_{i}}\\} caligraphic_B start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT } , with  j j j italic_j  representing the training iteration. Conversely, our DKDM objective requires obtaining a noisy sample  x ^ i t = G  T  ( T  t i ) superscript subscript bold-^ x i t subscript G subscript  T T subscript t i \\boldsymbol{\\hat{x}}_{i}^{t}=G_{\\boldsymbol{\\theta}_{T}}(T-t_{i}) overbold_^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = italic_G start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_T - italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  through  T  t i T subscript t i T-t_{i} italic_T - italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  denoising steps. Consequently, considering the denoising steps as the primary computational expense, the worst-case time complexity of assembling a denoising data batch  B ^ j = { x ^ i t i } subscript ^ B j superscript subscript bold-^ x i subscript t i {\\hat{\\mathcal{B}}}_{j}=\\{\\boldsymbol{\\hat{x}}_{i}^{t_{i}}\\} over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = { overbold_^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT }  for distillation is  O  ( T  b ) O T b {\\mathcal{O}(Tb)} caligraphic_O ( italic_T italic_b ) , where  b b b italic_b  denotes the batch size. This complexity significantly hinders the optimization process. To address this issue, we introduce a method called dynamic iterative distillation, detailed in Section  3.3 .",
            "where  x ^ t superscript bold-^ x t \\boldsymbol{\\hat{x}}^{t} overbold_^ start_ARG bold_italic_x end_ARG start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT  and  t t t italic_t  is produced by our proposed dynamic iterative distillation. The complete algorithm is detailed in Algorithm  1 .",
            "This section details extensive experiments that demonstrate the effectiveness of our proposed DKDM. In Section  4.1 , we establish appropriate metrics and baselines for evaluation. Section  4.2  compares the performance of baselines and our DKDM with different architectures. Additionally, we show that our DKDM can be combined with other methods to accelerate DMs. Finally, Section  4.3  describes an ablation study that validates the effectiveness of our proposed dynamic iterative distillation.",
            "Effectiveness.  Table  1  shows the performance comparison of our DKDM and baselines. Our DKDM consistently outperforms the baselines, demonstrating superior generative quality when maintaining identical architectures for the derived DMs. This performance validates the efficacy of our proposed DKDM objective and dynamic iterative distillation approach. The improvement over baselines is attributed to the complexity of the reverse diffusion process, which baselines struggle to learn, whereas the knowledge from pretrained teacher models is easier to learn, highlighting the advantage of our DKDM. Additionally, DKDM facilitates the distillation of generative capabilities into faster and more compact models, as evidenced by the  2  2\\times 2   faster architectures evaluated. The parameter count and generative speed are detailed in Table  2 , with further information on hyperparameters and network architecture available in Appendix  B.1 . Appendix  C  includes some exemplary generated samples, illustrating that the student DMs, derived through DKDM, are capable of producing high-quality images. Nevertheless, a limitation noted in Table  1  is that the performance of these student DMs falls behind their teacher counterparts, which will be discussed further in Section  5 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Params and speed of architectures we test.",
        "table": "S3.T1.4",
        "footnotes": [],
        "references": [
            "Ho et al. ( 2020 )  found that predicting   bold-italic- \\boldsymbol{\\epsilon} bold_italic_  is a more efficient way when parameterizing     ( x t , t ) subscript   superscript x t t \\mu_{\\boldsymbol{\\theta}}(\\boldsymbol{x}^{t},t) italic_ start_POSTSUBSCRIPT bold_italic_ end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_t )  in practice, which can be derived by Equation ( 1 ) and Equation ( 2 ):",
            "where    \\lambda italic_  is used for balance between the two objectives. 3 3 3 We set    \\lambda italic_  to 1 for all our experiments.  Guided by ( 6 ), the process of training and sampling are shown in Algorithm  2  and Algorithm  3  in Appendix  A .",
            "In this section, we introduce a novel paradigm, termed  D ata-Free  K nowledge  D istillation for  D iffusion  Models  ( DKDM ). Section  3.1  details the DKDM paradigm, focusing on two principal challenges: the formulation of the optimization objective and the acquisition of denoising data for distillation. Section  3.2  describes our proposed optimization objective tailored for DKDM. Section  3.3  details our proposed method for effective collection of denoising data.",
            "In standard training of DMs, as depicted in Figure  2  (a), a training sample  x 0  D similar-to superscript x 0 D \\boldsymbol{x}^{0}\\sim\\mathcal{D} bold_italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  caligraphic_D  is selected along with a timestep  t  [ 1 , 1000 ] similar-to t 1 1000 t\\sim\\left[1,1000\\right] italic_t  [ 1 , 1000 ]  and random noise    N  ( 0 , I ) similar-to bold-italic- N 0 I \\boldsymbol{\\epsilon}\\sim\\mathcal{N}(0,\\bm{I}) bold_italic_  caligraphic_N ( 0 , bold_italic_I ) . The input  x t superscript x t \\boldsymbol{x}^{t} bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT  is computed using Equation ( 1 ), and the denoising network is optimized according to Equation ( 6 ) to generate outputs close to   bold-italic- \\boldsymbol{\\epsilon} bold_italic_ . However, without dataset access, DKDM cannot obtain training data  ( x t , t ,  ) superscript x t t bold-italic- (\\boldsymbol{x}^{t},t,\\boldsymbol{\\epsilon}) ( bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_t , bold_italic_ )  to employ this standard method. A straightforward approach for DKDM, termed the intuitive baseline and depicted in Figure  2  (b), involves using DMs pretrained on  D D \\mathcal{D} caligraphic_D  to generate a synthetic dataset  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , 4 4 4 The number of samples in the synthetic dataset  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is equal to those in original dataset  D D \\mathcal{D} caligraphic_D  which is then used to train new DMs with varying architectures. Despite its simplicity, creating  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is time-intensive and impractical for large datasets.",
            "We propose an effective framework for DKDM paradigm, outlined in Figure  2  (c), which incorporates a DKDM Objective (described in Section  3.2 ) and a strategy for collecting denoising data  B i subscript B i \\mathcal{B}_{i} caligraphic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  during optimization (detailed in Section  3.3 ). This framework addresses the challenges of distillation without source dataset and reduces the costs associated with the intuitive baseline, since the synthetic  B i subscript B i \\mathcal{B}_{i} caligraphic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  requires much less computation than  D  superscript D  \\mathcal{D^{\\prime}} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT .",
            "However, the removal of the diffusion posterior and prior in the DKDM objective introduces a significant bottleneck, resulting in notably slow learning rates. As depicted in Figure  2  (a), standard training for DMs enable straightforward acquisition of noisy samples  x i t i superscript subscript x i subscript t i \\boldsymbol{x}_{i}^{t_{i}} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  at an arbitrary diffusion step  t  [ 1 , T ] similar-to t 1 T t\\sim[1,T] italic_t  [ 1 , italic_T ]  using Equation ( 1 ). These samples are compiled into a training data batch  B j = { x i t i } subscript B j superscript subscript x i subscript t i \\mathcal{B}_{j}=\\{\\boldsymbol{x}_{i}^{t_{i}}\\} caligraphic_B start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT } , with  j j j italic_j  representing the training iteration. Conversely, our DKDM objective requires obtaining a noisy sample  x ^ i t = G  T  ( T  t i ) superscript subscript bold-^ x i t subscript G subscript  T T subscript t i \\boldsymbol{\\hat{x}}_{i}^{t}=G_{\\boldsymbol{\\theta}_{T}}(T-t_{i}) overbold_^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = italic_G start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_T - italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  through  T  t i T subscript t i T-t_{i} italic_T - italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  denoising steps. Consequently, considering the denoising steps as the primary computational expense, the worst-case time complexity of assembling a denoising data batch  B ^ j = { x ^ i t i } subscript ^ B j superscript subscript bold-^ x i subscript t i {\\hat{\\mathcal{B}}}_{j}=\\{\\boldsymbol{\\hat{x}}_{i}^{t_{i}}\\} over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = { overbold_^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT }  for distillation is  O  ( T  b ) O T b {\\mathcal{O}(Tb)} caligraphic_O ( italic_T italic_b ) , where  b b b italic_b  denotes the batch size. This complexity significantly hinders the optimization process. To address this issue, we introduce a method called dynamic iterative distillation, detailed in Section  3.3 .",
            "This section details extensive experiments that demonstrate the effectiveness of our proposed DKDM. In Section  4.1 , we establish appropriate metrics and baselines for evaluation. Section  4.2  compares the performance of baselines and our DKDM with different architectures. Additionally, we show that our DKDM can be combined with other methods to accelerate DMs. Finally, Section  4.3  describes an ablation study that validates the effectiveness of our proposed dynamic iterative distillation.",
            "Baseline.  As the DKDM is a new paradigm proposed in this paper, previous methods are not suitable to serve as baselines. Therefore, in the data-free scenario, we take the intuitive baseline depicted in Figure  2  (a) as the baseline. Specifically, the teacher model consumes a lot of time to generate a substantial number of high-quality samples, equivalent in quantity to the source dataset, through 1,000 DDPM denoising steps. These samples then serve as the synthetic dataset ( D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ) for the training of randomly initialized student models, following the standard training (Algorithm  2  in Appendix  A ). We use the performance obtained from this method as our baseline for comparative analysis.",
            "Effectiveness.  Table  1  shows the performance comparison of our DKDM and baselines. Our DKDM consistently outperforms the baselines, demonstrating superior generative quality when maintaining identical architectures for the derived DMs. This performance validates the efficacy of our proposed DKDM objective and dynamic iterative distillation approach. The improvement over baselines is attributed to the complexity of the reverse diffusion process, which baselines struggle to learn, whereas the knowledge from pretrained teacher models is easier to learn, highlighting the advantage of our DKDM. Additionally, DKDM facilitates the distillation of generative capabilities into faster and more compact models, as evidenced by the  2  2\\times 2   faster architectures evaluated. The parameter count and generative speed are detailed in Table  2 , with further information on hyperparameters and network architecture available in Appendix  B.1 . Appendix  C  includes some exemplary generated samples, illustrating that the student DMs, derived through DKDM, are capable of producing high-quality images. Nevertheless, a limitation noted in Table  1  is that the performance of these student DMs falls behind their teacher counterparts, which will be discussed further in Section  5 .",
            "We further evaluated the performance of these student models across a diverse range of architectures. Specifically, we tested five different model sizes by directly specifying the architecture, bypassing complex methods like neural architecture search. Both the teacher and student models employ Convolutional Neural Networks (CNNs) and the results are shown in Figure  4 LABEL:sub@fig:compression . Detailed descriptions of these architectures are available in Appendix B.2 . Typically, we distilled a 14M model from a 57M teacher model, maintaining competitive performance and doubling the generation speed. Additionally, the 44M and 33M student models demonstrated similar speeds, suggesting that DKDM could benefit from integration with efficient architectural design techniques to further enhance the speed and quality of DMs. This aspect, however, is beyond our current scope and is designated for future research.",
            "In this section, we present the algorithms for training and sampling from standard DDPMs. The specific details have been previously introduced in Section  2 .",
            "Table  6  shows the architectures of different students trained in Section  4.2  for model compression.",
            "Similar to the configuration used by  Peebles and Xie [ 2023 ] , the hyperparameters employed for the ViT-based diffusion model in Section  4.2  are presented in Table  7 . This particular configuration was chosen due to the characteristic of ViT-based diffusion models generally requiring more time for image generation compared to their CNN-based counterparts. To illustrate this point, when generating 2,500 images on single A100 40GB GPU with 50 Improved DDPM steps, it takes approximately 57 seconds for a 57M CNN diffusion model, whereas a 19M ViT diffusion model requires 66 seconds. Our final choice of this configuration was driven by the aim to achieve fairness in our experimentation and analysis."
        ]
    },
    "id_table_3": {
        "caption": "Table 5:  Hyperparameters for main results. T. Arch. refers to the student model whose architecture mirrors that of the teacher. Faster Arch. denotes the smaller architecture we evaluated.",
        "table": "S3.T2.3",
        "footnotes": [],
        "references": [
            "where    \\lambda italic_  is used for balance between the two objectives. 3 3 3 We set    \\lambda italic_  to 1 for all our experiments.  Guided by ( 6 ), the process of training and sampling are shown in Algorithm  2  and Algorithm  3  in Appendix  A .",
            "In this section, we introduce a novel paradigm, termed  D ata-Free  K nowledge  D istillation for  D iffusion  Models  ( DKDM ). Section  3.1  details the DKDM paradigm, focusing on two principal challenges: the formulation of the optimization objective and the acquisition of denoising data for distillation. Section  3.2  describes our proposed optimization objective tailored for DKDM. Section  3.3  details our proposed method for effective collection of denoising data.",
            "We propose an effective framework for DKDM paradigm, outlined in Figure  2  (c), which incorporates a DKDM Objective (described in Section  3.2 ) and a strategy for collecting denoising data  B i subscript B i \\mathcal{B}_{i} caligraphic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  during optimization (detailed in Section  3.3 ). This framework addresses the challenges of distillation without source dataset and reduces the costs associated with the intuitive baseline, since the synthetic  B i subscript B i \\mathcal{B}_{i} caligraphic_B start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  requires much less computation than  D  superscript D  \\mathcal{D^{\\prime}} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT .",
            "However, the removal of the diffusion posterior and prior in the DKDM objective introduces a significant bottleneck, resulting in notably slow learning rates. As depicted in Figure  2  (a), standard training for DMs enable straightforward acquisition of noisy samples  x i t i superscript subscript x i subscript t i \\boldsymbol{x}_{i}^{t_{i}} bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  at an arbitrary diffusion step  t  [ 1 , T ] similar-to t 1 T t\\sim[1,T] italic_t  [ 1 , italic_T ]  using Equation ( 1 ). These samples are compiled into a training data batch  B j = { x i t i } subscript B j superscript subscript x i subscript t i \\mathcal{B}_{j}=\\{\\boldsymbol{x}_{i}^{t_{i}}\\} caligraphic_B start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = { bold_italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT } , with  j j j italic_j  representing the training iteration. Conversely, our DKDM objective requires obtaining a noisy sample  x ^ i t = G  T  ( T  t i ) superscript subscript bold-^ x i t subscript G subscript  T T subscript t i \\boldsymbol{\\hat{x}}_{i}^{t}=G_{\\boldsymbol{\\theta}_{T}}(T-t_{i}) overbold_^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT = italic_G start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_T - italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )  through  T  t i T subscript t i T-t_{i} italic_T - italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  denoising steps. Consequently, considering the denoising steps as the primary computational expense, the worst-case time complexity of assembling a denoising data batch  B ^ j = { x ^ i t i } subscript ^ B j superscript subscript bold-^ x i subscript t i {\\hat{\\mathcal{B}}}_{j}=\\{\\boldsymbol{\\hat{x}}_{i}^{t_{i}}\\} over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT = { overbold_^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT }  for distillation is  O  ( T  b ) O T b {\\mathcal{O}(Tb)} caligraphic_O ( italic_T italic_b ) , where  b b b italic_b  denotes the batch size. This complexity significantly hinders the optimization process. To address this issue, we introduce a method called dynamic iterative distillation, detailed in Section  3.3 .",
            "In this section, we present our efficient strategy for gathering denoising data for distillation, illustrated in Figure  3 . We begin by introducing a basic iterative distillation method that allows the student model to learn from the teacher model at each denoising step, instead of requiring the teacher to denoise multiple times within every training iteration to create a batch of noisy samples for the student to learn once. Subsequently, to enhance the diversity of noise levels within the batch samples, we develop an advanced method termed shuffled iterative distillation, which allows the student to learn denoising patterns across varying time steps. Lastly, we refine our approach to dynamic iterative distillation, significantly augmenting the diversity of data in the denoising batch. This adaptation ensures that the student model acquires knowledge from a broader array of samples over time, avoiding repetitive learning from identical samples.",
            "To better align the distribution of the denoising data with that of the standard training batch, we propose a method named dynamic iterative distillation. As shown in Figure  3  , this method employs shuffle denoise to construct an enlarged batch set  B ^ 1 + = { x ^ i t i } superscript subscript ^ B 1 superscript subscript bold-^ x i subscript t i {\\hat{\\mathcal{B}}}_{1}^{+}=\\{\\boldsymbol{\\hat{x}}_{i}^{t_{i}}\\} over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT = { overbold_^ start_ARG bold_italic_x end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_POSTSUPERSCRIPT } , where size  | B ^ j + | =   T  | B ^ j s | superscript subscript ^ B j  T superscript subscript ^ B j s |{\\hat{\\mathcal{B}}}_{j}^{+}|=\\rho T|{\\hat{\\mathcal{B}}}_{j}^{s}| | over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT | = italic_ italic_T | over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT | , where    \\rho italic_  is a scaling factor. During distillation, a subset  B ^ j s superscript subscript ^ B j s {\\hat{\\mathcal{B}}}_{j}^{s} over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_s end_POSTSUPERSCRIPT  is sampled from  B ^ j + superscript subscript ^ B j {\\hat{\\mathcal{B}}}_{j}^{+} over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT  through random selection for optimization. The one-step denoised samples replace their counterparts in  B ^ j + 1 + superscript subscript ^ B j 1 {\\hat{\\mathcal{B}}}_{j+1}^{+} over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT italic_j + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT + end_POSTSUPERSCRIPT . This method only has a time complexity of  O  ( b ) O b {\\mathcal{O}(b)} caligraphic_O ( italic_b )  and significantly improves distillation performance. The final DKDM objective is defined as:",
            "This section details extensive experiments that demonstrate the effectiveness of our proposed DKDM. In Section  4.1 , we establish appropriate metrics and baselines for evaluation. Section  4.2  compares the performance of baselines and our DKDM with different architectures. Additionally, we show that our DKDM can be combined with other methods to accelerate DMs. Finally, Section  4.3  describes an ablation study that validates the effectiveness of our proposed dynamic iterative distillation.",
            "Cross-Architecture Distillation.  Our DKDM transcends specific model architectures, enabling the distillation of generative capabilities from CNN-based DMs to Vision Transformers (ViT) and vice versa. We utilized DiT  (Peebles and Xie,  2023 )  for ViT-based DMs to further affirm the superiority of our approach. Detailed structural descriptions are available in Appendix  B.3 . For experimental purposes, we pretrained a small ViT-based DM to serve as the teacher. As shown in Table  3 , DKDM effectively facilitates cross-architecture distillation, yielding superior performance compared to baselines. Additionally, our results suggest that CNNs are more effective as compressed DMs than ViTs."
        ]
    },
    "id_table_4": {
        "caption": "Table 6:  Hyperparameters for CNN diffusion model compression.",
        "table": "S4.T3.3",
        "footnotes": [],
        "references": [
            "This section details extensive experiments that demonstrate the effectiveness of our proposed DKDM. In Section  4.1 , we establish appropriate metrics and baselines for evaluation. Section  4.2  compares the performance of baselines and our DKDM with different architectures. Additionally, we show that our DKDM can be combined with other methods to accelerate DMs. Finally, Section  4.3  describes an ablation study that validates the effectiveness of our proposed dynamic iterative distillation.",
            "We further evaluated the performance of these student models across a diverse range of architectures. Specifically, we tested five different model sizes by directly specifying the architecture, bypassing complex methods like neural architecture search. Both the teacher and student models employ Convolutional Neural Networks (CNNs) and the results are shown in Figure  4 LABEL:sub@fig:compression . Detailed descriptions of these architectures are available in Appendix B.2 . Typically, we distilled a 14M model from a 57M teacher model, maintaining competitive performance and doubling the generation speed. Additionally, the 44M and 33M student models demonstrated similar speeds, suggesting that DKDM could benefit from integration with efficient architectural design techniques to further enhance the speed and quality of DMs. This aspect, however, is beyond our current scope and is designated for future research.",
            "Combination with Orthogonal Methods.  The DMs derived by our DKDM are compatible with various orthogonal methods, such as denoising step reduction, quantization, pruning and so on. Here we conducted experiments to integrate the DDIM method, which reduces denoising steps, as illustrated in Figure  4   LABEL:sub@fig:ddim . This integration demonstrates that DDIM can further accelerate our derived student models, although with some performance trade-offs.",
            "However, in practice, there is invariably some discrepancy between  D D \\mathcal{D} caligraphic_D  and  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , limiting the performance of the student model. We report these scores, denoted as  FID  superscript FID  \\text{FID}^{\\prime} FID start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  and  sFID  superscript sFID  \\text{sFID}^{\\prime} sFID start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , calculated over the distribution  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  instead of  D D \\mathcal{D} caligraphic_D  in Table  4 . The results indicate that the  FID  superscript FID  \\text{FID}^{\\prime} FID start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  and  sFID  superscript sFID  \\text{sFID}^{\\prime} sFID start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  scores of the student closely mirror those of the teacher, suggesting effective optimization. Nevertheless, these scores are inferior to those of the teacher, primarily due to the gap between  D D \\mathcal{D} caligraphic_D  and  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . A potential solution to enhance DKDM involves improving the generative capabilities of the teacher, which we leave as a direction for future work.",
            "Table  6  shows the architectures of different students trained in Section  4.2  for model compression.",
            "Similar to the configuration used by  Peebles and Xie [ 2023 ] , the hyperparameters employed for the ViT-based diffusion model in Section  4.2  are presented in Table  7 . This particular configuration was chosen due to the characteristic of ViT-based diffusion models generally requiring more time for image generation compared to their CNN-based counterparts. To illustrate this point, when generating 2,500 images on single A100 40GB GPU with 50 Improved DDPM steps, it takes approximately 57 seconds for a 57M CNN diffusion model, whereas a 19M ViT diffusion model requires 66 seconds. Our final choice of this configuration was driven by the aim to achieve fairness in our experimentation and analysis."
        ]
    },
    "id_table_5": {
        "caption": "Table 7:  Hyperparameters for ViT-based diffusion models.",
        "table": "S5.T4.8.2",
        "footnotes": [],
        "references": [
            "Effectiveness.  Table  1  shows the performance comparison of our DKDM and baselines. Our DKDM consistently outperforms the baselines, demonstrating superior generative quality when maintaining identical architectures for the derived DMs. This performance validates the efficacy of our proposed DKDM objective and dynamic iterative distillation approach. The improvement over baselines is attributed to the complexity of the reverse diffusion process, which baselines struggle to learn, whereas the knowledge from pretrained teacher models is easier to learn, highlighting the advantage of our DKDM. Additionally, DKDM facilitates the distillation of generative capabilities into faster and more compact models, as evidenced by the  2  2\\times 2   faster architectures evaluated. The parameter count and generative speed are detailed in Table  2 , with further information on hyperparameters and network architecture available in Appendix  B.1 . Appendix  C  includes some exemplary generated samples, illustrating that the student DMs, derived through DKDM, are capable of producing high-quality images. Nevertheless, a limitation noted in Table  1  is that the performance of these student DMs falls behind their teacher counterparts, which will be discussed further in Section  5 .",
            "To validate our designed paradigm DKDM, we tested FID score of our progressively designed methods, including iterative, shuffled iterative, and dynamic iterative distillation, over 200K training iterations. For the dynamic iterative distillation, the parameter    \\rho italic_  was set to 0.4. The results, shown in Figure  5   LABEL:sub@fig:strategy , demonstrate that our dynamic iterative distillation strategy not only converges more rapidly but also delivers superior performance. The convergence curve for our method closely matches that of the baseline, which confirms the effectiveness of the DKDM objective in alignment with the standard optimization objective ( 6 ).",
            "Further experiments explored the effects of varying    \\rho italic_  on the performance of dynamic iterative distillation. As illustrated in Figure  5   LABEL:sub@fig:rho , higher    \\rho italic_  values enhance the distillation process up to a point, beyond which performance gains diminish. This outcome supports our hypothesis that dynamic iterative distillation enhances batch construction flexibility, thereby improving distillation efficiency. Beyond a certain level of flexibility, increasing    \\rho italic_  does not significantly benefit the distillation process. Further stability analysis of our dynamic iterative distillation is available in Appendix  D .",
            "For experiments of both DKDM and baseline, we use the hyperparameters specified by  Ning et al. [ 2023 ] , which are also in line with those adopted by  Dhariwal and Nichol [ 2021 ] , as reported in Table  5 . Settings of our training process are basically the same with  Dhariwal and Nichol [ 2021 ]  and  Ning et al. [ 2023 ] , including mixed precision training, EMA and so on. All the models are trained on 8 NVIDIA A100 GPUs (with 40G memory)."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A2.T5.3",
        "footnotes": [],
        "references": [
            "where    \\lambda italic_  is used for balance between the two objectives. 3 3 3 We set    \\lambda italic_  to 1 for all our experiments.  Guided by ( 6 ), the process of training and sampling are shown in Algorithm  2  and Algorithm  3  in Appendix  A .",
            "In standard training of DMs, as depicted in Figure  2  (a), a training sample  x 0  D similar-to superscript x 0 D \\boldsymbol{x}^{0}\\sim\\mathcal{D} bold_italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT  caligraphic_D  is selected along with a timestep  t  [ 1 , 1000 ] similar-to t 1 1000 t\\sim\\left[1,1000\\right] italic_t  [ 1 , 1000 ]  and random noise    N  ( 0 , I ) similar-to bold-italic- N 0 I \\boldsymbol{\\epsilon}\\sim\\mathcal{N}(0,\\bm{I}) bold_italic_  caligraphic_N ( 0 , bold_italic_I ) . The input  x t superscript x t \\boldsymbol{x}^{t} bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT  is computed using Equation ( 1 ), and the denoising network is optimized according to Equation ( 6 ) to generate outputs close to   bold-italic- \\boldsymbol{\\epsilon} bold_italic_ . However, without dataset access, DKDM cannot obtain training data  ( x t , t ,  ) superscript x t t bold-italic- (\\boldsymbol{x}^{t},t,\\boldsymbol{\\epsilon}) ( bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_t , bold_italic_ )  to employ this standard method. A straightforward approach for DKDM, termed the intuitive baseline and depicted in Figure  2  (b), involves using DMs pretrained on  D D \\mathcal{D} caligraphic_D  to generate a synthetic dataset  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , 4 4 4 The number of samples in the synthetic dataset  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is equal to those in original dataset  D D \\mathcal{D} caligraphic_D  which is then used to train new DMs with varying architectures. Despite its simplicity, creating  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is time-intensive and impractical for large datasets.",
            "Eliminating the diffusion posterior   q  ( x t  1 | x t , x 0 ) q conditional superscript x t 1 superscript x t superscript x 0 \\boldsymbol{q(\\boldsymbol{x}^{t-1}|\\boldsymbol{x}^{t},\\boldsymbol{x}^{0})} bold_italic_q bold_( bold_italic_x start_POSTSUPERSCRIPT bold_italic_t bold_- bold_1 end_POSTSUPERSCRIPT bold_| bold_italic_x start_POSTSUPERSCRIPT bold_italic_t end_POSTSUPERSCRIPT bold_, bold_italic_x start_POSTSUPERSCRIPT bold_0 end_POSTSUPERSCRIPT bold_) .  In our framework, we introduce a teacher DM with parameters   T subscript  T \\boldsymbol{\\theta}_{T} bold_italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , trained on dataset  D D \\mathcal{D} caligraphic_D . This model can generate samples that conform to the learned distribution  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . Optimized with the objective ( 6 ), the distribution  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  within a well-learned teacher model closely matches  D D \\mathcal{D} caligraphic_D . Our goal is for a student DM, parameterized by   S subscript  S \\boldsymbol{\\theta}_{S} bold_italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT , to replicate  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  instead of  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , thereby obviating the need for  q q q italic_q  during optimization.",
            "Specifically, the pretrained teacher model is optimized via the hybrid objective in Equation ( 6 ), which indicates that both the KL divergence  D K  L ( q ( x t  1 | x t , x 0 )  p  T ( x t  1 | x t ) ) D_{KL}(q(\\boldsymbol{x}^{t-1}|\\boldsymbol{x}^{t},\\boldsymbol{x}^{0})\\|p_{% \\boldsymbol{\\theta}_{T}}(\\boldsymbol{x}^{t-1}|\\boldsymbol{x}^{t})) italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q ( bold_italic_x start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT | bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , bold_italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT )  italic_p start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT | bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) )  and the mean squared error  E x t ,  , t  [      T  ( x t , t )  2 ] subscript E superscript x t bold-italic- t delimited-[] superscript norm bold-italic- subscript bold-italic- subscript  T superscript x t t 2 \\mathbb{E}_{\\boldsymbol{x}^{t},\\boldsymbol{\\epsilon},t}[\\|\\boldsymbol{\\epsilon% }-\\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}_{T}}(\\boldsymbol{x}^{t},t)\\|^{2}] blackboard_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , bold_italic_ , italic_t end_POSTSUBSCRIPT [  bold_italic_ - bold_italic_ start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_t )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ]  are minimized. Given the similarity in distribution between the teacher model and the dataset, we propose a DKDM objective that optimizes the student model through minimizing  D K  L ( p  T ( x t  1 | x t ) )  p  S ( x t  1 | x t ) ) ) D_{KL}(p_{\\boldsymbol{\\theta}_{T}}(\\boldsymbol{x}^{t-1}|\\boldsymbol{x}^{t}))\\|% p_{\\boldsymbol{\\theta}_{S}}(\\boldsymbol{x}^{t-1}|\\boldsymbol{x}^{t}))) italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_p start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT | bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) )  italic_p start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT | bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) ) )  and  E x t  [    T  ( x t , t )    S  ( x t , t )  2 ] subscript E superscript x t delimited-[] superscript norm subscript bold-italic- subscript  T superscript x t t subscript bold-italic- subscript  S superscript x t t 2 \\mathbb{E}_{\\boldsymbol{x}^{t}}[\\|\\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}_{% T}}(\\boldsymbol{x}^{t},t)-\\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}_{S}}(% \\boldsymbol{x}^{t},t)\\|^{2}] blackboard_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT end_POSTSUBSCRIPT [  bold_italic_ start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_t ) - bold_italic_ start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_t )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] . Indirectly, the DKDM objective facilitates the minimization of  D K  L ( q ( x t  1 | x t , x 0 )  p  S ( x t  1 | x t ) ) ) D_{KL}(q(\\boldsymbol{x}^{t-1}|\\boldsymbol{x}^{t},\\boldsymbol{x}^{0})\\|p_{% \\boldsymbol{\\theta}_{S}}(\\boldsymbol{x}^{t-1}|\\boldsymbol{x}^{t}))) italic_D start_POSTSUBSCRIPT italic_K italic_L end_POSTSUBSCRIPT ( italic_q ( bold_italic_x start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT | bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , bold_italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT )  italic_p start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t - 1 end_POSTSUPERSCRIPT | bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) ) )  and  E x 0 ,  , t  [      S  ( x t , t )  2 ] subscript E superscript x 0 bold-italic- t delimited-[] superscript norm bold-italic- subscript bold-italic- subscript  S superscript x t t 2 \\mathbb{E}_{\\boldsymbol{x}^{0},\\boldsymbol{\\epsilon},t}[\\|\\boldsymbol{\\epsilon% }-\\boldsymbol{\\epsilon}_{\\boldsymbol{\\theta}_{S}}(\\boldsymbol{x}^{t},t)\\|^{2}] blackboard_E start_POSTSUBSCRIPT bold_italic_x start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT , bold_italic_ , italic_t end_POSTSUBSCRIPT [  bold_italic_ - bold_italic_ start_POSTSUBSCRIPT bold_italic_ start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( bold_italic_x start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , italic_t )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ] , despite the inaccessibility of the posterior. Consequently, we propose the DKDM objective as follows:",
            "To validate our designed paradigm DKDM, we tested FID score of our progressively designed methods, including iterative, shuffled iterative, and dynamic iterative distillation, over 200K training iterations. For the dynamic iterative distillation, the parameter    \\rho italic_  was set to 0.4. The results, shown in Figure  5   LABEL:sub@fig:strategy , demonstrate that our dynamic iterative distillation strategy not only converges more rapidly but also delivers superior performance. The convergence curve for our method closely matches that of the baseline, which confirms the effectiveness of the DKDM objective in alignment with the standard optimization objective ( 6 ).",
            "The primary concept of our proposed DKDM paradigm is illustrated in Figure  6 . In this paradigm, the teacher DM   T subscript  T \\boldsymbol{\\theta}_{T} bold_italic_ start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT , is trained on a real dataset  D D \\mathcal{D} caligraphic_D , which follows the distribution  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . There are two critical relationships between  D D \\mathcal{D} caligraphic_D  and  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT . First, the distribution  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  of a well-trained teacher closely approximates  D D \\mathcal{D} caligraphic_D . Second, the FID scores, when computed using  D D \\mathcal{D} caligraphic_D  as a reference, correlate with those using  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , as demonstrated by the linear fitting in Figure  6 . This correlation underpins the effectiveness of the DKDM paradigm. By transferring the distribution  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  from the teacher DM to a lighter student DM, DKDM enables the student to generate data whose distribution closely approximates  D D \\mathcal{D} caligraphic_D .",
            "Table  6  shows the architectures of different students trained in Section  4.2  for model compression."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A2.T6.3",
        "footnotes": [],
        "references": [
            "Similar to the configuration used by  Peebles and Xie [ 2023 ] , the hyperparameters employed for the ViT-based diffusion model in Section  4.2  are presented in Table  7 . This particular configuration was chosen due to the characteristic of ViT-based diffusion models generally requiring more time for image generation compared to their CNN-based counterparts. To illustrate this point, when generating 2,500 images on single A100 40GB GPU with 50 Improved DDPM steps, it takes approximately 57 seconds for a 57M CNN diffusion model, whereas a 19M ViT diffusion model requires 66 seconds. Our final choice of this configuration was driven by the aim to achieve fairness in our experimentation and analysis."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A2.T7.2",
        "footnotes": [],
        "references": [
            "During our exploration, we discovered that the utilization of the  Random Discard  technique proves to be a straightforward yet highly effective approach for enhancing the distillation process. The idea behind it involves the random elimination of some batch of noisy samples generated by the teacher model during the iterative distillation. For instance, in iterative distillation during the initial five training iterations, batches  B ^ 1 , B ^ 3 , B ^ 4 subscript ^ B 1 subscript ^ B 3 subscript ^ B 4 {\\hat{\\mathcal{B}}}_{1},{\\hat{\\mathcal{B}}}_{3},{\\hat{\\mathcal{B}}}_{4} over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT  may be discarded, while  B ^ 2 , B ^ 5 subscript ^ B 2 subscript ^ B 5 {\\hat{\\mathcal{B}}}_{2},{\\hat{\\mathcal{B}}}_{5} over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , over^ start_ARG caligraphic_B end_ARG start_POSTSUBSCRIPT 5 end_POSTSUBSCRIPT  are utilized for the students learning. We present an analysis of the impact of random discarding in our devised methodologies. Specifically, we introduce the parameter  p p p italic_p  to denote the probability of discarding certain noisy samples. Subsequently, we apply varying discard probabilities to the iterative distillation, shuffled iterative distillation, and dynamic iterative distillation, and assess their respective performance alterations over a training duration of 200k iterations. The outcomes are presented in Figure  8 . It is noteworthy that both iterative distillation and shuffled iterative distillation face limitations in constructing flexible batches, where random discard emerges as a noteworthy solution to enhance their efficacy. Conversely, for dynamic iterative distillation, when    \\rho italic_  attains a sufficiently large value, it becomes apparent that random discard fails to confer additional advantages. This observation underscores the inherent stability of our dynamic iterative distillation method and we ultimately omitted random discard from the final implementation. This is beneficial because of its inefficiency in requiring the teacher model to prepare a larger number of noisy samples."
        ]
    }
}