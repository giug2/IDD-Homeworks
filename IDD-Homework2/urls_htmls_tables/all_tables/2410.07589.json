{
    "id_table_1": {
        "caption": "Table 1:  Fairness evaluation metrics for diverse tasks (higher values indicate greater unfairness).",
        "table": "S4.T1.11.11",
        "footnotes": [],
        "references": [
            "RAG enables LLMs to combine external knowledge with internal information, thereby enhancing content generation capabilities. Typically, the external knowledge has been shown to improve reasoning in domain-specific tasks and mitigate hallucinations. However, there is no reason to dismiss the possibility that externally retrieved knowledge will also inadvertently bring out undesired biased information, which might lead to discriminatory outputs from LLMs. To comprehensively understand the underlying risks, we conduct a practical fairness evaluation from the perspective of practitioners. We recognize the users varying levels of awareness regarding the fairness of their datasets can lead to different degrees of scrutiny and bias mitigation before the data is through RAG, as illustrated in Fig.  1 . Specifically, we explore three levels of fairness awareness: (1) Low fairness awareness: users directly use uncensored datasets for RAG; (2) Medium fairness awareness: users only mitigate prominent biases in the external dataset; (3) High fairness awareness: users carefully check for all possible biases. The following sections outline the risks we identify within each fairness awareness level.",
            "In practical applications, many users employ RAG to improve specific tasks, often inadvertently overlooking fairness implications of the external datasets they rely on. Numerous widely used datasets have been shown to contain biases related to certain sensitive attributes [ 43 ,  44 ] . Consequently, a significant concern arises when users lack awareness of fairness and directly utilize uncensored original data as external knowledge, as they risk introducing substantial biased information into the LLMs, which may lead to unfair outcomes (shown in the left part of Fig  1 ). This concern is particularly critical in fairness-sensitive domains such as education, healthcare, and employment, where biased outputs can have serious ramifications in decision-making processes. To reveal these risks, we investigate the impact of using uncensored external datasets containing unfair samples on the fairness performance of RAG. Specifically, our study examines how varying levels of bias in external datasets influence the fairness of LLM-generated outputs, providing valuable insights into the implications of biased external knowledge on equitable decision-making.",
            "Even when users actively mitigate prominent biases, such as those related to gender and race, they may still inadvertently overlook less conspicuous biases, like those related to age as shown in the middle part of Fig  1 . This scenario is particularly relevant in commercial contexts, where prioritizing the addressing of well-known societal biases often aligns with goals of political correctness and marketing optimization. For instance, Googles Gemini product was criticized for overcompensating for racial biases by overrepresenting AI-generated images of people of coloran attempt to address historical racial disparities that resulted in unintended overcorrection [ 45 ] . Similarly, in academic research, while extensive efforts are made to mitigate popular biases such as gender and ethnicity [ 46 ,  47 ,  48 ] , less popular biases often receive less attention [ 49 ] . This trend leads to a disproportionate focus on well-known biases, potentially neglecting less conspicuous biases. Moreover, many bias mitigation techniques in NLP models are designed to address specific bias categories, requiring manual identification of examples for each type [ 50 ,  17 ] . This further entrenches the disparity between the focus on major versus minor biases. As a result, datasets that are considered fair with respect to popular biases may still contain overlooked biases.",
            "Imagine a scenario where users with high awareness of fairness meticulously ensure that all sensitive attributes within an external dataset are unbiased, resulting in a dataset that appears to have be censored (right part of Fig  1 ). Intuitively, one might assume that such a carefully curated dataset would guarantee fairness in downstream tasks. However, recent findings [ 14 ,  15 ]  reveal a surprising risk: even when models are fine-tuned with seemingly benign data, they can still experience safety degradation, undermining their previous well-aligned fairness and ethical standards.  This raises a disconcerting question in the context of RAG-based LLMs: could the interaction with a dataset that is ostensibly fair still compromise the fairness of the model? In contrast to fine-tuning, RAG-based LLMs integrate external knowledge from ready-made datasets, meaning fairness degradation could occur through the simple act of retrieving information, without modifying the models internal parameters. Such a scenario would be deeply concerning. It suggests that even routine usage of RAG-based LLMs could lead to biased or discriminatory outputs, posing a subtle but serious vulnerability. Adversaries might exploit this mechanism to degrade fairness without directly manipulating the model, raising critical concerns about the reliability of current LLMs.",
            "This section presents empirical evidence regarding the fairness risks associated with the integration of RAG into LLMs, as discussed in Sec.  3 . We conduct a comprehensive investigation of the fairness implications by designing a robust set of experiments that encompass a variety of NLP tasks, including classification, question answering, and sentence completion. Specifically, Sec. 4.1  details the experimental setup, including the tasks, metrics, and LLMs employed in our study. Following this, Sec.  4.2 , Sec.  4.3 , and Sec.  4.4  analyze the fairness risks posed by RAG-based LLMs, considering different levels of dataset censorship across the various tasks.",
            "Generation Task:  HolisticBias [ 36 ]  contains 460,000 sentence prompts reflecting demographic identities (e.g., Hi! I am a Catholic grandmother.) used in two-person dialogues. these prompts aim to enable LLMs to generate large text corpora for examining fairness and potential bias in models. However, single-sentence prompts lack the rich context necessary to be used as external knowledge. To address this, we utilize OPT-1.3B [ 53 ]  to extend the prompts into richer dialogues, which are then evaluated for toxicity using the widely adopted Perspective API [ 40 ] . Specifically, this API assigns a toxicity probability (ranging from 0 to 1) to each input. Consequently, samples with toxicity scores below 0.1 are categorized as fair samples, while those above 0.5 are deemed unfair. In the evaluation, we also adopt the toxicity score from the Perspective API as our evaluation metric, with the average toxicity score serving as the primary evaluation criterion. An overview of the metrics is presented in Table  1 .",
            "To address the redundancy issue, we calculate the similarity between all texts using Levenshtein distance during the pre-processing phase. After that we remove samples with similarity above a specified threshold. For the balance issue, we conduct resampling and alignment in the post-processing phase based on a fixed unfairness rate and a scale parameter. This ensures that the final samples strictly meet the unfairness rate while maintaining a count no more than the scale we want. The improved BBQ processing Algorithm  1  takes the unfairness rate and scale parameters as inputs to generate non-redundant and balanced BBQ data for RAG.",
            "Fig.  10  presents fine-grained evaluation results across different bias categories for GPT series, supplemented by results from disambiguated contexts. Generally, the bias spacethe area enclosed by each colored line in the radar plottends to expand as unfairness increases across most categories.",
            "Fig.  11  shows the evaluation results for Llama-series models when different categories of bias are introduced in uncensored data, where Ambig and Disambig denote the ambiguous test data and disambiguated test data in the BBQ dataset, respectively. A similar finding observed with the GPT series LLMs can also be seen in the Llama-series models. Specifically, different bias categories show varying extents of fairness degradation, which may be attributed to the differing levels of fairness alignment efforts made by Llama for each category.",
            "We present a comparison between no RAG and clean RAG based on the Llama series model in Fig  12 . We observe a similar trend to that of the GPT series: even with a fully censored dataset, the fairness of LLMs can still be compromised. In particular, for the PISA dataset, all models demonstrate consistent degradation in fairness after applying clean RAG. However, Llama series models do not show a clear pattern regarding which bias categories are more vulnerable in terms of fairness.",
            "As shown in Fig.  13 , we present the impact of pre-retrieval and post-retrieval strategies on fairness performance in terms of all bias categories. A similar trend is observed as in the main text: the summarizer can alleviate fairness degradation across all bias categories, while reranker and query expansion strategies do not show significant influence on fairness with respect to these categories."
        ]
    },
    "id_table_2": {
        "caption": "Table 3:  Descriptions of LLM-answer types for BBQ",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "Level 1: fairness risk of uncensored datasets (  4.2 ).  Many users leverage RAG to enhance specific tasks, often inadvertently overlooking the fairness implications of the external dataset they utilize. Consequently, they may inadvertently rely on uncensored datasets that contain significant biased information. In our experiments, we systematically simulate varying levels of uncensorship by incorporating different proportions of unfair samples into the external dataset. Our findings demonstrate that even a small fraction of unfair samples-such as 20 % percent \\% % -is sufficient to elicit biased responses. Furthermore, we observe that  the greater the extent of uncensorship, the more pronounced the decrease in fairness .",
            "This section presents empirical evidence regarding the fairness risks associated with the integration of RAG into LLMs, as discussed in Sec.  3 . We conduct a comprehensive investigation of the fairness implications by designing a robust set of experiments that encompass a variety of NLP tasks, including classification, question answering, and sentence completion. Specifically, Sec. 4.1  details the experimental setup, including the tasks, metrics, and LLMs employed in our study. Following this, Sec.  4.2 , Sec.  4.3 , and Sec.  4.4  analyze the fairness risks posed by RAG-based LLMs, considering different levels of dataset censorship across the various tasks.",
            "Building on the scenario in Sec.  4.2 , we investigate how an uncensored external dataset containing unfair samples affects the fairness of RAG-based LLMs. Specifically, we evaluate the fairness performance of RAG-based LLMs across different levels of unfairness in the external dataset.",
            "Uncensored data significantly degrades fairness.  Figs.  2  and first two sub-figures in Fig.  3  shows the comparison between the No-RAG baseline and RAG-based LLMs across different unfairness rates on three datasets. The results consistently indicate a decline in fairness as the unfairness rate increases, highlighting that higher levels of unfairness in the external dataset lead to more pronounced fairness degradation across most RAG-based LLMs.",
            "Fairness implications vary across task scenarios and model quality.  Fig.  2  and first two sub-figures in Fig.  3  also reveal that fairness degradation patterns differ between LLMs, even within the same task. For instance, GPT series LLMs outperform Llama series LLMs in the generation task (Holistic). However, in the classification task (PISA) and the question-answering task (BBQ), Llama series LLMs exhibit superior fairness across all unfairness rates. This is unexpected, given that GPT series LLMs are typically regarded as more advanced, with better alignment to trustworthiness. To explore this further, we analyzed the accuracy results, as shown in last two sub-figures in Fig.  3 . The findings reveal that Llama models perform significantly worse in terms of accuracy compared to GPT series LLMs. On BBQ, Llama series LLMs achieve less than 50 % percent \\% %  accuracy, performing not much better than random guessing. This suggests that the apparent fairness advantage in Llama series LLMs might stem from their inability to properly understand the questions, leading to random responses rather than informed, fairness-aware decision. Moreover, as shown in Fig.  4 , Llama series LLMs are notably more cautious than GPT series LLMs, often refusing to answer a higher proportion of questions.",
            "Given the practical scenario discussed in Sec.  3.2 , it is critical to assess whether mitigating bias in one specific category is sufficient on its own. More broadly, we explore whether bias in one category (RAG bias category,  RC ) affects fairness in another category (test bias category,  TC ) with RAG-based LLMs. To investigate this, we create partially censored datasets where unfair samples from one RC (with a 1.0 unfairness rate) are combined with fair samples from one TC (with a 0.0 unfairness rate). We then measure the impact of the biased RC on the TC by comparing RAG with partially biased data against RAG with fully censored data (clean RAG). The difference in fairness scores allows us to quantify how bias in the RC impacts fairness in TC.",
            "Varying fairness relationships across bias categories.  Fig.  6  further illustrates that bias categories such as disability status, age, and religion are more vulnerable to the influence of other biased RCs, as reflected by the predominantly red columns. However, some bias categories exhibit no consistent direction of change, resulting in mixed red and blue scores. Interestingly, we also observe a backfiring phenomenon, where certain categories (e.g., physical appearance and socioeconomic status) become even less biased when the dataset contains unfair samples from unrelated categories. Based on the above observations, we categorize some typical bias types based on their response to biased RCs (as shown in Table  2 ): (1) Vulnerable Categories:  categories where unfairness increases due to biased data from other categories; (2) Passive Categories:  categories showing little or inconsistent change in fairness; (3) Backfiring Categories:  categories where fairness improves (toxicity decreases) when exposed to biased data from other categories. In particular, the backfiring effect may raise from the low correlation between these categories and others. For example, physical appearance and socioeconomic status might be more individualistic, making them less susceptible to biased knowledge retrieved during RAG, allowing responses based primarily on fair knowledge from their original class.",
            "We present a comparison between no RAG and clean RAG based on the Llama series model in Fig  12 . We observe a similar trend to that of the GPT series: even with a fully censored dataset, the fairness of LLMs can still be compromised. In particular, for the PISA dataset, all models demonstrate consistent degradation in fairness after applying clean RAG. However, Llama series models do not show a clear pattern regarding which bias categories are more vulnerable in terms of fairness."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A3.T3.2",
        "footnotes": [],
        "references": [
            "Level 2: fairness risk of partially mitigated datasets (  4.3 ).  While users often focus on addressing well-known and extensively studied biases (e.g., race and gender) in external datasets, our experimental findings indicate that  merely removing these prominent biases does not guarantee fair generation within those categories (Fig.  6 ). Specifically, biased samples from less recognized categories (e.g., nationality) can still adversely affect the fairness of popular bias categories, even when biases from these commonly acknowledged categories have been eliminated. This underscores the need for future research to consider a wider range of bias categories when training or evaluating large language models (LLMs) to create a more robust fairness framework.",
            "This section presents empirical evidence regarding the fairness risks associated with the integration of RAG into LLMs, as discussed in Sec.  3 . We conduct a comprehensive investigation of the fairness implications by designing a robust set of experiments that encompass a variety of NLP tasks, including classification, question answering, and sentence completion. Specifically, Sec. 4.1  details the experimental setup, including the tasks, metrics, and LLMs employed in our study. Following this, Sec.  4.2 , Sec.  4.3 , and Sec.  4.4  analyze the fairness risks posed by RAG-based LLMs, considering different levels of dataset censorship across the various tasks.",
            "Uncensored data significantly degrades fairness.  Figs.  2  and first two sub-figures in Fig.  3  shows the comparison between the No-RAG baseline and RAG-based LLMs across different unfairness rates on three datasets. The results consistently indicate a decline in fairness as the unfairness rate increases, highlighting that higher levels of unfairness in the external dataset lead to more pronounced fairness degradation across most RAG-based LLMs.",
            "Fairness implications vary across task scenarios and model quality.  Fig.  2  and first two sub-figures in Fig.  3  also reveal that fairness degradation patterns differ between LLMs, even within the same task. For instance, GPT series LLMs outperform Llama series LLMs in the generation task (Holistic). However, in the classification task (PISA) and the question-answering task (BBQ), Llama series LLMs exhibit superior fairness across all unfairness rates. This is unexpected, given that GPT series LLMs are typically regarded as more advanced, with better alignment to trustworthiness. To explore this further, we analyzed the accuracy results, as shown in last two sub-figures in Fig.  3 . The findings reveal that Llama models perform significantly worse in terms of accuracy compared to GPT series LLMs. On BBQ, Llama series LLMs achieve less than 50 % percent \\% %  accuracy, performing not much better than random guessing. This suggests that the apparent fairness advantage in Llama series LLMs might stem from their inability to properly understand the questions, leading to random responses rather than informed, fairness-aware decision. Moreover, as shown in Fig.  4 , Llama series LLMs are notably more cautious than GPT series LLMs, often refusing to answer a higher proportion of questions.",
            "Given the practical scenario discussed in Sec.  3.2 , it is critical to assess whether mitigating bias in one specific category is sufficient on its own. More broadly, we explore whether bias in one category (RAG bias category,  RC ) affects fairness in another category (test bias category,  TC ) with RAG-based LLMs. To investigate this, we create partially censored datasets where unfair samples from one RC (with a 1.0 unfairness rate) are combined with fair samples from one TC (with a 0.0 unfairness rate). We then measure the impact of the biased RC on the TC by comparing RAG with partially biased data against RAG with fully censored data (clean RAG). The difference in fairness scores allows us to quantify how bias in the RC impacts fairness in TC.",
            "This section investigates the fairness of LLMs under the premise that users are highly aware of fairness and implement mitigation strategies for both prominent and less prominent bias categories. This scenario, as discussed in Sec.  3.3 , presents significant concerns regarding fairness outcomes. We define fully censored datasets as those with a zero unfairness rate for conducting clean RAG. To assess the implications of clean RAG, we compare the fairness performance of four LLMs operating under clean RAG against those without RAG across the three dataset. Fig.  7  presents the evaluation results for the GPT series LLMs across three datasets, with additional results for Llama series LLMs detailed in Appendix  F . The results reveal that even when using fully censored datasets, LLMs can still experience compromised fairness. Specifically, all LLMs demonstrate consistent fairness degradation on the PISA dataset after the application of clean RAG. Additionally, results from other datasets indicate that the majority of bias categories exhibit differing extents of fairness decline. Notably, categories such as age, socioeconomic status (SES), and gender consistently show reductions in fairness after clean RAG is applied to the GPT series LLMs.",
            "As shown in Fig.  13 , we present the impact of pre-retrieval and post-retrieval strategies on fairness performance in terms of all bias categories. A similar trend is observed as in the main text: the summarizer can alleviate fairness degradation across all bias categories, while reranker and query expansion strategies do not show significant influence on fairness with respect to these categories."
        ]
    },
    "global_footnotes": []
}