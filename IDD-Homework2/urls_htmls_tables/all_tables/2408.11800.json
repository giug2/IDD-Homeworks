{
    "id_table_1": {
        "caption": "Table 1:  Land Cover Types, Coverage, and Composition within the Pleasant Ridge Project Area, Based on National Land Cover Database in May of 2014  Invenergy ( 2014 )",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "We constructed a data extraction and curation pipeline to extract text, image, and table information from wind energy-related documents as depicted in Figure  1 . Utilizing large language model (LLM) based methods such as the  Unstructured.io  tool  Raymond ( 2023 ) , we efficiently extracted information and converted it into JSON elements. These JSON elements were then organized into a schema, creating a page-wise assortment of text, table, and image elements. This structured format ensures that the extracted data is easily accessible and can be accurately referenced during model training and evaluation.",
            "Table  1  shows a table from the report  Invenergy ( 2014 )  and we generate questions from this table as follows."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Performance of the models on the WeQA benchmark scored using the RAGAS framework across evaluators. The \"Prec.\" and \"Rec.\" mean Context Precision and Context Recall respectively, while \"Type\" refers to the Question Type. The best performance for each question type per evaluator is highlighted in bold.",
        "table": "S4.T2.3",
        "footnotes": [],
        "references": [
            "Figure  2  provides an overview of the proposed  LLM  benchmarking framework.  The core of the benchmarking framework is the question generation aspect, where automatic generation of questions forms the foundation.  We combine this with human curation to select high-quality questions, ensuring relevance and clarity.  Corresponding answers to these questions are then validated by humans, establishing a reliable ground truth.  This curated set of questions and validated answers is used to evaluate the responses of other  LLM  s and  RAG  models.",
            "We evaluate three  RAG -based  LLM  s, namely GPT-4, Gemini, and Claude, on our WeQA benchmark. The  RAGAS RAGAS \\mathsf{RAGAS} sansserif_RAGAS  framework is employed for this evaluation, utilizing an evaluator LLM to assess the models performance. The assessment includes metrics such as answer correctness, context precision, and context recall, providing a comprehensive understanding of each models capabilities in retrieving and generating accurate information from the given context. In our case, we have used GPT-4 and Gemini-1.5Pro as choices for the evaluator  LLM  s. Figure  4  presents the answer correctness score, while context precision and context recall depicted in Table  2  show the ability of the models to retrieve the context accurately."
        ]
    },
    "global_footnotes": [
        "This benchmark will be made publicly available."
    ]
}