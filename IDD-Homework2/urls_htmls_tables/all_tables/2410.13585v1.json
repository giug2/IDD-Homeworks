{
    "S5.T1": {
        "caption": "TABLE I: \nComparison to Baseline.\nMulti-camera view recommendation models generalize poorly to a never-before-seen domain.\nID: in-Domain, OOD: out-of-Domain.\n",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T1.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T1.2.2.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Method<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.1.1.1.m1.1\"><semantics id=\"S5.T1.1.1.1.m1.1a\"><mo id=\"S5.T1.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T1.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.1.1.1.m1.1b\"><ci id=\"S5.T1.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.1.1.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T1.1.1.1.m1.1d\">&#8595;</annotation></semantics></math> / Test Set<math alttext=\"\\rightarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.2.2.2.m2.1\"><semantics id=\"S5.T1.2.2.2.m2.1a\"><mo id=\"S5.T1.2.2.2.m2.1.1\" stretchy=\"false\" xref=\"S5.T1.2.2.2.m2.1.1.cmml\">&#8594;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.2.2.2.m2.1b\"><ci id=\"S5.T1.2.2.2.m2.1.1.cmml\" xref=\"S5.T1.2.2.2.m2.1.1\">&#8594;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.2.2.2.m2.1c\">\\rightarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T1.2.2.2.m2.1d\">&#8594;</annotation></semantics></math>\n</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T1.2.2.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">TVMCE (ID)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T1.2.2.4\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Sitcoms (OOD)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.2.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T1.2.3.1.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Random</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T1.2.3.1.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">16.67</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.2.3.1.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">16.67</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T1.2.4.2.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">TC Transformer</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T1.2.4.2.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">28.77&#177;0.86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.2.4.2.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">14.04&#177;1.59</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.2.5.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T1.2.5.3.1\" style=\"padding-left:5.0pt;padding-right:5.0pt;\">Ours</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T1.2.5.3.2\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.5.3.2.1\" style=\"color:#000000;\">31.83&#177;0.83</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.2.5.3.3\" style=\"padding-left:5.0pt;padding-right:5.0pt;\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T1.2.5.3.3.1\">22.65&#177;2.43</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Baseline Comparison.\nA few works have been proposed for multi-camera view recommendation <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.13585v1#bib.bib3\" title=\"\">3</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.13585v1#bib.bib8\" title=\"\">8</a>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.13585v1#bib.bib11\" title=\"\">11</a>]</cite>. Yet, neither their codes nor their datasets are publicly available.\nWe re-implement the Temporal and Contextual Transformer (TC Transformer) in \n[3, 8, 11]. Yet, neither their codes nor their datasets are publicly available.\nWe re-implement the Temporal and Contextual Transformer (TC Transformer) in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.13585v1#bib.bib12\" title=\"\">12</a>]</cite> as the primary baseline we compare to.\nIt achieves state-of-the-art (SoTA) classification accuracy in multi-camera view recommendation on TVMCE dataset.\nNote that we remove the training data in the middle of a shot (without camera switches), which improves the accuracy (22.48 in \n[12] as the primary baseline we compare to.\nIt achieves state-of-the-art (SoTA) classification accuracy in multi-camera view recommendation on TVMCE dataset.\nNote that we remove the training data in the middle of a shot (without camera switches), which improves the accuracy (22.48 in <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.13585v1#bib.bib12\" title=\"\">12</a>]</cite>) of TC Transformer to 28.77.\nAs shown in Table \n[12]) of TC Transformer to 28.77.\nAs shown in Table I, the proposed framework outperforms TC Transformer by 11% (31.83 vs. 28.77).\nHowever, both models suffer when applied to a never-before-seen domain."
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: \nImpact of Video Scene. The model trained in different scenes to the test set achieves lower accuracy.\nGreen, and red means same and different.\n[Best viewed in color.]\n",
        "table": "<span class=\"ltx_tabular ltx_align_middle\" id=\"S5.T2.1.2\">\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S5.T2.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S5.T2.1.2.1.1.1\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">Train Set</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" id=\"S5.T2.1.2.1.1.2\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">Video Scene</span>\n<span class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.1.2.1.1.3\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">Accuracy</span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.1.2.2.2\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.1.2.2.2.1\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">TVMCE_stage</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.1.2.2.2.2\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><span class=\"ltx_text\" id=\"S5.T2.1.2.2.2.2.1\" style=\"color:#00E000;\">stage, concert hall</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.2.2.2.3\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">31.83&#177;0.83</span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.1.2.3.3\">\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.2.3.3.1\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">TVMCE_sport</span>\n<span class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.2.3.3.2\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><span class=\"ltx_text\" id=\"S5.T2.1.2.3.3.2.1\" style=\"color:#FF0000;\">sports</span></span>\n<span class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.2.3.3.3\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">25.56&#177;0.87</span></span>\n<span class=\"ltx_tr\" id=\"S5.T2.1.2.4.4\">\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt\" id=\"S5.T2.1.2.4.4.1\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">ClipShots <span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.2.4.4.1.1\">(Pseudo)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_tt\" id=\"S5.T2.1.2.4.4.2\" style=\"padding-left:10.0pt;padding-right:10.0pt;\"><span class=\"ltx_text\" id=\"S5.T2.1.2.4.4.2.1\" style=\"color:#00E000;\">broad<sup class=\"ltx_sup\" id=\"S5.T2.1.2.4.4.2.1.1\">1</sup></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_tt\" id=\"S5.T2.1.2.4.4.3\" style=\"padding-left:10.0pt;padding-right:10.0pt;\">26.81&#177;0.22</span></span>\n</span>\n</span>\n\n",
        "footnotes": [
            "TVMCE test set \u2014 video scenes: stage, concert hall\n<br class=\"ltx_break\"/>\n\n\n\n\n\nTrain Set\nVideo Scene\nAccuracy\n\nTVMCE_stage\nstage, concert hall\n31.83\u00b10.83\n\nTVMCE_sport\nsports\n25.56\u00b10.87\n\nClipShots (Pseudo)\nbroad<sup class=\"ltx_sup\" id=\"S5.T2.1.2.4.4.2.1.1\">1</sup>\n1\n26.81\u00b10.22\n\n\n<br class=\"ltx_break\"/>\n<sup class=\"ltx_sup\" id=\"S5.T2.1.3\">\n  <span class=\"ltx_text\" id=\"S5.T2.1.3.1\" style=\"font-size:80%;\">1</span>\n</sup>\n1consists of videos from a broad scene coverage."
        ],
        "references": [
            "First, we fix video type and examine the impact of video scene.\nConcretely, we split the TVMCE training set into two roughly equal-sized subsets based on the video scenes.\nOne subset, TVMCE_stage, similar to the test set, consists of videos of stages and concert halls. The other subset, TVMCE_sport, contains sports videos.\nIn Table II, the model trained on the dissimilar scenes achieves significantly lower accuracy than one trained on the same scenes as the test set.",
            "Better accuracy of the model trained on pseudo multi-camera editing datasets \u2014 ClipShots in Table II, and ClipShots and Condensed Movies in Table III demonstrate their efficacy in improving model performance in the target domain without labeling by professionals."
        ]
    },
    "S5.T3": {
        "caption": "TABLE III: \nImpact of Video Scene and Type (Sitcoms).\nA more significant domain difference (video scenes + video types) severely impacts accuracy.\nPseudo datasets from a broad range of videos could cover the target video scenes and types, achieving better accuracy.\nGreen, orange, and red means same, covered, and different.\n[Best viewed in color.]\n",
        "table": "<span class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.1.2\">\n<span class=\"ltx_thead\">\n<span class=\"ltx_tr\" id=\"S5.T3.1.2.1.1\">\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T3.1.2.1.1.1\">Train Set</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T3.1.2.1.1.2\">Video Type</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T3.1.2.1.1.3\">Video Scene</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.2.1.1.4\">Accuracy</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.2.2.2\">\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T3.1.2.2.2.1\">TVMCE</span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T3.1.2.2.2.2\"><span class=\"ltx_text\" id=\"S5.T3.1.2.2.2.2.1\" style=\"color:#FF0000;\">live</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" id=\"S5.T3.1.2.2.2.3\"><span class=\"ltx_text\" id=\"S5.T3.1.2.2.2.3.1\" style=\"color:#FF0000;\">stages, sports</span></span>\n<span class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T3.1.2.2.2.4\">22.65&#177;2.43</span></span>\n</span>\n<span class=\"ltx_tbody\">\n<span class=\"ltx_tr\" id=\"S5.T3.1.2.3.1\">\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T3.1.2.3.1.1\">ClipShots <span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.2.3.1.1.1\">(Pseudo)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T3.1.2.3.1.2\"><span class=\"ltx_text\" id=\"S5.T3.1.2.3.1.2.1\" style=\"color:#FF8000;\">mixed<sup class=\"ltx_sup\" id=\"S5.T3.1.2.3.1.2.1.1\">1</sup></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T3.1.2.3.1.3\"><span class=\"ltx_text\" id=\"S5.T3.1.2.3.1.3.1\" style=\"color:#00E000;\">broad<sup class=\"ltx_sup\" id=\"S5.T3.1.2.3.1.3.1.1\">2</sup></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.3.1.4\">27.61&#177;1.20</span></span>\n<span class=\"ltx_tr\" id=\"S5.T3.1.2.4.2\">\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T3.1.2.4.2.1\">Condensed <span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.2.4.2.1.1\">(Pseudo)</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T3.1.2.4.2.2\"><span class=\"ltx_text\" id=\"S5.T3.1.2.4.2.2.1\" style=\"color:#00E000;\">movie</span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T3.1.2.4.2.3\"><span class=\"ltx_text\" id=\"S5.T3.1.2.4.2.3.1\" style=\"color:#00E000;\">broad<sup class=\"ltx_sup\" id=\"S5.T3.1.2.4.2.3.1.1\">2</sup></span></span>\n<span class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.2.4.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.2.4.2.4.1\">38.14&#177;1.50</span></span></span>\n</span>\n</span>\n\n",
        "footnotes": [
            "Test set \u2014 video type: movie, video scenes: living room, kitchen\n<br class=\"ltx_break\"/>\n\n\n\n\n\nTrain Set\nVideo Type\nVideo Scene\nAccuracy\n\nTVMCE\nlive\nstages, sports\n22.65\u00b12.43\n\n\n\nClipShots (Pseudo)\nmixed<sup class=\"ltx_sup\" id=\"S5.T3.1.2.3.1.2.1.1\">1</sup>\n1\nbroad<sup class=\"ltx_sup\" id=\"S5.T3.1.2.3.1.3.1.1\">2</sup>\n2\n27.61\u00b11.20\n\nCondensed (Pseudo)\nmovie\nbroad<sup class=\"ltx_sup\" id=\"S5.T3.1.2.4.2.3.1.1\">2</sup>\n2\n38.14\u00b11.50\n\n\n<br class=\"ltx_break\"/>\n<sup class=\"ltx_sup\" id=\"S5.T3.1.3\">\n  <span class=\"ltx_text\" id=\"S5.T3.1.3.1\" style=\"font-size:80%;\">1</span>\n</sup>\n1consists of videos of movies and live performance.",
            "<sup class=\"ltx_sup\" id=\"S5.T3.2.1\">\n  <span class=\"ltx_text\" id=\"S5.T3.2.1.1\" style=\"font-size:80%;\">2</span>\n</sup>\n2consists of videos from a broad scene coverage."
        ],
        "references": [
            "This paper proposes a methodology for generating pseudo-labeled multi-camera editing data from regular videos with shots, alleviating data scarcity.\nWith the proposed approach, sufficient data on a given domain (e.g., movie scenes in living rooms) could be obtained.\nOur insights stem from two observations. (1) Many shot transitions within a regular video result from camera switches<sup class=\"ltx_note_mark\">1</sup>\n1<sup class=\"ltx_note_mark\">1</sup>\n11Other transitions such as video effect and trimming could be filtered by heuristics or treated as noises..\nNamely, videos are edited from their original multi-camera videos.\n(2) In a multiple-camera system, cameras often remain stationary (extrinsics and intrinsics) and are usually responsible for shots of different scales.\nBased on these two insights, we perform clustering on shots in a video to simulate different cameras and select the most visually similar shot from each camera as candidates alongside the ground truth to generate pseudo-labeled data.\nA model trained on the proposed dataset enjoys a significant improvement in accuracy in the target domain (22.65 vs. 38.14, cf. Table III).\nOur contributions are summarized as follows:",
            "With the proposed pseudo-labeled multi-camera editing datasets, we achieve a 68% relative improvement in the model\u2019s classification accuracy in the target domain. (cf. Table III).",
            "Next, we investigate the impact of both video scenes and video types.\nIn this experiment, all models are evaluated on Sitcoms, consisting of videos that differ in both scenes and types from those in TVMCE.\nSpecifically, video scenes in TVMCE are mostly stages and sports, whereas video scenes in Sitcoms are kitchens and living rooms. Also, TVMCE contains recorded live performances, whereas Sitcoms contains movies.\nThree models are evaluated, each trained on a dataset with different degrees of domain difference to Sitcoms. (1) TVMCE that differs in both video type and scenes, (2) ClipShots differs only in video type, and (3) Condensed Movies that covers the same video type and scenes.\nTwo observations can be made in Table III.\n(1) Pseudo datasets bridge the data scarcity and improve the accuracy in the target domain.\n(2) Video type and scene can compound to impact performance. TVMCE, different in both video type and scenes from Sitcoms, performs the worst.\nOn the contrary, Condensed Movies with the same video type and scenes as Sitcoms achieves the best performance.",
            "Better accuracy of the model trained on pseudo multi-camera editing datasets \u2014 ClipShots in Table II, and ClipShots and Condensed Movies in Table III demonstrate their efficacy in improving model performance in the target domain without labeling by professionals."
        ]
    }
}