{
    "PAPER'S NUMBER OF TABLES": 7,
    "S1.35": {
        "caption": "",
        "table": "",
        "footnotes": "NOMENCLATURE LIST \n\n\n\n\n\n\nx\nData sample\nyğ‘¦y\nLabel\n\nğ’Ÿisubscriptğ’Ÿğ‘–\\mathcal{D}_{i}\nClient iğ‘–i dataset\nğ’Ÿi,csubscriptğ’Ÿğ‘–ğ‘\\mathcal{D}_{i,c}\nğ’Ÿisubscriptğ’Ÿğ‘–\\mathcal{D}_{i} w/ label cğ‘c\n\nğ’Ÿğ’Ÿ\\mathcal{D}\nGlobal dataset\nCğ¶C\nTotal class number\n\nnisubscriptğ‘›ğ‘–n_{i}\nClient iğ‘–i sample number\nnğ‘›n\nTotal sample number\n\nNğ‘N\nTotal client number\nâ„’â€‹(â‹…)â„’â‹…\\mathcal{L}(\\cdot)\nGlobal loss\n\nâ„’supâ€‹(â‹…)subscriptâ„’supâ‹…\\mathcal{L}_{{\\rm sup}}(\\cdot)\nSupervised loss\nâ„’iâ€‹(â‹…)subscriptâ„’ğ‘–â‹…\\mathcal{L}_{i}(\\cdot)\nClient iğ‘–i loss\n\nğ°ğ°\\mathbf{w}\nGlobal model\nğ°isubscriptğ°ğ‘–\\mathbf{w}_{i}\nClient iğ‘–i model\n\nğœ½ğœ½\\bm{\\theta}\nFeature extractor\nğ¡ğ¡\\mathbf{h}\nFeature\n\nfğœ½â€‹(â‹…)subscriptğ‘“ğœ½â‹…f_{\\bm{\\theta}}(\\cdot)\nForward function of ğœ½ğœ½\\bm{\\theta}\nÏ•bold-italic-Ï•\\bm{\\phi}\nLinear classifier\n\nfÏ•â€‹(â‹…)subscriptğ‘“bold-italic-Ï•â‹…f_{\\bm{\\phi}}(\\cdot)\nForward function of Ï•bold-italic-Ï•\\bm{\\phi}\npâ‹…,csubscriptğ‘â‹…ğ‘p_{\\cdot,c}\nclassifier output on class cğ‘c\n\nÎ”Ï•subscriptÎ”bold-italic-Ï•\\Delta_{\\bm{\\phi}}\nClassifier update\nÎ”ğ¡subscriptÎ”ğ¡\\Delta_{\\mathbf{h}}\nFeature update\n\nğšcsubscriptğšğ‘\\mathbf{a}_{c}\nClass cğ‘c feature anchor\nğ¦c,â‹…subscriptğ¦ğ‘â‹…\\mathbf{m}_{c,\\cdot}\nClass cğ‘c anchor momentum\n\n",
        "references": [
            "According to [34], existing works may not provide stable better performance gains over FedAvg[1] in classification tasks, which motivates us to analyze the relationship between classifier updates and features in local training.\nWe find that existing methods ignore the inherent relationship (i.e., a vicious cycle) between these two updates and then still suffer from either feature inconsistency or classifier divergence.\nDifferent from these methods, our method breaks the vicious cycle by taking feature anchors to align both feature and classifier updates across clients.\nMoreover, our method addresses both label and feature distribution skew, unlike others that only tackle label distribution skew and improve the performance of federated learning under both skews.",
            "This work aims at image classification tasks under label and feature distribution skews, and it uses federated benchmark datasets as [1, 42, 7], including EMNIST[43], FMNIST, CIFAR-10, CIFAR-100 [44], and Mixed Digits dataset [36].\nSpecifically, for label distribution skew, we consider two settings: (i) Same size of local dataset: following [1], we split data samples based on class to clients (e.g., #â€‹C=2#ğ¶2\\#C=2 denotes that each client holds two class samples);\n(ii) Different sizes of local dataset: following [42], we set Î±ğ›¼\\alpha of Dirichlet distribution Dâ€‹iâ€‹râ€‹(Î±)ğ·ğ‘–ğ‘Ÿğ›¼Dir(\\alpha) as 0.1 and 0.5 to generate distribution pi,csubscriptğ‘ğ‘–ğ‘p_{i,c} by which the cğ‘c-th class samples are split to client iğ‘–i.\nFor feature distribution skew, we consider two settings:\n(i) Real-world feature skew: we sample a subset with 10 classes of a real-world dataset EMNIST with natural feature skew;\n(ii) Artificial feature skew: we use a mixed-digit dataset from [36]\nconsisting of MNIST[45], SVHN[46], USPS[47], SynthDigits and MNIST-M[48].\nIn Table II and Table IV, we test the top-1 accuracy based on the global model, except for Mixed Digits where we report the average top-1 accuracy on five-benchmark digit datasets.",
            "In Tables I and II, 100 clients attend federated training, 10 clients participate in each round, the local batch size is 64, the local epochs number is 5, and the targeted communication round is 200.\nWe use the SGD optimizer with a 0.01 learning rate and 0.001 weight decay for all experiments.\nFurthermore, in Table III and Figures 3, we follow the setups of [14] to investigate the impact of different federated setups with 200 rounds and a local SGD with a 0.01 learning rate and 0.9 momentum.\nAll experiments are performed based on PyTorch and one node of the High-Performance Computing platform with 4 NVIDIA A30 Tensor Core GPUs with 24GB.",
            "Table I shows that FedFA provides significant gains in different label-skew settings regardless of the dataset.\nCompared with Î±=0.5ğ›¼0.5\\alpha=0.5, both #â€‹C=2#ğ¶2\\#C=2 and Î±=0.1ğ›¼0.1\\alpha=0.1 indicate more severe label distribution skew, but clients under #â€‹C=2#ğ¶2\\#C=2 have the same sample number while the ones with Î±=0.1ğ›¼0.1\\alpha=0.1 do not.\nFirstly, we find that the performance of all methods degrades as the degree of data heterogeneity increases.\nNevertheless, the decline of FedFA is much smaller than that of other methods.\nFor example, when Î±ğ›¼\\alpha changes from 0.50.50.5 to 0.10.10.1, the top-1 accuracy of all the baselines goes down by about 13%percent1313\\% on FMNIST and CIFAR-10, which is twice as large as FedFA.\nSecondly, under the same label skew, FedFA achieves larger gains over other methods when label distribution skew becomes more severe, up to 18.06%percent18.0618.06\\% (i.e., MOON: 34.89%percent34.8934.89\\% and FedFA 52.95%percent52.9552.95\\% under Î±=0.1ğ›¼0.1\\alpha=0.1 in CIFAR-10).\nThirdly, to explore more difficult tasks, we test on CIFAR-100 with ResNet18, and our method still achieves the best performance (i.e., about 3%percent33\\% accuracy advance).\nIt is important to note that FedDyn exhibits unstable performance compared to other methods when considering the same hyperparameter setups.\nFor instance, it shows significantly low accuracy in the CIFAR-100 task.\nThis is due to the fact that FedDyn is much more sensitive to hyperparameters.",
            "According to Table II, our method obtains higher accuracy than all baselines on EMNIST and Mixed Digits.\nSpecifically, the accuracy of FedFA in EMNIST reaches 99.28%percent99.2899.28\\%, which is 0.77%percent0.770.77\\% higher than the best baseline (i.e., MOON 98.51%percent98.5198.51\\%).\nMoreover, we split each digit dataset of Mixed Digits into 20 subsets, one for each client with the same sample number (i.e., a skewed feature distribution exists between the clients with a subset of SVHN and the ones with a subset of MNIST).\nCompared with the best baseline (Feddyn: 83.59%percent83.5983.59\\%) on Mixed Digits, our method achieves performance gains by 7.14%percent7.147.14\\%.",
            "We combine label skew and feature skew to explore the impact of data heterogeneity further.\nNamely, we not only split each dataset in Mixed Digits into 20 subsets, one for each client but also set different label distributions for various clients (i.e., clients are subject to at least one of label distribution skew and feature distribution skew).\nThe results in Table II show that all the methods are more susceptible under this setting than that of feature distribution skew.\nFor example, the most significant performance drop reaches 31.93%percent31.9331.93\\% (i.e., FedDyn from 83.59%percent83.5983.59\\% to 51.66%percent51.6651.66\\% under #â€‹C=2#ğ¶2\\#C=2).\nNevertheless, FedFA significantly mitigates this performance degradation with a mild decrease from 90.73%percent90.7390.73\\% to 83.46%percent83.4683.46\\%.\nMeanwhile, FedFA maintains at least 10%percent1010\\% performance advantage over all baselines under this case, with the largest gap reaching 31.8%percent31.831.8\\% (i.e., FedFA from 83.59%percent83.5983.59\\% to FedDyn 51.66%percent51.6651.66\\% under #â€‹C=2#ğ¶2\\#C=2).",
            "We compare our method with FedAvg under more homogeneous data and take the same learning rate of this case as that of data heterogeneity for comparison, where the results are reported in Table I and Table II.\nThe results demonstrate that FedFA still brings a significant advance in the presence of data homogeneity.\nFor example, FedFA is 8.64%percent8.648.64\\% more accurate than FedAvg on CIFAR-100.\nIncredibly, FedFA under mild data heterogeneity (e.g., Î±=0.5ğ›¼0.5\\alpha=0.5 in FMNIST or Mixed Digits) even obtains higher accuracy than FedAvg without any label or feature skew (e.g., FedFA: 88.40%percent88.4088.40\\% vs. FedAvg: 85.90%percent85.9085.90\\% in FMNIST).\nThis reveals that the effect of data heterogeneity on federated learning deserves to be further explored.",
            "Following the setup of Table III, we further explore the impact of federated setups.\nAs shown in Figure 3(a), a larger client sample rate achieves better test accuracy for all methods. Especially, the accuracy gains (about 10%percent1010\\%) when increasing the sample rate from 0.10.10.1 to 0.30.30.3 is much larger than that from 0.30.30.3 to 0.50.50.5.\nAs shown in Figure 3(b), larger local epochs have a negative impact on performance, but FedProx and MOON have worse performance degradation than FedAvg and FedFA.\nBesides, the performance advantage of FedFA under various batch sizes and client numbers is shown in Figures 9 and 10.\nOverall, our method FedFA consistently achieves better than all baselines under different setups.",
            "In [50, 51], it has been found that the local optimizer with momentum is more robust to different smoothness of loss to improve generalization.\nWe explore the effect of FedFA on the Lipschitzness of loss by comparing local SGD optimizers with or without momentum on CIFAR-10.\nNote that FedDyd and FedProc are not compatible with local SGD with momentum, and thus Table III and Figure 3 do not include their results.\nComparing Table I with Table III, all methods with momentum work better than that without momentum, but FedFA without momentum has superior performance than baselines with momentum under #â€‹C=2#ğ¶2\\#C=2 and Î±=0.1ğ›¼0.1\\alpha=0.1 (e.g., FedFA without momentum: 52.95%percent\\% vs. FedProx with momentum: 49.87%percent\\% under Î±=0.1ğ›¼0.1\\alpha=0.1).\nAs expected by Theorem 4, a smoother loss of FedFA achieves better generalization.",
            "As shown in Table IV, we conduct ablation studies on FedFA without anchor updating in (4) and FedFA without classifier calibration in (5) to give an intuition of FedFA performance.\nOn the one side, feature anchors can be fixed during federated training (i.e., the client would not aggregate any information into feature anchors, which would not bring potential privacy leakage).\nFedFA performs better than the best baseline under label and feature skew.\nMeanwhile, the anchor updating brings consistent performance benefits (i.e., at least around 2%percent22\\% boost) since the updated anchors keep more representative in the shared feature space across clients.\nOn the other hand, classifier calibration plays the most crucial role in FedFA because data heterogeneity induces a low classifier update similarity as observed in Figure 2.\nFor instance, classifier calibration boosts performance by 22.37%percent22.3722.37\\% in the case combined by label skew and feature skew.\nOverall, both feature alignment and classifier calibration play an essential role in FedFA to overcome data heterogeneity.",
            "We experiment on the FMNIST with different momentum coefficients (Î»ğœ†\\lambda) of feature anchor updates with the label-skew case of #â€‹C=2#ğ¶2\\#C=2.\nThis experiment setting is the same as Table I.\nSpecifically, when Î»=1ğœ†1\\lambda=1, feature anchors will not be updated; when Î»=0ğœ†0\\lambda=0, feature anchors will be set as the mean feature of the last epoch.\nFigure 4(a) shows that the performance of FedFA with different Î»ğœ†\\lambda is similar.\nMeanwhile, although FedFA with Î»=0ğœ†0\\lambda=0 introduces more oscillations during training, it performs similarly to other cases.\nThis means that FedFA is not sensitive to the momentum coefficient Î»ğœ†\\lambda.",
            "Compared with [19] that calibrates classifier with virtual representation (CCVR) after training, we perform it\nduring different phases of training and the setting of Table V is the same as Table I.\nThe result of each mini-batch calibration done by FedFA is the best in all cases.\nThis reveals that the classifier divergence induced by data heterogeneity should be corrected as early as possible.\nMeanwhile, maintaining the virtuous cycle between feature and classifier updates during local training helps the final model converge at a point that generalizes better, since compared with FedFA without classifier calibration, FedFA with classifier calibration after training only improves little.",
            "Firstly, feature anchors can be fixed/without updates during federated training (i.e., the client would not aggregate any information into feature anchors, which would not bring potential privacy leakage) because of the powerful representation of neural networks. That is, the fixed anchors in FedFA specified a feature space and a classifier between clients before training. The third row of Table IV shows better results of experiments of FedFA without feature-anchor updates than the best baseline under label and feature skew even though the performance goes down about 2%percent22\\% than FedFA with anchor updates. Therefore, there is a trade-off between privacy and generalization performance for FedFA.\nTherefore, there is a trade-off between privacy and generalization performance for FedFA.",
            "Our validation and test experiments, including label distribution skew, feature distribution skew, and label &\\& feature distribution skews, use the models according to Table VI.\nHerein, to ablate the effect of BN layers, we\nreplace the BN layer with the GroupNorm layer in all experiments.\nFor fair comparison, our models follow those reported in the baselinesâ€™ works.\nSpecifically, following [18], we use a CNN model for EMNIST, FMNIST, and CIFAR-10, consisting of\ntwo 5x5 convolution layers followed by 2x2 max pooling and two fully-connected layers with ReLU activation.\nFollowing [14] and [36], we utilize the ResNet-18 [49] with a linear projector for CIFAR-100 and a CNN model with three 5x5 convolution layers followed by five GroupNorm layers for the Mixed Digits dataset."
        ]
    },
    "S6.T1": {
        "caption": "TABLE I: The top-1 accuracy of FedFA and all the baselines under label distribution skew on the test datasets. We run three trials and report the mean and standard deviation. For FedAvg and FedFA, we also report their top-1 accuracy without label skew.",
        "table": "<table id=\"S6.T1.9.9\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T1.9.9.10.1\" class=\"ltx_tr\">\n<th id=\"S6.T1.9.9.10.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"3\"><span id=\"S6.T1.9.9.10.1.1.1\" class=\"ltx_text\">\n<span id=\"S6.T1.9.9.10.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S6.T1.9.9.10.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S6.T1.9.9.10.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Method</span></span>\n<span id=\"S6.T1.9.9.10.1.1.1.1.2\" class=\"ltx_tr\">\n<span id=\"S6.T1.9.9.10.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(lr = 0.01)</span></span>\n</span></span></th>\n<td id=\"S6.T1.9.9.10.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"9\">Label Distribution Skew</td>\n</tr>\n<tr id=\"S6.T1.9.9.11.2\" class=\"ltx_tr\">\n<td id=\"S6.T1.9.9.11.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">FMNIST</td>\n<td id=\"S6.T1.9.9.11.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">CIFAR-10</td>\n<td id=\"S6.T1.9.9.11.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">CIFAR-100</td>\n</tr>\n<tr id=\"S6.T1.9.9.9\" class=\"ltx_tr\">\n<td id=\"S6.T1.1.1.1.1\" class=\"ltx_td ltx_align_center\"><math id=\"S6.T1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\#C=2\" display=\"inline\"><semantics id=\"S6.T1.1.1.1.1.m1.1a\"><mrow id=\"S6.T1.1.1.1.1.m1.1.1\" xref=\"S6.T1.1.1.1.1.m1.1.1.cmml\"><mrow id=\"S6.T1.1.1.1.1.m1.1.1.2\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.cmml\"><mi mathvariant=\"normal\" id=\"S6.T1.1.1.1.1.m1.1.1.2.2\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.2.cmml\">#</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S6.T1.1.1.1.1.m1.1.1.2.1\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.1.cmml\">â€‹</mo><mi id=\"S6.T1.1.1.1.1.m1.1.1.2.3\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.3.cmml\">C</mi></mrow><mo id=\"S6.T1.1.1.1.1.m1.1.1.1\" xref=\"S6.T1.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T1.1.1.1.1.m1.1.1.3\" xref=\"S6.T1.1.1.1.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.1.1.1.1.m1.1b\"><apply id=\"S6.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1\"><eq id=\"S6.T1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.1\"></eq><apply id=\"S6.T1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.2\"><times id=\"S6.T1.1.1.1.1.m1.1.1.2.1.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.1\"></times><ci id=\"S6.T1.1.1.1.1.m1.1.1.2.2.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.2\">#</ci><ci id=\"S6.T1.1.1.1.1.m1.1.1.2.3.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.2.3\">ğ¶</ci></apply><cn type=\"integer\" id=\"S6.T1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S6.T1.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.1.1.1.1.m1.1c\">\\#C=2</annotation></semantics></math></td>\n<td id=\"S6.T1.2.2.2.2\" class=\"ltx_td ltx_align_center\"><math id=\"S6.T1.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.1\" display=\"inline\"><semantics id=\"S6.T1.2.2.2.2.m1.1a\"><mrow id=\"S6.T1.2.2.2.2.m1.1.1\" xref=\"S6.T1.2.2.2.2.m1.1.1.cmml\"><mi id=\"S6.T1.2.2.2.2.m1.1.1.2\" xref=\"S6.T1.2.2.2.2.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T1.2.2.2.2.m1.1.1.1\" xref=\"S6.T1.2.2.2.2.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T1.2.2.2.2.m1.1.1.3\" xref=\"S6.T1.2.2.2.2.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.2.2.2.2.m1.1b\"><apply id=\"S6.T1.2.2.2.2.m1.1.1.cmml\" xref=\"S6.T1.2.2.2.2.m1.1.1\"><eq id=\"S6.T1.2.2.2.2.m1.1.1.1.cmml\" xref=\"S6.T1.2.2.2.2.m1.1.1.1\"></eq><ci id=\"S6.T1.2.2.2.2.m1.1.1.2.cmml\" xref=\"S6.T1.2.2.2.2.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T1.2.2.2.2.m1.1.1.3.cmml\" xref=\"S6.T1.2.2.2.2.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.2.2.2.2.m1.1c\">\\alpha=0.1</annotation></semantics></math></td>\n<td id=\"S6.T1.3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><math id=\"S6.T1.3.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.5\" display=\"inline\"><semantics id=\"S6.T1.3.3.3.3.m1.1a\"><mrow id=\"S6.T1.3.3.3.3.m1.1.1\" xref=\"S6.T1.3.3.3.3.m1.1.1.cmml\"><mi id=\"S6.T1.3.3.3.3.m1.1.1.2\" xref=\"S6.T1.3.3.3.3.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T1.3.3.3.3.m1.1.1.1\" xref=\"S6.T1.3.3.3.3.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T1.3.3.3.3.m1.1.1.3\" xref=\"S6.T1.3.3.3.3.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.3.3.3.3.m1.1b\"><apply id=\"S6.T1.3.3.3.3.m1.1.1.cmml\" xref=\"S6.T1.3.3.3.3.m1.1.1\"><eq id=\"S6.T1.3.3.3.3.m1.1.1.1.cmml\" xref=\"S6.T1.3.3.3.3.m1.1.1.1\"></eq><ci id=\"S6.T1.3.3.3.3.m1.1.1.2.cmml\" xref=\"S6.T1.3.3.3.3.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T1.3.3.3.3.m1.1.1.3.cmml\" xref=\"S6.T1.3.3.3.3.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.3.3.3.3.m1.1c\">\\alpha=0.5</annotation></semantics></math></td>\n<td id=\"S6.T1.4.4.4.4\" class=\"ltx_td ltx_align_center\"><math id=\"S6.T1.4.4.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\#C=2\" display=\"inline\"><semantics id=\"S6.T1.4.4.4.4.m1.1a\"><mrow id=\"S6.T1.4.4.4.4.m1.1.1\" xref=\"S6.T1.4.4.4.4.m1.1.1.cmml\"><mrow id=\"S6.T1.4.4.4.4.m1.1.1.2\" xref=\"S6.T1.4.4.4.4.m1.1.1.2.cmml\"><mi mathvariant=\"normal\" id=\"S6.T1.4.4.4.4.m1.1.1.2.2\" xref=\"S6.T1.4.4.4.4.m1.1.1.2.2.cmml\">#</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S6.T1.4.4.4.4.m1.1.1.2.1\" xref=\"S6.T1.4.4.4.4.m1.1.1.2.1.cmml\">â€‹</mo><mi id=\"S6.T1.4.4.4.4.m1.1.1.2.3\" xref=\"S6.T1.4.4.4.4.m1.1.1.2.3.cmml\">C</mi></mrow><mo id=\"S6.T1.4.4.4.4.m1.1.1.1\" xref=\"S6.T1.4.4.4.4.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T1.4.4.4.4.m1.1.1.3\" xref=\"S6.T1.4.4.4.4.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.4.4.4.4.m1.1b\"><apply id=\"S6.T1.4.4.4.4.m1.1.1.cmml\" xref=\"S6.T1.4.4.4.4.m1.1.1\"><eq id=\"S6.T1.4.4.4.4.m1.1.1.1.cmml\" xref=\"S6.T1.4.4.4.4.m1.1.1.1\"></eq><apply id=\"S6.T1.4.4.4.4.m1.1.1.2.cmml\" xref=\"S6.T1.4.4.4.4.m1.1.1.2\"><times id=\"S6.T1.4.4.4.4.m1.1.1.2.1.cmml\" xref=\"S6.T1.4.4.4.4.m1.1.1.2.1\"></times><ci id=\"S6.T1.4.4.4.4.m1.1.1.2.2.cmml\" xref=\"S6.T1.4.4.4.4.m1.1.1.2.2\">#</ci><ci id=\"S6.T1.4.4.4.4.m1.1.1.2.3.cmml\" xref=\"S6.T1.4.4.4.4.m1.1.1.2.3\">ğ¶</ci></apply><cn type=\"integer\" id=\"S6.T1.4.4.4.4.m1.1.1.3.cmml\" xref=\"S6.T1.4.4.4.4.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.4.4.4.4.m1.1c\">\\#C=2</annotation></semantics></math></td>\n<td id=\"S6.T1.5.5.5.5\" class=\"ltx_td ltx_align_center\"><math id=\"S6.T1.5.5.5.5.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.1\" display=\"inline\"><semantics id=\"S6.T1.5.5.5.5.m1.1a\"><mrow id=\"S6.T1.5.5.5.5.m1.1.1\" xref=\"S6.T1.5.5.5.5.m1.1.1.cmml\"><mi id=\"S6.T1.5.5.5.5.m1.1.1.2\" xref=\"S6.T1.5.5.5.5.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T1.5.5.5.5.m1.1.1.1\" xref=\"S6.T1.5.5.5.5.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T1.5.5.5.5.m1.1.1.3\" xref=\"S6.T1.5.5.5.5.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.5.5.5.5.m1.1b\"><apply id=\"S6.T1.5.5.5.5.m1.1.1.cmml\" xref=\"S6.T1.5.5.5.5.m1.1.1\"><eq id=\"S6.T1.5.5.5.5.m1.1.1.1.cmml\" xref=\"S6.T1.5.5.5.5.m1.1.1.1\"></eq><ci id=\"S6.T1.5.5.5.5.m1.1.1.2.cmml\" xref=\"S6.T1.5.5.5.5.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T1.5.5.5.5.m1.1.1.3.cmml\" xref=\"S6.T1.5.5.5.5.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.5.5.5.5.m1.1c\">\\alpha=0.1</annotation></semantics></math></td>\n<td id=\"S6.T1.6.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r\"><math id=\"S6.T1.6.6.6.6.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.5\" display=\"inline\"><semantics id=\"S6.T1.6.6.6.6.m1.1a\"><mrow id=\"S6.T1.6.6.6.6.m1.1.1\" xref=\"S6.T1.6.6.6.6.m1.1.1.cmml\"><mi id=\"S6.T1.6.6.6.6.m1.1.1.2\" xref=\"S6.T1.6.6.6.6.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T1.6.6.6.6.m1.1.1.1\" xref=\"S6.T1.6.6.6.6.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T1.6.6.6.6.m1.1.1.3\" xref=\"S6.T1.6.6.6.6.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.6.6.6.6.m1.1b\"><apply id=\"S6.T1.6.6.6.6.m1.1.1.cmml\" xref=\"S6.T1.6.6.6.6.m1.1.1\"><eq id=\"S6.T1.6.6.6.6.m1.1.1.1.cmml\" xref=\"S6.T1.6.6.6.6.m1.1.1.1\"></eq><ci id=\"S6.T1.6.6.6.6.m1.1.1.2.cmml\" xref=\"S6.T1.6.6.6.6.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T1.6.6.6.6.m1.1.1.3.cmml\" xref=\"S6.T1.6.6.6.6.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.6.6.6.6.m1.1c\">\\alpha=0.5</annotation></semantics></math></td>\n<td id=\"S6.T1.7.7.7.7\" class=\"ltx_td ltx_align_center\"><math id=\"S6.T1.7.7.7.7.m1.1\" class=\"ltx_Math\" alttext=\"\\#C=20\" display=\"inline\"><semantics id=\"S6.T1.7.7.7.7.m1.1a\"><mrow id=\"S6.T1.7.7.7.7.m1.1.1\" xref=\"S6.T1.7.7.7.7.m1.1.1.cmml\"><mrow id=\"S6.T1.7.7.7.7.m1.1.1.2\" xref=\"S6.T1.7.7.7.7.m1.1.1.2.cmml\"><mi mathvariant=\"normal\" id=\"S6.T1.7.7.7.7.m1.1.1.2.2\" xref=\"S6.T1.7.7.7.7.m1.1.1.2.2.cmml\">#</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S6.T1.7.7.7.7.m1.1.1.2.1\" xref=\"S6.T1.7.7.7.7.m1.1.1.2.1.cmml\">â€‹</mo><mi id=\"S6.T1.7.7.7.7.m1.1.1.2.3\" xref=\"S6.T1.7.7.7.7.m1.1.1.2.3.cmml\">C</mi></mrow><mo id=\"S6.T1.7.7.7.7.m1.1.1.1\" xref=\"S6.T1.7.7.7.7.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T1.7.7.7.7.m1.1.1.3\" xref=\"S6.T1.7.7.7.7.m1.1.1.3.cmml\">20</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.7.7.7.7.m1.1b\"><apply id=\"S6.T1.7.7.7.7.m1.1.1.cmml\" xref=\"S6.T1.7.7.7.7.m1.1.1\"><eq id=\"S6.T1.7.7.7.7.m1.1.1.1.cmml\" xref=\"S6.T1.7.7.7.7.m1.1.1.1\"></eq><apply id=\"S6.T1.7.7.7.7.m1.1.1.2.cmml\" xref=\"S6.T1.7.7.7.7.m1.1.1.2\"><times id=\"S6.T1.7.7.7.7.m1.1.1.2.1.cmml\" xref=\"S6.T1.7.7.7.7.m1.1.1.2.1\"></times><ci id=\"S6.T1.7.7.7.7.m1.1.1.2.2.cmml\" xref=\"S6.T1.7.7.7.7.m1.1.1.2.2\">#</ci><ci id=\"S6.T1.7.7.7.7.m1.1.1.2.3.cmml\" xref=\"S6.T1.7.7.7.7.m1.1.1.2.3\">ğ¶</ci></apply><cn type=\"integer\" id=\"S6.T1.7.7.7.7.m1.1.1.3.cmml\" xref=\"S6.T1.7.7.7.7.m1.1.1.3\">20</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.7.7.7.7.m1.1c\">\\#C=20</annotation></semantics></math></td>\n<td id=\"S6.T1.8.8.8.8\" class=\"ltx_td ltx_align_center\"><math id=\"S6.T1.8.8.8.8.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.1\" display=\"inline\"><semantics id=\"S6.T1.8.8.8.8.m1.1a\"><mrow id=\"S6.T1.8.8.8.8.m1.1.1\" xref=\"S6.T1.8.8.8.8.m1.1.1.cmml\"><mi id=\"S6.T1.8.8.8.8.m1.1.1.2\" xref=\"S6.T1.8.8.8.8.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T1.8.8.8.8.m1.1.1.1\" xref=\"S6.T1.8.8.8.8.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T1.8.8.8.8.m1.1.1.3\" xref=\"S6.T1.8.8.8.8.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.8.8.8.8.m1.1b\"><apply id=\"S6.T1.8.8.8.8.m1.1.1.cmml\" xref=\"S6.T1.8.8.8.8.m1.1.1\"><eq id=\"S6.T1.8.8.8.8.m1.1.1.1.cmml\" xref=\"S6.T1.8.8.8.8.m1.1.1.1\"></eq><ci id=\"S6.T1.8.8.8.8.m1.1.1.2.cmml\" xref=\"S6.T1.8.8.8.8.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T1.8.8.8.8.m1.1.1.3.cmml\" xref=\"S6.T1.8.8.8.8.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.8.8.8.8.m1.1c\">\\alpha=0.1</annotation></semantics></math></td>\n<td id=\"S6.T1.9.9.9.9\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S6.T1.9.9.9.9.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.5\" display=\"inline\"><semantics id=\"S6.T1.9.9.9.9.m1.1a\"><mrow id=\"S6.T1.9.9.9.9.m1.1.1\" xref=\"S6.T1.9.9.9.9.m1.1.1.cmml\"><mi id=\"S6.T1.9.9.9.9.m1.1.1.2\" xref=\"S6.T1.9.9.9.9.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T1.9.9.9.9.m1.1.1.1\" xref=\"S6.T1.9.9.9.9.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T1.9.9.9.9.m1.1.1.3\" xref=\"S6.T1.9.9.9.9.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T1.9.9.9.9.m1.1b\"><apply id=\"S6.T1.9.9.9.9.m1.1.1.cmml\" xref=\"S6.T1.9.9.9.9.m1.1.1\"><eq id=\"S6.T1.9.9.9.9.m1.1.1.1.cmml\" xref=\"S6.T1.9.9.9.9.m1.1.1.1\"></eq><ci id=\"S6.T1.9.9.9.9.m1.1.1.2.cmml\" xref=\"S6.T1.9.9.9.9.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T1.9.9.9.9.m1.1.1.3.cmml\" xref=\"S6.T1.9.9.9.9.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T1.9.9.9.9.m1.1c\">\\alpha=0.5</annotation></semantics></math></td>\n</tr>\n<tr id=\"S6.T1.9.9.12.3\" class=\"ltx_tr\">\n<th id=\"S6.T1.9.9.12.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedAvg w/o skew</th>\n<td id=\"S6.T1.9.9.12.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">85.90(0.14)</td>\n<td id=\"S6.T1.9.9.12.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">59.66(0.05)</td>\n<td id=\"S6.T1.9.9.12.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">25.37(0.28)</td>\n</tr>\n<tr id=\"S6.T1.9.9.13.4\" class=\"ltx_tr\">\n<th id=\"S6.T1.9.9.13.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">FedFA w/o skew</th>\n<td id=\"S6.T1.9.9.13.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\"><span id=\"S6.T1.9.9.13.4.2.1\" class=\"ltx_text ltx_font_bold\">89.67(0.16)</span></td>\n<td id=\"S6.T1.9.9.13.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\" colspan=\"3\"><span id=\"S6.T1.9.9.13.4.3.1\" class=\"ltx_text ltx_font_bold\">64.95(0.53)</span></td>\n<td id=\"S6.T1.9.9.13.4.4\" class=\"ltx_td ltx_align_center\" colspan=\"3\"><span id=\"S6.T1.9.9.13.4.4.1\" class=\"ltx_text ltx_font_bold\">33.94(0.44)</span></td>\n</tr>\n<tr id=\"S6.T1.9.9.14.5\" class=\"ltx_tr\">\n<th id=\"S6.T1.9.9.14.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedAvg</th>\n<td id=\"S6.T1.9.9.14.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">74.60(1.42)</td>\n<td id=\"S6.T1.9.9.14.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">69.81(3.00)</td>\n<td id=\"S6.T1.9.9.14.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">82.80(0.65)</td>\n<td id=\"S6.T1.9.9.14.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">36.07(3.02)</td>\n<td id=\"S6.T1.9.9.14.5.6\" class=\"ltx_td ltx_align_center ltx_border_t\">35.20(3.72)</td>\n<td id=\"S6.T1.9.9.14.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">48.66(3.00)</td>\n<td id=\"S6.T1.9.9.14.5.8\" class=\"ltx_td ltx_align_center ltx_border_t\">22.62(0.84)</td>\n<td id=\"S6.T1.9.9.14.5.9\" class=\"ltx_td ltx_align_center ltx_border_t\">21.79(0.79)</td>\n<td id=\"S6.T1.9.9.14.5.10\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">26.52(1.09)</td>\n</tr>\n<tr id=\"S6.T1.9.9.15.6\" class=\"ltx_tr\">\n<th id=\"S6.T1.9.9.15.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">FedProx</th>\n<td id=\"S6.T1.9.9.15.6.2\" class=\"ltx_td ltx_align_center\">74.63(1.30)</td>\n<td id=\"S6.T1.9.9.15.6.3\" class=\"ltx_td ltx_align_center\">69.59(2.99)</td>\n<td id=\"S6.T1.9.9.15.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">82.92(0.38)</td>\n<td id=\"S6.T1.9.9.15.6.5\" class=\"ltx_td ltx_align_center\">36.63(2.64)</td>\n<td id=\"S6.T1.9.9.15.6.6\" class=\"ltx_td ltx_align_center\">35.21(3.78)</td>\n<td id=\"S6.T1.9.9.15.6.7\" class=\"ltx_td ltx_align_center ltx_border_r\">48.43(2.27)</td>\n<td id=\"S6.T1.9.9.15.6.8\" class=\"ltx_td ltx_align_center\">22.27(0.90)</td>\n<td id=\"S6.T1.9.9.15.6.9\" class=\"ltx_td ltx_align_center\">22.30(0.47)</td>\n<td id=\"S6.T1.9.9.15.6.10\" class=\"ltx_td ltx_nopad_r ltx_align_center\">26.03(0.73)</td>\n</tr>\n<tr id=\"S6.T1.9.9.16.7\" class=\"ltx_tr\">\n<th id=\"S6.T1.9.9.16.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">FedDyn</th>\n<td id=\"S6.T1.9.9.16.7.2\" class=\"ltx_td ltx_align_center\">74.77(1.76)</td>\n<td id=\"S6.T1.9.9.16.7.3\" class=\"ltx_td ltx_align_center\">70.09(2.24)</td>\n<td id=\"S6.T1.9.9.16.7.4\" class=\"ltx_td ltx_align_center ltx_border_r\">83.95(0.29)</td>\n<td id=\"S6.T1.9.9.16.7.5\" class=\"ltx_td ltx_align_center\">36.11(3.35)</td>\n<td id=\"S6.T1.9.9.16.7.6\" class=\"ltx_td ltx_align_center\">36.00(3.78)</td>\n<td id=\"S6.T1.9.9.16.7.7\" class=\"ltx_td ltx_align_center ltx_border_r\">50.46(2.33)</td>\n<td id=\"S6.T1.9.9.16.7.8\" class=\"ltx_td ltx_align_center\">13.28(2.19)</td>\n<td id=\"S6.T1.9.9.16.7.9\" class=\"ltx_td ltx_align_center\">1.00(0.00)</td>\n<td id=\"S6.T1.9.9.16.7.10\" class=\"ltx_td ltx_nopad_r ltx_align_center\">1.00(0.00)</td>\n</tr>\n<tr id=\"S6.T1.9.9.17.8\" class=\"ltx_tr\">\n<th id=\"S6.T1.9.9.17.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">MOON</th>\n<td id=\"S6.T1.9.9.17.8.2\" class=\"ltx_td ltx_align_center\">74.25(1.59)</td>\n<td id=\"S6.T1.9.9.17.8.3\" class=\"ltx_td ltx_align_center\">68.52(2.26)</td>\n<td id=\"S6.T1.9.9.17.8.4\" class=\"ltx_td ltx_align_center ltx_border_r\">82.72(0.42)</td>\n<td id=\"S6.T1.9.9.17.8.5\" class=\"ltx_td ltx_align_center\">35.90(3.17)</td>\n<td id=\"S6.T1.9.9.17.8.6\" class=\"ltx_td ltx_align_center\">34.89(3.18)</td>\n<td id=\"S6.T1.9.9.17.8.7\" class=\"ltx_td ltx_align_center ltx_border_r\">48.74(2.45)</td>\n<td id=\"S6.T1.9.9.17.8.8\" class=\"ltx_td ltx_align_center\">22.03(1.00)</td>\n<td id=\"S6.T1.9.9.17.8.9\" class=\"ltx_td ltx_align_center\">22.04(0.62)</td>\n<td id=\"S6.T1.9.9.17.8.10\" class=\"ltx_td ltx_nopad_r ltx_align_center\">26.69(1.03)</td>\n</tr>\n<tr id=\"S6.T1.9.9.18.9\" class=\"ltx_tr\">\n<th id=\"S6.T1.9.9.18.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">FedProc</th>\n<td id=\"S6.T1.9.9.18.9.2\" class=\"ltx_td ltx_align_center\">74.96(1.94)</td>\n<td id=\"S6.T1.9.9.18.9.3\" class=\"ltx_td ltx_align_center\">69.80(3.26)</td>\n<td id=\"S6.T1.9.9.18.9.4\" class=\"ltx_td ltx_align_center ltx_border_r\">82.94(0.34)</td>\n<td id=\"S6.T1.9.9.18.9.5\" class=\"ltx_td ltx_align_center\">36.57(3.61)</td>\n<td id=\"S6.T1.9.9.18.9.6\" class=\"ltx_td ltx_align_center\">35.02(4.53)</td>\n<td id=\"S6.T1.9.9.18.9.7\" class=\"ltx_td ltx_align_center ltx_border_r\">48.99(2.85)</td>\n<td id=\"S6.T1.9.9.18.9.8\" class=\"ltx_td ltx_align_center\">23.00(0.35)</td>\n<td id=\"S6.T1.9.9.18.9.9\" class=\"ltx_td ltx_align_center\">22.32(0.63)</td>\n<td id=\"S6.T1.9.9.18.9.10\" class=\"ltx_td ltx_nopad_r ltx_align_center\">26.38(0.52)</td>\n</tr>\n<tr id=\"S6.T1.9.9.19.10\" class=\"ltx_tr\">\n<th id=\"S6.T1.9.9.19.10.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">FedFA (Our)</th>\n<td id=\"S6.T1.9.9.19.10.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T1.9.9.19.10.2.1\" class=\"ltx_text ltx_font_bold\">84.08(1.22)</span></td>\n<td id=\"S6.T1.9.9.19.10.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T1.9.9.19.10.3.1\" class=\"ltx_text ltx_font_bold\">83.42(1.14)</span></td>\n<td id=\"S6.T1.9.9.19.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S6.T1.9.9.19.10.4.1\" class=\"ltx_text ltx_font_bold\">88.40(0.12)</span></td>\n<td id=\"S6.T1.9.9.19.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T1.9.9.19.10.5.1\" class=\"ltx_text ltx_font_bold\">52.64(1.46)</span></td>\n<td id=\"S6.T1.9.9.19.10.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T1.9.9.19.10.6.1\" class=\"ltx_text ltx_font_bold\">52.95(2.01)</span></td>\n<td id=\"S6.T1.9.9.19.10.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S6.T1.9.9.19.10.7.1\" class=\"ltx_text ltx_font_bold\">60.40(0.38)</span></td>\n<td id=\"S6.T1.9.9.19.10.8\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T1.9.9.19.10.8.1\" class=\"ltx_text ltx_font_bold\">26.68(1.18)</span></td>\n<td id=\"S6.T1.9.9.19.10.9\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T1.9.9.19.10.9.1\" class=\"ltx_text ltx_font_bold\">24.05(2.32)</span></td>\n<td id=\"S6.T1.9.9.19.10.10\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S6.T1.9.9.19.10.10.1\" class=\"ltx_text ltx_font_bold\">29.16(1.03)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table I shows that FedFA provides significant gains in different label-skew settings regardless of the dataset.\nCompared with Î±=0.5ğ›¼0.5\\alpha=0.5, both #â€‹C=2#ğ¶2\\#C=2 and Î±=0.1ğ›¼0.1\\alpha=0.1 indicate more severe label distribution skew, but clients under #â€‹C=2#ğ¶2\\#C=2 have the same sample number while the ones with Î±=0.1ğ›¼0.1\\alpha=0.1 do not.\nFirstly, we find that the performance of all methods degrades as the degree of data heterogeneity increases.\nNevertheless, the decline of FedFA is much smaller than that of other methods.\nFor example, when Î±ğ›¼\\alpha changes from 0.50.50.5 to 0.10.10.1, the top-1 accuracy of all the baselines goes down by about 13%percent1313\\% on FMNIST and CIFAR-10, which is twice as large as FedFA.\nSecondly, under the same label skew, FedFA achieves larger gains over other methods when label distribution skew becomes more severe, up to 18.06%percent18.0618.06\\% (i.e., MOON: 34.89%percent34.8934.89\\% and FedFA 52.95%percent52.9552.95\\% under Î±=0.1ğ›¼0.1\\alpha=0.1 in CIFAR-10).\nThirdly, to explore more difficult tasks, we test on CIFAR-100 with ResNet18, and our method still achieves the best performance (i.e., about 3%percent33\\% accuracy advance).\nIt is important to note that FedDyn exhibits unstable performance compared to other methods when considering the same hyperparameter setups.\nFor instance, it shows significantly low accuracy in the CIFAR-100 task.\nThis is due to the fact that FedDyn is much more sensitive to hyperparameters.",
            "We compare our method with FedAvg under more homogeneous data and take the same learning rate of this case as that of data heterogeneity for comparison, where the results are reported in Table I and Table II.\nThe results demonstrate that FedFA still brings a significant advance in the presence of data homogeneity.\nFor example, FedFA is 8.64%percent8.648.64\\% more accurate than FedAvg on CIFAR-100.\nIncredibly, FedFA under mild data heterogeneity (e.g., Î±=0.5ğ›¼0.5\\alpha=0.5 in FMNIST or Mixed Digits) even obtains higher accuracy than FedAvg without any label or feature skew (e.g., FedFA: 88.40%percent88.4088.40\\% vs. FedAvg: 85.90%percent85.9085.90\\% in FMNIST).\nThis reveals that the effect of data heterogeneity on federated learning deserves to be further explored.",
            "In [50, 51], it has been found that the local optimizer with momentum is more robust to different smoothness of loss to improve generalization.\nWe explore the effect of FedFA on the Lipschitzness of loss by comparing local SGD optimizers with or without momentum on CIFAR-10.\nNote that FedDyd and FedProc are not compatible with local SGD with momentum, and thus Table III and Figure 3 do not include their results.\nComparing Table I with Table III, all methods with momentum work better than that without momentum, but FedFA without momentum has superior performance than baselines with momentum under #â€‹C=2#ğ¶2\\#C=2 and Î±=0.1ğ›¼0.1\\alpha=0.1 (e.g., FedFA without momentum: 52.95%percent\\% vs. FedProx with momentum: 49.87%percent\\% under Î±=0.1ğ›¼0.1\\alpha=0.1).\nAs expected by Theorem 4, a smoother loss of FedFA achieves better generalization.",
            "We experiment on the FMNIST with different momentum coefficients (Î»ğœ†\\lambda) of feature anchor updates with the label-skew case of #â€‹C=2#ğ¶2\\#C=2.\nThis experiment setting is the same as Table I.\nSpecifically, when Î»=1ğœ†1\\lambda=1, feature anchors will not be updated; when Î»=0ğœ†0\\lambda=0, feature anchors will be set as the mean feature of the last epoch.\nFigure 4(a) shows that the performance of FedFA with different Î»ğœ†\\lambda is similar.\nMeanwhile, although FedFA with Î»=0ğœ†0\\lambda=0 introduces more oscillations during training, it performs similarly to other cases.\nThis means that FedFA is not sensitive to the momentum coefficient Î»ğœ†\\lambda.",
            "Compared with [19] that calibrates classifier with virtual representation (CCVR) after training, we perform it\nduring different phases of training and the setting of Table V is the same as Table I.\nThe result of each mini-batch calibration done by FedFA is the best in all cases.\nThis reveals that the classifier divergence induced by data heterogeneity should be corrected as early as possible.\nMeanwhile, maintaining the virtuous cycle between feature and classifier updates during local training helps the final model converge at a point that generalizes better, since compared with FedFA without classifier calibration, FedFA with classifier calibration after training only improves little."
        ]
    },
    "S6.T2": {
        "caption": "TABLE II: The top-1 accuracy of all methods under label &\\& feature distribution skews. Note that we report the average top-1 accuracy on five-digit datasets in Mixed Digits. The first horizontal frame reports the results of w/o skew.",
        "table": "<table id=\"S6.T2.6.4\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T2.3.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T2.3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" rowspan=\"3\"><span id=\"S6.T2.3.1.1.2.1\" class=\"ltx_text\">\n<span id=\"S6.T2.3.1.1.2.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S6.T2.3.1.1.2.1.1.1\" class=\"ltx_tr\">\n<span id=\"S6.T2.3.1.1.2.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Method</span></span>\n</span></span></td>\n<td id=\"S6.T2.3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\">Feature Distribution Skew</td>\n<td id=\"S6.T2.3.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"3\">Label <math id=\"S6.T2.3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\&amp;\" display=\"inline\"><semantics id=\"S6.T2.3.1.1.1.m1.1a\"><mo id=\"S6.T2.3.1.1.1.m1.1.1\" xref=\"S6.T2.3.1.1.1.m1.1.1.cmml\">&amp;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.3.1.1.1.m1.1b\"><and id=\"S6.T2.3.1.1.1.m1.1.1.cmml\" xref=\"S6.T2.3.1.1.1.m1.1.1\"></and></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.3.1.1.1.m1.1c\">\\&amp;</annotation></semantics></math> Feature Distribution Skew</td>\n</tr>\n<tr id=\"S6.T2.6.4.5.1\" class=\"ltx_tr\">\n<td id=\"S6.T2.6.4.5.1.1\" class=\"ltx_td ltx_align_center ltx_border_t\" rowspan=\"2\"><span id=\"S6.T2.6.4.5.1.1.1\" class=\"ltx_text\">EMNIST</span></td>\n<td id=\"S6.T2.6.4.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S6.T2.6.4.5.1.2.1\" class=\"ltx_text\">Mixed Digits</span></td>\n<td id=\"S6.T2.6.4.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">Mixed Digits</td>\n</tr>\n<tr id=\"S6.T2.6.4.4\" class=\"ltx_tr\">\n<td id=\"S6.T2.4.2.2.1\" class=\"ltx_td ltx_align_center\"><math id=\"S6.T2.4.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\#C=2\" display=\"inline\"><semantics id=\"S6.T2.4.2.2.1.m1.1a\"><mrow id=\"S6.T2.4.2.2.1.m1.1.1\" xref=\"S6.T2.4.2.2.1.m1.1.1.cmml\"><mrow id=\"S6.T2.4.2.2.1.m1.1.1.2\" xref=\"S6.T2.4.2.2.1.m1.1.1.2.cmml\"><mi mathvariant=\"normal\" id=\"S6.T2.4.2.2.1.m1.1.1.2.2\" xref=\"S6.T2.4.2.2.1.m1.1.1.2.2.cmml\">#</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S6.T2.4.2.2.1.m1.1.1.2.1\" xref=\"S6.T2.4.2.2.1.m1.1.1.2.1.cmml\">â€‹</mo><mi id=\"S6.T2.4.2.2.1.m1.1.1.2.3\" xref=\"S6.T2.4.2.2.1.m1.1.1.2.3.cmml\">C</mi></mrow><mo id=\"S6.T2.4.2.2.1.m1.1.1.1\" xref=\"S6.T2.4.2.2.1.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T2.4.2.2.1.m1.1.1.3\" xref=\"S6.T2.4.2.2.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.4.2.2.1.m1.1b\"><apply id=\"S6.T2.4.2.2.1.m1.1.1.cmml\" xref=\"S6.T2.4.2.2.1.m1.1.1\"><eq id=\"S6.T2.4.2.2.1.m1.1.1.1.cmml\" xref=\"S6.T2.4.2.2.1.m1.1.1.1\"></eq><apply id=\"S6.T2.4.2.2.1.m1.1.1.2.cmml\" xref=\"S6.T2.4.2.2.1.m1.1.1.2\"><times id=\"S6.T2.4.2.2.1.m1.1.1.2.1.cmml\" xref=\"S6.T2.4.2.2.1.m1.1.1.2.1\"></times><ci id=\"S6.T2.4.2.2.1.m1.1.1.2.2.cmml\" xref=\"S6.T2.4.2.2.1.m1.1.1.2.2\">#</ci><ci id=\"S6.T2.4.2.2.1.m1.1.1.2.3.cmml\" xref=\"S6.T2.4.2.2.1.m1.1.1.2.3\">ğ¶</ci></apply><cn type=\"integer\" id=\"S6.T2.4.2.2.1.m1.1.1.3.cmml\" xref=\"S6.T2.4.2.2.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.4.2.2.1.m1.1c\">\\#C=2</annotation></semantics></math></td>\n<td id=\"S6.T2.5.3.3.2\" class=\"ltx_td ltx_align_center\"><math id=\"S6.T2.5.3.3.2.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.1\" display=\"inline\"><semantics id=\"S6.T2.5.3.3.2.m1.1a\"><mrow id=\"S6.T2.5.3.3.2.m1.1.1\" xref=\"S6.T2.5.3.3.2.m1.1.1.cmml\"><mi id=\"S6.T2.5.3.3.2.m1.1.1.2\" xref=\"S6.T2.5.3.3.2.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T2.5.3.3.2.m1.1.1.1\" xref=\"S6.T2.5.3.3.2.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T2.5.3.3.2.m1.1.1.3\" xref=\"S6.T2.5.3.3.2.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.5.3.3.2.m1.1b\"><apply id=\"S6.T2.5.3.3.2.m1.1.1.cmml\" xref=\"S6.T2.5.3.3.2.m1.1.1\"><eq id=\"S6.T2.5.3.3.2.m1.1.1.1.cmml\" xref=\"S6.T2.5.3.3.2.m1.1.1.1\"></eq><ci id=\"S6.T2.5.3.3.2.m1.1.1.2.cmml\" xref=\"S6.T2.5.3.3.2.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T2.5.3.3.2.m1.1.1.3.cmml\" xref=\"S6.T2.5.3.3.2.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.5.3.3.2.m1.1c\">\\alpha=0.1</annotation></semantics></math></td>\n<td id=\"S6.T2.6.4.4.3\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><math id=\"S6.T2.6.4.4.3.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.5\" display=\"inline\"><semantics id=\"S6.T2.6.4.4.3.m1.1a\"><mrow id=\"S6.T2.6.4.4.3.m1.1.1\" xref=\"S6.T2.6.4.4.3.m1.1.1.cmml\"><mi id=\"S6.T2.6.4.4.3.m1.1.1.2\" xref=\"S6.T2.6.4.4.3.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T2.6.4.4.3.m1.1.1.1\" xref=\"S6.T2.6.4.4.3.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T2.6.4.4.3.m1.1.1.3\" xref=\"S6.T2.6.4.4.3.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T2.6.4.4.3.m1.1b\"><apply id=\"S6.T2.6.4.4.3.m1.1.1.cmml\" xref=\"S6.T2.6.4.4.3.m1.1.1\"><eq id=\"S6.T2.6.4.4.3.m1.1.1.1.cmml\" xref=\"S6.T2.6.4.4.3.m1.1.1.1\"></eq><ci id=\"S6.T2.6.4.4.3.m1.1.1.2.cmml\" xref=\"S6.T2.6.4.4.3.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T2.6.4.4.3.m1.1.1.3.cmml\" xref=\"S6.T2.6.4.4.3.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T2.6.4.4.3.m1.1c\">\\alpha=0.5</annotation></semantics></math></td>\n</tr>\n<tr id=\"S6.T2.6.4.6.2\" class=\"ltx_tr\">\n<td id=\"S6.T2.6.4.6.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAvg</td>\n<td id=\"S6.T2.6.4.6.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S6.T2.6.4.6.2.2.1\" class=\"ltx_text ltx_font_bold\">-</span></td>\n<td id=\"S6.T2.6.4.6.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"4\">82.66(2.38)</td>\n</tr>\n<tr id=\"S6.T2.6.4.7.3\" class=\"ltx_tr\">\n<td id=\"S6.T2.6.4.7.3.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedFA</td>\n<td id=\"S6.T2.6.4.7.3.2\" class=\"ltx_td ltx_align_center\"><span id=\"S6.T2.6.4.7.3.2.1\" class=\"ltx_text ltx_font_bold\">-</span></td>\n<td id=\"S6.T2.6.4.7.3.3\" class=\"ltx_td ltx_align_center\" colspan=\"4\"><span id=\"S6.T2.6.4.7.3.3.1\" class=\"ltx_text ltx_font_bold\">88.10(0.39)</span></td>\n</tr>\n<tr id=\"S6.T2.6.4.8.4\" class=\"ltx_tr\">\n<td id=\"S6.T2.6.4.8.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedAvg</td>\n<td id=\"S6.T2.6.4.8.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">98.50(0.04)</td>\n<td id=\"S6.T2.6.4.8.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">82.66(2.83)</td>\n<td id=\"S6.T2.6.4.8.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">56.13(5.59)</td>\n<td id=\"S6.T2.6.4.8.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">63.74(2.35)</td>\n<td id=\"S6.T2.6.4.8.4.6\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">78.34(1.58)</td>\n</tr>\n<tr id=\"S6.T2.6.4.9.5\" class=\"ltx_tr\">\n<td id=\"S6.T2.6.4.9.5.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedProx</td>\n<td id=\"S6.T2.6.4.9.5.2\" class=\"ltx_td ltx_align_center\">98.44(0.06)</td>\n<td id=\"S6.T2.6.4.9.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">82.46(2.65)</td>\n<td id=\"S6.T2.6.4.9.5.4\" class=\"ltx_td ltx_align_center\">54.86(5.80)</td>\n<td id=\"S6.T2.6.4.9.5.5\" class=\"ltx_td ltx_align_center\">62.57(2.22)</td>\n<td id=\"S6.T2.6.4.9.5.6\" class=\"ltx_td ltx_nopad_r ltx_align_center\">78.08(1.84)</td>\n</tr>\n<tr id=\"S6.T2.6.4.10.6\" class=\"ltx_tr\">\n<td id=\"S6.T2.6.4.10.6.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedDyn</td>\n<td id=\"S6.T2.6.4.10.6.2\" class=\"ltx_td ltx_align_center\">97.63(0.19)</td>\n<td id=\"S6.T2.6.4.10.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\">83.59(2.33)</td>\n<td id=\"S6.T2.6.4.10.6.4\" class=\"ltx_td ltx_align_center\">51.66(7.33)</td>\n<td id=\"S6.T2.6.4.10.6.5\" class=\"ltx_td ltx_align_center\">63.55(2.02)</td>\n<td id=\"S6.T2.6.4.10.6.6\" class=\"ltx_td ltx_nopad_r ltx_align_center\">79.40(1.76)</td>\n</tr>\n<tr id=\"S6.T2.6.4.11.7\" class=\"ltx_tr\">\n<td id=\"S6.T2.6.4.11.7.1\" class=\"ltx_td ltx_align_center ltx_border_r\">MOON</td>\n<td id=\"S6.T2.6.4.11.7.2\" class=\"ltx_td ltx_align_center\">98.51(0.06)</td>\n<td id=\"S6.T2.6.4.11.7.3\" class=\"ltx_td ltx_align_center ltx_border_r\">81.46(2.84)</td>\n<td id=\"S6.T2.6.4.11.7.4\" class=\"ltx_td ltx_align_center\">55.40(5.69)</td>\n<td id=\"S6.T2.6.4.11.7.5\" class=\"ltx_td ltx_align_center\">62.18(1.93)</td>\n<td id=\"S6.T2.6.4.11.7.6\" class=\"ltx_td ltx_nopad_r ltx_align_center\">78.05(1.82)</td>\n</tr>\n<tr id=\"S6.T2.6.4.12.8\" class=\"ltx_tr\">\n<td id=\"S6.T2.6.4.12.8.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedProc</td>\n<td id=\"S6.T2.6.4.12.8.2\" class=\"ltx_td ltx_align_center\">98.28(0.04)</td>\n<td id=\"S6.T2.6.4.12.8.3\" class=\"ltx_td ltx_align_center ltx_border_r\">82.06(2.68)</td>\n<td id=\"S6.T2.6.4.12.8.4\" class=\"ltx_td ltx_align_center\">59.53(3.66)</td>\n<td id=\"S6.T2.6.4.12.8.5\" class=\"ltx_td ltx_align_center\">64.59(2.15)</td>\n<td id=\"S6.T2.6.4.12.8.6\" class=\"ltx_td ltx_nopad_r ltx_align_center\">78.66(1.16)</td>\n</tr>\n<tr id=\"S6.T2.6.4.13.9\" class=\"ltx_tr\">\n<td id=\"S6.T2.6.4.13.9.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\">FedFA</td>\n<td id=\"S6.T2.6.4.13.9.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.6.4.13.9.2.1\" class=\"ltx_text ltx_font_bold\">99.28(0.33)</span></td>\n<td id=\"S6.T2.6.4.13.9.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\"><span id=\"S6.T2.6.4.13.9.3.1\" class=\"ltx_text ltx_font_bold\">90.73(2.01)</span></td>\n<td id=\"S6.T2.6.4.13.9.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.6.4.13.9.4.1\" class=\"ltx_text ltx_font_bold\">83.46(2.57)</span></td>\n<td id=\"S6.T2.6.4.13.9.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T2.6.4.13.9.5.1\" class=\"ltx_text ltx_font_bold\">85.71(0.71)</span></td>\n<td id=\"S6.T2.6.4.13.9.6\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S6.T2.6.4.13.9.6.1\" class=\"ltx_text ltx_font_bold\">89.82(0.49)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "This work aims at image classification tasks under label and feature distribution skews, and it uses federated benchmark datasets as [1, 42, 7], including EMNIST[43], FMNIST, CIFAR-10, CIFAR-100 [44], and Mixed Digits dataset [36].\nSpecifically, for label distribution skew, we consider two settings: (i) Same size of local dataset: following [1], we split data samples based on class to clients (e.g., #â€‹C=2#ğ¶2\\#C=2 denotes that each client holds two class samples);\n(ii) Different sizes of local dataset: following [42], we set Î±ğ›¼\\alpha of Dirichlet distribution Dâ€‹iâ€‹râ€‹(Î±)ğ·ğ‘–ğ‘Ÿğ›¼Dir(\\alpha) as 0.1 and 0.5 to generate distribution pi,csubscriptğ‘ğ‘–ğ‘p_{i,c} by which the cğ‘c-th class samples are split to client iğ‘–i.\nFor feature distribution skew, we consider two settings:\n(i) Real-world feature skew: we sample a subset with 10 classes of a real-world dataset EMNIST with natural feature skew;\n(ii) Artificial feature skew: we use a mixed-digit dataset from [36]\nconsisting of MNIST[45], SVHN[46], USPS[47], SynthDigits and MNIST-M[48].\nIn Table II and Table IV, we test the top-1 accuracy based on the global model, except for Mixed Digits where we report the average top-1 accuracy on five-benchmark digit datasets.",
            "According to Table II, our method obtains higher accuracy than all baselines on EMNIST and Mixed Digits.\nSpecifically, the accuracy of FedFA in EMNIST reaches 99.28%percent99.2899.28\\%, which is 0.77%percent0.770.77\\% higher than the best baseline (i.e., MOON 98.51%percent98.5198.51\\%).\nMoreover, we split each digit dataset of Mixed Digits into 20 subsets, one for each client with the same sample number (i.e., a skewed feature distribution exists between the clients with a subset of SVHN and the ones with a subset of MNIST).\nCompared with the best baseline (Feddyn: 83.59%percent83.5983.59\\%) on Mixed Digits, our method achieves performance gains by 7.14%percent7.147.14\\%.",
            "We combine label skew and feature skew to explore the impact of data heterogeneity further.\nNamely, we not only split each dataset in Mixed Digits into 20 subsets, one for each client but also set different label distributions for various clients (i.e., clients are subject to at least one of label distribution skew and feature distribution skew).\nThe results in Table II show that all the methods are more susceptible under this setting than that of feature distribution skew.\nFor example, the most significant performance drop reaches 31.93%percent31.9331.93\\% (i.e., FedDyn from 83.59%percent83.5983.59\\% to 51.66%percent51.6651.66\\% under #â€‹C=2#ğ¶2\\#C=2).\nNevertheless, FedFA significantly mitigates this performance degradation with a mild decrease from 90.73%percent90.7390.73\\% to 83.46%percent83.4683.46\\%.\nMeanwhile, FedFA maintains at least 10%percent1010\\% performance advantage over all baselines under this case, with the largest gap reaching 31.8%percent31.831.8\\% (i.e., FedFA from 83.59%percent83.5983.59\\% to FedDyn 51.66%percent51.6651.66\\% under #â€‹C=2#ğ¶2\\#C=2).",
            "We compare our method with FedAvg under more homogeneous data and take the same learning rate of this case as that of data heterogeneity for comparison, where the results are reported in Table I and Table II.\nThe results demonstrate that FedFA still brings a significant advance in the presence of data homogeneity.\nFor example, FedFA is 8.64%percent8.648.64\\% more accurate than FedAvg on CIFAR-100.\nIncredibly, FedFA under mild data heterogeneity (e.g., Î±=0.5ğ›¼0.5\\alpha=0.5 in FMNIST or Mixed Digits) even obtains higher accuracy than FedAvg without any label or feature skew (e.g., FedFA: 88.40%percent88.4088.40\\% vs. FedAvg: 85.90%percent85.9085.90\\% in FMNIST).\nThis reveals that the effect of data heterogeneity on federated learning deserves to be further explored."
        ]
    },
    "S6.T3": {
        "caption": "TABLE III: Test accuracy under local SGD with momentum.",
        "table": "<table id=\"S6.T3.3.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T3.3.3.4.1\" class=\"ltx_tr\">\n<th id=\"S6.T3.3.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S6.T3.3.3.4.1.1.1\" class=\"ltx_text\">\n<span id=\"S6.T3.3.3.4.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<span id=\"S6.T3.3.3.4.1.1.1.1.1\" class=\"ltx_tr\">\n<span id=\"S6.T3.3.3.4.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Method</span></span>\n</span></span></th>\n<th id=\"S6.T3.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\">CIFAR-10</th>\n</tr>\n<tr id=\"S6.T3.3.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><math id=\"S6.T3.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\#C=2\" display=\"inline\"><semantics id=\"S6.T3.1.1.1.1.m1.1a\"><mrow id=\"S6.T3.1.1.1.1.m1.1.1\" xref=\"S6.T3.1.1.1.1.m1.1.1.cmml\"><mrow id=\"S6.T3.1.1.1.1.m1.1.1.2\" xref=\"S6.T3.1.1.1.1.m1.1.1.2.cmml\"><mi mathvariant=\"normal\" id=\"S6.T3.1.1.1.1.m1.1.1.2.2\" xref=\"S6.T3.1.1.1.1.m1.1.1.2.2.cmml\">#</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S6.T3.1.1.1.1.m1.1.1.2.1\" xref=\"S6.T3.1.1.1.1.m1.1.1.2.1.cmml\">â€‹</mo><mi id=\"S6.T3.1.1.1.1.m1.1.1.2.3\" xref=\"S6.T3.1.1.1.1.m1.1.1.2.3.cmml\">C</mi></mrow><mo id=\"S6.T3.1.1.1.1.m1.1.1.1\" xref=\"S6.T3.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T3.1.1.1.1.m1.1.1.3\" xref=\"S6.T3.1.1.1.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.1.1.1.1.m1.1b\"><apply id=\"S6.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T3.1.1.1.1.m1.1.1\"><eq id=\"S6.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T3.1.1.1.1.m1.1.1.1\"></eq><apply id=\"S6.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"S6.T3.1.1.1.1.m1.1.1.2\"><times id=\"S6.T3.1.1.1.1.m1.1.1.2.1.cmml\" xref=\"S6.T3.1.1.1.1.m1.1.1.2.1\"></times><ci id=\"S6.T3.1.1.1.1.m1.1.1.2.2.cmml\" xref=\"S6.T3.1.1.1.1.m1.1.1.2.2\">#</ci><ci id=\"S6.T3.1.1.1.1.m1.1.1.2.3.cmml\" xref=\"S6.T3.1.1.1.1.m1.1.1.2.3\">ğ¶</ci></apply><cn type=\"integer\" id=\"S6.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"S6.T3.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.1.1.1.1.m1.1c\">\\#C=2</annotation></semantics></math></th>\n<th id=\"S6.T3.2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><math id=\"S6.T3.2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.1\" display=\"inline\"><semantics id=\"S6.T3.2.2.2.2.m1.1a\"><mrow id=\"S6.T3.2.2.2.2.m1.1.1\" xref=\"S6.T3.2.2.2.2.m1.1.1.cmml\"><mi id=\"S6.T3.2.2.2.2.m1.1.1.2\" xref=\"S6.T3.2.2.2.2.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T3.2.2.2.2.m1.1.1.1\" xref=\"S6.T3.2.2.2.2.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T3.2.2.2.2.m1.1.1.3\" xref=\"S6.T3.2.2.2.2.m1.1.1.3.cmml\">0.1</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.2.2.2.2.m1.1b\"><apply id=\"S6.T3.2.2.2.2.m1.1.1.cmml\" xref=\"S6.T3.2.2.2.2.m1.1.1\"><eq id=\"S6.T3.2.2.2.2.m1.1.1.1.cmml\" xref=\"S6.T3.2.2.2.2.m1.1.1.1\"></eq><ci id=\"S6.T3.2.2.2.2.m1.1.1.2.cmml\" xref=\"S6.T3.2.2.2.2.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T3.2.2.2.2.m1.1.1.3.cmml\" xref=\"S6.T3.2.2.2.2.m1.1.1.3\">0.1</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.2.2.2.2.m1.1c\">\\alpha=0.1</annotation></semantics></math></th>\n<th id=\"S6.T3.3.3.3.3\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column\"><math id=\"S6.T3.3.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\alpha=0.5\" display=\"inline\"><semantics id=\"S6.T3.3.3.3.3.m1.1a\"><mrow id=\"S6.T3.3.3.3.3.m1.1.1\" xref=\"S6.T3.3.3.3.3.m1.1.1.cmml\"><mi id=\"S6.T3.3.3.3.3.m1.1.1.2\" xref=\"S6.T3.3.3.3.3.m1.1.1.2.cmml\">Î±</mi><mo id=\"S6.T3.3.3.3.3.m1.1.1.1\" xref=\"S6.T3.3.3.3.3.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T3.3.3.3.3.m1.1.1.3\" xref=\"S6.T3.3.3.3.3.m1.1.1.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.3.3.3.3.m1.1b\"><apply id=\"S6.T3.3.3.3.3.m1.1.1.cmml\" xref=\"S6.T3.3.3.3.3.m1.1.1\"><eq id=\"S6.T3.3.3.3.3.m1.1.1.1.cmml\" xref=\"S6.T3.3.3.3.3.m1.1.1.1\"></eq><ci id=\"S6.T3.3.3.3.3.m1.1.1.2.cmml\" xref=\"S6.T3.3.3.3.3.m1.1.1.2\">ğ›¼</ci><cn type=\"float\" id=\"S6.T3.3.3.3.3.m1.1.1.3.cmml\" xref=\"S6.T3.3.3.3.3.m1.1.1.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.3.3.3.3.m1.1c\">\\alpha=0.5</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T3.3.3.5.1\" class=\"ltx_tr\">\n<th id=\"S6.T3.3.3.5.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedAvg w/o skew</th>\n<td id=\"S6.T3.3.3.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"3\">67.58(0.23)</td>\n</tr>\n<tr id=\"S6.T3.3.3.6.2\" class=\"ltx_tr\">\n<th id=\"S6.T3.3.3.6.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">FedFA w/o skew</th>\n<td id=\"S6.T3.3.3.6.2.2\" class=\"ltx_td ltx_align_center\" colspan=\"3\"><span id=\"S6.T3.3.3.6.2.2.1\" class=\"ltx_text ltx_font_bold\">69.32(0.36)</span></td>\n</tr>\n<tr id=\"S6.T3.3.3.7.3\" class=\"ltx_tr\">\n<th id=\"S6.T3.3.3.7.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\">FedAvg</th>\n<td id=\"S6.T3.3.3.7.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\">48.17(3.40)</td>\n<td id=\"S6.T3.3.3.7.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">47.91(5.95)</td>\n<td id=\"S6.T3.3.3.7.3.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">64.12(1.02)</td>\n</tr>\n<tr id=\"S6.T3.3.3.8.4\" class=\"ltx_tr\">\n<th id=\"S6.T3.3.3.8.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">FedProx</th>\n<td id=\"S6.T3.3.3.8.4.2\" class=\"ltx_td ltx_align_center\">48.14(2.89)</td>\n<td id=\"S6.T3.3.3.8.4.3\" class=\"ltx_td ltx_align_center\">49.87(6.82)</td>\n<td id=\"S6.T3.3.3.8.4.4\" class=\"ltx_td ltx_nopad_r ltx_align_center\">63.71(1.15)</td>\n</tr>\n<tr id=\"S6.T3.3.3.9.5\" class=\"ltx_tr\">\n<th id=\"S6.T3.3.3.9.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\">MOON</th>\n<td id=\"S6.T3.3.3.9.5.2\" class=\"ltx_td ltx_align_center\">48.13(2.12)</td>\n<td id=\"S6.T3.3.3.9.5.3\" class=\"ltx_td ltx_align_center\">47.11(6.96)</td>\n<td id=\"S6.T3.3.3.9.5.4\" class=\"ltx_td ltx_nopad_r ltx_align_center\">64.08(1.20)</td>\n</tr>\n<tr id=\"S6.T3.3.3.10.6\" class=\"ltx_tr\">\n<th id=\"S6.T3.3.3.10.6.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">FedFA (Our)</th>\n<td id=\"S6.T3.3.3.10.6.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T3.3.3.10.6.2.1\" class=\"ltx_text ltx_font_bold\">57.30(2.05)</span></td>\n<td id=\"S6.T3.3.3.10.6.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S6.T3.3.3.10.6.3.1\" class=\"ltx_text ltx_font_bold\">54.21(5.55)</span></td>\n<td id=\"S6.T3.3.3.10.6.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\"><span id=\"S6.T3.3.3.10.6.4.1\" class=\"ltx_text ltx_font_bold\">64.63(0.57)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In Tables I and II, 100 clients attend federated training, 10 clients participate in each round, the local batch size is 64, the local epochs number is 5, and the targeted communication round is 200.\nWe use the SGD optimizer with a 0.01 learning rate and 0.001 weight decay for all experiments.\nFurthermore, in Table III and Figures 3, we follow the setups of [14] to investigate the impact of different federated setups with 200 rounds and a local SGD with a 0.01 learning rate and 0.9 momentum.\nAll experiments are performed based on PyTorch and one node of the High-Performance Computing platform with 4 NVIDIA A30 Tensor Core GPUs with 24GB.",
            "Following the setup of Table III, we further explore the impact of federated setups.\nAs shown in Figure 3(a), a larger client sample rate achieves better test accuracy for all methods. Especially, the accuracy gains (about 10%percent1010\\%) when increasing the sample rate from 0.10.10.1 to 0.30.30.3 is much larger than that from 0.30.30.3 to 0.50.50.5.\nAs shown in Figure 3(b), larger local epochs have a negative impact on performance, but FedProx and MOON have worse performance degradation than FedAvg and FedFA.\nBesides, the performance advantage of FedFA under various batch sizes and client numbers is shown in Figures 9 and 10.\nOverall, our method FedFA consistently achieves better than all baselines under different setups.",
            "In [50, 51], it has been found that the local optimizer with momentum is more robust to different smoothness of loss to improve generalization.\nWe explore the effect of FedFA on the Lipschitzness of loss by comparing local SGD optimizers with or without momentum on CIFAR-10.\nNote that FedDyd and FedProc are not compatible with local SGD with momentum, and thus Table III and Figure 3 do not include their results.\nComparing Table I with Table III, all methods with momentum work better than that without momentum, but FedFA without momentum has superior performance than baselines with momentum under #â€‹C=2#ğ¶2\\#C=2 and Î±=0.1ğ›¼0.1\\alpha=0.1 (e.g., FedFA without momentum: 52.95%percent\\% vs. FedProx with momentum: 49.87%percent\\% under Î±=0.1ğ›¼0.1\\alpha=0.1).\nAs expected by Theorem 4, a smoother loss of FedFA achieves better generalization."
        ]
    },
    "S6.T4": {
        "caption": "TABLE IV: The top-1 accuracy of FedFA in different ablations on anchor updating (AU) and classifier calibration (CC).",
        "table": "<table id=\"S6.T4.3.3\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T4.3.3.3\" class=\"ltx_tr\">\n<th id=\"S6.T4.3.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">\n<table id=\"S6.T4.3.3.3.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S6.T4.3.3.3.4.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.3.3.3.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Method</td>\n</tr>\n</table>\n</th>\n<th id=\"S6.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">\n<table id=\"S6.T4.1.1.1.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S6.T4.1.1.1.1.1.2\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.1.1.1.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Label Skew</td>\n</tr>\n<tr id=\"S6.T4.1.1.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.1.1.1.1.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(FMNIST <math id=\"S6.T4.1.1.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\#C=2\" display=\"inline\"><semantics id=\"S6.T4.1.1.1.1.1.1.1.m1.1a\"><mrow id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.cmml\"><mrow id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.cmml\"><mi mathvariant=\"normal\" id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.2\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.2.cmml\">#</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.1\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.1.cmml\">â€‹</mo><mi id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.3\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.3.cmml\">C</mi></mrow><mo id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.1\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.3\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.1.1.1.1.1.1.1.m1.1b\"><apply id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1\"><eq id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.1\"></eq><apply id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.cmml\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2\"><times id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.1.cmml\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.1\"></times><ci id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.2.cmml\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.2\">#</ci><ci id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.3.cmml\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.2.3\">ğ¶</ci></apply><cn type=\"integer\" id=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.3.cmml\" xref=\"S6.T4.1.1.1.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.1.1.1.1.1.1.1.m1.1c\">\\#C=2</annotation></semantics></math>)</td>\n</tr>\n</table>\n</th>\n<th id=\"S6.T4.3.3.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">\n<table id=\"S6.T4.3.3.3.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S6.T4.3.3.3.5.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.3.3.3.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Feature Skew</td>\n</tr>\n<tr id=\"S6.T4.3.3.3.5.1.2\" class=\"ltx_tr\">\n<td id=\"S6.T4.3.3.3.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(Mixed Digits)</td>\n</tr>\n</table>\n</th>\n<th id=\"S6.T4.3.3.3.3\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_tt\">\n<table id=\"S6.T4.3.3.3.3.2\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"S6.T4.2.2.2.2.1.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.2.2.2.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Label <math id=\"S6.T4.2.2.2.2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\&amp;\" display=\"inline\"><semantics id=\"S6.T4.2.2.2.2.1.1.1.m1.1a\"><mo id=\"S6.T4.2.2.2.2.1.1.1.m1.1.1\" xref=\"S6.T4.2.2.2.2.1.1.1.m1.1.1.cmml\">&amp;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.2.2.2.2.1.1.1.m1.1b\"><and id=\"S6.T4.2.2.2.2.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.2.2.2.2.1.1.1.m1.1.1\"></and></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.2.2.2.2.1.1.1.m1.1c\">\\&amp;</annotation></semantics></math> Feature Skew</td>\n</tr>\n<tr id=\"S6.T4.3.3.3.3.2.2\" class=\"ltx_tr\">\n<td id=\"S6.T4.3.3.3.3.2.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">(Mixed Digits <math id=\"S6.T4.3.3.3.3.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\#C=2\" display=\"inline\"><semantics id=\"S6.T4.3.3.3.3.2.2.1.m1.1a\"><mrow id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.cmml\"><mrow id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.cmml\"><mi mathvariant=\"normal\" id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.2\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.2.cmml\">#</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.1\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.1.cmml\">â€‹</mo><mi id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.3\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.3.cmml\">C</mi></mrow><mo id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.1\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.3\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.3.3.3.3.2.2.1.m1.1b\"><apply id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.cmml\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1\"><eq id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.1.cmml\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.1\"></eq><apply id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.cmml\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2\"><times id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.1.cmml\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.1\"></times><ci id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.2.cmml\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.2\">#</ci><ci id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.3.cmml\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.2.3\">ğ¶</ci></apply><cn type=\"integer\" id=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.3.cmml\" xref=\"S6.T4.3.3.3.3.2.2.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.3.3.3.3.2.2.1.m1.1c\">\\#C=2</annotation></semantics></math>)</td>\n</tr>\n</table>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T4.3.3.4.1\" class=\"ltx_tr\">\n<td id=\"S6.T4.3.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FedFA w/o AU</td>\n<td id=\"S6.T4.3.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">81.89(1.87)</td>\n<td id=\"S6.T4.3.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">88.69(0.75)</td>\n<td id=\"S6.T4.3.3.4.1.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">76.81(1.78)</td>\n</tr>\n<tr id=\"S6.T4.3.3.5.2\" class=\"ltx_tr\">\n<td id=\"S6.T4.3.3.5.2.1\" class=\"ltx_td ltx_align_center ltx_border_r\">FedFA w/o CC</td>\n<td id=\"S6.T4.3.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">78.07(2.23)</td>\n<td id=\"S6.T4.3.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">79.25(1.25)</td>\n<td id=\"S6.T4.3.3.5.2.4\" class=\"ltx_td ltx_nopad_r ltx_align_center\">61.36(4.00)</td>\n</tr>\n<tr id=\"S6.T4.3.3.6.3\" class=\"ltx_tr\">\n<td id=\"S6.T4.3.3.6.3.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">FedFA (Our)</td>\n<td id=\"S6.T4.3.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S6.T4.3.3.6.3.2.1\" class=\"ltx_text ltx_font_bold\">84.08(1.22)</span></td>\n<td id=\"S6.T4.3.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S6.T4.3.3.6.3.3.1\" class=\"ltx_text ltx_font_bold\">90.86(1.92)</span></td>\n<td id=\"S6.T4.3.3.6.3.4\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S6.T4.3.3.6.3.4.1\" class=\"ltx_text ltx_font_bold\">83.73(2.76)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "This work aims at image classification tasks under label and feature distribution skews, and it uses federated benchmark datasets as [1, 42, 7], including EMNIST[43], FMNIST, CIFAR-10, CIFAR-100 [44], and Mixed Digits dataset [36].\nSpecifically, for label distribution skew, we consider two settings: (i) Same size of local dataset: following [1], we split data samples based on class to clients (e.g., #â€‹C=2#ğ¶2\\#C=2 denotes that each client holds two class samples);\n(ii) Different sizes of local dataset: following [42], we set Î±ğ›¼\\alpha of Dirichlet distribution Dâ€‹iâ€‹râ€‹(Î±)ğ·ğ‘–ğ‘Ÿğ›¼Dir(\\alpha) as 0.1 and 0.5 to generate distribution pi,csubscriptğ‘ğ‘–ğ‘p_{i,c} by which the cğ‘c-th class samples are split to client iğ‘–i.\nFor feature distribution skew, we consider two settings:\n(i) Real-world feature skew: we sample a subset with 10 classes of a real-world dataset EMNIST with natural feature skew;\n(ii) Artificial feature skew: we use a mixed-digit dataset from [36]\nconsisting of MNIST[45], SVHN[46], USPS[47], SynthDigits and MNIST-M[48].\nIn Table II and Table IV, we test the top-1 accuracy based on the global model, except for Mixed Digits where we report the average top-1 accuracy on five-benchmark digit datasets.",
            "As shown in Table IV, we conduct ablation studies on FedFA without anchor updating in (4) and FedFA without classifier calibration in (5) to give an intuition of FedFA performance.\nOn the one side, feature anchors can be fixed during federated training (i.e., the client would not aggregate any information into feature anchors, which would not bring potential privacy leakage).\nFedFA performs better than the best baseline under label and feature skew.\nMeanwhile, the anchor updating brings consistent performance benefits (i.e., at least around 2%percent22\\% boost) since the updated anchors keep more representative in the shared feature space across clients.\nOn the other hand, classifier calibration plays the most crucial role in FedFA because data heterogeneity induces a low classifier update similarity as observed in Figure 2.\nFor instance, classifier calibration boosts performance by 22.37%percent22.3722.37\\% in the case combined by label skew and feature skew.\nOverall, both feature alignment and classifier calibration play an essential role in FedFA to overcome data heterogeneity.",
            "Firstly, feature anchors can be fixed/without updates during federated training (i.e., the client would not aggregate any information into feature anchors, which would not bring potential privacy leakage) because of the powerful representation of neural networks. That is, the fixed anchors in FedFA specified a feature space and a classifier between clients before training. The third row of Table IV shows better results of experiments of FedFA without feature-anchor updates than the best baseline under label and feature skew even though the performance goes down about 2%percent22\\% than FedFA with anchor updates. Therefore, there is a trade-off between privacy and generalization performance for FedFA.\nTherefore, there is a trade-off between privacy and generalization performance for FedFA."
        ]
    },
    "S6.T5": {
        "caption": "TABLE V: Classifier calibration (CC) at different phases.",
        "table": "<table id=\"S6.T5.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S6.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S6.T5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Method (FMNIST <math id=\"S6.T5.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\#C=2\" display=\"inline\"><semantics id=\"S6.T5.1.1.1.1.m1.1a\"><mrow id=\"S6.T5.1.1.1.1.m1.1.1\" xref=\"S6.T5.1.1.1.1.m1.1.1.cmml\"><mrow id=\"S6.T5.1.1.1.1.m1.1.1.2\" xref=\"S6.T5.1.1.1.1.m1.1.1.2.cmml\"><mi mathvariant=\"normal\" id=\"S6.T5.1.1.1.1.m1.1.1.2.2\" xref=\"S6.T5.1.1.1.1.m1.1.1.2.2.cmml\">#</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S6.T5.1.1.1.1.m1.1.1.2.1\" xref=\"S6.T5.1.1.1.1.m1.1.1.2.1.cmml\">â€‹</mo><mi id=\"S6.T5.1.1.1.1.m1.1.1.2.3\" xref=\"S6.T5.1.1.1.1.m1.1.1.2.3.cmml\">C</mi></mrow><mo id=\"S6.T5.1.1.1.1.m1.1.1.1\" xref=\"S6.T5.1.1.1.1.m1.1.1.1.cmml\">=</mo><mn id=\"S6.T5.1.1.1.1.m1.1.1.3\" xref=\"S6.T5.1.1.1.1.m1.1.1.3.cmml\">2</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S6.T5.1.1.1.1.m1.1b\"><apply id=\"S6.T5.1.1.1.1.m1.1.1.cmml\" xref=\"S6.T5.1.1.1.1.m1.1.1\"><eq id=\"S6.T5.1.1.1.1.m1.1.1.1.cmml\" xref=\"S6.T5.1.1.1.1.m1.1.1.1\"></eq><apply id=\"S6.T5.1.1.1.1.m1.1.1.2.cmml\" xref=\"S6.T5.1.1.1.1.m1.1.1.2\"><times id=\"S6.T5.1.1.1.1.m1.1.1.2.1.cmml\" xref=\"S6.T5.1.1.1.1.m1.1.1.2.1\"></times><ci id=\"S6.T5.1.1.1.1.m1.1.1.2.2.cmml\" xref=\"S6.T5.1.1.1.1.m1.1.1.2.2\">#</ci><ci id=\"S6.T5.1.1.1.1.m1.1.1.2.3.cmml\" xref=\"S6.T5.1.1.1.1.m1.1.1.2.3\">ğ¶</ci></apply><cn type=\"integer\" id=\"S6.T5.1.1.1.1.m1.1.1.3.cmml\" xref=\"S6.T5.1.1.1.1.m1.1.1.3\">2</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T5.1.1.1.1.m1.1c\">\\#C=2</annotation></semantics></math>)</th>\n<th id=\"S6.T5.1.1.1.2\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_border_tt\">Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S6.T5.1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S6.T5.1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedAvg</td>\n<td id=\"S6.T5.1.1.2.1.2\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">73.19</td>\n</tr>\n<tr id=\"S6.T5.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S6.T5.1.1.3.2.1\" class=\"ltx_td ltx_align_left\">FedAvg w/ CCVR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">19</a>]</cite>\n</td>\n<td id=\"S6.T5.1.1.3.2.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">75.95</td>\n</tr>\n<tr id=\"S6.T5.1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S6.T5.1.1.4.3.1\" class=\"ltx_td ltx_align_left\">FedFA w/ CCVR</td>\n<td id=\"S6.T5.1.1.4.3.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">84.94</td>\n</tr>\n<tr id=\"S6.T5.1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S6.T5.1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">FedFA w/o CC</td>\n<td id=\"S6.T5.1.1.5.4.2\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_t\">76.81</td>\n</tr>\n<tr id=\"S6.T5.1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S6.T5.1.1.6.5.1\" class=\"ltx_td ltx_align_left\">FedFA w/ CC after training</td>\n<td id=\"S6.T5.1.1.6.5.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">76.94</td>\n</tr>\n<tr id=\"S6.T5.1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S6.T5.1.1.7.6.1\" class=\"ltx_td ltx_align_left\">FedFA w/ CC at the end of each epoch</td>\n<td id=\"S6.T5.1.1.7.6.2\" class=\"ltx_td ltx_nopad_r ltx_align_left\">82.05</td>\n</tr>\n<tr id=\"S6.T5.1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S6.T5.1.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\">FedFA at the end of each mini batch</td>\n<td id=\"S6.T5.1.1.8.7.2\" class=\"ltx_td ltx_nopad_r ltx_align_left ltx_border_bb ltx_border_t\">84.90</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Compared with [19] that calibrates classifier with virtual representation (CCVR) after training, we perform it\nduring different phases of training and the setting of Table V is the same as Table I.\nThe result of each mini-batch calibration done by FedFA is the best in all cases.\nThis reveals that the classifier divergence induced by data heterogeneity should be corrected as early as possible.\nMeanwhile, maintaining the virtuous cycle between feature and classifier updates during local training helps the final model converge at a point that generalizes better, since compared with FedFA without classifier calibration, FedFA with classifier calibration after training only improves little."
        ]
    },
    "A0.T6": {
        "caption": "TABLE VI: The specific parameters settings for all the models used in our experiments.",
        "table": "<table id=\"A0.T6.1.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"A0.T6.1.1.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.1.1.1\" class=\"ltx_td ltx_border_r ltx_border_tt\"></td>\n<td id=\"A0.T6.1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\">Validation Experiment</td>\n<td id=\"A0.T6.1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\">Test Experiment</td>\n</tr>\n<tr id=\"A0.T6.1.1.2.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A0.T6.1.1.2.2.1.1\" class=\"ltx_text\">Layer</span></td>\n<td id=\"A0.T6.1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">Label Skew</td>\n<td id=\"A0.T6.1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Feature Skew</td>\n<td id=\"A0.T6.1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\">Label Skew</td>\n<td id=\"A0.T6.1.1.2.2.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">Feature Skew</td>\n</tr>\n<tr id=\"A0.T6.1.1.3.3\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_t\">FMNIST</td>\n<td id=\"A0.T6.1.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">Mixed-digit dataset</td>\n<td id=\"A0.T6.1.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">FMNIST/EMNIST</td>\n<td id=\"A0.T6.1.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">CIFAR-10</td>\n<td id=\"A0.T6.1.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">CIFAR-100</td>\n<td id=\"A0.T6.1.1.3.3.6\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">Mixed-digit dataset</td>\n</tr>\n<tr id=\"A0.T6.1.1.4.4\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"A0.T6.1.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.4.4.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.4.4.2.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(1, 32, 5)</td>\n</tr>\n<tr id=\"A0.T6.1.1.4.4.2.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU,MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"A0.T6.1.1.4.4.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.4.4.3.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(3, 64, 5)</td>\n</tr>\n<tr id=\"A0.T6.1.1.4.4.3.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU,MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.4.4.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.4.4.4.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(1, 32, 5)</td>\n</tr>\n<tr id=\"A0.T6.1.1.4.4.4.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU,MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.4.4.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.4.4.5.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(3, 64, 5)</td>\n</tr>\n<tr id=\"A0.T6.1.1.4.4.5.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU,MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.4.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"A0.T6.1.1.4.4.6.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.4.4.6.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.6.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Basicbone of Resnet18</td>\n</tr>\n<tr id=\"A0.T6.1.1.4.4.6.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.6.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">with GroupNorm</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.4.4.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.4.4.7.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.4.4.7.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.7.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(3, 64, 5, 1, 2)</td>\n</tr>\n<tr id=\"A0.T6.1.1.4.4.7.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.4.4.7.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU,MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"A0.T6.1.1.5.5\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"A0.T6.1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.5.5.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.5.5.2.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(32, 32, 5)</td>\n</tr>\n<tr id=\"A0.T6.1.1.5.5.2.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU,MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"A0.T6.1.1.5.5.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.5.5.3.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(64, 64, 5)</td>\n</tr>\n<tr id=\"A0.T6.1.1.5.5.3.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU, MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.5.5.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.5.5.4.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(32, 32, 5)</td>\n</tr>\n<tr id=\"A0.T6.1.1.5.5.4.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU,MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.5.5.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.5.5.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.5.5.5.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(64, 64, 5)</td>\n</tr>\n<tr id=\"A0.T6.1.1.5.5.5.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU, MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.5.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"A0.T6.1.1.5.5.6.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.5.5.6.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.6.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FC(512,512)</td>\n</tr>\n<tr id=\"A0.T6.1.1.5.5.6.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.6.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.5.5.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.5.5.7.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.5.5.7.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.7.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(64, 64, 5, 1, 2)</td>\n</tr>\n<tr id=\"A0.T6.1.1.5.5.7.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.5.5.7.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU,MaxPool2D(2,2)</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"A0.T6.1.1.6.6\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3</td>\n<td id=\"A0.T6.1.1.6.6.2\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.6.6.2.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.6.6.2.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.2.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FC(992,384)</td>\n</tr>\n<tr id=\"A0.T6.1.1.6.6.2.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.2.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.6.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<table id=\"A0.T6.1.1.6.6.3.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.6.6.3.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.3.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FC(1024,384)</td>\n</tr>\n<tr id=\"A0.T6.1.1.6.6.3.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.3.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.6.6.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.6.6.4.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FC(992,384)</td>\n</tr>\n<tr id=\"A0.T6.1.1.6.6.4.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.6.6.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.6.6.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.6.6.5.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FC(1600,384)</td>\n</tr>\n<tr id=\"A0.T6.1.1.6.6.5.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.6.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FC(512,256)</td>\n<td id=\"A0.T6.1.1.6.6.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.6.6.7.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.6.6.7.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.7.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">Conv2d(3, 128, 5, 1, 2)</td>\n</tr>\n<tr id=\"A0.T6.1.1.6.6.7.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.6.6.7.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"A0.T6.1.1.7.7\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.7.7.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4</td>\n<td id=\"A0.T6.1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_t\">FC(384,100)</td>\n<td id=\"A0.T6.1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FC(384,100)</td>\n<td id=\"A0.T6.1.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.7.7.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.7.7.4.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.7.7.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FC(384,192)</td>\n</tr>\n<tr id=\"A0.T6.1.1.7.7.4.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.7.7.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.7.7.5\" class=\"ltx_td ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.7.7.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.7.7.5.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.7.7.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FC(384,192)</td>\n</tr>\n<tr id=\"A0.T6.1.1.7.7.5.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.7.7.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.7.7.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FC(256,100)</td>\n<td id=\"A0.T6.1.1.7.7.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.7.7.7.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.7.7.7.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.7.7.7.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FC(6272, 2048)</td>\n</tr>\n<tr id=\"A0.T6.1.1.7.7.7.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.7.7.7.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"A0.T6.1.1.8.8\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.8.8.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5</td>\n<td id=\"A0.T6.1.1.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_t\">FC(100,10)</td>\n<td id=\"A0.T6.1.1.8.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">FC(100,10)</td>\n<td id=\"A0.T6.1.1.8.8.4\" class=\"ltx_td ltx_align_center ltx_border_t\">FC(192,10)</td>\n<td id=\"A0.T6.1.1.8.8.5\" class=\"ltx_td ltx_align_center ltx_border_t\">FC(192,10)</td>\n<td id=\"A0.T6.1.1.8.8.6\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"A0.T6.1.1.8.8.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">\n<table id=\"A0.T6.1.1.8.8.7.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.8.8.7.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.8.8.7.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">FC(2048,512)</td>\n</tr>\n<tr id=\"A0.T6.1.1.8.8.7.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.8.8.7.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">ReLU</td>\n</tr>\n</table>\n</td>\n</tr>\n<tr id=\"A0.T6.1.1.9.9\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.9.9.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6</td>\n<td id=\"A0.T6.1.1.9.9.2\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A0.T6.1.1.9.9.3\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"A0.T6.1.1.9.9.4\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A0.T6.1.1.9.9.5\" class=\"ltx_td ltx_border_t\"></td>\n<td id=\"A0.T6.1.1.9.9.6\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n<td id=\"A0.T6.1.1.9.9.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">FC(512,10)</td>\n</tr>\n<tr id=\"A0.T6.1.1.10.10\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.10.10.1\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">Source</td>\n<td id=\"A0.T6.1.1.10.10.2\" class=\"ltx_td ltx_border_bb ltx_border_t\"></td>\n<td id=\"A0.T6.1.1.10.10.3\" class=\"ltx_td ltx_border_bb ltx_border_r ltx_border_t\"></td>\n<td id=\"A0.T6.1.1.10.10.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<table id=\"A0.T6.1.1.10.10.4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.10.10.4.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.10.10.4.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">model from</td>\n</tr>\n<tr id=\"A0.T6.1.1.10.10.4.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.10.10.4.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">18</a>]</cite></td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.10.10.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">\n<table id=\"A0.T6.1.1.10.10.5.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.10.10.5.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.10.10.5.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">model from</td>\n</tr>\n<tr id=\"A0.T6.1.1.10.10.5.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.10.10.5.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib18\" title=\"\" class=\"ltx_ref\">18</a>]</cite></td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.10.10.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">\n<table id=\"A0.T6.1.1.10.10.6.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.10.10.6.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.10.10.6.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">model from</td>\n</tr>\n<tr id=\"A0.T6.1.1.10.10.6.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.10.10.6.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">14</a>]</cite></td>\n</tr>\n</table>\n</td>\n<td id=\"A0.T6.1.1.10.10.7\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\">\n<table id=\"A0.T6.1.1.10.10.7.1\" class=\"ltx_tabular ltx_align_middle\">\n<tr id=\"A0.T6.1.1.10.10.7.1.1\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.10.10.7.1.1.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\">model from</td>\n</tr>\n<tr id=\"A0.T6.1.1.10.10.7.1.2\" class=\"ltx_tr\">\n<td id=\"A0.T6.1.1.10.10.7.1.2.1\" class=\"ltx_td ltx_nopad_r ltx_align_center\"><cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">36</a>]</cite></td>\n</tr>\n</table>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Our validation and test experiments, including label distribution skew, feature distribution skew, and label &\\& feature distribution skews, use the models according to Table VI.\nHerein, to ablate the effect of BN layers, we\nreplace the BN layer with the GroupNorm layer in all experiments.\nFor fair comparison, our models follow those reported in the baselinesâ€™ works.\nSpecifically, following [18], we use a CNN model for EMNIST, FMNIST, and CIFAR-10, consisting of\ntwo 5x5 convolution layers followed by 2x2 max pooling and two fully-connected layers with ReLU activation.\nFollowing [14] and [36], we utilize the ResNet-18 [49] with a linear projector for CIFAR-100 and a CNN model with three 5x5 convolution layers followed by five GroupNorm layers for the Mixed Digits dataset."
        ]
    }
}