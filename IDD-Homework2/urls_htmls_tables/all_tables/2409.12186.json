{
    "id_table_1": {
        "caption": "Table 1:  Architecture of Qwen2.5-Coder.",
        "table": "id3.2",
        "footnotes": [],
        "references": [
            "The architecture of Qwen2.5-Coder is the same as Qwen2.5. Table  1  shows the architecture of Qwen2.5-Coder for two different model sizes: 1.5B and 7B parameters. Both sizes share the same architecture in terms of layers, having 28 layers and a head size of 128. However, they differ in several key aspects. The 1.5B model has a hidden size of 1,536, while the 7B model has a much larger hidden size of 3,584. The 1.5B model uses 12 query heads and 2 key-value heads, whereas the 7B model uses 28 query heads and 4 key-value heads, reflecting its larger capacity. The intermediate size also scales with model size, being 8,960 for the 1.5B model and 18,944 for the 7B model. Additionally, the 1.5B model employs embedding tying, while the 7B model does not. Both models share the same vocabulary size of 151,646 tokens and have been trained on 5.5 trillion tokens.",
            "In Qwen2.5-Coder, we applied this process iteratively. As shown in Figure  1 , each iteration resulted in improvement. Through 4-stage filtering, the average scores on HumanEval and MBPP increased from 41.6% to 46.8% compared to the baseline, demonstrating the value of high-quality Text-Code Grounding Data for code generation.",
            "In addition to mathematical ability, we aim to retain as much of the base models general-purpose capabilities as possible, such as general knowledge. To evaluate general natural language understanding, we selected MMLU  (Hendrycks et al.,  2021 )  and its variant MMLU-Redux  (Gema et al.,  2024 ) , along with four other benchmarks: ARC-Challenge  (Clark et al.,  2018 ) , TruthfulQA  (Lin et al.,  2021 ) , WinoGrande  (Sakaguchi et al.,  2019 ) , and HellaSwag  (Zellers et al.,  2019 ) . Similar to the results in mathematics, Table  11  highlights Qwen2.5-Coders advantage in general natural language capabilities compared to other coders, further validating the effectiveness of Qwen2.5-Coder data mixing strategy.",
            "We also evaluated the code generation capabilities of the Qwen2.5-Coder series instruct models using the EvalPlus  (Liu et al.,  2023 )  dataset. As illustrated by the experimental results in Table  13 , our Qwen2.5-Coder-7B-Instruct model demonstrated superior accuracy, significantly outperforming other models with a comparable number of parameters. Remarkably, it even exceeded the performance of larger models with over 20 billion parameters, such as CodeStral-22B and DS-Coder-33B-Instruct. Notably, Qwen2.5-Coder-7B-Instruct was the only model in our evaluation to surpass an 80% accuracy rate on HumanEval+, achieving an impressive 84.1%.",
            "The  instruct  split provided by BigCodeBench  (Zhuo et al.,  2024 )  is intended for assessing the code generation abilities of instruct models. We assessed the Qwen2.5-Coder series instruct models on the BigCodeBench-Instruct. As shown in Table  13 , the Qwen2.5-Coder-7B-Instruct outperformed other instruct models with similar parameter sizes, achieving higher accuracy scores on both the full and hard subsets, reaching 41.0% on the full subset and 18.2% on the hard subset, demonstrating the Qwen2.5-Coder series instruct models powerful code generation capabilities.",
            "To better demonstrate our models effectiveness on real-world competitive programming tasks, we conduct evaluation of the Qwen-2.5-Coder series instruct models on the LiveCodeBench (2305-2409) dataset. As illustrated in Table  13 , the Qwen-2.5-Coder-7B-Instruct model achieved an impressive Pass@1 accuracy of 37.6%, significantly outperforming other models of comparable parameter scales. Notably, it also surpassed larger models such as CodeStral-22B and DS-Coder-33B-Instruct, underscoring the Qwen-2.5-Coder series exceptional capabilities in handling complex code generation challenges.",
            "As demonstrated by the evaluation results in Table  14 , Qwen2.5-Coder-7B-Instruct consistently outperforms other models with the same number of parameters, including DS-Coder-V2-Lite-Instruct, in code generation tasks across eight programming languages. With an average accuracy of 76.5%, Qwen2.5-Coder-7B-Instruct surpasses even larger models such as CodeStral-22B and DS-Coder-33B-Instruct (despite having over 20 billion parameters), highlighting its powerful code generation capabilities in multiple programming languages.",
            "To assess the code reasoning capabilities of the Qwen2.5-Coder series instruct models, we performed an evaluation on CRUXEval  (Gu et al.,  2024 ) . As illustrated by the experimental results in Table  15 , the Qwen2.5-Coder-7B-Instruct model achieved Input-CoT and Output-CoT accuracies of 65.8% and 65.9%, respectively. This represents a notable improvement over the DS-Coder-V2-Lite-Instruct model, with gains of 12.8% in Input-CoT accuracy and 13.0% in Output-CoT accuracy. Furthermore, the Qwen2.5-Coder-7B-Instruct model outperformed larger models, such as the CodeStral-22B and DS-Coder-33B-Instruct, underscoring its superior code reasoning capabilities despite its smaller size.",
            "Table  16  highlights the performance of several language models in the Code Editing task. Among them, Qwen2.5-Coder-7B-Instruct demonstrates outstanding code repair capabilities. Despite its relatively modest size of 7 billion parameters, it achieves an impressive PASS@1 accuracy of 50.4%, significantly outperforming comparable models. Notably, it also surpasses larger models such as CodeStral-22B (22 billion parameters) and DS-Coder-33B-Instruct (33 billion parameters), showcasing its remarkable efficiency and effectiveness in code editing tasks.",
            "In this section, we present a comparative analysis of the performance between our Qwen2.5-Coder-7B-Instruct model and the DS-Coder-V2-Lite-Instruct model, focusing on both mathematical computation and general natural language processing tasks. As indicated in Table  17 , the Qwen2.5-Coder-7B-Instruct model outperforms the DS-Coder-V2-Lite-Instruct in 11 out of 12 tasks. This result underscores the models versatility, excelling not only in complex coding tasks but also in sophisticated general tasks, thus distinguishing it from its competitors."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Overview of the special tokens.",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "Qwen2.5-Coder inherits the vocabulary from Qwen2.5 but introduces several special tokens to help the model better understand code. Table  2  presents an overview of the special tokens added during training to better capture different forms of code data. These tokens serve specific purposes in the code-processing pipeline. For instance,  <|endoftext|>  marks the end of a text or sequence, while the  <|fim_prefix|> ,  <|fim_middle|> , and  <|fim_suffix|>  tokens are used to implement the Fill-in-the-Middle (FIM)  (Bavarian et al.,  2022 )  technique, where a model predicts the missing parts of a code block. Additionally,  <|fim_pad|>  is used for padding during FIM operations. Other tokens include  <|repo_name|> , which identifies repository names, and  <|file_sep|> , used as a file separator to better manage repository-level information. These tokens are essential in helping the model learn from diverse code structures and enable it to handle longer and more complex contexts during both file-level and repo-level pretraining.",
            "As shown in  2 , we employed a three-stage training approach to train Qwen2.5-Coder, including file-level pretraining, repo-level pretraining, and instruction tuning."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  The performance of Qwen2.5-Coder training on different data mixture policy.",
        "table": "S2.T2.1",
        "footnotes": [],
        "references": [
            "Balancing Code, Math, and Text data is crucial for building a robust foundational model. Although the research community has explored this balance before, there is limited evidence regarding its scalability to large datasets. To address this, we conducted empirical experiments with different ratios of Code, Math, and Text data, designing multiple experiments to identify an optimal combination rapidly. Specifically, as shown in Table  3 , we compared three different Code: Text ratios  100:0:0, 85:10:5, and 70:20:10.",
            "File-level pretraining focuses on learning from individual code files. In this stage, the maximum training sequence length is set to 8,192 tokens, covering 5.2T of high-quality data. The training objectives include next token prediction and fill-in-the-middle (FIM)  (Bavarian et al.,  2022 ) . The specific FIM format is shown in Figure  3 .",
            "We also evaluated the code generation capabilities of the Qwen2.5-Coder series instruct models using the EvalPlus  (Liu et al.,  2023 )  dataset. As illustrated by the experimental results in Table  13 , our Qwen2.5-Coder-7B-Instruct model demonstrated superior accuracy, significantly outperforming other models with a comparable number of parameters. Remarkably, it even exceeded the performance of larger models with over 20 billion parameters, such as CodeStral-22B and DS-Coder-33B-Instruct. Notably, Qwen2.5-Coder-7B-Instruct was the only model in our evaluation to surpass an 80% accuracy rate on HumanEval+, achieving an impressive 84.1%.",
            "The  instruct  split provided by BigCodeBench  (Zhuo et al.,  2024 )  is intended for assessing the code generation abilities of instruct models. We assessed the Qwen2.5-Coder series instruct models on the BigCodeBench-Instruct. As shown in Table  13 , the Qwen2.5-Coder-7B-Instruct outperformed other instruct models with similar parameter sizes, achieving higher accuracy scores on both the full and hard subsets, reaching 41.0% on the full subset and 18.2% on the hard subset, demonstrating the Qwen2.5-Coder series instruct models powerful code generation capabilities.",
            "To better demonstrate our models effectiveness on real-world competitive programming tasks, we conduct evaluation of the Qwen-2.5-Coder series instruct models on the LiveCodeBench (2305-2409) dataset. As illustrated in Table  13 , the Qwen-2.5-Coder-7B-Instruct model achieved an impressive Pass@1 accuracy of 37.6%, significantly outperforming other models of comparable parameter scales. Notably, it also surpassed larger models such as CodeStral-22B and DS-Coder-33B-Instruct, underscoring the Qwen-2.5-Coder series exceptional capabilities in handling complex code generation challenges."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  All artifacts released and used in this section.",
        "table": "S3.T3.1.1",
        "footnotes": [],
        "references": [
            "In this stage, we used a large amount of high-quality, long-code data (  \\approx   300B) and extended file-level FIM to the repo-level FIM followed by methods described in  Lozhkov et al. ( 2024 ) , with the specific format shown in Figure  4 .",
            "As demonstrated by the evaluation results in Table  14 , Qwen2.5-Coder-7B-Instruct consistently outperforms other models with the same number of parameters, including DS-Coder-V2-Lite-Instruct, in code generation tasks across eight programming languages. With an average accuracy of 76.5%, Qwen2.5-Coder-7B-Instruct surpasses even larger models such as CodeStral-22B and DS-Coder-33B-Instruct (despite having over 20 billion parameters), highlighting its powerful code generation capabilities in multiple programming languages."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance of various models on HumanEval, MBPP and the complete task of BigCodeBench.",
        "table": "S6.T4.1",
        "footnotes": [],
        "references": [
            "As shown in Table  5 , Qwen2.5-Coder have shown impressive performance in basic code generation, achieving state-of-the-art results among open-source models of the same size and surpassing even larger models. In particular, Qwen2.5-Coder-7B-Base outperforms the previous best dense model, DS-Coder-33B-Base, across all five metrics.",
            "Table  5  illustrates that Qwen2.5-Coder continues to show strong performance on BigCodeBench-Complete, underscoring the models generalization potential.",
            "To assess the code reasoning capabilities of the Qwen2.5-Coder series instruct models, we performed an evaluation on CRUXEval  (Gu et al.,  2024 ) . As illustrated by the experimental results in Table  15 , the Qwen2.5-Coder-7B-Instruct model achieved Input-CoT and Output-CoT accuracies of 65.8% and 65.9%, respectively. This represents a notable improvement over the DS-Coder-V2-Lite-Instruct model, with gains of 12.8% in Input-CoT accuracy and 13.0% in Output-CoT accuracy. Furthermore, the Qwen2.5-Coder-7B-Instruct model outperformed larger models, such as the CodeStral-22B and DS-Coder-33B-Instruct, underscoring its superior code reasoning capabilities despite its smaller size."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Performance of different models on MultiPL-E.",
        "table": "S6.T5.1.1",
        "footnotes": [],
        "references": [
            "As shown in the table  6 , Qwen2.5-Coder also achieved state-of-the-art results in the multi-programming language evaluation, with its capabilities well-balanced across various languages. It scored over 60% in five out of the eight languages.",
            "To comprehensively assess the code generation capabilities of the Qwen2.5-Coder series models across a broader range of programming languages, we evaluated them on the McEval benchmark  (Chai et al.,  2024 ) , which spans 40 programming languages and includes 16,000 test cases. As shown in Figure  6 , the Qwen2.5-Coder-7B-Instruct model excels when compared to other open-source models on the McEval benchmark, particularly across a wide range of programming languages. Its average accuracy not only exceeds that of much larger models such as DS-Coder-33B-Instruct and CodeStral-22B but also demonstrates a notable advantage over models of comparable parameter size.",
            "Table  16  highlights the performance of several language models in the Code Editing task. Among them, Qwen2.5-Coder-7B-Instruct demonstrates outstanding code repair capabilities. Despite its relatively modest size of 7 billion parameters, it achieves an impressive PASS@1 accuracy of 50.4%, significantly outperforming comparable models. Notably, it also surpasses larger models such as CodeStral-22B (22 billion parameters) and DS-Coder-33B-Instruct (33 billion parameters), showcasing its remarkable efficiency and effectiveness in code editing tasks."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Performance of different approaches on the HumanEval Infilling Tasks",
        "table": "S6.T6.1.1",
        "footnotes": [],
        "references": [
            "The table  7  illustrates that Qwen2.5-Coder surpasses alternative models concerning model size. Specifically,  Qwen2.5-Coder-1.5B  achieves an average performance improvement of 3.7%, rivaling the majority of models exceeding 6 billion parameters. Moreover,  Qwen2.5-Coder-7B-Base  stands as the leading model among those over 6 billion parameters, matching the performance of the formidable 33 billion parameter model, DS-Coder-33B-Base. Notably, we excluded DS-Coder-v2-236B from comparison due to its design focus not being on code completion tasks.",
            "Figure  7  illustrates the relationship between model sizes and code reasoning capabilities. The Qwen2.5-Coder instruct models stand out for delivering superior code reasoning performance with the fewest parameters, surpassing the results of other open-source large language models by a significant margin. According to this trend, we expect that code reasoning performance comparable to GPT-4o could be achieved with a model around the 30 billion parameters scale.",
            "In this section, we present a comparative analysis of the performance between our Qwen2.5-Coder-7B-Instruct model and the DS-Coder-V2-Lite-Instruct model, focusing on both mathematical computation and general natural language processing tasks. As indicated in Table  17 , the Qwen2.5-Coder-7B-Instruct model outperforms the DS-Coder-V2-Lite-Instruct in 11 out of 12 tasks. This result underscores the models versatility, excelling not only in complex coding tasks but also in sophisticated general tasks, thus distinguishing it from its competitors."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Performance of different models on CRUXEval with  Input-CoT  and  Output-CoT  settings.",
        "table": "S6.T7.1",
        "footnotes": [],
        "references": [
            "As shown in Table  8 , Qwen2.5-Coder delivered highly promising results, achieving a score of 56.5 on CRUXEval-I and 56.0 on CRUXEval-O, thanks to our focus on executable quality during the code cleaning process.",
            "Thanks to the use of finely crafted synthetic data during both pre-training and fine-tuning, we significantly enhanced Qwen2.5-Coders capability in Text-to-SQL tasks. We selected two well-known benchmarks, Spider  (Yu et al.,  2018 )  and BIRD  (Li et al.,  2024 ) , for comprehensive evaluation. To ensure a fair comparison between Qwen2.5-Coder and other open-source language models on this task, we used a unified prompt template as input, following the work of  Chang & Fosler-Lussier ( 2023 ) . As shown in Figure  8 , the prompt consists of table representations aligned with database instructions, examples of table content, optional additional knowledge, and natural language questions. This standardized prompt template minimizes biases that may arise from prompt variations. As shown in Figure  9 , Qwen2.5-Coder outperforms other code models of the same size on the Text-to-SQL task."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Performance of various models on four math benchmark, named MATH, GSM8K, MMLU STEM and TheoremQA respectively.",
        "table": "S6.T8.1",
        "footnotes": [],
        "references": [
            "Mathematics and coding have always been closely intertwined. Mathematics forms the foundational discipline for coding, while coding serves as a vital tool in mathematical fields. As such, we expect an open and powerful code model to exhibit strong mathematical capabilities as well. To assess Qwen2.5-Coders mathematical performance, we selected five popular benchmarks, including MATH  (Hendrycks et al.,  2021 ) , GSM8K  (Cobbe et al.,  2021 ) , MMLU-STEM  (Hendrycks et al.,  2020 )  and TheoremQA  (Chen et al.,  2023 ) . As shown in Table  9 , Table 3 highlights Qwen2.5-Coders strengths in mathematics, which likely stem from two key factors: first, the models strong foundation built on Qwen2.5, and second, the careful mixing of code and mathematical data during training, which has ensured a well-balanced performance across these domains.",
            "Thanks to the use of finely crafted synthetic data during both pre-training and fine-tuning, we significantly enhanced Qwen2.5-Coders capability in Text-to-SQL tasks. We selected two well-known benchmarks, Spider  (Yu et al.,  2018 )  and BIRD  (Li et al.,  2024 ) , for comprehensive evaluation. To ensure a fair comparison between Qwen2.5-Coder and other open-source language models on this task, we used a unified prompt template as input, following the work of  Chang & Fosler-Lussier ( 2023 ) . As shown in Figure  8 , the prompt consists of table representations aligned with database instructions, examples of table content, optional additional knowledge, and natural language questions. This standardized prompt template minimizes biases that may arise from prompt variations. As shown in Figure  9 , Qwen2.5-Coder outperforms other code models of the same size on the Text-to-SQL task."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  MMLU results of different models, a general benchmark for common knowledge.",
        "table": "S6.T9.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table 11:  General performance of different models on four popular general benchmark, ARC-Challenge, TruthfulQA, WinoGrande and HellaSwag.",
        "table": "S6.T10.1",
        "footnotes": [],
        "references": [
            "In addition to mathematical ability, we aim to retain as much of the base models general-purpose capabilities as possible, such as general knowledge. To evaluate general natural language understanding, we selected MMLU  (Hendrycks et al.,  2021 )  and its variant MMLU-Redux  (Gema et al.,  2024 ) , along with four other benchmarks: ARC-Challenge  (Clark et al.,  2018 ) , TruthfulQA  (Lin et al.,  2021 ) , WinoGrande  (Sakaguchi et al.,  2019 ) , and HellaSwag  (Zellers et al.,  2019 ) . Similar to the results in mathematics, Table  11  highlights Qwen2.5-Coders advantage in general natural language capabilities compared to other coders, further validating the effectiveness of Qwen2.5-Coder data mixing strategy."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  All artifacts released and used in this section.",
        "table": "S6.T11.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13:  The performance of different instruct models on code generation by HumanEval, MBPP, bigcodebench and livecodebench. For bigcodebench here, we report instruct tasks score.",
        "table": "S7.T12.1",
        "footnotes": [],
        "references": [
            "We also evaluated the code generation capabilities of the Qwen2.5-Coder series instruct models using the EvalPlus  (Liu et al.,  2023 )  dataset. As illustrated by the experimental results in Table  13 , our Qwen2.5-Coder-7B-Instruct model demonstrated superior accuracy, significantly outperforming other models with a comparable number of parameters. Remarkably, it even exceeded the performance of larger models with over 20 billion parameters, such as CodeStral-22B and DS-Coder-33B-Instruct. Notably, Qwen2.5-Coder-7B-Instruct was the only model in our evaluation to surpass an 80% accuracy rate on HumanEval+, achieving an impressive 84.1%.",
            "The  instruct  split provided by BigCodeBench  (Zhuo et al.,  2024 )  is intended for assessing the code generation abilities of instruct models. We assessed the Qwen2.5-Coder series instruct models on the BigCodeBench-Instruct. As shown in Table  13 , the Qwen2.5-Coder-7B-Instruct outperformed other instruct models with similar parameter sizes, achieving higher accuracy scores on both the full and hard subsets, reaching 41.0% on the full subset and 18.2% on the hard subset, demonstrating the Qwen2.5-Coder series instruct models powerful code generation capabilities.",
            "To better demonstrate our models effectiveness on real-world competitive programming tasks, we conduct evaluation of the Qwen-2.5-Coder series instruct models on the LiveCodeBench (2305-2409) dataset. As illustrated in Table  13 , the Qwen-2.5-Coder-7B-Instruct model achieved an impressive Pass@1 accuracy of 37.6%, significantly outperforming other models of comparable parameter scales. Notably, it also surpassed larger models such as CodeStral-22B and DS-Coder-33B-Instruct, underscoring the Qwen-2.5-Coder series exceptional capabilities in handling complex code generation challenges."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  The performance of different approaches on instruct format MultiPL-E.",
        "table": "S7.T13.1.1",
        "footnotes": [],
        "references": [
            "As demonstrated by the evaluation results in Table  14 , Qwen2.5-Coder-7B-Instruct consistently outperforms other models with the same number of parameters, including DS-Coder-V2-Lite-Instruct, in code generation tasks across eight programming languages. With an average accuracy of 76.5%, Qwen2.5-Coder-7B-Instruct surpasses even larger models such as CodeStral-22B and DS-Coder-33B-Instruct (despite having over 20 billion parameters), highlighting its powerful code generation capabilities in multiple programming languages."
        ]
    },
    "id_table_15": {
        "caption": "Table 15:  The CRUXEval performance of different instruct models, with  Input-CoT  and  Output-CoT  settings.",
        "table": "S7.T14.1.1",
        "footnotes": [],
        "references": [
            "To assess the code reasoning capabilities of the Qwen2.5-Coder series instruct models, we performed an evaluation on CRUXEval  (Gu et al.,  2024 ) . As illustrated by the experimental results in Table  15 , the Qwen2.5-Coder-7B-Instruct model achieved Input-CoT and Output-CoT accuracies of 65.8% and 65.9%, respectively. This represents a notable improvement over the DS-Coder-V2-Lite-Instruct model, with gains of 12.8% in Input-CoT accuracy and 13.0% in Output-CoT accuracy. Furthermore, the Qwen2.5-Coder-7B-Instruct model outperformed larger models, such as the CodeStral-22B and DS-Coder-33B-Instruct, underscoring its superior code reasoning capabilities despite its smaller size."
        ]
    },
    "id_table_16": {
        "caption": "Table 16:   The code editing ability of different instruct models evaluated by Aider benchmark. * indicates that the experimental results have been reproduced in our experiments, and the  whole  edit-format was consistently applied across all experiments.",
        "table": "S7.T15.1.1",
        "footnotes": [],
        "references": [
            "Table  16  highlights the performance of several language models in the Code Editing task. Among them, Qwen2.5-Coder-7B-Instruct demonstrates outstanding code repair capabilities. Despite its relatively modest size of 7 billion parameters, it achieves an impressive PASS@1 accuracy of 50.4%, significantly outperforming comparable models. Notably, it also surpasses larger models such as CodeStral-22B (22 billion parameters) and DS-Coder-33B-Instruct (33 billion parameters), showcasing its remarkable efficiency and effectiveness in code editing tasks."
        ]
    },
    "id_table_17": {
        "caption": "Table 17:  The performance of math and General.",
        "table": "S7.T16.1",
        "footnotes": [],
        "references": [
            "In this section, we present a comparative analysis of the performance between our Qwen2.5-Coder-7B-Instruct model and the DS-Coder-V2-Lite-Instruct model, focusing on both mathematical computation and general natural language processing tasks. As indicated in Table  17 , the Qwen2.5-Coder-7B-Instruct model outperforms the DS-Coder-V2-Lite-Instruct in 11 out of 12 tasks. This result underscores the models versatility, excelling not only in complex coding tasks but also in sophisticated general tasks, thus distinguishing it from its competitors."
        ]
    },
    "id_table_18": {
        "caption": "",
        "table": "S7.T17.1.1",
        "footnotes": [],
        "references": []
    }
}