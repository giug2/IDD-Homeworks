{
    "id_table_1": {
        "caption": "Table 1:  Impact of selecting  5 vs 20 few-shot  samples for TST and Pre-trained Model without adding API Function Definitions using GPT-4. The baseline uses Pre-trained Transformer Model with 5 few-shot samples.",
        "table": "S4.T1.12",
        "footnotes": [],
        "references": [
            "Domain Specific Languages (or DSLs) are custom computer languages designed and optimized for specific applications. Examples of DSLs include SQL and industry-specific languages for formalizing API calls, often using formats like JSON or YAML to represent API sequences. In this paper, we focus on the task of generating a DSL used for authoring high-level automation workflows across thousands of web-scale APIs. These workflows support a variety of customer scenarios like invoice processing, sales lead integration with forms/emails etc. The automation DSL represents API names as functions and codifies a sequence of API calls (or a plan) along with conditional logic over the invocation of APIs. The syntax itself borrows from known languages like Javascript, however, the logic resembling the workflow along with the custom function names, make it unique. An example of the DSL is shown in Figure  1 .",
            "NL2DSL generation suffers from the hallucination and quality issues we discussed in Section  1 . Few studies address the challenges of end-to-end DSL generation, specifically over a large set of custom APIs. In this paper, we present an end-to-end system architecture with improved strategies to add grounding context with known RAG techniques. We also present an ablation study showing improvements for DSL generation quality for enterprise settings. DSL samples in our test set consider API or tool selection sequences of 5-6 API calls, also referred to as chain of tools, which is a first to the best of our knowledge. We also consider the real-world scenarios of adding conditional logic with API calls as shown with an example in Figure  1 .",
            "We took the Codex base model from OpenAI due to its pre-training with code samples and used LoRA-based fine-tuning approach. The training set consists of NL-DSL pairs, NL refers to the user query and the DSL represents the workflow that the user is looking to automate. The training set consists of a pool of 67k samples in the form of (prompt, flow) tuples with the NL generated synthetically (details in Section  4.1 , examples of NL-DSL are shared in Figure  1  and Appendix  8 ).",
            "The loss function used by TST (Equation  1  from  Poesia et al.,  2022 ) is minimizing the Mean-Squared Error between the vanilla loss functions comparing the utterances ( u i , u j subscript u i subscript u j u_{i},u_{j} italic_u start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_u start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ) and the target programs ( p i , p j subscript p i subscript p j p_{i},p_{j} italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ). Program similarity is denoted by  S S S italic_S . We used a Jaccard score over lists of API function names as the similarity metric between programs.",
            "We compare the impact of number of code samples added to the meta prompt with two different settings i.e.  5 5 5 5  few-shots vs  20 20 20 20  few-shots. We measured the results for both Pre-trained model as well as TST model. Results are shared in Table  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Impact of selecting  5 few-shot  samples using TST vs. Pre-trained Model with and without API Function Definitions using GPT4 model. The baseline uses Pre-trained Transformer Model without API Function Definitions.",
        "table": "S4.T2.12",
        "footnotes": [],
        "references": [
            "The remainder of this study is structured as follows. In Section  2 , we present the NL2DSL problem formulation along with literature review. Section  3  lays out and describes the optimizations we made to RAG as discussed above along with the benchmark Fine-Tuned model. Section  4  discusses Data Generation, Metric definition and Section  5  shares our results and discussion followed by Conclusion in Section  6 .",
            "Another approach for selecting function definitions is to retrieve semantically similar functions from a vector database created with API metadata. This approach is similar to the one followed by ( LlamaIndex,  ). We created an index of all API definitions and used the input NL query for search. Please note that this is different from the faiss index created for few-shot samples in Section  3.2 .",
            "To compare the impact of selecting samples using TST vs Pre-trained model, we look at the impact with and without the presence of API Function Definitions (see Table  2  and Table  3 ). Here, we have used GPT4 model with 5 and 20 few-shots, respectively. TST with FD setting performs overall better than all other options with values close to the best in every metric.",
            "Counter intuitively, with the exception of out-of-domain data, this benefit does not transfer to hallucinated API names and their parameter keys where the fine-tuned model holds the advantage (Table  4 ). Among RAG approaches (Tables  2  and  3 ), TST + Regular Function Definitions reduced hallucinations the most. Adding Semantic Function Definitions to TST + FD did not confer any advantage for in-domain APIs, but greatly improved code similarity for out-of domain APIs.",
            "Overall, we were able to significantly improve the performance of RAG for DSL generation, with hallucination rate for API names dropping by 6.29 pts. and by 20 pts for parameter keys (Table  2 ). The performance of RAG is now comparable to that of fine-tuned model (see Avg. Similarity in Table  4 ), with better performance for unseen APIs. This reduces the need to fine-tune the model frequently for new APIs saving compute and resources."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Impact of selecting  20 few-shot  samples using TST vs. Pre-trained Model with and without Function Definitions using GPT4 model. The baseline uses Pre-trained Transformer Model without API Function Definitions.",
        "table": "S5.T3.12",
        "footnotes": [],
        "references": [
            "The remainder of this study is structured as follows. In Section  2 , we present the NL2DSL problem formulation along with literature review. Section  3  lays out and describes the optimizations we made to RAG as discussed above along with the benchmark Fine-Tuned model. Section  4  discusses Data Generation, Metric definition and Section  5  shares our results and discussion followed by Conclusion in Section  6 .",
            "Another approach for selecting function definitions is to retrieve semantically similar functions from a vector database created with API metadata. This approach is similar to the one followed by ( LlamaIndex,  ). We created an index of all API definitions and used the input NL query for search. Please note that this is different from the faiss index created for few-shot samples in Section  3.2 .",
            "To compare the impact of selecting samples using TST vs Pre-trained model, we look at the impact with and without the presence of API Function Definitions (see Table  2  and Table  3 ). Here, we have used GPT4 model with 5 and 20 few-shots, respectively. TST with FD setting performs overall better than all other options with values close to the best in every metric.",
            "This leads us to conclude that the presence of few-shot examples is supported by adding their API functions definitions (as described in Section  3 ). The addition predominantly helps reducing the hallucination rate for API names and parameters, which improves the overall response rate of NL2DSL generation. This supports our initial premise: adding tool descriptions (like it is done in planning tasks) along with few-shot code samples helps improve reliability of plan generation.",
            "Counter intuitively, with the exception of out-of-domain data, this benefit does not transfer to hallucinated API names and their parameter keys where the fine-tuned model holds the advantage (Table  4 ). Among RAG approaches (Tables  2  and  3 ), TST + Regular Function Definitions reduced hallucinations the most. Adding Semantic Function Definitions to TST + FD did not confer any advantage for in-domain APIs, but greatly improved code similarity for out-of domain APIs."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Impact of adding API or tool related metadata on performance (with GPT-4 model and 20 few-shots). FD refers to including only metadata for APIs present in few-shots. SFD refers to extracting APIs similar to the input query (refer to Section  3 ) for details. The baseline uses fine-tuned Codex model.",
        "table": "S5.T4.12",
        "footnotes": [],
        "references": [
            "The remainder of this study is structured as follows. In Section  2 , we present the NL2DSL problem formulation along with literature review. Section  3  lays out and describes the optimizations we made to RAG as discussed above along with the benchmark Fine-Tuned model. Section  4  discusses Data Generation, Metric definition and Section  5  shares our results and discussion followed by Conclusion in Section  6 .",
            "We took the Codex base model from OpenAI due to its pre-training with code samples and used LoRA-based fine-tuning approach. The training set consists of NL-DSL pairs, NL refers to the user query and the DSL represents the workflow that the user is looking to automate. The training set consists of a pool of 67k samples in the form of (prompt, flow) tuples with the NL generated synthetically (details in Section  4.1 , examples of NL-DSL are shared in Figure  1  and Appendix  8 ).",
            "We used a Fine-Tuned model as baseline for this experiment (Table  4 ). Based on the insights from the previous step, we used 20 few-shots for TST along with including FDs.",
            "Based on the presented results, we see that the role of dynamically selected few-shot samples is very important in making RAG useful for syntactically correct generation of DSL as well as improving code similarity (Table  4 ). This could be due to the fact that few-shot examples have been successfully teaching the correct syntax to the LLM model. The positive role of relevant few-shot samples in improving RAGs syntactic accuracy is further confirmed by the drop seen for out of domain data. In absence of relevant few-shots for unseen APIs, we chose examples with low similarity, directly impacting the syntactic accuracy (Table  5 ).",
            "Counter intuitively, with the exception of out-of-domain data, this benefit does not transfer to hallucinated API names and their parameter keys where the fine-tuned model holds the advantage (Table  4 ). Among RAG approaches (Tables  2  and  3 ), TST + Regular Function Definitions reduced hallucinations the most. Adding Semantic Function Definitions to TST + FD did not confer any advantage for in-domain APIs, but greatly improved code similarity for out-of domain APIs.",
            "Overall, we were able to significantly improve the performance of RAG for DSL generation, with hallucination rate for API names dropping by 6.29 pts. and by 20 pts for parameter keys (Table  2 ). The performance of RAG is now comparable to that of fine-tuned model (see Avg. Similarity in Table  4 ), with better performance for unseen APIs. This reduces the need to fine-tune the model frequently for new APIs saving compute and resources."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance of RAG based model (TST + FD + SFD) on out of domain (OOD) and full test sets (with GPT-4 model and 20 few-shots). The baseline is fine-tuned Codex model on updated training data.",
        "table": "S5.T5.8",
        "footnotes": [],
        "references": [
            "The remainder of this study is structured as follows. In Section  2 , we present the NL2DSL problem formulation along with literature review. Section  3  lays out and describes the optimizations we made to RAG as discussed above along with the benchmark Fine-Tuned model. Section  4  discusses Data Generation, Metric definition and Section  5  shares our results and discussion followed by Conclusion in Section  6 .",
            "We share the results in Table  5 . The baseline is a fine-tuned Codex model with the updated training data. The RAG-based approach notably enhances average similarity (by 7 pts) and reduces API hallucinations (by 1.5 pts) for out of domain APIs. This indicates that when samples are not present in the train set, grounding with RAG context can provide the LLM support for improving code quality.",
            "Based on the presented results, we see that the role of dynamically selected few-shot samples is very important in making RAG useful for syntactically correct generation of DSL as well as improving code similarity (Table  4 ). This could be due to the fact that few-shot examples have been successfully teaching the correct syntax to the LLM model. The positive role of relevant few-shot samples in improving RAGs syntactic accuracy is further confirmed by the drop seen for out of domain data. In absence of relevant few-shots for unseen APIs, we chose examples with low similarity, directly impacting the syntactic accuracy (Table  5 )."
        ]
    },
    "global_footnotes": []
}