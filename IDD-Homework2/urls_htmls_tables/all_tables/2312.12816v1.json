{
    "Sx3.T1": {
        "caption": "Table 1: Comparison with existing methods on the MUSIC-AVQA dataset. We evaluate the performance of each model on different question types and use the QA accuracy (%) as the metric. We implement our APL method with different object detectors, e.g., the Faster R-CNN and DETR. The best and second-best results are bolded and underlined, respectively.",
        "table": "",
        "footnotes": "",
        "references": [
            "We conduct the proposed APL model with the DETR object detector here. An ablation study of different object detectors will be shown in the next section.\nThe experimental results are shown in TableÂ 1.\nWe report the model performance on different question types.\nFirst, our method significantly surpasses the previous SOTA method COCAÂ (Lao etÂ al. 2023) on audio question answering and visual question answering: the corresponding average accuracy values are improved by 2.67%\nand 4.46%,\nrespectively.\nQuestions under these two settings are simpler and only related to a single modality.\nThe performance improvements indicate the proposed method has a better ability to perceive single-modal scenes.\nSecond, our method is also superior in more challenging audio-visual question answering. Performance of the most competitive method COCA is close to the previous method STGÂ (Li etÂ al. 2022), while our method achieves 1% improvements.\nThese results reveal that our approach is superior in various QA settings.\nMoreover, we can also notice that it is more difficult for a model simultaneously handle different audio-visual question types as they focus on different audio-visual clues.\nParticularly, our method brings obvious improvement for the question type of â€˜countingâ€™, achieving 73.29% accuracy which considerably surpasses the second-best method STG (69.88%). This result verifies the benefits of our object-aware method design.",
            "Impacts of Object Detectors.\nTo explore the impact of object detectors in our method, here we employ the popular Faster R-CNN and DETR to implement the proposed method, respectively.\nAs shown in the bottom part of TableÂ 1, our method using the stronger DETR has the best performance, approaching 74.53% QA accuracy.\nAlso, it is noteworthy that our method equipped with each object detector is superior to existing methods.\nThese results demonstrate the generalization and robustness of our method.\nIn the following experiments, unless otherwise specified, we adopt the DETR as the object detector."
        ]
    },
    "Sx4.T2": {
        "caption": "Table 2: Ablation study of main network modules. We report the average accuracy of audio, visual, and audio-visual question answering. â€˜-â€™ denotes that module is not used.",
        "table": "",
        "footnotes": "",
        "references": [
            "Effects of Main Network Modules.\nOur network uses the question-conditioned clue discovery (QCD) module and modality-conditioned clue collection (MCC) module to encode question-related multi-modal features.\nWe conduct an ablation study to explore the impacts of these modules.\nThe experimental results are shown in TableÂ 2.\nThe first row (#1) denotes that the QCD and MCC are simply replaced with several fully connected layers to encode features.\nThe model performance in this extreme case without QCD and MCC modules is very low.\nAfter adopting the QCD or MCC module independently, the model performance improved substantially to âˆ¼similar-to\\sim72% (#2 and #3 in Table).\nThe model achieves the best performance when using both QCD and MCC modules (#4).\nThese results directly show the effectiveness of each proposed module, which is helpful to encode question-related audio-visual features for answer prediction."
        ]
    },
    "Sx4.T3": {
        "caption": "Table 3: Impact of threshold Ï†ğœ‘\\varphi used in positivity learning. This experiment uses the DETR that detects Nğ‘N=100 objects in each frame. We explore Ï†ğœ‘\\varphi around 1/N1ğ‘1/N = 0.010, i.e., the same attention to all objects.",
        "table": "",
        "footnotes": "",
        "references": [
            "Effects of Threshold Ï†ğœ‘\\varphi in â„’qâ€‹osubscriptâ„’ğ‘ğ‘œ\\mathcal{L}_{qo} and â„’aâ€‹osubscriptâ„’ğ‘ğ‘œ\\mathcal{L}_{ao}.\nÏ†ğœ‘\\varphi is the threshold for selecting highly relevant question-object and audio-object positivity pairs for contrastive learning (Eq.Â 5).\nHere we test several values of Ï†ğœ‘\\varphi to explore its effect.\nAs shown in TableÂ 3, the QA accuracy values of most question types change as the Ï†ğœ‘\\varphi varies.\nAnd the average performance across all question types approaches the highest when Ï†ğœ‘\\varphi is set to 0.011.\nNote that we use the DETR object detector in this experiment and it extracts N=100ğ‘100N=100 objects for each video frame. The cross-modal similarity of each question-object (or audio-object) pair will be 0.01 if they share the same attention.\nÏ†=0.011ğœ‘0.011\\varphi=0.011 is slightly larger than this value to ensure that fewer highly matched pairs are selected.\nWhen all the objects are selected as positivities (Ï†=0ğœ‘0\\varphi=0), â„’qâ€‹osubscriptâ„’ğ‘ğ‘œ\\mathcal{L}_{qo} (Eq.Â 6) and â„’aâ€‹osubscriptâ„’ğ‘ğ‘œ\\mathcal{L}_{ao} are equal to 0, the model has sub-optimal performance."
        ]
    }
}