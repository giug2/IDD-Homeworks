{
    "S3.T1": {
        "caption": "TABLE I: Component substitutions to be tested",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_rr ltx_border_t\"><span id=\"S3.T1.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Module</span></th>\n<th id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Baseline</span></th>\n<th id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Alternative Component</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_tt\"><span id=\"S3.T1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Image Encoder</span></th>\n<td id=\"S3.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">VGG-16</td>\n<td id=\"S3.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">ResNet-50, ResNet-152</td>\n</tr>\n<tr id=\"S3.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t\"><span id=\"S3.T1.1.3.2.1.1\" class=\"ltx_text ltx_font_bold\">Question Encoder</span></th>\n<td id=\"S3.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">LSTM</td>\n<td id=\"S3.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">BERT Transformer</td>\n</tr>\n<tr id=\"S3.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr ltx_border_t\"><span id=\"S3.T1.1.4.3.1.1\" class=\"ltx_text ltx_font_bold\">Feature Fusion</span></th>\n<td id=\"S3.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Concatenation</td>\n<td id=\"S3.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Stacked Attention Network</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "To improve performance over the baseline, we considered other components that could be used for the image encoder, question encoder and fusion algorithms in the model. These substitutions are summarised in Table I."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Test accuracy achieved by each model variation.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model Variation</span></th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><span id=\"S4.T2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Test Accuracy</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">VGG-16 + LSTM + Concatenation</td>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.56</td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_left\">ResNet-50 + LSTM + Concatenation</td>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.54</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_left\">ResNet-152 + LSTM + Concatenation</td>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.53</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T2.1.5.4.1.1\" class=\"ltx_text ltx_font_bold\">VGG-16 + BERT + Concatenation</span></td>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.5.4.2.1\" class=\"ltx_text ltx_font_bold\">0.60</span></td>\n</tr>\n<tr id=\"S4.T2.1.6.5\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.6.5.1\" class=\"ltx_td ltx_align_left\">VGG-16 + BERT + SAN</td>\n<td id=\"S4.T2.1.6.5.2\" class=\"ltx_td ltx_align_center\">0.58</td>\n</tr>\n<tr id=\"S4.T2.1.7.6\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.7.6.1\" class=\"ltx_td ltx_align_left\"><span id=\"S4.T2.1.7.6.1.1\" class=\"ltx_text ltx_font_bold\">VGG-16 + BioBERT + Concatenation</span></td>\n<td id=\"S4.T2.1.7.6.2\" class=\"ltx_td ltx_align_center\"><span id=\"S4.T2.1.7.6.2.1\" class=\"ltx_text ltx_font_bold\">0.60</span></td>\n</tr>\n<tr id=\"S4.T2.1.8.7\" class=\"ltx_tr\">\n<td id=\"S4.T2.1.8.7.1\" class=\"ltx_td ltx_align_left ltx_border_bb\"><span id=\"S4.T2.1.8.7.1.1\" class=\"ltx_text ltx_font_bold\">Pretrained VGG-16 + BERT + Concatenation</span></td>\n<td id=\"S4.T2.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S4.T2.1.8.7.2.1\" class=\"ltx_text ltx_font_bold\">0.60</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "In Table II the overall test accuracy achieved by each of the model variations as detailed in Section III are shown. Overall best performance is achieved using a VGG-16 image encoder, BERT Transformer question encoder and concatenation as the feature fusion strategy, achieving a test accuracy of 0.60±0.01plus-or-minus0.600.010.60\\pm 0.01. For the image encoder, we find that deeper and more complex networks such as ResNet-50 or ResNet-152 do not provide better results and in fact demonstrate a higher degree of overfitting. This clearly highlights the issue of the small dataset size of Med-VQA. For question encoding, the BERT transformer gave higher test performance compared to the LSTM network.",
            "Our results in Table II show that SAN as a feature fusion method did not improve results compared to the concatenation method. This is because there is simply not enough data for the model to learn a useful refined attention distribution, and the added complexity of generating an attention distribution can actually cause the model to obscure some useful information. By qualitatively examining the model’s generated attention distributions (examples of which are shown in Figure 5, with original images on the left, and attention distributions on the right), we find that the issue is that the questions in the dataset need either a holistic view of the image (for example most questions in the modality, plane and organ system categories), or a very strong focus on a small part of the image (for example most abnormality questions). For the former type, we find that the model either attends to all parts of the image more or less equally (e.g. Figure 5a), making attention redundant, or the attention distribution obscures relevant parts of the image (e.g. Figure 5b). For questions requiring good localisation, the model instead tends to produce attention distributions of the type shown in Figure 5c, where the model does not attend to any part of the image in particular. This is due to insufficient examples of particular abnormalities in the dataset, therefore the model is not able to learn the small areas of interest in these images, and instead does not focus on any part, leading to performance loss.",
            "The results in Table II show that neither domain-specific pretraining method that we tested provided a performance benefit. Our testing of BioBERT as the question encoder showed that medical domain-specific pretraining does not appear to be important to the question encoder for the VQA-Med 2019 dataset. This is likely because of the lack of medical language used in the questions of this dataset. As previously noted, the questions in this dataset are rigid in structure, likely generated from a limited number of templates. Almost all medical language in the text comes from the answers, rather than the questions. Although there are some exceptions (e.g. “angiogram”, “gastrointestinal”, “ultrasound”), these medical terms are never crucial to understanding the question as a whole. Therefore, using a question encoder that has been pretrained on the medical domain does not appear to help the model better understand the questions in this dataset."
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: Accuracy of the baseline versus BERT model per category type.",
        "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\"><span id=\"S4.T3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Model Variation</span></th>\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">Baseline</th>\n<th id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\">+BERT</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Modality</th>\n<td id=\"S4.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\">0.64</td>\n<td id=\"S4.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\">0.76</td>\n</tr>\n<tr id=\"S4.T3.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Plane</th>\n<td id=\"S4.T3.1.3.2.2\" class=\"ltx_td ltx_align_center\">0.78</td>\n<td id=\"S4.T3.1.3.2.3\" class=\"ltx_td ltx_align_center\">0.77</td>\n</tr>\n<tr id=\"S4.T3.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Organ</th>\n<td id=\"S4.T3.1.4.3.2\" class=\"ltx_td ltx_align_center\">0.74</td>\n<td id=\"S4.T3.1.4.3.3\" class=\"ltx_td ltx_align_center\">0.74</td>\n</tr>\n<tr id=\"S4.T3.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">Abnormality</th>\n<td id=\"S4.T3.1.5.4.2\" class=\"ltx_td ltx_align_center\">0.06</td>\n<td id=\"S4.T3.1.5.4.3\" class=\"ltx_td ltx_align_center\">0.08</td>\n</tr>\n<tr id=\"S4.T3.1.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Overall</th>\n<td id=\"S4.T3.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">0.56</td>\n<td id=\"S4.T3.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S4.T3.1.6.5.3.1\" class=\"ltx_text ltx_font_bold\">0.60</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "In Table III the test accuracy is shown by question category for the baseline model compared to the model with highest test accuracy, showing in more detail exactly what aspects the latter model improves on. There is a significant increase in accuracy in the modality category (+12%) and a modest increase in the abnormality category (+2%), which allows the +BERT model to achieve an increased overall accuracy."
        ]
    }
}