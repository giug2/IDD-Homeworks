{
    "id_table_1": {
        "caption": "Table 1:  Performance of IS and FID of StackGAN++, AttnGAN, SSGAN, DM-GAN, DTGAN, DF-GAN and our method on the CUB, Oxford and MS COCO datasets. The results are taken from the authors own papers.  The best results are in bold.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "In this paper, we explore data linear extrapolation to augment training data. Linear extrapolation can be risky, as similar text-image pairs may not be nearby in Euclidean space. For information reliability, as depicted in Figure  1 , we explore linear extrapolation only on text data, and new image data are retrieved from the internet by search engines. And then outlier detectors are designed to purify retrieved web images. In this way, the reliability of new text-image pairs are guaranteed by search precision and outlier detection.",
            "We present results for the CUB dataset of bird images, the Oxford-102 dataset of flower images, and the MS COCO dataset of common objects, as shown in Table  1 . On the CUB dataset, our model achieve an IS score of 6.56 and an FID score of 6.36, outperforming all the previous models. For the Oxford dataset, we achieve an IS score of 4.35 and an FID score of 6.36, outperforming all the previous models. On the COCO dataset, our model achieves an FID score of 5.00 that is competitive with previous best result.Compared with VQ-Diffusion, our model uses less training data and achieve much better performance. This comparison reveals that pre-training on large datasets can be inefficient and lead to suboptimal results.  Moreover, results in Table  1  reveal that Inception model pre-trained on ImageNet is less sensitive than fine-tuned on small datasets.  Additionally, the Inception score on the Oxford dataset exceeds that of real images (4.10). Extensive results demonstrate the effectiveness and generalization ability of the proposed data extrapolation method."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Ablation studies on the CUB dataset. We utilize a NULL-guidance ratio of 1.5 during sampling. The FID score was employed to evaluate generation performance.",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "In Table  2 , we present text-to-image results without cluster detector or classification detector. According to ID 0,1 and 2, the FID score without outlier detectors degrade severely because noisy images force the diffusion model to generate irrelevant objects. Although fine-tuning on small datasets could alleviate noise pollution but parameters also forget general knowledge at the same time. According to ID 2 and 3, classification detector performs better than cluster detector because it has utilized fine-grained classification labels.",
            "Text injection is crucial for text-to-image generation. As shown in ID 4 and 5 of Table  2 , RAT significantly improves the FID score. Further experiments indicate that directly mixing text feature with time embedding results in an FID score of 25.41, which is much worse than 16.74 achieved by RAT. This suggests that time embedding provides information very different to text embedding. Additionally, incorporating a scaling operator into RAT can lead to model collapse, as information becomes highly compressed in latent space. Consequently, the mean value of the latent code becomes sensitive, and the scaling operation disrupts the information structure."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:   Training consumption on the CUB, Oxford and COCO datasets. Fine-tuning is performed on the original dataset until the FID scores increase.",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "The text encoder is a pre-trained CLIP text encoder with an output of size  512 512 512 512 . The latent encoder and decoder is pre-trained by Stable Diffusion  (Rombach et al.,  2022 ) . We have tried to pre-train new latent encoders on extrapolated data but the results are not satisfying.  Adam optimizer is used to optimize the network with base learning rates of 0.0001 and weight decay of 0. The same as RAT-GAN, we used a mini-batch size of 24 to train the model. Most training and testing of our model are conducted on 2 RTX 3090 Ti and the detailed training consumption is listed in Table  3 .",
            "We present qualitative results for the CUB dataset of bird images and the Oxford-102 dataset of flower images. In Figure  3   , we compare the visualization results of DF-GAN, RAT-GAN, and our model. DF-GAN and RAT-GAN are previous state-of-the-art methods for text-to-image synthesis.  On the CUB dataset, with more clear details such as feathers, eyes, and feet, our model clearly outperforms DF-GAN and RAT-GAN. Additionally, the background in our models results is more coherent compared to RAT-GAN. On the Oxford dataset, our model exhibits better texture and more relevant colors than the others. With the proposed text extrapolation, RAT block, and null-guidance, our model demonstrates fewer distorted shapes and more relevant content compared to the other two models.",
            "We conduct ablation studies on the MS COCO dataset, as presented in Table  5 . The MS COCO dataset differs significantly from the CUB and Oxford datasets in terms of variety and image quantity. Experimental results demonstrate that linear extrapolation and fine-tuning (5.00) outperform the original COCO dataset (7.99). However, unlike CUB and Oxford, fine-tuning on COCO requires much more time, as shown in Table  3 . Additionally, we observe that early stopping is unnecessary for fine-tuning on the COCO dataset due to its larger image volume compared to CUB and Oxford."
        ]
    },
    "id_table_4": {
        "caption": "Table 5:   Ablation studies on the MS COCO dataset. We adopt A picture as the NULL prompt.",
        "table": "S4.T5.3",
        "footnotes": [],
        "references": [
            "The qualitative results for the COCO dataset are shown in Figure  4 . The COCO dataset includes a wide variety of common objects, which makes it particularly susceptible to the long-tail problem  (Chen et al.,  2022 ) . With additional training data obtained through extrapolation, our model generates more realistic objects compared to RAT-GAN. However, the collected 770,059 images are still insufficient to cover the entire distribution of images in COCO. As a result, the outputs from COCO are not as realistic as those from the CUB and Oxford datasets.",
            "The performance of NULL guidance is influenced by both the NULL prompt and the guidance ratio. The results in Table  4  indicate that a NULL prompt reflecting the average meaning of the dataset achieves the best performance. Additionally, a suitable guidance ratio is crucial for optimal results, and we find that a ratio around 1.5 yields the best performance on the CUB and COCO datasets. However, on the Oxford dataset, NULL guidance improves the Inception Score from 4.10 to 4.35 but degrades the FID score from 9.52 to 11.07."
        ]
    },
    "id_table_5": {
        "caption": "Table 6:  Comparison of pre-training dataset and parameter quantity of different models on the MS COCO dataset. Parameters for text encoder, latent encoder and super resolution are not counted.",
        "table": "S4.T6.1.1",
        "footnotes": [],
        "references": [
            "We conduct ablation studies on the MS COCO dataset, as presented in Table  5 . The MS COCO dataset differs significantly from the CUB and Oxford datasets in terms of variety and image quantity. Experimental results demonstrate that linear extrapolation and fine-tuning (5.00) outperform the original COCO dataset (7.99). However, unlike CUB and Oxford, fine-tuning on COCO requires much more time, as shown in Table  3 . Additionally, we observe that early stopping is unnecessary for fine-tuning on the COCO dataset due to its larger image volume compared to CUB and Oxford.",
            "To qualitatively evaluate the diversity of our proposed model, we generate random images conditioned on the same text description and different random noises. In Figure  5 , we present 10 images generated from the same text. These images exhibit similar foreground elements while showcasing high diversity in spatial structure, demonstrating that our model effectively controls the image content."
        ]
    },
    "global_footnotes": []
}