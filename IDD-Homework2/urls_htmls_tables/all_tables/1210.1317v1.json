{
    "S5.T1": {
        "caption": "TABLE I: Default top-10 workflows and their frequency in the top-5 positions.",
        "table": "<table id=\"S5.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.3.4.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">1</th>\n<td id=\"S5.T1.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"S5.T1.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3</td>\n<td id=\"S5.T1.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4</td>\n<td id=\"S5.T1.3.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5</td>\n<td id=\"S5.T1.3.4.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6</td>\n<td id=\"S5.T1.3.4.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">7</td>\n<td id=\"S5.T1.3.4.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"S5.T1.3.4.1.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9</td>\n<td id=\"S5.T1.3.4.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10</td>\n</tr>\n<tr id=\"S5.T1.3.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.3.4.1\" class=\"ltx_text ltx_font_italic\">LR</span></th>\n<td id=\"S5.T1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.5.1\" class=\"ltx_text ltx_font_italic\">IG</span>+<span id=\"S5.T1.3.3.5.2\" class=\"ltx_text ltx_font_italic\">LR</span>\n</td>\n<td id=\"S5.T1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.6.1\" class=\"ltx_text ltx_font_italic\">RF</span>+<span id=\"S5.T1.3.3.6.2\" class=\"ltx_text ltx_font_italic\">LR</span>\n</td>\n<td id=\"S5.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.1.1.1.2\" class=\"ltx_text ltx_font_italic\">SVMRFE</span>+<span id=\"S5.T1.1.1.1.1\" class=\"ltx_text ltx_font_italic\">SVM<sub id=\"S5.T1.1.1.1.1.1\" class=\"ltx_sub\">l</sub></span>\n</td>\n<td id=\"S5.T1.3.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.7.1\" class=\"ltx_text ltx_font_italic\">SVMRFE</span>+<span id=\"S5.T1.3.3.7.2\" class=\"ltx_text ltx_font_italic\">LR</span>\n</td>\n<td id=\"S5.T1.3.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.8.1\" class=\"ltx_text ltx_font_italic\">IG</span>+<span id=\"S5.T1.3.3.8.2\" class=\"ltx_text ltx_font_italic\">NBN</span>\n</td>\n<td id=\"S5.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.2.2.2.2\" class=\"ltx_text ltx_font_italic\">IG</span>+<span id=\"S5.T1.2.2.2.1\" class=\"ltx_text ltx_font_italic\">SVM<sub id=\"S5.T1.2.2.2.1.1\" class=\"ltx_sub\">l</sub></span>\n</td>\n<td id=\"S5.T1.3.3.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.9.1\" class=\"ltx_text ltx_font_italic\">CHI</span>+<span id=\"S5.T1.3.3.9.2\" class=\"ltx_text ltx_font_italic\">NBN</span>\n</td>\n<td id=\"S5.T1.3.3.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.10.1\" class=\"ltx_text ltx_font_italic\">SVMRFE</span>+<span id=\"S5.T1.3.3.10.2\" class=\"ltx_text ltx_font_italic\">NBN</span>\n</td>\n<td id=\"S5.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">\n<span id=\"S5.T1.3.3.3.2\" class=\"ltx_text ltx_font_italic\">RF</span>+<span id=\"S5.T1.3.3.3.1\" class=\"ltx_text ltx_font_italic\">SVM<sub id=\"S5.T1.3.3.3.1.1\" class=\"ltx_sub\">l</sub></span>\n</td>\n</tr>\n<tr id=\"S5.T1.3.5.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.5.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">25</th>\n<td id=\"S5.T1.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">12</td>\n<td id=\"S5.T1.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13</td>\n<td id=\"S5.T1.3.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">16</td>\n<td id=\"S5.T1.3.5.2.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13</td>\n<td id=\"S5.T1.3.5.2.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">17</td>\n<td id=\"S5.T1.3.5.2.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">14</td>\n<td id=\"S5.T1.3.5.2.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">10</td>\n<td id=\"S5.T1.3.5.2.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">13</td>\n<td id=\"S5.T1.3.5.2.10\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">8</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": [
            "In order to meta-mine we first need to perform a set of base-level experiments over which we will construct our meta-mining models. To do so we used\n65 real world cancer microarray datasets, most of them were taken from the National Center for Biotechnology Information 222http://www.ncbi.nlm.nih.gov/. \nMicroarray datasets are characterized by a high-dimensionality and a small sample size, and a relatively low number of classes, most often two.\nThese datasets have an average of 79.26 instances, 15268.57 attributes, and 2.33 classes.\n\nOn these datasets we applied a total of 35 classification data mining workflows;\n28 of them were workflows that contained one feature selection and one classification algorithm, while the seven remaining ones had only a single\nclassification algorithm. We used the four following feature selection algorithms:\nInformation Gain, IG,\nChi-square, CHI,\nReliefF [20], RF,\nand recursive feature elimination with SVM [21], SVMRFE,\nand fixed the number of selected features to ten. For classification we used the seven following algorithms:\none-nearest-neighbor, 1NN,\nthe C4.5 [23] and CART [24] decision tree algorithms,\na Naive Bayes algorithm with normal probability estimation, NBN,\na logistic regression algorithm, LR,\nand SVM [19] with the linear, SVMl\nand the rbf, SVMr,\nkernels. We used the implementations of these algorithms provided by the RapidMiner data mining suite with their default parameters.\nOverall we had a total of 65×(28+7)=227565287227565\\times(28+7)=2275 base-level DM experiments, i.e. applications of these workflows on the datasets.\nTo construct the 𝐑𝐑\\mathbf{R} preference matrix we estimated the performance of the workflows using 10-fold cross-validation and applied\nthe scoring McNemar based scoring schema described in section II.\nIn table I we give for each of the ten top workflows over the full set of 65 datasets\nthe number of times that these were ranked in the top five positions."
        ]
    },
    "S5.T2": {
        "caption": "TABLE II: Evaluation results. δ𝛿\\delta and δE​Csubscript𝛿𝐸𝐶\\delta_{EC} denote comparison results with the default (def) and the Euclidean\nbaseline strategy (EC) respectively. ρ𝜌\\rho is the Spearman’s rank correlation coefficient, in t5p we give\nthe average accuracy of the top five workflows proposed by each strategy, and mae is the mean average error. X/Y indicates the\nnumber of times X that a method was better overall the experiments Y than the default or the baseline\nstrategy.",
        "table": null,
        "footnotes": [],
        "references": [
            "We will now take a close look on the experimental results for the different meta-mining tasks\nand objective functions that we have presented to address them. The full results are given in\nTable II.",
            "Learning algorithm preferences is the most popular formulation in the traditional stream of meta-learning. There\ngiven a dataset description we seek to identify the algorithm that will most probably deliver the best results for\nthe given dataset. In that sense this meta-mining task is the most similar to the typical meta-learning task. We have presented\nthree different objective functions that can be used to address this problem. F1subscript𝐹1F_{1}, optimization problem (1),\nmakes use of only the dataset descriptors and learns a similarity measure in that space that best approximates\ntheir similarity with respect to their relative workflow preference vectors. In traditional meta-learning this similarity\nis computed directly in the dataset space, it is not learned, and most importantly it does not try to model the relative\nworkflow preference vector, [11, 25]. In our experimental setting the strategy that implements this\ntraditional meta-learning approach is the Euclidean distance-based dataset similarity, EC. In addition to the homogeneous metric learning\napproach we can also use the two heterogeneous metric learning variants to provide the workflow preferences. The simplest\none, corresponding to the optimization function F3subscript𝐹3F_{3}, optimization problem III-C, uses both dataset\nand workflow characteristics and tries to directly approximate the relative preference matrix. However this approach\nignores the fact that the learned metric should reflect two basic meta-mining requirements, that similar datasets should have\nsimilar workflow preferences, and that similar workflows should have similar dataset preferences. The optimization\nfunction F4subscript𝐹4F_{4}, optimization problem III-C, reflects exactly this bias by regularizing appropriately\nthe learned metrics in the dataset and workflow spaces so that they reflect well the similarities of the respective preference\nvectors. Before discussing the actual results, given in the left table of Table II, we give the parameter settings for\nthe different variants. F1subscript𝐹1F_{1}: μ1=0.5subscript𝜇10.5\\mu_{1}=0.5, Nxn=5subscript𝑁subscript𝑥𝑛5N_{x_{n}}=5; F3subscript𝐹3F_{3}: μ1=μ2=0.5subscript𝜇1subscript𝜇20.5\\mu_{1}=\\mu_{2}=0.5; F4subscript𝐹4F_{4}: α=1​e−10𝛼1superscript𝑒10\\alpha=1e^{-10},\nβ=1​e−3𝛽1superscript𝑒3\\beta=1e^{-3}, γ=1​e−3𝛾1superscript𝑒3\\gamma=1e^{-3}, μ1=10subscript𝜇110\\mu_{1}=10, μ2=0subscript𝜇20\\mu_{2}=0.\n\nThese parameters reflect what we think are appropriate choices based on our prior knowledge\nof the meta-mining problem. Better results would have been obtained if we had tuned, at least\nsome of them, via inner cross validation.",
            "The goal of this meta-mining task is given a new workflow and a collection of\ndatasets to provide a dataset preference vector that will reflect the order of appropriateness of the datasets for the given workflow.\nAs already mentioned the default strategy provides here a vector of equal ranks thus we cannot compute its Sperman’s rank correlation\ncoefficient.\n\nWe will compare the performance of the F2subscript𝐹2F_{2} objective function that makes use of only of the workflow descriptors when\nit tries to approximate the similarity of the dataset preference vectors, and these of F3subscript𝐹3F_{3} and F4subscript𝐹4F_{4}.\nWe used the following parameter: F2subscript𝐹2F_{2}: μ1=10subscript𝜇110\\mu_{1}=10, Nan=5subscript𝑁subscript𝑎𝑛5N_{a_{n}}=5; F3subscript𝐹3F_{3}: μ1=μ2=10subscript𝜇1subscript𝜇210\\mu_{1}=\\mu_{2}=10;\nF4subscript𝐹4F_{4}: α=1​e−10𝛼1superscript𝑒10\\alpha=1e^{-10}, β=1​e−3𝛽1superscript𝑒3\\beta=1e^{-3}, γ=1​e−3𝛾1superscript𝑒3\\gamma=1e^{-3}, μ1=0.5subscript𝜇10.5\\mu_{1}=0.5, μ2=0subscript𝜇20\\mu_{2}=0.\nLooking at the results, middle table of Table II, we see that when it comes to the mean average error,\nall methods achieve a performance that is statistically significant better than that of the default strategy, suggesting\nthat this meta-mining task is probably easier than the first one. This makes sense since it is easier to describe\na workflow similarity in terms of the concepts that these workflows use, than what it is to describe a dataset similarity\nin terms of the datasets characteristics. Neither F3subscript𝐹3F_{3} nor F4subscript𝐹4F_{4} have a mae performance that is statistically significant\nbetter than the Euclidean baseline. Nevertheless F4subscript𝐹4F_{4} is statistically significant better than the Euclidean when it comes\nto the Sperman’s rank correlation coefficient. Thus for this meta-mining task there is also evidence that we should take\na more global approach by accounting for all the different constraints on the dataset and workflow metrics as F4subscript𝐹4F_{4} does."
        ]
    }
}