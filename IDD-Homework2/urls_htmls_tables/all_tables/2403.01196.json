{
    "S3.T1.1": {
        "caption": "Datasets used in proposed approach",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.1.1\">Approach</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.2.1\">Source</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S3.T1.1.1.1.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.1.3.1\">Lines</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.2.1.1\">Covid baseline</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.2.1.2\">Baseline</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T1.1.2.1.3\">8k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.2.1\">Covid extended</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.2.2\">Baseline + Covid_DCU</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.3.2.3\">13k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.3.1\">Out-of-domain</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.3.2\">DGT</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.4.3.3\">52k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.5.4\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.5.4.1\">Fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.5.4.2\">Baseline + Covid_DCU + DGT</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.5.4.3\">65k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.6.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.6.5.1\">Mixed fine-tuned</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.6.5.2\">Baseline + Covid_DCU + DGT</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S3.T1.1.6.5.3\">65k</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.7.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S3.T1.1.7.6.1\">Combined domains</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S3.T1.1.7.6.2\">Baseline + Covid_DCU + DGT</td>\n<td class=\"ltx_td ltx_align_left ltx_border_b\" id=\"S3.T1.1.7.6.3\">65k</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "The approach adopted is illustrated in Figure 1 and the datasets used in evaluating this approach are outlined in Table 1. All models were developed using a Transformer architecture.\n"
        ]
    },
    "S3.T2.1": {
        "caption": "Hyperparameter optimization for Transformer models. Optimal parameters are highlighted in bold  .",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.1.1\">Hyperparameter</span></th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.1.1.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.1.2.1\">Values</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.2.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.2.2.1\">Learning rate</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.2.2.2\">0.1, 0.01, 0.001, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.2.2.2.1\">2</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.3.3.1\">Batch size</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.3.3.2\">1024, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.3.3.2.1\">2048</span>, 4096, 8192</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.4.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.4.4.1\">Attention heads</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.4.4.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.4.4.2.1\">2</span>, 4, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.4.4.2.2\">8</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.5.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.5.5.1\">Number of layers</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.5.5.2\">5, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.5.5.2.1\">6</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.6.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.6.6.1\">Feed-forward dimension</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.6.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.6.6.2.1\">2048</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.7.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.7.7.1\">Embedding dimension</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.7.7.2\">128, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.7.7.2.1\">256</span>, 512</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.8.8\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.8.8.1\">Label smoothing</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.8.8.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.8.8.2.1\">0.1</span>, 0.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.9.9\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.9.9.1\">Dropout</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.9.9.2\">0.1, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.9.9.2.1\">0.3</span>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.10.10\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S3.T2.1.10.10.1\">Attention dropout</th>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S3.T2.1.10.10.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.10.10.2.1\">0.1</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.11.11\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\" id=\"S3.T2.1.11.11.1\">Average Decay</th>\n<td class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\" id=\"S3.T2.1.11.11.2\">0, <span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.11.11.2.1\">0.0001</span>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [
            []
        ],
        "references": [
            "Long training times associated with NMT make it costly to tune systems using conventional Grid Search approaches. A previous study identified the hyperparameters required for optimal performance  (Lankford et al.,, 2021). Reducing the number of hidden layer neurons and increasing dropout led to significantly better performance. Furthermore, within the context of low-resource English to Irish translation, using a 16k BPE submodel resulted in the highest performing models. The Transformer hyperparameters, chosen in line with these findings, are outlined in Table 2.\n"
        ]
    },
    "S4.T3.3": {
        "caption": "Comparison of optimized Transformer performance with 2 attention heads",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T3.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.4.1\">System</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T3.3.3.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.5.1\">Heads</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.3.3.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.6.1\">Lines</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.3.3.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.7.1\">Steps</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.1.1.1.1\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.1.1.1.1.m1.1\"><semantics id=\"S4.T3.1.1.1.1.m1.1a\"><mo id=\"S4.T3.1.1.1.1.m1.1.1\" mathvariant=\"normal\" stretchy=\"false\" xref=\"S4.T3.1.1.1.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.1.1.1.1.m1.1b\"><ci id=\"S4.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T3.1.1.1.1.m1.1.1\">normal-&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.1.1.1.1.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T3.1.1.1.1.m1.1d\">&#8593;</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.2.2.2.1\">TER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.2.2.2.1.m1.1\"><semantics id=\"S4.T3.2.2.2.1.m1.1a\"><mo id=\"S4.T3.2.2.2.1.m1.1.1\" mathvariant=\"normal\" stretchy=\"false\" xref=\"S4.T3.2.2.2.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.2.2.2.1.m1.1b\"><ci id=\"S4.T3.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T3.2.2.2.1.m1.1.1\">normal-&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.2.2.2.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T3.2.2.2.1.m1.1d\">&#8595;</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T3.3.3.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T3.3.3.3.1\">ChrF3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T3.3.3.3.1.m1.1\"><semantics id=\"S4.T3.3.3.3.1.m1.1a\"><mo id=\"S4.T3.3.3.3.1.m1.1.1\" mathvariant=\"normal\" stretchy=\"false\" xref=\"S4.T3.3.3.3.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T3.3.3.3.1.m1.1b\"><ci id=\"S4.T3.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T3.3.3.3.1.m1.1.1\">normal-&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T3.3.3.3.1.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T3.3.3.3.1.m1.1d\">&#8593;</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T3.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.3.4.1.1\">Covid baseline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S4.T3.3.4.1.2\">2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.4.1.3\">8k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.4.1.4\">35k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.4.1.5\">9.0</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.4.1.6\">0.89</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T3.3.4.1.7\">0.32</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.3.5.2.1\">Covid extended</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.3.5.2.2\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.5.2.3\">13k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.5.2.4\">35k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.5.2.5\">36.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.5.2.6\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.5.2.7\">0.54</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.3.6.3.1\">Out-of-domain</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.3.6.3.2\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.6.3.3\">52k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.6.3.4\">200k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.6.3.5\">13.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.6.3.6\">0.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.6.3.7\">0.41</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.7.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.3.7.4.1\">Fine-tuned</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.3.7.4.2\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.7.4.3\">65k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.7.4.4\">35k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.7.4.5\">22.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.7.4.6\">0.64</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.7.4.7\">0.42</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.8.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T3.3.8.5.1\">Mixed fine-tuned</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T3.3.8.5.2\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.8.5.3\">65k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.8.5.4\">35k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.8.5.5\">18.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.8.5.6\">0.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T3.3.8.5.7\">0.42</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T3.3.9.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S4.T3.3.9.6.1\">Combined domains</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" id=\"S4.T3.3.9.6.2\">2</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.3.9.6.3\">65k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.3.9.6.4\">35k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.3.9.6.5\">32.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.3.9.6.6\">0.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T3.3.9.6.7\">0.55</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Experimental results achieved using a Transformer architecture, with either 2 or 8 attention heads, are summarized in Table 3 and in Table 4. Clearly in the context of our low-resource experiments, it can be seen there is little performance difference using Transformer architectures with a differing number of attention heads. The largest difference occurs when using a fine-tuned approach (2.1 BLEU points). However the difference between a 2 head and an 8 head approach is less than 1 BLEU point for all other models. The highest performing approach uses the extended Covid dataset (13k) which is a combination of the MT summit Covid baseline and a custom DCU Covid dataset. This Transformer model, with 2 heads, performs well across all key translation metrics (BLEU: 36.0, TER: 0.63 and ChrF3: 0.32).\n"
        ]
    },
    "S4.T4.3": {
        "caption": "Comparison of optimized Transformer performance with 8 attention heads",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T4.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T4.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.4.1\">System</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\" id=\"S4.T4.3.3.5\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.5.1\">Heads</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.3.6\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.6.1\">Lines</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.3.7\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.7.1\">Steps</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.1.1.1\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.1.1.1.1\">BLEU<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.1.1.1.1.m1.1\"><semantics id=\"S4.T4.1.1.1.1.m1.1a\"><mo id=\"S4.T4.1.1.1.1.m1.1.1\" mathvariant=\"normal\" stretchy=\"false\" xref=\"S4.T4.1.1.1.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.1.1.1.1.m1.1b\"><ci id=\"S4.T4.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T4.1.1.1.1.m1.1.1\">normal-&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.1.1.1.1.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.1.1.1.1.m1.1d\">&#8593;</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.2.2.2\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.2.2.2.1\">TER<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.2.2.2.1.m1.1\"><semantics id=\"S4.T4.2.2.2.1.m1.1a\"><mo id=\"S4.T4.2.2.2.1.m1.1.1\" mathvariant=\"normal\" stretchy=\"false\" xref=\"S4.T4.2.2.2.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.2.2.2.1.m1.1b\"><ci id=\"S4.T4.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T4.2.2.2.1.m1.1.1\">normal-&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.2.2.2.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.2.2.2.1.m1.1d\">&#8595;</annotation></semantics></math></span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" id=\"S4.T4.3.3.3\">\n<span class=\"ltx_text ltx_font_bold\" id=\"S4.T4.3.3.3.1\">ChrF3<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T4.3.3.3.1.m1.1\"><semantics id=\"S4.T4.3.3.3.1.m1.1a\"><mo id=\"S4.T4.3.3.3.1.m1.1.1\" mathvariant=\"normal\" stretchy=\"false\" xref=\"S4.T4.3.3.3.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T4.3.3.3.1.m1.1b\"><ci id=\"S4.T4.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T4.3.3.3.1.m1.1.1\">normal-&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T4.3.3.3.1.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T4.3.3.3.1.m1.1d\">&#8593;</annotation></semantics></math></span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T4.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.3.4.1.1\">Covid baseline</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S4.T4.3.4.1.2\">8</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.4.1.3\">8k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.4.1.4\">35k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.4.1.5\">9.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.4.1.6\">0.91</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S4.T4.3.4.1.7\">0.33</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.5.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.3.5.2.1\">Covid extended</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T4.3.5.2.2\">8</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.5.2.3\">13k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.5.2.4\">35k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.5.2.5\">35.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.5.2.6\">0.61</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.5.2.7\">0.55</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.6.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.3.6.3.1\">Out-of-domain</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T4.3.6.3.2\">8</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.6.3.3\">52k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.6.3.4\">200k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.6.3.5\">13.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.6.3.6\">0.80</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.6.3.7\">0.40</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.7.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.3.7.4.1\">Fine-tuned</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T4.3.7.4.2\">8</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.7.4.3\">65k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.7.4.4\">35k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.7.4.5\">25.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.7.4.6\">0.63</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.7.4.7\">0.43</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.8.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S4.T4.3.8.5.1\">Mixed fine-tuned</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row\" id=\"S4.T4.3.8.5.2\">8</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.8.5.3\">65k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.8.5.4\">35k</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.8.5.5\">18.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.8.5.6\">0.71</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S4.T4.3.8.5.7\">0.42</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T4.3.9.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b\" id=\"S4.T4.3.9.6.1\">Combined domains</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b\" id=\"S4.T4.3.9.6.2\">8</th>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.9.6.3\">65k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.9.6.4\">35k</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.9.6.5\">32.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.9.6.6\">0.59</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S4.T4.3.9.6.7\">0.57</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Experimental results achieved using a Transformer architecture, with either 2 or 8 attention heads, are summarized in Table 3 and in Table 4. Clearly in the context of our low-resource experiments, it can be seen there is little performance difference using Transformer architectures with a differing number of attention heads. The largest difference occurs when using a fine-tuned approach (2.1 BLEU points). However the difference between a 2 head and an 8 head approach is less than 1 BLEU point for all other models. The highest performing approach uses the extended Covid dataset (13k) which is a combination of the MT summit Covid baseline and a custom DCU Covid dataset. This Transformer model, with 2 heads, performs well across all key translation metrics (BLEU: 36.0, TER: 0.63 and ChrF3: 0.32).\n"
        ]
    }
}