{
    "PAPER'S NUMBER OF TABLES": 1,
    "S4.T1": {
        "caption": "Table 1. Statistical information of the investigated benchmarks: the number of clients K𝐾K, the total number of classes C𝐶C, the number of training samples on each client on average Ntrk¯¯subscriptsuperscript𝑁𝑘tr\\overline{{N^{k}_{\\text{tr}}}}, the number of test samples on each client on average Ntek¯¯subscriptsuperscript𝑁𝑘te\\overline{{N^{k}_{\\text{te}}}}, and the number of global test samples M𝑀M.",
        "table": "<table id=\"S4.T1.20\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.20.10\" class=\"ltx_tr\">\n<th id=\"S4.T1.20.10.11\" class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T1.11.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><math id=\"S4.T1.11.1.1.m1.1\" class=\"ltx_Math\" alttext=\"K\" display=\"inline\"><semantics id=\"S4.T1.11.1.1.m1.1a\"><mi id=\"S4.T1.11.1.1.m1.1.1\" xref=\"S4.T1.11.1.1.m1.1.1.cmml\">K</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.11.1.1.m1.1b\"><ci id=\"S4.T1.11.1.1.m1.1.1.cmml\" xref=\"S4.T1.11.1.1.m1.1.1\">𝐾</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.11.1.1.m1.1c\">K</annotation></semantics></math></th>\n<th id=\"S4.T1.12.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><math id=\"S4.T1.12.2.2.m1.1\" class=\"ltx_Math\" alttext=\"C\" display=\"inline\"><semantics id=\"S4.T1.12.2.2.m1.1a\"><mi id=\"S4.T1.12.2.2.m1.1.1\" xref=\"S4.T1.12.2.2.m1.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.12.2.2.m1.1b\"><ci id=\"S4.T1.12.2.2.m1.1.1.cmml\" xref=\"S4.T1.12.2.2.m1.1.1\">𝐶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.12.2.2.m1.1c\">C</annotation></semantics></math></th>\n<th id=\"S4.T1.13.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><math id=\"S4.T1.13.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\overline{{N^{k}_{\\text{tr}}}}\" display=\"inline\"><semantics id=\"S4.T1.13.3.3.m1.1a\"><mover accent=\"true\" id=\"S4.T1.13.3.3.m1.1.1\" xref=\"S4.T1.13.3.3.m1.1.1.cmml\"><msubsup id=\"S4.T1.13.3.3.m1.1.1.2\" xref=\"S4.T1.13.3.3.m1.1.1.2.cmml\"><mi id=\"S4.T1.13.3.3.m1.1.1.2.2.2\" xref=\"S4.T1.13.3.3.m1.1.1.2.2.2.cmml\">N</mi><mtext id=\"S4.T1.13.3.3.m1.1.1.2.3\" xref=\"S4.T1.13.3.3.m1.1.1.2.3a.cmml\">tr</mtext><mi id=\"S4.T1.13.3.3.m1.1.1.2.2.3\" xref=\"S4.T1.13.3.3.m1.1.1.2.2.3.cmml\">k</mi></msubsup><mo id=\"S4.T1.13.3.3.m1.1.1.1\" xref=\"S4.T1.13.3.3.m1.1.1.1.cmml\">¯</mo></mover><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.13.3.3.m1.1b\"><apply id=\"S4.T1.13.3.3.m1.1.1.cmml\" xref=\"S4.T1.13.3.3.m1.1.1\"><ci id=\"S4.T1.13.3.3.m1.1.1.1.cmml\" xref=\"S4.T1.13.3.3.m1.1.1.1\">¯</ci><apply id=\"S4.T1.13.3.3.m1.1.1.2.cmml\" xref=\"S4.T1.13.3.3.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T1.13.3.3.m1.1.1.2.1.cmml\" xref=\"S4.T1.13.3.3.m1.1.1.2\">subscript</csymbol><apply id=\"S4.T1.13.3.3.m1.1.1.2.2.cmml\" xref=\"S4.T1.13.3.3.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T1.13.3.3.m1.1.1.2.2.1.cmml\" xref=\"S4.T1.13.3.3.m1.1.1.2\">superscript</csymbol><ci id=\"S4.T1.13.3.3.m1.1.1.2.2.2.cmml\" xref=\"S4.T1.13.3.3.m1.1.1.2.2.2\">𝑁</ci><ci id=\"S4.T1.13.3.3.m1.1.1.2.2.3.cmml\" xref=\"S4.T1.13.3.3.m1.1.1.2.2.3\">𝑘</ci></apply><ci id=\"S4.T1.13.3.3.m1.1.1.2.3a.cmml\" xref=\"S4.T1.13.3.3.m1.1.1.2.3\"><mtext mathsize=\"70%\" id=\"S4.T1.13.3.3.m1.1.1.2.3.cmml\" xref=\"S4.T1.13.3.3.m1.1.1.2.3\">tr</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.13.3.3.m1.1c\">\\overline{{N^{k}_{\\text{tr}}}}</annotation></semantics></math></th>\n<th id=\"S4.T1.14.4.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><math id=\"S4.T1.14.4.4.m1.1\" class=\"ltx_Math\" alttext=\"\\overline{{N^{k}_{\\text{te}}}}\" display=\"inline\"><semantics id=\"S4.T1.14.4.4.m1.1a\"><mover accent=\"true\" id=\"S4.T1.14.4.4.m1.1.1\" xref=\"S4.T1.14.4.4.m1.1.1.cmml\"><msubsup id=\"S4.T1.14.4.4.m1.1.1.2\" xref=\"S4.T1.14.4.4.m1.1.1.2.cmml\"><mi id=\"S4.T1.14.4.4.m1.1.1.2.2.2\" xref=\"S4.T1.14.4.4.m1.1.1.2.2.2.cmml\">N</mi><mtext id=\"S4.T1.14.4.4.m1.1.1.2.3\" xref=\"S4.T1.14.4.4.m1.1.1.2.3a.cmml\">te</mtext><mi id=\"S4.T1.14.4.4.m1.1.1.2.2.3\" xref=\"S4.T1.14.4.4.m1.1.1.2.2.3.cmml\">k</mi></msubsup><mo id=\"S4.T1.14.4.4.m1.1.1.1\" xref=\"S4.T1.14.4.4.m1.1.1.1.cmml\">¯</mo></mover><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.14.4.4.m1.1b\"><apply id=\"S4.T1.14.4.4.m1.1.1.cmml\" xref=\"S4.T1.14.4.4.m1.1.1\"><ci id=\"S4.T1.14.4.4.m1.1.1.1.cmml\" xref=\"S4.T1.14.4.4.m1.1.1.1\">¯</ci><apply id=\"S4.T1.14.4.4.m1.1.1.2.cmml\" xref=\"S4.T1.14.4.4.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T1.14.4.4.m1.1.1.2.1.cmml\" xref=\"S4.T1.14.4.4.m1.1.1.2\">subscript</csymbol><apply id=\"S4.T1.14.4.4.m1.1.1.2.2.cmml\" xref=\"S4.T1.14.4.4.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T1.14.4.4.m1.1.1.2.2.1.cmml\" xref=\"S4.T1.14.4.4.m1.1.1.2\">superscript</csymbol><ci id=\"S4.T1.14.4.4.m1.1.1.2.2.2.cmml\" xref=\"S4.T1.14.4.4.m1.1.1.2.2.2\">𝑁</ci><ci id=\"S4.T1.14.4.4.m1.1.1.2.2.3.cmml\" xref=\"S4.T1.14.4.4.m1.1.1.2.2.3\">𝑘</ci></apply><ci id=\"S4.T1.14.4.4.m1.1.1.2.3a.cmml\" xref=\"S4.T1.14.4.4.m1.1.1.2.3\"><mtext mathsize=\"70%\" id=\"S4.T1.14.4.4.m1.1.1.2.3.cmml\" xref=\"S4.T1.14.4.4.m1.1.1.2.3\">te</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.14.4.4.m1.1c\">\\overline{{N^{k}_{\\text{te}}}}</annotation></semantics></math></th>\n<th id=\"S4.T1.15.5.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt\"><math id=\"S4.T1.15.5.5.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S4.T1.15.5.5.m1.1a\"><mi id=\"S4.T1.15.5.5.m1.1.1\" xref=\"S4.T1.15.5.5.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.15.5.5.m1.1b\"><ci id=\"S4.T1.15.5.5.m1.1.1.cmml\" xref=\"S4.T1.15.5.5.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.15.5.5.m1.1c\">M</annotation></semantics></math></th>\n<th id=\"S4.T1.20.10.12\" class=\"ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt\"></th>\n<th id=\"S4.T1.16.6.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><math id=\"S4.T1.16.6.6.m1.1\" class=\"ltx_Math\" alttext=\"K\" display=\"inline\"><semantics id=\"S4.T1.16.6.6.m1.1a\"><mi id=\"S4.T1.16.6.6.m1.1.1\" xref=\"S4.T1.16.6.6.m1.1.1.cmml\">K</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.16.6.6.m1.1b\"><ci id=\"S4.T1.16.6.6.m1.1.1.cmml\" xref=\"S4.T1.16.6.6.m1.1.1\">𝐾</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.16.6.6.m1.1c\">K</annotation></semantics></math></th>\n<th id=\"S4.T1.17.7.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><math id=\"S4.T1.17.7.7.m1.1\" class=\"ltx_Math\" alttext=\"C\" display=\"inline\"><semantics id=\"S4.T1.17.7.7.m1.1a\"><mi id=\"S4.T1.17.7.7.m1.1.1\" xref=\"S4.T1.17.7.7.m1.1.1.cmml\">C</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.17.7.7.m1.1b\"><ci id=\"S4.T1.17.7.7.m1.1.1.cmml\" xref=\"S4.T1.17.7.7.m1.1.1\">𝐶</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.17.7.7.m1.1c\">C</annotation></semantics></math></th>\n<th id=\"S4.T1.18.8.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><math id=\"S4.T1.18.8.8.m1.1\" class=\"ltx_Math\" alttext=\"\\overline{{N^{k}_{\\text{tr}}}}\" display=\"inline\"><semantics id=\"S4.T1.18.8.8.m1.1a\"><mover accent=\"true\" id=\"S4.T1.18.8.8.m1.1.1\" xref=\"S4.T1.18.8.8.m1.1.1.cmml\"><msubsup id=\"S4.T1.18.8.8.m1.1.1.2\" xref=\"S4.T1.18.8.8.m1.1.1.2.cmml\"><mi id=\"S4.T1.18.8.8.m1.1.1.2.2.2\" xref=\"S4.T1.18.8.8.m1.1.1.2.2.2.cmml\">N</mi><mtext id=\"S4.T1.18.8.8.m1.1.1.2.3\" xref=\"S4.T1.18.8.8.m1.1.1.2.3a.cmml\">tr</mtext><mi id=\"S4.T1.18.8.8.m1.1.1.2.2.3\" xref=\"S4.T1.18.8.8.m1.1.1.2.2.3.cmml\">k</mi></msubsup><mo id=\"S4.T1.18.8.8.m1.1.1.1\" xref=\"S4.T1.18.8.8.m1.1.1.1.cmml\">¯</mo></mover><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.18.8.8.m1.1b\"><apply id=\"S4.T1.18.8.8.m1.1.1.cmml\" xref=\"S4.T1.18.8.8.m1.1.1\"><ci id=\"S4.T1.18.8.8.m1.1.1.1.cmml\" xref=\"S4.T1.18.8.8.m1.1.1.1\">¯</ci><apply id=\"S4.T1.18.8.8.m1.1.1.2.cmml\" xref=\"S4.T1.18.8.8.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T1.18.8.8.m1.1.1.2.1.cmml\" xref=\"S4.T1.18.8.8.m1.1.1.2\">subscript</csymbol><apply id=\"S4.T1.18.8.8.m1.1.1.2.2.cmml\" xref=\"S4.T1.18.8.8.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T1.18.8.8.m1.1.1.2.2.1.cmml\" xref=\"S4.T1.18.8.8.m1.1.1.2\">superscript</csymbol><ci id=\"S4.T1.18.8.8.m1.1.1.2.2.2.cmml\" xref=\"S4.T1.18.8.8.m1.1.1.2.2.2\">𝑁</ci><ci id=\"S4.T1.18.8.8.m1.1.1.2.2.3.cmml\" xref=\"S4.T1.18.8.8.m1.1.1.2.2.3\">𝑘</ci></apply><ci id=\"S4.T1.18.8.8.m1.1.1.2.3a.cmml\" xref=\"S4.T1.18.8.8.m1.1.1.2.3\"><mtext mathsize=\"70%\" id=\"S4.T1.18.8.8.m1.1.1.2.3.cmml\" xref=\"S4.T1.18.8.8.m1.1.1.2.3\">tr</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.18.8.8.m1.1c\">\\overline{{N^{k}_{\\text{tr}}}}</annotation></semantics></math></th>\n<th id=\"S4.T1.19.9.9\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\"><math id=\"S4.T1.19.9.9.m1.1\" class=\"ltx_Math\" alttext=\"\\overline{{N^{k}_{\\text{te}}}}\" display=\"inline\"><semantics id=\"S4.T1.19.9.9.m1.1a\"><mover accent=\"true\" id=\"S4.T1.19.9.9.m1.1.1\" xref=\"S4.T1.19.9.9.m1.1.1.cmml\"><msubsup id=\"S4.T1.19.9.9.m1.1.1.2\" xref=\"S4.T1.19.9.9.m1.1.1.2.cmml\"><mi id=\"S4.T1.19.9.9.m1.1.1.2.2.2\" xref=\"S4.T1.19.9.9.m1.1.1.2.2.2.cmml\">N</mi><mtext id=\"S4.T1.19.9.9.m1.1.1.2.3\" xref=\"S4.T1.19.9.9.m1.1.1.2.3a.cmml\">te</mtext><mi id=\"S4.T1.19.9.9.m1.1.1.2.2.3\" xref=\"S4.T1.19.9.9.m1.1.1.2.2.3.cmml\">k</mi></msubsup><mo id=\"S4.T1.19.9.9.m1.1.1.1\" xref=\"S4.T1.19.9.9.m1.1.1.1.cmml\">¯</mo></mover><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.19.9.9.m1.1b\"><apply id=\"S4.T1.19.9.9.m1.1.1.cmml\" xref=\"S4.T1.19.9.9.m1.1.1\"><ci id=\"S4.T1.19.9.9.m1.1.1.1.cmml\" xref=\"S4.T1.19.9.9.m1.1.1.1\">¯</ci><apply id=\"S4.T1.19.9.9.m1.1.1.2.cmml\" xref=\"S4.T1.19.9.9.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T1.19.9.9.m1.1.1.2.1.cmml\" xref=\"S4.T1.19.9.9.m1.1.1.2\">subscript</csymbol><apply id=\"S4.T1.19.9.9.m1.1.1.2.2.cmml\" xref=\"S4.T1.19.9.9.m1.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S4.T1.19.9.9.m1.1.1.2.2.1.cmml\" xref=\"S4.T1.19.9.9.m1.1.1.2\">superscript</csymbol><ci id=\"S4.T1.19.9.9.m1.1.1.2.2.2.cmml\" xref=\"S4.T1.19.9.9.m1.1.1.2.2.2\">𝑁</ci><ci id=\"S4.T1.19.9.9.m1.1.1.2.2.3.cmml\" xref=\"S4.T1.19.9.9.m1.1.1.2.2.3\">𝑘</ci></apply><ci id=\"S4.T1.19.9.9.m1.1.1.2.3a.cmml\" xref=\"S4.T1.19.9.9.m1.1.1.2.3\"><mtext mathsize=\"70%\" id=\"S4.T1.19.9.9.m1.1.1.2.3.cmml\" xref=\"S4.T1.19.9.9.m1.1.1.2.3\">te</mtext></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.19.9.9.m1.1c\">\\overline{{N^{k}_{\\text{te}}}}</annotation></semantics></math></th>\n<th id=\"S4.T1.20.10.10\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\"><math id=\"S4.T1.20.10.10.m1.1\" class=\"ltx_Math\" alttext=\"M\" display=\"inline\"><semantics id=\"S4.T1.20.10.10.m1.1a\"><mi id=\"S4.T1.20.10.10.m1.1.1\" xref=\"S4.T1.20.10.10.m1.1.1.cmml\">M</mi><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.20.10.10.m1.1b\"><ci id=\"S4.T1.20.10.10.m1.1.1.cmml\" xref=\"S4.T1.20.10.10.m1.1.1\">𝑀</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.20.10.10.m1.1c\">M</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.20.11.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.20.11.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">FeCifar10</td>\n<td id=\"S4.T1.20.11.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">100</td>\n<td id=\"S4.T1.20.11.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">10</td>\n<td id=\"S4.T1.20.11.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">400</td>\n<td id=\"S4.T1.20.11.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">100</td>\n<td id=\"S4.T1.20.11.1.6\" class=\"ltx_td ltx_align_center ltx_border_rr ltx_border_tt\">10k</td>\n<td id=\"S4.T1.20.11.1.7\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_tt\">FeCifar100</td>\n<td id=\"S4.T1.20.11.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">100</td>\n<td id=\"S4.T1.20.11.1.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">100</td>\n<td id=\"S4.T1.20.11.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">400</td>\n<td id=\"S4.T1.20.11.1.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">100</td>\n<td id=\"S4.T1.20.11.1.12\" class=\"ltx_td ltx_align_center ltx_border_tt\">10k</td>\n</tr>\n<tr id=\"S4.T1.20.12.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.20.12.2.1\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">Shakespeare</td>\n<td id=\"S4.T1.20.12.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">1129</td>\n<td id=\"S4.T1.20.12.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">81</td>\n<td id=\"S4.T1.20.12.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">2994</td>\n<td id=\"S4.T1.20.12.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">749</td>\n<td id=\"S4.T1.20.12.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_rr ltx_border_t\">845k</td>\n<td id=\"S4.T1.20.12.2.7\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t\">FeMnist</td>\n<td id=\"S4.T1.20.12.2.8\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">3550</td>\n<td id=\"S4.T1.20.12.2.9\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">62</td>\n<td id=\"S4.T1.20.12.2.10\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">181</td>\n<td id=\"S4.T1.20.12.2.11\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\">45</td>\n<td id=\"S4.T1.20.12.2.12\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">161k</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Similar to where to transfer/share in TL/MTL, where to privatize in FL is also a key problem to be solved. In this paper, we propose several automatical algorithms to learn to aggregate in FL. Specifically, we first take a basic architecture as the “AaBb”, where the global server keeps a global encoder ",
                "E",
                "s",
                "subscript",
                "𝐸",
                "s",
                "E_{{\\text{s}}}",
                " and a global classifier ",
                "C",
                "s",
                "subscript",
                "𝐶",
                "s",
                "C_{{\\text{s}}}",
                ", and for each ",
                "k",
                "𝑘",
                "k",
                ", the ",
                "k",
                "𝑘",
                "k",
                "th client keeps a local encoder ",
                "E",
                "p",
                "k",
                "superscript",
                "subscript",
                "𝐸",
                "p",
                "𝑘",
                "E_{{\\text{p}}}^{k}",
                " and a local classifier ",
                "C",
                "p",
                "k",
                "superscript",
                "subscript",
                "𝐶",
                "p",
                "𝑘",
                "C_{{\\text{p}}}^{k}",
                ". The parameters of them are ",
                "θ",
                "E",
                "s",
                "subscript",
                "𝜃",
                "subscript",
                "𝐸",
                "s",
                "{\\theta}_{E_{{\\text{s}}}}",
                ", ",
                "θ",
                "C",
                "s",
                "subscript",
                "𝜃",
                "subscript",
                "𝐶",
                "s",
                "{\\theta}_{C_{{\\text{s}}}}",
                ", ",
                "θ",
                "E",
                "p",
                "k",
                "superscript",
                "subscript",
                "𝜃",
                "subscript",
                "𝐸",
                "p",
                "𝑘",
                "{\\theta}_{E_{{\\text{p}}}}^{k}",
                ", and ",
                "θ",
                "C",
                "p",
                "k",
                "superscript",
                "subscript",
                "𝜃",
                "subscript",
                "𝐶",
                "p",
                "𝑘",
                "{\\theta}_{C_{{\\text{p}}}}^{k}",
                " correspondingly. To automatically learn a privatization way, some additional parameters should also be globally learned, and we denote them as ",
                "ψ",
                "𝜓",
                "\\psi",
                ".",
                "In the ",
                "t",
                "𝑡",
                "t",
                "th round, the server sends the ",
                "θ",
                "E",
                "s",
                ",",
                "t",
                "subscript",
                "𝜃",
                "subscript",
                "𝐸",
                "s",
                "𝑡",
                "{\\theta}_{E_{{\\text{s}}},t}",
                ", ",
                "θ",
                "C",
                "s",
                ",",
                "t",
                "subscript",
                "𝜃",
                "subscript",
                "𝐶",
                "s",
                "𝑡",
                "{\\theta}_{C_{{\\text{s}}},t}",
                ", and ",
                "ψ",
                "t",
                "subscript",
                "𝜓",
                "𝑡",
                "\\psi_{t}",
                " to the selected clients. Take the ",
                "k",
                "𝑘",
                "k",
                "th client as an example, it receives these parameters as: ",
                "θ",
                "E",
                "s",
                ",",
                "t",
                "k",
                "←",
                "θ",
                "E",
                "s",
                ",",
                "t",
                "←",
                "superscript",
                "subscript",
                "𝜃",
                "subscript",
                "𝐸",
                "s",
                "𝑡",
                "𝑘",
                "subscript",
                "𝜃",
                "subscript",
                "𝐸",
                "s",
                "𝑡",
                "{\\theta}_{E_{{\\text{s}}},t}^{k}\\leftarrow{\\theta}_{E_{{\\text{s}}},t}",
                ", ",
                "θ",
                "C",
                "s",
                ",",
                "t",
                "k",
                "←",
                "θ",
                "C",
                "s",
                ",",
                "t",
                "←",
                "superscript",
                "subscript",
                "𝜃",
                "subscript",
                "𝐶",
                "s",
                "𝑡",
                "𝑘",
                "subscript",
                "𝜃",
                "subscript",
                "𝐶",
                "s",
                "𝑡",
                "{\\theta}_{C_{{\\text{s}}},t}^{k}\\leftarrow{\\theta}_{C_{{\\text{s}}},t}",
                ", and ",
                "ψ",
                "t",
                "k",
                "←",
                "ψ",
                "t",
                "←",
                "superscript",
                "subscript",
                "𝜓",
                "𝑡",
                "𝑘",
                "subscript",
                "𝜓",
                "𝑡",
                "\\psi_{t}^{k}\\leftarrow\\psi_{t}",
                ". We take the training sample pair ",
                "(",
                "𝐱",
                "i",
                "k",
                ",",
                "y",
                "i",
                "k",
                ")",
                "superscript",
                "subscript",
                "𝐱",
                "𝑖",
                "𝑘",
                "superscript",
                "subscript",
                "𝑦",
                "𝑖",
                "𝑘",
                "({\\mathbf{x}}_{i}^{k},y_{i}^{k})",
                " as an example to show the local learning process. The ",
                "𝐱",
                "i",
                "k",
                "superscript",
                "subscript",
                "𝐱",
                "𝑖",
                "𝑘",
                "{\\mathbf{x}}_{i}^{k}",
                " will be first processed via the shared and private encoders respectively, i.e.,",
                "where ",
                "𝐡",
                "s",
                ",",
                "i",
                "k",
                "superscript",
                "subscript",
                "𝐡",
                "s",
                "𝑖",
                "𝑘",
                "{\\mathbf{h}}_{{\\text{s}},i}^{k}",
                " and ",
                "𝐡",
                "p",
                ",",
                "i",
                "k",
                "superscript",
                "subscript",
                "𝐡",
                "p",
                "𝑖",
                "𝑘",
                "{\\mathbf{h}}_{{\\text{p}},i}^{k}",
                " denote the shared and private features respectively, and we omit the index of ",
                "t",
                "𝑡",
                "t",
                " for simplification. Then we propose three methods to fuse these features, i.e., cross-stitch, soft attention, and hard selection. For cross-stitch, we imitate the learning process in MTL with cross-stitch ",
                "(Misra\net al",
                ".",
                ", ",
                "2016",
                ")",
                ", i.e.,",
                "where ",
                "α",
                "^",
                "i",
                ",",
                "j",
                "k",
                "∈",
                "[",
                "0",
                ",",
                "1",
                "]",
                ",",
                "i",
                "∈",
                "{",
                "0",
                ",",
                "1",
                "}",
                ",",
                "j",
                "∈",
                "{",
                "0",
                ",",
                "1",
                "}",
                "formulae-sequence",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "𝑖",
                "𝑗",
                "𝑘",
                "0",
                "1",
                "formulae-sequence",
                "𝑖",
                "0",
                "1",
                "𝑗",
                "0",
                "1",
                "\\hat{\\alpha}_{i,j}^{k}\\in[0,1],i\\in\\{0,1\\},j\\in\\{0,1\\}",
                " are coefficients to be learned. In order to obtain a meaningful ",
                "α",
                "^",
                "i",
                ",",
                "j",
                "k",
                "∈",
                "[",
                "0",
                ",",
                "1",
                "]",
                "subscript",
                "superscript",
                "^",
                "𝛼",
                "𝑘",
                "𝑖",
                "𝑗",
                "0",
                "1",
                "\\hat{\\alpha}^{k}_{i,j}\\in[0,1]",
                ", we take the softmax to normalize ",
                "α",
                "i",
                ",",
                "j",
                "k",
                "∈",
                "ℛ",
                "superscript",
                "subscript",
                "𝛼",
                "𝑖",
                "𝑗",
                "𝑘",
                "ℛ",
                "\\alpha_{i,j}^{k}\\in\\mathcal{R}",
                ". Specifically, we define ",
                "SoftMax",
                "​",
                "(",
                "⋅",
                ",",
                "⋅",
                ")",
                "SoftMax",
                "⋅",
                "⋅",
                "\\text{SoftMax}(\\cdot,\\cdot)",
                " as a function that takes ",
                "α",
                "0",
                "subscript",
                "𝛼",
                "0",
                "\\alpha_{0}",
                " and ",
                "α",
                "1",
                "subscript",
                "𝛼",
                "1",
                "\\alpha_{1}",
                " as inputs and returns ",
                "e",
                "α",
                "0",
                "/",
                "(",
                "e",
                "α",
                "0",
                "+",
                "e",
                "α",
                "1",
                ")",
                "superscript",
                "𝑒",
                "subscript",
                "𝛼",
                "0",
                "superscript",
                "𝑒",
                "subscript",
                "𝛼",
                "0",
                "superscript",
                "𝑒",
                "subscript",
                "𝛼",
                "1",
                "e^{\\alpha_{0}}/(e^{\\alpha_{0}}+e^{\\alpha_{1}})",
                " and ",
                "e",
                "α",
                "1",
                "/",
                "(",
                "e",
                "α",
                "0",
                "+",
                "e",
                "α",
                "1",
                ")",
                "superscript",
                "𝑒",
                "subscript",
                "𝛼",
                "1",
                "superscript",
                "𝑒",
                "subscript",
                "𝛼",
                "0",
                "superscript",
                "𝑒",
                "subscript",
                "𝛼",
                "1",
                "e^{\\alpha_{1}}/(e^{\\alpha_{0}}+e^{\\alpha_{1}})",
                " respectively. Then, we can let ",
                "α",
                "^",
                "0",
                ",",
                "0",
                "k",
                ",",
                "α",
                "^",
                "0",
                ",",
                "1",
                "k",
                "=",
                "SoftMax",
                "​",
                "(",
                "α",
                "0",
                ",",
                "0",
                "k",
                "/",
                "λ",
                ",",
                "α",
                "0",
                ",",
                "1",
                "k",
                "/",
                "λ",
                ")",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "0",
                "0",
                "𝑘",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "0",
                "1",
                "𝑘",
                "SoftMax",
                "superscript",
                "subscript",
                "𝛼",
                "0",
                "0",
                "𝑘",
                "𝜆",
                "superscript",
                "subscript",
                "𝛼",
                "0",
                "1",
                "𝑘",
                "𝜆",
                "\\hat{\\alpha}_{0,0}^{k},\\hat{\\alpha}_{0,1}^{k}=\\text{SoftMax}(\\alpha_{0,0}^{k}/\\lambda,\\alpha_{0,1}^{k}/\\lambda)",
                ", and ",
                "α",
                "^",
                "1",
                ",",
                "0",
                "k",
                ",",
                "α",
                "^",
                "1",
                ",",
                "1",
                "k",
                "=",
                "SoftMax",
                "​",
                "(",
                "α",
                "1",
                ",",
                "0",
                "k",
                "/",
                "λ",
                ",",
                "α",
                "1",
                ",",
                "1",
                "k",
                "/",
                "λ",
                ")",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "1",
                "0",
                "𝑘",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "1",
                "1",
                "𝑘",
                "SoftMax",
                "superscript",
                "subscript",
                "𝛼",
                "1",
                "0",
                "𝑘",
                "𝜆",
                "superscript",
                "subscript",
                "𝛼",
                "1",
                "1",
                "𝑘",
                "𝜆",
                "\\hat{\\alpha}_{1,0}^{k},\\hat{\\alpha}_{1,1}^{k}=\\text{SoftMax}(\\alpha_{1,0}^{k}/\\lambda,\\alpha_{1,1}^{k}/\\lambda)",
                ". ",
                "λ",
                "𝜆",
                "\\lambda",
                " is the temperature. In our experiments, taking it as ",
                "2.0",
                "2.0",
                "2.0",
                " could lead to slightly better results.",
                "For soft attention, we take a simple weighted average of the features as follows:",
                "where we can find that this differs from cross-stitch only in that it takes the same weights to generate ",
                "𝐡",
                "^",
                "s",
                ",",
                "i",
                "k",
                "superscript",
                "subscript",
                "^",
                "𝐡",
                "s",
                "𝑖",
                "𝑘",
                "\\hat{{\\mathbf{h}}}_{{\\text{s}},i}^{k}",
                " and ",
                "𝐡",
                "^",
                "p",
                ",",
                "i",
                "k",
                "superscript",
                "subscript",
                "^",
                "𝐡",
                "p",
                "𝑖",
                "𝑘",
                "\\hat{{\\mathbf{h}}}_{{\\text{p}},i}^{k}",
                ". Similarly, we take ",
                "α",
                "^",
                "0",
                "k",
                ",",
                "α",
                "^",
                "1",
                "k",
                "=",
                "SoftMax",
                "​",
                "(",
                "α",
                "0",
                "k",
                "/",
                "λ",
                ",",
                "α",
                "1",
                "k",
                "/",
                "λ",
                ")",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "0",
                "𝑘",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "1",
                "𝑘",
                "SoftMax",
                "superscript",
                "subscript",
                "𝛼",
                "0",
                "𝑘",
                "𝜆",
                "superscript",
                "subscript",
                "𝛼",
                "1",
                "𝑘",
                "𝜆",
                "\\hat{\\alpha}_{0}^{k},\\hat{\\alpha}_{1}^{k}=\\text{SoftMax}(\\alpha_{0}^{k}/\\lambda,\\alpha_{1}^{k}/\\lambda)",
                ". We propose this approach to investigate whether the symmetric property of the combination weights matters.",
                "For hard selection, we directly select one group of features via the Gumbel-softmax ",
                "(Jang\net al",
                ".",
                ", ",
                "2017",
                "; Li\net al",
                ".",
                ", ",
                "2021",
                ")",
                ", i.e.,",
                "where ",
                "α",
                "^",
                "0",
                "k",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "0",
                "𝑘",
                "\\hat{\\alpha}_{0}^{k}",
                " and ",
                "α",
                "^",
                "1",
                "k",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "1",
                "𝑘",
                "\\hat{\\alpha}_{1}^{k}",
                " can be generated via the following steps. First, sample values from the Gumbel distribution, i.e., ",
                "g",
                "0",
                ",",
                "g",
                "1",
                "∼",
                "Gumbel",
                "​",
                "(",
                "0",
                ",",
                "1",
                ")",
                "similar-to",
                "subscript",
                "𝑔",
                "0",
                "subscript",
                "𝑔",
                "1",
                "Gumbel",
                "0",
                "1",
                "g_{0},g_{1}\\sim\\text{Gumbel}(0,1)",
                ", where ",
                "Gumbel",
                "​",
                "(",
                "0",
                ",",
                "1",
                ")",
                "Gumbel",
                "0",
                "1",
                "\\text{Gumbel}(0,1)",
                " can be seen as first sampling ",
                "u",
                "0",
                ",",
                "u",
                "1",
                "∼",
                "Uniform",
                "​",
                "(",
                "0",
                ",",
                "1",
                ")",
                "similar-to",
                "subscript",
                "𝑢",
                "0",
                "subscript",
                "𝑢",
                "1",
                "Uniform",
                "0",
                "1",
                "u_{0},u_{1}\\sim\\text{Uniform}(0,1)",
                " and then taking the transformation ",
                "g",
                "=",
                "−",
                "log",
                "⁡",
                "(",
                "−",
                "log",
                "⁡",
                "(",
                "u",
                ")",
                ")",
                "𝑔",
                "𝑢",
                "g=-\\log(-\\log(u))",
                ". Second, take the reparametrized softmax as ",
                "α",
                "^",
                "0",
                "k",
                ",",
                "α",
                "^",
                "1",
                "k",
                "=",
                "SoftMax",
                "​",
                "(",
                "(",
                "α",
                "0",
                "k",
                "+",
                "g",
                "0",
                ")",
                "/",
                "λ",
                ",",
                "(",
                "α",
                "1",
                "k",
                "+",
                "g",
                "1",
                ")",
                "/",
                "λ",
                ")",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "0",
                "𝑘",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "1",
                "𝑘",
                "SoftMax",
                "superscript",
                "subscript",
                "𝛼",
                "0",
                "𝑘",
                "subscript",
                "𝑔",
                "0",
                "𝜆",
                "superscript",
                "subscript",
                "𝛼",
                "1",
                "𝑘",
                "subscript",
                "𝑔",
                "1",
                "𝜆",
                "\\hat{\\alpha}_{0}^{k},\\hat{\\alpha}_{1}^{k}=\\text{SoftMax}((\\alpha_{0}^{k}+g_{0})/\\lambda,(\\alpha_{1}^{k}+g_{1})/\\lambda)",
                ". We denote the process as ",
                "α",
                "^",
                "0",
                "k",
                ",",
                "α",
                "^",
                "1",
                "k",
                "=",
                "GumbelSoftMax",
                "​",
                "(",
                "α",
                "0",
                "k",
                ",",
                "α",
                "1",
                "k",
                ")",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "0",
                "𝑘",
                "superscript",
                "subscript",
                "^",
                "𝛼",
                "1",
                "𝑘",
                "GumbelSoftMax",
                "superscript",
                "subscript",
                "𝛼",
                "0",
                "𝑘",
                "superscript",
                "subscript",
                "𝛼",
                "1",
                "𝑘",
                "\\hat{\\alpha}_{0}^{k},\\hat{\\alpha}_{1}^{k}=\\text{GumbelSoftMax}(\\alpha_{0}^{k},\\alpha_{1}^{k})",
                ". Using Gumbel softmax can lead to more spiked weights or even discrete selection codes ",
                "(Jang\net al",
                ".",
                ", ",
                "2017",
                ")",
                ", and hence we expect it could lead to purer aggregation and more understandable results.",
                "With either of the three ways, we can obtain ",
                "𝐡",
                "^",
                "s",
                ",",
                "i",
                "k",
                "superscript",
                "subscript",
                "^",
                "𝐡",
                "s",
                "𝑖",
                "𝑘",
                "\\hat{{\\mathbf{h}}}_{{\\text{s}},i}^{k}",
                " and ",
                "𝐡",
                "^",
                "p",
                ",",
                "i",
                "k",
                "superscript",
                "subscript",
                "^",
                "𝐡",
                "p",
                "𝑖",
                "𝑘",
                "\\hat{{\\mathbf{h}}}_{{\\text{p}},i}^{k}",
                ". Then we make predictions via:",
                "where ",
                "𝐨",
                "s",
                ",",
                "i",
                "k",
                "superscript",
                "subscript",
                "𝐨",
                "s",
                "𝑖",
                "𝑘",
                "{\\mathbf{o}}_{{\\text{s}},i}^{k}",
                " and ",
                "𝐨",
                "p",
                ",",
                "i",
                "k",
                "superscript",
                "subscript",
                "𝐨",
                "p",
                "𝑖",
                "𝑘",
                "{\\mathbf{o}}_{{\\text{p}},i}^{k}",
                " are the unnormalized outputs of shared classifier and private classifier correspondingly. Before using softmax and calculating cross-entropy loss, we similarly take a symmetric fusion as follows:",
                "where ",
                "𝐨",
                "^",
                "i",
                "k",
                "superscript",
                "subscript",
                "^",
                "𝐨",
                "𝑖",
                "𝑘",
                "\\hat{{\\mathbf{o}}}_{i}^{k}",
                " is the final weighted output, and it will be applied with softmax to calculate the cross-entopy loss. For cross-stitch and soft attention, ",
                "β",
                "^",
                "0",
                "k",
                ",",
                "β",
                "^",
                "1",
                "k",
                "=",
                "SoftMax",
                "​",
                "(",
                "β",
                "0",
                "k",
                "/",
                "λ",
                ",",
                "β",
                "1",
                "k",
                "/",
                "λ",
                ")",
                "superscript",
                "subscript",
                "^",
                "𝛽",
                "0",
                "𝑘",
                "superscript",
                "subscript",
                "^",
                "𝛽",
                "1",
                "𝑘",
                "SoftMax",
                "superscript",
                "subscript",
                "𝛽",
                "0",
                "𝑘",
                "𝜆",
                "superscript",
                "subscript",
                "𝛽",
                "1",
                "𝑘",
                "𝜆",
                "\\hat{\\beta}_{0}^{k},\\hat{\\beta}_{1}^{k}=\\text{SoftMax}(\\beta_{0}^{k}/\\lambda,\\beta_{1}^{k}/\\lambda)",
                ", while for hard selection, ",
                "β",
                "^",
                "0",
                "k",
                ",",
                "β",
                "^",
                "1",
                "k",
                "=",
                "GumbelSoftMax",
                "​",
                "(",
                "β",
                "0",
                "k",
                ",",
                "β",
                "1",
                "k",
                ")",
                "superscript",
                "subscript",
                "^",
                "𝛽",
                "0",
                "𝑘",
                "superscript",
                "subscript",
                "^",
                "𝛽",
                "1",
                "𝑘",
                "GumbelSoftMax",
                "superscript",
                "subscript",
                "𝛽",
                "0",
                "𝑘",
                "superscript",
                "subscript",
                "𝛽",
                "1",
                "𝑘",
                "\\hat{\\beta}_{0}^{k},\\hat{\\beta}_{1}^{k}=\\text{GumbelSoftMax}(\\beta_{0}^{k},\\beta_{1}^{k})",
                ".",
                "In total, for cross-stitch, we have ",
                "ψ",
                "=",
                "{",
                "α",
                "0",
                ",",
                "0",
                ",",
                "α",
                "0",
                ",",
                "1",
                ",",
                "α",
                "1",
                ",",
                "0",
                ",",
                "α",
                "1",
                ",",
                "1",
                ",",
                "β",
                "0",
                ",",
                "β",
                "1",
                "}",
                "𝜓",
                "subscript",
                "𝛼",
                "0",
                "0",
                "subscript",
                "𝛼",
                "0",
                "1",
                "subscript",
                "𝛼",
                "1",
                "0",
                "subscript",
                "𝛼",
                "1",
                "1",
                "subscript",
                "𝛽",
                "0",
                "subscript",
                "𝛽",
                "1",
                "\\psi=\\{\\alpha_{0,0},\\alpha_{0,1},\\alpha_{1,0},\\alpha_{1,1},\\beta_{0},\\beta_{1}\\}",
                "; for soft attention or hard selection, we have ",
                "ψ",
                "=",
                "{",
                "α",
                "0",
                ",",
                "α",
                "1",
                ",",
                "β",
                "0",
                ",",
                "β",
                "1",
                "}",
                "𝜓",
                "subscript",
                "𝛼",
                "0",
                "subscript",
                "𝛼",
                "1",
                "subscript",
                "𝛽",
                "0",
                "subscript",
                "𝛽",
                "1",
                "\\psi=\\{\\alpha_{0},\\alpha_{1},\\beta_{0},\\beta_{1}\\}",
                ". These parameters will be end-to-end updated in local training and will also be aggregated during the global procedure. The three algorithms are named as AutoCS, AutoSA, and AutoHS respectively, whose pseudo codes can be found in Algo. ",
                "1",
                ". The ",
                "ψ",
                "𝜓",
                "\\psi",
                " to be learned and the architectures are shown in Fig. ",
                "2",
                "."
            ]
        ]
    }
}