{
    "id_table_1": {
        "caption": "Table 1:  Split of contexts and questions from SQuAD. The  comp.  splits are derived from the dev split, used to evaluate the performance of the reward model during training.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "To control difficulty, we adapt the Reinforcement Learning from Human Feedback protocol used in AI assistant steering  (Ouyang et al.,  2022 ; Bai et al.,  2022 ) . In this regime, samples are ranked based on specific criteria and paired into  chosen  and  rejected  samples for training a reward model. This reward model learns to distinguish good samples from bad and outputs a signal which steers a policy model. Rather than relying on costly human annotations, we generate synthetic preference data by evaluating question-answering model performance on a subset of SQuAD, assuming that questions answered correctly less frequently are inherently more difficult. This approach leverages the language models innate feature extraction capabilities, eliminating the need to explicitly define difficulty components. Figure  1  demonstrates this feature extraction ability by comparing questions generated with and without reinforcement fine-tuning.",
            "In our training process for question generation and response formatting, we begin by employing a reformatted version of the SQuAD v1 training split (see Table  1 ). The reformatting converts SQuAD to the task of question-answer pair generation, as shown in Figure  3 . We select the \"correct\" answer as the one that appears most frequently in the list of answers for each question in the dataset, selecting randomly among the most common if there is no victor. To ensure model robustness without overfitting, the model undergoes a single epoch of training, enabling it to effectively capture the nuances of the task.",
            "Only the dev set of our SQuAD dataset was used to derive difficulty comparison data, to ensure the reward model never sees the samples used for evaluation. To evaluate the reward model, we extract 10% of the comparison contexts. Full dataset statistics can be found in Table  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Question-Answering model performance on each set of samples. Models were only supplied samples which passed the format critics and were unanimously deemed answerable by GPT-4o and Gemini-1.5-pro. The  Total Valid  column indicates this number of valid samples used during question answering. Accuracy is based on exact match and results are mean and standard deviation across three sets of generated samples. Lower accuracy indicates harder questions.",
        "table": "S4.T2.20",
        "footnotes": [],
        "references": [
            "We embed this synthetic RLHF process into a greater pipeline for generating samples, shown in Figure  2 . This ensures the quality of the final dataset. The pipeline also contains a set of rule-based critics which are used to exclude samples that are malformed and those with non-unique answers in the source text. Samples are then deduplicated using exact string matching.",
            "Model Accuracy.   To measure performance, we observe the difference in prediction accuracy for QA models on each dataset. Table  2  shows that in all cases of PPO training, we observe a decrease in average model prediction accuracy and an increase in the total number of valid generations. The consistent decrease in absolute prediction accuracy for all models when using the PPO trained models over both zero-shot and SFT signifies an increase in average question difficulty. The SFT process vastly improves the models ability to generate valid questions. The PPO process further bolsters this capability which illustrates that the model is learning the intrinsic properties of high-quality questions. The performance of the reward models, shown in Appendix  A , is reflected here, showing lesser degrees of improvement for those models fine-tuned without access to the input passage.",
            "Human Evaluation.   To evaluate question quality, we conduct a human evaluation on a subset of 50 passages from the test split. Each input passage and question is filtered through the format critic then provided to two annotators who select either the correct answer span or indicate that the question cannot be answered. In the case of annotator disagreement or the annotated answers differing from the model generated answer, the annotator responses and the model answer are provided to two new annotators who both select which responses are appropriate. We allow annotators to select multiple responses as correct but only include those that were selected unanimously by both annotators as valid. We observe an agreement of   = 0.7975  0.7975 \\kappa=0.7975 italic_ = 0.7975  between annotators. The results of this evaluation, shown in Table  3 , displays an equivalent or improved rate of answerability when fine-tuning with PPO; the answerability proportions for each dataset are roughly equivalent to those presented in Table  2 . This further corroborates the efficacy of our approach."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Results of human evaluation for question quality.  Full  indicates that the model generated answer was a valid answer according to the format critics and identified by human annotators and  Partial  indicates that the sample passed format critics and a valid answer was identified for the question but the model generated answer did not match.",
        "table": "S5.T3.1",
        "footnotes": [],
        "references": [
            "In our training process for question generation and response formatting, we begin by employing a reformatted version of the SQuAD v1 training split (see Table  1 ). The reformatting converts SQuAD to the task of question-answer pair generation, as shown in Figure  3 . We select the \"correct\" answer as the one that appears most frequently in the list of answers for each question in the dataset, selecting randomly among the most common if there is no victor. To ensure model robustness without overfitting, the model undergoes a single epoch of training, enabling it to effectively capture the nuances of the task.",
            "We base our splits off the original SQuAD to minimise the risk of data leakage. We maintain the full train split unchanged as any model previously trained on SQuAD will have seen the full train split. We extract a test split of 500 contexts from the dev split, ensuring no contexts appear in both the dev and test splits. We extract 50 unique contexts from the test split for a human evaluation of question quality and answerability. In all cases, context-question pairs were only kept if they fit into the context length of LLaMa-2 when formatted in the correct prompt format. All samples were formatted into the three instruction components:  instruction ,  input ,  response  as shown in Figure  3 .",
            "Human Evaluation.   To evaluate question quality, we conduct a human evaluation on a subset of 50 passages from the test split. Each input passage and question is filtered through the format critic then provided to two annotators who select either the correct answer span or indicate that the question cannot be answered. In the case of annotator disagreement or the annotated answers differing from the model generated answer, the annotator responses and the model answer are provided to two new annotators who both select which responses are appropriate. We allow annotators to select multiple responses as correct but only include those that were selected unanimously by both annotators as valid. We observe an agreement of   = 0.7975  0.7975 \\kappa=0.7975 italic_ = 0.7975  between annotators. The results of this evaluation, shown in Table  3 , displays an equivalent or improved rate of answerability when fine-tuning with PPO; the answerability proportions for each dataset are roughly equivalent to those presented in Table  2 . This further corroborates the efficacy of our approach."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Accuracy of reward model variants based on the test split of the comparisons dataset.  input  indicates that the model was trained with the question and associated text passage as input and  margin  indicates that marginal ranking loss was used.",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": [
            "External Metrics.    Figure  4  shows results for the reference-free metrics. RQUGE is clearly effective at discriminating between human-written SQuAD samples, those generated by the fine-tuned models and the zero-shot examples, but it is unable to separate out the SFT and PPO results. The particularly high score for SQuAD could in part be due to data leakage as the answer generation model for the metric was trained on SQuAD  (Khashabi et al.,  2022 ) . This would indicate why our newly generated questions might score lower as it cannot have memorised the answer. Syntactic divergence results for the SQuAD test split and all trained model generations follow a consistent distribution but the zero-shot results appear much better, despite having a higher average prediction accuracy than the SFT and PPO models. Zero-shot obtaining higher syntactic divergence could stem from the general purpose language generation objective of LLaMa-2-chat. This could cause the model to generate boilerplate text which distances the structure of the question from that of the answer sentence but doesnt necessarily result in a more difficult question. QAScore proves uninformative, only being able to subtly identify SQuAD samples from model generated samples. Self-BLEU indicates that SQuAD samples are the most diverse, which is to be expected, but that zero-shot samples exhibit a distinct lack of diversity when compared with fine-tuned models. This result is, in part, misleading as Self-BLEU was only calculable for input passages with at least two valid questions. As the number of valid generations was so low for the zero-shot model, the cases where multiple valid questions were generated for a context was disproportionately in favour of identical generations.",
            "To understand the relative contributions of marginal ranking loss and the use of the input when training reward models to discriminate based on difficulty, we trained all four permutations of settings on the whole training split of the comparisons dataset and evaluated on the test split. As shown in Table  4 , the inclusion of the input text had a very significant impact on performance. This was expected as the difficulty of a question is not independent of the related passage. Surprisingly, marginal ranking loss had a very slight negative impact on reward model performance. We believe this could be due to the fact that features of difficulty are very subtle and the marginal component may have caused too significant adjustments due to higher loss values."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Results of answerability task posed to GPT-4o and Gemini-1.5-pro. Results represent the proportion of samples that are answerable, unanswerable and undecided, taken from those samples which passed the format critic.",
        "table": "A1.T5.5",
        "footnotes": [],
        "references": [
            "Failure Modes.   At a high level, we can observe the reasons for sample rejection for each model. As shown in Figure  5 , the zero-shot model is generally unable to generate samples that have a single answer span in the text, despite exactly specifying this in the prompt. The high number of incorrectly formatted samples was a result of only a question being generated or neither a question nor answer being generated. For all the trained model variants, the dominant failure mode was unanswerable questions. As shown in Appendix  C , each of the fine-tuned models show a similar proportion of otherwise valid samples being unanswerable. The answerability rate could potentially be improved by generating candidate answers, as in  (Zhang et al.,  2022 ) , and passing an input passage and answer to the question generation model.",
            "To ensure that we evaluate performance on as high-quality questions as possible, we extract only those questions deemed  answerable , by our definition, by both GPT-4o and Gemini-1.5-pro. Table  5  shows that the zero-shot samples had the highest rate of predicted answerability; each other variant shows very consistent rates of answerability. This outcome should be tempered by the results in Figure  5  which indicates that the zero-shot model had an extremely high failure rate in many other regards."
        ]
    },
    "global_footnotes": [
        "We release all code and a set of three LLaMa-2 adapters on",
        ".",
        "gpt-4o as of 1st June 2024",
        "gemini-1.5-pro as of 1st June 2024"
    ]
}