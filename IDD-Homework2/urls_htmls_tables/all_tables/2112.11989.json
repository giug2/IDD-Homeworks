{
    "PAPER'S NUMBER OF TABLES": 5,
    "S2.T1": {
        "caption": "TABLE I: Notations summary",
        "table": "<table id=\"S2.T1.17\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt ltx_border_tt\"><math id=\"S2.T1.1.1.1.m1.2\" class=\"ltx_Math\" alttext=\"N,i\" display=\"inline\"><semantics id=\"S2.T1.1.1.1.m1.2a\"><mrow id=\"S2.T1.1.1.1.m1.2.3.2\" xref=\"S2.T1.1.1.1.m1.2.3.1.cmml\"><mi id=\"S2.T1.1.1.1.m1.1.1\" xref=\"S2.T1.1.1.1.m1.1.1.cmml\">N</mi><mo id=\"S2.T1.1.1.1.m1.2.3.2.1\" xref=\"S2.T1.1.1.1.m1.2.3.1.cmml\">,</mo><mi id=\"S2.T1.1.1.1.m1.2.2\" xref=\"S2.T1.1.1.1.m1.2.2.cmml\">i</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.1.1.1.m1.2b\"><list id=\"S2.T1.1.1.1.m1.2.3.1.cmml\" xref=\"S2.T1.1.1.1.m1.2.3.2\"><ci id=\"S2.T1.1.1.1.m1.1.1.cmml\" xref=\"S2.T1.1.1.1.m1.1.1\">𝑁</ci><ci id=\"S2.T1.1.1.1.m1.2.2.cmml\" xref=\"S2.T1.1.1.1.m1.2.2\">𝑖</ci></list></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.1.1.1.m1.2c\">N,i</annotation></semantics></math></th>\n<td id=\"S2.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt ltx_border_tt\">total number, index of the remote device</td>\n</tr>\n<tr id=\"S2.T1.2.2\" class=\"ltx_tr\">\n<th id=\"S2.T1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math id=\"S2.T1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"f(\\cdot)\" display=\"inline\"><semantics id=\"S2.T1.2.2.1.m1.1a\"><mrow id=\"S2.T1.2.2.1.m1.1.2\" xref=\"S2.T1.2.2.1.m1.1.2.cmml\"><mi id=\"S2.T1.2.2.1.m1.1.2.2\" xref=\"S2.T1.2.2.1.m1.1.2.2.cmml\">f</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.T1.2.2.1.m1.1.2.1\" xref=\"S2.T1.2.2.1.m1.1.2.1.cmml\">​</mo><mrow id=\"S2.T1.2.2.1.m1.1.2.3.2\" xref=\"S2.T1.2.2.1.m1.1.2.cmml\"><mo stretchy=\"false\" id=\"S2.T1.2.2.1.m1.1.2.3.2.1\" xref=\"S2.T1.2.2.1.m1.1.2.cmml\">(</mo><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.T1.2.2.1.m1.1.1\" xref=\"S2.T1.2.2.1.m1.1.1.cmml\">⋅</mo><mo stretchy=\"false\" id=\"S2.T1.2.2.1.m1.1.2.3.2.2\" xref=\"S2.T1.2.2.1.m1.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.2.2.1.m1.1b\"><apply id=\"S2.T1.2.2.1.m1.1.2.cmml\" xref=\"S2.T1.2.2.1.m1.1.2\"><times id=\"S2.T1.2.2.1.m1.1.2.1.cmml\" xref=\"S2.T1.2.2.1.m1.1.2.1\"></times><ci id=\"S2.T1.2.2.1.m1.1.2.2.cmml\" xref=\"S2.T1.2.2.1.m1.1.2.2\">𝑓</ci><ci id=\"S2.T1.2.2.1.m1.1.1.cmml\" xref=\"S2.T1.2.2.1.m1.1.1\">⋅</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.2.2.1.m1.1c\">f(\\cdot)</annotation></semantics></math></th>\n<td id=\"S2.T1.2.2.2\" class=\"ltx_td ltx_align_center\">joint objective of FL</td>\n</tr>\n<tr id=\"S2.T1.4.4\" class=\"ltx_tr\">\n<th id=\"S2.T1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math id=\"S2.T1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"F_{i}(\\cdot)\" display=\"inline\"><semantics id=\"S2.T1.3.3.1.m1.1a\"><mrow id=\"S2.T1.3.3.1.m1.1.2\" xref=\"S2.T1.3.3.1.m1.1.2.cmml\"><msub id=\"S2.T1.3.3.1.m1.1.2.2\" xref=\"S2.T1.3.3.1.m1.1.2.2.cmml\"><mi id=\"S2.T1.3.3.1.m1.1.2.2.2\" xref=\"S2.T1.3.3.1.m1.1.2.2.2.cmml\">F</mi><mi id=\"S2.T1.3.3.1.m1.1.2.2.3\" xref=\"S2.T1.3.3.1.m1.1.2.2.3.cmml\">i</mi></msub><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.T1.3.3.1.m1.1.2.1\" xref=\"S2.T1.3.3.1.m1.1.2.1.cmml\">​</mo><mrow id=\"S2.T1.3.3.1.m1.1.2.3.2\" xref=\"S2.T1.3.3.1.m1.1.2.cmml\"><mo stretchy=\"false\" id=\"S2.T1.3.3.1.m1.1.2.3.2.1\" xref=\"S2.T1.3.3.1.m1.1.2.cmml\">(</mo><mo lspace=\"0em\" rspace=\"0em\" id=\"S2.T1.3.3.1.m1.1.1\" xref=\"S2.T1.3.3.1.m1.1.1.cmml\">⋅</mo><mo stretchy=\"false\" id=\"S2.T1.3.3.1.m1.1.2.3.2.2\" xref=\"S2.T1.3.3.1.m1.1.2.cmml\">)</mo></mrow></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.3.3.1.m1.1b\"><apply id=\"S2.T1.3.3.1.m1.1.2.cmml\" xref=\"S2.T1.3.3.1.m1.1.2\"><times id=\"S2.T1.3.3.1.m1.1.2.1.cmml\" xref=\"S2.T1.3.3.1.m1.1.2.1\"></times><apply id=\"S2.T1.3.3.1.m1.1.2.2.cmml\" xref=\"S2.T1.3.3.1.m1.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S2.T1.3.3.1.m1.1.2.2.1.cmml\" xref=\"S2.T1.3.3.1.m1.1.2.2\">subscript</csymbol><ci id=\"S2.T1.3.3.1.m1.1.2.2.2.cmml\" xref=\"S2.T1.3.3.1.m1.1.2.2.2\">𝐹</ci><ci id=\"S2.T1.3.3.1.m1.1.2.2.3.cmml\" xref=\"S2.T1.3.3.1.m1.1.2.2.3\">𝑖</ci></apply><ci id=\"S2.T1.3.3.1.m1.1.1.cmml\" xref=\"S2.T1.3.3.1.m1.1.1\">⋅</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.3.3.1.m1.1c\">F_{i}(\\cdot)</annotation></semantics></math></th>\n<td id=\"S2.T1.4.4.2\" class=\"ltx_td ltx_align_center\">local objective for remote device <math id=\"S2.T1.4.4.2.m1.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S2.T1.4.4.2.m1.1a\"><mi id=\"S2.T1.4.4.2.m1.1.1\" xref=\"S2.T1.4.4.2.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.4.4.2.m1.1b\"><ci id=\"S2.T1.4.4.2.m1.1.1.cmml\" xref=\"S2.T1.4.4.2.m1.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.4.4.2.m1.1c\">i</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S2.T1.6.6\" class=\"ltx_tr\">\n<th id=\"S2.T1.5.5.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math id=\"S2.T1.5.5.1.m1.1\" class=\"ltx_Math\" alttext=\"\\mathcal{X}_{i}\" display=\"inline\"><semantics id=\"S2.T1.5.5.1.m1.1a\"><msub id=\"S2.T1.5.5.1.m1.1.1\" xref=\"S2.T1.5.5.1.m1.1.1.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S2.T1.5.5.1.m1.1.1.2\" xref=\"S2.T1.5.5.1.m1.1.1.2.cmml\">𝒳</mi><mi id=\"S2.T1.5.5.1.m1.1.1.3\" xref=\"S2.T1.5.5.1.m1.1.1.3.cmml\">i</mi></msub><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.5.5.1.m1.1b\"><apply id=\"S2.T1.5.5.1.m1.1.1.cmml\" xref=\"S2.T1.5.5.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.5.5.1.m1.1.1.1.cmml\" xref=\"S2.T1.5.5.1.m1.1.1\">subscript</csymbol><ci id=\"S2.T1.5.5.1.m1.1.1.2.cmml\" xref=\"S2.T1.5.5.1.m1.1.1.2\">𝒳</ci><ci id=\"S2.T1.5.5.1.m1.1.1.3.cmml\" xref=\"S2.T1.5.5.1.m1.1.1.3\">𝑖</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.5.5.1.m1.1c\">\\mathcal{X}_{i}</annotation></semantics></math></th>\n<td id=\"S2.T1.6.6.2\" class=\"ltx_td ltx_align_center\">private training dataset on remote device <math id=\"S2.T1.6.6.2.m1.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S2.T1.6.6.2.m1.1a\"><mi id=\"S2.T1.6.6.2.m1.1.1\" xref=\"S2.T1.6.6.2.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.6.6.2.m1.1b\"><ci id=\"S2.T1.6.6.2.m1.1.1.cmml\" xref=\"S2.T1.6.6.2.m1.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.6.6.2.m1.1c\">i</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S2.T1.7.7\" class=\"ltx_tr\">\n<th id=\"S2.T1.7.7.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math id=\"S2.T1.7.7.1.m1.1\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics id=\"S2.T1.7.7.1.m1.1a\"><mi id=\"S2.T1.7.7.1.m1.1.1\" xref=\"S2.T1.7.7.1.m1.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.7.7.1.m1.1b\"><ci id=\"S2.T1.7.7.1.m1.1.1.cmml\" xref=\"S2.T1.7.7.1.m1.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.7.7.1.m1.1c\">t</annotation></semantics></math></th>\n<td id=\"S2.T1.7.7.2\" class=\"ltx_td ltx_align_center\">index of global communication round</td>\n</tr>\n<tr id=\"S2.T1.8.8\" class=\"ltx_tr\">\n<th id=\"S2.T1.8.8.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math id=\"S2.T1.8.8.1.m1.1\" class=\"ltx_Math\" alttext=\"e\" display=\"inline\"><semantics id=\"S2.T1.8.8.1.m1.1a\"><mi id=\"S2.T1.8.8.1.m1.1.1\" xref=\"S2.T1.8.8.1.m1.1.1.cmml\">e</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.8.8.1.m1.1b\"><ci id=\"S2.T1.8.8.1.m1.1.1.cmml\" xref=\"S2.T1.8.8.1.m1.1.1\">𝑒</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.8.8.1.m1.1c\">e</annotation></semantics></math></th>\n<td id=\"S2.T1.8.8.2\" class=\"ltx_td ltx_align_center\">index of local epoch step</td>\n</tr>\n<tr id=\"S2.T1.10.10\" class=\"ltx_tr\">\n<th id=\"S2.T1.9.9.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math id=\"S2.T1.9.9.1.m1.1\" class=\"ltx_Math\" alttext=\"\\bm{w}^{t}\" display=\"inline\"><semantics id=\"S2.T1.9.9.1.m1.1a\"><msup id=\"S2.T1.9.9.1.m1.1.1\" xref=\"S2.T1.9.9.1.m1.1.1.cmml\"><mi id=\"S2.T1.9.9.1.m1.1.1.2\" xref=\"S2.T1.9.9.1.m1.1.1.2.cmml\">𝒘</mi><mi id=\"S2.T1.9.9.1.m1.1.1.3\" xref=\"S2.T1.9.9.1.m1.1.1.3.cmml\">t</mi></msup><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.9.9.1.m1.1b\"><apply id=\"S2.T1.9.9.1.m1.1.1.cmml\" xref=\"S2.T1.9.9.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S2.T1.9.9.1.m1.1.1.1.cmml\" xref=\"S2.T1.9.9.1.m1.1.1\">superscript</csymbol><ci id=\"S2.T1.9.9.1.m1.1.1.2.cmml\" xref=\"S2.T1.9.9.1.m1.1.1.2\">𝒘</ci><ci id=\"S2.T1.9.9.1.m1.1.1.3.cmml\" xref=\"S2.T1.9.9.1.m1.1.1.3\">𝑡</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.9.9.1.m1.1c\">\\bm{w}^{t}</annotation></semantics></math></th>\n<td id=\"S2.T1.10.10.2\" class=\"ltx_td ltx_align_center\">joint model after the aggregation of <math id=\"S2.T1.10.10.2.m1.1\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics id=\"S2.T1.10.10.2.m1.1a\"><mi id=\"S2.T1.10.10.2.m1.1.1\" xref=\"S2.T1.10.10.2.m1.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.10.10.2.m1.1b\"><ci id=\"S2.T1.10.10.2.m1.1.1.cmml\" xref=\"S2.T1.10.10.2.m1.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.10.10.2.m1.1c\">t</annotation></semantics></math>-th global round</td>\n</tr>\n<tr id=\"S2.T1.12.12\" class=\"ltx_tr\">\n<th id=\"S2.T1.11.11.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math id=\"S2.T1.11.11.1.m1.2\" class=\"ltx_Math\" alttext=\"\\bm{w}^{t}_{i,e}\" display=\"inline\"><semantics id=\"S2.T1.11.11.1.m1.2a\"><msubsup id=\"S2.T1.11.11.1.m1.2.3\" xref=\"S2.T1.11.11.1.m1.2.3.cmml\"><mi id=\"S2.T1.11.11.1.m1.2.3.2.2\" xref=\"S2.T1.11.11.1.m1.2.3.2.2.cmml\">𝒘</mi><mrow id=\"S2.T1.11.11.1.m1.2.2.2.4\" xref=\"S2.T1.11.11.1.m1.2.2.2.3.cmml\"><mi id=\"S2.T1.11.11.1.m1.1.1.1.1\" xref=\"S2.T1.11.11.1.m1.1.1.1.1.cmml\">i</mi><mo id=\"S2.T1.11.11.1.m1.2.2.2.4.1\" xref=\"S2.T1.11.11.1.m1.2.2.2.3.cmml\">,</mo><mi id=\"S2.T1.11.11.1.m1.2.2.2.2\" xref=\"S2.T1.11.11.1.m1.2.2.2.2.cmml\">e</mi></mrow><mi id=\"S2.T1.11.11.1.m1.2.3.2.3\" xref=\"S2.T1.11.11.1.m1.2.3.2.3.cmml\">t</mi></msubsup><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.11.11.1.m1.2b\"><apply id=\"S2.T1.11.11.1.m1.2.3.cmml\" xref=\"S2.T1.11.11.1.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.11.11.1.m1.2.3.1.cmml\" xref=\"S2.T1.11.11.1.m1.2.3\">subscript</csymbol><apply id=\"S2.T1.11.11.1.m1.2.3.2.cmml\" xref=\"S2.T1.11.11.1.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.11.11.1.m1.2.3.2.1.cmml\" xref=\"S2.T1.11.11.1.m1.2.3\">superscript</csymbol><ci id=\"S2.T1.11.11.1.m1.2.3.2.2.cmml\" xref=\"S2.T1.11.11.1.m1.2.3.2.2\">𝒘</ci><ci id=\"S2.T1.11.11.1.m1.2.3.2.3.cmml\" xref=\"S2.T1.11.11.1.m1.2.3.2.3\">𝑡</ci></apply><list id=\"S2.T1.11.11.1.m1.2.2.2.3.cmml\" xref=\"S2.T1.11.11.1.m1.2.2.2.4\"><ci id=\"S2.T1.11.11.1.m1.1.1.1.1.cmml\" xref=\"S2.T1.11.11.1.m1.1.1.1.1\">𝑖</ci><ci id=\"S2.T1.11.11.1.m1.2.2.2.2.cmml\" xref=\"S2.T1.11.11.1.m1.2.2.2.2\">𝑒</ci></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.11.11.1.m1.2c\">\\bm{w}^{t}_{i,e}</annotation></semantics></math></th>\n<td id=\"S2.T1.12.12.2\" class=\"ltx_td ltx_align_center\">joint model after the aggregation of <math id=\"S2.T1.12.12.2.m1.1\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics id=\"S2.T1.12.12.2.m1.1a\"><mi id=\"S2.T1.12.12.2.m1.1.1\" xref=\"S2.T1.12.12.2.m1.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.12.12.2.m1.1b\"><ci id=\"S2.T1.12.12.2.m1.1.1.cmml\" xref=\"S2.T1.12.12.2.m1.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.12.12.2.m1.1c\">t</annotation></semantics></math>-th global round</td>\n</tr>\n<tr id=\"S2.T1.16.16\" class=\"ltx_tr\">\n<th id=\"S2.T1.13.13.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row\"><math id=\"S2.T1.13.13.1.m1.2\" class=\"ltx_Math\" alttext=\"\\Delta^{t}_{i,E}\" display=\"inline\"><semantics id=\"S2.T1.13.13.1.m1.2a\"><msubsup id=\"S2.T1.13.13.1.m1.2.3\" xref=\"S2.T1.13.13.1.m1.2.3.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.13.13.1.m1.2.3.2.2\" xref=\"S2.T1.13.13.1.m1.2.3.2.2.cmml\">Δ</mi><mrow id=\"S2.T1.13.13.1.m1.2.2.2.4\" xref=\"S2.T1.13.13.1.m1.2.2.2.3.cmml\"><mi id=\"S2.T1.13.13.1.m1.1.1.1.1\" xref=\"S2.T1.13.13.1.m1.1.1.1.1.cmml\">i</mi><mo id=\"S2.T1.13.13.1.m1.2.2.2.4.1\" xref=\"S2.T1.13.13.1.m1.2.2.2.3.cmml\">,</mo><mi id=\"S2.T1.13.13.1.m1.2.2.2.2\" xref=\"S2.T1.13.13.1.m1.2.2.2.2.cmml\">E</mi></mrow><mi id=\"S2.T1.13.13.1.m1.2.3.2.3\" xref=\"S2.T1.13.13.1.m1.2.3.2.3.cmml\">t</mi></msubsup><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.13.13.1.m1.2b\"><apply id=\"S2.T1.13.13.1.m1.2.3.cmml\" xref=\"S2.T1.13.13.1.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.13.13.1.m1.2.3.1.cmml\" xref=\"S2.T1.13.13.1.m1.2.3\">subscript</csymbol><apply id=\"S2.T1.13.13.1.m1.2.3.2.cmml\" xref=\"S2.T1.13.13.1.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.13.13.1.m1.2.3.2.1.cmml\" xref=\"S2.T1.13.13.1.m1.2.3\">superscript</csymbol><ci id=\"S2.T1.13.13.1.m1.2.3.2.2.cmml\" xref=\"S2.T1.13.13.1.m1.2.3.2.2\">Δ</ci><ci id=\"S2.T1.13.13.1.m1.2.3.2.3.cmml\" xref=\"S2.T1.13.13.1.m1.2.3.2.3\">𝑡</ci></apply><list id=\"S2.T1.13.13.1.m1.2.2.2.3.cmml\" xref=\"S2.T1.13.13.1.m1.2.2.2.4\"><ci id=\"S2.T1.13.13.1.m1.1.1.1.1.cmml\" xref=\"S2.T1.13.13.1.m1.1.1.1.1\">𝑖</ci><ci id=\"S2.T1.13.13.1.m1.2.2.2.2.cmml\" xref=\"S2.T1.13.13.1.m1.2.2.2.2\">𝐸</ci></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.13.13.1.m1.2c\">\\Delta^{t}_{i,E}</annotation></semantics></math></th>\n<td id=\"S2.T1.16.16.4\" class=\"ltx_td ltx_align_center\">local update for device <math id=\"S2.T1.14.14.2.m1.1\" class=\"ltx_Math\" alttext=\"i\" display=\"inline\"><semantics id=\"S2.T1.14.14.2.m1.1a\"><mi id=\"S2.T1.14.14.2.m1.1.1\" xref=\"S2.T1.14.14.2.m1.1.1.cmml\">i</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.14.14.2.m1.1b\"><ci id=\"S2.T1.14.14.2.m1.1.1.cmml\" xref=\"S2.T1.14.14.2.m1.1.1\">𝑖</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.14.14.2.m1.1c\">i</annotation></semantics></math> at <math id=\"S2.T1.15.15.3.m2.1\" class=\"ltx_Math\" alttext=\"t\" display=\"inline\"><semantics id=\"S2.T1.15.15.3.m2.1a\"><mi id=\"S2.T1.15.15.3.m2.1.1\" xref=\"S2.T1.15.15.3.m2.1.1.cmml\">t</mi><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.15.15.3.m2.1b\"><ci id=\"S2.T1.15.15.3.m2.1.1.cmml\" xref=\"S2.T1.15.15.3.m2.1.1\">𝑡</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.15.15.3.m2.1c\">t</annotation></semantics></math>-th round as <math id=\"S2.T1.16.16.4.m3.2\" class=\"ltx_Math\" alttext=\"\\bm{w}_{i}^{t}-\\bm{w}_{i,E}^{t}\" display=\"inline\"><semantics id=\"S2.T1.16.16.4.m3.2a\"><mrow id=\"S2.T1.16.16.4.m3.2.3\" xref=\"S2.T1.16.16.4.m3.2.3.cmml\"><msubsup id=\"S2.T1.16.16.4.m3.2.3.2\" xref=\"S2.T1.16.16.4.m3.2.3.2.cmml\"><mi id=\"S2.T1.16.16.4.m3.2.3.2.2.2\" xref=\"S2.T1.16.16.4.m3.2.3.2.2.2.cmml\">𝒘</mi><mi id=\"S2.T1.16.16.4.m3.2.3.2.2.3\" xref=\"S2.T1.16.16.4.m3.2.3.2.2.3.cmml\">i</mi><mi id=\"S2.T1.16.16.4.m3.2.3.2.3\" xref=\"S2.T1.16.16.4.m3.2.3.2.3.cmml\">t</mi></msubsup><mo id=\"S2.T1.16.16.4.m3.2.3.1\" xref=\"S2.T1.16.16.4.m3.2.3.1.cmml\">−</mo><msubsup id=\"S2.T1.16.16.4.m3.2.3.3\" xref=\"S2.T1.16.16.4.m3.2.3.3.cmml\"><mi id=\"S2.T1.16.16.4.m3.2.3.3.2.2\" xref=\"S2.T1.16.16.4.m3.2.3.3.2.2.cmml\">𝒘</mi><mrow id=\"S2.T1.16.16.4.m3.2.2.2.4\" xref=\"S2.T1.16.16.4.m3.2.2.2.3.cmml\"><mi id=\"S2.T1.16.16.4.m3.1.1.1.1\" xref=\"S2.T1.16.16.4.m3.1.1.1.1.cmml\">i</mi><mo id=\"S2.T1.16.16.4.m3.2.2.2.4.1\" xref=\"S2.T1.16.16.4.m3.2.2.2.3.cmml\">,</mo><mi id=\"S2.T1.16.16.4.m3.2.2.2.2\" xref=\"S2.T1.16.16.4.m3.2.2.2.2.cmml\">E</mi></mrow><mi id=\"S2.T1.16.16.4.m3.2.3.3.3\" xref=\"S2.T1.16.16.4.m3.2.3.3.3.cmml\">t</mi></msubsup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.16.16.4.m3.2b\"><apply id=\"S2.T1.16.16.4.m3.2.3.cmml\" xref=\"S2.T1.16.16.4.m3.2.3\"><minus id=\"S2.T1.16.16.4.m3.2.3.1.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.1\"></minus><apply id=\"S2.T1.16.16.4.m3.2.3.2.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.T1.16.16.4.m3.2.3.2.1.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.2\">superscript</csymbol><apply id=\"S2.T1.16.16.4.m3.2.3.2.2.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.2\"><csymbol cd=\"ambiguous\" id=\"S2.T1.16.16.4.m3.2.3.2.2.1.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.2\">subscript</csymbol><ci id=\"S2.T1.16.16.4.m3.2.3.2.2.2.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.2.2.2\">𝒘</ci><ci id=\"S2.T1.16.16.4.m3.2.3.2.2.3.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.2.2.3\">𝑖</ci></apply><ci id=\"S2.T1.16.16.4.m3.2.3.2.3.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.2.3\">𝑡</ci></apply><apply id=\"S2.T1.16.16.4.m3.2.3.3.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.16.16.4.m3.2.3.3.1.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.3\">superscript</csymbol><apply id=\"S2.T1.16.16.4.m3.2.3.3.2.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.16.16.4.m3.2.3.3.2.1.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.3\">subscript</csymbol><ci id=\"S2.T1.16.16.4.m3.2.3.3.2.2.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.3.2.2\">𝒘</ci><list id=\"S2.T1.16.16.4.m3.2.2.2.3.cmml\" xref=\"S2.T1.16.16.4.m3.2.2.2.4\"><ci id=\"S2.T1.16.16.4.m3.1.1.1.1.cmml\" xref=\"S2.T1.16.16.4.m3.1.1.1.1\">𝑖</ci><ci id=\"S2.T1.16.16.4.m3.2.2.2.2.cmml\" xref=\"S2.T1.16.16.4.m3.2.2.2.2\">𝐸</ci></list></apply><ci id=\"S2.T1.16.16.4.m3.2.3.3.3.cmml\" xref=\"S2.T1.16.16.4.m3.2.3.3.3\">𝑡</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.16.16.4.m3.2c\">\\bm{w}_{i}^{t}-\\bm{w}_{i,E}^{t}</annotation></semantics></math>\n</td>\n</tr>\n<tr id=\"S2.T1.17.17\" class=\"ltx_tr\">\n<th id=\"S2.T1.17.17.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb\"><math id=\"S2.T1.17.17.1.m1.2\" class=\"ltx_Math\" alttext=\"\\hat{\\Delta}^{t}_{i,E}\" display=\"inline\"><semantics id=\"S2.T1.17.17.1.m1.2a\"><msubsup id=\"S2.T1.17.17.1.m1.2.3\" xref=\"S2.T1.17.17.1.m1.2.3.cmml\"><mover accent=\"true\" id=\"S2.T1.17.17.1.m1.2.3.2.2\" xref=\"S2.T1.17.17.1.m1.2.3.2.2.cmml\"><mi mathvariant=\"normal\" id=\"S2.T1.17.17.1.m1.2.3.2.2.2\" xref=\"S2.T1.17.17.1.m1.2.3.2.2.2.cmml\">Δ</mi><mo id=\"S2.T1.17.17.1.m1.2.3.2.2.1\" xref=\"S2.T1.17.17.1.m1.2.3.2.2.1.cmml\">^</mo></mover><mrow id=\"S2.T1.17.17.1.m1.2.2.2.4\" xref=\"S2.T1.17.17.1.m1.2.2.2.3.cmml\"><mi id=\"S2.T1.17.17.1.m1.1.1.1.1\" xref=\"S2.T1.17.17.1.m1.1.1.1.1.cmml\">i</mi><mo id=\"S2.T1.17.17.1.m1.2.2.2.4.1\" xref=\"S2.T1.17.17.1.m1.2.2.2.3.cmml\">,</mo><mi id=\"S2.T1.17.17.1.m1.2.2.2.2\" xref=\"S2.T1.17.17.1.m1.2.2.2.2.cmml\">E</mi></mrow><mi id=\"S2.T1.17.17.1.m1.2.3.2.3\" xref=\"S2.T1.17.17.1.m1.2.3.2.3.cmml\">t</mi></msubsup><annotation-xml encoding=\"MathML-Content\" id=\"S2.T1.17.17.1.m1.2b\"><apply id=\"S2.T1.17.17.1.m1.2.3.cmml\" xref=\"S2.T1.17.17.1.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.17.17.1.m1.2.3.1.cmml\" xref=\"S2.T1.17.17.1.m1.2.3\">subscript</csymbol><apply id=\"S2.T1.17.17.1.m1.2.3.2.cmml\" xref=\"S2.T1.17.17.1.m1.2.3\"><csymbol cd=\"ambiguous\" id=\"S2.T1.17.17.1.m1.2.3.2.1.cmml\" xref=\"S2.T1.17.17.1.m1.2.3\">superscript</csymbol><apply id=\"S2.T1.17.17.1.m1.2.3.2.2.cmml\" xref=\"S2.T1.17.17.1.m1.2.3.2.2\"><ci id=\"S2.T1.17.17.1.m1.2.3.2.2.1.cmml\" xref=\"S2.T1.17.17.1.m1.2.3.2.2.1\">^</ci><ci id=\"S2.T1.17.17.1.m1.2.3.2.2.2.cmml\" xref=\"S2.T1.17.17.1.m1.2.3.2.2.2\">Δ</ci></apply><ci id=\"S2.T1.17.17.1.m1.2.3.2.3.cmml\" xref=\"S2.T1.17.17.1.m1.2.3.2.3\">𝑡</ci></apply><list id=\"S2.T1.17.17.1.m1.2.2.2.3.cmml\" xref=\"S2.T1.17.17.1.m1.2.2.2.4\"><ci id=\"S2.T1.17.17.1.m1.1.1.1.1.cmml\" xref=\"S2.T1.17.17.1.m1.1.1.1.1\">𝑖</ci><ci id=\"S2.T1.17.17.1.m1.2.2.2.2.cmml\" xref=\"S2.T1.17.17.1.m1.2.2.2.2\">𝐸</ci></list></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S2.T1.17.17.1.m1.2c\">\\hat{\\Delta}^{t}_{i,E}</annotation></semantics></math></th>\n<td id=\"S2.T1.17.17.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">approximated local update from the proposed FedLGA</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Due to the consideration of device-heterogeneity in the FL network, recent studies mainly focus on the partial participation scheme, which can avoid waiting for the slowest devices in fully participated scenario ",
                "[",
                "8",
                ", ",
                "14",
                ", ",
                "15",
                "]",
                ". Typically, partially participated FL algorithms establish a threshold ",
                "K",
                "<<",
                "N",
                "much-less-than",
                "𝐾",
                "𝑁",
                "K<<N",
                " at each iteration, i.e., it only selects the first ",
                "K",
                "𝐾",
                "K",
                " responded remote devices, all of which complete ",
                "E",
                "𝐸",
                "E",
                " local training epochs prior to sending their updated local models to the aggregator.",
                "However, such a partial participation scheme suffers from the known performance-speed dilemma in a system-heterogeneous FL network: a small ",
                "K",
                "𝐾",
                "K",
                " can speed up the distributed training but it would also significantly degrade the learning performance as it discards many important training data only stored in those slow devices (i.e., data-heterogeneity ",
                "[",
                "17",
                "]",
                "), while a large ",
                "K",
                "𝐾",
                "K",
                " can utilize more training data but its distributed training process would be greatly slowed down (i.e., data-heterogeneity ",
                "[",
                "17",
                "]",
                "). Though there have been existing studies in literature, the optimization of system-heterogeneous FL lacks formalization. For example, ",
                "[",
                "9",
                "]",
                " targets this problem by adding a proximal term, which empirically improves the learning performance, and ",
                "[",
                "18",
                "]",
                " proves that existing FL algorithms will converge to a stationary status with mismatched objective functions under heterogeneous local epochs.",
                "Instead of only waiting for all devices to complete ",
                "E",
                "𝐸",
                "E",
                " local epochs, a better solution to address this dilemma is to gather all current local learning models and aggregate them in a manner such that all local training data are utilized to learn the joint model. Specifically, we formalize the training process of FL under system-heterogeneity with the following three steps at the ",
                "t",
                "𝑡",
                "t",
                "-th global iteration.",
                "Step. I.",
                " ",
                "K",
                "𝐾",
                "K",
                " remote devices are selected by the aggregator as a subset ",
                "𝒦",
                "𝒦",
                "\\mathcal{K}",
                ", where ",
                "|",
                "𝒦",
                "|",
                "=",
                "K",
                "𝒦",
                "𝐾",
                "|\\mathcal{K}|=K",
                ", which receive the current joint model ",
                "𝒘",
                "t",
                "superscript",
                "𝒘",
                "𝑡",
                "\\bm{w}^{t}",
                " as their local model ",
                "𝒘",
                "i",
                "t",
                "=",
                "𝒘",
                "t",
                "subscript",
                "superscript",
                "𝒘",
                "𝑡",
                "𝑖",
                "superscript",
                "𝒘",
                "𝑡",
                "\\bm{w}^{t}_{i}=\\bm{w}^{t}",
                ". The aggregator also delivers an expected epoch number ",
                "E",
                "𝐸",
                "E",
                ".",
                "Step. II",
                " Due to the diverse computational capacity, the ",
                "i",
                "𝑖",
                "i",
                "-th device performs local training for ",
                "E",
                "i",
                "subscript",
                "𝐸",
                "𝑖",
                "E_{i}",
                " steps, where ",
                "1",
                "≤",
                "E",
                "i",
                "<",
                "E",
                "1",
                "subscript",
                "𝐸",
                "𝑖",
                "𝐸",
                "1\\leq E_{i}<E",
                ". Then, the learning results are sent back to the aggregator synchronously.",
                "Step. III",
                " The aggregator updates the joint model ",
                "𝒘",
                "t",
                "+",
                "1",
                "superscript",
                "𝒘",
                "𝑡",
                "1",
                "\\bm{w}^{t+1}",
                " with the received local learning results under a well-designed aggregation rule.",
                "Note that the system-heterogeneous FL formulation has one main difference from the settings in prior works such as FedAvg ",
                "[",
                "8",
                "]",
                ": the ",
                "K",
                "𝐾",
                "K",
                " remote devices are randomly selected in each iteration, instead of only considering the ",
                "K",
                "𝐾",
                "K",
                " fastest devices, which guarantees that each device shares the same probability ",
                "K",
                "N",
                "𝐾",
                "𝑁",
                "\\frac{K}{N}",
                " of being selected at each iteration. Particularly, we use a virtual subset ",
                "𝒦",
                "1",
                "∈",
                "𝒦",
                "subscript",
                "𝒦",
                "1",
                "𝒦",
                "\\mathcal{K}_{1}\\in\\mathcal{K}",
                " to represent the remote device ",
                "i",
                "𝑖",
                "i",
                " that only performs ",
                "E",
                "i",
                "<",
                "E",
                "subscript",
                "𝐸",
                "𝑖",
                "𝐸",
                "E_{i}<E",
                " local training epochs, where ",
                "|",
                "𝒦",
                "1",
                "|",
                "=",
                "K",
                "1",
                "subscript",
                "𝒦",
                "1",
                "subscript",
                "𝐾",
                "1",
                "|\\mathcal{K}_{1}|=K_{1}",
                ", and introduce a hyper-parameter ",
                "ρ",
                "=",
                "K",
                "1",
                "/",
                "K",
                "𝜌",
                "subscript",
                "𝐾",
                "1",
                "𝐾",
                "\\rho={K_{1}}/{K}",
                " as the device-heterogeneous ratio. To better present the diverse local updates due to the system-heterogeneity, we denote the local update of the ",
                "i",
                "𝑖",
                "i",
                "-th device at iteration ",
                "t",
                "𝑡",
                "t",
                " after ",
                "E",
                "i",
                "subscript",
                "𝐸",
                "𝑖",
                "E_{i}",
                " epochs as ",
                "Δ",
                "i",
                ",",
                "E",
                "i",
                "t",
                "=",
                "𝒘",
                "i",
                "t",
                "−",
                "𝒘",
                "i",
                ",",
                "E",
                "i",
                "t",
                "subscript",
                "superscript",
                "Δ",
                "𝑡",
                "𝑖",
                "subscript",
                "𝐸",
                "𝑖",
                "superscript",
                "subscript",
                "𝒘",
                "𝑖",
                "𝑡",
                "superscript",
                "subscript",
                "𝒘",
                "𝑖",
                "subscript",
                "𝐸",
                "𝑖",
                "𝑡",
                "\\Delta^{t}_{i,E_{i}}=\\bm{w}_{i}^{t}-\\bm{w}_{i,E_{i}}^{t}",
                ", where ",
                "𝒘",
                "i",
                "t",
                "superscript",
                "subscript",
                "𝒘",
                "𝑖",
                "𝑡",
                "\\bm{w}_{i}^{t}",
                " is the initial model before local training (i.e., ",
                "𝒘",
                "i",
                "t",
                "=",
                "𝒘",
                "i",
                ",",
                "0",
                "t",
                "superscript",
                "subscript",
                "𝒘",
                "𝑖",
                "𝑡",
                "superscript",
                "subscript",
                "𝒘",
                "𝑖",
                "0",
                "𝑡",
                "\\bm{w}_{i}^{t}=\\bm{w}_{i,0}^{t}",
                ") and the expected update with full ",
                "E",
                "𝐸",
                "E",
                " epochs is ",
                "Δ",
                "i",
                ",",
                "E",
                "t",
                "subscript",
                "superscript",
                "Δ",
                "𝑡",
                "𝑖",
                "𝐸",
                "\\Delta^{t}_{i,E}",
                ". Hence, under the system-heterogeneity of FL, we aim to minimize the following objective between ",
                "Δ",
                "i",
                ",",
                "E",
                "t",
                "subscript",
                "superscript",
                "Δ",
                "𝑡",
                "𝑖",
                "𝐸",
                "\\Delta^{t}_{i,E}",
                " and ",
                "Δ",
                "i",
                ",",
                "E",
                "i",
                "t",
                "subscript",
                "superscript",
                "Δ",
                "𝑡",
                "𝑖",
                "subscript",
                "𝐸",
                "𝑖",
                "\\Delta^{t}_{i,E_{i}}",
                " at each communication round ",
                "t",
                "𝑡",
                "t",
                "In other words, we want to approximate the expected model update ",
                "Δ",
                "i",
                ",",
                "E",
                "t",
                "subscript",
                "superscript",
                "Δ",
                "𝑡",
                "𝑖",
                "𝐸",
                "\\Delta^{t}_{i,E}",
                " from the received ",
                "Δ",
                "i",
                ",",
                "E",
                "i",
                "t",
                "subscript",
                "superscript",
                "Δ",
                "𝑡",
                "𝑖",
                "subscript",
                "𝐸",
                "𝑖",
                "\\Delta^{t}_{i,E_{i}}",
                ". This approximation can be performed in the aggregator, which does not introduce any extra computations in remote devices. To achieve this, inspired by prior studies on gradient approximation for improving centralized SGD optimization problems ",
                "[",
                "19",
                ", ",
                "20",
                ", ",
                "21",
                "]",
                ", we propose the Federated local gradient approximation (FedLGA) algorithm, which is introduced in detail in the next section."
            ]
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Convergence rates for FL optimization approaches.",
        "table": "",
        "footnotes": "[tb]\n\n\n\n\nMethod\nDataset\nConvexity1\nPartial Worker2\nDevice Heterogeneous3\nOther Assumptions4\nConvergence Rate\n\n\n\nStich et al. [5]\ni.i.d.\nSC\n✗\n✗\nBCGV; BOGV\n𝒪​(N​ET)+𝒪​(1N​E​T)𝒪𝑁𝐸𝑇𝒪1𝑁𝐸𝑇\\mathcal{O}(\\frac{NE}{T})+\\mathcal{O}(\\frac{1}{\\sqrt{NET}})\n\nKhaled et al. [35]\nnon-i.i.d.\nC\n✗\n✗\nBOGV; LBG\n𝒪​(1T)+𝒪​(1N​T)𝒪1𝑇𝒪1𝑁𝑇\\mathcal{O}(\\frac{1}{T})+\\mathcal{O}(\\frac{1}{\\sqrt{NT}})\n\nLi et al. [8]\nnon-i.i.d.\nSC\n✓\n✗\nBOBD; BLGV; BLGN\n𝒪​(ET)𝒪𝐸𝑇\\mathcal{O}(\\frac{E}{T})\n\nFedProx [9]\nnon-i.i.d.\nNC\n✓\n✓\nBGV; Prox\n𝒪​(1T)𝒪1𝑇\\mathcal{O}(\\frac{1}{\\sqrt{T}})\n\nScaffold [14]\nnon-i.i.d.\nNC\n✓\n✗\nBLGV; VR\n𝒪​(1T)+𝒪​(1N​E​T)𝒪1𝑇𝒪1𝑁𝐸𝑇\\mathcal{O}(\\frac{1}{T})+\\mathcal{O}(\\frac{1}{\\sqrt{NET}})\n\nYang et al. [15]\nnon-i.i.d.\nNC\n✓\n✗\nBLGV\n𝒪​(1T)+𝒪​(1N​E​T)𝒪1𝑇𝒪1𝑁𝐸𝑇\\mathcal{O}(\\frac{1}{T})+\\mathcal{O}(\\frac{1}{\\sqrt{NET}})\n\nFedLGA\nnon-i.i.d\nNC\n✓\n✓\nBLGV\n𝒪​(𝟏𝐓)+𝒪​((𝟏+ρ)​𝐄𝐓𝐊)𝒪1𝐓𝒪1𝜌𝐄𝐓𝐊\\mathbf{\\mathcal{O}(\\frac{1}{T})+\\mathcal{O}(\\frac{(1+\\rho)\\sqrt{E}}{\\sqrt{TK}})}\n\nShorthand notations for the convexity of the introduced methods: SC: Strongly Convex, C: Convex and NC: Non-Convex.Shorthand summaries for whether the compared method satisfies the partial participation scheme: ✓: satisfy and ✗: not satisfy.Shorthand summaries for whether the device-heterogeneity of FL is considered: ✓: yes and ✗: no.Shorthand notation for other assumptions and variants. BCGV: the remote gradients are bounded as 𝔼​[‖∇Fi​(𝒘it,ℬi,et)−∇f​(𝒘it)‖2]≤σ2𝔼delimited-[]superscriptnorm∇subscript𝐹𝑖superscriptsubscript𝒘𝑖𝑡superscriptsubscriptℬ𝑖𝑒𝑡∇𝑓superscriptsubscript𝒘𝑖𝑡2superscript𝜎2\\mathbb{E}[||\\nabla F_{i}(\\bm{w}_{i}^{t},\\mathcal{B}_{i,e}^{t})-\\nabla f(\\bm{w}_{i}^{t})||^{2}]\\leq\\sigma^{2}. BOGV: the variance of optimal gradient is bounded as 𝔼[||∇f(𝒘⋆||2]≤σ2\\mathbb{E}[||\\nabla f(\\bm{w}^{\\star}||^{2}]\\leq\\sigma^{2}. BOBD: the difference of optimal objective is bounded as f​(𝒘⋆)−𝔼​[Fi​(𝒘⋆)]≤σ2𝑓superscript𝒘⋆𝔼delimited-[]subscript𝐹𝑖superscript𝒘⋆superscript𝜎2f(\\bm{w}^{\\star})-\\mathbb{E}[{F_{i}(\\bm{w}^{\\star})}]\\leq\\sigma^{2}. BGV: the dissimilarity of remote gradients are bounded 𝔼​[‖∇Fi​(𝒘it)‖2]/‖∇f​(𝒘t)‖2≤σ2𝔼delimited-[]superscriptnorm∇subscript𝐹𝑖subscriptsuperscript𝒘𝑡𝑖2superscriptnorm∇𝑓superscript𝒘𝑡2superscript𝜎2\\mathbb{E}[||\\nabla F_{i}(\\bm{w}^{t}_{i})||^{2}]/||\\nabla f(\\bm{w}^{t})||^{2}\\leq\\sigma^{2}. BLGV: the variance of stochastic gradients on each remote device is bounded (same as our Assumption. 3). BLGN: the norm of an arbitrary remote update is bounded. LBG: each remote devices use the full batch of local training data for update computing. Prox: the remote objective considers proximal gradient steps. VR: followed by trackable states, there is variance reduction.\nNote that for better presentation, we use a unified σ𝜎\\sigma symbol, which can vary depending on the detailed method.",
        "references": [
            [
                "We first provide the convergence analysis of the proposed FedLGA algorithm under the full device participation scheme, where we have the following results.",
                "Let Assumptions ",
                "1",
                "-",
                "4",
                " hold. The local and global learning rates ",
                "η",
                "l",
                "subscript",
                "𝜂",
                "𝑙",
                "\\eta_{l}",
                " and ",
                "η",
                "g",
                "subscript",
                "𝜂",
                "𝑔",
                "\\eta_{g}",
                " are chosen such that ",
                "η",
                "l",
                "<",
                "1",
                "30",
                "​",
                "(",
                "1",
                "+",
                "ρ",
                ")",
                "​",
                "L",
                "​",
                "E",
                "subscript",
                "𝜂",
                "𝑙",
                "1",
                "30",
                "1",
                "𝜌",
                "𝐿",
                "𝐸",
                "\\eta_{l}<\\frac{1}{\\sqrt{30(1+\\rho)}LE}",
                " and ",
                "η",
                "g",
                "​",
                "η",
                "l",
                "≤",
                "1",
                "(",
                "1",
                "+",
                "ρ",
                ")",
                "​",
                "L",
                "​",
                "E",
                "subscript",
                "𝜂",
                "𝑔",
                "subscript",
                "𝜂",
                "𝑙",
                "1",
                "1",
                "𝜌",
                "𝐿",
                "𝐸",
                "\\eta_{g}\\eta_{l}\\leq\\frac{1}{(1+\\rho)LE}",
                ". Under full device participation scheme, the iterates of FedLGA satisfy",
                "where ",
                "f",
                "0",
                "=",
                "f",
                "​",
                "(",
                "𝐰",
                "0",
                ")",
                ",",
                "f",
                "⋆",
                "=",
                "f",
                "​",
                "(",
                "𝐰",
                "⋆",
                ")",
                "formulae-sequence",
                "superscript",
                "𝑓",
                "0",
                "𝑓",
                "superscript",
                "𝐰",
                "0",
                "superscript",
                "𝑓",
                "⋆",
                "𝑓",
                "superscript",
                "𝐰",
                "⋆",
                "f^{0}=f(\\bm{w}^{0}),f^{\\star}=f(\\bm{w}^{\\star})",
                ", ",
                "c",
                "1",
                "subscript",
                "𝑐",
                "1",
                "c_{1}",
                " is constant, the expectation is over the remote training dataset among all devices, and ",
                "Φ",
                "1",
                "=",
                "1",
                "c",
                "1",
                "​",
                "[",
                "(",
                "1",
                "+",
                "ρ",
                ")",
                "​",
                "η",
                "g",
                "​",
                "η",
                "l",
                "​",
                "σ",
                "l",
                "2",
                "2",
                "​",
                "N",
                "+",
                "5",
                "2",
                "​",
                "η",
                "l",
                "2",
                "​",
                "E",
                "​",
                "L",
                "2",
                "​",
                "(",
                "σ",
                "l",
                "2",
                "+",
                "6",
                "​",
                "E",
                "​",
                "σ",
                "g",
                "2",
                ")",
                "+",
                "c",
                "2",
                "​",
                "𝔼",
                "​",
                "‖",
                "∇",
                "F",
                "i",
                "​",
                "(",
                "𝐰",
                "T",
                ")",
                "‖",
                "4",
                "]",
                "subscript",
                "Φ",
                "1",
                "1",
                "subscript",
                "𝑐",
                "1",
                "delimited-[]",
                "1",
                "𝜌",
                "subscript",
                "𝜂",
                "𝑔",
                "subscript",
                "𝜂",
                "𝑙",
                "superscript",
                "subscript",
                "𝜎",
                "𝑙",
                "2",
                "2",
                "𝑁",
                "5",
                "2",
                "superscript",
                "subscript",
                "𝜂",
                "𝑙",
                "2",
                "𝐸",
                "superscript",
                "𝐿",
                "2",
                "superscript",
                "subscript",
                "𝜎",
                "𝑙",
                "2",
                "6",
                "𝐸",
                "superscript",
                "subscript",
                "𝜎",
                "𝑔",
                "2",
                "subscript",
                "𝑐",
                "2",
                "𝔼",
                "superscript",
                "norm",
                "∇",
                "subscript",
                "𝐹",
                "𝑖",
                "superscript",
                "𝐰",
                "𝑇",
                "4",
                "\\Phi_{1}=\\frac{1}{c_{1}}[\\frac{(1+\\rho)\\eta_{g}\\eta_{l}\\sigma_{l}^{2}}{2N}+\\frac{5}{2}\\eta_{l}^{2}EL^{2}(\\sigma_{l}^{2}+6E\\sigma_{g}^{2})+c_{2}\\mathbb{E}||\\nabla F_{i}(\\bm{w}^{T})||^{4}]",
                ", ",
                "(",
                "1",
                "2",
                "−",
                "15",
                "​",
                "(",
                "1",
                "+",
                "ρ",
                ")",
                "​",
                "E",
                "2",
                "​",
                "η",
                "l",
                "2",
                "​",
                "L",
                "2",
                ")",
                ">",
                "c",
                "1",
                ">",
                "0",
                "1",
                "2",
                "15",
                "1",
                "𝜌",
                "superscript",
                "𝐸",
                "2",
                "superscript",
                "subscript",
                "𝜂",
                "𝑙",
                "2",
                "superscript",
                "𝐿",
                "2",
                "subscript",
                "𝑐",
                "1",
                "0",
                "(\\frac{1}{2}-15(1+\\rho)E^{2}\\eta_{l}^{2}L^{2})>c_{1}>0",
                ", and ",
                "c",
                "2",
                "=",
                "η",
                "g",
                "​",
                "η",
                "l",
                "2",
                "​",
                "ρ",
                "​",
                "M",
                "2",
                "​",
                "τ",
                "m",
                "​",
                "a",
                "​",
                "x",
                "2",
                "N",
                "​",
                "η",
                "g",
                "​",
                "η",
                "l",
                "​",
                "(",
                "η",
                "g",
                "​",
                "L",
                "+",
                "η",
                "l",
                "3",
                "​",
                "τ",
                "m",
                "​",
                "a",
                "​",
                "x",
                "2",
                ")",
                "subscript",
                "𝑐",
                "2",
                "subscript",
                "𝜂",
                "𝑔",
                "superscript",
                "subscript",
                "𝜂",
                "𝑙",
                "2",
                "𝜌",
                "superscript",
                "𝑀",
                "2",
                "superscript",
                "subscript",
                "𝜏",
                "𝑚",
                "𝑎",
                "𝑥",
                "2",
                "𝑁",
                "subscript",
                "𝜂",
                "𝑔",
                "subscript",
                "𝜂",
                "𝑙",
                "subscript",
                "𝜂",
                "𝑔",
                "𝐿",
                "superscript",
                "subscript",
                "𝜂",
                "𝑙",
                "3",
                "superscript",
                "subscript",
                "𝜏",
                "𝑚",
                "𝑎",
                "𝑥",
                "2",
                "c_{2}=\\frac{\\eta_{g}\\eta_{l}^{2}\\rho M^{2}\\tau_{max}^{2}}{N\\eta_{g}\\eta_{l}}(\\eta_{g}L+\\eta_{l}^{3}\\tau_{max}^{2})",
                ".",
                "See in online Appendix A, available in ",
                "[",
                "34",
                "]",
                ".\n∎",
                "Suppose the learning rates ",
                "η",
                "l",
                "subscript",
                "𝜂",
                "𝑙",
                "\\eta_{l}",
                " and ",
                "η",
                "g",
                "subscript",
                "𝜂",
                "𝑔",
                "\\eta_{g}",
                " are such that the condition in Theorem ",
                "1",
                " are satisfied. Let ",
                "η",
                "l",
                "=",
                "1",
                "T",
                "​",
                "E",
                "​",
                "L",
                "subscript",
                "𝜂",
                "𝑙",
                "1",
                "𝑇",
                "𝐸",
                "𝐿",
                "\\eta_{l}=\\frac{1}{\\sqrt{T}EL}",
                " and ",
                "η",
                "g",
                "=",
                "E",
                "​",
                "N",
                "subscript",
                "𝜂",
                "𝑔",
                "𝐸",
                "𝑁",
                "\\eta_{g}=\\sqrt{EN}",
                ". The convergence rate of proposed FedLGA under full device participation scheme satisfies",
                "From the results in Theorem ",
                "1",
                ", the convergence bound of full device participation FedLGA contains two parts: a vanishing term ",
                "f",
                "0",
                "−",
                "f",
                "⋆",
                "c",
                "1",
                "​",
                "η",
                "g",
                "​",
                "η",
                "l",
                "​",
                "E",
                "​",
                "T",
                "superscript",
                "𝑓",
                "0",
                "superscript",
                "𝑓",
                "⋆",
                "subscript",
                "𝑐",
                "1",
                "subscript",
                "𝜂",
                "𝑔",
                "subscript",
                "𝜂",
                "𝑙",
                "𝐸",
                "𝑇",
                "\\frac{f^{0}-f^{\\star}}{c_{1}\\eta_{g}\\eta_{l}ET}",
                " corresponding to the increase of ",
                "T",
                "𝑇",
                "T",
                " and a constant term ",
                "Φ",
                "1",
                "subscript",
                "Φ",
                "1",
                "\\Phi_{1}",
                ", which is independent of ",
                "T",
                "𝑇",
                "T",
                ". We can notice that as the value of ",
                "c",
                "1",
                "subscript",
                "𝑐",
                "1",
                "c_{1}",
                " is related to ",
                "ρ",
                "𝜌",
                "\\rho",
                ", the vanishing term which dominates the convergence of FedLGA algorithm is impacted by the device-heterogeneity. Additionally, we find an interesting boundary phenomenon on the vanishing term in Theorem ",
                "1",
                " that, when the FL network satisfies ",
                "ρ",
                "=",
                "0",
                "𝜌",
                "0",
                "\\rho=0",
                ", the decay rate of the vanishing term matches the prior studies of FedAVG with two-sided learning rates ",
                "[",
                "15",
                "]",
                ".",
                "For the constant term ",
                "Φ",
                "1",
                "subscript",
                "Φ",
                "1",
                "\\Phi_{1}",
                " in Theorem ",
                "1",
                ", we consider the first part ",
                "(",
                "1",
                "+",
                "ρ",
                ")",
                "​",
                "η",
                "g",
                "​",
                "η",
                "l",
                "​",
                "σ",
                "l",
                "2",
                "2",
                "​",
                "N",
                "1",
                "𝜌",
                "subscript",
                "𝜂",
                "𝑔",
                "subscript",
                "𝜂",
                "𝑙",
                "superscript",
                "subscript",
                "𝜎",
                "𝑙",
                "2",
                "2",
                "𝑁",
                "\\frac{(1+\\rho)\\eta_{g}\\eta_{l}\\sigma_{l}^{2}}{2N}",
                " is from the local gradient variance of remote devices, which is linear to ",
                "ρ",
                "𝜌",
                "\\rho",
                ". And the second part ",
                "5",
                "2",
                "​",
                "η",
                "l",
                "2",
                "​",
                "E",
                "​",
                "L",
                "2",
                "​",
                "(",
                "σ",
                "l",
                "2",
                "+",
                "6",
                "​",
                "E",
                "​",
                "σ",
                "g",
                "2",
                ")",
                "5",
                "2",
                "superscript",
                "subscript",
                "𝜂",
                "𝑙",
                "2",
                "𝐸",
                "superscript",
                "𝐿",
                "2",
                "superscript",
                "subscript",
                "𝜎",
                "𝑙",
                "2",
                "6",
                "𝐸",
                "superscript",
                "subscript",
                "𝜎",
                "𝑔",
                "2",
                "\\frac{5}{2}\\eta_{l}^{2}EL^{2}(\\sigma_{l}^{2}+6E\\sigma_{g}^{2})",
                " denotes the cumulative variance of ",
                "E",
                "𝐸",
                "E",
                " local training epochs, which is also influenced by the data-heterogeneity ",
                "σ",
                "g",
                "subscript",
                "𝜎",
                "𝑔",
                "\\sigma_{g}",
                ". Inspired by ",
                "[",
                "15",
                "]",
                ", we consider an inverse relationship between ",
                "η",
                "l",
                "subscript",
                "𝜂",
                "𝑙",
                "\\eta_{l}",
                " and ",
                "E",
                "𝐸",
                "E",
                ", e.g., ",
                "η",
                "l",
                "∝",
                "𝒪",
                "​",
                "(",
                "1",
                "K",
                ")",
                "proportional-to",
                "subscript",
                "𝜂",
                "𝑙",
                "𝒪",
                "1",
                "𝐾",
                "\\eta_{l}\\propto\\mathcal{O}(\\frac{1}{K})",
                ". For the third term, we can notice that it is quadratically amplified by the variance of optimal gradient as ",
                "𝔼",
                "​",
                "‖",
                "∇",
                "F",
                "i",
                "​",
                "(",
                "𝒘",
                "⋆",
                ")",
                "‖",
                "2",
                "𝔼",
                "superscript",
                "norm",
                "∇",
                "subscript",
                "𝐹",
                "𝑖",
                "superscript",
                "𝒘",
                "⋆",
                "2",
                "\\mathbb{E}||\\nabla F_{i}(\\bm{w}^{\\star})||^{2}",
                ". Note that different from other FL optimization analysis that assume a bounded optimal gradient ",
                "[",
                "9",
                ", ",
                "8",
                "]",
                ", the proposed FedLGA does not require such assumption. Hence, in order to address the high power third term of ",
                "𝔼",
                "​",
                "‖",
                "∇",
                "F",
                "i",
                "​",
                "(",
                "𝒘",
                "⋆",
                ")",
                "‖",
                "2",
                "𝔼",
                "superscript",
                "norm",
                "∇",
                "subscript",
                "𝐹",
                "𝑖",
                "superscript",
                "𝒘",
                "⋆",
                "2",
                "\\mathbb{E}||\\nabla F_{i}(\\bm{w}^{\\star})||^{2}",
                ", we apply a weighted decay ",
                "γ",
                "𝛾",
                "\\gamma",
                " factor to local learning rate as ",
                "η",
                "l",
                "t",
                "+",
                "1",
                "=",
                "(",
                "1",
                "−",
                "γ",
                ")",
                "​",
                "η",
                "l",
                "t",
                "superscript",
                "subscript",
                "𝜂",
                "𝑙",
                "𝑡",
                "1",
                "1",
                "𝛾",
                "superscript",
                "subscript",
                "𝜂",
                "𝑙",
                "𝑡",
                "\\eta_{l}^{t+1}=(1-\\gamma)\\eta_{l}^{t}",
                ". Additionally, as suggested in ",
                "[",
                "25",
                "]",
                ", the third term indicates the staleness, which could be controlled via a inverse function such as ",
                "τ",
                "i",
                "​",
                "(",
                "t",
                ")",
                "∝",
                "𝒪",
                "​",
                "(",
                "1",
                "t",
                "+",
                "1",
                ")",
                "proportional-to",
                "subscript",
                "𝜏",
                "𝑖",
                "𝑡",
                "𝒪",
                "1",
                "𝑡",
                "1",
                "\\tau_{i}(t)\\propto\\mathcal{O}(\\frac{1}{t+1})",
                "."
            ]
        ]
    },
    "S5.T3": {
        "caption": "TABLE III: Dataset information overview.",
        "table": "",
        "footnotes": "[tb]\n\n\n\n\nDataset\nDataset Size\nClasses\nP𝑃P1\nImage Feature\n\n\n\nFMNIST [38]\n60,0006000060,000\n101010\n2\n28×28282828\\times 28\n\nCIFAR-10 [39]\n60,0006000060,000\n101010\n2\n32×32×33232332\\times 32\\times 3\n\nCIFAR-100 [39]\n60,0006000060,000\n100100100\n20\n32×32×33232332\\times 32\\times 3\n\nShorthand notation for the number of classes in one remote device.",
        "references": [
            [
                "To evaluate the proposed FedLGA, we conducted comprehensive experiments under the system-heterogeneous FL network studied in this paper on multiple real-world datasets. Note that the experiments are performed with 1 GeForce GTX 1080Ti GPU on Pytorch ",
                "[",
                "36",
                "]",
                " and we follow the settings in ",
                "[",
                "37",
                "]",
                " to implement the FL baseline (e.g., FedAVG).",
                "Datasets and models:",
                "\nThree popular read-world dataset are considered in this paper: FMNIST ",
                "[",
                "38",
                "]",
                " (Fashion MMNIST), CIFAR-10 and CIFAR-100 ",
                "[",
                "39",
                "]",
                ". Considering a FL network with ",
                "N",
                "=",
                "50",
                "𝑁",
                "50",
                "N=50",
                " remote devices, we introduce the general information of each dataset as shown in Table. ",
                "III",
                ". Note that for the ",
                "32",
                "×",
                "32",
                "×",
                "3",
                "32",
                "32",
                "3",
                "32\\times 32\\times 3",
                " color images in CIFAR-10 and CIFAR-100 datasets, we make the following data pre-processing to improve the FL training performance: each image sample is normalized, cropped to size ",
                "32",
                "32",
                "32",
                ", horizontally flipped with the probability of ",
                "50",
                "%",
                "percent",
                "50",
                "50\\%",
                " and resized to ",
                "224",
                "×",
                "224",
                "224",
                "224",
                "224\\times 224",
                ".",
                "Then, we follow the previous settings in ",
                "[",
                "37",
                ", ",
                "2",
                "]",
                " to present the data-heterogeneity of FL. In this paper, we consider the following non-overlapped non-i.i.d. training data partition scenario, where the ",
                "i",
                "𝑖",
                "i",
                "-th remote private dataset ",
                "𝒳",
                "i",
                "subscript",
                "𝒳",
                "𝑖",
                "\\mathcal{X}_{i}",
                " and the total training dataset ",
                "𝒳",
                "𝒳",
                "\\mathcal{X}",
                " satisfy: ",
                "|",
                "𝒳",
                "|",
                "=",
                "∑",
                "i",
                "|",
                "𝒳",
                "i",
                "|",
                "𝒳",
                "subscript",
                "𝑖",
                "subscript",
                "𝒳",
                "𝑖",
                "|\\mathcal{X}|=\\sum_{i}|\\mathcal{X}_{i}|",
                ". Then, for each remote training dataset ",
                "𝒳",
                "i",
                "subscript",
                "𝒳",
                "𝑖",
                "\\mathcal{X}_{i}",
                ", we consider it contains ",
                "P",
                "𝑃",
                "P",
                " classes of samples. Note that for FMNIST and CIFAR-10, we set ",
                "P",
                "=",
                "2",
                "𝑃",
                "2",
                "P=2",
                " and for CIFAR-100, we set ",
                "P",
                "=",
                "20",
                "𝑃",
                "20",
                "P=20",
                " by default. To solve the classification problems from the introduced datasets, we run two different neuron network models. For FMNIST, we run a two-layer fully connect MLP network with 400 hidden nodes. For CIFAR-10 and CIFAR-100, we run a ResNet network, which follows the settings in ",
                "[",
                "40",
                "]",
                ".",
                "Implementation:",
                " In this work, we simulated a FL network with the formulated system-heterogeneous problem. Note that we would like to emphasize that the initialized hyper-parameter settings are directly from the default setups of previous FL works ",
                "[",
                "41",
                ", ",
                "37",
                "]",
                ", which are not manually tuned to make the proposed FedLGA algorithm perform better. The system-heterogeneous FL network in our simulation is with the following settings by default",
                "The total number of remote devices ",
                "N",
                "=",
                "50",
                "𝑁",
                "50",
                "N=50",
                ".",
                "For each global communication round, the number of devices being chosen by the aggregator is ",
                "K",
                "=",
                "10",
                "𝐾",
                "10",
                "K=10",
                ".",
                "For the local training process, we set ",
                "E",
                "=",
                "5",
                "𝐸",
                "5",
                "E=5",
                " and ",
                "|",
                "ℬ",
                "|",
                "=",
                "10",
                "ℬ",
                "10",
                "|\\mathcal{B}|=10",
                ".",
                "To illustrate the device-heterogeneity, we set ",
                "ρ",
                "=",
                "0.5",
                "𝜌",
                "0.5",
                "\\rho=0.5",
                " and ",
                "τ",
                "m",
                "​",
                "a",
                "​",
                "x",
                "=",
                "E",
                "−",
                "1",
                "subscript",
                "𝜏",
                "𝑚",
                "𝑎",
                "𝑥",
                "𝐸",
                "1",
                "\\tau_{max}=E-1",
                ", where ",
                "τ",
                "i",
                "subscript",
                "𝜏",
                "𝑖",
                "\\tau_{i}",
                " for the ",
                "i",
                "𝑖",
                "i",
                "-th device is uniformly distributed within ",
                "[",
                "1",
                ",",
                "τ",
                "m",
                "​",
                "a",
                "​",
                "x",
                "]",
                "1",
                "subscript",
                "𝜏",
                "𝑚",
                "𝑎",
                "𝑥",
                "[1,\\tau_{max}]",
                ".",
                "Compared Methods:",
                " We compared the performance of FedLGA with the following five representative FL methods",
                "FedAvg:",
                " ",
                "[",
                "2",
                "]",
                " is considered as onE of the groundbreaking works in the FL research field. We set up the FedAvg approach based on the settings in ",
                "[",
                "8",
                "]",
                ", which firstly provides a convergence guarantee against data-heterogeneous FL. Note that in our simulation, we follow the scheme I in ",
                "[",
                "8",
                "]",
                " for the partial participation.",
                "FedProx:",
                " ",
                "[",
                "9",
                "]",
                " is one popular variant of FedAvg which adds a quadratic proximal term to limit the impact from local updates in the device-heterogeneous FL. In this paper, we follow the instructions provided in ",
                "[",
                "9",
                "]",
                " that set the ",
                "μ",
                "=",
                "1",
                "𝜇",
                "1",
                "\\mu=1",
                ", which controls the local objective dissimilarity.",
                "FedNova:",
                " ",
                "[",
                "18",
                "]",
                " improves FedAvg from the aggregator side. It assumes a diverse local update scenario where each remote device may perform the different number of local epochs. To achieve this, FedNova normalizes and scales the local updates, which is also considered as a modification to FedAvg.",
                "Scaffold:",
                " ",
                "[",
                "14",
                "]",
                " model the data-heterogeneous FL problem as the global variance among each remote device in the network. Scaffold address this problem by controlling the variates between the aggregator and the devices to estimate the joint model update direction, which is achieved via applying the variance reduction technique ",
                "[",
                "42",
                ", ",
                "43",
                "]",
                ".",
                "FedDyn:",
                " ",
                "[",
                "44",
                "]",
                " adds a regularization term on FedAvg on the remote device side at each local training epoch, which is developed based on the joint model and the local training model at the previous global round.",
                "Evaluation Metrics:",
                " To evaluate the experimental results accurately, we introduce the following two categories of evaluation metrics, each of which is investigated in multiple ways. Note that in our analysis, we define a target testing accuracy for each dataset as: FMNIST ",
                "65",
                "%",
                "percent",
                "65",
                "65\\%",
                ", CIFAR-10 ",
                "55",
                "%",
                "percent",
                "55",
                "55\\%",
                " and CIFAR-100 ",
                "40",
                "%",
                "percent",
                "40",
                "40\\%",
                ".",
                "Model performance:",
                " To evaluate the learned joint model under the formulated system-heterogeneous FL network, we investigate the training loss, the testing accuracy and the best-achieved accuracy for each FL approach.",
                "Communication in FL network:",
                " As the FL network is simulated on one desktop with the Python threading library and all the computations are performed on a single GPU card, we represent the communication in FL by calculating the number of iterations and the program running time for each compared method to achieve the targeted testing accuracy."
            ]
        ]
    },
    "S5.T4": {
        "caption": "TABLE IV: Running time (seconds) to target testing accuracy.",
        "table": "",
        "footnotes": "\n\n\n\n\n\n\nFMNIST\nCIFAR-10\nCIFAR-100\n\n\nSingle\nTotal\nSingle\nTotal\nSingle\nTotal\n\nFedLGA\n9.4\n565.8\n12.1\n2668.6\n11.8\n1711.0\n\nFedAvg\n8.9\n1032.4\n10.7\n3741.5\n11.3\n3130.1\n\nFedProx\n12.2\n1171.7\n13.4\n3932.1\n12.9\n2747.7\n\nFedNova\n9.1\n910.0\n10.9\n3640.6\n11.6\n3120.4\n\nScaffold\n11.2\n806.7\n13.1\n3636.2\n12.4\n2287.8\n\nFedDyn\n12.2\n869.3\n12.8\n3251.2\n12.7\n2057.4\n\n\n\n",
        "references": [
            "Running Time: Table IV shows the experimental result of the running time (seconds) for each compared method to achieve the target testing accuracy. Note that to describe the performance accurately, we take both the “Single” and “Total” cost time into consideration. The “Single” represents the averaged time for running one global communication round during the training process, and the “Total” is the total required running time for a compared method to reach the targeted testing accuracy. We can notice that FedLGA reaches the best “total” running time for all of the three introduced dataset, while only the third-best on the “single” running time. We consider this might be because of the following reasons. Compared to FedAvg and FedNova which reach better “single” running time, the proposed FedLGA algorithm requires a lower number of global communication round to the target accuracy. And comparing to FedProx, Scaffold and FedDyn, the results support our theoretical claim that as the extra computation complexity of the proposed FedLGA is on the aggregator, it outperforms other FL methods which perform extra computation costs on the remote devices."
        ]
    },
    "S5.T5": {
        "caption": "TABLE V: Impact of τm​a​xsubscript𝜏𝑚𝑎𝑥\\tau_{max}. ",
        "table": "",
        "footnotes": "\n\n\n\n\n\n\nτm​a​xsubscript𝜏𝑚𝑎𝑥\\tau_{max}\nFMNIST\nCIFAR-10\nCIFAR-100\n\nFedLGA\n0.8​E0.8𝐸0.8E\n60\n220\n145\n\n0.6​E0.6𝐸0.6E\n54\n186\n140\n\n0.4​E0.4𝐸0.4E\n41\n157\n126\n\n\n0.2​E0.2𝐸0.2E\n29\n109\n122\n\nFedAvg\n-\n116\n350\n277\n\nFedProx\n-\n96\n293\n213\n\nFedNova\n-\n100\n334\n269\n\nScaffold\n-\n72\n278\n185\n\nFedDyn\n-\n71\n254\n162\n\n\n",
        "references": [
            "Impact of τm​a​xsubscript𝜏𝑚𝑎𝑥\\tau_{max}: We then evaluate the performance of the proposed FedLGA algorithm under further settings of the introduced hyper-parameters in this paper. The required communication rounds of FedLGA to achieve the target testing accuracy on the introduced dataset with different τm​a​xsubscript𝜏𝑚𝑎𝑥\\tau_{max} values are shown in Table V. Note that for better presentation, the performance of the compared FL methods is also introduced in the table. We can notice from the results that on each considered value of τm​a​xsubscript𝜏𝑚𝑎𝑥\\tau_{max}, FedLGA outperforms the compared FL methods. In addition, as τm​a​xsubscript𝜏𝑚𝑎𝑥\\tau_{max} becomes larger, the performance of FedLGA degrades. We consider that this is due to the reason that when τm​a​xsubscript𝜏𝑚𝑎𝑥\\tau_{max} is smaller, the variance of the obtained local model update approximation in FedLGA becomes larger. This may also indicate that the performance of FedLGA is also related to E−Ei𝐸subscript𝐸𝑖E-E_{i}. Specifically, when E−Ei𝐸subscript𝐸𝑖E-E_{i} becomes larger (i.e., the FL network is with higher device-heterogeneity), the performance of FedLGA is more limited."
        ]
    }
}