{
    "id_table_1": {
        "caption": "Table 1.  Datasets, including generative models included, release date, image format, authentic or synthetic, and number of samples within train, validation and test.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "To focus on the properties of synthetic images, we will fix the detector architecture used in all our experiments ( 3.1 ). In contrast, we consider a variety of recent synthetic image datasets, and by extension of AI image generators, as this is the real target of our work (see  3.2  and  3.3 ). Finally, we report computation details, enabling full reproducibility of our work, in  3.4 .",
            "In this work we fix the architecture, to focus on the evolution, nature and practical use of generalization models. For our experimentation, we choose a ResNet (He et al . ,  2016 ) , as this has shown competitive results  (Zhu et al . ,  2024 ; Grommelt et al . ,  2024 ; Cazenavette et al . ,  2024 ) , and its a relatively lightweight architecture. In particular, we follow the staircase design proposed in  (Lopez Cuena,  2023 )  with a ResNet-18 backbone (12.7M parameters), which feeds features extracted at different depths into an MLP (see Figure  1 ).",
            "In  4 , during the training stages of this work,  COCO  and  dif - fusiondb  datasets are undersampled to include a maximum of 5,435 samples. Existing train, validation and test partitions are respected. When undefined a 60%-20%-20% random split is performed. For the  SDXL  dataset we include the  realistic-2.2  split for training and validation, and the  realistic-1  split for test. Table  1  shows the exact composition per data source.",
            "The datasets in this section will assess the detectors generalization ability when trained on limited data and applied to diverse external sources (  5.1 ). The evaluation aims to cover three scenarios: datasets from the same model used in training but generated by different users or slightly different software or model versions; datasets from entirely different models; and samples from various unknown sources and models, simulating real-world applications. Examples of these datasets are provided in Appendix  B , with the resolution distribution of each dataset detailed in Appendix  6 .",
            "The software stack used to develop and evaluate SID models was based on PyTorch. To enable the full reproducibility of our work, we share the code needed to run the experiments  1 1 1 https://github.com/HPAI-BSC/SuSy , the datasets as compiled and cleaned  2 2 2 https://huggingface.co/datasets/HPAI-BSC/SuSy-Dataset , and the model weights  3 3 3 https://huggingface.co/HPAI-BSC/SuSy  for our best detector (see  4.3.1 ).",
            "To study the relation between the images produced using the latest image generation models, let us consider the generalization capacity of single-class discriminative models trained on those images. In this section we train binary classifiers, using each synthetic dataset of Table  1  as a positive class, and the  COCO  dataset as a negative class. Those models are then tested on the remaining datasets, to see how they generalize. Single-class models are trained for a maximum of 20 epochs with an early-stopping mechanism monitoring validation accuracy with patience of 2. Data augmentation is limited to horizontal flips with a 50% probability, with further transformation studies left for   4.3 .",
            "We train two models to evaluate the effect of multi-class learning on generalization capacity. First, we train a binary classifier merging all synthetic data sources from Table  1  into a single synthetic class, including 14,323 images. An analogous amount of samples is drawn from the  COCO  dataset to compose the authentic class. Then, we train a six-class recognition model using the original splits defined in Table  1 . To enable a direct comparison, in this experiment, a prediction of a multiclass model will be considered correct for a synthetic image if any synthetic class is predicted, regardless of whether it matches the ground truth class.",
            "In the following section (  5 ), generalization is explored by evaluating different data sources. To scale experimentation, we fix a model based on the insights from this section, which we call  SuSy . In detail, we train a six-class classifier, using the original splits defined in Table  1  and incorporating the alteration methods explored in the previous section to enhance its resilience against image manipulations. Each transformation is applied with a 20% probability, besides the horizontal flip which remains at 50%, allowing the model to see unaltered images and images that have suffered one or multiple transformations. We train this model for a maximum of 20 epochs with early stopping monitoring validation loss with patience 2, as in previous experiments.",
            "In this situation, the biggest challenge for synthetic image detectors is the adaptation to a changing and uncontrolled generative environment. To test generalization in this context, we explore the role of data source, using the datasets described in  3.3 . First, this section considers the performance of  SuSy  (described in  4 ), using the central patch of images. Then, the same experiments are conducted by making predictions at image level, using the patching strategy defined in   3.1 .",
            "The second set of data sources considered include datasets produced using models that were also used during the training of  SuSy , listed in Table  1 . Even though the underlying generative models are the same, differences in the generation process ( e.g.,  prompts, generator hyperparameters,  etc.  ) can induce significant biases. In this case, generalization holds significantly well, with all datasets reaching a recall between 73% and 89%.",
            "While our initial experiments focused solely on the central patch of each sample, a prediction at image level is likely to be of interest in real-world scenarios. In this section, we explore the transition from patch-level to image-level predictions, and how this affects generalization. To achieve this, we employ the patch selection method described in  3.1 , selecting five patches and using a fixed threshold (minimum number of synthetic patches) to classify an entire image as synthetic."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  On each row, patch-level recall of a single-class discriminative model when evaluating all datasets. In bold, performance on the dataset used for training.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "To focus on the properties of synthetic images, we will fix the detector architecture used in all our experiments ( 3.1 ). In contrast, we consider a variety of recent synthetic image datasets, and by extension of AI image generators, as this is the real target of our work (see  3.2  and  3.3 ). Finally, we report computation details, enabling full reproducibility of our work, in  3.4 .",
            "To address these challenges, most detectors are trained on image patches. This introduces new variables in the detection process: the number of patches and their selection strategy, as well as the criterion to convert single-patch predictions to whole image ones. To determine our patch selection strategy, we conduct early experiments and find that patches which exhibit the highest contrast in their grey-level co-occurrence matrix (GLCM) produce slightly more performant models. Due to the size of our input images, we consider a maximum of five  224  224 224 224 224\\times 224 224  224  patches. The predictions of these individual patches can then be aggregated according to the use case requirements ( e.g.,  maximizing the detection of false positives, or prioritizing false negatives instead). In particular, we select a voting mechanism and set a minimum amount of patches for an image to be considered synthetic. We will study the effect of the value of this threshold in  5.2 .",
            "The results from the single-class model evaluations, shown in Table  2 , reveal a discrepancy between performance in academic settings and practical applicability. While all models achieved high recall (over 97%) on their respective binary tasks, their performance degraded significantly when applied to other datasets.",
            "On the other hand, the time of release of the generative model correlates well with generalized performance. The models trained on  DALLE3  and  MJ 5/6  strongly generalize to each other. While this may be influenced by shared dataset authorship, both models also generalize well to  SDXL , demonstrating accurate performance on recent models regardless of their provenance. In fact, the three discriminators trained on models from 2023 are the top three models on average. That is,  newer models are good at detecting new and old synthetic images . This could be caused by the quality and the lack of artifacts in the images generated by the latest models. In which case it would hold that  images generated by older models are harder to identify than images from newer models . The bottom row of Table  2 , shows this is the case, with detection accuracy on older datasets performing worse of all ( SD1.X  and  MJ 1/2 , from 2022).",
            "The biggest challenge for detectors is generalization. Changes of generative model, data source, image post-processing and training task, affect detector performance, limiting their reliability. This work studied all these factors, starting with the underlying generative model used to produce the images. By temporally grounding our experiments, we observe how older models/datasets (less realistic and producing bigger artifacts) can be easily detected when trained for them, but are harder to generalize to when not seen during training (when compared to more recent generative models, see Table  2 ). This indicates the stronger biases defining older synthetic images are rarely found in newer datasets. As a result, including early generative models in the train set of new detector models should become common practice. Another consistent family of generative models that is consistently harder to generalize to are private models ( e.g.,  DALLE3 ,  Firefly ), as shown in Tables  2 ,  5  and  7 . This speaks of the importance of open science in the field.",
            "The race between generators and detectors will go on, particularly since data from better generative models produces detectors that generalize better (see Table  2 ), while better detectors are likely to be used to improve generators. This leads to a race equilibrium paradox, ensuring that the race for synthetic content detection is always going to be a close one."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  On each row, patch level recall for different models. On columns, recall for the authentic class, and each synthetic class in a binary setting.  Single : Trained on the dataset of each column (see Table  2  diagonal).  Binary : Trained with all synthetic datasets merged into one.  Six-class : Multi-class recognition model, trained to discriminate all six datasets. In bold best performance per dataset.",
        "table": "S4.T3.1",
        "footnotes": [
            ""
        ],
        "references": [
            "To focus on the properties of synthetic images, we will fix the detector architecture used in all our experiments ( 3.1 ). In contrast, we consider a variety of recent synthetic image datasets, and by extension of AI image generators, as this is the real target of our work (see  3.2  and  3.3 ). Finally, we report computation details, enabling full reproducibility of our work, in  3.4 .",
            "The software stack used to develop and evaluate SID models was based on PyTorch. To enable the full reproducibility of our work, we share the code needed to run the experiments  1 1 1 https://github.com/HPAI-BSC/SuSy , the datasets as compiled and cleaned  2 2 2 https://huggingface.co/datasets/HPAI-BSC/SuSy-Dataset , and the model weights  3 3 3 https://huggingface.co/HPAI-BSC/SuSy  for our best detector (see  4.3.1 ).",
            "To study the relation between the images produced using the latest image generation models, let us consider the generalization capacity of single-class discriminative models trained on those images. In this section we train binary classifiers, using each synthetic dataset of Table  1  as a positive class, and the  COCO  dataset as a negative class. Those models are then tested on the remaining datasets, to see how they generalize. Single-class models are trained for a maximum of 20 epochs with an early-stopping mechanism monitoring validation accuracy with patience of 2. Data augmentation is limited to horizontal flips with a 50% probability, with further transformation studies left for   4.3 .",
            "Results in Table  3  show that, when trained on multiple data sources, both the classifier and the recognition perform equally well, outperforming the models trained on a single dataset (top row). This indicates that a certain amount of useful features are shared among generators, and these are better characterized in conjunction. The results of the six-class model can be attributed to the training in the harder multi-class classification task, which enriches the models understanding of the feature space, enhancing its ability to distinguish between classes, even in a binary context.",
            "In this situation, the biggest challenge for synthetic image detectors is the adaptation to a changing and uncontrolled generative environment. To test generalization in this context, we explore the role of data source, using the datasets described in  3.3 . First, this section considers the performance of  SuSy  (described in  4 ), using the central patch of images. Then, the same experiments are conducted by making predictions at image level, using the patching strategy defined in   3.1 .",
            "Let us explore the performance of  SuSy  when evaluating it on the datasets described in  3.3 . Table  5  shows the results, sorted by category. For the authentic datasets, reported at the top part of the table,  SuSy  generalizes well to the  Flickr30k  dataset and decently to  Google Landmarks v2 , but suffers a large drop in performance for  In-the-wild  images. This could be caused by changes in the scale of images, differences in the domain of information represented in the pictures and a higher amount of image postprocessing, and could potentially be mitigated by using a richer authentic class that combines images from different sources in the training dataset.",
            "While our initial experiments focused solely on the central patch of each sample, a prediction at image level is likely to be of interest in real-world scenarios. In this section, we explore the transition from patch-level to image-level predictions, and how this affects generalization. To achieve this, we employ the patch selection method described in  3.1 , selecting five patches and using a fixed threshold (minimum number of synthetic patches) to classify an entire image as synthetic.",
            "In this section, we study the performance of state-of-the-art models on the evaluation datasets listed in   3.3 . We analyzed ten different models available in SIDBench  (Schinas and Papadopoulos,  2024 ) , focusing on the six that demonstrated superior performance across our tests. The results for the four underperforming models (CNNDetect  (Wang et al . ,  2020 ) , FreqDetect  (Frank et al . ,  2020 ) , Fusing  (Ju et al . ,  2022 )  and UnivFD  (Ojha et al . ,  2023 ) ) are presented in Appendix  A  for completeness. Table  7  showcases the performance of the best models, LGrad  (Tan et al . ,  2023 ) , GramNet  (Liu et al . ,  2020 ) , Rine  (Koutlis and Papadopoulos,  2024 ) , DIMD  (Koutlis and Papadopoulos,  2024 ) , DeFake  (Corvi et al . ,  2023 )  and Dire  (Sha et al . ,  2023 ) .",
            "A common pitfall of detector models is the lack of a proper characterization of  authentic  images. This has a higher prevalence on binary classifiers (Table  3 ) but also happens on multi-class ones (Table  5 ). Since characterizing authentic images is essential for practical use, specific tests using different sources of authentic data should be prioritized. Training on authentic data from many sources should also be common practice."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  On each row, patch-level accuracy of a six-class recognition model when trained on one alteration method and evaluated on all. In bold, performance on the alteration used for training. Last column shows the average performance models across all transformations. Bottom row shows the average performance of all models for each transformation.",
        "table": "S4.T4.2",
        "footnotes": [],
        "references": [
            "To focus on the properties of synthetic images, we will fix the detector architecture used in all our experiments ( 3.1 ). In contrast, we consider a variety of recent synthetic image datasets, and by extension of AI image generators, as this is the real target of our work (see  3.2  and  3.3 ). Finally, we report computation details, enabling full reproducibility of our work, in  3.4 .",
            "To train the synthetic content detectors we consider two types of datasets. The first contains real-world images, that is photographs of different scenes taken under different light conditions. We use  COCO   (Lin et al . ,  2015 )  for training and validation ( 4 ). The second type of datasets considered are those composed of synthetic images. That is, AI-generated images. Five datasets are used, created using five different AI image generators:  dalle3-images ,  diffusiondb ,  SDXL ,  mj-tti  and  mj-images .",
            "Beyond DALL-E and Stable Diffusion, the third main provider of synthetic images is Midjourney. Its early iterations, the V1 and V2 models, date from early 2022, and were used to populate the  mj-tti   (Turc and Nemade,  2022 )  dataset, which contains 4,530 images. Collage images and mosaics made of synthetic images were removed from this dataset. Later models, the V5 and V6 models from 2023 compose our last training dataset,  mj-images   (ehristoforu,  2024b ) , with 1,226 images. This dataset also had to be deduplicated. All these synthetic models will be used for training and validation in  4 .",
            "In  4 , during the training stages of this work,  COCO  and  dif - fusiondb  datasets are undersampled to include a maximum of 5,435 samples. Existing train, validation and test partitions are respected. When undefined a 60%-20%-20% random split is performed. For the  SDXL  dataset we include the  realistic-2.2  split for training and validation, and the  realistic-1  split for test. Table  1  shows the exact composition per data source.",
            "The software stack used to develop and evaluate SID models was based on PyTorch. To enable the full reproducibility of our work, we share the code needed to run the experiments  1 1 1 https://github.com/HPAI-BSC/SuSy , the datasets as compiled and cleaned  2 2 2 https://huggingface.co/datasets/HPAI-BSC/SuSy-Dataset , and the model weights  3 3 3 https://huggingface.co/HPAI-BSC/SuSy  for our best detector (see  4.3.1 ).",
            "To study the relation between the images produced using the latest image generation models, let us consider the generalization capacity of single-class discriminative models trained on those images. In this section we train binary classifiers, using each synthetic dataset of Table  1  as a positive class, and the  COCO  dataset as a negative class. Those models are then tested on the remaining datasets, to see how they generalize. Single-class models are trained for a maximum of 20 epochs with an early-stopping mechanism monitoring validation accuracy with patience of 2. Data augmentation is limited to horizontal flips with a 50% probability, with further transformation studies left for   4.3 .",
            "In addition to the baseline with no alterations, five additional models are trained (one for each alteration), and five new test sets are created (also one per alteration). With these, cross-testing is conducted to explore generalization across alteration methods. The results, how well a model trained with one alteration detects images subjected to another alteration, are shown in Table  4 . Multi-class macro average recall is reported, where a confusion between synthetic classes is considered an error, unlike in the previous section where we used binary metrics for comparison.",
            "The effectiveness of targeted data augmentation can be seen by comparing the first row of Table  4  with the values in bold. Specialized models demonstrate a significant performance boost over the baseline across most alterations, with the exception of JPEG compression. This anomaly may be attributed to the inclusion of JPEG data in the baseline training set. Despite this apparent performance drop, training with compressed images remains relevant to prevent the recognition of JPEG-introduced patterns as class-specific artifacts, as both authentic and synthetic images may undergo this transformation in real-world scenarios.",
            "Among the various transformations, blur particularly GaussianBlur has the most pronounced impact on detector performance. Models not specifically trained on blur transformations experience a substantial drop in recall, up to 30%, when faced with blurred images. In contrast, models trained on blur transformations exhibit remarkable consistency and competitive performance across all experiments. Based on the average performance metrics presented in the last column of Table  4 , these blur-trained models emerge as the overall best performers among the alteration-based models.",
            "In this situation, the biggest challenge for synthetic image detectors is the adaptation to a changing and uncontrolled generative environment. To test generalization in this context, we explore the role of data source, using the datasets described in  3.3 . First, this section considers the performance of  SuSy  (described in  4 ), using the central patch of images. Then, the same experiments are conducted by making predictions at image level, using the patching strategy defined in   3.1 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Recall of  SuSy  on authentic images (top), from unseen datasets. In the middle, recall on synthetic datasets generated by generative models represented in the train data, but produced by alternative sources. Bottom, synthetic image datasets generated by models not seen before. Performance at patch and image level using majority voting (threshold of three out of five).",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "To address these challenges, most detectors are trained on image patches. This introduces new variables in the detection process: the number of patches and their selection strategy, as well as the criterion to convert single-patch predictions to whole image ones. To determine our patch selection strategy, we conduct early experiments and find that patches which exhibit the highest contrast in their grey-level co-occurrence matrix (GLCM) produce slightly more performant models. Due to the size of our input images, we consider a maximum of five  224  224 224 224 224\\times 224 224  224  patches. The predictions of these individual patches can then be aggregated according to the use case requirements ( e.g.,  maximizing the detection of false positives, or prioritizing false negatives instead). In particular, we select a voting mechanism and set a minimum amount of patches for an image to be considered synthetic. We will study the effect of the value of this threshold in  5.2 .",
            "The datasets in this section will assess the detectors generalization ability when trained on limited data and applied to diverse external sources (  5.1 ). The evaluation aims to cover three scenarios: datasets from the same model used in training but generated by different users or slightly different software or model versions; datasets from entirely different models; and samples from various unknown sources and models, simulating real-world applications. Examples of these datasets are provided in Appendix  B , with the resolution distribution of each dataset detailed in Appendix  6 .",
            "In the following section (  5 ), generalization is explored by evaluating different data sources. To scale experimentation, we fix a model based on the insights from this section, which we call  SuSy . In detail, we train a six-class classifier, using the original splits defined in Table  1  and incorporating the alteration methods explored in the previous section to enhance its resilience against image manipulations. Each transformation is applied with a 20% probability, besides the horizontal flip which remains at 50%, allowing the model to see unaltered images and images that have suffered one or multiple transformations. We train this model for a maximum of 20 epochs with early stopping monitoring validation loss with patience 2, as in previous experiments.",
            "Let us explore the performance of  SuSy  when evaluating it on the datasets described in  3.3 . Table  5  shows the results, sorted by category. For the authentic datasets, reported at the top part of the table,  SuSy  generalizes well to the  Flickr30k  dataset and decently to  Google Landmarks v2 , but suffers a large drop in performance for  In-the-wild  images. This could be caused by changes in the scale of images, differences in the domain of information represented in the pictures and a higher amount of image postprocessing, and could potentially be mitigated by using a richer authentic class that combines images from different sources in the training dataset.",
            "By default, we set the threshold at three out of five patches, which equates to majority voting. This approach ensures a balanced performance between authentic and synthetic classes. In the case of  GLIDE , due to image size constraints, we adjust the requirement to two out of three patches for a synthetic prediction. The results of this image-level analysis are presented in Table  5 , showing consistent improvements across all datasets. We observe performance increases of up to 11% of recall, with an average improvement of 3.4% across datasets.",
            "Optimizing the threshold for a specific domain or dataset can be done using a small subset of data, adjusting the detectors sensitivity to the frequency of artifacts found in a specific domain. For instance, in the case of the  In-the-wild  dataset, Table  5  shows a high recall in the synthetic class (89.9% and 92.9%) but also a large number of false positives in the authentic class (33.0% and 33.8%). To address this, the threshold can be increased ( e.g.,  requiring all five patches to indicate a synthetic prediction for the entire image to be classified as such). As shown in Table  6 , this setup significantly improves the models balance between authentic and synthetic image classification, increasing authentic recall by 31% while decreasing synthetic recall by 20% points. Notably, this adjustment results in a performance comparable to that of the average human evaluator.",
            "The biggest challenge for detectors is generalization. Changes of generative model, data source, image post-processing and training task, affect detector performance, limiting their reliability. This work studied all these factors, starting with the underlying generative model used to produce the images. By temporally grounding our experiments, we observe how older models/datasets (less realistic and producing bigger artifacts) can be easily detected when trained for them, but are harder to generalize to when not seen during training (when compared to more recent generative models, see Table  2 ). This indicates the stronger biases defining older synthetic images are rarely found in newer datasets. As a result, including early generative models in the train set of new detector models should become common practice. Another consistent family of generative models that is consistently harder to generalize to are private models ( e.g.,  DALLE3 ,  Firefly ), as shown in Tables  2 ,  5  and  7 . This speaks of the importance of open science in the field.",
            "Data source is another key factor in detection. Using the same generative model, individuals employing various software and hardware configurations can produce significantly different content, to the point where generalization to the same model but a different source often becomes challenging (see Tables  5  and  7 ). As a result, detector models can never be assumed to work in data obtained from an untested source. This is further reinforced by tests conducted on data gathered  In-the-wild , showing the dangerous effect of uncontrolled data sources. To mitigate this risk, recurrent sanity checks are recommended, as well as dataset-specific fine-tunes.",
            "A common pitfall of detector models is the lack of a proper characterization of  authentic  images. This has a higher prevalence on binary classifiers (Table  3 ) but also happens on multi-class ones (Table  5 ). Since characterizing authentic images is essential for practical use, specific tests using different sources of authentic data should be prioritized. Training on authentic data from many sources should also be common practice.",
            "To finalize, let us consider the ethical risks associated with this work and with the materials released. The most obvious of which is the fallibility of the detectors trained. The  SuSy , as any other detector model, makes both false positive and negative predictions (see Table  5 ), potentially labelling authentic images as synthetic and vice versa. Digital rights could be affected and censorship could be enabled through its use. For this reason, when applied in a setting affecting the rights and privileges of humans, the model should always be overseen by a human expert, and its predictions should never be taken as conclusive evidence."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  Recall of the  In-the-wild  dataset by  SuSy  and human evaluators. For  SuSy , performance for central patch, and at image level when requiring five positive detections on the five patches with maximum contrast.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "The datasets in this section will assess the detectors generalization ability when trained on limited data and applied to diverse external sources (  5.1 ). The evaluation aims to cover three scenarios: datasets from the same model used in training but generated by different users or slightly different software or model versions; datasets from entirely different models; and samples from various unknown sources and models, simulating real-world applications. Examples of these datasets are provided in Appendix  B , with the resolution distribution of each dataset detailed in Appendix  6 .",
            "For the benchmarking of other detectors (  6 ), we use the  sidbench  library  (Schinas and Papadopoulos,  2024 ) , which supports several models while facilitating inference on custom datasets. Available checkpoints were downloaded from the AIGCDetectBenchmark  (Zhong et al . ,  2023 )  repository. We download the missing models from their corresponding official sources.",
            "Additionally, to confirm the validity and difficulty of the  In-the - wild  dataset, we conduct a small human experiment. We ask 10 volunteers aged 22-30 who have social media accounts and are potential targets of AI-generated deception, to discriminate between both  In-the-wild  versions (authentic and synthetic). To ensure unbiased results, images were presented in random order and the evaluators were not informed about the distribution of the data. Participants took an average of 15 minutes to label them, with no time constraints imposed. Results are reported in Table  6  and used as a baseline.",
            "Regardless of its blind spots,  SuSy  performs significantly well in the most realistic setting, the one using  In-the-wild  images collected from unknown sources and manually selected based on their quality, aligning with the performance on the latest and most realistic models (SD3 and Flux). A comparison of performance on this task with human evaluators is shown in Table  6 . Results indicate synthetic image detectors trained on varied sets of data, like  SuSy , are already at the level of expert humans, making them useful tools in practice (as long as false positive rates are contained).",
            "Optimizing the threshold for a specific domain or dataset can be done using a small subset of data, adjusting the detectors sensitivity to the frequency of artifacts found in a specific domain. For instance, in the case of the  In-the-wild  dataset, Table  5  shows a high recall in the synthetic class (89.9% and 92.9%) but also a large number of false positives in the authentic class (33.0% and 33.8%). To address this, the threshold can be increased ( e.g.,  requiring all five patches to indicate a synthetic prediction for the entire image to be classified as such). As shown in Table  6 , this setup significantly improves the models balance between authentic and synthetic image classification, increasing authentic recall by 31% while decreasing synthetic recall by 20% points. Notably, this adjustment results in a performance comparable to that of the average human evaluator.",
            "The race between synthetic image generators and detectors is far from over. New and better generative models appear regularly, and detectors cannot generalize to all (see  6 ). Meanwhile, the increasing realism of synthetically generated content brings along a proportional demand for detectors, as tools for protecting social trust and digital rights, while preventing disinformation.",
            "This work shows how to train detectors that maximize generalization, by focusing on many different data sources and generative models. This alone is no guarantee, and diversity, even within the same source or model, is of essence. Our limited field study indicates that, while we are far from a universal detector, models trained for specific targets may already be as good as humans at identifying synthetic content (see Table  6 ).",
            "We calculate the resolution distribution for each of the evaluation dataset.  Figure 6  contains information regarding the size and aspect ratio of the datasets used. Top plot shows the width distribution of all samples, dataset-wise. The bottom plot shows the same for height."
        ]
    },
    "id_table_7": {
        "caption": "Table 7.  Center-patch recall of state-of-the-art detector models across evaluation datasets. On top, performance for authentic images. At bottom, performance on synthetic datasets. Recalls below 50%, akin to random chance, in bold. Best recall for each dataset is underlined.  Average  shows aggregated center-patch performance across datasets of the same class (auth. or synth.).  Average (Resize)  shows the same with images resized instead of cropped (see  6.2 ), together with the increment or decrement  w.r.t.  the patched approach.",
        "table": "S6.T7.1",
        "footnotes": [
            ""
        ],
        "references": [
            "In this section, we study the performance of state-of-the-art models on the evaluation datasets listed in   3.3 . We analyzed ten different models available in SIDBench  (Schinas and Papadopoulos,  2024 ) , focusing on the six that demonstrated superior performance across our tests. The results for the four underperforming models (CNNDetect  (Wang et al . ,  2020 ) , FreqDetect  (Frank et al . ,  2020 ) , Fusing  (Ju et al . ,  2022 )  and UnivFD  (Ojha et al . ,  2023 ) ) are presented in Appendix  A  for completeness. Table  7  showcases the performance of the best models, LGrad  (Tan et al . ,  2023 ) , GramNet  (Liu et al . ,  2020 ) , Rine  (Koutlis and Papadopoulos,  2024 ) , DIMD  (Koutlis and Papadopoulos,  2024 ) , DeFake  (Corvi et al . ,  2023 )  and Dire  (Sha et al . ,  2023 ) .",
            "Overall, results in Table  7  indicate the lack of universal detectors, with the performance of all methods failing at random classifier level in six or more datasets. Performance inconsistency is generalized, with all models being the best and worst detectors for at least one dataset.",
            "Focusing on the models with a reasonable amount of false positives (LGrad to DIMD in Table  7 ) still leads to contractive insights. At times the underlying generative model seems relevant for detection (variants of SD models are identified at higher rates, even for models not trained on diffusion) while other cases show discrepancies between datasets generated using the same model ( DALLE3 ,  SDXL ). This evidence suggests a complex relationship between training data, data sources and detection efficacy.",
            "In our final generalization study, rather than cropping, we resize all images to  224  224 224 224 224\\times 224 224  224  pixels regardless of the original size. This approach notably impacts the frequency components of the images, as well as the content itself, particularly for high-resolution originals. The resizing process can alter or eliminate certain frequency artifacts and defects that some SID models rely on. Aggregated results are shown in Table  7  ( Resize  row). See Appendix  A  for further details.",
            "The biggest challenge for detectors is generalization. Changes of generative model, data source, image post-processing and training task, affect detector performance, limiting their reliability. This work studied all these factors, starting with the underlying generative model used to produce the images. By temporally grounding our experiments, we observe how older models/datasets (less realistic and producing bigger artifacts) can be easily detected when trained for them, but are harder to generalize to when not seen during training (when compared to more recent generative models, see Table  2 ). This indicates the stronger biases defining older synthetic images are rarely found in newer datasets. As a result, including early generative models in the train set of new detector models should become common practice. Another consistent family of generative models that is consistently harder to generalize to are private models ( e.g.,  DALLE3 ,  Firefly ), as shown in Tables  2 ,  5  and  7 . This speaks of the importance of open science in the field.",
            "Data source is another key factor in detection. Using the same generative model, individuals employing various software and hardware configurations can produce significantly different content, to the point where generalization to the same model but a different source often becomes challenging (see Tables  5  and  7 ). As a result, detector models can never be assumed to work in data obtained from an untested source. This is further reinforced by tests conducted on data gathered  In-the-wild , showing the dangerous effect of uncontrolled data sources. To mitigate this risk, recurrent sanity checks are recommended, as well as dataset-specific fine-tunes."
        ]
    },
    "id_table_8": {
        "caption": "Table 8.  Recall of four additional detection methods when applied to the evaluation datasets.",
        "table": "A1.T8.1",
        "footnotes": [],
        "references": [
            "Table  8  summarizes the recall rates of four detection methodsCNNDetect, FreqDetect, Fusing, and UnivFDacross various datasets, when applying a center crop of size  224  224 224 224 224\\times 224 224  224  to the input image."
        ]
    },
    "id_table_9": {
        "caption": "Table 9.  Recall of state-of-the-art detector models across evaluation datasets when resizing the input image to  224  224 224 224 224\\times 224 224  224 .",
        "table": "A1.T9.3",
        "footnotes": [],
        "references": []
    }
}