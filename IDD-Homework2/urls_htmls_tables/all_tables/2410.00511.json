{
    "id_table_1": {
        "caption": "TABLE I :    Comparison of transferability of image-based pre-training methods.  SL = Supervised Learning, FDSL = Formula-Driven Supervised Learning, IN-1k = ImageNet1k, exF21k = exFractal21k  [ 35 ]  VA = VisualAtom  [ 16 ] .    indicates results from  [ 4 ] . Note that we are missing 30k data in the balanced and the eval set in AS-20k. Therefore there is a performance gap between ours and ImageNet1k SL  [ 4 ] . However, the gap narrows when the same data is used.",
        "table": "S3.T1.4",
        "footnotes": [
            "",
            "",
            "",
            ""
        ],
        "references": [
            "In this paper, inspired by these methods, we propose to pre-train audio encoders using synthetic patterns, addressing issues related to privacy and licensing during audio pre-training. Our framework (Fig.  1 ) combines two key elements. The first is an MAE, which is trained to reconstruct the whole input from randomly masked counterparts. Since MAEs tend to focus on low-level information like visual patterns and regularities within an input, it is not important to what is portrayed in the input, whether it be real images, real audio mel-spectrograms, even or synthetic patterns. This leads to the second key element, which is synthetic data. Synthetic data, unlike real images and real audio, is free from concerns about privacy and licensing. By combining MAEs and synthetic patterns, our framework enables the model to learn transferable feature representations without real audio, performing well on various audio downstream tasks.",
            "To address privacy and licensing concerns in pre-training audio encoders, we propose pre-training an MAE with synthetic patterns and then transferring it to audio tasks (Fig.  1 ). In this section, We first provide an overview of MAEs (Sec.  II-A ), and then describe the synthetic patterns used in our experiments (Sec.  II-B ). Finally, we explain how to transfer MAEs to audio data (Sec.  II-C ).",
            "The MAE pre-training process (illustrated in the upper half of Fig.  1 ) begins by splitting the input the input  X  R C  H  W X superscript R C H W X\\in\\mathbb{R}^{C\\times H\\times W} italic_X  blackboard_R start_POSTSUPERSCRIPT italic_C  italic_H  italic_W end_POSTSUPERSCRIPT   1 1 1 C C C italic_C  represents the number of channels and  H H H italic_H  and  W W W italic_W  each represent the height and width of the input.   into non-overlapping  P  P P P P\\times P italic_P  italic_P  patches, which are then linearly embedded to form  X p  R N  D subscript X p superscript R N D X_{p}\\in\\mathbb{R}^{N\\times D} italic_X start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N  italic_D end_POSTSUPERSCRIPT , where  N = H  W P 2 N H W superscript P 2 N=\\frac{HW}{P^{2}} italic_N = divide start_ARG italic_H italic_W end_ARG start_ARG italic_P start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG  is the number of patches and  D D D italic_D  is the embedding dimension. Subsequently, a high proportion of patches (e.g., 75%) are randomly masked out using a binary mask, producing visible embeddings. These embeddings are then fed into a transformer-based autoencoder, which is trained to reconstruct the original pixel values using mean squared error as the loss function. After pre-training, the encoder part of the MAE, which is equivalent to a Vision Transformer (ViT), will be fine-tuned for downstream tasks (in the lower part of Fig.  1 )."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II :    Comparison with other existing methods.  IN-1k = ImageNet1k, AS = AudioSet.",
        "table": "S3.T2.8",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "While various types of synthetic patterns can be used for training MAEs, it is challenging to explore effective synthetic patterns for MAEs without any constraints. Therefore, in this study, we focus on synthetic images as one form of synthetic patterns. Specifically, we utilize 17 synthetic image datasets proposed in computer vision, as illustrated in Fig.  2 .",
            "First, we used small-scale synthetic image datasets (labeled a-n in Fig.  2 ) to investigate which types of synthetic images are effective for our framework. We examined the Pearson correlation coefficient  r r r italic_r  between the following four properties of the datasets and their performance on ESC-50 fold5 (Fig.  3 )."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III :    Evaluation on datasets from HEAR and ARCH benchmark.     indicates results from  [ 14 ] . SL = Supervised Learning.",
        "table": "S3.T3.4",
        "footnotes": [
            ""
        ],
        "references": [
            "First, we used small-scale synthetic image datasets (labeled a-n in Fig.  2 ) to investigate which types of synthetic images are effective for our framework. We examined the Pearson correlation coefficient  r r r italic_r  between the following four properties of the datasets and their performance on ESC-50 fold5 (Fig.  3 ).",
            "Color Entropy:  To measure the diversity of colors in images, we calculated color histograms for each image and computed the average entropy of these histograms. Fig.  3(a)  shows that color diversity has little correlation with performance.",
            "Brightness Entropy:  We converted images to grayscale, calculated brightness histograms, and computed their average entropy. We found there is a weak correlation with performance (Fig.  3(b) ).",
            "Edge Density:  Using the Canny edge detector  [ 36 ] , we calculated the average ratio of edge pixels in the images. As shown in Fig.  3(d) , we found that edge density showed less correlation than TV.",
            "CIFAR100 Performance:  We also evaluate each model on the image classification task with CIFAR100  [ 37 ] . Note that when we fine-tune the MAE pre-trained on AudioSet2M, we inflate the weights of patch embedding along the channel dimension to handle RGB images. As shown in Fig.  3(e) , models that perform well on CIFAR100 also tend to perform well on ESC50, and vice versa ( r = 0.5 r 0.5 r=0.5 italic_r = 0.5 ). Notably, the model pre-trained on AudioSet2M struggles with the image classification tasks. This suggests that it is difficult to transfer audio models to image tasks, while image models tend to learn feature representations that are transferable to audio tasks.",
            "Table  I  shows the results. Although supervised learning on ImageNet1k achieves superior performance, it requires real images and high-quality annotations. In contrast, our framework with Shaders1k enables effective pre-training solely with synthetic images, thereby alleviating privacy and licensing issues. Notably, the MAE pre-trained on Shaders1k demonstrates higher performance than those pre-trained on ImageNet1k. As observed in our previous experiments (Fig.  3 ), pre-training with FractalDB1k and VisualAtom1k also fails to transfer effectively to audio tagging on AudioSet-20k and environment sound classification on ESC50, despite their high performance on ImageNet1k. This indicates that models pre-trained by MAE have higher transferability than models pre-trained by FDSL. Based on these results, we will use Shaders1k for pre-training in our framework for subsequent experiments."
        ]
    }
}