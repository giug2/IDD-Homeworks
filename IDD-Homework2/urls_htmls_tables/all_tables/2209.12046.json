{
    "PAPER'S NUMBER OF TABLES": 2,
    "S2.T1": {
        "caption": "Table 1. Comparison between Blinder and prior work on machine learning-based privacy protection.\n",
        "table": "<table id=\"S2.T1.2.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S2.T1.2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Related Work</span></th>\n<th id=\"S2.T1.2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">End-to-end Privacy</th>\n<th id=\"S2.T1.2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Compatible w/ Legacy Apps</th>\n<th id=\"S2.T1.2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Require Hardware</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.2.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Privacy-Preserving Feature ExtractionÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Li etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">2021a</a>; Liu etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib58\" title=\"\" class=\"ltx_ref\">2019</a>; Li etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">2020a</a>; Hamm, <a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">2017</a>; Zhao etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</td>\n<td id=\"S2.T1.2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ—</td>\n<td id=\"S2.T1.2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ—</td>\n<td id=\"S2.T1.2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ—</td>\n</tr>\n<tr id=\"S2.T1.2.1.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Format-Preserving Data AnonymizationÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Raval etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib67\" title=\"\" class=\"ltx_ref\">2019</a>; Hajihassnai etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib36\" title=\"\" class=\"ltx_ref\">2021</a>; Malekzadeh etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib60\" title=\"\" class=\"ltx_ref\">2019</a>; Bertran etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib13\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"S2.T1.2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ—</td>\n<td id=\"S2.T1.2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ“</td>\n<td id=\"S2.T1.2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ—</td>\n</tr>\n<tr id=\"S2.T1.2.1.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Privacy-Preserving Federated LearningÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Truex etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib73\" title=\"\" class=\"ltx_ref\">2019</a>; Xu etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib76\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"S2.T1.2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ“</td>\n<td id=\"S2.T1.2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ—</td>\n<td id=\"S2.T1.2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ—</td>\n</tr>\n<tr id=\"S2.T1.2.1.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">Privacy-Preserving Federated Learning in TEEÂ <cite class=\"ltx_cite ltx_citemacro_citep\">(Mo etÂ al<span class=\"ltx_text\">.</span>, <a href=\"#bib.bib62\" title=\"\" class=\"ltx_ref\">2021</a>)</cite>\n</td>\n<td id=\"S2.T1.2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ“</td>\n<td id=\"S2.T1.2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ—</td>\n<td id=\"S2.T1.2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">âœ“</td>\n</tr>\n<tr id=\"S2.T1.2.1.6.5\" class=\"ltx_tr\">\n<td id=\"S2.T1.2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Blinder (Our Approach)</td>\n<td id=\"S2.T1.2.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">âœ“</td>\n<td id=\"S2.T1.2.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">âœ“</td>\n<td id=\"S2.T1.2.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">âœ—</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Distributed privacy-preserving techniques eliminate the need for\nsharing (potentially private) user data with a service provider\nby taking advantage of local computation.\nThere have been substantial efforts in recent yearsÂ (Truex etÂ al., 2019; Xu etÂ al., 2019)\nto utilize federated learning (FL) to avoid sharing private user data with untrusted servers during model training.\nHowever, these papers assume that application developers\nwill redesign the application to fit their federated learning framework.\nMoÂ etÂ al.Â (Mo etÂ al., 2021) study the privacy risks in FL frameworks\nand propose deploying FL onto Trusted Execution Environments (TEE)\nto preserve privacy and defend against data reconstruction attacks with a small system overhead.\nWhile TEEs can become prevalent in the future,\nwe take a different approach and\ndevelop a privacy-preserving framework that does not require additional hardware.\nTIPRDCÂ (Li etÂ al., 2020a) is a privacy-preserving data crowdsourcing framework\nenabled by training a feature extractor through an adversarial game to conceal private attributes.\nIt maintains data utility by maximizing the mutual information\nbetween raw data and the combination of the private attribute and extracted feature.\nYet, TIPRDC extracts privacy-preserving feature representations,\nhence developers must update their application for it to be compatible with the anonymized features.\nIt differs from our data anonymization model which generates an anonymized version of\ndata that has the same dimensions as its input (both are in the same space),\nallowing existing applications to readily use the anonymized data without any modification.\nTableÂ 1 compares Blinder and the previous work\non privacy protection that uses machine learning techniques."
        ]
    },
    "S8.T2": {
        "caption": "Table 2. Anonymization latency of Blinder in the weight anonymization task. ",
        "table": "<table id=\"S8.T2.2.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S8.T2.2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S8.T2.2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\"><span id=\"S8.T2.2.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Device Name</span></th>\n<th id=\"S8.T2.2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row\"><span id=\"S8.T2.2.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Processor</span></th>\n<th id=\"S8.T2.2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Total (ms)</th>\n<th id=\"S8.T2.2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Preparation (ms)</th>\n<th id=\"S8.T2.2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Encoder (ms)</th>\n<th id=\"S8.T2.2.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\">Decoder (ms)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S8.T2.2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S8.T2.2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">OnePlus 6</th>\n<th id=\"S8.T2.2.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">Snapdragon 845</th>\n<td id=\"S8.T2.2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">10.84</td>\n<td id=\"S8.T2.2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">7.55</td>\n<td id=\"S8.T2.2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\">1.83</td>\n<td id=\"S8.T2.2.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\">1.47</td>\n</tr>\n<tr id=\"S8.T2.2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S8.T2.2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Nokia 6.1 Plus</th>\n<th id=\"S8.T2.2.1.3.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Snapdragon 636</th>\n<td id=\"S8.T2.2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">7.05</td>\n<td id=\"S8.T2.2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">3.90</td>\n<td id=\"S8.T2.2.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_t\">1.69</td>\n<td id=\"S8.T2.2.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_t\">1.46</td>\n</tr>\n<tr id=\"S8.T2.2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S8.T2.2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Samsung Note 4</th>\n<th id=\"S8.T2.2.1.4.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Exynos 5433</th>\n<td id=\"S8.T2.2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\">8.55</td>\n<td id=\"S8.T2.2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\">4.10</td>\n<td id=\"S8.T2.2.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\">2.38</td>\n<td id=\"S8.T2.2.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\">2.06</td>\n</tr>\n<tr id=\"S8.T2.2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S8.T2.2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">NVIDIA Jetson Nano</th>\n<th id=\"S8.T2.2.1.5.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">Arm Cortex-A57</th>\n<td id=\"S8.T2.2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_t\">34.45</td>\n<td id=\"S8.T2.2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_t\">22.47</td>\n<td id=\"S8.T2.2.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_t\">6.16</td>\n<td id=\"S8.T2.2.1.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_t\">5.82</td>\n</tr>\n<tr id=\"S8.T2.2.1.6.5\" class=\"ltx_tr\">\n<th id=\"S8.T2.2.1.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">NVIDIA Jetson Nano</th>\n<th id=\"S8.T2.2.1.6.5.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Maxwell GPU</th>\n<td id=\"S8.T2.2.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">14.76</td>\n<td id=\"S8.T2.2.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">6.57</td>\n<td id=\"S8.T2.2.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">3.68</td>\n<td id=\"S8.T2.2.1.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\">4.51</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We develop an Android app and deploy it on smartphones\nto measure Blinderâ€™s anonymization latency and power consumption in the real world.\nBlinderâ€™s VAE is trained via the proposed personalized federated learning algorithm\nusing PyTorchÂ (Paszke etÂ al., 2019).\nThe pre-trained model is quantized and serialized\nsuch that it becomes compatible with the PyTorch Mobile framework.\nThe quantized encoder and decoder have the same size of 6â€‹Mâ€‹B6ğ‘€ğµ6MB.\nWe select 333 Android smartphones with diverse computing powers. They are listed in TableÂ 2.\nOur Android app pulls sensor readings from the smartphoneâ€™s onboard accelerometer and gyroscope.\nThe sampling rate of both the accelerometer and gyroscope is set to 50â€‹Hâ€‹z50ğ»ğ‘§50Hz.\nThus, to achieve real-time data anonymization,\nBlinder must complete the anonymization of a data segment\nbefore the next data segment is ready, i.e., within a 200200200 ms interval.\nWe use the pre-processing techniques described in SectionÂ 7.1.\nSince the public attribute needs to be appended to the learned latent representation,\na pre-trained inference model is utilized to predict the userâ€™s public attribute\nfrom the original sensor data segment.\nIn our experiments, we reuse the desired inference model\ndescribed in SectionÂ 7.4 to predict the public attribute.\nThe overhead of running this inference model is measured\nand lumped with the execution time of other pre-processing steps.\nWe refer to this total time as preparation time. The private attribute passed to the decoder is chosen randomly for stochastic anonymization.",
            "We use PowerTutorÂ (Zhang etÂ al., 2010),\nan open-source app that takes accurate app-level CPU power consumption measurements,\nto estimate the battery drain due to the execution of Blinder. We evaluate the performance of Blinder when trained for weight anonymization\non MobiAct. The total anonymization latency is reported\nin TableÂ 2.\nThe latency measurement is averaged over anonymizing 100010001000 sensor data segments and the power consumption is averaged over 20+limit-from2020+ minutes of its execution.\nWe find that Blinderâ€™s total anonymization latency is 8.818.818.81 ms on average\namong the 3 Android smartphones,\nwhich is âˆ¼22Ã—\\sim{22}\\times faster than our real-time anonymization budget.\nFurthermore, the smartphoneâ€™s CPU consumes battery at around 6.386.386.38 J/min when running Blinder,\nwhich is around 4Ã—4\\times its idle power consumption.\nTo put it in context, Blinder consumes around 1.5âˆ’2Ã—1.5-2\\times more power than the Google Maps app.\nNote that we use a 50â€‹Hâ€‹z50ğ»ğ‘§50Hz sampling rate in our experiment,\nwhich is 10Ã—10\\times faster than the default sampling rate used in Android.\nThus, Blinderâ€™s power consumption can be further reduced by adjusting the sampling rate\naccording to the data need of the target application.\nWe also remark that since the discriminator is just used to train Blinder\nand is not loaded when anonymizing sensor data,\nprotecting multiple private attributes is not expected to increase Blinderâ€™s\npower draw or computational overhead.",
            "Blinder is also deployed on NVIDIA Jetson Nano, which is a representative IoT edge device. Jetson is a power-efficient computing platform,\nideal for running applications that could benefit from GPU acceleration.\nOur Jetson Nano model has 222 GB of RAM that is shared by a quad-core CPU and a 128-core GPU.\nIt runs Linux4Tegra (L4T) OS and NVIDIA Jetpack 4.4 SDK.\nWe installed the PyTorch for Jetson library to allow the pre-trained Blinder model\nto be effortlessly deployed for real-time anonymization.\nSince Jetson does not come with IMU sensors,\nwe simulate sensor data generation using the APScheduler libraryÂ (aps, line).\nWe perform two sets of experiments: utilizing only the CPU and enabling Jetsonâ€™s GPU acceleration.\nOther aspects of these experiments are identical to the ones described in SectionÂ 8.1.\nAs TableÂ 2 shows, when utilizing the CPU only,\nJetson can complete the anonymization task in 34.4534.4534.45 ms on average,\nwhich is much less than our 200200200 ms time budget.\nBy enabling the GPU acceleration capability,\nthe total anonymization latency decreases by over 40%percent4040\\% to 14.7614.7614.76 ms on average.\nIt should be noted that the total anonymization latency on Jetson is higher than on smartphones.\nWe believe this is because the OS, required services, and Blinder are using 222 GB of shared RAM.\nAs a result, running Blinder on Jetson requires frequent usage of the swap memory,\nwhich runs off a microSD card and has slower I/O performance than the eMMC/UFS-based storage in smartphones.\nWe do not report Blinderâ€™s power draw because Jetson has a reliable power supply (via a USB connector)\nand is not powered by a battery."
        ]
    }
}