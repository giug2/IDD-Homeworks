{
    "id_table_1": {
        "caption": "Table 1 :  Traditional Measures",
        "table": "S2.F1.2",
        "footnotes": [],
        "references": [
            "Table   1  shows formulas for traditional binary classification performance measures derived from the confusion matrix in  Figure   1 [ 19 ,  20 ,  21 ] . Sensitivity (recall/true positive rate) and specificity (true negative rate) measure correct classifications of positive and negative cases, respectively  [ 19 ] . Precision (positive predictive value) and negative predictive value measures correctly classified positive and negative cases among classified cases, respectively  [ 19 ] . Accuracy measures overall correct classifications  [ 19 ] . Ideally, all these measures are unity, indicating no false predictions.",
            "where   t = 1   t subscript  t 1 subscript  t \\alpha_{t}=1-\\beta_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and   t   =  i = 1 t  i   subscript  t superscript subscript product i 1 t subscript  i \\overline{\\alpha_{t}}=\\prod_{i=1}^{t}\\alpha_{i} over  start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG =  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . As the likelihood in  Equation   1  is intractable, training these models is done by maximising its variational lower bound (ELBO).  Ho et al.  found that using a loss as defined in  Equation   5  leads to higher quality generation.",
            "This two stage decomposition and reconstruction described in  Equation   16  is done twice to create  s H  P  S  S 1  ( t ) subscript s H P S subscript S 1 t \\mathbb{s}_{HPSS_{1}}(t) blackboard_s start_POSTSUBSCRIPT italic_H italic_P italic_S italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t )  and  s H  P  S  S 2  ( t ) subscript s H P S subscript S 2 t \\mathbb{s}_{HPSS_{2}}(t) blackboard_s start_POSTSUBSCRIPT italic_H italic_P italic_S italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t ) , which are then combined to get the final augmented signal  s H  P  S  S f  i  n  a  l  ( t ) subscript s H P S subscript S f i n a l t \\mathbb{s}_{HPSS_{final}}(t) blackboard_s start_POSTSUBSCRIPT italic_H italic_P italic_S italic_S start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t ) ,",
            "There is then a 75% chance of adding in amplitude modulation. The modulation is done as described in  Equation   19 , where  b A  M 1 = r  a  n  d  ( 0.01 , 0.25 ) subscript b A subscript M 1 r a n d 0.01 0.25 b_{AM_{1}}=rand(0.01,0.25) italic_b start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.25 ) ,  b A  M 2 = r  a  n  d  ( 0.01 , 0.25 ) subscript b A subscript M 2 r a n d 0.01 0.25 b_{AM_{2}}=rand(0.01,0.25) italic_b start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.25 ) ,  c A  M 1 = r  a  n  d  ( 0.05 , 0.5 ) subscript c A subscript M 1 r a n d 0.05 0.5 c_{AM_{1}}=rand(0.05,0.5) italic_c start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.05 , 0.5 ) ,  c A  M 2 = r  a  n  d  ( 0.001 , 0.05 ) subscript c A subscript M 2 r a n d 0.001 0.05 c_{AM_{2}}=rand(0.001,0.05) italic_c start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.001 , 0.05 ) ,  d A  M 1 = r  a  n  d  ( 0 , 1 ) subscript d A subscript M 1 r a n d 0 1 d_{AM_{1}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 ) ,  d A  M 2 = r  a  n  d  ( 0 , 1 ) subscript d A subscript M 2 r a n d 0 1 d_{AM_{2}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 )  and  s T  S  ( t ) subscript s T S t s_{TS}(t) italic_s start_POSTSUBSCRIPT italic_T italic_S end_POSTSUBSCRIPT ( italic_t )  is signal after the time stretch augmentation stage, which depending on the random chance may have been time-stretched.",
            "Next, there is another 7.5% chance of introducing the same noise as done in  Equation   18 . Following this, there is a 25% chance of applying parametric equalisation to boost frequency bands. Given the frequency range of  2   Hz  500   Hz range times 2 hertz times 500 hertz 2\\text{\\,}\\mathrm{Hz}500\\text{\\,}\\mathrm{Hz} start_ARG start_ARG 2 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG end_ARG  start_ARG start_ARG 500 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG end_ARG , the bandwidth is randomly selected between 5% and 20% of this range, and the signal is attenuated using a bandpass filter. After repeating this process 5 times, the filtered signal and original signal are summed and normalised.",
            "Random noise is applied the same way as the PCG noise, as defined in  Equation   18 , with this augmentation occurring with a probability of 7.5%. Next, a baseline wander is added 30% of this time. This is done as described in  Equation   20 , where  b B  W 1 = r  a  n  d  ( 0.01 , 0.2 ) subscript b B subscript W 1 r a n d 0.01 0.2 b_{BW_{1}}=rand(0.01,0.2) italic_b start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.2 ) ,  b B  W 2 = r  a  n  d  ( 0.01 , 0.2 ) subscript b B subscript W 2 r a n d 0.01 0.2 b_{BW_{2}}=rand(0.01,0.2) italic_b start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.2 ) ,  c B  W 1 = r  a  n  d  ( 0.05 , 0.5 ) subscript c B subscript W 1 r a n d 0.05 0.5 c_{BW_{1}}=rand(0.05,0.5) italic_c start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.05 , 0.5 ) ,  c B  W 2 = r  a  n  d  ( 0.001 , 0.05 ) subscript c B subscript W 2 r a n d 0.001 0.05 c_{BW_{2}}=rand(0.001,0.05) italic_c start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.001 , 0.05 ) ,  d B  W 1 = r  a  n  d  ( 0 , 1 ) subscript d B subscript W 1 r a n d 0 1 d_{BW_{1}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 ) ,  d B  W 2 = r  a  n  d  ( 0 , 1 ) subscript d B subscript W 2 r a n d 0 1 d_{BW_{2}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 ) .  s S  N E  ( t ) subscript s S subscript N E t \\mathbb{s}_{SN_{E}}(t) blackboard_s start_POSTSUBSCRIPT italic_S italic_N start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t )  is the ECG signal after the random noise augmentation stage, which may include the random noise as per the random chance.",
            "The signals were then bandpass filtered between  2   Hz times 2 hertz 2\\text{\\,}\\mathrm{Hz} start_ARG 2 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG  to  500   Hz times 500 hertz 500\\text{\\,}\\mathrm{Hz} start_ARG 500 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG  for PCG and  0.25   Hz times 0.25 hertz 0.25\\text{\\,}\\mathrm{Hz} start_ARG 0.25 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG  to  100   Hz times 100 hertz 100\\text{\\,}\\mathrm{Hz} start_ARG 100 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG  for ECG, the conditioning signal. A mel-spectrogram of the ECG was created as the local conditioning signal. The mel-spectrogram was created using a sample rate of  4   kHz times 4 kilohertz 4\\text{\\,}\\mathrm{kHz} start_ARG 4 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG , window length 1024, hop length 256, and 80 mel bins. Crossfading was used to ensure minimal audio artifacts when rearranging heart cycles. As the signals are joined when they are both in the same state, the end of the cycle in the diastole phase, they are assumed to be roughly correlated. The crossfade occurs between the last 40 samples of the first signal,   1  t  0 1 t 0 -1\\leq t\\leq 0 - 1  italic_t  0 , and the first 40 samples from the second signal,  0  t  1 0 t 1 0\\leq t\\leq 1 0  italic_t  1 . If one of the signals has a low variance, then a simple linear crossfade is used between the two. A linear crossfade can be described from  Equations   21  and  22  below,",
            "The out-of-distribution results are for the datasets the models were not trained on. Hence, this shows an increase in the generalisation of the models to other datasets that were not trained on. As the dataset being trained on was training-a, all other datasets are presented for the out-of-distribution performance.  Table   9  shows the OOD performance on the original dataset, with  Table   10  showing the OOD performance when trained on the augmented dataset."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Summary of Challenge Data",
        "table": "S2.T1.17",
        "footnotes": [],
        "references": [
            "The generative learning trilemma may guide the trade-offs in choosing a generative learning model. As  Figure   2  (adapted from  [ 30 ] ) shows, models often excel at only two of three desired goals: high sample quality, fast sample speed, and large sample variety. However, as mentioned earlier, performing the diffusion process in latent space allows LDMs to generate samples much faster, such that some argue it bypasses the trilemma in practice  [ 29 ,  30 ] . For this reason, LDMs have seen recent use in expanding datasets in biomedical projects, where data collection is prohibitively costly  [ 31 ] . As such, this work aims to use both the WaveGrad and DiffWave diffusion models for the creation of PCG from ECG signals.",
            "Recordings were divided into either  normal  (healthy),  abnormal  (diagnosed with CVD or other cardiac problems), or  unsure  (low quality signals)  [ 8 ] . A summary of the data, shown in  Table   2 , was adapted from  [ 42 ]  and  [ 8 ] . These datasets also include additional information, such as individual disease diagnoses and annotations of the heart cycles. These can be used to assist with the data augmentation.",
            "Random noise is applied the same way as the PCG noise, as defined in  Equation   18 , with this augmentation occurring with a probability of 7.5%. Next, a baseline wander is added 30% of this time. This is done as described in  Equation   20 , where  b B  W 1 = r  a  n  d  ( 0.01 , 0.2 ) subscript b B subscript W 1 r a n d 0.01 0.2 b_{BW_{1}}=rand(0.01,0.2) italic_b start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.2 ) ,  b B  W 2 = r  a  n  d  ( 0.01 , 0.2 ) subscript b B subscript W 2 r a n d 0.01 0.2 b_{BW_{2}}=rand(0.01,0.2) italic_b start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.2 ) ,  c B  W 1 = r  a  n  d  ( 0.05 , 0.5 ) subscript c B subscript W 1 r a n d 0.05 0.5 c_{BW_{1}}=rand(0.05,0.5) italic_c start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.05 , 0.5 ) ,  c B  W 2 = r  a  n  d  ( 0.001 , 0.05 ) subscript c B subscript W 2 r a n d 0.001 0.05 c_{BW_{2}}=rand(0.001,0.05) italic_c start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.001 , 0.05 ) ,  d B  W 1 = r  a  n  d  ( 0 , 1 ) subscript d B subscript W 1 r a n d 0 1 d_{BW_{1}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 ) ,  d B  W 2 = r  a  n  d  ( 0 , 1 ) subscript d B subscript W 2 r a n d 0 1 d_{BW_{2}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 ) .  s S  N E  ( t ) subscript s S subscript N E t \\mathbb{s}_{SN_{E}}(t) blackboard_s start_POSTSUBSCRIPT italic_S italic_N start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t )  is the ECG signal after the random noise augmentation stage, which may include the random noise as per the random chance.",
            "The signals were then bandpass filtered between  2   Hz times 2 hertz 2\\text{\\,}\\mathrm{Hz} start_ARG 2 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG  to  500   Hz times 500 hertz 500\\text{\\,}\\mathrm{Hz} start_ARG 500 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG  for PCG and  0.25   Hz times 0.25 hertz 0.25\\text{\\,}\\mathrm{Hz} start_ARG 0.25 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG  to  100   Hz times 100 hertz 100\\text{\\,}\\mathrm{Hz} start_ARG 100 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG  for ECG, the conditioning signal. A mel-spectrogram of the ECG was created as the local conditioning signal. The mel-spectrogram was created using a sample rate of  4   kHz times 4 kilohertz 4\\text{\\,}\\mathrm{kHz} start_ARG 4 end_ARG start_ARG times end_ARG start_ARG roman_kHz end_ARG , window length 1024, hop length 256, and 80 mel bins. Crossfading was used to ensure minimal audio artifacts when rearranging heart cycles. As the signals are joined when they are both in the same state, the end of the cycle in the diastole phase, they are assumed to be roughly correlated. The crossfade occurs between the last 40 samples of the first signal,   1  t  0 1 t 0 -1\\leq t\\leq 0 - 1  italic_t  0 , and the first 40 samples from the second signal,  0  t  1 0 t 1 0\\leq t\\leq 1 0  italic_t  1 . If one of the signals has a low variance, then a simple linear crossfade is used between the two. A linear crossfade can be described from  Equations   21  and  22  below,",
            "Otherwise, the following crossfade function will be used to ensure a crossfade is applied that represents how correlated the two signals are. For two fully uncorrelated signals, a constant power crossfade would be desired, and for two fully correlated signals, a constant voltage crossfade would be desired and something in between if not fully correlated or uncorrelated. Assuming that the crossfade function is deterministic, the two signals are a random process. Along with the assumption, the mean power of the signals at the point of crossfading is equal as they are being crossfaded when in the same phase of the heart cycle. This allows the following generalised crossfade function  [ 49 ]  to be used to satisfy a crossfade related to the signals correlation. The crossfade is defined in  Equations   23 ,  24  and  25 ,"
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  DiffWave parameters",
        "table": "S3.T2.4.1",
        "footnotes": [],
        "references": [
            "WaveGrad is a DDPM for audio synthesis using conditional generation. The model utilises the architecture consisting of multiple upsampling blocks (UBlocks) and downsampling blocks (DBlocks), with the input signal and the conditioning signal as inputs into the network. The conditioning signal is converted to a mel-spectrogram representation before being input to the model  [ 6 ] . These UBlocks and DBlocks follow the architecture of the upsampling and downsampling blocks utilised in the Generative Adversarial Network text-to-speech (GAN-TTS) model  [ 40 ] . The feature-wise linear modulation (FilM) modules combine information from the noisy waveform and the conditioning mel-spectrogram  [ 6 ] . The UBlock, DBlock and feature-wise linear modulation (FiLM) modules are shown in  Figure   3 , with  Figure   4  showing the entire WaveGrad architecture. The loss function is based on the difference between the noise added in each step of the forward diffusion process and the noise predicted during the reverse process  [ 6 ]  as described in  Equation   7 , with the Markov process being conditioned on the continuous noise level instead of the time-step. Also, note that the L1 norm was used over the L2 norm as it was found to provide better training stability  [ 6 ] . WaveGrad only includes a local conditioner in the form of a conditioning signal.",
            "Synthetic signals were generated using the mel-spectrogram of the ECG signal as a conditioner for both the WaveGrad  [ 6 ]  and DiffWave  [ 5 ]  diffusion models. They are trained before data is generated for use. These diffusion models generated data for 3200 patients, 800 abnormal and 2400 normal, with three segments used to train the classification models. This is done to reduce the effect of overfitting to the synthetic signals. The ECG signals for conditioning were taken from the icentia database  [ 44 ]  to introduce new data, with abnormal ECG used for abnormal PCG. The generative models were trained to create individual conditions and make them more realistic using additional labels from the dataset. To get around the lack of training data, the order of heart cycles was rearranged to increase training diversity. DiffWave and WaveGrad models were trained on an Nvidia RTX 4090 for 24 hours. The parameters for the DiffWave model that differ from the default are shown below in  Table   3 . Parameters used for the WaveGrad model that differ from the default are shown in Table  Table   4 . Both models differ slightly from their base implementations as they use a custom global conditioner. Additional global conditioners were added for specific abnormalities or lack of abnormalities, such as mitral valve prolapse, innocent or benign murmurs, aortic disease, miscellaneous conditions, and normal.",
            "Otherwise, the following crossfade function will be used to ensure a crossfade is applied that represents how correlated the two signals are. For two fully uncorrelated signals, a constant power crossfade would be desired, and for two fully correlated signals, a constant voltage crossfade would be desired and something in between if not fully correlated or uncorrelated. Assuming that the crossfade function is deterministic, the two signals are a random process. Along with the assumption, the mean power of the signals at the point of crossfading is equal as they are being crossfaded when in the same phase of the heart cycle. This allows the following generalised crossfade function  [ 49 ]  to be used to satisfy a crossfade related to the signals correlation. The crossfade is defined in  Equations   23 ,  24  and  25 ,"
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  WaveGrad parameters",
        "table": "S3.T3.4",
        "footnotes": [],
        "references": [
            "WaveGrad is a DDPM for audio synthesis using conditional generation. The model utilises the architecture consisting of multiple upsampling blocks (UBlocks) and downsampling blocks (DBlocks), with the input signal and the conditioning signal as inputs into the network. The conditioning signal is converted to a mel-spectrogram representation before being input to the model  [ 6 ] . These UBlocks and DBlocks follow the architecture of the upsampling and downsampling blocks utilised in the Generative Adversarial Network text-to-speech (GAN-TTS) model  [ 40 ] . The feature-wise linear modulation (FilM) modules combine information from the noisy waveform and the conditioning mel-spectrogram  [ 6 ] . The UBlock, DBlock and feature-wise linear modulation (FiLM) modules are shown in  Figure   3 , with  Figure   4  showing the entire WaveGrad architecture. The loss function is based on the difference between the noise added in each step of the forward diffusion process and the noise predicted during the reverse process  [ 6 ]  as described in  Equation   7 , with the Markov process being conditioned on the continuous noise level instead of the time-step. Also, note that the L1 norm was used over the L2 norm as it was found to provide better training stability  [ 6 ] . WaveGrad only includes a local conditioner in the form of a conditioning signal.",
            "Synthetic signals were generated using the mel-spectrogram of the ECG signal as a conditioner for both the WaveGrad  [ 6 ]  and DiffWave  [ 5 ]  diffusion models. They are trained before data is generated for use. These diffusion models generated data for 3200 patients, 800 abnormal and 2400 normal, with three segments used to train the classification models. This is done to reduce the effect of overfitting to the synthetic signals. The ECG signals for conditioning were taken from the icentia database  [ 44 ]  to introduce new data, with abnormal ECG used for abnormal PCG. The generative models were trained to create individual conditions and make them more realistic using additional labels from the dataset. To get around the lack of training data, the order of heart cycles was rearranged to increase training diversity. DiffWave and WaveGrad models were trained on an Nvidia RTX 4090 for 24 hours. The parameters for the DiffWave model that differ from the default are shown below in  Table   3 . Parameters used for the WaveGrad model that differ from the default are shown in Table  Table   4 . Both models differ slightly from their base implementations as they use a custom global conditioner. Additional global conditioners were added for specific abnormalities or lack of abnormalities, such as mitral valve prolapse, innocent or benign murmurs, aortic disease, miscellaneous conditions, and normal.",
            "Otherwise, the following crossfade function will be used to ensure a crossfade is applied that represents how correlated the two signals are. For two fully uncorrelated signals, a constant power crossfade would be desired, and for two fully correlated signals, a constant voltage crossfade would be desired and something in between if not fully correlated or uncorrelated. Assuming that the crossfade function is deterministic, the two signals are a random process. Along with the assumption, the mean power of the signals at the point of crossfading is equal as they are being crossfaded when in the same phase of the heart cycle. This allows the following generalised crossfade function  [ 49 ]  to be used to satisfy a crossfade related to the signals correlation. The crossfade is defined in  Equations   23 ,  24  and  25 ,"
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :   Adam Optimiser Parameters",
        "table": "S3.T4.4",
        "footnotes": [],
        "references": [
            "where   t = 1   t subscript  t 1 subscript  t \\alpha_{t}=1-\\beta_{t} italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = 1 - italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  and   t   =  i = 1 t  i   subscript  t superscript subscript product i 1 t subscript  i \\overline{\\alpha_{t}}=\\prod_{i=1}^{t}\\alpha_{i} over  start_ARG italic_ start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG =  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . As the likelihood in  Equation   1  is intractable, training these models is done by maximising its variational lower bound (ELBO).  Ho et al.  found that using a loss as defined in  Equation   5  leads to higher quality generation.",
            "DiffWave is another DDPM for raw audio synthesis with conditional and unconditional generation. The loss function utilises a single ELBO-based training objective without auxiliary losses  [ 5 ] , as described in  Equation   8 . One-dimensional convolutions are used on the input and conditioning signals that go through multiple fully connected layers. The model contains a WaveNet  [ 41 ]   backbone , consisting of bi-directional dilated convolutions and residual layers and connections. The architecture is shown in  Figure   5 . DiffWave can be used for both conditional and unconditional generation. For conditional generation, it uses a local conditioning signal and a global conditioner (discrete labels)  [ 5 ] .",
            "Otherwise, the following crossfade function will be used to ensure a crossfade is applied that represents how correlated the two signals are. For two fully uncorrelated signals, a constant power crossfade would be desired, and for two fully correlated signals, a constant voltage crossfade would be desired and something in between if not fully correlated or uncorrelated. Assuming that the crossfade function is deterministic, the two signals are a random process. Along with the assumption, the mean power of the signals at the point of crossfading is equal as they are being crossfaded when in the same phase of the heart cycle. This allows the following generalised crossfade function  [ 49 ]  to be used to satisfy a crossfade related to the signals correlation. The crossfade is defined in  Equations   23 ,  24  and  25 ,",
            "The Adam optimiser is used for training along with a cyclic triangular learning rate scheduler with parameters below in  Table   5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :   Training Schedule",
        "table": "S3.T5.7",
        "footnotes": [],
        "references": [
            "To achieve a more robust model, the augmented training dataset must first be created.  Figure   6  depicts the dataset creation process. Once this dataset is created, various classification models can be trained and evaluated to measure the increase in ID and OOD performance.",
            "This two stage decomposition and reconstruction described in  Equation   16  is done twice to create  s H  P  S  S 1  ( t ) subscript s H P S subscript S 1 t \\mathbb{s}_{HPSS_{1}}(t) blackboard_s start_POSTSUBSCRIPT italic_H italic_P italic_S italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t )  and  s H  P  S  S 2  ( t ) subscript s H P S subscript S 2 t \\mathbb{s}_{HPSS_{2}}(t) blackboard_s start_POSTSUBSCRIPT italic_H italic_P italic_S italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t ) , which are then combined to get the final augmented signal  s H  P  S  S f  i  n  a  l  ( t ) subscript s H P S subscript S f i n a l t \\mathbb{s}_{HPSS_{final}}(t) blackboard_s start_POSTSUBSCRIPT italic_H italic_P italic_S italic_S start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t ) ,",
            "During the models training on the original dataset, as a CNN is being finetuned, only 10 epochs are used in which the best weights are chosen from the highest MCC value from the validation set to reduce overfitting. The model is only updated for each dataset if it performed better on the validation set than previously. A schedule is used to reduce the overfitting of the synthetic data for training on the augmented dataset. This schedule can be found below in  Table   6  and was experimentally determined to provide the best results, where max-mix is all of the data with no augmentations being applied to the original dataset and 3 augmentations applied to the DiffWave and WaveGrad data. From the synthetic data, only three random segments were taken to ensure the model does not overfit to the synthetic data. The max-aug data is the original data with 30 augmentations being applied and no synthetic data."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Models performance ID trained on the original dataset.",
        "table": "S3.T6.4",
        "footnotes": [],
        "references": [
            "WaveGrad is a DDPM for audio synthesis using conditional generation. The model utilises the architecture consisting of multiple upsampling blocks (UBlocks) and downsampling blocks (DBlocks), with the input signal and the conditioning signal as inputs into the network. The conditioning signal is converted to a mel-spectrogram representation before being input to the model  [ 6 ] . These UBlocks and DBlocks follow the architecture of the upsampling and downsampling blocks utilised in the Generative Adversarial Network text-to-speech (GAN-TTS) model  [ 40 ] . The feature-wise linear modulation (FilM) modules combine information from the noisy waveform and the conditioning mel-spectrogram  [ 6 ] . The UBlock, DBlock and feature-wise linear modulation (FiLM) modules are shown in  Figure   3 , with  Figure   4  showing the entire WaveGrad architecture. The loss function is based on the difference between the noise added in each step of the forward diffusion process and the noise predicted during the reverse process  [ 6 ]  as described in  Equation   7 , with the Markov process being conditioned on the continuous noise level instead of the time-step. Also, note that the L1 norm was used over the L2 norm as it was found to provide better training stability  [ 6 ] . WaveGrad only includes a local conditioner in the form of a conditioning signal.",
            "The augmentation procedure of the PCG and ECG signals is shown in  Figure   7 . The time stretching augmentation is synchronised to ensure that they are both stretched the same amount, with the black lines representing the flow of the ECG data and the white lines representing the flow of PCG data. Augmentation stages have different percentage chances of occurring, where the chances chosen were determined to provide the widest variety of augmented signals after every stage has been completed whilst also resulting in the best performance. The augmentations vary slightly between PCG and ECG to best meet the physiological constraints.",
            "The PCG signals are augmented in various ways: harmonic percussive source separation (HPSS) for emphasis on certain parts of the signal, time stretching, emphasis on certain bands of the signal using a parametric equalisation (EQ) filter and introducing noise from the EPHNOGRAM dataset  [ 45 ] . Before these operations are applied, the signals are normalised to have a zero mean and be between -1 and 1. Shown in  Figure   7  is the augmentation procedure applied to PCG data, noted with the white lines.",
            "The ECG signals are also augmented in numerous ways; these include introducing random noise, adding baseline wander, time stretching, adding noise from the MIT-BIH dataset, and emphasising certain signal bands.  Figure   7  shows the order of processing on the ECG, indicated with the black lines.",
            "The ID results are for the datasets on which the models were trained. This shows the increase in performance when training on the augmented dataset compared to the original dataset. As the only dataset being trained on was training-a, these are the only models presented for in-distribution performance.  Table   7  displays the ID performance when the models are trained on the original dataset, with  Table   8  displaying the ID performance for models trained on the augmented dataset."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Models performance ID trained on the augmented dataset.",
        "table": "S4.T7.2.2",
        "footnotes": [],
        "references": [
            "DiffWave is another DDPM for raw audio synthesis with conditional and unconditional generation. The loss function utilises a single ELBO-based training objective without auxiliary losses  [ 5 ] , as described in  Equation   8 . One-dimensional convolutions are used on the input and conditioning signals that go through multiple fully connected layers. The model contains a WaveNet  [ 41 ]   backbone , consisting of bi-directional dilated convolutions and residual layers and connections. The architecture is shown in  Figure   5 . DiffWave can be used for both conditional and unconditional generation. For conditional generation, it uses a local conditioning signal and a global conditioner (discrete labels)  [ 5 ] .",
            "Next, there is another 7.5% chance of introducing the same noise as done in  Equation   18 . Following this, there is a 25% chance of applying parametric equalisation to boost frequency bands. Given the frequency range of  2   Hz  500   Hz range times 2 hertz times 500 hertz 2\\text{\\,}\\mathrm{Hz}500\\text{\\,}\\mathrm{Hz} start_ARG start_ARG 2 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG end_ARG  start_ARG start_ARG 500 end_ARG start_ARG times end_ARG start_ARG roman_Hz end_ARG end_ARG , the bandwidth is randomly selected between 5% and 20% of this range, and the signal is attenuated using a bandpass filter. After repeating this process 5 times, the filtered signal and original signal are summed and normalised.",
            "Random noise is applied the same way as the PCG noise, as defined in  Equation   18 , with this augmentation occurring with a probability of 7.5%. Next, a baseline wander is added 30% of this time. This is done as described in  Equation   20 , where  b B  W 1 = r  a  n  d  ( 0.01 , 0.2 ) subscript b B subscript W 1 r a n d 0.01 0.2 b_{BW_{1}}=rand(0.01,0.2) italic_b start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.2 ) ,  b B  W 2 = r  a  n  d  ( 0.01 , 0.2 ) subscript b B subscript W 2 r a n d 0.01 0.2 b_{BW_{2}}=rand(0.01,0.2) italic_b start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.2 ) ,  c B  W 1 = r  a  n  d  ( 0.05 , 0.5 ) subscript c B subscript W 1 r a n d 0.05 0.5 c_{BW_{1}}=rand(0.05,0.5) italic_c start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.05 , 0.5 ) ,  c B  W 2 = r  a  n  d  ( 0.001 , 0.05 ) subscript c B subscript W 2 r a n d 0.001 0.05 c_{BW_{2}}=rand(0.001,0.05) italic_c start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.001 , 0.05 ) ,  d B  W 1 = r  a  n  d  ( 0 , 1 ) subscript d B subscript W 1 r a n d 0 1 d_{BW_{1}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 ) ,  d B  W 2 = r  a  n  d  ( 0 , 1 ) subscript d B subscript W 2 r a n d 0 1 d_{BW_{2}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_B italic_W start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 ) .  s S  N E  ( t ) subscript s S subscript N E t \\mathbb{s}_{SN_{E}}(t) blackboard_s start_POSTSUBSCRIPT italic_S italic_N start_POSTSUBSCRIPT italic_E end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_t )  is the ECG signal after the random noise augmentation stage, which may include the random noise as per the random chance.",
            "where  e e e italic_e  is the even component of the crossfade function, and  o o o italic_o  is the odd component, and  r r r italic_r  is the correlation coefficient of the two signals at zero lag and  0  r  1 0 r 1 0\\leq r\\leq 1 0  italic_r  1 . The crossfade is then interpolated to double the length using a univariate spline, with a degree of 3 and a smoothing factor equal to the length of the signal. The implementation is the scipy implementation of the univariate spline  [ 50 ] . The final signal consists of the first signal before the last 40 samples, the crossfaded and interpolated signal, and the second signal after the first 40 samples.  Figure   8  demonstrates the effect that this crossfade has on reducing artifacts. Rearranging of the heart cycles can be seen through the rearranging of the chirp in the last row. The first column shows the original signal, the second shows the rearranging of all heart cycles, the third shows the rearranging of a few heart cycles, and the final shows the rearranging of larger groups of heart cycles.",
            "The ID results are for the datasets on which the models were trained. This shows the increase in performance when training on the augmented dataset compared to the original dataset. As the only dataset being trained on was training-a, these are the only models presented for in-distribution performance.  Table   7  displays the ID performance when the models are trained on the original dataset, with  Table   8  displaying the ID performance for models trained on the augmented dataset."
        ]
    },
    "id_table_9": {
        "caption": "Table 9 :  Models performance in OOD trained on the original dataset.",
        "table": "S4.T8.2.2",
        "footnotes": [],
        "references": [
            "There is then a 75% chance of adding in amplitude modulation. The modulation is done as described in  Equation   19 , where  b A  M 1 = r  a  n  d  ( 0.01 , 0.25 ) subscript b A subscript M 1 r a n d 0.01 0.25 b_{AM_{1}}=rand(0.01,0.25) italic_b start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.25 ) ,  b A  M 2 = r  a  n  d  ( 0.01 , 0.25 ) subscript b A subscript M 2 r a n d 0.01 0.25 b_{AM_{2}}=rand(0.01,0.25) italic_b start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.01 , 0.25 ) ,  c A  M 1 = r  a  n  d  ( 0.05 , 0.5 ) subscript c A subscript M 1 r a n d 0.05 0.5 c_{AM_{1}}=rand(0.05,0.5) italic_c start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.05 , 0.5 ) ,  c A  M 2 = r  a  n  d  ( 0.001 , 0.05 ) subscript c A subscript M 2 r a n d 0.001 0.05 c_{AM_{2}}=rand(0.001,0.05) italic_c start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0.001 , 0.05 ) ,  d A  M 1 = r  a  n  d  ( 0 , 1 ) subscript d A subscript M 1 r a n d 0 1 d_{AM_{1}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 ) ,  d A  M 2 = r  a  n  d  ( 0 , 1 ) subscript d A subscript M 2 r a n d 0 1 d_{AM_{2}}=rand(0,1) italic_d start_POSTSUBSCRIPT italic_A italic_M start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT = italic_r italic_a italic_n italic_d ( 0 , 1 )  and  s T  S  ( t ) subscript s T S t s_{TS}(t) italic_s start_POSTSUBSCRIPT italic_T italic_S end_POSTSUBSCRIPT ( italic_t )  is signal after the time stretch augmentation stage, which depending on the random chance may have been time-stretched.",
            "The out-of-distribution results are for the datasets the models were not trained on. Hence, this shows an increase in the generalisation of the models to other datasets that were not trained on. As the dataset being trained on was training-a, all other datasets are presented for the out-of-distribution performance.  Table   9  shows the OOD performance on the original dataset, with  Table   10  showing the OOD performance when trained on the augmented dataset."
        ]
    },
    "id_table_10": {
        "caption": "Table 10 :  Models performance in OOD trained on the augmented dataset.",
        "table": "S4.T9.2.2",
        "footnotes": [],
        "references": [
            "The out-of-distribution results are for the datasets the models were not trained on. Hence, this shows an increase in the generalisation of the models to other datasets that were not trained on. As the dataset being trained on was training-a, all other datasets are presented for the out-of-distribution performance.  Table   9  shows the OOD performance on the original dataset, with  Table   10  showing the OOD performance when trained on the augmented dataset."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "S4.T10.2.2",
        "footnotes": [],
        "references": []
    }
}