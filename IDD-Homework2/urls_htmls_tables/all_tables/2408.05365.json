{
    "id_table_1": {
        "caption": "TABLE I:  Existing algorithms developed for Financial insights, trends and assessments.",
        "table": "S2.T1.1.1",
        "footnotes": [],
        "references": [
            "While training a financial-domain specific LLM poses challenges with regards to training data quality and hardware resources needed for large scale training epochs, most of these well-established financial use-cases rely on web-search approaches combined with knowledge graphs (KGs) to ensure  recency  in the data quality and standards of responses/outputs  [ 5 ] . The generative AI-based approach of retrieving updated financial information and serving the analysis in domain-representative jargon, also known as the agentic-RAG, comprises of two steps. First, a web-search is orchestrated for the user-query; second from the retrieved web-link texts, paragraphs that contain relevant information are identified using the KGs. Typically, KGs are a structured representation of data or textual information and they consist of nodes (entities) and edges (relationships) that connect them. For example, a node might represent an entity such as an organization, place, or person, while an edge could represent a relationship like  resulted in ,  risen/fallen  etc. An example of KG representation from financial data is shown in Fig.  1 . The most appropriate paragraphs/KGs from the web links are then used to serve the information in a personalized and structured manner to create accurate and reliable AI systems that are capable of fact-checking, improved understanding, enhanced domain-specific reasoning  [ 7 ] .",
            "The goal of this work is to generate domain-friendly natural language text from a minimal prompt that contains basic instructions and financial data in a tabular format. For a sequence of words/tokens, the  i i i italic_i th generated token  x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and the log probability associated with the token is  p i subscript p i p_{i} italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . It is noteworthy that each sequential token is generated as a function ( F F F italic_F ) of the prior tokens and the token with highest probability across the top contenders for the  i i i italic_i th position ( x i subscript x i x_{i} italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ), represented by equation ( 1 ). Also, the log-probability of top 5 contenders for each sequential position is collected as  P i subscript P i P_{i} italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  from the output of OpenAIs GPT3.5 to quantitatively detect creativity and hallucinations.",
            "Additionally, the quality of generated text is further shown in terms of scaled metrics in Fig.  10 . Here we observe that the two-stage FT model has consistently high BLEU, ROUGE and chrF++ metrics when compared to the untrained and one-stage FT models.",
            "In the previous subsection, we observe the importance in two-stage FT process to control for hallucinations and incomplete sentences. However, since creative and hallucinated words/tokens have similar probabilistic nature, we qualitatively assess the reports generated from the prompts shown in Fig.  7  and Fig.  8  for hallucinations and creativity. Fig.  11  demonstrates sample reports generated from untrained, one-stage FT and two-stage FT models, respectively.",
            "In Fig.  12  we observe that the cross-entropy of the generated text significantly reduces after each stage of FT leading to less hallucinations and more certain text generation.",
            "Fig.  13  shows the ASLS metrics at sentence level for the untrained, one-stage FT and two-stage FT models, respectively. We observe that for the untrained and one-stage FT models, the last two lines have the lowest log-loss, or low certainty for text generation. These sentences have a higher tendency to contain hallucinations or creativity. However, for the two-stage FT model, the hallucinations significantly reduce, thereby leading to more certain and higher quality of text generation."
        ]
    },
    "id_table_2": {
        "caption": "TABLE II:  Characteristics of financial report data.",
        "table": "S2.T2.1.1",
        "footnotes": [],
        "references": [
            "Although agentic-RAG processes have benefited some financial use-cases, these approaches do not scale to financial report generation tasks, wherein, the intention is to transfer the domain-specific writing style. As an experiment, we used as tabular data and English language instructions describing the persona of a financial analyst and specific instructions to GPT4o model to generate multiple paragraphs of financial reporting text. The sample output is shown in Fig.  2 , where unwanted text is struck out by our financial domain experts. Advanced prompt engineering and instructional guardrails  [ 2 ]  for this use-case led to the following observations.",
            "Perplexity ( P  e  r P e r Per italic_P italic_e italic_r ): A lower value signifies that each sequential token is generated with high confidence following the FT process in equation ( 2 ).  t t t italic_t  represents the number of tokens per sentence.",
            "In Fig.  12  we observe that the cross-entropy of the generated text significantly reduces after each stage of FT leading to less hallucinations and more certain text generation."
        ]
    },
    "id_table_3": {
        "caption": "TABLE III:  Examples of Style Transfer works.",
        "table": "S2.T3.1.1",
        "footnotes": [],
        "references": [
            "This paper makes two key contributions. First, we propose a two-stage LLM fine-tuning process that minimizes hallucinations and incomplete responses, while promoting creative and compound sentences that align with the Financial reporting writing styles. We demonstrate the step-wise enhancements in the knowledge density per generated paragraph across the fine-tuning stages, while ensuring minimal fine-tuning cost of under $18 for fine tuning GPT3.5 model. Second, we propose multiple novel metrics that can assess the performance of fine-tuned LLMs that are based on KG-based approaches. These metrics enable tracking the required creativity standards per generated sentence while flagging hallucinations using spacy-based libraries. We test the fine-tuned model by applying a basic prompt that includes minimal instructions and tabular data shown in Fig.  3  as input and the corresponding output with creative and compound sentences is shown in Fig.  4 .",
            "Fig.  13  shows the ASLS metrics at sentence level for the untrained, one-stage FT and two-stage FT models, respectively. We observe that for the untrained and one-stage FT models, the last two lines have the lowest log-loss, or low certainty for text generation. These sentences have a higher tendency to contain hallucinations or creativity. However, for the two-stage FT model, the hallucinations significantly reduce, thereby leading to more certain and higher quality of text generation."
        ]
    },
    "id_table_4": {
        "caption": "TABLE IV:  Examples of text generation Scores on well known user queries.",
        "table": "S3.T4.3.3",
        "footnotes": [],
        "references": [
            "This paper makes two key contributions. First, we propose a two-stage LLM fine-tuning process that minimizes hallucinations and incomplete responses, while promoting creative and compound sentences that align with the Financial reporting writing styles. We demonstrate the step-wise enhancements in the knowledge density per generated paragraph across the fine-tuning stages, while ensuring minimal fine-tuning cost of under $18 for fine tuning GPT3.5 model. Second, we propose multiple novel metrics that can assess the performance of fine-tuned LLMs that are based on KG-based approaches. These metrics enable tracking the required creativity standards per generated sentence while flagging hallucinations using spacy-based libraries. We test the fine-tuned model by applying a basic prompt that includes minimal instructions and tabular data shown in Fig.  3  as input and the corresponding output with creative and compound sentences is shown in Fig.  4 ."
        ]
    },
    "id_table_5": {
        "caption": "TABLE V:  Average Sectional perplexity in fine-tuned validation reports.",
        "table": "S4.T5.2.2",
        "footnotes": [],
        "references": [
            "Domain-specific generative AI led automation tasks such as a financial chatbot or financial news writer, have seen specific improvements in overall knowledge retrieval tasks by using search engine and search engine along with KG capabilities as shown in Fig.  5   [ 10 ] . The major reason for improved answering capabilities with web searches and KG isolation of text is that the LLMs follow a unique knowledge distribution, with a head, body/torso and tail  [ 11 ] . Knowledge from the LLM head (commonly occurring and rarely changing facts) are easily retrievable with minimal hallucinations. Contrastingly, knowledge from the LLM tail (rapidly changing facts, such as share prices etc.) can lead to stale data in the responses or inaccuracies/hallucinations or I dont know responses from LLMs. Thus, it is imperative to teach the LLM to work with latest data and reasonably modify the text generation style to ensure lowered hallucinations and unknown responses as shown in  [ 11 ] . A summary of latest works on the financial domain, style-transfer and detection of hallucination and creativity are presented below."
        ]
    },
    "global_footnotes": []
}