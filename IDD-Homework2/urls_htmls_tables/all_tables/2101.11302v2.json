{
    "S3.T1": {
        "caption": "Table 1: Search range per hyper-parameter. We consider the number of update steps in the inner-loop, Num inner-loop steps, the (initial) learning rate of the inner-loop, Inner-loop lr, the factor by which the learning rate of the classification head is multiplied, Class-head lr multiplier, and, if applicable, the learning rate with which the inner-loop optimizer is updated, Inner-optimizer lr. The chosen value is underlined.",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S3.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S3.T1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">MetaUpdate Method</span></td>\n<td id=\"S3.T1.1.1.2\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S3.T1.1.1.2.1\" class=\"ltx_text ltx_font_bold\">Num inner-loop steps</span></td>\n<td id=\"S3.T1.1.1.3\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S3.T1.1.1.3.1\" class=\"ltx_text ltx_font_bold\">Inner-loop lr</span></td>\n<td id=\"S3.T1.1.1.4\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S3.T1.1.1.4.1\" class=\"ltx_text ltx_font_bold\">Class-head lr multiplier</span></td>\n<td id=\"S3.T1.1.1.5\" class=\"ltx_td ltx_align_left ltx_border_tt\"><span id=\"S3.T1.1.1.5.1\" class=\"ltx_text ltx_font_bold\">Inner-optimizer lr</span></td>\n</tr>\n<tr id=\"S3.T1.1.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Reptile</td>\n<td id=\"S3.T1.1.2.2\" class=\"ltx_td ltx_align_left ltx_border_t\">2,3,<span id=\"S3.T1.1.2.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">5</span>\n</td>\n<td id=\"S3.T1.1.2.3\" class=\"ltx_td ltx_align_left ltx_border_t\">1e-5, <span id=\"S3.T1.1.2.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">5e-5</span>, 1e-4</td>\n<td id=\"S3.T1.1.2.4\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S3.T1.1.2.4.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">1</span>, 10</td>\n<td id=\"S3.T1.1.2.5\" class=\"ltx_td ltx_align_left ltx_border_t\">-</td>\n</tr>\n<tr id=\"S3.T1.1.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.3.1\" class=\"ltx_td ltx_align_left\">foMAML</td>\n<td id=\"S3.T1.1.3.2\" class=\"ltx_td ltx_align_left\">2,3,<span id=\"S3.T1.1.3.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">5</span>\n</td>\n<td id=\"S3.T1.1.3.3\" class=\"ltx_td ltx_align_left\">\n<span id=\"S3.T1.1.3.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">1e-5</span>, 1e-4, 1e-3</td>\n<td id=\"S3.T1.1.3.4\" class=\"ltx_td ltx_align_left\">1, <span id=\"S3.T1.1.3.4.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">10</span>\n</td>\n<td id=\"S3.T1.1.3.5\" class=\"ltx_td ltx_align_left\">3e-5, <span id=\"S3.T1.1.3.5.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">6e-5</span>, 1e-4</td>\n</tr>\n<tr id=\"S3.T1.1.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.4.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">foProtoMAMLn</td>\n<td id=\"S3.T1.1.4.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">2,3,<span id=\"S3.T1.1.4.2.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">5</span>\n</td>\n<td id=\"S3.T1.1.4.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">\n<span id=\"S3.T1.1.4.3.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">1e-5</span>, 1e-4, 1e-3</td>\n<td id=\"S3.T1.1.4.4\" class=\"ltx_td ltx_align_left ltx_border_bb\">1, <span id=\"S3.T1.1.4.4.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">10</span>\n</td>\n<td id=\"S3.T1.1.4.5\" class=\"ltx_td ltx_align_left ltx_border_bb\">3e-5, <span id=\"S3.T1.1.4.5.1\" class=\"ltx_text ltx_framed ltx_framed_underline\">6e-5</span>, 1e-4</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "We perform grid search on MLDoc in order to determine optimal hyperparameters for the MetaUpdate methods.\nThe hyper-parameters resulting in the lowest loss on ld​e​v=subscript𝑙𝑑𝑒𝑣absentl_{dev}= Spanish are used in all experiments.\nThe number of update steps in the inner-loop is 5; the (initial) learning rate of the inner-loop is 1e-5 for MAML and ProtoMAML and 5e-5 for Reptile; the factor by which the learning rate of the classification head is multiplied is 10 for MAML and ProtoMAML and 1 for Reptile; when applicable, the learning rate with which the inner-loop optimizer is updated is 6e-5. See Table 1 for the considered grid."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Average accuracy of 5 different seeds on the unseen target languages for MLDoc. ΔΔ\\Delta corresponds to the average accuracy across test languages.",
        "table": null,
        "footnotes": [],
        "references": [
            "Tables 2 and 3 show the accuracy scores on the target languages on MLDoc and Amazon respectively. We start by noting the strong multilingual capabilities of XLM-RoBERTa as our base-learner:\nAdding the full training datasets in three extra languages (i.e., comparing the zero-shot with the non-episodic baseline in the high-resource, ‘Included’ setting) results in a mere 1.2% points increase in accuracy on average for MLDoc and 0.6% points for Amazon.\nAlthough the zero-shot333The zero-shot baseline is only applicable in the ‘Included’ setting, as the English data is not available under ‘Excluded’. and non-episodic baselines are strong, in the majority of cases, a meta-learning approach improves performance. This holds especially for our version of ProtoMAML (ProtoMAMLn), which achieves the highest average accuracy in all considered settings.",
            "The substantial improvements for Russian on MLDoc and Chinese on Amazon indicate that meta-learning is most advantageous when the considered task distribution is somewhat heterogeneous or, in other words, when domain drift Lai et al. (2019) is present. For the Chinese data used for the sentiment polarity task, the presence of domain drift is obvious as the data is collected from a different website and concerns different products than the other languages. For Russian in the MLDoc dataset, it holds that the non-episodic baseline has the smallest gain in performance when adding English data (ls​r​csubscript𝑙𝑠𝑟𝑐l_{src}) in the limited-resource setting (0.2% absolute gain as opposed to 5.7% on average for the remaining languages) and even a decrease of 2.4% points when adding English data in the high-resource setting. Especially for these languages with domain drift, our version of ProtoMAML (foProtoMAMLn) outperforms the non-episodic baselines with a relatively large margin. For instance, in Table 2 in the high-resource setting with English included during training, foProtoMAMLn improves over the non-episodic baseline with 9.1% points whereas the average gain over the remaining languages is 0.9% points. A similar trend can be seen in Table 3 where, in the limited-resource setting, foProtoMAMLn outperforms the non-episodic baseline with 1.9% points on Chinese, with comparatively smaller gains on average for the remaining languages.",
            "In this setting, we achieve a new state of the art on MLDoc for German, Italian, Japanese and Russian using our method, foProtoMAMLn (Table 4).444The zero-shot baselines are the same as in Tables 2 and 3. The previous state of the art for German and Russian is held by Lai et al. (2019) (95.73% and 84.65% respectively).\nFor Japanese and Italian, it is held by Eisenschlos et al. (2019) (80.55% and 80.12% respectively). The state of the art for French and Chinese is also held by Lai et al. (2019) (96.05% and 93.32% respectively).\nOn the Amazon dataset, foProtoMAMLn also outperforms all other methods on average. The state of the art is held by Lai et al. (2019) with 93.3%, 94.2% and 90.6% for French, German and Chinese respectively and, although we do not outperform it, the differences are rather small – between 0.2% (Chinese) and 3.4% points (German) – even when grid search is based on MLDoc, while we use a much less computationally expensive approach."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Average accuracy of 5 different seeds on the unseen target languages for Amazon. ΔΔ\\Delta corresponds to the average accuracy across test languages.",
        "table": "<table id=\"S5.T3.2\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T3.2.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.3.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S5.T3.2.3.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td id=\"S5.T3.2.3.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S5.T3.2.3.2.1\" class=\"ltx_text ltx_font_bold\">Limited-resource setting</span></td>\n<td id=\"S5.T3.2.3.3\" class=\"ltx_td ltx_border_r ltx_border_tt\"/>\n<td id=\"S5.T3.2.3.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S5.T3.2.3.4.1\" class=\"ltx_text ltx_font_bold\">High-resource setting</span></td>\n<td id=\"S5.T3.2.3.5\" class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr id=\"S5.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.2.3\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.2.3.1\" class=\"ltx_text ltx_font_bold\">de</span></td>\n<td id=\"S5.T3.2.2.4\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.2.4.1\" class=\"ltx_text ltx_font_bold\">fr</span></td>\n<td id=\"S5.T3.2.2.5\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.2.5.1\" class=\"ltx_text ltx_font_bold\">ja</span></td>\n<td id=\"S5.T3.2.2.6\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S5.T3.2.2.6.1\" class=\"ltx_text ltx_font_bold\">zh</span></td>\n<td id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_r\"><math id=\"S5.T3.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta\" display=\"inline\"><semantics id=\"S5.T3.1.1.1.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T3.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.m1.1.1.cmml\">Δ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.m1.1b\"><ci id=\"S5.T3.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.m1.1.1\">Δ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.m1.1c\">\\Delta</annotation></semantics></math></td>\n<td id=\"S5.T3.2.2.7\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.2.7.1\" class=\"ltx_text ltx_font_bold\">de</span></td>\n<td id=\"S5.T3.2.2.8\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.2.8.1\" class=\"ltx_text ltx_font_bold\">fr</span></td>\n<td id=\"S5.T3.2.2.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.2.9.1\" class=\"ltx_text ltx_font_bold\">ja</span></td>\n<td id=\"S5.T3.2.2.10\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S5.T3.2.2.10.1\" class=\"ltx_text ltx_font_bold\">zh</span></td>\n<td id=\"S5.T3.2.2.2\" class=\"ltx_td ltx_align_left\"><math id=\"S5.T3.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\Delta\" display=\"inline\"><semantics id=\"S5.T3.2.2.2.m1.1a\"><mi mathvariant=\"normal\" id=\"S5.T3.2.2.2.m1.1.1\" xref=\"S5.T3.2.2.2.m1.1.1.cmml\">Δ</mi><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.2.2.2.m1.1b\"><ci id=\"S5.T3.2.2.2.m1.1.1.cmml\" xref=\"S5.T3.2.2.2.m1.1.1\">Δ</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.2.2.2.m1.1c\">\\Delta</annotation></semantics></math></td>\n</tr>\n<tr id=\"S5.T3.2.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.4.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Zero-shot</td>\n<td id=\"S5.T3.2.4.2\" class=\"ltx_td ltx_align_left ltx_border_t\"><span id=\"S5.T3.2.4.2.1\" class=\"ltx_text ltx_font_bold\">91.2</span></td>\n<td id=\"S5.T3.2.4.3\" class=\"ltx_td ltx_align_left ltx_border_t\">90.7</td>\n<td id=\"S5.T3.2.4.4\" class=\"ltx_td ltx_align_left ltx_border_t\">87.0</td>\n<td id=\"S5.T3.2.4.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">84.6</td>\n<td id=\"S5.T3.2.4.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">88.4</td>\n<td id=\"S5.T3.2.4.7\" class=\"ltx_td ltx_align_left ltx_border_t\">91.2</td>\n<td id=\"S5.T3.2.4.8\" class=\"ltx_td ltx_align_left ltx_border_t\">90.7</td>\n<td id=\"S5.T3.2.4.9\" class=\"ltx_td ltx_align_left ltx_border_t\">87.0</td>\n<td id=\"S5.T3.2.4.10\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">84.6</td>\n<td id=\"S5.T3.2.4.11\" class=\"ltx_td ltx_align_left ltx_border_t\">88.4</td>\n</tr>\n<tr id=\"S5.T3.2.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.5.1\" class=\"ltx_td ltx_align_left\">Non-episodic</td>\n<td id=\"S5.T3.2.5.2\" class=\"ltx_td ltx_align_left\">90.9</td>\n<td id=\"S5.T3.2.5.3\" class=\"ltx_td ltx_align_left\">90.6</td>\n<td id=\"S5.T3.2.5.4\" class=\"ltx_td ltx_align_left\">86.1</td>\n<td id=\"S5.T3.2.5.5\" class=\"ltx_td ltx_align_left ltx_border_r\">86.9</td>\n<td id=\"S5.T3.2.5.6\" class=\"ltx_td ltx_align_left ltx_border_r\">88.6</td>\n<td id=\"S5.T3.2.5.7\" class=\"ltx_td ltx_align_left\">91.6</td>\n<td id=\"S5.T3.2.5.8\" class=\"ltx_td ltx_align_left\">91.0</td>\n<td id=\"S5.T3.2.5.9\" class=\"ltx_td ltx_align_left\">85.5</td>\n<td id=\"S5.T3.2.5.10\" class=\"ltx_td ltx_align_left ltx_border_r\">87.9</td>\n<td id=\"S5.T3.2.5.11\" class=\"ltx_td ltx_align_left\">89.0</td>\n</tr>\n<tr id=\"S5.T3.2.6\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.6.1\" class=\"ltx_td ltx_align_left\">ProtoNet</td>\n<td id=\"S5.T3.2.6.2\" class=\"ltx_td ltx_align_left\">89.7</td>\n<td id=\"S5.T3.2.6.3\" class=\"ltx_td ltx_align_left\">90.2</td>\n<td id=\"S5.T3.2.6.4\" class=\"ltx_td ltx_align_left\">86.6</td>\n<td id=\"S5.T3.2.6.5\" class=\"ltx_td ltx_align_left ltx_border_r\">85.2</td>\n<td id=\"S5.T3.2.6.6\" class=\"ltx_td ltx_align_left ltx_border_r\">87.9</td>\n<td id=\"S5.T3.2.6.7\" class=\"ltx_td ltx_align_left\">90.7</td>\n<td id=\"S5.T3.2.6.8\" class=\"ltx_td ltx_align_left\">92.0</td>\n<td id=\"S5.T3.2.6.9\" class=\"ltx_td ltx_align_left\">86.7</td>\n<td id=\"S5.T3.2.6.10\" class=\"ltx_td ltx_align_left ltx_border_r\">84.0</td>\n<td id=\"S5.T3.2.6.11\" class=\"ltx_td ltx_align_left\">88.4</td>\n</tr>\n<tr id=\"S5.T3.2.7\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.7.1\" class=\"ltx_td ltx_align_left\">foMAML</td>\n<td id=\"S5.T3.2.7.2\" class=\"ltx_td ltx_align_left\">88.3</td>\n<td id=\"S5.T3.2.7.3\" class=\"ltx_td ltx_align_left\">90.5</td>\n<td id=\"S5.T3.2.7.4\" class=\"ltx_td ltx_align_left\">86.8</td>\n<td id=\"S5.T3.2.7.5\" class=\"ltx_td ltx_align_left ltx_border_r\">88.1</td>\n<td id=\"S5.T3.2.7.6\" class=\"ltx_td ltx_align_left ltx_border_r\">88.4</td>\n<td id=\"S5.T3.2.7.7\" class=\"ltx_td ltx_align_left\">91.4</td>\n<td id=\"S5.T3.2.7.8\" class=\"ltx_td ltx_align_left\">92.5</td>\n<td id=\"S5.T3.2.7.9\" class=\"ltx_td ltx_align_left\">88.0</td>\n<td id=\"S5.T3.2.7.10\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S5.T3.2.7.10.1\" class=\"ltx_text ltx_font_bold\">90.4</span></td>\n<td id=\"S5.T3.2.7.11\" class=\"ltx_td ltx_align_left\">90.6</td>\n</tr>\n<tr id=\"S5.T3.2.8\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.8.1\" class=\"ltx_td ltx_align_left\">foProtoMAMLn</td>\n<td id=\"S5.T3.2.8.2\" class=\"ltx_td ltx_align_left\">89.0</td>\n<td id=\"S5.T3.2.8.3\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.8.3.1\" class=\"ltx_text ltx_font_bold\">91.1</span></td>\n<td id=\"S5.T3.2.8.4\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.8.4.1\" class=\"ltx_text ltx_font_bold\">87.3</span></td>\n<td id=\"S5.T3.2.8.5\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S5.T3.2.8.5.1\" class=\"ltx_text ltx_font_bold\">88.8</span></td>\n<td id=\"S5.T3.2.8.6\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S5.T3.2.8.6.1\" class=\"ltx_text ltx_font_bold\">89.1</span></td>\n<td id=\"S5.T3.2.8.7\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.8.7.1\" class=\"ltx_text ltx_font_bold\">92.0</span></td>\n<td id=\"S5.T3.2.8.8\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.8.8.1\" class=\"ltx_text ltx_font_bold\">93.1</span></td>\n<td id=\"S5.T3.2.8.9\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.8.9.1\" class=\"ltx_text ltx_font_bold\">88.6</span></td>\n<td id=\"S5.T3.2.8.10\" class=\"ltx_td ltx_align_left ltx_border_r\">89.8</td>\n<td id=\"S5.T3.2.8.11\" class=\"ltx_td ltx_align_left\"><span id=\"S5.T3.2.8.11.1\" class=\"ltx_text ltx_font_bold\">90.9</span></td>\n</tr>\n<tr id=\"S5.T3.2.9\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.9.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Reptile</td>\n<td id=\"S5.T3.2.9.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">88.1</td>\n<td id=\"S5.T3.2.9.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">87.9</td>\n<td id=\"S5.T3.2.9.4\" class=\"ltx_td ltx_align_left ltx_border_bb\">86.8</td>\n<td id=\"S5.T3.2.9.5\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">87.5</td>\n<td id=\"S5.T3.2.9.6\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">87.6</td>\n<td id=\"S5.T3.2.9.7\" class=\"ltx_td ltx_align_left ltx_border_bb\">90.6</td>\n<td id=\"S5.T3.2.9.8\" class=\"ltx_td ltx_align_left ltx_border_bb\">91.7</td>\n<td id=\"S5.T3.2.9.9\" class=\"ltx_td ltx_align_left ltx_border_bb\">87.3</td>\n<td id=\"S5.T3.2.9.10\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">86.2</td>\n<td id=\"S5.T3.2.9.11\" class=\"ltx_td ltx_align_left ltx_border_bb\">89.0</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "Tables 2 and 3 show the accuracy scores on the target languages on MLDoc and Amazon respectively. We start by noting the strong multilingual capabilities of XLM-RoBERTa as our base-learner:\nAdding the full training datasets in three extra languages (i.e., comparing the zero-shot with the non-episodic baseline in the high-resource, ‘Included’ setting) results in a mere 1.2% points increase in accuracy on average for MLDoc and 0.6% points for Amazon.\nAlthough the zero-shot333The zero-shot baseline is only applicable in the ‘Included’ setting, as the English data is not available under ‘Excluded’. and non-episodic baselines are strong, in the majority of cases, a meta-learning approach improves performance. This holds especially for our version of ProtoMAML (ProtoMAMLn), which achieves the highest average accuracy in all considered settings.",
            "The substantial improvements for Russian on MLDoc and Chinese on Amazon indicate that meta-learning is most advantageous when the considered task distribution is somewhat heterogeneous or, in other words, when domain drift Lai et al. (2019) is present. For the Chinese data used for the sentiment polarity task, the presence of domain drift is obvious as the data is collected from a different website and concerns different products than the other languages. For Russian in the MLDoc dataset, it holds that the non-episodic baseline has the smallest gain in performance when adding English data (ls​r​csubscript𝑙𝑠𝑟𝑐l_{src}) in the limited-resource setting (0.2% absolute gain as opposed to 5.7% on average for the remaining languages) and even a decrease of 2.4% points when adding English data in the high-resource setting. Especially for these languages with domain drift, our version of ProtoMAML (foProtoMAMLn) outperforms the non-episodic baselines with a relatively large margin. For instance, in Table 2 in the high-resource setting with English included during training, foProtoMAMLn improves over the non-episodic baseline with 9.1% points whereas the average gain over the remaining languages is 0.9% points. A similar trend can be seen in Table 3 where, in the limited-resource setting, foProtoMAMLn outperforms the non-episodic baseline with 1.9% points on Chinese, with comparatively smaller gains on average for the remaining languages.",
            "In this setting, we achieve a new state of the art on MLDoc for German, Italian, Japanese and Russian using our method, foProtoMAMLn (Table 4).444The zero-shot baselines are the same as in Tables 2 and 3. The previous state of the art for German and Russian is held by Lai et al. (2019) (95.73% and 84.65% respectively).\nFor Japanese and Italian, it is held by Eisenschlos et al. (2019) (80.55% and 80.12% respectively). The state of the art for French and Chinese is also held by Lai et al. (2019) (96.05% and 93.32% respectively).\nOn the Amazon dataset, foProtoMAMLn also outperforms all other methods on average. The state of the art is held by Lai et al. (2019) with 93.3%, 94.2% and 90.6% for French, German and Chinese respectively and, although we do not outperform it, the differences are rather small – between 0.2% (Chinese) and 3.4% points (German) – even when grid search is based on MLDoc, while we use a much less computationally expensive approach."
        ]
    },
    "S5.T4": {
        "caption": "Table 4: Average accuracy of 5 different seeds on the target languages in the joint-training setting for MLDoc and Amazon. ΔΔ\\Delta corresponds to the average accuracy across test languages.",
        "table": null,
        "footnotes": [],
        "references": [
            "In this setting, we achieve a new state of the art on MLDoc for German, Italian, Japanese and Russian using our method, foProtoMAMLn (Table 4).444The zero-shot baselines are the same as in Tables 2 and 3. The previous state of the art for German and Russian is held by Lai et al. (2019) (95.73% and 84.65% respectively).\nFor Japanese and Italian, it is held by Eisenschlos et al. (2019) (80.55% and 80.12% respectively). The state of the art for French and Chinese is also held by Lai et al. (2019) (96.05% and 93.32% respectively).\nOn the Amazon dataset, foProtoMAMLn also outperforms all other methods on average. The state of the art is held by Lai et al. (2019) with 93.3%, 94.2% and 90.6% for French, German and Chinese respectively and, although we do not outperform it, the differences are rather small – between 0.2% (Chinese) and 3.4% points (German) – even when grid search is based on MLDoc, while we use a much less computationally expensive approach."
        ]
    },
    "S6.T5": {
        "caption": "Table 5: Average accuracy of 5 different seeds on unseen target languages using the original/unnormalized foProtoMAML model. Diff is the difference in average accuracy ΔΔ\\Delta across languages against foProtoMAMLn.",
        "table": null,
        "footnotes": [],
        "references": [
            "Figure 1 shows the development of the validation accuracy during training for 25 epochs for the original foProtoMAML and our model, foProtoMAMLn. By applying L2subscript𝐿2L_{2} normalization to the prototypes, we obtain a more stable version of foProtoMAML which empirically converges faster. We furthermore re-run the high-resource experiments with English for both MLDoc and Amazon using the original foProtoMAML (Table 5) and find it performs 4.3% and 1.7% accuracy points worse on average, respectively, further demonstrating the effectiveness of our approach."
        ]
    },
    "S6.T6": {
        "caption": "Table 6: Average accuracy of 5 different seeds on unseen target languages for Amazon when initializing from monolingual classifier in ls​r​csubscript𝑙𝑠𝑟𝑐l_{src}. Diff: difference in average accuracy ΔΔ\\Delta across languages compared to initializing from the XLM-RoBERTa language model.",
        "table": "<table id=\"S6.T6.5\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S6.T6.5.1\" class=\"ltx_tr\">\n<td id=\"S6.T6.5.1.1\" class=\"ltx_td ltx_align_center ltx_border_tt\" rowspan=\"2\"><span id=\"S6.T6.5.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></td>\n<td id=\"S6.T6.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S6.T6.5.1.2.1\" class=\"ltx_text ltx_font_bold\">Limited-resource setting</span></td>\n<td id=\"S6.T6.5.1.3\" class=\"ltx_td ltx_border_r ltx_border_tt\"/>\n<td id=\"S6.T6.5.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"4\"><span id=\"S6.T6.5.1.4.1\" class=\"ltx_text ltx_font_bold\">High-resource setting</span></td>\n<td id=\"S6.T6.5.1.5\" class=\"ltx_td ltx_border_tt\"/>\n</tr>\n<tr id=\"S6.T6.5.2\" class=\"ltx_tr\">\n<td id=\"S6.T6.5.2.1\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T6.5.2.1.1\" class=\"ltx_text ltx_font_bold\">de</span></td>\n<td id=\"S6.T6.5.2.2\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T6.5.2.2.1\" class=\"ltx_text ltx_font_bold\">fr</span></td>\n<td id=\"S6.T6.5.2.3\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T6.5.2.3.1\" class=\"ltx_text ltx_font_bold\">ja</span></td>\n<td id=\"S6.T6.5.2.4\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S6.T6.5.2.4.1\" class=\"ltx_text ltx_font_bold\">zh</span></td>\n<td id=\"S6.T6.5.2.5\" class=\"ltx_td ltx_align_left ltx_border_r\">Diff</td>\n<td id=\"S6.T6.5.2.6\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T6.5.2.6.1\" class=\"ltx_text ltx_font_bold\">de</span></td>\n<td id=\"S6.T6.5.2.7\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T6.5.2.7.1\" class=\"ltx_text ltx_font_bold\">fr</span></td>\n<td id=\"S6.T6.5.2.8\" class=\"ltx_td ltx_align_left\"><span id=\"S6.T6.5.2.8.1\" class=\"ltx_text ltx_font_bold\">ja</span></td>\n<td id=\"S6.T6.5.2.9\" class=\"ltx_td ltx_align_left ltx_border_r\"><span id=\"S6.T6.5.2.9.1\" class=\"ltx_text ltx_font_bold\">zh</span></td>\n<td id=\"S6.T6.5.2.10\" class=\"ltx_td ltx_align_left\">Diff</td>\n</tr>\n<tr id=\"S6.T6.5.3\" class=\"ltx_tr\">\n<td id=\"S6.T6.5.3.1\" class=\"ltx_td ltx_align_left ltx_border_t\">ProtoNet</td>\n<td id=\"S6.T6.5.3.2\" class=\"ltx_td ltx_align_left ltx_border_t\">91.1</td>\n<td id=\"S6.T6.5.3.3\" class=\"ltx_td ltx_align_left ltx_border_t\">90.9</td>\n<td id=\"S6.T6.5.3.4\" class=\"ltx_td ltx_align_left ltx_border_t\">87.1</td>\n<td id=\"S6.T6.5.3.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">85.5</td>\n<td id=\"S6.T6.5.3.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">+0.75</td>\n<td id=\"S6.T6.5.3.7\" class=\"ltx_td ltx_align_left ltx_border_t\">91.3</td>\n<td id=\"S6.T6.5.3.8\" class=\"ltx_td ltx_align_left ltx_border_t\">91.1</td>\n<td id=\"S6.T6.5.3.9\" class=\"ltx_td ltx_align_left ltx_border_t\">87.4</td>\n<td id=\"S6.T6.5.3.10\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\">88.7</td>\n<td id=\"S6.T6.5.3.11\" class=\"ltx_td ltx_align_left ltx_border_t\">+1.44</td>\n</tr>\n<tr id=\"S6.T6.5.4\" class=\"ltx_tr\">\n<td id=\"S6.T6.5.4.1\" class=\"ltx_td ltx_align_left\">foMAML</td>\n<td id=\"S6.T6.5.4.2\" class=\"ltx_td ltx_align_left\">90.8</td>\n<td id=\"S6.T6.5.4.3\" class=\"ltx_td ltx_align_left\">87.4</td>\n<td id=\"S6.T6.5.4.4\" class=\"ltx_td ltx_align_left\">87.3</td>\n<td id=\"S6.T6.5.4.5\" class=\"ltx_td ltx_align_left ltx_border_r\">85.2</td>\n<td id=\"S6.T6.5.4.6\" class=\"ltx_td ltx_align_left ltx_border_r\">-0.75</td>\n<td id=\"S6.T6.5.4.7\" class=\"ltx_td ltx_align_left\">91.7</td>\n<td id=\"S6.T6.5.4.8\" class=\"ltx_td ltx_align_left\">91.2</td>\n<td id=\"S6.T6.5.4.9\" class=\"ltx_td ltx_align_left\">87.2</td>\n<td id=\"S6.T6.5.4.10\" class=\"ltx_td ltx_align_left ltx_border_r\">88.1</td>\n<td id=\"S6.T6.5.4.11\" class=\"ltx_td ltx_align_left\">-1.13</td>\n</tr>\n<tr id=\"S6.T6.5.5\" class=\"ltx_tr\">\n<td id=\"S6.T6.5.5.1\" class=\"ltx_td ltx_align_left\">foProtoMAMLn</td>\n<td id=\"S6.T6.5.5.2\" class=\"ltx_td ltx_align_left\">87.7</td>\n<td id=\"S6.T6.5.5.3\" class=\"ltx_td ltx_align_left\">87.8</td>\n<td id=\"S6.T6.5.5.4\" class=\"ltx_td ltx_align_left\">83.9</td>\n<td id=\"S6.T6.5.5.5\" class=\"ltx_td ltx_align_left ltx_border_r\">84.4</td>\n<td id=\"S6.T6.5.5.6\" class=\"ltx_td ltx_align_left ltx_border_r\">-3.1</td>\n<td id=\"S6.T6.5.5.7\" class=\"ltx_td ltx_align_left\">90.8</td>\n<td id=\"S6.T6.5.5.8\" class=\"ltx_td ltx_align_left\">89.8</td>\n<td id=\"S6.T6.5.5.9\" class=\"ltx_td ltx_align_left\">86.2</td>\n<td id=\"S6.T6.5.5.10\" class=\"ltx_td ltx_align_left ltx_border_r\">82.3</td>\n<td id=\"S6.T6.5.5.11\" class=\"ltx_td ltx_align_left\">-3.96</td>\n</tr>\n<tr id=\"S6.T6.5.6\" class=\"ltx_tr\">\n<td id=\"S6.T6.5.6.1\" class=\"ltx_td ltx_align_left ltx_border_bb\">Reptile</td>\n<td id=\"S6.T6.5.6.2\" class=\"ltx_td ltx_align_left ltx_border_bb\">89.3</td>\n<td id=\"S6.T6.5.6.3\" class=\"ltx_td ltx_align_left ltx_border_bb\">90.2</td>\n<td id=\"S6.T6.5.6.4\" class=\"ltx_td ltx_align_left ltx_border_bb\">86.7</td>\n<td id=\"S6.T6.5.6.5\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">85.5</td>\n<td id=\"S6.T6.5.6.6\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">+0.35</td>\n<td id=\"S6.T6.5.6.7\" class=\"ltx_td ltx_align_left ltx_border_bb\">90.0</td>\n<td id=\"S6.T6.5.6.8\" class=\"ltx_td ltx_align_left ltx_border_bb\">89.3</td>\n<td id=\"S6.T6.5.6.9\" class=\"ltx_td ltx_align_left ltx_border_bb\">87.1</td>\n<td id=\"S6.T6.5.6.10\" class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_r\">85.7</td>\n<td id=\"S6.T6.5.6.11\" class=\"ltx_td ltx_align_left ltx_border_bb\">-1.04</td>\n</tr>\n</table>\n",
        "footnotes": [],
        "references": [
            "In our experiments, we often assume the presence of a source language (English). We now investigate (in the ls​r​csubscript𝑙𝑠𝑟𝑐l_{src} = en ‘Excluded’ setting) whether it is beneficial to pre-train the base-learner in a standard supervised way on this source language and use the obtained checkpoint θm​o​n​osubscript𝜃𝑚𝑜𝑛𝑜\\theta_{mono} as an initialization for meta-training (Table 6) rather than initializing from the transformer checkpoint."
        ]
    }
}