{
    "PAPER'S NUMBER OF TABLES": 1,
    "S3.T1": {
        "caption": "Table 1: Models in rows 1-3 are found in the literature. Models in rows 4,5 were designed by us taking into account the size factor. Model2 has 3 convolution layers and 2 fully connected layers. Model1 has a Global MaxPool layer (denoted as GMPool) in between the convolution and fully connected layers which drastically reduces the model size. Sixth row is Model2 trained in an FL way involving 10 devices.",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\">#</th>\n<td id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\">Dataset</td>\n<td id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\">Method</td>\n<td id=\"S3.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\">Accuracy</td>\n<td id=\"S3.T1.1.1.1.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_tt\">Size(MB)</td>\n</tr>\n<tr id=\"S3.T1.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">1</th>\n<td id=\"S3.T1.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_t\">ImageNet(pretrain)+IMDB-WIKI( fine tuning) Adience(testing)</td>\n<td id=\"S3.T1.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_t\">AL-ResNets-34 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib37\" title=\"\" class=\"ltx_ref\">37</a>]</cite>\n</td>\n<td id=\"S3.T1.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_t\">66.03</td>\n<td id=\"S3.T1.1.2.2.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\"> 87.3</td>\n</tr>\n<tr id=\"S3.T1.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.3.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">2</th>\n<td id=\"S3.T1.1.3.3.2\" class=\"ltx_td ltx_align_center\">ImageNet(pretrain)+Adience(training and testing)</td>\n<td id=\"S3.T1.1.3.3.3\" class=\"ltx_td ltx_align_center\">RESF-EMD <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib38\" title=\"\" class=\"ltx_ref\">38</a>]</cite>\n</td>\n<td id=\"S3.T1.1.3.3.4\" class=\"ltx_td ltx_align_center\">62.2</td>\n<td id=\"S3.T1.1.3.3.5\" class=\"ltx_td ltx_nopad_r ltx_align_center\"> 103</td>\n</tr>\n<tr id=\"S3.T1.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.4.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">3</th>\n<td id=\"S3.T1.1.4.4.2\" class=\"ltx_td ltx_align_center\">ImageNet(pretrain)+IMDB-WIKI( fine tuning) Adience(testing)</td>\n<td id=\"S3.T1.1.4.4.3\" class=\"ltx_td ltx_align_center\">VGGF-DEX with IMDB-WIKI <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib39\" title=\"\" class=\"ltx_ref\">39</a>]</cite>\n</td>\n<td id=\"S3.T1.1.4.4.4\" class=\"ltx_td ltx_align_center\">64</td>\n<td id=\"S3.T1.1.4.4.5\" class=\"ltx_td ltx_nopad_r ltx_align_center\"> 553</td>\n</tr>\n<tr id=\"S3.T1.1.5.5\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.5.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\">4</th>\n<td id=\"S3.T1.1.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_t\">IMDB-WIKI <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib40\" title=\"\" class=\"ltx_ref\">40</a>]</cite> dataset</td>\n<td id=\"S3.T1.1.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_t\">Model1 (4Conv+GMPool+ 2FC)</td>\n<td id=\"S3.T1.1.5.5.4\" class=\"ltx_td ltx_align_center ltx_border_t\">46.83</td>\n<td id=\"S3.T1.1.5.5.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_t\">1.61</td>\n</tr>\n<tr id=\"S3.T1.1.6.6\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.6.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row\">5</th>\n<td id=\"S3.T1.1.6.6.2\" class=\"ltx_td ltx_align_center\">IMDB-WIKI dataset</td>\n<td id=\"S3.T1.1.6.6.3\" class=\"ltx_td ltx_align_center\">Model2 (3Conv+2FC)</td>\n<td id=\"S3.T1.1.6.6.4\" class=\"ltx_td ltx_align_center\">49.34</td>\n<td id=\"S3.T1.1.6.6.5\" class=\"ltx_td ltx_nopad_r ltx_align_center\">24.03</td>\n</tr>\n<tr id=\"S3.T1.1.7.7\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.7.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\">6</th>\n<td id=\"S3.T1.1.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_bb\">IMDB-WIKI dataset</td>\n<td id=\"S3.T1.1.7.7.3\" class=\"ltx_td ltx_align_center ltx_border_bb\">Model2 FL</td>\n<td id=\"S3.T1.1.7.7.4\" class=\"ltx_td ltx_align_center ltx_border_bb\">45.23</td>\n<td id=\"S3.T1.1.7.7.5\" class=\"ltx_td ltx_nopad_r ltx_align_center ltx_border_bb\">24.03</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Take the example of image classification. Table 1 shows the performance and model sizes of various age prediction models from images. Usually, age prediction is considered as a classification problem, where the intention is to classify subjects into different age buckets. The purpose of table is not a comparison of models, but to understand the accuracy levels achieved by different models of different complexities (sizes). As we see, complex models evidently perform well for the task, however the models may not be adopted for federated learning as the communication costs would be immensely high. For instance, using the assumptions mentioned in section 5, the training cost for low performing, low sized models, Model1 and Model2 in table 1 would be $1111currency-dollar1111\\$1111 and $6290currency-dollar6290\\$6290 respectively. For heavy models AL-ResNets-34, RESF-EMD, VGGF-DEX, the training costs are $20905,$24532, and ​$128482currency-dollar20905currency-dollar24532 and currency-dollar128482\\$20905,\\$24532,\\mbox{ and }\\$128482 respectively! In addition there would also be a huge deployment cost, that would be directly dependent on the model size."
        ]
    }
}