{
    "S4.T1": {
        "caption": "Table 1: Benefit of meta task modulation in (%) on three few-task meta-learning challenges. Our meta task modulation (MTM) achieves better performance compared to a vanilla ProtoNet.",
        "table": null,
        "footnotes": [],
        "references": [
            "Benefit of meta task modulation.\nTo show the benefit of meta task modulation, we first compare our method with a vanilla Prototypical network (Snell et al., 2017) on all tasks, without using task interpolation, in Table 1. Our model performs better under various shot configurations on all few-task meta-learning benchmarks. We then compare our model with the state-of-the-art MLTI (Yao et al., 2021b) in Table 5, which interpolates the task distribution by Mixup (Verma et al., 2019).\nOur meta task modulation also compares favorably to MLTI under various shot configurations. On ISIC, for example, we surpass MLTI by 2.71%percent2.712.71\\% on the 5-way 5-shot setting. This is because our model can learn how to modulate the base task features to better capture the task distribution instead of using linear interpolation as described in the  (Yao et al., 2021b)."
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Benefit of variational task modulation for varying layers on miniImageNet-S. Variational task modulation (VTM) improves over any of the selected individual layers using MTM.",
        "table": null,
        "footnotes": [],
        "references": [
            "Benefit of variational task modulation.\nWe investigate the benefit of variational task modulation by comparing it with deterministic meta task modulation. The results are reported on miniImageNet-S under various shots in Table 2.\n{1st,2nd,3rd,4th}superscript1stsuperscript2ndsuperscript3rdsuperscript4th\\{1^{\\mathrm{st}},2^{\\mathrm{nd}},3^{\\mathrm{rd}},4^{\\mathrm{th}}\\}, random and, all are the selected determined layer, the randomly chosen one layer and all the layers to be modulated, respectively.\nThe variational task modulation consistently outperforms the deterministic meta task modulation on any selected layers, demonstrating the benefit of probabilistic modeling. By using probabilistic task modulation, the base task can be modulated in a more informative way, allowing it to encompass a larger range of task distributions and ultimately improve performance on the meta-test task."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Hierarchical vs. flat variational modulation.\nHierarchical variational task modulation (HVTM) is more effective than flat variational task modulation (VTM) for few-task meta-learning.",
        "table": null,
        "footnotes": [],
        "references": [
            "Hierarchical vs. flat variational task modulation. We compare hierarchical modulation with flat variational modulation, which only selects one layer to modulate. As shown in Table 3, the hierarchical variational modulation improves the overall performance under both the 1-shot and 5-shot settings on all three benchmarks.\nThe hierarchical structure is well-suited for increasing the density of the task distribution across different levels of features, which leads to better performance compared to flat variational modulation. This makes sense because the hierarchical structure allows for more informative transformations of the base task, enabling it to encompass a broader range of task distributions. Note that, we use hierarchical variational task modulation to compare the state-of-the-art methods in the subsequent experiments."
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Cross-domain adaptation ability. MetaModulation achieves better performance even in a challenging cross-domain adaptation setting compared to a vanilla prototype network and MLTI by Yao et al. (2021b).",
        "table": null,
        "footnotes": [],
        "references": [
            "Cross-domain adaptation ability. To further evaluate the effectiveness of our proposed method, we conducted additional tests to assess the performance of MetaModulation in cross-domain adaptation scenarios. We trained MetaModulation on one source domain and then evaluated it on a different target domain. Specifically, we chose the miniImagenet-S and Dermnet-S domains. The results, as shown in Table 4, indicate MetaModulation generalizes better even in this more challenging scenario."
        ]
    },
    "S4.T5": {
        "caption": "Table 5: Comparison with state-of-the-art. All results, except for the MetaInterpolation (Lee et al., 2022), are sourced from MLTI (Yao et al., 2021b). MetaModulation is a consistent top performer for all settings and datasets.",
        "table": null,
        "footnotes": [],
        "references": [
            "Benefit of meta task modulation.\nTo show the benefit of meta task modulation, we first compare our method with a vanilla Prototypical network (Snell et al., 2017) on all tasks, without using task interpolation, in Table 1. Our model performs better under various shot configurations on all few-task meta-learning benchmarks. We then compare our model with the state-of-the-art MLTI (Yao et al., 2021b) in Table 5, which interpolates the task distribution by Mixup (Verma et al., 2019).\nOur meta task modulation also compares favorably to MLTI under various shot configurations. On ISIC, for example, we surpass MLTI by 2.71%percent2.712.71\\% on the 5-way 5-shot setting. This is because our model can learn how to modulate the base task features to better capture the task distribution instead of using linear interpolation as described in the  (Yao et al., 2021b).",
            "Comparison with state-of-the-art.\nWe evaluate MetaModulation on the four different datasets under 5-way 1-shot and 5-way 5-shot in Table 5. Our model achieves state-of-the-art performance on all four few-task meta-learning benchmarks under each setting. On miniImagenet-S, our model achieves 43.21% under 1-shot, surpassing the second-best MLTI (Yao et al., 2021b), by a margin of 1.85%. On ISIC (Milton, 2019), our method delivers 76.40% for 5-shot, outperforming MLTI (Yao et al., 2021b) with 4.88%.\nEven on the most challenging DermNet-S, which forms the largest dermatology dataset, our model delivers 50.45% on the 5-way 1-shot setting. The consistent improvements on all benchmarks under various configurations confirm that our approach is effective for few-task meta-learning."
        ]
    },
    "A3.T6": {
        "caption": "Table 6: Ablation on the λ𝜆\\lambda.",
        "table": null,
        "footnotes": [],
        "references": [
            "We would like to emphasize that the hyper-parameters λ𝜆\\lambda (Eq. 19, 20, 21) enable us to introduce constraints on new tasks, beyond just minimizing prediction loss.\nBy adjusting the value of λ𝜆\\lambda, we can control the trade-off between the prediction loss of the new tasks and the constraints imposed by the meta-training tasks. To clarify the impact of λ𝜆\\lambda, we performed an ablation on the HVTM (Eq. 21).\nThe results in Table 6 show that when the original tasks have higher weight, the performance is worse. Additionally, we have conducted experiments to investigate the distribution differences between the meta-training and generated tasks. Specifically, in Table 6, we analyze the task representations of meta-training and generated tasks and show that they are similar, indicating that the generated tasks do indeed have a similar distribution as the meta-training tasks."
        ]
    }
}