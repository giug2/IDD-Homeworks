{
    "id_table_1": {
        "caption": "Table 1:   Data used for training in different stages. We convert text based data to speech using TTS.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "Developing a full-duplex spoken dialog agent is challenging for four reasons: 1) Understanding and generating turn-taking cues in spoken dialogue requires the model to have a common reference clock with the real-world. However, current LLMs do not have such a sense of time. 2) Compared to text-based chat datasets, spoken dialogue data is limited. A combination of all significant spoken dialogue datasets  Cieri et al. ( 2004 ); Godfrey et al. ( 1992 ); Reece et al. ( 2023 )  would still result in only   similar-to \\sim  3k hours of spoken dialogue data. 3) Full-duplex dialogue entails model to be always listening and should always be ready to speak, because back-channels or overlaps could occur at arbitrary points in time. This requires the model to be streaming for the duration of the dialogue. 4) Since the spoken dialogue agent might run on cloud infrastructure, it must address the fundamental latency inherent in Internet transmissions. Thus, the model may not have immediate access to the current tokens or speech generated by the user and must operate with delayed input (Fig.  1 ).",
            "SyncLLM is an auto-regressive transformer decoder architecture, that natively models discrete speech units in a wall-clock synchronous fashion. SyncLLM is trained to predict interleaving chunks of speech units corresponding to both sides of the dialogue as shown in Fig.  1 . In each time step, the model predicts speech units corresponding to a fixed duration, referred to as the models  chunk size , for its side of the dialogue followed by speech units corresponding to users side of the dialogue. With this approach, the model is capable of generating two streams of speech synchronized with a real-world clock. This allows our method to model all conversational cues such as backchannels, overlaps, interruptions etc. Furthermore, since we use the same architecture as existing LLMs, our approach can leverage large scale pre-training of LLMs.",
            "The model trained to predict interleaved chunks of token sequences can be used for full-duplex voice interaction if we could replace one of the two token streams, with that corresponding to the real-world user. In Fig.  1 , purple boxes correspond to token sequences of the LLMs side of the conversation in each time chunk and the green boxes correspond to the users side of the dialogue. We achieve full-duplex LLM-user voice interaction by discarding the LLMs predictions of users response and replace it with the users speech.",
            "In Fig.  1 , consider the Nth time chunk to be current time step. We could interleave the LLMs output speech chunks until the Nth chunk, with the users input chunks corresponding to only N-1 chunks. The reasoning here is that the users input for the Nth chunk is not available until the end of Nth time step. To handle this intrinsic latency, similar to the way humans anticipate the next few words of what the other person taking part in the dialogue would say  Levinson and Torreira ( 2015b ) , the LLMs output for the next chunk (N+1) is computed by first estimating the users response for the Nth time chunk (depicted in the figure with green boxes with dotted border). We then append this estimated chunk to the LLMs context to generate the LLMs next chunk (N+1). For generating subsequent chunks (N+2, N+3, ...), we discard the estimated users chunk for Nth time step and replace that with the users real-world input, thus grounding the subsequent interaction with actual input from the user.",
            "We evaluate SyncLLM in both continuation and interaction settings. In the continuation setting, given a spoken dialogue prompt, the model generates both sides of the dialogue. For interaction setting, we simulate interaction between two instances of SyncLLM as described in  3.1 . We denote SyncLLM trained on Fisher in continuation setting as SyncLLM-F and use dGSLM as the continuation setting baseline. Both dGSLM and SyncLLM-F use Fisher as the only real-world spoken dialogue dataset for training. We denote SyncLLM trained on Fisher interacting with an instance trained on Fisher as SyncLLM-F-F, and SyncLLM trained on Fisher interacting with an instance trained on CANDOR  Reece et al. ( 2023 )  as SyncLLM-F-C.",
            "We adapt the Mean Opinion Score (MOS) protocol (a 5-pt Likert scale)  ITU-T Recommendation P.808 ( 2018 )  to evaluate  Naturalness  (N-MOS) of turn-taking and  Meaningfulness  (M-MOS) of dialogue content. For both N-MOS and M-MOS, annotators are presented with the prompt- and continuation-audio. Annotators are instructed to first read the descriptions of N-MOS and M-MOS, listen to the prompt audio, then listen to the continuation audio. Finally, they are asked to provide a rating considering the quality of the continuation audio relative to the information contained in the prompt. Each annotator assigned to a given prompt / continuation pair provides a rating for both N-MOS and M-MOS (see  B.1 )."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of Pearson correlation of turn-taking event durations between generations and ground-truth continuations, given same set of prompts. SyncLLMs chunk sizes are shown in parenthesis.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "Following prior works in spoken language modeling  Nguyen et al. ( 2022 ,  2024 ) , we use HuBERT  Hsu et al. ( 2021 )  to represent speech. We use the tokenization parameters from  Nguyen et al. ( 2024 ) , with a token sampling rate of 25 Hz  resulting in one token for every 40 ms of audio  and a vocabulary size of 501. To model dialog between two speakers 0 & 1, we define two special tokens  [S0]  and  [S1] , referred to as speaker tags, specifying the start of each speakers token sequence, respectively. We represent dialogue as two parallel speech streams, one for each speaker, interleaved, as shown in the top row of Fig.  2 . For each stream, we embed a periodic speaker tag, with the time period equal to chunk size of the model.",
            "So, instead, SyncLLM is trained to predict deduplicated HuBERT sequences, with coarse timing information maintained by periodically interleaved special tokens,  [S0]  and  [S1] , as in the second row of Fig.  2 . In the first chunk of the example in Fig.  2 , the two speaker streams contained 4 repetitions of  [75]  and  [89] , respectively. After deduplication, the interleaved token sequence corresponding to the first chunk would be  [S0][75][S1][89] . In the second chunk, speaker 0 has 2 new tokens ( [17]  &  [338] ), but speaker 1 tokens are just a repetition of the last token in the previous chunk,  [89] . So, the second chunks token sequence would just be  [S0][17][338] . Note that when a chunk contains no novel tokens corresponding to speaker 1, we exclude speaker 1s special token  [S1]  as well. However, this is not the case for speaker 0, as we need one of the speakers special token to be present in all chunks to unambiguously distinguish chunks. This is shown in the third chunk of Fig.  2 .",
            "Interpolation.  While deduplicated token sequences are beneficial for auto-regressive modeling, to generate token sequences suitable for speech synthesis, we need periodic HuBERT tokens in the original format. Since the speaker tag  [S0]  maintains the timing information, we know the number of tokens removed after deduplication within each chunk. We use this to interpolate the deduplicated token to match the expected number of token in each chunk. For example, in the first chunk of Fig.  2 , speaker 0s stream only has one token after deduplication. But since chunk size in that case is 160ms, each chunk would contain 160/40 = 4 tokens. So as shown in the third row of Fig.  2 , we repeat the deduplicated token thrice to reconstruct the chunk. If a chunk has multiple deduplicated tokens, like the second in Fig.  2 , we repeat each token by an equal amount. We note this approach could result in an error because the original chunk may not follow this heuristic. We observed that the effect of this is imperceptible even with a chunk size of 240 ms, likely because the error in the predicted duration of each token is upper bounded by the chunk size. Further, in chunks with more novel tokens, the error would be even smaller.",
            "Stage 2: Full-duplex dialogue assuming no overlaps.  Turn-based spoken dialogue is special case of full-duplex dialogue with no overlaps. Based on this observation, we could treat synthetic spoken dialogue data as full-duplex spoken dialogue data where during one speakers turn, other speaker is completely silent. In this stage, we create synthetic spoken dialogue data from text-dialogue data similarly to the previous stage with one main difference: From each turn in the dialogue, we generate a speech utterance corresponding to one speaker and silence of equal duration corresponding to the other speaker. We then tokenize the parallel speech dialog data in the format shown in the second row of Fig.  2 . This way, we can further leverage text-dialogue data for help our model learn the token sequence format in Fig.  2 . This stage of finetuning models timing within an utterance. The model cannot learn turn-taking cues such as back-channeling or overlaps between two speakers yet.",
            "Table.  2  compares this correlation with in-distribution Fisher  Cieri et al. ( 2004 )  test-split and out-of-distribution Candor test-split. We observe that, generations with our models achieve better turn-taking event correlation with ground-truth continuations compared to dGSLM for both in-distribution and out-of-distribution testsets. In addition to this, we provide turn-taking event correlation with prompts and re-synthesized ground-truth continuations (Resynth-GT). Resynth-GT is obtained by re-synthesizing the tokenized ground-truth continuation. Resynth-GT does not perfectly correlate with ground-truth owing to variance in timing introduced by the tokenization process, and serves as a topline for our method.",
            "In Fig.  8 , we compare the performance in the interaction setting with different latencies. We find that our method is robust to a latency as much as 200 ms, but the performance drops with latency greater than that. Similar to our naturalness evaluation in the continuation setting in  5.2 , to evaluate turn-taking capability of SyncLLM in interaction setting, we compare Pearson correlation of the duration of turn-taking events in generation and ground-truth continuations. In Table  6 , we observe that on a combined test set of in-distribution and out-of-distribution prompts, performance in the interaction setting closely matches with latencies 160 ms and 200 ms, but drops with 240 ms.",
            "Similar to the naturalness evaluation in Table  2 , where we consider ground-truth continuation as the reference for turn-taking event statistics, we could also consider prompt as the reference. In a way, this measures style consistency between prompt and the continuation. In Table  7 , we compare turn-taking event correlation of generations of our method in continuation setting, with that of dGSLM method. We observed that our method demonstrates better turn-taking correlation with the prompts as well."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Meaningfulness (Meaning.) and Naturalness (Nat.) (scores 1-5) mean estimates and standard errors (in parentheses), aggregated overall and for Fisher and CANDOR subsets. We use a 160ms chunk size for this study.",
        "table": "S5.T3.6",
        "footnotes": [],
        "references": [
            "Deduplication.  The fixed time period of HuBERT tokens is useful for modeling time in the full-duplex dialogue. However, raw HuBERT sequences consist of significant repeated tokens, mainly caused by silence within and across utterances. The number of repetitions of each unique token denote the duration of the acoustic unit represented by the token. The semantic content, however, can be modeled by only considering unique tokens while deduplicating the token sequence  Kharitonov et al. ( 2022 ); Nguyen et al. ( 2022 ) . Duplicate token sequences can adversely affect the semantic capability of the final spoken dialogue model  Nguyen et al. ( 2022 ) , because as shown in Fig.  3 , they contain   50 % similar-to absent percent 50 \\sim 50\\%  50 %  lower semantic content per token compared to deduplicated sequences.",
            "We evaluate SyncLLM in both continuation and interaction settings. In the continuation setting, given a spoken dialogue prompt, the model generates both sides of the dialogue. For interaction setting, we simulate interaction between two instances of SyncLLM as described in  3.1 . We denote SyncLLM trained on Fisher in continuation setting as SyncLLM-F and use dGSLM as the continuation setting baseline. Both dGSLM and SyncLLM-F use Fisher as the only real-world spoken dialogue dataset for training. We denote SyncLLM trained on Fisher interacting with an instance trained on Fisher as SyncLLM-F-F, and SyncLLM trained on Fisher interacting with an instance trained on CANDOR  Reece et al. ( 2023 )  as SyncLLM-F-C.",
            "Overall results.  The two left-most columns of Table.  3  indicate that nearly all models are at parity in perceived  Naturalness  (N-MOS) of turn-taking, while close to re-synthesized ground-truth values. On the perceived  Meaningfulness  (M-MOS) of the dialogue content, SyncLLM-based models significantly outperform dGSLM, approaching re-synthesized ground-truth values. Resynth-GT here accounts for the tokenization process and is the topline number for the implementation of our method using the HuBERT tokenizer.",
            "In-distribution and OOD.  Table.  3  also highlights the difference between in-distribution (Fisher) and OOD (CANDOR) between dGSLM and Fisher-trained SyncLLM-F. While dGSLM suffers from significant degradation OOD (dropping -0.24 and -0.51 in M-MOS and N-MOS ratings), these declines are reduced in SyncLLM-F only dropping -0.15 and -0.16 moving OOD. SyncLLM trained on CANDOR dataset (SyncLLM-C) shows a decline OOD on M-MOS (-0.52), but not N-MOS (+0.03). We note that dGSLM  Nguyen et al. ( 2022 )  uses speech representations fine-tuned on the Fisher dataset, while our method uses general-purpose speech representations for all domains of speech. This results in our method outperforming the baseline on the out-of-distribution Candor testset in naturalness, as judged by human evaluators in Table.  3 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Human evaluation results for Meaningfulness (Meaning.) and Naturalness (Nat.) mean estimates and standard errors (in parentheses) across all data.",
        "table": "S5.T4.2",
        "footnotes": [],
        "references": [
            "Each sentence is randomly chosen to either be text or deduplicated speech token sequences during training. For each training sample, we sample the percentage of speech sentences in the training sequence from the truncated normal distribution (Fig.  4 ). Training only with fully speech sequences or step-wise increment of speech percentage resulted in unstable training. Sentence level text-speech interleaving not only trains the model to be capable of performing dialog, but also achieves text/speech alignment in the context of dialog.",
            "Human evaluation.  Table.  4  shows ratings for dGSLM, the Fisher-trained continuation model, and LLM-LLM interactions. Results corroborate findings in  5.4   LLM-LLM interactions outperform dGSLM on M-MOS, but are slightly worse compared to the single model continuation setting."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Ablation evaluations over interleaving level.  WUGGY, BLIMP, Topic-StoryCloze, and StoryCloze assess the knowledge and capacity of the model in lexical, syntactical, and semantic levels respectively. We report the accuracy based on negative-log-likelihood  normalized by the number of tokens  minimization prediction. The tasks are evaluated in the zero-shot setting.",
        "table": "A1.T5.4",
        "footnotes": [],
        "references": [
            "Fig.  5  compares the semantic quality of spoken dialogues generated by SyncLLM with different chunk sizes to the prior state-of-the-art full-duplex dGSLM model  Nguyen et al. ( 2022 )  and ground-truth continuations. We find that dGSLM has a perplexity drop of   similar-to \\sim  70 relative to the ground-truth, while SyncLLM only has a drop of   similar-to \\sim  15. Fig.  6  also compares median perplexities measured with prompts sampled from Fisher and Candor test splits separately, with all models trained only on Fisher training split. Here, Candor test split is an out-of-distribution testset.",
            "Human evaluation.  Table.  4  shows ratings for dGSLM, the Fisher-trained continuation model, and LLM-LLM interactions. Results corroborate findings in  5.4   LLM-LLM interactions outperform dGSLM on M-MOS, but are slightly worse compared to the single model continuation setting.",
            "We explore two text-speech interleaving strategies in stage 1 of our training: i) Sentence-level interleaving: each sentence is chosen randomly to be either text modality or speech modality. ii) Turn-level interleaving: each turn is chosen randomly to be either text modality or speech modality, resulting in consistent modality for all the sentences within the turn. We compare them by evaluating on a set of spoken language understanding benchmarks proposed in  Nguyen et al. ( 2020 ) . We report these results in Table  5 . On these tasks, we observe that sentence-level interleaving outperforms turn-level interleaving across all benchmarks.",
            "In Fig.  8 , we compare the performance in the interaction setting with different latencies. We find that our method is robust to a latency as much as 200 ms, but the performance drops with latency greater than that. Similar to our naturalness evaluation in the continuation setting in  5.2 , to evaluate turn-taking capability of SyncLLM in interaction setting, we compare Pearson correlation of the duration of turn-taking events in generation and ground-truth continuations. In Table  6 , we observe that on a combined test set of in-distribution and out-of-distribution prompts, performance in the interaction setting closely matches with latencies 160 ms and 200 ms, but drops with 240 ms."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Comparison of average Pearson correlation of turn-taking event durations between generation and ground-truth continuation with SyncLLM in the two-model interaction setting. Measured on testsets comprising both Fisher and Candor testsets.",
        "table": "A1.T6.1",
        "footnotes": [],
        "references": [
            "Fig.  5  compares the semantic quality of spoken dialogues generated by SyncLLM with different chunk sizes to the prior state-of-the-art full-duplex dGSLM model  Nguyen et al. ( 2022 )  and ground-truth continuations. We find that dGSLM has a perplexity drop of   similar-to \\sim  70 relative to the ground-truth, while SyncLLM only has a drop of   similar-to \\sim  15. Fig.  6  also compares median perplexities measured with prompts sampled from Fisher and Candor test splits separately, with all models trained only on Fisher training split. Here, Candor test split is an out-of-distribution testset.",
            "In Fig.  8 , we compare the performance in the interaction setting with different latencies. We find that our method is robust to a latency as much as 200 ms, but the performance drops with latency greater than that. Similar to our naturalness evaluation in the continuation setting in  5.2 , to evaluate turn-taking capability of SyncLLM in interaction setting, we compare Pearson correlation of the duration of turn-taking events in generation and ground-truth continuations. In Table  6 , we observe that on a combined test set of in-distribution and out-of-distribution prompts, performance in the interaction setting closely matches with latencies 160 ms and 200 ms, but drops with 240 ms."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Comparison of Pearson correlation of turn-taking event durations between prompt and generation.",
        "table": "A1.T7.1",
        "footnotes": [],
        "references": [
            "In Fig.  7 , we compare median perplexities obtained with prompts sampled from Fisher and Candor test splits. We also show the perplexity of ground-truth and samples generated in the dialog continuation setting for reference. We find that SyncLLM in the LLM-LLM interaction setting is able to closely match the performance of the continuation setting, and perform significantly better than dGSLM in continuation setting. Furthermore, we find that interaction between instances of SyncLLM trained with Fisher and Candor datasets, respectively is are almost the same signifying that SyncLLM can perform a coherent conversation even when users side of the conversation is generated by a model trained with a different dataset.",
            "Similar to the naturalness evaluation in Table  2 , where we consider ground-truth continuation as the reference for turn-taking event statistics, we could also consider prompt as the reference. In a way, this measures style consistency between prompt and the continuation. In Table  7 , we compare turn-taking event correlation of generations of our method in continuation setting, with that of dGSLM method. We observed that our method demonstrates better turn-taking correlation with the prompts as well."
        ]
    }
}