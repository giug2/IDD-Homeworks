{
    "S6.T1": {
        "caption": "TABLE I: Comparison of various PAC-Bayes bounds in training phase in both shuffled pixels and permuted labels environment with different prior model.",
        "table": null,
        "footnotes": [],
        "references": [
            "We also compare five meta-learning PAC-Bayes bounds on two different MNIST environments. Those consist of meta-learning McAllester PAC-Bayes bound (fclassicsubscript𝑓classicf_{\\rm classic}), meta-learning Seeger PAC-Bayes bound (fSeegersubscript𝑓Seegerf_{\\rm Seeger}), meta-learning PAC-Bayes λ𝜆\\lambda bound (fλsubscript𝑓𝜆f_{\\lambda}), meta-learning PAC-Bayes quadratic bound (fquadsubscript𝑓quadf_{\\rm quad}) and meta-learning PAC-Bayes variational bound (fvariasubscript𝑓variaf_{\\rm varia}). As shown in Table I, the performance of various training objectives in the training phase with a random prior model is analyzed, in terms of bound, task complexity, meta complexity, empirical loss and estimated error. Figure 3 demonstrates that the proposed three meta-learning PAC-Bayes achieve a competitive bound, especially for the PAC-Bayes variational bound.",
            "Furthermore, five meta-learning training objectives on two different MNIST environments are substantiated (See Table I and Table II). As shown in Figure 3 and Figure 3, comparison between two classical meta-learning PAC-Bayes bounds, the proposed meta-learning PAC-Bayes λ𝜆\\lambda bound and meta-learning PAC-Bayes variational bound achieve a competitive generalization performance. The same conclusions can also can be drawn in the testing phase as shown in Figure 3 and Figure 3."
        ]
    },
    "S6.T2": {
        "caption": "TABLE II: Comparison of various PAC-Bayes bounds in testing phase in both shuffled pixels and permuted labels environment with different prior model (±plus-or-minus\\pm indicates the 95%percent9595\\% confidence interval).",
        "table": null,
        "footnotes": [],
        "references": [
            "Furthermore, the performance of a learned meta-learner on new tasks with five training objectives is also established. Table II shows the specific result of generalization performance and accuracy in the testing phase. As indicated in Figure 3, the proposed meta-learning PAC-Bayes λ𝜆\\lambda bound and meta-learning PAC-Bayes variational bound perform tighter generalization error bound. In addition, these two training objectives lead to improved accuracy.",
            "Furthermore, five meta-learning training objectives on two different MNIST environments are substantiated (See Table I and Table II). As shown in Figure 3 and Figure 3, comparison between two classical meta-learning PAC-Bayes bounds, the proposed meta-learning PAC-Bayes λ𝜆\\lambda bound and meta-learning PAC-Bayes variational bound achieve a competitive generalization performance. The same conclusions can also can be drawn in the testing phase as shown in Figure 3 and Figure 3."
        ]
    }
}