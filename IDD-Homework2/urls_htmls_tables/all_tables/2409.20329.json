{
    "id_table_1": {
        "caption": "",
        "table": "A4.EGx1",
        "footnotes": [],
        "references": [
            "Given the set of  n n n italic_n  clients, we aim to tolerate the presence of at most  f < n / 2 f n 2 f<n/2 italic_f < italic_n / 2  adversarial clients. Such clients can respond arbitrarily to the queries made by the server, e.g., a gradient computation on their local dataset. We also call these clients Byzantine adversaries. The identity of Byzantine adversaries is a priori unknown to the correct (i.e., non-adversarial) clients, otherwise, the problem is rendered trivial. As we explain above, when no client is adversarial, the objective of each client  i i i italic_i  is to minimize its own risk, defined in ( 1 ), by solving for the interpolated empirical loss minimization problem ( 2 ). However, in the presence of adversarial clients, a correct client cannot simply seek a solution to ( 2 ), as they can never have truthful access to the information about Byzantine adversaries datasets. A more reasonable goal is to minimize an interpolation between their local and the  correct  clients average loss functions. We formally define the  robust-interpolated  objective for client  i i i italic_i  as follows:",
            "The remainder of the paper is organized as follows. Section  2 presents the mean estimation setting as a warm-up and special case of the framework presented in  Section   1.1 . The goal is to give an intuition on settings in which personalization can or cannot help reduce the estimation error in the presence of adversarial clients. Section  3  presents our analysis in the general binary classification setting, where we quantify the tension between the optimization error and the generalization gap in the presence of Byzantine adversaries. We also empirically validate our theory on the MNIST dataset. We defer full proofs to Appendix  C  and our full experimental setup to Appendix  D . We also include further information on related work in  Appendix   A .",
            "While the right-hand side of Proposition  1  may not be tight in general, we note that it is tight for   = 0  0 \\lambda=0 italic_ = 0  and   = 1  1 \\lambda=1 italic_ = 1 , in the homogeneous setting (  = 0  0 \\Delta=0 roman_ = 0 ), provided that we use robust averaging with    O  ( f / n )  O f n \\kappa\\in\\mathcal{O}(f/n) italic_  caligraphic_O ( italic_f / italic_n ) . Indeed, when   = 0  0 \\lambda=0 italic_ = 0  the squared error in estimating the mean of a distribution with variance   2 superscript  2 \\sigma^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  from  m m m italic_m  i.i.d. samples is in    (  2 / m )  superscript  2 m \\Omega(\\sigma^{2}/m) roman_ ( italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT / italic_m ) , refer  (Wu,,  2017 ) . Furthermore, when   = 1  1 \\lambda=1 italic_ = 1 , the squared error in estimating the mean of a distribution with variance   2 superscript  2 \\sigma^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  is in    ( f + 1 n   2 m )  f 1 n superscript  2 m \\Omega(\\frac{f+1}{n}\\frac{\\sigma^{2}}{m}) roman_ ( divide start_ARG italic_f + 1 end_ARG start_ARG italic_n end_ARG divide start_ARG italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_m end_ARG ) , refer  (Zhu et al.,,  2023 ) .",
            "Note that, in the above, the terms that represent the hardness of the local mean estimation task and the heterogeneity among correct clients are   2 m superscript  2 m \\frac{\\sigma^{2}}{m} divide start_ARG italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_m end_ARG , and    i     C  2 +    2 superscript norm subscript  i subscript    C 2  superscript  2 \\|\\mu_{i}-\\overline{\\mu}_{\\mathcal{C}}\\|^{2}+\\kappa\\Delta^{2}  italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT + italic_ roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  respectively. Indeed   2 superscript  2 \\sigma^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  is the variance of each distribution, hence the local mean estimation task is essentially as hard as   2 superscript  2 \\sigma^{2} italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  is large with respect to the number of points  m m m italic_m  each client has access to. Similarly,   2 superscript  2 \\Delta^{2} roman_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  essentially computes how far apart the true means of the correct clients are on average, and    i     C  2 superscript norm subscript  i subscript    C 2 \\|\\mu_{i}-\\overline{\\mu}_{\\mathcal{C}}\\|^{2}  italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  penalizes especially the distance to the average of the correct clients for the one we observe (i.e.,  i  C i C i\\in\\mathcal{C} italic_i  caligraphic_C ). By minimizing the right-hand side of Proposition  1 , we get",
            "To validate our theoretical observations on Byzantine-robust federated mean estimation, we run a series of experiments using simulated datasets sampled from 1-dimensional Gaussian distributions. Specifically, for each of the  n  f n f n-f italic_n - italic_f  correct clients, we consider that their distribution is such that  D i = N  (  i ,  2 ) subscript D i N subscript  i superscript  2 \\mathcal{D}_{i}=\\mathcal{N}(\\mu_{i},\\sigma^{2}) caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = caligraphic_N ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , where the unknown local means  (  i ) i  C subscript subscript  i i C (\\mu_{i})_{i\\in\\mathcal{C}} ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i  caligraphic_C end_POSTSUBSCRIPT  have been sampled i.i.d. from a Gaussian distribution  N  ( 10 ,  h 2 ) N 10 superscript subscript  h 2 \\mathcal{N}(10,\\sigma_{h}^{2}) caligraphic_N ( 10 , italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) 2 2 2 We use a mean at  10 10 10 10  to break the symmetry of the mean simulation around  0 0 . This is simply to present a case in which the sign-flipping attack is indeed disrupting the federated mean estimation procedure , where   h subscript  h \\sigma_{h} italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  determines the expected squared distance between the local means of each correct client. Each honest client  i  C i C i\\in\\mathcal{C} italic_i  caligraphic_C  samples  m m m italic_m  datapoints from  D i subscript D i \\mathcal{D}_{i} caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and the error is computed with respect to   i subscript  i \\mu_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . To evaluate the theoretical expression of    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , defined in ( 6 ), in this scenario, we replace the distances of the form  (  i     C ) 2 superscript subscript  i subscript    C 2 (\\mu_{i}-\\overline{\\mu}_{\\mathcal{C}})^{2} ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  by the variance  ( 1  1 / n  f )   h 2 1 1 n f superscript subscript  h 2 (1-\\nicefrac{{1}}{{n-f}})\\sigma_{h}^{2} ( 1 - / start_ARG 1 end_ARG start_ARG italic_n - italic_f end_ARG ) italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We present in  Figure   1  the average error (squared distance to the true mean) of the estimator defined as per ( 5 ) on  20 20 20 20  runs, for different values of    \\lambda italic_ . The robust aggregation being used here is an NNM pre-aggregation rule  (Allouah et al.,,  2023 )  followed by trimmed mean  (Yin et al.,,  2018 )  and adversarial clients implement the sign-flipping attack. Below we analyze our result presented in the first row of  Figure   1 .",
            "Heterogeneity.   First, we study the impact of the heterogeneity level in  Figure   1 . Doing so, we fix  n = 600 , f = 100 , m = 20 ,  = 15 formulae-sequence n 600 formulae-sequence f 100 formulae-sequence m 20  15 n=600,f=100,m=20,\\sigma=15 italic_n = 600 , italic_f = 100 , italic_m = 20 , italic_ = 15  and vary the level of heterogeneity   h  { 0 , 1 , 2 , 3 , 4 } subscript  h 0 1 2 3 4 \\sigma_{h}\\in\\{0,1,2,3,4\\} italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  { 0 , 1 , 2 , 3 , 4 } . We observe that for low levels of heterogeneity, the optimal choice for    \\lambda italic_  is close to  1 1 1 1 . In this case, personalization does not really help.",
            "Byzantine fraction.  Second, we analyze the impact of the fraction of Byzantine adversaries. For  Figure   1 , we fix  n = 600 , m = 20 ,  = 15 ,  h = 2 formulae-sequence n 600 formulae-sequence m 20 formulae-sequence  15 subscript  h 2 n=600,m=20,\\sigma=15,\\sigma_{h}=2 italic_n = 600 , italic_m = 20 , italic_ = 15 , italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT = 2  and vary  f  { 0 , 50 , 100 , 150 } f 0 50 100 150 f\\in\\{0,50,100,150\\} italic_f  { 0 , 50 , 100 , 150 } . As per our theoretical analysis, as the Byzantine fraction increases    \\kappa italic_  should increase; hence the correct clients need to rely less and less on the global aggregate. However, simply using a local estimator can be detrimental as showcased by the red curve, which shows the case  f = 150 f 150 f=150 italic_f = 150 . Then, by choosing the right collaboration, one reduces the error by  50 % percent 50 50\\% 50 %  compared to the local estimator.",
            "The second line of  Figure   1  shows a comparison between the optimal theoretical value predicted by our analysis and the empirical minimizer of the error on average. 3 3 3   \\kappa italic_  is replaced by  f / n  2  f f n 2 f \\nicefrac{{f}}{{n-2f}} / start_ARG italic_f end_ARG start_ARG italic_n - 2 italic_f end_ARG , following  (Allouah et al.,,  2023 )  We see that the value we predicted for    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  and the actual empirical best choice for    \\lambda italic_  has very similar trends in all the settings we consider.",
            "In this section, we consider the more general problem of binary classification. We study the learning-theoretic setup given in Section  1  with binary output space  Y := { 0 , 1 } assign Y 0 1 \\mathcal{Y}:=\\{0,1\\} caligraphic_Y := { 0 , 1 }  and  Y  = [ 0 , 1 ] superscript Y  0 1 \\mathcal{Y}^{\\prime}=[0,1] caligraphic_Y start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT = [ 0 , 1 ] . Throughout, we place ourselves in a hypothesis space  H H \\mathcal{H} caligraphic_H  of finite pseudo-dimension  (Vidyasagar,,  2003 ; Mohri et al.,,  2018 ) , denoted  Pdim  ( H ) Pdim H \\mathrm{Pdim}{\\left(\\mathcal{H}\\right)} roman_Pdim ( caligraphic_H ) , which we recall reduces to the VC dimension for the  0  1 0 1 0-1 0 - 1  loss. We further make the following assumptions on the loss functions. These assumptions are standard in the Byzantine robustness literature see, e.g.,   (Karimireddy et al.,,  2022 ; Allouah et al.,,  2023 ) .",
            "To conduct an analysis of the effect of collaboration on the generalization performance of correct clients when trying to solve ( 1 ), we proceed in two steps. We first evaluate the optimization error that a standard gradient descent algorithm incurs in our setting. Then we bound the generalization gap induced when minimizing ( 1 ) and combine the two bounds into our main result in Theorem  1 .",
            "We focus our analysis on a simple personalized variant of the robust distributed gradient descent algorithm, which is the standard algorithm in the Byzantine-robust FL literature and is shown to achieve tight optimization bounds  (Allouah et al.,,  2023 ) . Our variant, presented in Algorithm  1 , essentially corresponds to gradient descent on the function  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT , but using a robust estimate  R i t superscript subscript R i t R_{i}^{t} italic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT  (iteration 7) of the gradient of  L C subscript L C \\mathcal{L}_{\\mathcal{C}} caligraphic_L start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT , which cannot be computed exactly due to Byzantine adversaries. This robust estimate is computed using a robust aggregation  F F F italic_F , e.g., trimmed mean or median.",
            "Executing  Algorithm   1  in practice implies that each client acts as a local server, and runs an independent federated learning procedure. We further assume that the clients have access to a communication protocol that allows them to broadcast their model to the other clients. This can either be done through the use of the central server or directly through a decentralized communication (i.e., without a server). This algorithm is not communication efficient and we believe that it could be improved in that direction. Nevertheless, it allows us to discuss the information-theoretic aspects of the problem in a simple manner. We first present the optimization error of  Algorithm   1  in Lemma  1  below.",
            "Let assumptions  1 ,  2 , and  3  hold. Consider Algorithm  1  with learning rate   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG ,    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] , and assume the aggregation function  F F F italic_F  to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. For any  T  1 T 1 T\\geq 1 italic_T  1 , we have:",
            "When specializing the result of Lemma  2  to   = 0  0 \\lambda=0 italic_ = 0  and   = 1  1 \\lambda=1 italic_ = 1 , we recover the standard generalization gaps  O  ( Pdim  ( H ) / m ) O Pdim H m \\mathcal{O}{\\left(\\sqrt{\\nicefrac{{\\mathrm{Pdim}{\\left(\\mathcal{H}\\right)}}}{{% m}}}\\right)} caligraphic_O ( square-root start_ARG / start_ARG roman_Pdim ( caligraphic_H ) end_ARG start_ARG italic_m end_ARG end_ARG )  and  O  ( Pdim  ( H ) / m  ( n  f ) ) O Pdim H m n f \\mathcal{O}{\\left(\\sqrt{\\nicefrac{{\\mathrm{Pdim}{\\left(\\mathcal{H}\\right)}}}{{% m(n-f)}}}\\right)} caligraphic_O ( square-root start_ARG / start_ARG roman_Pdim ( caligraphic_H ) end_ARG start_ARG italic_m ( italic_n - italic_f ) end_ARG end_ARG )  for local learning and federated learning (with correct clients only), respectively. However, in addition to the generalization gap bounded in Lemma  2 , we need to bound the gap between the interpolated risk ( 3 ) and the original local risk ( 1 ). In fact, these two objectives are statistically different when there is data heterogeneity among clients since the interpolated risk ( 3 ) involves the average of local loss functions. To quantify this difference, we leverage tools from domain adaptation theory  (Ben-David et al.,,  2010 )  and consider a function    \\Phi roman_  measuring the discrepancy between two statistical distributions. Formally, we require for every  i  C i C i\\in\\mathcal{C} italic_i  caligraphic_C ,",
            "For example, for the case of 0-1 loss in binary classification, for any two distributions  D 1 subscript D 1 \\mathcal{D}_{1} caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  D 2 subscript D 2 \\mathcal{D}_{2} caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,  (Blitzer et al.,,  2007 )  propose the discrepancy measure to be    ( D 1 , D 2 ) = d H    H  ( D 1 , D 2 )  2  sup h  H | P D 1  ( I  ( h ) )  P D 2  ( I  ( h ) ) |  subscript D 1 subscript D 2 subscript d H  H subscript D 1 subscript D 2  2 subscript supremum h H subscript P subscript D 1 I h subscript P subscript D 2 I h \\Phi(\\mathcal{D}_{1},\\mathcal{D}_{2})=d_{\\mathcal{H}\\Delta\\mathcal{H}}(% \\mathcal{D}_{1},\\mathcal{D}_{2})\\coloneqq 2\\sup_{h\\in\\mathcal{H}}|\\mathbb{P}_{% \\mathcal{D}_{1}}(I(h))-\\mathbb{P}_{\\mathcal{D}_{2}}(I(h))| roman_ ( caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = italic_d start_POSTSUBSCRIPT caligraphic_H roman_ caligraphic_H end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )  2 roman_sup start_POSTSUBSCRIPT italic_h  caligraphic_H end_POSTSUBSCRIPT | blackboard_P start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_I ( italic_h ) ) - blackboard_P start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_I ( italic_h ) ) | , where  I  ( h )  { x  X : h  ( x ) = 1 }  I h conditional-set x X h x 1 I(h)\\coloneqq\\{x\\in\\mathcal{X}:h(x)=1\\} italic_I ( italic_h )  { italic_x  caligraphic_X : italic_h ( italic_x ) = 1 } . For more general losses, we can use hypothesis space-dependent Integral Probability Metrics  (Sriperumbudur et al.,,  2009 ; Bao et al.,,  2023 ) , which we include in  Section   B.2 . We now combine our bounds on the optimization error and generalization gap into Theorem  1  below.",
            "Let assumptions  1 ,  2 ,  3 , and  4  hold, and let    \\Phi roman_  be a function such that ( 7 ) holds. Consider Algorithm  1  with learning rate   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG ,    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] , and assume the aggregation function  F F F italic_F  to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. Then, for any   > 0 , T  1 formulae-sequence  0 T 1 \\delta>0,T\\geq 1 italic_ > 0 , italic_T  1 , we have with probability at least  1   1  1-\\delta 1 - italic_  (over the choice of samples) that",
            "The bound in Theorem  1  features a trade-off between  data heterogeneity ,  sample size , and  model complexity . Indeed, we minimize the bound in Theorem  1  and obtain the following closed-form approximation by ignoring constant and logarithmic terms, and assuming that  n , T  1 much-greater-than n T 1 n,T\\gg 1 italic_n , italic_T  1  (see Appendix  C  for details)",
            "We empirically investigate the impact of Byzantine adversaries on the generalization performance, in our personalization framework, using the Phishing dataset  (Chiew et al.,,  2019 )  and the MNIST dataset  (LeCun and Cortes,,  2010 ) . Other experiments are included in  Appendix   D . For the different experiments, we use the state-of-the-art defense under data heterogeneity: NNM pre-aggregation  (Allouah et al.,,  2023 )  rule followed by trimmed mean as robust aggregation  (Yin et al.,,  2018 ) . We simulate the Byzantine attack with the Sign Flipping attack  (Li et al.,,  2020 ) . Throughout the experiments, we fix  n = 20 n 20 n=20 italic_n = 20 , and only vary the local dataset size  m m m italic_m , the fraction of Byzantine adversaries  f f f italic_f , and the heterogeneity level    \\alpha italic_  which we will explain in the next paragraph. The clients execute  Algorithm   1  for  T T T italic_T  iterations which depends on the dataset being used.  Figure   2 , as well as the figures in  Appendix   D , show the average results and error bars for  5 5 5 5  random runs.",
            "In this paper, we have taken a first step towards understanding how fine-tuning personalization in FL can mitigate the impact of adversarial clients. The results we obtain suggest that the use of personalization can improve the performance of FL algorithms in the presence of adversarial clients, but also that the level of collaboration needs to be chosen carefully. Our theoretical analysis accounts for the necessity to strike a balance between the optimization error, which is impacted by adversarial clients, and the generalization gap, which is likely to benefit from a larger pool of data. We identify the main factors that impact the local performance, namely data heterogeneity, the fraction of adversarial clients, and data scarcity. This work could be extended in several directions. Firstly, as explained in Section  3 , the study of Algorithm  1  primarily serves our understanding of the robust interpolated optimization problem in ( 3 ) from an information-theoretic perspective. However, it may not be efficient enough to cope with high-dimensional learning tasks involving a number of clients. An interesting future direction could be to investigate alternative algorithms with lower communication and computational costs, which would be able to handle these high-dimensional problems. Second, our study focuses on a personalization strategy that interpolates between client loss functions. Although this strategy is simple to interpret and allows us to derive optimization and generalization bounds, it is only one of many possible personalized FL schemes. Another interesting avenue would be to investigate whether the insights we drew from our interpolated problem could be derived in a similar fashion from other personalization frameworks such as regularization, layer specialization, or clustering.",
            "See  1",
            "Conclusion.  By substituting the above in ( 13 ), we get the following.",
            "See  1",
            "Let assumptions  1 ,  2 , and  3  hold. First, we note that  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is  L L L italic_L -smooth (and    \\mu italic_ -strongly convex) as a convex combination of  L i subscript L i \\mathcal{L}_{i} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  L C subscript L C \\mathcal{L}_{\\mathcal{C}} caligraphic_L start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT , which are both  L L L italic_L -smooth (and    \\mu italic_ -strongly convex) by Assumption  1 . In the remainder of this proof, we denote by   ^ i  superscript subscript ^  i  \\widehat{\\theta}_{i}^{\\lambda} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  the minimizer of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  on    \\Theta roman_ , i.e.,  L i   (  ^ i  ) = L i ,   superscript subscript L i  superscript subscript ^  i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda}(\\widehat{\\theta}_{i}^{\\lambda})=\\mathcal{L}_{i,*}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) = caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT . Recall also that, by Assumption  3 ,   ^ i  superscript subscript ^  i  \\widehat{\\theta}_{i}^{\\lambda} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is also a critical point of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT .",
            "By developing the right-hand side ( 15 ) and using Jensens inequality, we get",
            "Furthermore, by Assumption  3 , we know that   ^ i  superscript subscript ^  i  \\widehat{\\theta}_{i}^{\\lambda} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is a critical point of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  (Assumption  1 ), hence using the  L L L italic_L -smooth of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  we have    L i   (  i t )  2  2  L  ( L i   (  i t )  L i ,   ) superscript norm  superscript subscript L i  superscript subscript  i t 2 2 L superscript subscript L i  superscript subscript  i t superscript subscript L i  \\|\\nabla\\mathcal{L}_{i}^{\\lambda}(\\theta_{i}^{t})\\|^{2}\\leq 2L(\\mathcal{L}_{i}% ^{\\lambda}(\\theta_{i}^{t})-\\mathcal{L}_{i,*}^{\\lambda})   caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  2 italic_L ( caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) - caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) . Moreover, since  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is    \\mu italic_ -strongly convex (Assumption  1 ), we have    i t   ^ i  ,  L i   (  i t )   L i   (  i t )  L i ,   +  2    i t   ^ i   2 superscript subscript  i t superscript subscript ^  i   superscript subscript L i  superscript subscript  i t superscript subscript L i  superscript subscript  i t superscript subscript L i   2 superscript norm superscript subscript  i t superscript subscript ^  i  2 \\left<\\theta_{i}^{t}-\\widehat{\\theta}_{i}^{\\lambda},\\nabla\\mathcal{L}_{i}^{% \\lambda}(\\theta_{i}^{t})\\right>\\geq\\mathcal{L}_{i}^{\\lambda}(\\theta_{i}^{t})-% \\mathcal{L}_{i,*}^{\\lambda}+\\tfrac{\\mu}{2}\\|\\theta_{i}^{t}-\\widehat{\\theta}_{i% }^{\\lambda}\\|^{2}  italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ,  caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )   caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) - caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT + divide start_ARG italic_ end_ARG start_ARG 2 end_ARG  italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Substituting these in the above yields",
            "Combining Assumption  1  and Assumption  3 , we easily get that   2      ^ i   2  L i   (  )  L i ,    L 2      ^ i   2  2 superscript norm  superscript subscript ^  i  2 superscript subscript L i   superscript subscript L i  L 2 superscript norm  superscript subscript ^  i  2 \\tfrac{\\mu}{2}\\|\\theta-\\widehat{\\theta}_{i}^{\\lambda}\\|^{2}\\leq\\mathcal{L}_{i}% ^{\\lambda}(\\theta)-\\mathcal{L}_{i,*}^{\\lambda}\\leq\\tfrac{L}{2}\\|\\theta-% \\widehat{\\theta}_{i}^{\\lambda}\\|^{2} divide start_ARG italic_ end_ARG start_ARG 2 end_ARG  italic_ - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ ) - caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  divide start_ARG italic_L end_ARG start_ARG 2 end_ARG  italic_ - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  for all       \\theta\\in\\Theta italic_  roman_ . Using this, and specializing ( 16 ) for  t = T  1 t T 1 t=T-1 italic_t = italic_T - 1 , we have",
            "See  1",
            "For line  3 3 3 3 , recall that assumptions  1  and  2 , that   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG , and that  F F F italic_F  is assumed to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. Hence, Lemma  1  holds true. we use ( 16 ) in the proof of Lemma  1 , to get",
            "Here, our goal is to reasonably approximate the value of lambda    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  for which the right-hand side of ( 24 ) is minimized. To do so, we first ignore the asymptotically negligible terms. Essentially, we consider that  T T T italic_T  is large enough so that the term  ( 1   2  L ) T  L   ( L i   (  i 0 )  L i   (  ^ i  ) ) superscript 1  2 L T L  superscript subscript L i  superscript subscript  i 0 superscript subscript L i  superscript subscript ^  i  \\left(1-\\frac{\\mu}{2L}\\right)^{T}\\frac{L}{\\mu}\\left(\\mathcal{L}_{i}^{\\lambda}(% \\theta_{i}^{0})-\\mathcal{L}_{i}^{\\lambda}(\\widehat{\\theta}_{i}^{\\lambda})\\right) ( 1 - divide start_ARG italic_ end_ARG start_ARG 2 italic_L end_ARG ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_L end_ARG start_ARG italic_ end_ARG ( caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ) - caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) )  is close enough to zero. In the reminder, we denote by   i  superscript subscript  i \\theta_{i}^{\\infty} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  the output of Algorithm  1  for  T    T T\\rightarrow\\infty italic_T   . Even after ignoring this term, seeking the optimal value    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the solution to a 4th-degree equation, which may not admit a close-form solution in general. To make the computation of an approximate minimizer more tractable, we proceed with the following simplification using the fact that  a + b  a + b a b a b \\sqrt{a+b}\\leq\\sqrt{a}+\\sqrt{b} square-root start_ARG italic_a + italic_b end_ARG  square-root start_ARG italic_a end_ARG + square-root start_ARG italic_b end_ARG  for any  a , b  0 a b 0 a,b\\geq 0 italic_a , italic_b  0 :",
            "In this section, we give the implementation details of our experiments for each setting and we provide some additional figures. For all the experiments, we run  Algorithm   1  with full gradients for a number  T T T italic_T  of iterations which we change depending on the dataset."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "A4.EGx2",
        "footnotes": [],
        "references": [
            "where  L  (  )  1 n   i = 1 n L i  (  ) ,     formulae-sequence  L  1 n superscript subscript i 1 n subscript L i  for-all   \\mathcal{L}{(\\theta)}\\coloneqq\\tfrac{1}{n}\\sum_{i=1}^{n}\\mathcal{L}_{i}{(% \\theta)},~{}\\forall\\theta\\in\\Theta caligraphic_L ( italic_ )  divide start_ARG 1 end_ARG start_ARG italic_n end_ARG  start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_ ) ,  italic_  roman_ , and    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] . When   = 1  1 \\lambda=1 italic_ = 1 , the problem reduces to the standard FL objective and corresponds to a full collaboration amongst the clients. The case of   = 0  0 \\lambda=0 italic_ = 0  represents the absence of collaboration, i.e., each client minimizes its local empirical loss. We use the terminology  fine-tuned  personalization to refer to the objective of solving for ( 2 ) when    ( 0 , 1 )  0 1 \\lambda\\in(0,1) italic_  ( 0 , 1 )  is determined by optimizing the learning performance in retrospect.",
            "Given the set of  n n n italic_n  clients, we aim to tolerate the presence of at most  f < n / 2 f n 2 f<n/2 italic_f < italic_n / 2  adversarial clients. Such clients can respond arbitrarily to the queries made by the server, e.g., a gradient computation on their local dataset. We also call these clients Byzantine adversaries. The identity of Byzantine adversaries is a priori unknown to the correct (i.e., non-adversarial) clients, otherwise, the problem is rendered trivial. As we explain above, when no client is adversarial, the objective of each client  i i i italic_i  is to minimize its own risk, defined in ( 1 ), by solving for the interpolated empirical loss minimization problem ( 2 ). However, in the presence of adversarial clients, a correct client cannot simply seek a solution to ( 2 ), as they can never have truthful access to the information about Byzantine adversaries datasets. A more reasonable goal is to minimize an interpolation between their local and the  correct  clients average loss functions. We formally define the  robust-interpolated  objective for client  i i i italic_i  as follows:",
            "where  C C \\mathcal{C} caligraphic_C  represents the set of correct clients,  L C  (  )  1 | C |   i  C L i  (  ) ,     formulae-sequence  subscript L C  1 C subscript i C subscript L i  for-all   \\mathcal{L}_{\\mathcal{C}}{(\\theta)}\\coloneqq\\tfrac{1}{\\left\\lvert{\\mathcal{C}}% \\right\\rvert}\\sum_{i\\in\\mathcal{C}}\\mathcal{L}_{i}{(\\theta)},~{}\\forall\\theta\\in\\Theta caligraphic_L start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ( italic_ )  divide start_ARG 1 end_ARG start_ARG | caligraphic_C | end_ARG  start_POSTSUBSCRIPT italic_i  caligraphic_C end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_ ) ,  italic_  roman_ , and    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ]  is called the  collaboration level . Similar to ( 2 ),   = 0  0 \\lambda=0 italic_ = 0  and   = 1  1 \\lambda=1 italic_ = 1 , respectively, reduce to local learning which can be solved with local SGD  (Stich,,  2018 ) , and Byzantine-robust FL which can be tackled using a robust variant of distributed gradient descent  (Allouah et al.,,  2023 ) . Recall that  C C \\mathcal{C} caligraphic_C  is a priori unknown to the correct clients, hence they can neither have access to  L C subscript L C \\mathcal{L}_{\\mathcal{C}} caligraphic_L start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT  nor to its gradients. A correct client can only approximate any information on  L C subscript L C \\mathcal{L}_{\\mathcal{C}} caligraphic_L start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT , where the approximation error grows with both  f / n f n f/n italic_f / italic_n  and data heterogeneity, shown in  (Karimireddy et al.,,  2022 ; Allouah et al.,,  2024 ) .",
            "The remainder of the paper is organized as follows. Section  2 presents the mean estimation setting as a warm-up and special case of the framework presented in  Section   1.1 . The goal is to give an intuition on settings in which personalization can or cannot help reduce the estimation error in the presence of adversarial clients. Section  3  presents our analysis in the general binary classification setting, where we quantify the tension between the optimization error and the generalization gap in the presence of Byzantine adversaries. We also empirically validate our theory on the MNIST dataset. We defer full proofs to Appendix  C  and our full experimental setup to Appendix  D . We also include further information on related work in  Appendix   A .",
            "Let assumptions  1 ,  2 , and  3  hold. Consider Algorithm  1  with learning rate   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG ,    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] , and assume the aggregation function  F F F italic_F  to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. For any  T  1 T 1 T\\geq 1 italic_T  1 , we have:",
            "Our optimization error bound recovers the classical GD linear rate when   = 0  0 \\lambda=0 italic_ = 0  (local learning), and achieves an asymptotic error in  O  (   G 2 ) O  superscript G 2 \\mathcal{O}{(\\kappa G^{2})} caligraphic_O ( italic_ italic_G start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT )  when   = 1  1 \\lambda=1 italic_ = 1  (full collaboration), which is provably tight under Assumption  2   (Karimireddy et al.,,  2022 ) . Interestingly, for    ( 0 , 1 )  0 1 \\lambda\\in(0,1) italic_  ( 0 , 1 ) , the asymptotic error smoothly interpolates in  O  (  2    G 2 ) O superscript  2  superscript G 2 \\mathcal{O}{(\\lambda^{2}\\kappa G^{2})} caligraphic_O ( italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT italic_ italic_G start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , which is expected as lesser collaboration limits the influence of the adversary on the optimization error.",
            "Although the presence of adversarial clients impairs the optimization process, one expects a generalization benefit from collaboration if data distributions are similar enough. We formalize this intuition by bounding the generalization gap, of the personalized learning problem ( 2 ). We first bound the generalization gap on the interpolated loss  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  using a result adapted from  (Blitzer et al.,,  2007 ) .",
            "When specializing the result of Lemma  2  to   = 0  0 \\lambda=0 italic_ = 0  and   = 1  1 \\lambda=1 italic_ = 1 , we recover the standard generalization gaps  O  ( Pdim  ( H ) / m ) O Pdim H m \\mathcal{O}{\\left(\\sqrt{\\nicefrac{{\\mathrm{Pdim}{\\left(\\mathcal{H}\\right)}}}{{% m}}}\\right)} caligraphic_O ( square-root start_ARG / start_ARG roman_Pdim ( caligraphic_H ) end_ARG start_ARG italic_m end_ARG end_ARG )  and  O  ( Pdim  ( H ) / m  ( n  f ) ) O Pdim H m n f \\mathcal{O}{\\left(\\sqrt{\\nicefrac{{\\mathrm{Pdim}{\\left(\\mathcal{H}\\right)}}}{{% m(n-f)}}}\\right)} caligraphic_O ( square-root start_ARG / start_ARG roman_Pdim ( caligraphic_H ) end_ARG start_ARG italic_m ( italic_n - italic_f ) end_ARG end_ARG )  for local learning and federated learning (with correct clients only), respectively. However, in addition to the generalization gap bounded in Lemma  2 , we need to bound the gap between the interpolated risk ( 3 ) and the original local risk ( 1 ). In fact, these two objectives are statistically different when there is data heterogeneity among clients since the interpolated risk ( 3 ) involves the average of local loss functions. To quantify this difference, we leverage tools from domain adaptation theory  (Ben-David et al.,,  2010 )  and consider a function    \\Phi roman_  measuring the discrepancy between two statistical distributions. Formally, we require for every  i  C i C i\\in\\mathcal{C} italic_i  caligraphic_C ,",
            "For example, for the case of 0-1 loss in binary classification, for any two distributions  D 1 subscript D 1 \\mathcal{D}_{1} caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and  D 2 subscript D 2 \\mathcal{D}_{2} caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ,  (Blitzer et al.,,  2007 )  propose the discrepancy measure to be    ( D 1 , D 2 ) = d H    H  ( D 1 , D 2 )  2  sup h  H | P D 1  ( I  ( h ) )  P D 2  ( I  ( h ) ) |  subscript D 1 subscript D 2 subscript d H  H subscript D 1 subscript D 2  2 subscript supremum h H subscript P subscript D 1 I h subscript P subscript D 2 I h \\Phi(\\mathcal{D}_{1},\\mathcal{D}_{2})=d_{\\mathcal{H}\\Delta\\mathcal{H}}(% \\mathcal{D}_{1},\\mathcal{D}_{2})\\coloneqq 2\\sup_{h\\in\\mathcal{H}}|\\mathbb{P}_{% \\mathcal{D}_{1}}(I(h))-\\mathbb{P}_{\\mathcal{D}_{2}}(I(h))| roman_ ( caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) = italic_d start_POSTSUBSCRIPT caligraphic_H roman_ caligraphic_H end_POSTSUBSCRIPT ( caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT )  2 roman_sup start_POSTSUBSCRIPT italic_h  caligraphic_H end_POSTSUBSCRIPT | blackboard_P start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_I ( italic_h ) ) - blackboard_P start_POSTSUBSCRIPT caligraphic_D start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_POSTSUBSCRIPT ( italic_I ( italic_h ) ) | , where  I  ( h )  { x  X : h  ( x ) = 1 }  I h conditional-set x X h x 1 I(h)\\coloneqq\\{x\\in\\mathcal{X}:h(x)=1\\} italic_I ( italic_h )  { italic_x  caligraphic_X : italic_h ( italic_x ) = 1 } . For more general losses, we can use hypothesis space-dependent Integral Probability Metrics  (Sriperumbudur et al.,,  2009 ; Bao et al.,,  2023 ) , which we include in  Section   B.2 . We now combine our bounds on the optimization error and generalization gap into Theorem  1  below.",
            "Let assumptions  1 ,  2 ,  3 , and  4  hold, and let    \\Phi roman_  be a function such that ( 7 ) holds. Consider Algorithm  1  with learning rate   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG ,    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] , and assume the aggregation function  F F F italic_F  to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. Then, for any   > 0 , T  1 formulae-sequence  0 T 1 \\delta>0,T\\geq 1 italic_ > 0 , italic_T  1 , we have with probability at least  1   1  1-\\delta 1 - italic_  (over the choice of samples) that",
            "We empirically investigate the impact of Byzantine adversaries on the generalization performance, in our personalization framework, using the Phishing dataset  (Chiew et al.,,  2019 )  and the MNIST dataset  (LeCun and Cortes,,  2010 ) . Other experiments are included in  Appendix   D . For the different experiments, we use the state-of-the-art defense under data heterogeneity: NNM pre-aggregation  (Allouah et al.,,  2023 )  rule followed by trimmed mean as robust aggregation  (Yin et al.,,  2018 ) . We simulate the Byzantine attack with the Sign Flipping attack  (Li et al.,,  2020 ) . Throughout the experiments, we fix  n = 20 n 20 n=20 italic_n = 20 , and only vary the local dataset size  m m m italic_m , the fraction of Byzantine adversaries  f f f italic_f , and the heterogeneity level    \\alpha italic_  which we will explain in the next paragraph. The clients execute  Algorithm   1  for  T T T italic_T  iterations which depends on the dataset being used.  Figure   2 , as well as the figures in  Appendix   D , show the average results and error bars for  5 5 5 5  random runs.",
            "Figure   2 , as well as  Figure   6  from  Appendix   D , show the final local test accuracy performance as a function of the degree of collaboration (   \\lambda italic_ ) for different values of  f , m f m f,m italic_f , italic_m  and    \\alpha italic_ . These experimental results shed light on the impact of these different factors and allow us to confirm the main insights of our theory.",
            "This can be gleaned from settings where the adversarial fraction is substantial (over  6 6 6 6  adversarial clients, e.g. green and red curves in  Figure   2 ), when the heterogeneity is large (small values of    \\alpha italic_ , e.g. blue and orange curves in  Figure   6  in  Appendix   D ) and when the local dataset is large enough (e.g.  Figure   2(b)  and  Figure   2(c) ). In situations where the adversarial fraction is critical ( f = 9 f 9 f=9 italic_f = 9 , corresponding to red curves in  Figure   2  ), robust federated learning completely fails, achieving accuracy scores under  50 % percent 50 50\\% 50 %  on the local test datasets.  Figure   5  in  Appendix   D  further illustrates this point, showing that even local learning can be better than state-of-the-art robust Federated Learning in these circumstances.",
            "Figure   2 (Bottom) shows that in data scarcity scenarios, for moderate values of the number of adversarial clients ( f = 3 f 3 f=3 italic_f = 3  or  f = 6 f 6 f=6 italic_f = 6 ), using a collaboration degree strictly between  0 0  and  1 1 1 1  yields better accuracy on the local test dataset. The same effect also appears in  Figure   2 (Top) for  f = 3 f 3 f=3 italic_f = 3  and  Figure   4  in  Appendix   D . Additionally, even for extreme adversarial fractions ( f = 9 f 9 f=9 italic_f = 9  in our experiments), the gain from decreasing the collaboration degree can be substantial compared to simply using Robust Federated Learning methods. This suggests that fine-tuning the collaboration degree can help dampen the detrimental effects of adversarial clients.",
            "Let assumptions  1 ,  2 , and  3  hold. First, we note that  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is  L L L italic_L -smooth (and    \\mu italic_ -strongly convex) as a convex combination of  L i subscript L i \\mathcal{L}_{i} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  L C subscript L C \\mathcal{L}_{\\mathcal{C}} caligraphic_L start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT , which are both  L L L italic_L -smooth (and    \\mu italic_ -strongly convex) by Assumption  1 . In the remainder of this proof, we denote by   ^ i  superscript subscript ^  i  \\widehat{\\theta}_{i}^{\\lambda} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  the minimizer of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  on    \\Theta roman_ , i.e.,  L i   (  ^ i  ) = L i ,   superscript subscript L i  superscript subscript ^  i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda}(\\widehat{\\theta}_{i}^{\\lambda})=\\mathcal{L}_{i,*}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) = caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT . Recall also that, by Assumption  3 ,   ^ i  superscript subscript ^  i  \\widehat{\\theta}_{i}^{\\lambda} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is also a critical point of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT .",
            "By  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robustness of  F F F italic_F , and Assumption  2 , we have    i t  2   | C |   i  C   L i  (  i t )   L C  (  i t )  2    G 2 superscript norm superscript subscript  i t 2  C subscript i C superscript norm  subscript L i superscript subscript  i t  subscript L C superscript subscript  i t 2  superscript G 2 \\|\\xi_{i}^{t}\\|^{2}\\leq\\tfrac{\\kappa}{\\left\\lvert{\\mathcal{C}}\\right\\rvert}% \\sum_{i\\in\\mathcal{C}}\\|\\nabla\\mathcal{L}_{i}(\\theta_{i}^{t})-\\nabla\\mathcal{L% }_{\\mathcal{C}}(\\theta_{i}^{t})\\|^{2}\\leq\\kappa G^{2}  italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  divide start_ARG italic_ end_ARG start_ARG | caligraphic_C | end_ARG  start_POSTSUBSCRIPT italic_i  caligraphic_C end_POSTSUBSCRIPT   caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) -  caligraphic_L start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  italic_ italic_G start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Hence, we get",
            "See  2",
            "For lines  2 2 2 2  and  5 5 5 5 , since Assumption ( 4 ) holds true, we can use the results from  Lemma   2 . Let    Pdim  ( H )  log  ( 2  m  | C | ) + log  ( 4 /  )   Pdim H 2 m C 4  \\beta\\coloneqq\\mathrm{Pdim}{\\left(\\mathcal{H}\\right)}\\log(2m\\left\\lvert{% \\mathcal{C}}\\right\\rvert)+\\log(4/\\delta) italic_  roman_Pdim ( caligraphic_H ) roman_log ( 2 italic_m | caligraphic_C | ) + roman_log ( 4 / italic_ ) . By applying  Lemma   2  with   / 2  2 \\delta/2 italic_ / 2  for both  ( 2 ) 2 (2) ( 2 )  and  ( 5 ) 5 (5) ( 5 ) , we get that with probability at least  1   1  1-\\delta 1 - italic_ :",
            "For line  3 3 3 3 , recall that assumptions  1  and  2 , that   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG , and that  F F F italic_F  is assumed to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. Hence, Lemma  1  holds true. we use ( 16 ) in the proof of Lemma  1 , to get",
            "Here, our goal is to reasonably approximate the value of lambda    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  for which the right-hand side of ( 24 ) is minimized. To do so, we first ignore the asymptotically negligible terms. Essentially, we consider that  T T T italic_T  is large enough so that the term  ( 1   2  L ) T  L   ( L i   (  i 0 )  L i   (  ^ i  ) ) superscript 1  2 L T L  superscript subscript L i  superscript subscript  i 0 superscript subscript L i  superscript subscript ^  i  \\left(1-\\frac{\\mu}{2L}\\right)^{T}\\frac{L}{\\mu}\\left(\\mathcal{L}_{i}^{\\lambda}(% \\theta_{i}^{0})-\\mathcal{L}_{i}^{\\lambda}(\\widehat{\\theta}_{i}^{\\lambda})\\right) ( 1 - divide start_ARG italic_ end_ARG start_ARG 2 italic_L end_ARG ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_L end_ARG start_ARG italic_ end_ARG ( caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ) - caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) )  is close enough to zero. In the reminder, we denote by   i  superscript subscript  i \\theta_{i}^{\\infty} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  the output of Algorithm  1  for  T    T T\\rightarrow\\infty italic_T   . Even after ignoring this term, seeking the optimal value    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the solution to a 4th-degree equation, which may not admit a close-form solution in general. To make the computation of an approximate minimizer more tractable, we proceed with the following simplification using the fact that  a + b  a + b a b a b \\sqrt{a+b}\\leq\\sqrt{a}+\\sqrt{b} square-root start_ARG italic_a + italic_b end_ARG  square-root start_ARG italic_a end_ARG + square-root start_ARG italic_b end_ARG  for any  a , b  0 a b 0 a,b\\geq 0 italic_a , italic_b  0 :",
            "with   := 6  L    G 2  2 assign  6 L  superscript G 2 superscript  2 \\alpha:=\\frac{6L\\kappa G^{2}}{\\mu^{2}} italic_ := divide start_ARG 6 italic_L italic_ italic_G start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG . Now the right-hand side is a quadratic function of    \\lambda italic_  which can be easily minimized. Specifically, the solution of ( 25 ) is",
            "Using these compute resources, each experiment on the MNIST dataset (meaning each subfigure in  Figure   2 (bottom)) took less than  72 72 72 72  hours to run. Each experiment on Phishing dataset took less than  8 8 8 8  hours to run."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A4.EGx3",
        "footnotes": [],
        "references": [
            "We establish optimization and generalization guarantees on the interpolated personalized objective ( 3 ) in the presence of adversarial clients, and show how the degree of collaboration    \\lambda italic_ , asymptotically navigates the trade-off between the fundamental optimization error due to adversarial clients and the improved generalization performance thanks to the collaboration with correct ones.",
            "The remainder of the paper is organized as follows. Section  2 presents the mean estimation setting as a warm-up and special case of the framework presented in  Section   1.1 . The goal is to give an intuition on settings in which personalization can or cannot help reduce the estimation error in the presence of adversarial clients. Section  3  presents our analysis in the general binary classification setting, where we quantify the tension between the optimization error and the generalization gap in the presence of Byzantine adversaries. We also empirically validate our theory on the MNIST dataset. We defer full proofs to Appendix  C  and our full experimental setup to Appendix  D . We also include further information on related work in  Appendix   A .",
            "Let assumptions  1 ,  2 , and  3  hold. Consider Algorithm  1  with learning rate   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG ,    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] , and assume the aggregation function  F F F italic_F  to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. For any  T  1 T 1 T\\geq 1 italic_T  1 , we have:",
            "When specializing the result of Lemma  2  to   = 0  0 \\lambda=0 italic_ = 0  and   = 1  1 \\lambda=1 italic_ = 1 , we recover the standard generalization gaps  O  ( Pdim  ( H ) / m ) O Pdim H m \\mathcal{O}{\\left(\\sqrt{\\nicefrac{{\\mathrm{Pdim}{\\left(\\mathcal{H}\\right)}}}{{% m}}}\\right)} caligraphic_O ( square-root start_ARG / start_ARG roman_Pdim ( caligraphic_H ) end_ARG start_ARG italic_m end_ARG end_ARG )  and  O  ( Pdim  ( H ) / m  ( n  f ) ) O Pdim H m n f \\mathcal{O}{\\left(\\sqrt{\\nicefrac{{\\mathrm{Pdim}{\\left(\\mathcal{H}\\right)}}}{{% m(n-f)}}}\\right)} caligraphic_O ( square-root start_ARG / start_ARG roman_Pdim ( caligraphic_H ) end_ARG start_ARG italic_m ( italic_n - italic_f ) end_ARG end_ARG )  for local learning and federated learning (with correct clients only), respectively. However, in addition to the generalization gap bounded in Lemma  2 , we need to bound the gap between the interpolated risk ( 3 ) and the original local risk ( 1 ). In fact, these two objectives are statistically different when there is data heterogeneity among clients since the interpolated risk ( 3 ) involves the average of local loss functions. To quantify this difference, we leverage tools from domain adaptation theory  (Ben-David et al.,,  2010 )  and consider a function    \\Phi roman_  measuring the discrepancy between two statistical distributions. Formally, we require for every  i  C i C i\\in\\mathcal{C} italic_i  caligraphic_C ,",
            "Let assumptions  1 ,  2 ,  3 , and  4  hold, and let    \\Phi roman_  be a function such that ( 7 ) holds. Consider Algorithm  1  with learning rate   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG ,    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] , and assume the aggregation function  F F F italic_F  to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. Then, for any   > 0 , T  1 formulae-sequence  0 T 1 \\delta>0,T\\geq 1 italic_ > 0 , italic_T  1 , we have with probability at least  1   1  1-\\delta 1 - italic_  (over the choice of samples) that",
            "In this paper, we have taken a first step towards understanding how fine-tuning personalization in FL can mitigate the impact of adversarial clients. The results we obtain suggest that the use of personalization can improve the performance of FL algorithms in the presence of adversarial clients, but also that the level of collaboration needs to be chosen carefully. Our theoretical analysis accounts for the necessity to strike a balance between the optimization error, which is impacted by adversarial clients, and the generalization gap, which is likely to benefit from a larger pool of data. We identify the main factors that impact the local performance, namely data heterogeneity, the fraction of adversarial clients, and data scarcity. This work could be extended in several directions. Firstly, as explained in Section  3 , the study of Algorithm  1  primarily serves our understanding of the robust interpolated optimization problem in ( 3 ) from an information-theoretic perspective. However, it may not be efficient enough to cope with high-dimensional learning tasks involving a number of clients. An interesting future direction could be to investigate alternative algorithms with lower communication and computational costs, which would be able to handle these high-dimensional problems. Second, our study focuses on a personalization strategy that interpolates between client loss functions. Although this strategy is simple to interpret and allows us to derive optimization and generalization bounds, it is only one of many possible personalized FL schemes. Another interesting avenue would be to investigate whether the insights we drew from our interpolated problem could be derived in a similar fashion from other personalization frameworks such as regularization, layer specialization, or clustering.",
            "Conclusion.  By substituting the above in ( 13 ), we get the following.",
            "Let assumptions  1 ,  2 , and  3  hold. First, we note that  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is  L L L italic_L -smooth (and    \\mu italic_ -strongly convex) as a convex combination of  L i subscript L i \\mathcal{L}_{i} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and  L C subscript L C \\mathcal{L}_{\\mathcal{C}} caligraphic_L start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT , which are both  L L L italic_L -smooth (and    \\mu italic_ -strongly convex) by Assumption  1 . In the remainder of this proof, we denote by   ^ i  superscript subscript ^  i  \\widehat{\\theta}_{i}^{\\lambda} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  the minimizer of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  on    \\Theta roman_ , i.e.,  L i   (  ^ i  ) = L i ,   superscript subscript L i  superscript subscript ^  i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda}(\\widehat{\\theta}_{i}^{\\lambda})=\\mathcal{L}_{i,*}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) = caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT . Recall also that, by Assumption  3 ,   ^ i  superscript subscript ^  i  \\widehat{\\theta}_{i}^{\\lambda} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is also a critical point of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT .",
            "Furthermore, by Assumption  3 , we know that   ^ i  superscript subscript ^  i  \\widehat{\\theta}_{i}^{\\lambda} over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is a critical point of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  (Assumption  1 ), hence using the  L L L italic_L -smooth of  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  we have    L i   (  i t )  2  2  L  ( L i   (  i t )  L i ,   ) superscript norm  superscript subscript L i  superscript subscript  i t 2 2 L superscript subscript L i  superscript subscript  i t superscript subscript L i  \\|\\nabla\\mathcal{L}_{i}^{\\lambda}(\\theta_{i}^{t})\\|^{2}\\leq 2L(\\mathcal{L}_{i}% ^{\\lambda}(\\theta_{i}^{t})-\\mathcal{L}_{i,*}^{\\lambda})   caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  2 italic_L ( caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) - caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) . Moreover, since  L i  superscript subscript L i  \\mathcal{L}_{i}^{\\lambda} caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  is    \\mu italic_ -strongly convex (Assumption  1 ), we have    i t   ^ i  ,  L i   (  i t )   L i   (  i t )  L i ,   +  2    i t   ^ i   2 superscript subscript  i t superscript subscript ^  i   superscript subscript L i  superscript subscript  i t superscript subscript L i  superscript subscript  i t superscript subscript L i   2 superscript norm superscript subscript  i t superscript subscript ^  i  2 \\left<\\theta_{i}^{t}-\\widehat{\\theta}_{i}^{\\lambda},\\nabla\\mathcal{L}_{i}^{% \\lambda}(\\theta_{i}^{t})\\right>\\geq\\mathcal{L}_{i}^{\\lambda}(\\theta_{i}^{t})-% \\mathcal{L}_{i,*}^{\\lambda}+\\tfrac{\\mu}{2}\\|\\theta_{i}^{t}-\\widehat{\\theta}_{i% }^{\\lambda}\\|^{2}  italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ,  caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT )   caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ) - caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT + divide start_ARG italic_ end_ARG start_ARG 2 end_ARG  italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . Substituting these in the above yields",
            "Combining Assumption  1  and Assumption  3 , we easily get that   2      ^ i   2  L i   (  )  L i ,    L 2      ^ i   2  2 superscript norm  superscript subscript ^  i  2 superscript subscript L i   superscript subscript L i  L 2 superscript norm  superscript subscript ^  i  2 \\tfrac{\\mu}{2}\\|\\theta-\\widehat{\\theta}_{i}^{\\lambda}\\|^{2}\\leq\\mathcal{L}_{i}% ^{\\lambda}(\\theta)-\\mathcal{L}_{i,*}^{\\lambda}\\leq\\tfrac{L}{2}\\|\\theta-% \\widehat{\\theta}_{i}^{\\lambda}\\|^{2} divide start_ARG italic_ end_ARG start_ARG 2 end_ARG  italic_ - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ ) - caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  divide start_ARG italic_L end_ARG start_ARG 2 end_ARG  italic_ - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  for all       \\theta\\in\\Theta italic_  roman_ . Using this, and specializing ( 16 ) for  t = T  1 t T 1 t=T-1 italic_t = italic_T - 1 , we have"
        ]
    },
    "id_table_4": {
        "caption": "",
        "table": "A4.EGx4",
        "footnotes": [],
        "references": [
            "Let Assumption  4  hold. For any   > 0  0 \\delta>0 italic_ > 0 ,       \\theta\\in\\Theta italic_  roman_  and    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] , we have with probability at least  1   1  1-\\delta 1 - italic_  (over the choice of samples) that",
            "Let assumptions  1 ,  2 ,  3 , and  4  hold, and let    \\Phi roman_  be a function such that ( 7 ) holds. Consider Algorithm  1  with learning rate   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG ,    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] , and assume the aggregation function  F F F italic_F  to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. Then, for any   > 0 , T  1 formulae-sequence  0 T 1 \\delta>0,T\\geq 1 italic_ > 0 , italic_T  1 , we have with probability at least  1   1  1-\\delta 1 - italic_  (over the choice of samples) that",
            "Figure   2 (Bottom) shows that in data scarcity scenarios, for moderate values of the number of adversarial clients ( f = 3 f 3 f=3 italic_f = 3  or  f = 6 f 6 f=6 italic_f = 6 ), using a collaboration degree strictly between  0 0  and  1 1 1 1  yields better accuracy on the local test dataset. The same effect also appears in  Figure   2 (Top) for  f = 3 f 3 f=3 italic_f = 3  and  Figure   4  in  Appendix   D . Additionally, even for extreme adversarial fractions ( f = 9 f 9 f=9 italic_f = 9  in our experiments), the gain from decreasing the collaboration degree can be substantial compared to simply using Robust Federated Learning methods. This suggests that fine-tuning the collaboration degree can help dampen the detrimental effects of adversarial clients.",
            "Let us now consider the set of  m  | C | m C m\\left\\lvert{\\mathcal{C}}\\right\\rvert italic_m | caligraphic_C |  real-valued independent 4 4 4 The data sets have been sampled independently of each other and are composed of points sampled i.i.d from the local data distributions.  random variables defined as  { | C |  ( 1   +  | C | )  l  ( h   ( x ) , y )  ( x , y )  S i }  {   l  ( h   ( x ) , y )  ( x , y )  S j ,  j = i } conditional C 1   C l subscript h  x y x y subscript S i conditional-set  l subscript h  x y formulae-sequence x y subscript S j for-all j i \\left\\{|\\mathcal{C}|\\left(1-\\lambda+\\frac{\\lambda}{|\\mathcal{C}|}\\right)\\ell(h% _{\\theta}(x),y)\\mid(x,y)\\in S_{i}\\right\\}\\cup\\left\\{\\lambda\\ell(h_{\\theta}(x),% y)\\mid(x,y)\\in S_{j},\\forall j\\neq i\\right\\} { | caligraphic_C | ( 1 - italic_ + divide start_ARG italic_ end_ARG start_ARG | caligraphic_C | end_ARG ) roman_l ( italic_h start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x ) , italic_y )  ( italic_x , italic_y )  italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }  { italic_ roman_l ( italic_h start_POSTSUBSCRIPT italic_ end_POSTSUBSCRIPT ( italic_x ) , italic_y )  ( italic_x , italic_y )  italic_S start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT ,  italic_j = italic_i } . As per  4 , we know that  l l \\ell roman_l  takes its values in  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ] , hence this set is composed of  m m m italic_m  random variables with values in  [ 0 , | C |  ( 1   +  | C | ) ] 0 C 1   C \\left[0,|\\mathcal{C}|\\left(1-\\lambda+\\frac{\\lambda}{|\\mathcal{C}|}\\right)\\right] [ 0 , | caligraphic_C | ( 1 - italic_ + divide start_ARG italic_ end_ARG start_ARG | caligraphic_C | end_ARG ) ]  and  ( | C |  1 )  m C 1 m (\\left\\lvert{\\mathcal{C}}\\right\\rvert-1)m ( | caligraphic_C | - 1 ) italic_m  others with values in  [ 0 ,  ] 0  \\left[0,\\lambda\\right] [ 0 , italic_ ] . Finally, note that by definition  E  [ L i   (  ) ] = R i   (  ) E delimited-[] superscript subscript L i   superscript subscript R i   \\mathbb{E}\\left[\\mathcal{L}_{i}^{\\lambda}(\\theta)\\right]=\\mathcal{R}_{i}^{% \\lambda}(\\theta) blackboard_E [ caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ ) ] = caligraphic_R start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ ) . Where the expectation is taken over the independent sampling of the local data sets  ( S i ) i  C subscript subscript S i i C (S_{i})_{i\\in\\mathcal{C}} ( italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i  caligraphic_C end_POSTSUBSCRIPT  from the local data distribution of the correct clients. Thus using Hoeffding inequality, we get",
            "For lines  2 2 2 2  and  5 5 5 5 , since Assumption ( 4 ) holds true, we can use the results from  Lemma   2 . Let    Pdim  ( H )  log  ( 2  m  | C | ) + log  ( 4 /  )   Pdim H 2 m C 4  \\beta\\coloneqq\\mathrm{Pdim}{\\left(\\mathcal{H}\\right)}\\log(2m\\left\\lvert{% \\mathcal{C}}\\right\\rvert)+\\log(4/\\delta) italic_  roman_Pdim ( caligraphic_H ) roman_log ( 2 italic_m | caligraphic_C | ) + roman_log ( 4 / italic_ ) . By applying  Lemma   2  with   / 2  2 \\delta/2 italic_ / 2  for both  ( 2 ) 2 (2) ( 2 )  and  ( 5 ) 5 (5) ( 5 ) , we get that with probability at least  1   1  1-\\delta 1 - italic_ :",
            "Here, our goal is to reasonably approximate the value of lambda    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  for which the right-hand side of ( 24 ) is minimized. To do so, we first ignore the asymptotically negligible terms. Essentially, we consider that  T T T italic_T  is large enough so that the term  ( 1   2  L ) T  L   ( L i   (  i 0 )  L i   (  ^ i  ) ) superscript 1  2 L T L  superscript subscript L i  superscript subscript  i 0 superscript subscript L i  superscript subscript ^  i  \\left(1-\\frac{\\mu}{2L}\\right)^{T}\\frac{L}{\\mu}\\left(\\mathcal{L}_{i}^{\\lambda}(% \\theta_{i}^{0})-\\mathcal{L}_{i}^{\\lambda}(\\widehat{\\theta}_{i}^{\\lambda})\\right) ( 1 - divide start_ARG italic_ end_ARG start_ARG 2 italic_L end_ARG ) start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT divide start_ARG italic_L end_ARG start_ARG italic_ end_ARG ( caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 0 end_POSTSUPERSCRIPT ) - caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ) )  is close enough to zero. In the reminder, we denote by   i  superscript subscript  i \\theta_{i}^{\\infty} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  the output of Algorithm  1  for  T    T T\\rightarrow\\infty italic_T   . Even after ignoring this term, seeking the optimal value    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  is the solution to a 4th-degree equation, which may not admit a close-form solution in general. To make the computation of an approximate minimizer more tractable, we proceed with the following simplification using the fact that  a + b  a + b a b a b \\sqrt{a+b}\\leq\\sqrt{a}+\\sqrt{b} square-root start_ARG italic_a + italic_b end_ARG  square-root start_ARG italic_a end_ARG + square-root start_ARG italic_b end_ARG  for any  a , b  0 a b 0 a,b\\geq 0 italic_a , italic_b  0 :"
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A4.EGx5",
        "footnotes": [],
        "references": [
            "Consider the mean estimation setting described. For any  i  C i C i\\in\\mathcal{C} italic_i  caligraphic_C , let  y i  superscript subscript y i  y_{i}^{\\lambda} italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  be as defined in ( 5 ) with an aggregation rule  F F F italic_F  that satisfies  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robustness. Then the following holds true:",
            "To validate our theoretical observations on Byzantine-robust federated mean estimation, we run a series of experiments using simulated datasets sampled from 1-dimensional Gaussian distributions. Specifically, for each of the  n  f n f n-f italic_n - italic_f  correct clients, we consider that their distribution is such that  D i = N  (  i ,  2 ) subscript D i N subscript  i superscript  2 \\mathcal{D}_{i}=\\mathcal{N}(\\mu_{i},\\sigma^{2}) caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = caligraphic_N ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , where the unknown local means  (  i ) i  C subscript subscript  i i C (\\mu_{i})_{i\\in\\mathcal{C}} ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i  caligraphic_C end_POSTSUBSCRIPT  have been sampled i.i.d. from a Gaussian distribution  N  ( 10 ,  h 2 ) N 10 superscript subscript  h 2 \\mathcal{N}(10,\\sigma_{h}^{2}) caligraphic_N ( 10 , italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) 2 2 2 We use a mean at  10 10 10 10  to break the symmetry of the mean simulation around  0 0 . This is simply to present a case in which the sign-flipping attack is indeed disrupting the federated mean estimation procedure , where   h subscript  h \\sigma_{h} italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  determines the expected squared distance between the local means of each correct client. Each honest client  i  C i C i\\in\\mathcal{C} italic_i  caligraphic_C  samples  m m m italic_m  datapoints from  D i subscript D i \\mathcal{D}_{i} caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and the error is computed with respect to   i subscript  i \\mu_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . To evaluate the theoretical expression of    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , defined in ( 6 ), in this scenario, we replace the distances of the form  (  i     C ) 2 superscript subscript  i subscript    C 2 (\\mu_{i}-\\overline{\\mu}_{\\mathcal{C}})^{2} ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  by the variance  ( 1  1 / n  f )   h 2 1 1 n f superscript subscript  h 2 (1-\\nicefrac{{1}}{{n-f}})\\sigma_{h}^{2} ( 1 - / start_ARG 1 end_ARG start_ARG italic_n - italic_f end_ARG ) italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We present in  Figure   1  the average error (squared distance to the true mean) of the estimator defined as per ( 5 ) on  20 20 20 20  runs, for different values of    \\lambda italic_ . The robust aggregation being used here is an NNM pre-aggregation rule  (Allouah et al.,,  2023 )  followed by trimmed mean  (Yin et al.,,  2018 )  and adversarial clients implement the sign-flipping attack. Below we analyze our result presented in the first row of  Figure   1 .",
            "This can be gleaned from settings where the adversarial fraction is substantial (over  6 6 6 6  adversarial clients, e.g. green and red curves in  Figure   2 ), when the heterogeneity is large (small values of    \\alpha italic_ , e.g. blue and orange curves in  Figure   6  in  Appendix   D ) and when the local dataset is large enough (e.g.  Figure   2(b)  and  Figure   2(c) ). In situations where the adversarial fraction is critical ( f = 9 f 9 f=9 italic_f = 9 , corresponding to red curves in  Figure   2  ), robust federated learning completely fails, achieving accuracy scores under  50 % percent 50 50\\% 50 %  on the local test datasets.  Figure   5  in  Appendix   D  further illustrates this point, showing that even local learning can be better than state-of-the-art robust Federated Learning in these circumstances.",
            "By developing the right-hand side ( 15 ) and using Jensens inequality, we get",
            "with   := 6  L    G 2  2 assign  6 L  superscript G 2 superscript  2 \\alpha:=\\frac{6L\\kappa G^{2}}{\\mu^{2}} italic_ := divide start_ARG 6 italic_L italic_ italic_G start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG start_ARG italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT end_ARG . Now the right-hand side is a quadratic function of    \\lambda italic_  which can be easily minimized. Specifically, the solution of ( 25 ) is"
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A4.EGx6",
        "footnotes": [],
        "references": [
            "To validate our theoretical observations on Byzantine-robust federated mean estimation, we run a series of experiments using simulated datasets sampled from 1-dimensional Gaussian distributions. Specifically, for each of the  n  f n f n-f italic_n - italic_f  correct clients, we consider that their distribution is such that  D i = N  (  i ,  2 ) subscript D i N subscript  i superscript  2 \\mathcal{D}_{i}=\\mathcal{N}(\\mu_{i},\\sigma^{2}) caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = caligraphic_N ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , italic_ start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) , where the unknown local means  (  i ) i  C subscript subscript  i i C (\\mu_{i})_{i\\in\\mathcal{C}} ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i  caligraphic_C end_POSTSUBSCRIPT  have been sampled i.i.d. from a Gaussian distribution  N  ( 10 ,  h 2 ) N 10 superscript subscript  h 2 \\mathcal{N}(10,\\sigma_{h}^{2}) caligraphic_N ( 10 , italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) 2 2 2 We use a mean at  10 10 10 10  to break the symmetry of the mean simulation around  0 0 . This is simply to present a case in which the sign-flipping attack is indeed disrupting the federated mean estimation procedure , where   h subscript  h \\sigma_{h} italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT  determines the expected squared distance between the local means of each correct client. Each honest client  i  C i C i\\in\\mathcal{C} italic_i  caligraphic_C  samples  m m m italic_m  datapoints from  D i subscript D i \\mathcal{D}_{i} caligraphic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  and the error is computed with respect to   i subscript  i \\mu_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT . To evaluate the theoretical expression of    superscript  \\lambda^{*} italic_ start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT , defined in ( 6 ), in this scenario, we replace the distances of the form  (  i     C ) 2 superscript subscript  i subscript    C 2 (\\mu_{i}-\\overline{\\mu}_{\\mathcal{C}})^{2} ( italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - over  start_ARG italic_ end_ARG start_POSTSUBSCRIPT caligraphic_C end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  by the variance  ( 1  1 / n  f )   h 2 1 1 n f superscript subscript  h 2 (1-\\nicefrac{{1}}{{n-f}})\\sigma_{h}^{2} ( 1 - / start_ARG 1 end_ARG start_ARG italic_n - italic_f end_ARG ) italic_ start_POSTSUBSCRIPT italic_h end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT . We present in  Figure   1  the average error (squared distance to the true mean) of the estimator defined as per ( 5 ) on  20 20 20 20  runs, for different values of    \\lambda italic_ . The robust aggregation being used here is an NNM pre-aggregation rule  (Allouah et al.,,  2023 )  followed by trimmed mean  (Yin et al.,,  2018 )  and adversarial clients implement the sign-flipping attack. Below we analyze our result presented in the first row of  Figure   1 .",
            "Figure   2 , as well as  Figure   6  from  Appendix   D , show the final local test accuracy performance as a function of the degree of collaboration (   \\lambda italic_ ) for different values of  f , m f m f,m italic_f , italic_m  and    \\alpha italic_ . These experimental results shed light on the impact of these different factors and allow us to confirm the main insights of our theory.",
            "This can be gleaned from settings where the adversarial fraction is substantial (over  6 6 6 6  adversarial clients, e.g. green and red curves in  Figure   2 ), when the heterogeneity is large (small values of    \\alpha italic_ , e.g. blue and orange curves in  Figure   6  in  Appendix   D ) and when the local dataset is large enough (e.g.  Figure   2(b)  and  Figure   2(c) ). In situations where the adversarial fraction is critical ( f = 9 f 9 f=9 italic_f = 9 , corresponding to red curves in  Figure   2  ), robust federated learning completely fails, achieving accuracy scores under  50 % percent 50 50\\% 50 %  on the local test datasets.  Figure   5  in  Appendix   D  further illustrates this point, showing that even local learning can be better than state-of-the-art robust Federated Learning in these circumstances.",
            "Combining Assumption  1  and Assumption  3 , we easily get that   2      ^ i   2  L i   (  )  L i ,    L 2      ^ i   2  2 superscript norm  superscript subscript ^  i  2 superscript subscript L i   superscript subscript L i  L 2 superscript norm  superscript subscript ^  i  2 \\tfrac{\\mu}{2}\\|\\theta-\\widehat{\\theta}_{i}^{\\lambda}\\|^{2}\\leq\\mathcal{L}_{i}% ^{\\lambda}(\\theta)-\\mathcal{L}_{i,*}^{\\lambda}\\leq\\tfrac{L}{2}\\|\\theta-% \\widehat{\\theta}_{i}^{\\lambda}\\|^{2} divide start_ARG italic_ end_ARG start_ARG 2 end_ARG  italic_ - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ ) - caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  divide start_ARG italic_L end_ARG start_ARG 2 end_ARG  italic_ - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  for all       \\theta\\in\\Theta italic_  roman_ . Using this, and specializing ( 16 ) for  t = T  1 t T 1 t=T-1 italic_t = italic_T - 1 , we have",
            "For line  3 3 3 3 , recall that assumptions  1  and  2 , that   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG , and that  F F F italic_F  is assumed to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. Hence, Lemma  1  holds true. we use ( 16 ) in the proof of Lemma  1 , to get"
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A4.EGx7",
        "footnotes": [],
        "references": [
            "Let assumptions  1 ,  2 ,  3 , and  4  hold, and let    \\Phi roman_  be a function such that ( 7 ) holds. Consider Algorithm  1  with learning rate   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG ,    [ 0 , 1 ]  0 1 \\lambda\\in[0,1] italic_  [ 0 , 1 ] , and assume the aggregation function  F F F italic_F  to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. Then, for any   > 0 , T  1 formulae-sequence  0 T 1 \\delta>0,T\\geq 1 italic_ > 0 , italic_T  1 , we have with probability at least  1   1  1-\\delta 1 - italic_  (over the choice of samples) that",
            "Define the symmetric difference hypothesis space :  H    H := { h  ( x )  h   ( x ) } : h , h   H : assign H  H direct-sum h x superscript h  x h superscript h  H \\mathcal{H}\\Delta\\mathcal{H}:=\\{h(x)\\oplus h^{\\prime}(x)\\}:h,h^{\\prime}\\in% \\mathcal{H} caligraphic_H roman_ caligraphic_H := { italic_h ( italic_x )  italic_h start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ( italic_x ) } : italic_h , italic_h start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  caligraphic_H . Then following  (Blitzer et al.,,  2007 ) ,  d H    H subscript d H  H d_{\\mathcal{H}\\Delta\\mathcal{H}} italic_d start_POSTSUBSCRIPT caligraphic_H roman_ caligraphic_H end_POSTSUBSCRIPT  satisfies ( 7 ).",
            "For lines  1 1 1 1  and  6 6 6 6 , we use the property of our loss function described in ( 7 ), to write"
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A4.EGx8",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": "",
        "table": "A4.EGx9",
        "footnotes": [],
        "references": [
            "Equation  ( 9 )"
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A4.EGx10",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "",
        "table": "A4.EGx11",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A4.EGx12",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A4.EGx13",
        "footnotes": [],
        "references": [
            "Conclusion.  By substituting the above in ( 13 ), we get the following."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A4.EGx14",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "",
        "table": "A4.EGx15",
        "footnotes": [],
        "references": [
            "By developing the right-hand side ( 15 ) and using Jensens inequality, we get"
        ]
    },
    "id_table_16": {
        "caption": "",
        "table": "A4.EGx16",
        "footnotes": [],
        "references": [
            "Combining Assumption  1  and Assumption  3 , we easily get that   2      ^ i   2  L i   (  )  L i ,    L 2      ^ i   2  2 superscript norm  superscript subscript ^  i  2 superscript subscript L i   superscript subscript L i  L 2 superscript norm  superscript subscript ^  i  2 \\tfrac{\\mu}{2}\\|\\theta-\\widehat{\\theta}_{i}^{\\lambda}\\|^{2}\\leq\\mathcal{L}_{i}% ^{\\lambda}(\\theta)-\\mathcal{L}_{i,*}^{\\lambda}\\leq\\tfrac{L}{2}\\|\\theta-% \\widehat{\\theta}_{i}^{\\lambda}\\|^{2} divide start_ARG italic_ end_ARG start_ARG 2 end_ARG  italic_ - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  caligraphic_L start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT ( italic_ ) - caligraphic_L start_POSTSUBSCRIPT italic_i ,  end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  divide start_ARG italic_L end_ARG start_ARG 2 end_ARG  italic_ - over^ start_ARG italic_ end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_ end_POSTSUPERSCRIPT  start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT  for all       \\theta\\in\\Theta italic_  roman_ . Using this, and specializing ( 16 ) for  t = T  1 t T 1 t=T-1 italic_t = italic_T - 1 , we have",
            "For line  3 3 3 3 , recall that assumptions  1  and  2 , that   = 1 2  L  1 2 L \\eta=\\frac{1}{2L} italic_ = divide start_ARG 1 end_ARG start_ARG 2 italic_L end_ARG , and that  F F F italic_F  is assumed to be  ( f ,  ) f  (f,\\kappa) ( italic_f , italic_ ) -robust. Hence, Lemma  1  holds true. we use ( 16 ) in the proof of Lemma  1 , to get"
        ]
    },
    "id_table_17": {
        "caption": "",
        "table": "A4.EGx17",
        "footnotes": [],
        "references": []
    },
    "id_table_18": {
        "caption": "",
        "table": "A4.EGx18",
        "footnotes": [],
        "references": []
    },
    "id_table_19": {
        "caption": "",
        "table": "A4.EGx19",
        "footnotes": [],
        "references": []
    }
}