{
    "S3.T1.2.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" colspan=\"3\" id=\"S3.T1.2.1.1.1.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.1.1.1.1.1\">(a) Comparison with Previous Works</span></th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.2.2.1\">Previous works</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S3.T1.2.1.2.2.2\">\n<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.2.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.1.1\">1. MLLM quantization, Pruning, and KG distillation.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.2.1\">2. ViT pruning without LLM guidance.</td>\n</tr>\n</table>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.3.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.3.3.1\">This work</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\" colspan=\"2\" id=\"S3.T1.2.1.3.3.2\">ViT pruning under LLM guidance.</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.4.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt\" colspan=\"3\" id=\"S3.T1.2.1.4.4.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.2.1.4.4.1.1\">(b) Latency Breakdown (single image and single text)</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.5.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.5.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S3.T1.2.1.5.1.1.1\">MLLM (LLaVA 7B)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S3.T1.2.1.5.1.2\">Segment Anything (SAM)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.6.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.6.2.1\">ViT-H</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.1.6.2.2\">Mask Decoder</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.7.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.7.3.1\">71.0 ms</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.7.3.2\">97.7 ms</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.2.1.7.3.3\">8.9 ms</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.8.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.8.4.1\">40%</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t\" id=\"S3.T1.2.1.8.4.2\">55%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S3.T1.2.1.8.4.3\">5%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 1 :  (a) Comparison with previous works. (b) MLLM guide segmentation model latency breakdown. We conudct the benchmark using an RTX A6000 GPU with the RIO datasetÂ  [ 38 ] .",
        "footnotes": [
            "[38] \nMengxue Qu, Yu Wu, Wu Liu, Xiaodan Liang, Jingkuan Song, Yao Zhao, and Yunchao Wei.\n \n Rio: A benchmark for reasoning intention-oriented objects in open environments.\n \n Advances in Neural Information Processing Systems , 36, 2024.\n \n"
        ],
        "references": [
            "Nevertheless, a key challenge remains: the computational expense of two-stage framework is extremely high. Previous researchÂ [41, 51, 59] has primarily focused on MLLMâ€™s compression and pruning, largely overlooking the segmentation model, as depicted inÂ Tab.Â 1.(a). The segmentation model, as shown inÂ Tab.Â 1(b), requires even more computational resources than MLLM. Following this, we will explore ways to enhance the efficiency of the segmentation model under the guidance of MLLMâ€™s instruction token âŸ¨âŸ¨\\langleâŸ¨SEGâŸ©âŸ©\\rangleâŸ©.",
            "As illustrated inÂ Tab.Â 1(b), the ViTâ€™s computation latency constitutes more than 90% of the total latency of the entire segmentation model. Thus, enhancing the ViTâ€™s efficiency is essential for optimizing the segmentation modelâ€™s performance. This work introduces a vision-language guided token pruning method, which accelerates ViT when handling TOS. We detail the mechanism in the following sections.",
            "Refer toÂ Fig.Â 2, a segmentation model Fsâ¢eâ¢gsubscriptFğ‘ ğ‘’ğ‘”\\textbf{F}_{seg}F start_POSTSUBSCRIPT italic_s italic_e italic_g end_POSTSUBSCRIPT typically consists of two components: a vision backbone Feâ¢nâ¢csubscriptFğ‘’ğ‘›ğ‘\\textbf{F}_{enc}F start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT and a mask decoder Fdâ¢eâ¢csubscriptFğ‘‘ğ‘’ğ‘\\textbf{F}_{dec}F start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT. We illustrate this with SAM. The vision backbone in SAM is a ViTÂ [9, 12]. The ViT extracts features from the input image and passes the feature map to the mask decoder. The mask decoder then predicts the target mask based on the feature map and prompt embeddings. Within our system, the prompt embedding is represented by the âŸ¨âŸ¨\\langleâŸ¨SEGâŸ©âŸ©\\rangleâŸ© token embedding from MLLM, which serves as an ambiguous indicator towards the intended affordance to solve the task.\nAs depicted inÂ Tab.Â 1(b), given the large latency of vision backbone, the segmentation model will benefit significantly from our proposed VLTP."
        ]
    },
    "S3.T1.2.1.2.2.2.1": {
        "table": "<table class=\"ltx_tabular ltx_align_middle\" id=\"S3.T1.2.1.2.2.2.1\">\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.1\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.1.1\">1. MLLM quantization, Pruning, and KG distillation.</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.2.1.2.2.2.1.2\">\n<td class=\"ltx_td ltx_nopad_r ltx_align_left\" id=\"S3.T1.2.1.2.2.2.1.2.1\">2. ViT pruning without LLM guidance.</td>\n</tr>\n</table>\n\n",
        "caption": "Table 1 :  (a) Comparison with previous works. (b) MLLM guide segmentation model latency breakdown. We conudct the benchmark using an RTX A6000 GPU with the RIO datasetÂ  [ 38 ] .",
        "footnotes": [
            "[38] \nMengxue Qu, Yu Wu, Wu Liu, Xiaodan Liang, Jingkuan Song, Yao Zhao, and Yunchao Wei.\n \n Rio: A benchmark for reasoning intention-oriented objects in open environments.\n \n Advances in Neural Information Processing Systems , 36, 2024.\n \n"
        ],
        "references": [
            "Nevertheless, a key challenge remains: the computational expense of two-stage framework is extremely high. Previous researchÂ [41, 51, 59] has primarily focused on MLLMâ€™s compression and pruning, largely overlooking the segmentation model, as depicted inÂ Tab.Â 1.(a). The segmentation model, as shown inÂ Tab.Â 1(b), requires even more computational resources than MLLM. Following this, we will explore ways to enhance the efficiency of the segmentation model under the guidance of MLLMâ€™s instruction token âŸ¨âŸ¨\\langleâŸ¨SEGâŸ©âŸ©\\rangleâŸ©.",
            "As illustrated inÂ Tab.Â 1(b), the ViTâ€™s computation latency constitutes more than 90% of the total latency of the entire segmentation model. Thus, enhancing the ViTâ€™s efficiency is essential for optimizing the segmentation modelâ€™s performance. This work introduces a vision-language guided token pruning method, which accelerates ViT when handling TOS. We detail the mechanism in the following sections.",
            "Refer toÂ Fig.Â 2, a segmentation model Fsâ¢eâ¢gsubscriptFğ‘ ğ‘’ğ‘”\\textbf{F}_{seg}F start_POSTSUBSCRIPT italic_s italic_e italic_g end_POSTSUBSCRIPT typically consists of two components: a vision backbone Feâ¢nâ¢csubscriptFğ‘’ğ‘›ğ‘\\textbf{F}_{enc}F start_POSTSUBSCRIPT italic_e italic_n italic_c end_POSTSUBSCRIPT and a mask decoder Fdâ¢eâ¢csubscriptFğ‘‘ğ‘’ğ‘\\textbf{F}_{dec}F start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT. We illustrate this with SAM. The vision backbone in SAM is a ViTÂ [9, 12]. The ViT extracts features from the input image and passes the feature map to the mask decoder. The mask decoder then predicts the target mask based on the feature map and prompt embeddings. Within our system, the prompt embedding is represented by the âŸ¨âŸ¨\\langleâŸ¨SEGâŸ©âŸ©\\rangleâŸ© token embedding from MLLM, which serves as an ambiguous indicator towards the intended affordance to solve the task.\nAs depicted inÂ Tab.Â 1(b), given the large latency of vision backbone, the segmentation model will benefit significantly from our proposed VLTP."
        ]
    },
    "S5.T2.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T2.4.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T2.4.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.4.1.1.1.2\">Common</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.4.1.1.1.3\">Uncommon</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.4.1.2.2.1\">mIoU (%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T2.4.1.2.2.2\">mIoU (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt\" id=\"S5.T2.4.1.3.1.1\">MDETR&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib19\" title=\"\">19</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.4.1.3.1.2\">44.14</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T2.4.1.3.1.3\">22.03</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.4.2.1\">TOIST&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib25\" title=\"\">25</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.4.2.2\">45.07</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.4.2.3\">19.41</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.5.3.1\">Polyformer&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib29\" title=\"\">29</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.5.3.2\">48.75</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.5.3.3\">26.77</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.6.4.1\">GROUNDHOG&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib56\" title=\"\">56</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.6.4.2\">57.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.6.4.3\">33.9</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.7.5.1\">SAM ViT-H</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.7.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.4.1.7.5.2.1\">60.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.7.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.4.1.7.5.3.1\">37.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.8.6\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.4.1.8.6.1\">SAM ViT-L</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.8.6.2\">55.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.4.1.8.6.3\">32.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.4.1.9.7\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T2.4.1.9.7.1\">SAM ViT-B</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.4.1.9.7.2\">50.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.4.1.9.7.3\">27.9</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 2 :  Comparison with previous works on RIOÂ  [ 38 ]  dataset.",
        "footnotes": [
            "[38] \nMengxue Qu, Yu Wu, Wu Liu, Xiaodan Liang, Jingkuan Song, Yao Zhao, and Yunchao Wei.\n \n Rio: A benchmark for reasoning intention-oriented objects in open environments.\n \n Advances in Neural Information Processing Systems , 36, 2024.\n \n",
            "[19] \nAishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion.\n \n Mdetr-modulated detection for end-to-end multi-modal understanding.\n \n In  Proceedings of the IEEE/CVF international conference on computer vision , pages 1780â€“1790, 2021.\n \n",
            "[25] \nPengfei Li, Beiwen Tian, Yongliang Shi, Xiaoxue Chen, Hao Zhao, Guyue Zhou, and Ya-Qin Zhang.\n \n Toist: Task oriented instance segmentation transformer with noun-pronoun distillation.\n \n Advances in Neural Information Processing Systems , 35:17597â€“17611, 2022.\n \n",
            "[29] \nJiang Liu, Hui Ding, Zhaowei Cai, Yuting Zhang, RaviÂ Kumar Satzoda, Vijay Mahadevan, and R Manmatha.\n \n Polyformer: Referring image segmentation as sequential polygon generation.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 18653â€“18663, 2023.\n \n",
            "[56] \nYichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai.\n \n Groundhog: Grounding large language models to holistic segmentation.\n \n In  Proceedings of the IEEE/CVF conference on computer vision and pattern recognition , pages 14227â€“14238, 2024.\n \n"
        ],
        "references": [
            "We selected LLaVA 7BÂ [28] to serve as MLLM. LLaVA processes both textual tasks and images as inputs to produce reasoning guidance âŸ¨âŸ¨\\langleâŸ¨SEGâŸ©âŸ©\\rangleâŸ©. For the segmentation model, we opted for SAM, a well-known foundation model that enables prompt-guided segmentation. Following previous researchÂ [24, 49, 37], we perform end-to-end visual instruction fine-tuning on the entire system. For the LLaVA module, we utilize LoRAÂ [14] for efficient fine-tuning, while for the SAM module, we freeze the ViT backbone and only train the mask decoder.Â TableÂ 2 shows the reasoning segmentation accuracy of the entire system on the RIO dataset, along with a comparison to earlier studies. Compared to the SOTA methodÂ [56], guided by MLLM, the SAM with the ViT-H backbone model, exceeds it by 2.5% on RIO common parts and 3.3% on RIO uncommon parts. This result highlights the importance of integrating MLLM with the segmentation model in visual reasoning tasks."
        ]
    },
    "S5.T3.3.3": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T3.3.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.3.4.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.3.4.1.2\">GFLOPs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.3.3.4.1.3\">mIoU (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.3.3.5.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T3.3.3.5.1.1\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.3.3.5.1.2\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T3.3.3.5.1.3\">60.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.3.6.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.3.3.6.2.1\">+VLTP@Direct (&#9824;0)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.3.6.2.2\">2227</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.3.3.6.2.3\">39.5</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.1\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T3.1.1.1.1\">+VLTP@Finetune (<math alttext=\"F_{prune}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.1.1.1.1.m1.1\"><semantics id=\"S5.T3.1.1.1.1.m1.1a\"><msub id=\"S5.T3.1.1.1.1.m1.1.1\" xref=\"S5.T3.1.1.1.1.m1.1.1.cmml\"><mi id=\"S5.T3.1.1.1.1.m1.1.1.2\" xref=\"S5.T3.1.1.1.1.m1.1.1.2.cmml\">F</mi><mrow id=\"S5.T3.1.1.1.1.m1.1.1.3\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.cmml\"><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.2\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.2.cmml\">p</mi><mo id=\"S5.T3.1.1.1.1.m1.1.1.3.1\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.3\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.3.cmml\">r</mi><mo id=\"S5.T3.1.1.1.1.m1.1.1.3.1a\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.4\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.4.cmml\">u</mi><mo id=\"S5.T3.1.1.1.1.m1.1.1.3.1b\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.5\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.5.cmml\">n</mi><mo id=\"S5.T3.1.1.1.1.m1.1.1.3.1c\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.1.1.1.1.m1.1.1.3.6\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.6.cmml\">e</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.1.m1.1b\"><apply id=\"S5.T3.1.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T3.1.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T3.1.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.2\">&#119865;</ci><apply id=\"S5.T3.1.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3\"><times id=\"S5.T3.1.1.1.1.m1.1.1.3.1.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.1\"/><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.2.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.2\">&#119901;</ci><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.3.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.3\">&#119903;</ci><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.4.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.4\">&#119906;</ci><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.5.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.5\">&#119899;</ci><ci id=\"S5.T3.1.1.1.1.m1.1.1.3.6.cmml\" xref=\"S5.T3.1.1.1.1.m1.1.1.3.6\">&#119890;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.1.m1.1c\">F_{prune}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T3.1.1.1.1.m1.1d\">italic_F start_POSTSUBSCRIPT italic_p italic_r italic_u italic_n italic_e end_POSTSUBSCRIPT</annotation></semantics></math>) (&#9824;5)</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.1.2\">2227</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.1.1.3\">50.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.3.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T3.3.3.3.2\">+VLTP@Finetune (<math alttext=\"F_{dec}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.2.2.2.1.m1.1\"><semantics id=\"S5.T3.2.2.2.1.m1.1a\"><msub id=\"S5.T3.2.2.2.1.m1.1.1\" xref=\"S5.T3.2.2.2.1.m1.1.1.cmml\"><mi id=\"S5.T3.2.2.2.1.m1.1.1.2\" xref=\"S5.T3.2.2.2.1.m1.1.1.2.cmml\">F</mi><mrow id=\"S5.T3.2.2.2.1.m1.1.1.3\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.cmml\"><mi id=\"S5.T3.2.2.2.1.m1.1.1.3.2\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.2.cmml\">d</mi><mo id=\"S5.T3.2.2.2.1.m1.1.1.3.1\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.2.2.2.1.m1.1.1.3.3\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.3.cmml\">e</mi><mo id=\"S5.T3.2.2.2.1.m1.1.1.3.1a\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.2.2.2.1.m1.1.1.3.4\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.4.cmml\">c</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.2.2.2.1.m1.1b\"><apply id=\"S5.T3.2.2.2.1.m1.1.1.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T3.2.2.2.1.m1.1.1.1.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1\">subscript</csymbol><ci id=\"S5.T3.2.2.2.1.m1.1.1.2.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.2\">&#119865;</ci><apply id=\"S5.T3.2.2.2.1.m1.1.1.3.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3\"><times id=\"S5.T3.2.2.2.1.m1.1.1.3.1.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.1\"/><ci id=\"S5.T3.2.2.2.1.m1.1.1.3.2.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.2\">&#119889;</ci><ci id=\"S5.T3.2.2.2.1.m1.1.1.3.3.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.3\">&#119890;</ci><ci id=\"S5.T3.2.2.2.1.m1.1.1.3.4.cmml\" xref=\"S5.T3.2.2.2.1.m1.1.1.3.4\">&#119888;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.2.2.2.1.m1.1c\">F_{dec}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T3.2.2.2.1.m1.1d\">italic_F start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT</annotation></semantics></math> + <math alttext=\"F_{prune}\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.3.3.3.2.m2.1\"><semantics id=\"S5.T3.3.3.3.2.m2.1a\"><msub id=\"S5.T3.3.3.3.2.m2.1.1\" xref=\"S5.T3.3.3.3.2.m2.1.1.cmml\"><mi id=\"S5.T3.3.3.3.2.m2.1.1.2\" xref=\"S5.T3.3.3.3.2.m2.1.1.2.cmml\">F</mi><mrow id=\"S5.T3.3.3.3.2.m2.1.1.3\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.cmml\"><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.2\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.2.cmml\">p</mi><mo id=\"S5.T3.3.3.3.2.m2.1.1.3.1\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.3\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.3.cmml\">r</mi><mo id=\"S5.T3.3.3.3.2.m2.1.1.3.1a\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.4\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.4.cmml\">u</mi><mo id=\"S5.T3.3.3.3.2.m2.1.1.3.1b\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.5\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.5.cmml\">n</mi><mo id=\"S5.T3.3.3.3.2.m2.1.1.3.1c\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\">&#8290;</mo><mi id=\"S5.T3.3.3.3.2.m2.1.1.3.6\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.6.cmml\">e</mi></mrow></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.3.3.3.2.m2.1b\"><apply id=\"S5.T3.3.3.3.2.m2.1.1.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T3.3.3.3.2.m2.1.1.1.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1\">subscript</csymbol><ci id=\"S5.T3.3.3.3.2.m2.1.1.2.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.2\">&#119865;</ci><apply id=\"S5.T3.3.3.3.2.m2.1.1.3.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3\"><times id=\"S5.T3.3.3.3.2.m2.1.1.3.1.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.1\"/><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.2.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.2\">&#119901;</ci><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.3.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.3\">&#119903;</ci><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.4.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.4\">&#119906;</ci><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.5.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.5\">&#119899;</ci><ci id=\"S5.T3.3.3.3.2.m2.1.1.3.6.cmml\" xref=\"S5.T3.3.3.3.2.m2.1.1.3.6\">&#119890;</ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.3.3.3.2.m2.1c\">F_{prune}</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T3.3.3.3.2.m2.1d\">italic_F start_POSTSUBSCRIPT italic_p italic_r italic_u italic_n italic_e end_POSTSUBSCRIPT</annotation></semantics></math>) (&#9824;9)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.3.3.3.3\">2227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.3.3.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.3.3.3.4.1\">60.1</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 3 :  Comparison of training scheme. All results are reported based on RIO common dataset. â™  represents the extra finetune epochs.",
        "footnotes": [],
        "references": [
            "TableÂ 3 demonstrates the impact of different training strategies on final pruning outcomes. We assume a pruning rate of 50%, inserting the prune decoder at layers 16 and 24. This implies that starting from layer 16, only 50% of image tokens will be involved in the SAM ViT computation. The results indicate that when both Fpâ¢râ¢uâ¢nâ¢esubscriptğ¹ğ‘ğ‘Ÿğ‘¢ğ‘›ğ‘’F_{prune}italic_F start_POSTSUBSCRIPT italic_p italic_r italic_u italic_n italic_e end_POSTSUBSCRIPT and Fdâ¢eâ¢csubscriptğ¹ğ‘‘ğ‘’ğ‘F_{dec}italic_F start_POSTSUBSCRIPT italic_d italic_e italic_c end_POSTSUBSCRIPT are fine-tuned simultaneously, VLTP reduces the SAM ViTâ€™s computation from 2976 GFLOPs to 2227 GFLOPs, yielding approximately a 25% reduction in GFLOPs with only a 0.3% drop in mIoU. Consequently, in the subsequent sections, both the prune decoder and mask decoder will be fine-tuned to achieve optimal pruning results.",
            "TableÂ 4 illustrates the effects of pruning as the pruning rate rmsubscriptğ‘Ÿğ‘šr_{m}italic_r start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT increases. In this study, we set the pruning positions of SAM ViT at layers 16 and 24. Instead of finetuning the pruning and mask decoders from scratch, we base our experiments on a pre-trained model with a 50% pruning rate, as depicted inÂ Tab.Â 3. Initially, we increase the pruning rate without any finetuning, and the results inÂ Tab.Â 4 indicate an accuracy drop. Consequently, we subsequently finetune both the prune decoder and mask decoder together after adjusting the pruning rate. The experiment demonstrates that VLTP achieves approximately 35% GFLOPs with just a 0.8% mIoU reduction at a 70% pruning rate, and around 40% GFLOPs with only a 1% mIoU reduction at an 80% pruning rate, compared to the original SAM ViT-H. Additionally, we display the image patch-dropping visual results in Â Fig.Â 4. Under MLLM guidance, VLTP retains the most essential image patches while freezing irrelevant ones, in comparison to the ground truth."
        ]
    },
    "S5.T4.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T4.4.1.1.1.1\">Pruning Rate (%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.2\">50</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.3\">70</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.4\">70</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.5\">80</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.6\">80</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.7\">90</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.4.1.1.1.8\">90</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r\" id=\"S5.T4.4.1.2.2.1\">Training scheme</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.2\">-</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.3\">zero shot</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.4\">finetune</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.5\">zero shot</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.6\">finetune</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.7\">zero shot</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T4.4.1.2.2.8\">finetune</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T4.4.1.3.1.1\">GFLOPs</th>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.2\">2227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.3\">1930</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.3.1.4.1\">1930</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.5\">1782</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.3.1.6.1\">1782</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.7\">1636</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T4.4.1.3.1.8\">1636</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T4.4.1.4.2.1\">Common (%)</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.2\">60.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.3\">57.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.4.2.4.1\">59.6</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.5\">52.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.4.2.6.1\">59.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.7\">39.76</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T4.4.1.4.2.8\">51.3</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.4.1.5.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T4.4.1.5.3.1\">Uncommon (%)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.2\">37.5</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.3\">35</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.5.3.4.1\">37.9</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.5\">32.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.6\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T4.4.1.5.3.6.1\">37.6</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.7\">19.01</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T4.4.1.5.3.8\">28.4</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 4 :  Pruning ratio exploration for SAM ViT-H. Here we assume the pruning position is at layer 16 and layer 24 and both layerâ€™s pruning ratio is the same. All results are reported based on RIO common dataset.",
        "footnotes": [],
        "references": [
            "TableÂ 4 illustrates the effects of pruning as the pruning rate rmsubscriptğ‘Ÿğ‘šr_{m}italic_r start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT increases. In this study, we set the pruning positions of SAM ViT at layers 16 and 24. Instead of finetuning the pruning and mask decoders from scratch, we base our experiments on a pre-trained model with a 50% pruning rate, as depicted inÂ Tab.Â 3. Initially, we increase the pruning rate without any finetuning, and the results inÂ Tab.Â 4 indicate an accuracy drop. Consequently, we subsequently finetune both the prune decoder and mask decoder together after adjusting the pruning rate. The experiment demonstrates that VLTP achieves approximately 35% GFLOPs with just a 0.8% mIoU reduction at a 70% pruning rate, and around 40% GFLOPs with only a 1% mIoU reduction at an 80% pruning rate, compared to the original SAM ViT-H. Additionally, we display the image patch-dropping visual results in Â Fig.Â 4. Under MLLM guidance, VLTP retains the most essential image patches while freezing irrelevant ones, in comparison to the ground truth."
        ]
    },
    "S5.T5.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.4.1.1.1.1\">Position</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.4.1.1.1.2\">Pruning Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.4.1.1.1.3\">GFLOPS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.4.1.1.1.4\">mIoU (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.4.1.2.1.1\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.4.1.2.1.2\">{0}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.4.1.2.1.3\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T5.4.1.2.1.4\">60.4</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.3.2.1\">{8}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.3.2.2\">{20}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.3.2.3\">2529</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.3.2.4\">59.8 (-0.6)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.4.3.1\">{8}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.4.3.2\">{40}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.4.3.3\">2083</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.4.3.4\">53.1 (-7.3)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.5.4.1\">{8, 16}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.5.4.2\">{20, 40}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.5.4.3\">2232</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.5.4.4\">58.8 (-1.6)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.6.5.1\">{8, 16}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.6.5.2\">{20, 60}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.6.5.3\">1783</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.6.5.4\">53.7 (-6.7)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.7.6.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.7.6.2\">{50, 50}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.7.6.3\">2227</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.7.6.4\">60.1 (-0.3)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.8.7\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.8.7.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.8.7.2\">{80, 80}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.8.7.3\">1782</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.8.7.4\">58.9 (-1.2)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.9.8\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.9.8.1\">{8, 16, 24}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.9.8.2\">{20, 40, 40}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.9.8.3\">2232</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.4.1.9.8.4\">58.9 (-1.5)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.10.9\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.10.9.1\">{8, 16, 24}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.10.9.2\">{20, 40, 60}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.10.9.3\">1853</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.4.1.10.9.4\">53 (-7.4)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.4.1.11.10\">\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.1.11.10.1\">{8, 16, 24}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.1.11.10.2\">{50, 50, 50}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.1.11.10.3\">1860</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b\" id=\"S5.T5.4.1.11.10.4\">53.3 (-7.1)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 5 :  Investigation of SAM ViT-H pruning locations. The table presents the maximum accuracy and the most efficient outcome (measured in GFLOPs) for each pruning position combination. All outcomes are derived from the RIO common dataset. Each invocation of the prune decoder adds an additional 6 GFLOPs.",
        "footnotes": [],
        "references": [
            "InÂ Tab.Â 5, we illustrate the influence of different pruning positions on the ultimate pruning accuracy. The findings suggest that initiating pruning from layer 8 leads to a greater accuracy reduction compared to starting from layer 16, assuming the same level of ViT computation reduction. We attribute this to inadequate image feature extraction. Image patches that arenâ€™t directly pertinent to reasoning tasks still contribute to the final mask generation. Consequently, removing these patches at an earlier layer can negatively impact the final mask generation. Â FigureÂ 5 visualizes the patch dropping at various layer positions."
        ]
    },
    "S5.T6.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T6.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T6.4.1.1.1.1\">Position</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T6.4.1.1.1.2\">Pruning Rate</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T6.4.1.1.1.3\">GFLOPS</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T6.4.1.1.1.4\">mIoU (%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.4.1.2.1.1\">{16}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.4.1.2.1.2\">{50}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.4.1.2.1.3\">2221</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T6.4.1.2.1.4\">59.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.3.2.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.3.2.2\">{50, 50}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.3.2.3\">2227</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.3.2.4\">60.1 (+0.3)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.4.3.1\">{16}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.4.3.2\">{70}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.4.3.3\">1923</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.4.3.4\">58.2</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.5.4.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.5.4.2\">{70, 70}</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.5.4.3\">1930</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T6.4.1.5.4.4\">59.6 (+1.4)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.6.5.1\">{16}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.6.5.2\">{80}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.6.5.3\">1775</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T6.4.1.6.5.4\">56.8</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T6.4.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.4.1.7.6.1\">{16, 24}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.4.1.7.6.2\">{80, 80}</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.4.1.7.6.3\">1782</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T6.4.1.7.6.4\">59.4 (+2.6)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 6 :  Ablation study for effectâ€™s of pruned tokens reevaluation. Here the model is SAM ViT-H and dataset is RIO common. Each invo-\ncation of the prune decoder adds an additional 6 GFLOPs.",
        "footnotes": [],
        "references": [
            "TableÂ 6 illustrates the impact of reevaluating pruned image tokens. We compare the final reasoning segmentation accuracy between pruning only at layer 16 and pruning at both layer 16 and layer 24. Reassessing token relevance at layer 24 enhances the accuracy of the final reasoning. This is due to the prune decoder potentially making incorrect predictions about the relevance of image tokens to the reasoning task at earlier stages. Hence, it is crucial to reassess the importance of these tokens at deeper ViT layers."
        ]
    },
    "S5.T7.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T7.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.4.1.1.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.4.1.1.1.2\">VLM support</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.4.1.1.1.3\">GFLOPs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T7.4.1.1.1.4\">mIoU</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.4.1.2.1.1\">Baseline</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.4.1.2.1.2\">No</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.4.1.2.1.3\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T7.4.1.2.1.4\">60.4 (-0)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.3.2\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.3.2.1\">Random 50%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.3.2.2\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.3.2.3\">2297</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.3.2.4\">38.2 (-22.2)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.4.3\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.4.3.1\">CTS&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib31\" title=\"\">31</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.4.3.2\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.4.3.3\">2232</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.4.3.4\">35.7 (-24.7)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.5.4\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.5.4.1\">DToP&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib44\" title=\"\">44</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.5.4.2\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.5.4.3\">1892</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.5.4.4\">36.3 (-24.1)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.6.5\">\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.6.5.1\">SViT&#160;<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.08464v1#bib.bib30\" title=\"\">30</a>]</cite>\n</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.6.5.2\">No</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.6.5.3\">1935</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T7.4.1.6.5.4\">33.5 (-26.9)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T7.4.1.7.6\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.4.1.7.6.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.7.6.1.1\">VLTP (this work)</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.4.1.7.6.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.7.6.2.1\">Yes</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.4.1.7.6.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.7.6.3.1\">1782</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T7.4.1.7.6.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T7.4.1.7.6.4.1\">59.4 (-1.0)</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 7 :  A comparison of various pruning techniques. Each method is applied on SAM ViT-H. Random 50% indicates randomly dropping 50% of image tokens starting from layer 16.",
        "footnotes": [
            "[31] \nChenyang Lu, Daan de Geus, and Gijs Dubbelman.\n \n Content-aware token sharing for efficient semantic segmentation with vision transformers.\n \n In  Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pages 23631â€“23640, 2023.\n \n",
            "[44] \nQuan Tang, Bowen Zhang, Jiajun Liu, Fagui Liu, and Yifan Liu.\n \n Dynamic token pruning in plain vision transformers for semantic segmentation.\n \n In  Proceedings of the IEEE/CVF International Conference on Computer Vision , pages 777â€“786, 2023.\n \n",
            "[30] \nYifei Liu, Mathias Gehrig, Nico Messikommer, Marco Cannici, and Davide Scaramuzza.\n \n Revisiting token pruning for object detection and instance segmentation.\n \n In  Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision , pages 2658â€“2668, 2024.\n \n"
        ],
        "references": [
            "InÂ Tab.Â 7, we compare various pruning methods for task-oriented reasoning tasks. Although prior researchÂ [31, 44, 30] has shown effectiveness in pruning ViTs for traditional semantic or instance segmentation tasks, these methods are not suitable for TOS as they lack vision-language guidance. For instance, the policy network in CTSÂ [31] is unable to accurately predict whether four image tokens belong to the same semantic class because each tokenâ€™s class can change depending on the reasoning task. Consequently, the pruning networks proposed in DToPÂ [44] and SViTÂ [30] are also ineffective as they do not incorporate reasoning guidance. VLTP addresses these limitations by integrating both ViT image tokens and MLLM reasoning guidance, thus achieving superior performance."
        ]
    },
    "S5.T8.4.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T8.4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T8.4.1.1.1.1\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T8.4.1.1.1.1.1\">Model</span></th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T8.4.1.1.1.2\" rowspan=\"2\"><span class=\"ltx_text\" id=\"S5.T8.4.1.1.1.2.1\">pruning rate</span></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T8.4.1.1.1.3\">RIO common</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T8.4.1.1.1.4\">RIO uncommon</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T8.4.1.1.1.5\">COCO-Tasks</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.1\">mIoU(%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.2\">GFLOPs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.3\">mIoU(%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.4\">GFLOPs</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.5\">mIoU(%)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column\" id=\"S5.T8.4.1.2.2.6\">GFLOPs</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.3.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_tt\" id=\"S5.T8.4.1.3.1.1\">SAM ViT-H</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.3\">60.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.4\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.5\">37.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.6\">2976</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.7\">44.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_tt\" id=\"S5.T8.4.1.3.1.8\">2976</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.4.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.4.1.4.2.1\">VLTP@Finetune</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.2\">0.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.3\">60.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.4\">2227</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.5\">37.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.6\">2224</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.7\">44.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.4.2.8\">2219</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.5.3\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.4.1.5.3.1\">VLTP@Finetune</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.2\">0.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.3\">59.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.4\">1782</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.5\">37.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.6\">1774</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.7\">43.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.5.3.8\">1771</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.6.4\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T8.4.1.6.4.1\">SAM ViT-L</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.2\">-</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.3\">55.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.4\">1491</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.5\">32.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.6\">1491</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.7\">40.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T8.4.1.6.4.8\">1491</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.7.5\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T8.4.1.7.5.1\">VLTP@Finetune</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.2\">0.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.3\">55.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.4\">1118</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.5\">32.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.6\">1121</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.7\">39.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T8.4.1.7.5.8\">1127</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T8.4.1.8.6\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T8.4.1.8.6.1\">VLTP@Finetune</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.2\">0.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.3\">54.3</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.4\">895</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.5\">31.4</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.6\">901</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.7\">38.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T8.4.1.8.6.8\">905</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "Table 8 :  Main results on two different TOS benchmarks.",
        "footnotes": [],
        "references": [
            "InÂ Tab.Â 8, we provide a summary of the results when applying VLTP to SAM ViT-H and SAM ViT-L for the RIO common, RIO uncommon, and COCO-Tasks datasets. VLTP effectively maintains task-relevant image tokens while significantly reducing SAM ViTâ€™s computational costs. On average, VLTP reduces SAM ViTâ€™s computation by 40% with only a 1% loss in mIoU. FigureÂ 6 showcases our frameworkâ€™s segmentation visualized results on the RIO and COCO-Task datasets."
        ]
    }
}