{
    "S5.T1": {
        "caption": "Table 1: Performance comparison between GAP and other attention regularization methods using UpDn baseline on VQA v2. Results of other methods are taken from their respective papers. †Our reproduced results.",
        "table": "<table id=\"S5.T1.4\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T1.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S5.T1.3.1.2.1\" class=\"ltx_text\">Method</span></th>\n<th id=\"S5.T1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\">VQA v2 standard val<math id=\"S5.T1.3.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\uparrow\" display=\"inline\"><semantics id=\"S5.T1.3.1.1.m1.1a\"><mo stretchy=\"false\" id=\"S5.T1.3.1.1.m1.1.1\" xref=\"S5.T1.3.1.1.m1.1.1.cmml\">↑</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.3.1.1.m1.1b\"><ci id=\"S5.T1.3.1.1.m1.1.1.cmml\" xref=\"S5.T1.3.1.1.m1.1.1\">↑</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.3.1.1.m1.1c\">\\uparrow</annotation></semantics></math>\n</th>\n</tr>\n<tr id=\"S5.T1.4.3.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">All</th>\n<th id=\"S5.T1.4.3.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">Yes/No</th>\n<th id=\"S5.T1.4.3.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_t\">Num</th>\n<th id=\"S5.T1.4.3.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt\">Other</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T1.4.4.1\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">UpDn+Attn. Align <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</th>\n<td id=\"S5.T1.4.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">63.2</td>\n<td id=\"S5.T1.4.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">81.0</td>\n<td id=\"S5.T1.4.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">42.6</td>\n<td id=\"S5.T1.4.4.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.2</td>\n</tr>\n<tr id=\"S5.T1.4.5.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.5.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">UpDn+AdvReg <cite class=\"ltx_cite ltx_citemacro_citep\">(Ramakrishnan et al., <a href=\"#bib.bib19\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</th>\n<td id=\"S5.T1.4.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">62.7</td>\n<td id=\"S5.T1.4.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">79.8</td>\n<td id=\"S5.T1.4.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">42.3</td>\n<td id=\"S5.T1.4.5.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">55.2</td>\n</tr>\n<tr id=\"S5.T1.4.6.3\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.6.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">UpDn+SCR (w. ext.) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu and Mooney, <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</th>\n<td id=\"S5.T1.4.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_r\">62.2</td>\n<td id=\"S5.T1.4.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">78.8</td>\n<td id=\"S5.T1.4.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_r\">41.6</td>\n<td id=\"S5.T1.4.6.3.5\" class=\"ltx_td ltx_align_center ltx_border_r\">54.5</td>\n</tr>\n<tr id=\"S5.T1.4.7.4\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.7.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">UpDn+SCR (w/o ext.) <cite class=\"ltx_cite ltx_citemacro_citep\">(Wu and Mooney, <a href=\"#bib.bib11\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</th>\n<td id=\"S5.T1.4.7.4.2\" class=\"ltx_td ltx_align_center ltx_border_r\">62.3</td>\n<td id=\"S5.T1.4.7.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">77.4</td>\n<td id=\"S5.T1.4.7.4.4\" class=\"ltx_td ltx_align_center ltx_border_r\">40.9</td>\n<td id=\"S5.T1.4.7.4.5\" class=\"ltx_td ltx_align_center ltx_border_r\">56.5</td>\n</tr>\n<tr id=\"S5.T1.4.8.5\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.8.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">UpDn+DLR <cite class=\"ltx_cite ltx_citemacro_citep\">(Jing et al., <a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">2020</a>)</cite>\n</th>\n<td id=\"S5.T1.4.8.5.2\" class=\"ltx_td ltx_align_center ltx_border_r\">58.0</td>\n<td id=\"S5.T1.4.8.5.3\" class=\"ltx_td ltx_align_center ltx_border_r\">76.8</td>\n<td id=\"S5.T1.4.8.5.4\" class=\"ltx_td ltx_align_center ltx_border_r\">39.3</td>\n<td id=\"S5.T1.4.8.5.5\" class=\"ltx_td ltx_align_center ltx_border_r\">48.5</td>\n</tr>\n<tr id=\"S5.T1.4.2\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">UpDn+RUBi<sup id=\"S5.T1.4.2.1.1\" class=\"ltx_sup\"><span id=\"S5.T1.4.2.1.1.1\" class=\"ltx_text ltx_font_italic\">†</span></sup> <cite class=\"ltx_cite ltx_citemacro_citep\">(Cadene et al., <a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</th>\n<td id=\"S5.T1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">62.7</td>\n<td id=\"S5.T1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">79.2</td>\n<td id=\"S5.T1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">42.8</td>\n<td id=\"S5.T1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">55.5</td>\n</tr>\n<tr id=\"S5.T1.4.9.6\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.9.6.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r\">UpDn+HINT <cite class=\"ltx_cite ltx_citemacro_citep\">(Selvaraju et al., <a href=\"#bib.bib8\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</th>\n<td id=\"S5.T1.4.9.6.2\" class=\"ltx_td ltx_align_center ltx_border_r\">63.4</td>\n<td id=\"S5.T1.4.9.6.3\" class=\"ltx_td ltx_align_center ltx_border_r\"><span id=\"S5.T1.4.9.6.3.1\" class=\"ltx_text ltx_font_bold\">81.2</span></td>\n<td id=\"S5.T1.4.9.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">43.0</td>\n<td id=\"S5.T1.4.9.6.5\" class=\"ltx_td ltx_align_center ltx_border_r\">55.5</td>\n</tr>\n<tr id=\"S5.T1.4.10.7\" class=\"ltx_tr\">\n<th id=\"S5.T1.4.10.7.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">UpDn+GAP</th>\n<td id=\"S5.T1.4.10.7.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.10.7.2.1\" class=\"ltx_text ltx_font_bold\">64.3</span></td>\n<td id=\"S5.T1.4.10.7.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.10.7.3.1\" class=\"ltx_text ltx_font_bold\">81.2</span></td>\n<td id=\"S5.T1.4.10.7.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.10.7.4.1\" class=\"ltx_text ltx_font_bold\">44.1</span></td>\n<td id=\"S5.T1.4.10.7.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.4.10.7.5.1\" class=\"ltx_text ltx_font_bold\">56.9</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table 1 shows that our approach\n(UpDn+GAP) clearly has advantages over other methods in improving\nthe UpDn baseline. The favorable performance is consistent across\nall question types, especially on “Other” question type, which\nis the most important and challenging for open-ended answers (Teney et al., 2021, 2020)."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Grounding performance of the unsupervised RE-image grounding when evaluated on out-of-distribution image-caption Flickr30K Entities test set. Recall@k𝑘k: fraction of phrases with bounding boxes that have IOU≥\\geq0.5 with top-k𝑘k predictions.",
        "table": "<table id=\"S5.T2.9\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T2.9.4.1\" class=\"ltx_tr\">\n<th id=\"S5.T2.9.4.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S5.T2.9.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">R@1</th>\n<th id=\"S5.T2.9.4.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">R@5</th>\n<th id=\"S5.T2.9.4.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">R@10</th>\n<th id=\"S5.T2.9.4.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Acc.</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.7.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.7.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt\">  Unsupervised RE-image grounding</td>\n<td id=\"S5.T2.7.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">14.1</td>\n<td id=\"S5.T2.7.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">35.6</td>\n<td id=\"S5.T2.7.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">45.5</td>\n<td id=\"S5.T2.7.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">45.4</td>\n</tr>\n<tr id=\"S5.T2.8.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.8.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\">  Unsupervised grounding w/o REs</td>\n<td id=\"S5.T2.8.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T2.8.2.2.1\" class=\"ltx_text\">12.0</span></td>\n<td id=\"S5.T2.8.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">33.0</td>\n<td id=\"S5.T2.8.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">42.9</td>\n<td id=\"S5.T2.8.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44.3</td>\n</tr>\n<tr id=\"S5.T2.9.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.9.3.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">  Random alignment score (10 runs)</td>\n<td id=\"S5.T2.9.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">6.6</td>\n<td id=\"S5.T2.9.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">28.4</td>\n<td id=\"S5.T2.9.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">43.3</td>\n<td id=\"S5.T2.9.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">40.7</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The performance of our new unsupervised linguistic-visual alignments\nusing the query grammatical structure is shown in the top row of Table 2.\nThis is compared against the alignment scores produced by the same\nframework but without breaking the query into REs (Middle row) and\nthe random alignments (Bottom row). There is a 5 points gain compared\nto the random scores and over 1 point better than the question-image\npairs without phrases, indicating our linguistic-visual alignments\nis a reliable inductive prior for attention in VQA."
        ]
    },
    "S5.SS2.SSS0.Px1.tab1": {
        "caption": "Table 3: VQA performance on VQA v2 validation split with different sources of attention priors.",
        "table": "<table id=\"S5.T3.5\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.5.6.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.6.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T3.5.6.1.1.1\" class=\"ltx_text\">No.</span></th>\n<th id=\"S5.T3.5.6.1.2\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S5.T3.5.6.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.5.6.1.2.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\"><span id=\"S5.T3.5.6.1.2.1.1.1\" class=\"ltx_text\">Models</span></span>\n</span>\n</th>\n<th id=\"S5.T3.5.6.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T3.5.6.1.3.1\" class=\"ltx_text\">Acc.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T3.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\">1</td>\n<td id=\"S5.T3.1.1.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt\">\n<span id=\"S5.T3.1.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.1.1.1.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">  UpDn baseline</span>\n</span>\n</td>\n<td id=\"S5.T3.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">63.3</td>\n</tr>\n<tr id=\"S5.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T3.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">2</td>\n<td id=\"S5.T3.2.2.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t\">\n<span id=\"S5.T3.2.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.2.2.1.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">  +GAP w/ uniform-values vector</span>\n</span>\n</td>\n<td id=\"S5.T3.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">63.7</td>\n</tr>\n<tr id=\"S5.T3.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T3.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">3</td>\n<td id=\"S5.T3.3.3.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S5.T3.3.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.3.3.1.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">  +GAP w/ random-values vector</span>\n</span>\n</td>\n<td id=\"S5.T3.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r\">63.6</td>\n</tr>\n<tr id=\"S5.T3.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T3.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">4</td>\n<td id=\"S5.T3.4.4.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_r\">\n<span id=\"S5.T3.4.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.4.4.1.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">  +GAP w/ supervised grounding</span>\n</span>\n</td>\n<td id=\"S5.T3.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_r\">64.0</td>\n</tr>\n<tr id=\"S5.T3.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T3.5.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">5</td>\n<td id=\"S5.T3.5.5.1\" class=\"ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S5.T3.5.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S5.T3.5.5.1.1.1\" class=\"ltx_p\" style=\"width:113.8pt;\">  +GAP w/ unsup. visual grounding</span>\n</span>\n</td>\n<td id=\"S5.T3.5.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">64.3</td>\n</tr>\n</tbody>\n</table>\n<table id=\"S5.T4.13\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T4.13.14.1\" class=\"ltx_tr\">\n<th id=\"S5.T4.13.14.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Models</th>\n<th id=\"S5.T4.13.14.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S5.T4.13.14.1.2.1\" class=\"ltx_text\">Acc.</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T4.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T4.3.3.3\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_tt\">1.  UpDn baseline, <math id=\"S5.T4.2.2.2.m2.1\" class=\"ltx_Math\" alttext=\"\\beta^{\\prime}\\equiv\\beta\" display=\"inline\"><semantics id=\"S5.T4.2.2.2.m2.1a\"><mrow id=\"S5.T4.2.2.2.m2.1.1\" xref=\"S5.T4.2.2.2.m2.1.1.cmml\"><msup id=\"S5.T4.2.2.2.m2.1.1.2\" xref=\"S5.T4.2.2.2.m2.1.1.2.cmml\"><mi id=\"S5.T4.2.2.2.m2.1.1.2.2\" xref=\"S5.T4.2.2.2.m2.1.1.2.2.cmml\">β</mi><mo id=\"S5.T4.2.2.2.m2.1.1.2.3\" xref=\"S5.T4.2.2.2.m2.1.1.2.3.cmml\">′</mo></msup><mo id=\"S5.T4.2.2.2.m2.1.1.1\" xref=\"S5.T4.2.2.2.m2.1.1.1.cmml\">≡</mo><mi id=\"S5.T4.2.2.2.m2.1.1.3\" xref=\"S5.T4.2.2.2.m2.1.1.3.cmml\">β</mi></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.2.2.2.m2.1b\"><apply id=\"S5.T4.2.2.2.m2.1.1.cmml\" xref=\"S5.T4.2.2.2.m2.1.1\"><equivalent id=\"S5.T4.2.2.2.m2.1.1.1.cmml\" xref=\"S5.T4.2.2.2.m2.1.1.1\"></equivalent><apply id=\"S5.T4.2.2.2.m2.1.1.2.cmml\" xref=\"S5.T4.2.2.2.m2.1.1.2\"><csymbol cd=\"ambiguous\" id=\"S5.T4.2.2.2.m2.1.1.2.1.cmml\" xref=\"S5.T4.2.2.2.m2.1.1.2\">superscript</csymbol><ci id=\"S5.T4.2.2.2.m2.1.1.2.2.cmml\" xref=\"S5.T4.2.2.2.m2.1.1.2.2\">𝛽</ci><ci id=\"S5.T4.2.2.2.m2.1.1.2.3.cmml\" xref=\"S5.T4.2.2.2.m2.1.1.2.3\">′</ci></apply><ci id=\"S5.T4.2.2.2.m2.1.1.3.cmml\" xref=\"S5.T4.2.2.2.m2.1.1.3\">𝛽</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.2.2.2.m2.1c\">\\beta^{\\prime}\\equiv\\beta</annotation></semantics></math> (<math id=\"S5.T4.3.3.3.m3.1\" class=\"ltx_Math\" alttext=\"\\gamma(\\theta)\\equiv 1.0\" display=\"inline\"><semantics id=\"S5.T4.3.3.3.m3.1a\"><mrow id=\"S5.T4.3.3.3.m3.1.2\" xref=\"S5.T4.3.3.3.m3.1.2.cmml\"><mrow id=\"S5.T4.3.3.3.m3.1.2.2\" xref=\"S5.T4.3.3.3.m3.1.2.2.cmml\"><mi id=\"S5.T4.3.3.3.m3.1.2.2.2\" xref=\"S5.T4.3.3.3.m3.1.2.2.2.cmml\">γ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T4.3.3.3.m3.1.2.2.1\" xref=\"S5.T4.3.3.3.m3.1.2.2.1.cmml\">​</mo><mrow id=\"S5.T4.3.3.3.m3.1.2.2.3.2\" xref=\"S5.T4.3.3.3.m3.1.2.2.cmml\"><mo stretchy=\"false\" id=\"S5.T4.3.3.3.m3.1.2.2.3.2.1\" xref=\"S5.T4.3.3.3.m3.1.2.2.cmml\">(</mo><mi id=\"S5.T4.3.3.3.m3.1.1\" xref=\"S5.T4.3.3.3.m3.1.1.cmml\">θ</mi><mo stretchy=\"false\" id=\"S5.T4.3.3.3.m3.1.2.2.3.2.2\" xref=\"S5.T4.3.3.3.m3.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S5.T4.3.3.3.m3.1.2.1\" xref=\"S5.T4.3.3.3.m3.1.2.1.cmml\">≡</mo><mn id=\"S5.T4.3.3.3.m3.1.2.3\" xref=\"S5.T4.3.3.3.m3.1.2.3.cmml\">1.0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.3.3.3.m3.1b\"><apply id=\"S5.T4.3.3.3.m3.1.2.cmml\" xref=\"S5.T4.3.3.3.m3.1.2\"><equivalent id=\"S5.T4.3.3.3.m3.1.2.1.cmml\" xref=\"S5.T4.3.3.3.m3.1.2.1\"></equivalent><apply id=\"S5.T4.3.3.3.m3.1.2.2.cmml\" xref=\"S5.T4.3.3.3.m3.1.2.2\"><times id=\"S5.T4.3.3.3.m3.1.2.2.1.cmml\" xref=\"S5.T4.3.3.3.m3.1.2.2.1\"></times><ci id=\"S5.T4.3.3.3.m3.1.2.2.2.cmml\" xref=\"S5.T4.3.3.3.m3.1.2.2.2\">𝛾</ci><ci id=\"S5.T4.3.3.3.m3.1.1.cmml\" xref=\"S5.T4.3.3.3.m3.1.1\">𝜃</ci></apply><cn type=\"float\" id=\"S5.T4.3.3.3.m3.1.2.3.cmml\" xref=\"S5.T4.3.3.3.m3.1.2.3\">1.0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.3.3.3.m3.1c\">\\gamma(\\theta)\\equiv 1.0</annotation></semantics></math>)</td>\n<td id=\"S5.T4.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">63.3</td>\n</tr>\n<tr id=\"S5.T4.13.15.1\" class=\"ltx_tr\">\n<td id=\"S5.T4.13.15.1.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T4.13.15.1.1.1\" class=\"ltx_text ltx_font_bold\">Attention as priors</span></td>\n<td id=\"S5.T4.13.15.1.2\" class=\"ltx_td ltx_border_r ltx_border_t\"></td>\n</tr>\n<tr id=\"S5.T4.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T4.6.6.3\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">2.  w/ <math id=\"S5.T4.5.5.2.m2.1\" class=\"ltx_Math\" alttext=\"\\beta^{\\prime}\\equiv\\text{$\\beta^{*}$}\" display=\"inline\"><semantics id=\"S5.T4.5.5.2.m2.1a\"><mrow id=\"S5.T4.5.5.2.m2.1.2\" xref=\"S5.T4.5.5.2.m2.1.2.cmml\"><msup id=\"S5.T4.5.5.2.m2.1.2.2\" xref=\"S5.T4.5.5.2.m2.1.2.2.cmml\"><mi id=\"S5.T4.5.5.2.m2.1.2.2.2\" xref=\"S5.T4.5.5.2.m2.1.2.2.2.cmml\">β</mi><mo id=\"S5.T4.5.5.2.m2.1.2.2.3\" xref=\"S5.T4.5.5.2.m2.1.2.2.3.cmml\">′</mo></msup><mo id=\"S5.T4.5.5.2.m2.1.2.1\" xref=\"S5.T4.5.5.2.m2.1.2.1.cmml\">≡</mo><msup id=\"S5.T4.5.5.2.m2.1.1.1\" xref=\"S5.T4.5.5.2.m2.1.1.1.cmml\"><mi id=\"S5.T4.5.5.2.m2.1.1.1.3\" xref=\"S5.T4.5.5.2.m2.1.1.1.3.cmml\">β</mi><mo id=\"S5.T4.5.5.2.m2.1.1.1.4\" xref=\"S5.T4.5.5.2.m2.1.1.1.4.cmml\">∗</mo></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.5.5.2.m2.1b\"><apply id=\"S5.T4.5.5.2.m2.1.2.cmml\" xref=\"S5.T4.5.5.2.m2.1.2\"><equivalent id=\"S5.T4.5.5.2.m2.1.2.1.cmml\" xref=\"S5.T4.5.5.2.m2.1.2.1\"></equivalent><apply id=\"S5.T4.5.5.2.m2.1.2.2.cmml\" xref=\"S5.T4.5.5.2.m2.1.2.2\"><csymbol cd=\"ambiguous\" id=\"S5.T4.5.5.2.m2.1.2.2.1.cmml\" xref=\"S5.T4.5.5.2.m2.1.2.2\">superscript</csymbol><ci id=\"S5.T4.5.5.2.m2.1.2.2.2.cmml\" xref=\"S5.T4.5.5.2.m2.1.2.2.2\">𝛽</ci><ci id=\"S5.T4.5.5.2.m2.1.2.2.3.cmml\" xref=\"S5.T4.5.5.2.m2.1.2.2.3\">′</ci></apply><apply id=\"S5.T4.5.5.2.m2.1.1.1.cmml\" xref=\"S5.T4.5.5.2.m2.1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T4.5.5.2.m2.1.1.1.2.cmml\" xref=\"S5.T4.5.5.2.m2.1.1.1\">superscript</csymbol><ci id=\"S5.T4.5.5.2.m2.1.1.1.3.cmml\" xref=\"S5.T4.5.5.2.m2.1.1.1.3\">𝛽</ci><times id=\"S5.T4.5.5.2.m2.1.1.1.4.cmml\" xref=\"S5.T4.5.5.2.m2.1.1.1.4\"></times></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.5.5.2.m2.1c\">\\beta^{\\prime}\\equiv\\text{$\\beta^{*}$}</annotation></semantics></math>(<math id=\"S5.T4.6.6.3.m3.1\" class=\"ltx_Math\" alttext=\"\\gamma(\\theta)\\equiv 0.0\" display=\"inline\"><semantics id=\"S5.T4.6.6.3.m3.1a\"><mrow id=\"S5.T4.6.6.3.m3.1.2\" xref=\"S5.T4.6.6.3.m3.1.2.cmml\"><mrow id=\"S5.T4.6.6.3.m3.1.2.2\" xref=\"S5.T4.6.6.3.m3.1.2.2.cmml\"><mi id=\"S5.T4.6.6.3.m3.1.2.2.2\" xref=\"S5.T4.6.6.3.m3.1.2.2.2.cmml\">γ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T4.6.6.3.m3.1.2.2.1\" xref=\"S5.T4.6.6.3.m3.1.2.2.1.cmml\">​</mo><mrow id=\"S5.T4.6.6.3.m3.1.2.2.3.2\" xref=\"S5.T4.6.6.3.m3.1.2.2.cmml\"><mo stretchy=\"false\" id=\"S5.T4.6.6.3.m3.1.2.2.3.2.1\" xref=\"S5.T4.6.6.3.m3.1.2.2.cmml\">(</mo><mi id=\"S5.T4.6.6.3.m3.1.1\" xref=\"S5.T4.6.6.3.m3.1.1.cmml\">θ</mi><mo stretchy=\"false\" id=\"S5.T4.6.6.3.m3.1.2.2.3.2.2\" xref=\"S5.T4.6.6.3.m3.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S5.T4.6.6.3.m3.1.2.1\" xref=\"S5.T4.6.6.3.m3.1.2.1.cmml\">≡</mo><mn id=\"S5.T4.6.6.3.m3.1.2.3\" xref=\"S5.T4.6.6.3.m3.1.2.3.cmml\">0.0</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.6.6.3.m3.1b\"><apply id=\"S5.T4.6.6.3.m3.1.2.cmml\" xref=\"S5.T4.6.6.3.m3.1.2\"><equivalent id=\"S5.T4.6.6.3.m3.1.2.1.cmml\" xref=\"S5.T4.6.6.3.m3.1.2.1\"></equivalent><apply id=\"S5.T4.6.6.3.m3.1.2.2.cmml\" xref=\"S5.T4.6.6.3.m3.1.2.2\"><times id=\"S5.T4.6.6.3.m3.1.2.2.1.cmml\" xref=\"S5.T4.6.6.3.m3.1.2.2.1\"></times><ci id=\"S5.T4.6.6.3.m3.1.2.2.2.cmml\" xref=\"S5.T4.6.6.3.m3.1.2.2.2\">𝛾</ci><ci id=\"S5.T4.6.6.3.m3.1.1.cmml\" xref=\"S5.T4.6.6.3.m3.1.1\">𝜃</ci></apply><cn type=\"float\" id=\"S5.T4.6.6.3.m3.1.2.3.cmml\" xref=\"S5.T4.6.6.3.m3.1.2.3\">0.0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.6.6.3.m3.1c\">\\gamma(\\theta)\\equiv 0.0</annotation></semantics></math>)</td>\n<td id=\"S5.T4.6.6.4\" class=\"ltx_td ltx_align_center ltx_border_r\">60.0</td>\n</tr>\n<tr id=\"S5.T4.13.16.2\" class=\"ltx_tr\">\n<td id=\"S5.T4.13.16.2.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\"><span id=\"S5.T4.13.16.2.1.1\" class=\"ltx_text ltx_font_bold\">Effects of the direct use of attention priors</span></td>\n<td id=\"S5.T4.13.16.2.2\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S5.T4.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T4.7.7.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">3.  +GAP w/o 1st stage fine-tuning</td>\n<td id=\"S5.T4.7.7.2\" class=\"ltx_td ltx_align_center ltx_border_r\">63.9</td>\n</tr>\n<tr id=\"S5.T4.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T4.8.8.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">4.  w/ 1st stage fine-tuning with attention priors</td>\n<td id=\"S5.T4.8.8.2\" class=\"ltx_td ltx_align_center ltx_border_r\">64.0</td>\n</tr>\n<tr id=\"S5.T4.13.17.3\" class=\"ltx_tr\">\n<td id=\"S5.T4.13.17.3.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\"><span id=\"S5.T4.13.17.3.1.1\" class=\"ltx_text ltx_font_bold\">Effects of the gating mechanisms</span></td>\n<td id=\"S5.T4.13.17.3.2\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S5.T4.10.10\" class=\"ltx_tr\">\n<td id=\"S5.T4.10.10.2\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">5.  +GAP, fixed <math id=\"S5.T4.10.10.2.m2.1\" class=\"ltx_Math\" alttext=\"\\gamma(\\theta)\\equiv 0.5\" display=\"inline\"><semantics id=\"S5.T4.10.10.2.m2.1a\"><mrow id=\"S5.T4.10.10.2.m2.1.2\" xref=\"S5.T4.10.10.2.m2.1.2.cmml\"><mrow id=\"S5.T4.10.10.2.m2.1.2.2\" xref=\"S5.T4.10.10.2.m2.1.2.2.cmml\"><mi id=\"S5.T4.10.10.2.m2.1.2.2.2\" xref=\"S5.T4.10.10.2.m2.1.2.2.2.cmml\">γ</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S5.T4.10.10.2.m2.1.2.2.1\" xref=\"S5.T4.10.10.2.m2.1.2.2.1.cmml\">​</mo><mrow id=\"S5.T4.10.10.2.m2.1.2.2.3.2\" xref=\"S5.T4.10.10.2.m2.1.2.2.cmml\"><mo stretchy=\"false\" id=\"S5.T4.10.10.2.m2.1.2.2.3.2.1\" xref=\"S5.T4.10.10.2.m2.1.2.2.cmml\">(</mo><mi id=\"S5.T4.10.10.2.m2.1.1\" xref=\"S5.T4.10.10.2.m2.1.1.cmml\">θ</mi><mo stretchy=\"false\" id=\"S5.T4.10.10.2.m2.1.2.2.3.2.2\" xref=\"S5.T4.10.10.2.m2.1.2.2.cmml\">)</mo></mrow></mrow><mo id=\"S5.T4.10.10.2.m2.1.2.1\" xref=\"S5.T4.10.10.2.m2.1.2.1.cmml\">≡</mo><mn id=\"S5.T4.10.10.2.m2.1.2.3\" xref=\"S5.T4.10.10.2.m2.1.2.3.cmml\">0.5</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.T4.10.10.2.m2.1b\"><apply id=\"S5.T4.10.10.2.m2.1.2.cmml\" xref=\"S5.T4.10.10.2.m2.1.2\"><equivalent id=\"S5.T4.10.10.2.m2.1.2.1.cmml\" xref=\"S5.T4.10.10.2.m2.1.2.1\"></equivalent><apply id=\"S5.T4.10.10.2.m2.1.2.2.cmml\" xref=\"S5.T4.10.10.2.m2.1.2.2\"><times id=\"S5.T4.10.10.2.m2.1.2.2.1.cmml\" xref=\"S5.T4.10.10.2.m2.1.2.2.1\"></times><ci id=\"S5.T4.10.10.2.m2.1.2.2.2.cmml\" xref=\"S5.T4.10.10.2.m2.1.2.2.2\">𝛾</ci><ci id=\"S5.T4.10.10.2.m2.1.1.cmml\" xref=\"S5.T4.10.10.2.m2.1.1\">𝜃</ci></apply><cn type=\"float\" id=\"S5.T4.10.10.2.m2.1.2.3.cmml\" xref=\"S5.T4.10.10.2.m2.1.2.3\">0.5</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T4.10.10.2.m2.1c\">\\gamma(\\theta)\\equiv 0.5</annotation></semantics></math>\n</td>\n<td id=\"S5.T4.10.10.3\" class=\"ltx_td ltx_align_center ltx_border_r\">64.0</td>\n</tr>\n<tr id=\"S5.T4.11.11\" class=\"ltx_tr\">\n<td id=\"S5.T4.11.11.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">6.  +GAP (multiplicative gating)</td>\n<td id=\"S5.T4.11.11.2\" class=\"ltx_td ltx_align_center ltx_border_r\">64.1</td>\n</tr>\n<tr id=\"S5.T4.13.18.4\" class=\"ltx_tr\">\n<td id=\"S5.T4.13.18.4.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\"><span id=\"S5.T4.13.18.4.1.1\" class=\"ltx_text ltx_font_bold\">Effects of using visual-phrase associations</span></td>\n<td id=\"S5.T4.13.18.4.2\" class=\"ltx_td ltx_border_r\"></td>\n</tr>\n<tr id=\"S5.T4.12.12\" class=\"ltx_tr\">\n<td id=\"S5.T4.12.12.1\" class=\"ltx_td ltx_align_left ltx_border_l ltx_border_r\">7.  +GAP (w/o extracted phrases from questions)</td>\n<td id=\"S5.T4.12.12.2\" class=\"ltx_td ltx_align_center ltx_border_r\">63.9</td>\n</tr>\n<tr id=\"S5.T4.13.13\" class=\"ltx_tr\">\n<td id=\"S5.T4.13.13.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">8.  +GAP (full model)</td>\n<td id=\"S5.T4.13.13.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">64.3</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We examine the effectiveness of our attention prior by comparing it\nwith different ways of generating values for visual attention prior\nβ∗superscript𝛽\\beta^{*} on VQA performance. They include: (1) UpDn baseline\n(no use of attention prior) (2) uniform-values vector and (3) random-values\nvector (normalized normal distribution), (4) supervised grounding\n(pretrained MAttNet (Yu et al., 2018) on RefCOCO (Kazemzadeh et al., 2014)),\nand (5) GAP. Table 3 shows results\non UpDn baseline. GAP is significantly better than the baseline and\nother attention priors (2-3-4). Especially our unsupervised grounding\ngives better VQA performance than the supervised one (Row 5). This\nsurprising result suggests that pre-trained supervised model could\nnot generalize out of distribution, and is worse than underlying grounding\nphrase-image pairs extracted unsupervisedly.",
            "We quantify the visual attentions of the UpDn model before and after\napplying GAP on the GQA validation set. In particular, we use the\ngrounding score proposed by (Hudson and Manning, 2019a) to measure the\ncorrectness of the model’s attentions weights comparing to the groundtruth\ngrounding provided. Results are shown in Table 5.\nOur method improves the grounding scores of UpDn by 2.26 points (16.76\nvs. 14.50) for top-1 attention, 2.01 points (29.32 vs. 27.31) for\ntop-5 attention and 1.18 points (36.53 vs. 35.35) for top-10 attention.\nIt is to note that while the grounding scores reported by (Hudson and Manning, 2019a)\nsumming over all object regions, we report the grounding scores attributed\nby top-k𝑘k attentions to better emphasize how the attentions shift\ntowards most relevant objects. This analysis complements the VQA performance\nin Table 3 in a more definitive\nconfirmation of the role of GAP in improving both reasoning attention\nand VQA accuracy."
        ]
    },
    "S5.T5": {
        "caption": "Table 5: Grounding scores of UpDn before and applying GAP on GQA validation split.",
        "table": "<table id=\"S5.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T5.1.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Model</th>\n<th id=\"S5.T5.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Top-1 attention</th>\n<th id=\"S5.T5.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Top-5 attention</th>\n<th id=\"S5.T5.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Top-10 attention</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T5.1.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_tt\">UpDn baseline</th>\n<td id=\"S5.T5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">14.50</td>\n<td id=\"S5.T5.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">27.31</td>\n<td id=\"S5.T5.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">35.35</td>\n</tr>\n<tr id=\"S5.T5.1.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T5.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">UpDn + GAP</th>\n<td id=\"S5.T5.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T5.1.3.2.2.1\" class=\"ltx_text\">16.76</span></td>\n<td id=\"S5.T5.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">29.32</td>\n<td id=\"S5.T5.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">36.53</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "To provide more insights into our method, we conduct extensive ablation\nstudies on the VQA v2 dataset (see Table 4).\nThroughout these experiments, we examine the role of each component\ntoward the optimal performance of the full model. Experiments (1,\n2) in Table 4 show that UpDn model does\nnot perform well with either only its own attention or with the attention\nprior itself. This supports our intuition that they complement each\nother toward optimal reasoning. Rows 5,6 show that a soft combination\nof the two terms is necessary.",
            "We quantify the visual attentions of the UpDn model before and after\napplying GAP on the GQA validation set. In particular, we use the\ngrounding score proposed by (Hudson and Manning, 2019a) to measure the\ncorrectness of the model’s attentions weights comparing to the groundtruth\ngrounding provided. Results are shown in Table 5.\nOur method improves the grounding scores of UpDn by 2.26 points (16.76\nvs. 14.50) for top-1 attention, 2.01 points (29.32 vs. 27.31) for\ntop-5 attention and 1.18 points (36.53 vs. 35.35) for top-10 attention.\nIt is to note that while the grounding scores reported by (Hudson and Manning, 2019a)\nsumming over all object regions, we report the grounding scores attributed\nby top-k𝑘k attentions to better emphasize how the attentions shift\ntowards most relevant objects. This analysis complements the VQA performance\nin Table 3 in a more definitive\nconfirmation of the role of GAP in improving both reasoning attention\nand VQA accuracy."
        ]
    },
    "A2.T6": {
        "caption": "Table 6: Performance on VQA v2 val split and VQA-CP2 test split with UpDn baseline.",
        "table": "<table id=\"A2.T6.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"A2.T6.1.1.1\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"A2.T6.1.1.1.1.1\" class=\"ltx_text\">Model</span></th>\n<th id=\"A2.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\">VQA-CP2 test</th>\n<th id=\"A2.T6.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" colspan=\"4\">VQA v2 val</th>\n</tr>\n<tr id=\"A2.T6.1.2.2\" class=\"ltx_tr\">\n<th id=\"A2.T6.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Overall</th>\n<th id=\"A2.T6.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\">Yes/No</th>\n<th id=\"A2.T6.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_t\">Number</th>\n<th id=\"A2.T6.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt\">Other</th>\n<th id=\"A2.T6.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt ltx_border_t\">Overall</th>\n<th id=\"A2.T6.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt ltx_border_tt\">Yes/No</th>\n<th id=\"A2.T6.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt ltx_border_tt ltx_border_t\">Number</th>\n<th id=\"A2.T6.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt ltx_border_tt ltx_border_tt ltx_border_tt\">Other</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"A2.T6.1.3.1\" class=\"ltx_tr\">\n<td id=\"A2.T6.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\">UpDn baseline</td>\n<td id=\"A2.T6.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">40.6</td>\n<td id=\"A2.T6.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">41.2</td>\n<td id=\"A2.T6.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">13.0</td>\n<td id=\"A2.T6.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">48.1</td>\n<td id=\"A2.T6.1.3.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">63.3</td>\n<td id=\"A2.T6.1.3.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">79.7</td>\n<td id=\"A2.T6.1.3.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">42.8</td>\n<td id=\"A2.T6.1.3.1.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\">56.4</td>\n</tr>\n<tr id=\"A2.T6.1.4.2\" class=\"ltx_tr\">\n<td id=\"A2.T6.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r\">UpDn+GAP</td>\n<td id=\"A2.T6.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_r\">40.8</td>\n<td id=\"A2.T6.1.4.2.3\" class=\"ltx_td ltx_align_center ltx_border_r\">41.2</td>\n<td id=\"A2.T6.1.4.2.4\" class=\"ltx_td ltx_align_center ltx_border_r\">13.2</td>\n<td id=\"A2.T6.1.4.2.5\" class=\"ltx_td ltx_align_center ltx_border_r\">48.3</td>\n<td id=\"A2.T6.1.4.2.6\" class=\"ltx_td ltx_align_center ltx_border_r\">64.3</td>\n<td id=\"A2.T6.1.4.2.7\" class=\"ltx_td ltx_align_center ltx_border_r\">81.2</td>\n<td id=\"A2.T6.1.4.2.8\" class=\"ltx_td ltx_align_center ltx_border_r\">44.1</td>\n<td id=\"A2.T6.1.4.2.9\" class=\"ltx_td ltx_align_center ltx_border_r\">56.9</td>\n</tr>\n<tr id=\"A2.T6.1.5.3\" class=\"ltx_tr\">\n<td id=\"A2.T6.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">UpDn+RUBi</td>\n<td id=\"A2.T6.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">48.6</td>\n<td id=\"A2.T6.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">72.1</td>\n<td id=\"A2.T6.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">12.6</td>\n<td id=\"A2.T6.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">46.1</td>\n<td id=\"A2.T6.1.5.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">62.7</td>\n<td id=\"A2.T6.1.5.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">79.2</td>\n<td id=\"A2.T6.1.5.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">42.8</td>\n<td id=\"A2.T6.1.5.3.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">55.5</td>\n</tr>\n<tr id=\"A2.T6.1.6.4\" class=\"ltx_tr\">\n<td id=\"A2.T6.1.6.4.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r\">UpDn+RUBi+GAP</td>\n<td id=\"A2.T6.1.6.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">48.9</td>\n<td id=\"A2.T6.1.6.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">72.2</td>\n<td id=\"A2.T6.1.6.4.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">12.8</td>\n<td id=\"A2.T6.1.6.4.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">46.4</td>\n<td id=\"A2.T6.1.6.4.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">64.2</td>\n<td id=\"A2.T6.1.6.4.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">81.4</td>\n<td id=\"A2.T6.1.6.4.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">44.3</td>\n<td id=\"A2.T6.1.6.4.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r\">56.3</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We quantify the visual attentions of the UpDn model before and after\napplying GAP on the GQA validation set. In particular, we use the\ngrounding score proposed by (Hudson and Manning, 2019a) to measure the\ncorrectness of the model’s attentions weights comparing to the groundtruth\ngrounding provided. Results are shown in Table 5.\nOur method improves the grounding scores of UpDn by 2.26 points (16.76\nvs. 14.50) for top-1 attention, 2.01 points (29.32 vs. 27.31) for\ntop-5 attention and 1.18 points (36.53 vs. 35.35) for top-10 attention.\nIt is to note that while the grounding scores reported by (Hudson and Manning, 2019a)\nsumming over all object regions, we report the grounding scores attributed\nby top-k𝑘k attentions to better emphasize how the attentions shift\ntowards most relevant objects. This analysis complements the VQA performance\nin Table 3 in a more definitive\nconfirmation of the role of GAP in improving both reasoning attention\nand VQA accuracy.",
            "Apart from the experimental results in Sec. 5\nin the main paper, we provide additional results on VQA-CP2 dataset\n(Agrawal et al., 2018) to support our claim that GAP complements\nrelated works with regularization schemes. We choose RUBi (Cadene et al., 2019)\nas a representative bias reduction method for VQA with a general linguistic\ndebiasing technique and yet effective on VQA-CP2 dataset. Table 6\npresents our experimental results with UpDn baseline. As clearly seen,\neven though linguistic biases are not the main target, GAP still shows\nconsistent improvements on top of both UpDn baseline and UpDn+RUBi\nbaseline. It is to emphasize that applying the regularization by RUBi\nfor linguistic bias considerably hurts the performance on VQA v2 even\nthough RUBi largely improves performance on VQA-CP2 test split. GAP\nbrings the benefits of pre-computed attention priors and rejects the\ndamage caused by the regularization effects by RUBi to maintain its\nexcellent performance on VQA v2 while slightly improving the baseline’s\nperformance on VQA-CP2. Looking more closely at the results per question\ntype on VQA-CP2 (Row 1 vs. Row 2, and Row 3 vs. Row 4), GAP shows\nits universal effect on all question types with the strongest effect\non “Other” question type which contains open-ended arbitrary questions.\nOn the other hand, RUBi (Row 3 vs. Row 1) shows its significant impact\nonly on binary questions “Yes/No” but considerably hurts “Number”\nand especially “Other” question types. This reveals that the regularization\nscheme in RUBi is overfitted to “Yes/No” questions specifically\ndue to the limitation of the data generation process behind this dataset."
        ]
    }
}