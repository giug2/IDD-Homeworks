{
    "PAPER'S NUMBER OF TABLES": 4,
    "S2.T1": {
        "caption": "Table 1: List of some existing MRC datasets",
        "table": "<table id=\"S2.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S2.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S2.T1.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Dataset</th>\n<th id=\"S2.T1.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Answer Type</th>\n<th id=\"S2.T1.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Domain</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S2.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">MCTest <cite class=\"ltx_cite ltx_citemacro_citep\">(Richardson et al., <a href=\"#bib.bib14\" title=\"\" class=\"ltx_ref\">2013</a>)</cite>\n</td>\n<td id=\"S2.T1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">Multiple choice</td>\n<td id=\"S2.T1.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">Children’s</td>\n</tr>\n<tr id=\"S2.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.3.2.1\" class=\"ltx_td\"></td>\n<td id=\"S2.T1.1.3.2.2\" class=\"ltx_td\"></td>\n<td id=\"S2.T1.1.3.2.3\" class=\"ltx_td ltx_align_left\">stories</td>\n</tr>\n<tr id=\"S2.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.4.3.1\" class=\"ltx_td ltx_align_left\">CNN/Daily Mail <cite class=\"ltx_cite ltx_citemacro_citep\">(Hermann et al., <a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">2015</a>)</cite>\n</td>\n<td id=\"S2.T1.1.4.3.2\" class=\"ltx_td ltx_align_left\">Spans</td>\n<td id=\"S2.T1.1.4.3.3\" class=\"ltx_td ltx_align_left\">News</td>\n</tr>\n<tr id=\"S2.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.5.4.1\" class=\"ltx_td ltx_align_left\">Children’s book <cite class=\"ltx_cite ltx_citemacro_citep\">(Hill et al., <a href=\"#bib.bib22\" title=\"\" class=\"ltx_ref\">2016</a>)</cite>\n</td>\n<td id=\"S2.T1.1.5.4.2\" class=\"ltx_td ltx_align_left\">Spans</td>\n<td id=\"S2.T1.1.5.4.3\" class=\"ltx_td ltx_align_left\">Children’s</td>\n</tr>\n<tr id=\"S2.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.6.5.1\" class=\"ltx_td\"></td>\n<td id=\"S2.T1.1.6.5.2\" class=\"ltx_td\"></td>\n<td id=\"S2.T1.1.6.5.3\" class=\"ltx_td ltx_align_left\">stories</td>\n</tr>\n<tr id=\"S2.T1.1.7.6\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.7.6.1\" class=\"ltx_td ltx_align_left\">MS MARCO <cite class=\"ltx_cite ltx_citemacro_citep\">(Bajaj et al., <a href=\"#bib.bib23\" title=\"\" class=\"ltx_ref\">2018</a>)</cite>\n</td>\n<td id=\"S2.T1.1.7.6.2\" class=\"ltx_td ltx_align_left\">Free-form text</td>\n<td id=\"S2.T1.1.7.6.3\" class=\"ltx_td ltx_align_left\">Web Search</td>\n</tr>\n<tr id=\"S2.T1.1.8.7\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.8.7.1\" class=\"ltx_td ltx_align_left\">NewsQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Trischler et al., <a href=\"#bib.bib24\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>\n</td>\n<td id=\"S2.T1.1.8.7.2\" class=\"ltx_td ltx_align_left\">Spans</td>\n<td id=\"S2.T1.1.8.7.3\" class=\"ltx_td ltx_align_left\">News</td>\n</tr>\n<tr id=\"S2.T1.1.9.8\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.9.8.1\" class=\"ltx_td ltx_align_left\">SearchQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Dunn et al., <a href=\"#bib.bib25\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>\n</td>\n<td id=\"S2.T1.1.9.8.2\" class=\"ltx_td ltx_align_left\">Spans</td>\n<td id=\"S2.T1.1.9.8.3\" class=\"ltx_td ltx_align_left\">Jeopardy</td>\n</tr>\n<tr id=\"S2.T1.1.10.9\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.10.9.1\" class=\"ltx_td ltx_align_left\">TriviaQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Joshi et al., <a href=\"#bib.bib26\" title=\"\" class=\"ltx_ref\">2017</a>)</cite>\n</td>\n<td id=\"S2.T1.1.10.9.2\" class=\"ltx_td ltx_align_left\">Spans</td>\n<td id=\"S2.T1.1.10.9.3\" class=\"ltx_td ltx_align_left\">Trivia</td>\n</tr>\n<tr id=\"S2.T1.1.11.10\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.11.10.1\" class=\"ltx_td ltx_align_left\">SQuAD <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar et al., <a href=\"#bib.bib27\" title=\"\" class=\"ltx_ref\">2016b</a>)</cite>\n</td>\n<td id=\"S2.T1.1.11.10.2\" class=\"ltx_td ltx_align_left\">Spans</td>\n<td id=\"S2.T1.1.11.10.3\" class=\"ltx_td ltx_align_left\">Wikipedia</td>\n</tr>\n<tr id=\"S2.T1.1.12.11\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.12.11.1\" class=\"ltx_td ltx_align_left\">SQuAD 2.0 <cite class=\"ltx_cite ltx_citemacro_citep\">(Rajpurkar et al., <a href=\"#bib.bib28\" title=\"\" class=\"ltx_ref\">2018a</a>)</cite>\n</td>\n<td id=\"S2.T1.1.12.11.2\" class=\"ltx_td ltx_align_left\">Spans, Unanswerable</td>\n<td id=\"S2.T1.1.12.11.3\" class=\"ltx_td ltx_align_left\">Wikipedia</td>\n</tr>\n<tr id=\"S2.T1.1.13.12\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.13.12.1\" class=\"ltx_td ltx_align_left\">CoQA <cite class=\"ltx_cite ltx_citemacro_citep\">(Reddy et al., <a href=\"#bib.bib29\" title=\"\" class=\"ltx_ref\">2019</a>)</cite>\n</td>\n<td id=\"S2.T1.1.13.12.2\" class=\"ltx_td ltx_align_left\">Free-form text,</td>\n<td id=\"S2.T1.1.13.12.3\" class=\"ltx_td ltx_align_left\">News, Reddit</td>\n</tr>\n<tr id=\"S2.T1.1.14.13\" class=\"ltx_tr\">\n<td id=\"S2.T1.1.14.13.1\" class=\"ltx_td ltx_border_b\"></td>\n<td id=\"S2.T1.1.14.13.2\" class=\"ltx_td ltx_border_b\"></td>\n<td id=\"S2.T1.1.14.13.3\" class=\"ltx_td ltx_align_left ltx_border_b\">Wikipedia</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Machine reading comprehension was proposed for the first time in 1977 by Lehnert, who built a question answering program called the QUALM (Lehnert, 1977). In 1999, Hirschman et al. (Hirschman et al., 1999) built a reading comprehension system using a corpus of 60 development and 60 test stories of 3rd to 6th-grade material. Because of the lack of benchmark datasets in that period, most MRC systems were rule-based or statistical models (Riloff and Thelen, 2000; Charniak et al., 2000). In recent years, many benchmark datasets have been released that focus on MRC by answering questions, see Table 1. Since these datasets were made available, there has been considerable progress on MRC tasks. The Stanford Question Answering Dataset (SQuAD) is one of the most well-known reading comprehension datasets, consisting of 100.000 questions posed by crowd-workers on Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading context. Important progress based on SQUAD concerns include the attention method (Wang et al., 2017) and Bi-Directional Attention Flow (BiDAF)(Richardson et al., 2013) which improved considerably the question answering performance. These two methods compute Context to Question attention and Question to Context attention using a similarity matrix computed directly from context and question. Authors in (Wang et al., 2018) describe a novel hierarchical attention network for reading comprehension style question answering, which aims to answer questions for a given narrative paragraph. In their work, attention and fusion are conducted horizontally and vertically across layers at different levels of granularity between question and paragraph. In recent work in language modeling, authors in (Zhang et al., 2020) incorporate explicit contextual semantics from pre-trained semantic role labeling and introduce an improved language representation model, Semantics-aware BERT (SemBERT), which is capable of explicitly absorbing contextual semantics over a BERT backbone. Moreover, Zhuosheng et al.(Zhang et al., 2019) propose using syntax to guide the text modeling by incorporating explicit syntactic constraints into the attention mechanism for better linguistically motivated word representations. Other relevant works have been proposed including (Yamada et al., 2020; Lan et al., 2020; Zhang et al., 2021).\n"
        ]
    },
    "S4.T2": {
        "caption": "Table 2: Federated training configuration",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Rounds</th>\n<th id=\"S4.T2.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t\">Nbr of clients</th>\n<th id=\"S4.T2.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Update size</th>\n<th id=\"S4.T2.1.1.1.4\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Nbr of parameters</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\">5</th>\n<th id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_t\">5</th>\n<td id=\"S4.T2.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\">400 MB</td>\n<td id=\"S4.T2.1.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\">109.483.776</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "We conduct a validation experiment on the SQUAD 1.0 dataset, which contains 100.000+ question-answer pairs on 500+ articles. To ensure collaborative training, we randomly select and split data over 5 clients with 20% for validation dataset for all clients. We used the BERT base as an encoder to build our model and the implementations are based on the public TensorFlow implementation from Keras. For the fine-tuning in our task, we set the initial learning rate to 5e-5. The batch size is set to 8. The maximum number of epochs is set to 1. Texts are tokenized using Wordpieces (Wu et al., 2016) with a maximum length of 384. More configuration is shown in Table 2."
        ]
    },
    "S4.T3": {
        "caption": "Table 3: Comparisons with equivalent parameters on the validation set of SQuAD1.0",
        "table": "<table id=\"S4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Model</th>\n<th id=\"S4.T3.1.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">F1 score</th>\n<th id=\"S4.T3.1.1.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t\">Accuracy (EM)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">Baseline</td>\n<td id=\"S4.T3.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_t\">0.31</td>\n<td id=\"S4.T3.1.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_t\">0.75</td>\n</tr>\n<tr id=\"S4.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"S4.T3.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\">FedQAS</td>\n<td id=\"S4.T3.1.3.2.2\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\">0.33</td>\n<td id=\"S4.T3.1.3.2.3\" class=\"ltx_td ltx_align_left ltx_border_b ltx_border_t\">0.81</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To demonstrate the benefit of FedQAS, we partitioned the SQUAD dataset into 5 equal chunks, so that each client has \"20%\" of the total dataset. We then compare the federated scenario to centralized model training. Table 3 lists the available metrics for different training rounds of the global model. Our implemented model baselines show similar EM and F1 scores with the global model during the first rounds and slightly outperform the baseline with respect to data privacy and enabling knowledge sharing across participants. Overall, the result shows that question-answering in federated learning settings performs well compared to centralized settings, see Figure 5 for the convergence of accuracy (EM) and Figure 6 for the convergence of F1.\n"
        ]
    },
    "S4.T4": {
        "caption": "Table 4: Comparison of answer prediction on test data",
        "table": "<table id=\"S4.T4.1\" class=\"ltx_tabular ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T4.1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.1.1.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Title:</span> Project Apollo</td>\n</tr>\n<tr id=\"S4.T4.1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.2.2.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.2.2.1.1\" class=\"ltx_text ltx_font_bold\">Passage:</span> The Apollo program, also known as Project Apollo, was the third United States human spaceflight program</td>\n</tr>\n<tr id=\"S4.T4.1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.3.3.1\" class=\"ltx_td ltx_align_left\">carried out by the National Aeronautics and Space Administration(NASA), which accomplished landing the</td>\n</tr>\n<tr id=\"S4.T4.1.4.4\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.4.4.1\" class=\"ltx_td ltx_align_left\">first humans on the Moon from 1969 to 1972. First conceived during Dwight D. Eisenhower’s administration</td>\n</tr>\n<tr id=\"S4.T4.1.5.5\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.5.5.1\" class=\"ltx_td ltx_align_left\">as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space,</td>\n</tr>\n<tr id=\"S4.T4.1.6.6\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.6.6.1\" class=\"ltx_td ltx_align_left\">Apollo was later dedicated to President John F. Kennedy’s national goal of landing a man on the Moon and</td>\n</tr>\n<tr id=\"S4.T4.1.7.7\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.7.7.1\" class=\"ltx_td ltx_align_left\">returning him safely to the Earth by the end of the 1960s, which he proposed in a May 25, 1961,</td>\n</tr>\n<tr id=\"S4.T4.1.8.8\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.8.8.1\" class=\"ltx_td ltx_align_left\">address to Congress. Project Mercury was followed by the two-man Project Gemini. The first manned flight</td>\n</tr>\n<tr id=\"S4.T4.1.9.9\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.9.9.1\" class=\"ltx_td ltx_align_left\">of Apollo was in 1968. Apollo ran from 1961 to 1972, and was supported by the two man Gemini program</td>\n</tr>\n<tr id=\"S4.T4.1.10.10\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.10.10.1\" class=\"ltx_td ltx_align_left\">which ran concurrently with it from 1962 to 1966…</td>\n</tr>\n<tr id=\"S4.T4.1.11.11\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.11.11.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.11.11.1.1\" class=\"ltx_text ltx_font_bold\">Question 1:</span> How long did Project Apollo run?</td>\n</tr>\n<tr id=\"S4.T4.1.12.12\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.12.12.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.12.12.1.1\" class=\"ltx_text ltx_font_bold\">Gold answer (human):</span> 1961 to 1972</td>\n</tr>\n<tr id=\"S4.T4.1.13.13\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.13.13.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.13.13.1.1\" class=\"ltx_text ltx_font_bold\">Google search engine answer:</span> see Figure <a href=\"#S1.F1\" title=\"Figure 1 ‣ 1 Introduction ‣ FedQAS: Privacy-aware machine reading comprehension with federated learning\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\">1</span></a>\n</td>\n</tr>\n<tr id=\"S4.T4.1.14.14\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.14.14.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.14.14.1.1\" class=\"ltx_text ltx_font_bold\">Baseline model answer:</span> 1961 to 1972</td>\n</tr>\n<tr id=\"S4.T4.1.15.15\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.15.15.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.15.15.1.1\" class=\"ltx_text ltx_font_bold\">FedQAS answer:</span> 1961 to 1972</td>\n</tr>\n<tr id=\"S4.T4.1.16.16\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.16.16.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.16.16.1.1\" class=\"ltx_text ltx_font_bold\">Question 2:</span> What program was created to carry out these projects and missions?</td>\n</tr>\n<tr id=\"S4.T4.1.17.17\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.17.17.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.17.17.1.1\" class=\"ltx_text ltx_font_bold\">Gold answer (human):</span> Apollo program</td>\n</tr>\n<tr id=\"S4.T4.1.18.18\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.18.18.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.18.18.1.1\" class=\"ltx_text ltx_font_bold\">Baseline model answer:</span> National Aeronautics and Space Administration</td>\n</tr>\n<tr id=\"S4.T4.1.19.19\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.19.19.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.19.19.1.1\" class=\"ltx_text ltx_font_bold\">FedQAS answer:</span> Apollo program</td>\n</tr>\n<tr id=\"S4.T4.1.20.20\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.20.20.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.20.20.1.1\" class=\"ltx_text ltx_font_bold\">Question 3:</span> What year did the first manned Apollo flight occur?</td>\n</tr>\n<tr id=\"S4.T4.1.21.21\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.21.21.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.21.21.1.1\" class=\"ltx_text ltx_font_bold\">Gold answer (human):</span> 1968</td>\n</tr>\n<tr id=\"S4.T4.1.22.22\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.22.22.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.22.22.1.1\" class=\"ltx_text ltx_font_bold\">Baseline model answer:</span> 1968</td>\n</tr>\n<tr id=\"S4.T4.1.23.23\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.23.23.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.23.23.1.1\" class=\"ltx_text ltx_font_bold\">FedQAS answer:</span> 1968</td>\n</tr>\n<tr id=\"S4.T4.1.24.24\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.24.24.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.24.24.1.1\" class=\"ltx_text ltx_font_bold\">Question 4:</span> What President is credited with the original notion of putting Americans in space?</td>\n</tr>\n<tr id=\"S4.T4.1.25.25\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.25.25.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.25.25.1.1\" class=\"ltx_text ltx_font_bold\">Gold answer (human):</span> John F. Kennedy</td>\n</tr>\n<tr id=\"S4.T4.1.26.26\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.26.26.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.26.26.1.1\" class=\"ltx_text ltx_font_bold\">Baseline model answer:</span> John F. Kennedy</td>\n</tr>\n<tr id=\"S4.T4.1.27.27\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.27.27.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.27.27.1.1\" class=\"ltx_text ltx_font_bold\">FedQAS answer:</span> John F. Kennedy</td>\n</tr>\n<tr id=\"S4.T4.1.28.28\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.28.28.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.28.28.1.1\" class=\"ltx_text ltx_font_bold\">Question 5:</span> Who did the U.S. collaborate with on an Earth orbit mission in 1975?</td>\n</tr>\n<tr id=\"S4.T4.1.29.29\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.29.29.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.29.29.1.1\" class=\"ltx_text ltx_font_bold\">Gold answer (human):</span> Soviet Union</td>\n</tr>\n<tr id=\"S4.T4.1.30.30\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.30.30.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.30.30.1.1\" class=\"ltx_text ltx_font_bold\">Baseline model answer:</span> Soviet Union</td>\n</tr>\n<tr id=\"S4.T4.1.31.31\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.31.31.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.31.31.1.1\" class=\"ltx_text ltx_font_bold\">FedQAS answer:</span> Soviet Union</td>\n</tr>\n<tr id=\"S4.T4.1.32.32\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.32.32.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.32.32.1.1\" class=\"ltx_text ltx_font_bold\">Question 6:</span> How long did Project Apollo run?</td>\n</tr>\n<tr id=\"S4.T4.1.33.33\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.33.33.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.33.33.1.1\" class=\"ltx_text ltx_font_bold\">Gold answer (human):</span> 1962 to 1966</td>\n</tr>\n<tr id=\"S4.T4.1.34.34\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.34.34.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.34.34.1.1\" class=\"ltx_text ltx_font_bold\">Baseline model answer:</span> 1961 to 1972, and was supported by the two man Gemini program which ran 1966</td>\n</tr>\n<tr id=\"S4.T4.1.35.35\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.35.35.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.35.35.1.1\" class=\"ltx_text ltx_font_bold\">FedQAS answer:</span> 1962 to 1966</td>\n</tr>\n<tr id=\"S4.T4.1.36.36\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.36.36.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.36.36.1.1\" class=\"ltx_text ltx_font_bold\">Question 7:</span> What program helped develop space travel techniques that Project Apollo used?</td>\n</tr>\n<tr id=\"S4.T4.1.37.37\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.37.37.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.37.37.1.1\" class=\"ltx_text ltx_font_bold\">Gold answer (human):</span> Gemini</td>\n</tr>\n<tr id=\"S4.T4.1.38.38\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.38.38.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.38.38.1.1\" class=\"ltx_text ltx_font_bold\">Baseline model answer:</span> Gemini</td>\n</tr>\n<tr id=\"S4.T4.1.39.39\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.39.39.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.39.39.1.1\" class=\"ltx_text ltx_font_bold\">FedQAS answer:</span> Gemini</td>\n</tr>\n<tr id=\"S4.T4.1.40.40\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.40.40.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.40.40.1.1\" class=\"ltx_text ltx_font_bold\">Question 8:</span> What space station supported three manned missions in 1973-1974?</td>\n</tr>\n<tr id=\"S4.T4.1.41.41\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.41.41.1\" class=\"ltx_td ltx_align_left ltx_border_t\">\n<span id=\"S4.T4.1.41.41.1.1\" class=\"ltx_text ltx_font_bold\">Gold answer (human):</span> Skylab</td>\n</tr>\n<tr id=\"S4.T4.1.42.42\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.42.42.1\" class=\"ltx_td ltx_align_left\">\n<span id=\"S4.T4.1.42.42.1.1\" class=\"ltx_text ltx_font_bold\">Baseline model answer:</span> Skylab</td>\n</tr>\n<tr id=\"S4.T4.1.43.43\" class=\"ltx_tr\">\n<td id=\"S4.T4.1.43.43.1\" class=\"ltx_td ltx_align_left ltx_border_b\">\n<span id=\"S4.T4.1.43.43.1.1\" class=\"ltx_text ltx_font_bold\">FedQAS answer:</span> Skylab</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "To gain an intuitive observation of the predictions, we give a prediction example on SQuAD1.0 from both the baseline and federated model in Table 4, which shows that FedQAS works better at answering the question on a given passage. Hence, the proposed approach has contributed overall to a better understanding of QA, preserving data privacy, and contributed to low-cost training as well as a collaborative question answering system task using very large models.\n"
        ]
    }
}