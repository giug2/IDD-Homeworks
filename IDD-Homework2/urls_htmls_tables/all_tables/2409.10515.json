{
    "id_table_1": {
        "caption": "Table 1:   Word error rate reduction (WERR) on the  all  dataset when using different reformulation up-sampling rates.",
        "table": "S3.T1.6",
        "footnotes": [],
        "references": [
            "As discussed in  section 1 , supervised context modeling for automatic speech recognition largely falls into three categories:",
            "An overview of our approach is given in  Figure 1 , and consists of two key components: a context-aware teacher model, leveraging both explicit context signals and implicit user feedback in the dialogue, and a single-utterance student model distilled from the context-aware teacher.",
            "Instead, of such a complex all-reduce based approach, we target the root of the problem by aiming to build more effective local batches (i.e. batches that will induce high contrastive loss by leveraging hard negative mining, introduced by  Robinson et al. ( 2021 ) . Traditionally, such methods for hard-negative mining rely on a pre-labeling step, where batches are pre-constructed in an offline-scan, and then consumed during training. Unfortunately, such a pre-labeling approach does not scale well as the size of the training data increases. To remedy this, we introduce Ohm, a simple online hard-negative mining procedure that can run in line with traditional streaming data pipelines. An overview of the Ohm approach is given in  Algorithm 1 .",
            "In addition to contrastive learning we further over-sample interactions containing reformulations during training of both the transducer and re-scorer. This approach can help to emphasize loss from reformulation samples, without introducing additional overhead. In our experiments, we empirically find an over-sampling ratio of 1:5 (1 reformulation to every 5 standard samples) to be effective (See  Table 1 ).",
            "Our overall system comprises both a student and a teacher ASR system. Our student model is a Conformer-based transducer network  (Gulati et al.,  2020 )  - a conventional streaming model i.e. it operates on single-utterance and attends to only past frames in the utterance. During run-time, governed by latency constraints, we use the student model to recognize user queries. These interactions (including reformulations and repeats) are captured  details on how to determine reformulations are described in  subsection 4.1 . Such interactions are then decoded using a context-aware teacher (discussed in the previous section) to get a recognition hypothesis, which acts as a label for semi-supervised training of student model  (Parthasarathi & Strom,  2019 ) .",
            "We  train  our context encoder teacher models on 10M de-identified dialogues. Additionally, we upsample dialogues containing user reformulations, by 20%, during training of the transducer i.e. one in every five dialogues has reformulation. For  evaluation , we only select dialogues containing reformulations and ensure that all utterances in the dialog are human-transcribed. By focusing on dialogues where the user reformulated his query, we ensure that the selected dialog has significant ASR errors (as discussed in section  subsubsection 3.1.2 ) and where contextual signals are expected to be meaningfully related to user queries. We create two datasets for evaluation (1)  all : All transcribed utterances across all validation dialogues (60K utterances) and (2)  ref : A subset of  all  containing only utterances that lead to user reformulations of the query (8.5K utterances).",
            "Our overall results for the teacher model on the  all  and  ref  are given in  Table 2 . We can see that our system, combining the feature-concatenation audio context ( subsection 3.1 ), learned text context ( subsection 3.1 ), and CLC/Ohm losses ( subsubsection 3.1.2 ), outperforms the baseline model by up to 9.58% on the  ref  and up to 6.91% on the  all  dataset. These trends hold across model sizes, as our context-enabled model has similar improvement in both the 200M and 1B cases, implying such improvements are model-size independent. Note that for ASR models, 1B parameters is generally considered quite large, given the challenging latency and run-time requirements for ASR applications.",
            "We can further break down the performance in  Table 6  in terms of insertions, deletions and substitutions, which is given in  Table A.1 . We can see that adding CLC loss significantly improves the rate of deletion compared to baseline models. Unfortunately, this comes at the cost of improvement in substitution and insertion. CLC, instead of doing the best job of disambiguating generated tokens, focuses on recall as opposed to precision. Ohm improves the disambiguation, as at the cost of deletions: more tokens are dropped, but the tokens that are preserved are more accurate."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:   WER improvements on the  all  and  ref  datasets for the teacher model. LE: Learned Embeddings, AE: Audio Embeddings, FC: Feature Concatenation, WERR: Word Error Rate Reduction.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "In this work, we leverage several explicit context sources drawn from the dialogues themselves. The first is the audio context, formed by the sequence of user input queries in a given dialogue (preceding and succeeding audio context is represented as  X P superscript X P X^{P} italic_X start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT  and  X F superscript X F X^{F} italic_X start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT  respectively in  Figure 2 ). The second is text context, the ASR one-best hypothesis (indicated as  Y ^ P superscript ^ Y P \\hat{Y}^{P} over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT  and  Y ^ F superscript ^ Y F \\hat{Y}^{F} over^ start_ARG italic_Y end_ARG start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ) corresponding to the sequence of user input queries in a dialogue along with the response generated by assistant encoded in text form (indicated as  A P superscript A P A^{P} italic_A start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT  and  A F superscript A F A^{F} italic_A start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ).",
            "Audio Embeddings:  For audio embeddings, past and future context is encoded via a separate encoder (called context encoder). We use either a HuBERT pre-trained conformer encoder or the audio encoder of the transducer network as the context encoder. These audio embeddings are passed through a multi-headed self-attentive pooling layer  (Chang et al.,  2023 )  and concatenated in time dimension with keys and values of self-attention module (MHSA) in the audio encoder (represented in  Figure 2 . Thus the inputs to MHSA (query - q, key - k, value - v) can be represented as  q = X ; k = [ X P , X , X F ] ; v = [ X P , X , X F ] formulae-sequence q X formulae-sequence k superscript X P X superscript X F v superscript X P X superscript X F q=X;k=[{X}^{P},X,{X}^{F}];v=[{X}^{P},X,{X}^{F}] italic_q = italic_X ; italic_k = [ italic_X start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT , italic_X , italic_X start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ] ; italic_v = [ italic_X start_POSTSUPERSCRIPT italic_P end_POSTSUPERSCRIPT , italic_X , italic_X start_POSTSUPERSCRIPT italic_F end_POSTSUPERSCRIPT ] . This allows us to attend to the contextual signal on a per-query basis. Note here that the output and input of the MHSA module still have the same number of time frames. This ensures that no other component in the model needs to be modified. Another distinct advantage of re-purposing MHSA in this manner as opposed to introducing a separate cross-attention layer (to attend to context) is that it allows us to easily extend conventional single utterance models to be context aware.",
            "We  train  our context encoder teacher models on 10M de-identified dialogues. Additionally, we upsample dialogues containing user reformulations, by 20%, during training of the transducer i.e. one in every five dialogues has reformulation. For  evaluation , we only select dialogues containing reformulations and ensure that all utterances in the dialog are human-transcribed. By focusing on dialogues where the user reformulated his query, we ensure that the selected dialog has significant ASR errors (as discussed in section  subsubsection 3.1.2 ) and where contextual signals are expected to be meaningfully related to user queries. We create two datasets for evaluation (1)  all : All transcribed utterances across all validation dialogues (60K utterances) and (2)  ref : A subset of  all  containing only utterances that lead to user reformulations of the query (8.5K utterances).",
            "Our overall results for the teacher model on the  all  and  ref  are given in  Table 2 . We can see that our system, combining the feature-concatenation audio context ( subsection 3.1 ), learned text context ( subsection 3.1 ), and CLC/Ohm losses ( subsubsection 3.1.2 ), outperforms the baseline model by up to 9.58% on the  ref  and up to 6.91% on the  all  dataset. These trends hold across model sizes, as our context-enabled model has similar improvement in both the 200M and 1B cases, implying such improvements are model-size independent. Note that for ASR models, 1B parameters is generally considered quite large, given the challenging latency and run-time requirements for ASR applications."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:   Results on OD3 (overall and repeat/rephrase inducing) using the 200M model. WER (   \\downarrow  ): Word Error Rate, BERT-S (   \\uparrow  ): Bert Score. B: Basline, CX: Context, C: CLC Loss, O: Ohm.",
        "table": "S5.T3.9",
        "footnotes": [],
        "references": [
            "We  train  our context encoder teacher models on 10M de-identified dialogues. Additionally, we upsample dialogues containing user reformulations, by 20%, during training of the transducer i.e. one in every five dialogues has reformulation. For  evaluation , we only select dialogues containing reformulations and ensure that all utterances in the dialog are human-transcribed. By focusing on dialogues where the user reformulated his query, we ensure that the selected dialog has significant ASR errors (as discussed in section  subsubsection 3.1.2 ) and where contextual signals are expected to be meaningfully related to user queries. We create two datasets for evaluation (1)  all : All transcribed utterances across all validation dialogues (60K utterances) and (2)  ref : A subset of  all  containing only utterances that lead to user reformulations of the query (8.5K utterances).",
            "Our overall results for the teacher model on the  all  and  ref  are given in  Table 2 . We can see that our system, combining the feature-concatenation audio context ( subsection 3.1 ), learned text context ( subsection 3.1 ), and CLC/Ohm losses ( subsubsection 3.1.2 ), outperforms the baseline model by up to 9.58% on the  ref  and up to 6.91% on the  all  dataset. These trends hold across model sizes, as our context-enabled model has similar improvement in both the 200M and 1B cases, implying such improvements are model-size independent. Note that for ASR models, 1B parameters is generally considered quite large, given the challenging latency and run-time requirements for ASR applications.",
            "We can see that learning from the implicit context in the model is important for understanding and correcting dialog errors. As shown in  Table 6 , Adding CLC and Ohm to the baseline model leads to significant improvement in the overall performance, particularly on the  ref  dataset (so much that it enables a 200M parameter model to outperform a 1B parameter model without such losses). On the OD3 dataset ( Table 3 ), the performance is even more distinct, with learning from implicit context leading to up to a 26.6% relative improvement over a baseline non-context model. In addition, zero shot comparison with other open source benchmark models is shown in  Table 4 .",
            "Table 7  shows the performance of our model when distilled to a context-free student model. We can see that in all cases, the student model distilled from a context-trained model achieves superior performance. We also evaluate the distillation efficiency (DE) of the models  how much of the WER gains of the teacher model were retained during distillation. It is interesting to note that when leveraging the  ssrd  dataset, only 20% of the parameters in the model are necessary during the distillation process to achieve the same WERR, compared to when less reformulation data is used (see  subsection 4.3 ), indicating that the using the pre-trained teacher model with context not only is more accurate, but can be more efficient as well."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:   Zero-shot results on OD3 for several open-source models - Whisper  (Radford et al.,  2023 ) , Conformer  (Gulati et al.,  2020 ) , Wav2Vec 2  (Baevski et al.,  2020 ) , Streaming Conformer  (Tsunoo et al.,  2021 ) , CLC  (Chan et al.,  2024 ) . Models in this table are not directly comparable (trained on differing data, setups, hyperparameters, optimizers etc.), but serve as a benchmark for performance on OD3 under several varying setups. WER (   \\downarrow  ): Word Error Rate, BERT-S (   \\uparrow  ): Bert Score",
        "table": "S5.T4.29",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Our overall system comprises both a student and a teacher ASR system. Our student model is a Conformer-based transducer network  (Gulati et al.,  2020 )  - a conventional streaming model i.e. it operates on single-utterance and attends to only past frames in the utterance. During run-time, governed by latency constraints, we use the student model to recognize user queries. These interactions (including reformulations and repeats) are captured  details on how to determine reformulations are described in  subsection 4.1 . Such interactions are then decoded using a context-aware teacher (discussed in the previous section) to get a recognition hypothesis, which acts as a label for semi-supervised training of student model  (Parthasarathi & Strom,  2019 ) .",
            "As discussed in  section 4 , we evaluate our system on both closed-source data and the OD3 dataset. In general, we use both standard word error rate (WER,    \\downarrow  ) and relative word error rate improvement (WERR,    \\uparrow  ) to evaluate our system.",
            "We can see that learning from the implicit context in the model is important for understanding and correcting dialog errors. As shown in  Table 6 , Adding CLC and Ohm to the baseline model leads to significant improvement in the overall performance, particularly on the  ref  dataset (so much that it enables a 200M parameter model to outperform a 1B parameter model without such losses). On the OD3 dataset ( Table 3 ), the performance is even more distinct, with learning from implicit context leading to up to a 26.6% relative improvement over a baseline non-context model. In addition, zero shot comparison with other open source benchmark models is shown in  Table 4 .",
            "Table 7  shows the performance of our model when distilled to a context-free student model. We can see that in all cases, the student model distilled from a context-trained model achieves superior performance. We also evaluate the distillation efficiency (DE) of the models  how much of the WER gains of the teacher model were retained during distillation. It is interesting to note that when leveraging the  ssrd  dataset, only 20% of the parameters in the model are necessary during the distillation process to achieve the same WERR, compared to when less reformulation data is used (see  subsection 4.3 ), indicating that the using the pre-trained teacher model with context not only is more accurate, but can be more efficient as well."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:   Ablation experiments on the teacher model (200M). WERR: Word Error Rate Reduction.",
        "table": "S5.T5.1",
        "footnotes": [],
        "references": [
            "In  Table 5 , we ablate the types of context that we show to the model. We observe that injecting non-causal (future) context  during training  provides relative WERR of 7.39% as opposed to 5.08% on the  ref  dataset (as well as improvements on the  all  dataset), indicating that future context is significantly more important when correcting user reformulations. This is likely due to the fact that user reformulation is a future signal i.e. it follows the utterance that caused the error."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:   WER improvements on the  all  and  ref  datasets for the teacher model with CLC/Ohm and  context-aware teacher model as baseline . Note: Baseline is different from Table 1 to ensure comparable training setup. WERR: Word Error Rate Reduction.",
        "table": "S5.T6.1",
        "footnotes": [],
        "references": [
            "We can see that learning from the implicit context in the model is important for understanding and correcting dialog errors. As shown in  Table 6 , Adding CLC and Ohm to the baseline model leads to significant improvement in the overall performance, particularly on the  ref  dataset (so much that it enables a 200M parameter model to outperform a 1B parameter model without such losses). On the OD3 dataset ( Table 3 ), the performance is even more distinct, with learning from implicit context leading to up to a 26.6% relative improvement over a baseline non-context model. In addition, zero shot comparison with other open source benchmark models is shown in  Table 4 .",
            "We can further break down the performance in  Table 6  in terms of insertions, deletions and substitutions, which is given in  Table A.1 . We can see that adding CLC loss significantly improves the rate of deletion compared to baseline models. Unfortunately, this comes at the cost of improvement in substitution and insertion. CLC, instead of doing the best job of disambiguating generated tokens, focuses on recall as opposed to precision. Ohm improves the disambiguation, as at the cost of deletions: more tokens are dropped, but the tokens that are preserved are more accurate."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:   Table showing the impact of distillation from a teacher model trained with implicit/explicit context. WERR: Word Error Rate Reduction. DE: Distillation Efficiency.",
        "table": "S5.T7.2",
        "footnotes": [],
        "references": [
            "Table 7  shows the performance of our model when distilled to a context-free student model. We can see that in all cases, the student model distilled from a context-trained model achieves superior performance. We also evaluate the distillation efficiency (DE) of the models  how much of the WER gains of the teacher model were retained during distillation. It is interesting to note that when leveraging the  ssrd  dataset, only 20% of the parameters in the model are necessary during the distillation process to achieve the same WERR, compared to when less reformulation data is used (see  subsection 4.3 ), indicating that the using the pre-trained teacher model with context not only is more accurate, but can be more efficient as well."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:   WERR when normalized by the domain (instead of by-utterance) on the  all  dataset. WERR (   \\uparrow  ): Word Error Rate Reduction. SERR (   \\uparrow  ): Sentence Error Rate Improvement.",
        "table": "S5.T8.12",
        "footnotes": [],
        "references": [
            "While overall WER is an important measure, many times, a strong indicator of user satisfaction is performance on a wide range of queries on different topics (Such as home automation, calling/messaging and shopping). In  Table 8 , we present WERR and SERR (Sentence Error Rate Improvement) when the WER is computed on each topic independently, and then averaged instead of being averaged over all utterances (independent of domain, i.e.  Table 8  makes the assumption that all domains are equally likely). From this, we can see that while our non-context models perform well on the most common utterances, the contrastive models lead to significant improvements in less-common domains in our  all  dataset, including queries categorized into shopping (82.86% WERR), calling/messaging tasks (73.7% WERR), and music request tasks (36.8% WERR), all of which often need contextual disambiguation. On the other hand, while still in the long tail of the dataset, our approach performs worse than the baseline on home automation tasks (-22.68% WERR), one of the less diverse tasks that requires less contextual disambiguation. In such cases, our model may be relying more on the context, than the target utterance: leading to decreased performance. It remains interesting for future work to explore how we can dynamically trade off between context clues (for challenging utterances), and non-context learning (for utterances that dont require contextual disambiguation)."
        ]
    },
    "id_table_9": {
        "caption": "Table A.1:   WERR when normalized by the domain (instead of by-utterance) on the  all  dataset. WERR (   \\uparrow  ): Word Error Rate Reduction. SERR (   \\uparrow  ): Sentence Error Rate Improvement. INSR: Relative Insertion rate. SUBR: Relative Substitution Rate. DELR: Relative Deletion Rate",
        "table": "A1.T1.12",
        "footnotes": [],
        "references": []
    }
}