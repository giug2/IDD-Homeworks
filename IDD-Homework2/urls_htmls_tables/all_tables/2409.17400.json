{
    "S4.T1.3.3": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.3.3.4.1\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T1.3.3.4.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Dataset</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T1.3.3.4.1.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">#</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S4.T1.3.3.4.1.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Sensors(Resolution)</th>\n<th class=\"ltx_td ltx_nopad_l ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"3\" id=\"S4.T1.3.3.4.1.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Statistics</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.3.5.2\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row\" id=\"S4.T1.3.3.5.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_th_row\" id=\"S4.T1.3.3.5.2.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Images</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_column ltx_th_row\" id=\"S4.T1.3.3.5.2.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.3.3.5.2.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Objects</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.3.3.5.2.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Min/Max</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S4.T1.3.3.5.2.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Avg</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.1.1.1.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Apple Flower</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.1.1.1.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">325</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S4.T1.1.1.1.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">ZED2,Kinect V2 (1920 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.1.1.1.1.m1.1\"><semantics id=\"S4.T1.1.1.1.1.m1.1a\"><mo id=\"S4.T1.1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.1.m1.1b\"><times id=\"S4.T1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T1.1.1.1.1.m1.1d\">&#215;</annotation></semantics></math> 1080)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.1.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">43031</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.1.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">4/434</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t\" id=\"S4.T1.1.1.1.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">132</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.2.2.2\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row\" id=\"S4.T1.2.2.2.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_th ltx_th_row\" id=\"S4.T1.2.2.2.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row\" id=\"S4.T1.2.2.2.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Sony RX100 (5472 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.2.2.2.1.m1.1\"><semantics id=\"S4.T1.2.2.2.1.m1.1a\"><mo id=\"S4.T1.2.2.2.1.m1.1.1\" xref=\"S4.T1.2.2.2.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.2.1.m1.1b\"><times id=\"S4.T1.2.2.2.1.m1.1.1.cmml\" xref=\"S4.T1.2.2.2.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.2.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T1.2.2.2.1.m1.1d\">&#215;</annotation></semantics></math> 3648)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" id=\"S4.T1.2.2.2.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" id=\"S4.T1.2.2.2.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r\" id=\"S4.T1.2.2.2.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\"/>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.3.3.3\">\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T1.3.3.3.2\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Apple Fruit <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib7\" title=\"\">7</a>]</cite>\n</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T1.3.3.3.3\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">630</th>\n<th class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t\" id=\"S4.T1.3.3.3.1\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">Kinect V2 (1920 <math alttext=\"\\times\" class=\"ltx_Math\" display=\"inline\" id=\"S4.T1.3.3.3.1.m1.1\"><semantics id=\"S4.T1.3.3.3.1.m1.1a\"><mo id=\"S4.T1.3.3.3.1.m1.1.1\" xref=\"S4.T1.3.3.3.1.m1.1.1.cmml\">&#215;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.3.3.3.1.m1.1b\"><times id=\"S4.T1.3.3.3.1.m1.1.1.cmml\" xref=\"S4.T1.3.3.3.1.m1.1.1\"/></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.3.3.1.m1.1c\">\\times</annotation><annotation encoding=\"application/x-llamapun\" id=\"S4.T1.3.3.3.1.m1.1d\">&#215;</annotation></semantics></math> 1080)</th>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T1.3.3.3.4\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">34404</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T1.3.3.3.5\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">10/121</td>\n<td class=\"ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_bb ltx_border_t\" id=\"S4.T1.3.3.3.6\" style=\"padding-left:1.5pt;padding-right:1.5pt;\">55</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "TABLE I:  Dataset used in this study; Images were acquired from multiple apple varieties in different architectures and imaging sensors.",
        "footnotes": [
            "[7] \nF. Gao, L. Fu, X. Zhang, Y. Majeed, R. Li, M. Karkee, and Q. Zhang,\n“Multi-class fruit-on-plant detection for apple in snap system using faster\nr-cnn,”  Computers and Electronics in Agriculture , vol. 176, p.\n105634, 2020.\n\n"
        ],
        "references": [
            "The experimental evaluation consisted of two datasets with apple flowers and apples (see Table I). Apple flower images were collected over two growing seasons (2018 and 2019) using different imaging sensors. Images were collected from three commercial apple orchards in Washington, USA, with multiple varieties grown in fruiting wall canopy architecture. Trees were trained and pruned to create narrow 2D structures using vertical or V-shaped trellis systems in fruiting wall architectures. The flower dataset included images of early to late phases of the flower blooming in three apple varieties: Scifresh (Vertical Fruiting Wall), Envi (V trellis), and HoneyCrisp (V trellis). Additionally, the apple fruit dataset publicly released by [7] as a part of a multi-class fruit classification experiment was annotated and used. The fruit dataset consisted of images of 800 harvest-ready apple canopies in vertical fruiting wall architecture [7]. Images with similar appearances were identified using Structural Similarity Index Measure (SSIM) greater than 95%,\nwhich were later removed by human verification resulting in 630 unique apple canopy images[39]. For experimental evaluation, each dataset was divided into a training set (75%) and a test set (25%). The validation dataset was obtained by randomly cropping the test images."
        ]
    },
    "S5.T2.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T2.1.1.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T2.1.1.1.2\">Apple Flower</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T2.1.1.1.3\">Apple Fruit <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib7\" title=\"\">7</a>]</cite>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T2.1.2.2.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.1.2.2.2\">PSNR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.1.2.2.3\">SSIM</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.1.2.2.4\">PSNR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T2.1.2.2.5\">SSIM</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T2.1.3.1.1\">CSRNet<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib13\" title=\"\">13</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.2\">24.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.3\">0.845</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.4\">22.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.3.1.5\">0.734</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.1.4.2.1\">SFCN <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib14\" title=\"\">14</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.2\">28.5</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.3\">0.912</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.4\">22.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.2.5\">0.901</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T2.1.5.3.1\">SCAR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib15\" title=\"\">15</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.2\">27.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.3\">0.910</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.5.3.4.1\">24.7</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.5.3.5\">0.908</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T2.1.6.4.1\">AgRegNet (Ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.6.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.6.4.2.1\">31.2</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.6.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.6.4.3.1\">0.938</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.6.4.4\">24.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.6.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T2.1.6.4.5.1\">0.910</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "TABLE II:  Comparison of PSNR and SSIM of the proposed approach with different density map estimation methods.",
        "footnotes": [
            "[7] \nF. Gao, L. Fu, X. Zhang, Y. Majeed, R. Li, M. Karkee, and Q. Zhang,\n“Multi-class fruit-on-plant detection for apple in snap system using faster\nr-cnn,”  Computers and Electronics in Agriculture , vol. 176, p.\n105634, 2020.\n\n",
            "[13] \nY. Li, X. Zhang, and D. Chen, “Csrnet: Dilated convolutional neural networks\nfor understanding the highly congested scenes,” in  Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June\n2018.\n\n",
            "[14] \nQ. Wang, J. Gao, W. Lin, and Y. Yuan, “Learning from synthetic data for crowd\ncounting in the wild,” in  Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 8198–8207.\n\n",
            "[15] \nJ. Gao, Q. Wang, and Y. Yuan, “Scar: Spatial-/channel-wise attention\nregression networks for crowd counting,”  Neurocomputing , vol. 363,\npp. 1–8, 2019.\n\n"
        ],
        "references": [
            "Table II shows the quantitative comparison of the generated density maps for the apple flower and fruit dataset. Our proposed approach showed competitive performance, often outperforming other approaches with higher PSNR (except SCAR for Apple Fruit Dataset) and SSIM values inferring a greater similarity between the ground truth and generated density maps. The generated density maps were 93.8% and 91.0% similar to the ground truth density map in terms of luminance, contrast, and structure for the flower and fruit datasets, respectively. Fig. 5 shows a representative density maps for qualitative evaluation. The generated density maps from the proposed approach showed better localization of flower and fruit regions. The advantage of the proposed approach is more prominent in the flower images. In spite of the fact that the flowers are densely located in clusters with a high flower-to-flower occlusion, the individual flower regions from the proposed approach were separated with finer details compared to other approaches. We argue this is because of the way feature maps were processed, especially in the decoder section. First, the skip connection from the encoder to the decoder section allowed the low-level feature transfer to the deeper layers, which might have been lost otherwise due to processing and downsampling. Second, similar to the way feature maps were reduced in half in size at each stage in each ConvNeXt - T encoder, the feature maps were progressively up-sampled, concatenated, and processed in the decoder block with feature size at the current sub-block twice the size of the immediate past sub-block. In the case of CSRNet, the feature maps were directly upsampled by a factor of 8 to get the final density map which could be the reason for the non-smooth density map. Third, the segmentation map was helpful in gatekeeping to enhance the relevant features while setting up boundaries for individual flowers and fruits. Furthermore, the proposed approach showed robustness to suppress background objects (flowers/fruits in the immediate back row) that have a similar appearance to the object of interest. We argue that the result is a combined effect of the attention module and segmentation branch to enhance relevant features while suppressing background features."
        ]
    },
    "S5.T3.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T3.1.1.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T3.1.1.1.2\">Apple Flower</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T3.1.1.1.3\">Apple Fruit <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib7\" title=\"\">7</a>]</cite>\n</th>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.2.2\">\n<th class=\"ltx_td ltx_th ltx_th_row\" id=\"S5.T3.1.2.2.1\"/>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T3.1.2.2.2\">MAE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T3.1.2.2.3\">RMSE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T3.1.2.2.4\">MAE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S5.T3.1.2.2.5\">RMSE</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.3.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T3.1.3.1.1\">CSRNet<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib13\" title=\"\">13</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.3.1.2\">20.9</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.3.1.3\">29.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.3.1.4\">5.6</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.3.1.5\">7.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.4.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.1.4.2.1\">SFCN<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib14\" title=\"\">14</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.2.2\">35.8</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.2.3\">52.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.2.4\">3.9</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.2.5\">5.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.5.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.1.5.3.1\">SCAR<cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib15\" title=\"\">15</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.3.2\">18.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.3.3\">25.6</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.3.4\">4.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.3.5\">6.1</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.6.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T3.1.6.4.1\">Farjon et al. (DRN)<span class=\"ltx_note ltx_role_footnote\" id=\"footnote5\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_note_outer\"><span class=\"ltx_note_content\"><sup class=\"ltx_note_mark\">5</sup><span class=\"ltx_tag ltx_tag_note\">5</span>https://github.com/farjon/Leaf-Counting</span></span></span> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib31\" title=\"\">31</a>]</cite>\n</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.4.2\">25.7</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.4.3\">38.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.6.4.4.1\">2.4</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.6.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.6.4.5.1\">3.2</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.7.5\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T3.1.7.5.1\">AgRegNet (Ours)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.7.5.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.7.5.2.1\">18.1</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.7.5.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S5.T3.1.7.5.3.1\">23.8</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.7.5.4\">3.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.7.5.5\">4.0</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "TABLE III:  Comparison of MAE and RMSE of the proposed approach with different flower and fruit counting approaches.",
        "footnotes": [
            "[7] \nF. Gao, L. Fu, X. Zhang, Y. Majeed, R. Li, M. Karkee, and Q. Zhang,\n“Multi-class fruit-on-plant detection for apple in snap system using faster\nr-cnn,”  Computers and Electronics in Agriculture , vol. 176, p.\n105634, 2020.\n\n",
            "[13] \nY. Li, X. Zhang, and D. Chen, “Csrnet: Dilated convolutional neural networks\nfor understanding the highly congested scenes,” in  Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , June\n2018.\n\n",
            "[14] \nQ. Wang, J. Gao, W. Lin, and Y. Yuan, “Learning from synthetic data for crowd\ncounting in the wild,” in  Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2019, pp. 8198–8207.\n\n",
            "[15] \nJ. Gao, Q. Wang, and Y. Yuan, “Scar: Spatial-/channel-wise attention\nregression networks for crowd counting,”  Neurocomputing , vol. 363,\npp. 1–8, 2019.\n\n",
            "[31] \nG. Farjon, Y. Itzhaky, F. Khoroshevsky, and A. Bar-Hillel, “Leaf counting:\nFusing network components for improved accuracy,”  Frontiers in Plant\nScience , vol. 12, p. 575751, 2021.\n\n"
        ],
        "references": [
            "The proposed approach showed promising results for density estimation of apples and apple flowers regardless of the flowering stage and appearance, which was also reflected in the count evaluation (Table III). The proposed AgRegNet, while being lightweight with the least number of parameters (9.45M), showed superior performance compared to all evaluated approaches in the complicated dense flower image dataset with the highest PSNR (31.2) and SSIM (0.938) to estimate density map and lowest MAE (18.1) and RMSE (23.8) values to estimate the count. Furthermore, it achieved competitive performance in the fruit image dataset with MAE and RMSE of 3.1 and 4.0, respectively. The percentage MAE (%MAE=M⁢A⁢E#⁢A⁢v⁢g.f⁢l⁢o⁢w⁢e⁢r/f⁢r⁢u⁢i⁢t∗100%)\\Big{(}\\%MAE=\\frac{MAE}{\\#Avg.flower/fruit}*100\\%\\Big{)}( % italic_M italic_A italic_E = divide start_ARG italic_M italic_A italic_E end_ARG start_ARG # italic_A italic_v italic_g . italic_f italic_l italic_o italic_w italic_e italic_r / italic_f italic_r italic_u italic_i italic_t end_ARG ∗ 100 % ) for the proposed AgRegNet was 5.6% for fruit data and 13.7% for the flower dataset with average fruit and flower per image of 48 and 125 respectively. It is worth noting that remarkably better results were achieved in the fruit dataset compared to the flower dataset. Although fruits were located in clusters, the cluster size was substantially smaller, allowing larger spatial separation between the fruits leading to better-localized density maps and count results. As discussed earlier, the flower dataset was complex because the tightly aligned clusters with flowers in different blooming stages had different feature information."
        ]
    },
    "S5.T4.1": {
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T4.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T4.1.1.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T4.1.1.1.2\">Apple Flower</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"2\" id=\"S5.T4.1.1.1.3\">Apple Fruit <cite class=\"ltx_cite ltx_citemacro_cite\">[<a class=\"ltx_ref\" href=\"https://arxiv.org/html/2409.17400v1#bib.bib7\" title=\"\">7</a>]</cite>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T4.1.2.1\">\n<td class=\"ltx_td\" id=\"S5.T4.1.2.1.1\"/>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.1.2\">mAP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.1.3\">mAR</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.1.4\">mAP</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T4.1.2.1.5\">mAR</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T4.1.3.2\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb ltx_border_t\" id=\"S5.T4.1.3.2.1\">AgRegNet (Ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T4.1.3.2.2\">0.81</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T4.1.3.2.3\">0.79</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T4.1.3.2.4\">0.93</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\" id=\"S5.T4.1.3.2.5\">0.89</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "TABLE IV:  Mean Average Precision (mAP) and Average Recall (mAR) achieved by the proposed approach in flower and fruit localization",
        "footnotes": [
            "[7] \nF. Gao, L. Fu, X. Zhang, Y. Majeed, R. Li, M. Karkee, and Q. Zhang,\n“Multi-class fruit-on-plant detection for apple in snap system using faster\nr-cnn,”  Computers and Electronics in Agriculture , vol. 176, p.\n105634, 2020.\n\n"
        ],
        "references": [
            "In order to localize the flower and fruit centroid, the density maps were post-processed as per Algorithm 1 (see Section III C). The one-to-one matching between the ground truth and the predicted flower and fruit centroids was analyzed using the mean Average Precision (mAP) and mean Average Recall (mAR) metrics. The mean Average Precision (mAP) and mean Average Recall (mAR) metrics was obtained by averaging the AP and AR values of each image computed by varying the cutoff threshold σfsubscript𝜎𝑓\\sigma_{f}italic_σ start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT to 2.2⁢σf2.2subscript𝜎𝑓2.2\\sigma_{f}2.2 italic_σ start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT with an increment of 1 (see Section IV c). The cutoff threshold played a role analogous to the Intersection over Union (IoU) threshold in a detection-based approach. The mAP and mAR values for the fruit dataset were greater than the same for the flower dataset indicating better fruit localization capability with fewer false positive and false negative locations. Numerically, the mAP and mAR values for the fruit dataset were 0.93 and 0.89 respectively, while the same were 0.81 and 0.79 for the flower dataset (see Table IV). However, it is worth noting that the localization result depended on the generated density map, and the post-processing algorithms used to detect peak and one-to-one matching of the estimated centroids. Fig. 7 visualizes the localization results. The results were promising as the majority of the localized flower and fruit centroids from the proposed approach were correctly associated with the ground truth centroids in images acquired from a commercial orchard."
        ]
    },
    "S5.T5.1.1": {
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T5.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt\" id=\"S5.T5.1.1.1.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.1.2\">MAE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.1.3\">MSE</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.1.4\">PSNR</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T5.1.1.1.1.5\">SSIM</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.2.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t\" id=\"S5.T5.1.1.2.1.1\">Mod.ConvNeXt-T</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.2.1.2\">27.7</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.2.1.3\">37.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.2.1.4\">29.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T5.1.1.2.1.5\">0.841</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.3.2\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T5.1.1.3.2.1\">Mod.ConvNeXt-T+CBAM</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.3.2.2\">25.4</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.3.2.3\">36.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.3.2.4\">30.0</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.3.2.5\">0.855</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.4.3\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row\" id=\"S5.T5.1.1.4.3.1\">Mod.ConvNeXt-T+Seg.</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.3.2\">20.2</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.3.3\">29.3</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.3.4\">31.1</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T5.1.1.4.3.5\">0.931</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T5.1.1.5.4\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb\" id=\"S5.T5.1.1.5.4.1\">Mod.ConvNeXt-T+CBAM+Seg.</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.1.1.5.4.2\">18.1</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.1.1.5.4.3\">23.8</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.1.1.5.4.4\">31.2</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T5.1.1.5.4.5\">0.938</td>\n</tr>\n</tbody>\n</table>\n\n",
        "caption": "TABLE V:  Ablation experiment of the proposed approach to evaluate the contribution of each module in density map and count estimation.",
        "footnotes": [],
        "references": [
            "Table V shows that the addition of the CBAM and segmentation module showed improved performance of modified ConvNeXt - T. Fig. 8 shows the progressive refinement of generated density map with the addition of different modules. The modified ConvNeXt - T module suffered from high interference from the background objects, which was reduced to some extent by CBAM. The SSIM value of the modified ConvNeXt - T was 0.841, inferring 84.1% similarity between the ground truth and the predicted density map. It was also observed that better density map estimation inferred better localization capability, but it did not necessarily mean increased counting accuracy. Both CBAM and segmentation branches helped enhance the relevant features while suppressing the irrelevant background features. However, quantitative result showed that the effect of the segmentation branch in improving the density map and count was substantially higher compared to CBAM. The addition of the segmentation branch reduced the MAE and RMSE count by 27.1% and 21.2% while improving the density map SSIM by 10.7% compared to the Mod. ConvNeXt - T. On the other hand, the addition of CBAM reduced the MAE and RMSE by 8.3% and 2.4% while improving the density map SSIM by 1.6% compared to the Mod. ConvNeXt - T. As shown in Table V, the addition of the CBAM module in Mod. ConvNeXt - T with segmentation branch improved the system performance by a nominal value."
        ]
    }
}