{
    "id_table_1": {
        "caption": "Table 1:  Equivalent failure modes studied and reported in prior works. Existing studies focus on detecting and evaluating a subset of failure modes. For instance  FM1  is related to answer relevancy in  Es et al. ( 2023 ) ,  FM2  is related to negative rejection in  Chen et al. ( 2024 )   FM6  and  FM7  are related to faithfulness and more specifically to correctness and groundedness respectively in  Magesh et al.  ,  FM6  is related to  citation recall  in  Gao et al. ( 2023 ) .",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "In this section, we introduce more precisely the problem of  grounded question answering 3 3 3 Prior works often use the term RAG to denote the question answering task but this term is commonly used to refer to the broader pattern of combining retrieval and generation. To avoid confusion, we coin the term  grounded question answering  to denote the last step in RAG.  studied in this work. Given a question, RAG systems use information retrieval to match the question with a subset of documents from a knowledge base and then use an LLM to generate an answer grounded in the provided documents. LLMs have been shown to learn and store factual knowledge from data during their unsupervised pretraining  Petroni et al. ( 2019 ) , but this knowledge is static and can get outdated. Contrary to  Chen et al. ( 2024 ) , we thus require the LLM to stay faithful to the sources even if the documents contain information contradicting the LLM intrinsic knowledge. As interpretability and fact-checking are crucial in many domains for both the system developers and users, the LLM is also instructed to explicitly cite the reference for each affirmation in its answer as illustrated in the answers of Figure  1 .",
            "The information from the retrieved documents that helps answer the question is termed  relevant information . When the documents are insufficient to provide an answer, these situations are referred to as  adversarial . In such instances, the LLM should explicitly state that the question cannot be answered with the provided material. To avoid frustrating the user and to keep them engaged, it is common to include information related to the question, even if it does not directly answer it, as can be shown in type 2 ground truth answer in Figure  1 . This will be referred as  related information . Adversarial cases are evaluated using  negative rejection  in  Chen et al. ( 2024 )  but receive no special treatment in existing RAG automated evaluation frameworks.",
            "Table  1  relates the failure modes presented in our work with existing failure modes presented and reported by prior works. To quantify these seven failure modes, we introduce specific evaluation criteria for grounded question answering.  Answer relevancy  assesses the relevance of the information provided in the answer regarding the question, using a Likert scale (1 to 5), which helps to measure  FM1 .  Completeness  also uses a Likert scale to evaluate whether  all  relevant information from the documents is present in the answer, thus measuring  FM3 .  Faithfulness  is a binary score that checks if all facts in the answer are accurate and correctly attributed to the corresponding document, addressing  FM6  and  FM7 . In adversarial cases and when additional information is provided,  Usefulness  is a binary score that determines if the provided additional information is indeed useful and relevant to the question, measuring  FM5 . Usefulness can be considered a form of  soft relevancy  in adversarial cases. Lastly,  Positive Acceptance  and  Negative Rejection  are binary scores indicating a true positive and a true negative respectively in identifying whether the question is answerable, thereby measuring  FM4  and  FM2 . Not all failure modes can occur in all situations: Figure  2  clarifies the conditions under which each metric is defined, depending on whether the references contain an answer, if the answer provides a response, or if it adds related information when it does not provide a direct response.",
            "Our goal is to propose a benchmark to verify whether the evaluators assessments align with the defined metrics. We propose a typology of 16 test types designed to assess whether an evaluator appropriately penalizes all failure modes and rewards accurate answers across a diverse range of scenarios (Figure  1 ). Each test type specifies the expected characteristics for both references and answers, and defines an acceptable range of scores for each metric to be deemed valid. The tests focus primarily on edge cases or the detection of subtle errors.",
            "While  Es et al. ( 2023 )  showed that RAGAS metrics correlate with human judgment, our evaluation of their implementations on GroUSE reveals that they do not perform well on many individual tests, as illustrated in  Table   2  and Figure  3 . This observation suggests that correlation on judgement does not necessarily implies good calibration of grades on edge cases and thus good error detection. This hypothesis will be further explored in  section   5.1 . The proposed automatic metrics rely on several sequential LLM calls, which can increase the likelihood of errors and reduce the robustness of the evaluation across samples. Interestingly, different implementations of the same metrics can yield very different results. For instance, although faithfulness is defined similarly in RAGAS and DeepEval, the unit test results differ significantly due to differences in the prompts used in their respective implementations, showcasing the judge sensitivity to prompt details  Sclar et al. ( 2023 ) .",
            "The prompts used for GPT-4 are available on  Figures   7 ,  8 ,  9  and  10 .",
            "Detailed performances of models on GroUSE are available on Figure  13  for closed models, and Figure  14  for open-source models. In these Figures, each square represents the result of one test.",
            "Figure  12  shows a detailed comparison of the results between the Llama-3 8b before and after finetuning.",
            "To ensure the dataset encompassed a wide range of answer qualities, we utilized a diverse set of models to generate answers to the grounded QA statements. However, upon evaluating these answers, we observed certain GPT-4 biases in the distribution of marks: notably, a scarcity of score 2 for  Answer Relevancy , and an overabundance of scores 1 and 5 for  Completeness , as illustrated in the first row of Figure  11 . To avoid propagating these biases in the finetuned model, we kept on predicting answers until the dataset seemed balanced enough, trying to select models with intermediate performances to produce answers of average quality and fill the gaps. The final balanced dataset was built choosing the 1400 answers which best harmonized the metrics among 4k evaluated grounded QA answers, resulting in the distribution shown in the second row of Figure  11 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Example limitations of RAGAS on GroUSE unit tests.  Left:  While the answer contain extra irrelevant but faithful statements, RAGAS wrongly penalizes the answers faithfulness .  Right:  While all the provided information is relevant, RAGAS wrongly penalizes the answer relevancy.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "Table  1  relates the failure modes presented in our work with existing failure modes presented and reported by prior works. To quantify these seven failure modes, we introduce specific evaluation criteria for grounded question answering.  Answer relevancy  assesses the relevance of the information provided in the answer regarding the question, using a Likert scale (1 to 5), which helps to measure  FM1 .  Completeness  also uses a Likert scale to evaluate whether  all  relevant information from the documents is present in the answer, thus measuring  FM3 .  Faithfulness  is a binary score that checks if all facts in the answer are accurate and correctly attributed to the corresponding document, addressing  FM6  and  FM7 . In adversarial cases and when additional information is provided,  Usefulness  is a binary score that determines if the provided additional information is indeed useful and relevant to the question, measuring  FM5 . Usefulness can be considered a form of  soft relevancy  in adversarial cases. Lastly,  Positive Acceptance  and  Negative Rejection  are binary scores indicating a true positive and a true negative respectively in identifying whether the question is answerable, thereby measuring  FM4  and  FM2 . Not all failure modes can occur in all situations: Figure  2  clarifies the conditions under which each metric is defined, depending on whether the references contain an answer, if the answer provides a response, or if it adds related information when it does not provide a direct response.",
            "This subsection highlights the limitations of current automatic implementations, specifically RAGAS and DeepEval. Therefore,  in this subsection only, we will refer to answer relevancy and faithfulness using the RAGAS definitions (see  section   2 ) .",
            "While  Es et al. ( 2023 )  showed that RAGAS metrics correlate with human judgment, our evaluation of their implementations on GroUSE reveals that they do not perform well on many individual tests, as illustrated in  Table   2  and Figure  3 . This observation suggests that correlation on judgement does not necessarily implies good calibration of grades on edge cases and thus good error detection. This hypothesis will be further explored in  section   5.1 . The proposed automatic metrics rely on several sequential LLM calls, which can increase the likelihood of errors and reduce the robustness of the evaluation across samples. Interestingly, different implementations of the same metrics can yield very different results. For instance, although faithfulness is defined similarly in RAGAS and DeepEval, the unit test results differ significantly due to differences in the prompts used in their respective implementations, showcasing the judge sensitivity to prompt details  Sclar et al. ( 2023 ) .",
            "The metrics applicability being highly dependent on whether we are in an adversarial situation, and whether the answer provides a response, a straightforward strategy could involve first identifying which of the situations presented in  Figure   2  corresponds to the sample to evaluate. Identifying this scenario would allow to get the list of defined metrics and launch their evaluations consequently. However, to save on LLM calls, we decided to directly include instructions to set the score at  null  if we are in a situation in which the metric is undefined in the evaluation prompts of the  Answer relevancy  and  Completeness . Based on whether these metrics values are  null , it is easy to deduct the situation in which we are, and infer the value of  Positive Acceptance  and  Negative Rejection  at the same time. A similar strategy is also applied to detect the presence or absence of related information when evaluating the  Usefulness . Ultimately, this optimized pipeline requires at most four LLM calls, with some calls being skipped when the situation is not appropriate (Figure  4 ).",
            "Figure  12  shows a detailed comparison of the results between the Llama-3 8b before and after finetuning."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Percentage of tests passed for various models. The highest score in each column is highlighted in bold.",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "Table  1  relates the failure modes presented in our work with existing failure modes presented and reported by prior works. To quantify these seven failure modes, we introduce specific evaluation criteria for grounded question answering.  Answer relevancy  assesses the relevance of the information provided in the answer regarding the question, using a Likert scale (1 to 5), which helps to measure  FM1 .  Completeness  also uses a Likert scale to evaluate whether  all  relevant information from the documents is present in the answer, thus measuring  FM3 .  Faithfulness  is a binary score that checks if all facts in the answer are accurate and correctly attributed to the corresponding document, addressing  FM6  and  FM7 . In adversarial cases and when additional information is provided,  Usefulness  is a binary score that determines if the provided additional information is indeed useful and relevant to the question, measuring  FM5 . Usefulness can be considered a form of  soft relevancy  in adversarial cases. Lastly,  Positive Acceptance  and  Negative Rejection  are binary scores indicating a true positive and a true negative respectively in identifying whether the question is answerable, thereby measuring  FM4  and  FM2 . Not all failure modes can occur in all situations: Figure  2  clarifies the conditions under which each metric is defined, depending on whether the references contain an answer, if the answer provides a response, or if it adds related information when it does not provide a direct response.",
            "While  Es et al. ( 2023 )  showed that RAGAS metrics correlate with human judgment, our evaluation of their implementations on GroUSE reveals that they do not perform well on many individual tests, as illustrated in  Table   2  and Figure  3 . This observation suggests that correlation on judgement does not necessarily implies good calibration of grades on edge cases and thus good error detection. This hypothesis will be further explored in  section   5.1 . The proposed automatic metrics rely on several sequential LLM calls, which can increase the likelihood of errors and reduce the robustness of the evaluation across samples. Interestingly, different implementations of the same metrics can yield very different results. For instance, although faithfulness is defined similarly in RAGAS and DeepEval, the unit test results differ significantly due to differences in the prompts used in their respective implementations, showcasing the judge sensitivity to prompt details  Sclar et al. ( 2023 ) .",
            "Table  3  shows the performances of various models on GroUSE, across the six metrics, as well as all metrics combined. GPT-4 is the best evaluator, with an overall score of  95 % percent 95 95\\% 95 % , while the best open-source evaluator is Llama-3 70b with a score of  79 % percent 79 79\\% 79 % . The gap between closed and open-source models thus remains wide, with a  15.85 % percent 15.85 15.85\\% 15.85 %  difference on the tests results. Interestingly, Prometheus 2 8x7b does not outperform Mixtral 8x7b, despite Prometheus 2 being specialized in evaluation tasks. Its worth noting however that Prometheus 2 was trained to predict Likert scores ranging from 1 to 5, whereas our evaluation metrics include boolean and nullable scores, which is outside its intended scope.",
            "Table  3  presents the pass rate of various judge models on GroUSE, including Llama-3 8b (zero-shot) and our finetuned Llama-3 8b judge model. Finetuning significantly enhances the evaluation capabilities of Llama-3, as evidenced by the substantial improvement in pass rates. Despite extensive prompt engineering and intermediate CoT reasoning (see Appendix  C ), the non-finetuned Llama-3 8b passes only 40% of the unit tests. However, after finetuning, its pass rate increases to 83%, surpassing all other open-source judges, including Prometheus 2 8x7b (except in the category of faithfulness), despite its smaller size.",
            "Detailed performances of models on GroUSE are available on Figure  13  for closed models, and Figure  14  for open-source models. In these Figures, each square represents the result of one test."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Alignment with the ground truth (GPT-4) evaluations on the test set of 200 samples.",
        "table": "S5.T4.1",
        "footnotes": [],
        "references": [
            "Table  1  relates the failure modes presented in our work with existing failure modes presented and reported by prior works. To quantify these seven failure modes, we introduce specific evaluation criteria for grounded question answering.  Answer relevancy  assesses the relevance of the information provided in the answer regarding the question, using a Likert scale (1 to 5), which helps to measure  FM1 .  Completeness  also uses a Likert scale to evaluate whether  all  relevant information from the documents is present in the answer, thus measuring  FM3 .  Faithfulness  is a binary score that checks if all facts in the answer are accurate and correctly attributed to the corresponding document, addressing  FM6  and  FM7 . In adversarial cases and when additional information is provided,  Usefulness  is a binary score that determines if the provided additional information is indeed useful and relevant to the question, measuring  FM5 . Usefulness can be considered a form of  soft relevancy  in adversarial cases. Lastly,  Positive Acceptance  and  Negative Rejection  are binary scores indicating a true positive and a true negative respectively in identifying whether the question is answerable, thereby measuring  FM4  and  FM2 . Not all failure modes can occur in all situations: Figure  2  clarifies the conditions under which each metric is defined, depending on whether the references contain an answer, if the answer provides a response, or if it adds related information when it does not provide a direct response.",
            "The metrics applicability being highly dependent on whether we are in an adversarial situation, and whether the answer provides a response, a straightforward strategy could involve first identifying which of the situations presented in  Figure   2  corresponds to the sample to evaluate. Identifying this scenario would allow to get the list of defined metrics and launch their evaluations consequently. However, to save on LLM calls, we decided to directly include instructions to set the score at  null  if we are in a situation in which the metric is undefined in the evaluation prompts of the  Answer relevancy  and  Completeness . Based on whether these metrics values are  null , it is easy to deduct the situation in which we are, and infer the value of  Positive Acceptance  and  Negative Rejection  at the same time. A similar strategy is also applied to detect the presence or absence of related information when evaluating the  Usefulness . Ultimately, this optimized pipeline requires at most four LLM calls, with some calls being skipped when the situation is not appropriate (Figure  4 ).",
            "Interestingly, our results reveal a disconnect between the pass rate on GroUSE and the correlation with GPT-4s grades. As shown in Table  4 , Prometheus 2 7b and the finetuned Llama-3 8b exhibit similar correlations with GPT-4s judgments across all metrics. However, when evaluated on GroUSE, the two models show very different pass rates, with the finetuned Llama-3 consistently and significantly outperforming Prometheus 2 7b. Similarly, we observe higher correlation with GPT-4 in answer relevancy and completeness with Prometheus 2 8x7b than with its base model, Mixtral 8x7b, in accordance to what has been observed in  Kim et al. ( 2024 ) . However, this does not translate to better pass rates on the associated metrics on GroUSE: for answer relevancy, Mixtral 8x7b solves 81.25% of the tests versus 61.81% for Prometheus 2 8x7b, despite its intended use on evaluating Likert scores. For completeness, Mixtral 8x7b solves 61.11% of the tests versus 25% for Prometheus 2 8x7b.",
            "Detailed performances of models on GroUSE are available on Figure  13  for closed models, and Figure  14  for open-source models. In these Figures, each square represents the result of one test."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Percentage of tests passed for different prompts. The highest score in each column is highlighted in bold. The completeness base prompt did not involve any chain of thought, so the reported result is the same as with the base prompt for this ablation, as marked by an asterisk.",
        "table": "A3.T5.1",
        "footnotes": [],
        "references": [
            "Table  1  relates the failure modes presented in our work with existing failure modes presented and reported by prior works. To quantify these seven failure modes, we introduce specific evaluation criteria for grounded question answering.  Answer relevancy  assesses the relevance of the information provided in the answer regarding the question, using a Likert scale (1 to 5), which helps to measure  FM1 .  Completeness  also uses a Likert scale to evaluate whether  all  relevant information from the documents is present in the answer, thus measuring  FM3 .  Faithfulness  is a binary score that checks if all facts in the answer are accurate and correctly attributed to the corresponding document, addressing  FM6  and  FM7 . In adversarial cases and when additional information is provided,  Usefulness  is a binary score that determines if the provided additional information is indeed useful and relevant to the question, measuring  FM5 . Usefulness can be considered a form of  soft relevancy  in adversarial cases. Lastly,  Positive Acceptance  and  Negative Rejection  are binary scores indicating a true positive and a true negative respectively in identifying whether the question is answerable, thereby measuring  FM4  and  FM2 . Not all failure modes can occur in all situations: Figure  2  clarifies the conditions under which each metric is defined, depending on whether the references contain an answer, if the answer provides a response, or if it adds related information when it does not provide a direct response.",
            "While  Es et al. ( 2023 )  showed that RAGAS metrics correlate with human judgment, our evaluation of their implementations on GroUSE reveals that they do not perform well on many individual tests, as illustrated in  Table   2  and Figure  3 . This observation suggests that correlation on judgement does not necessarily implies good calibration of grades on edge cases and thus good error detection. This hypothesis will be further explored in  section   5.1 . The proposed automatic metrics rely on several sequential LLM calls, which can increase the likelihood of errors and reduce the robustness of the evaluation across samples. Interestingly, different implementations of the same metrics can yield very different results. For instance, although faithfulness is defined similarly in RAGAS and DeepEval, the unit test results differ significantly due to differences in the prompts used in their respective implementations, showcasing the judge sensitivity to prompt details  Sclar et al. ( 2023 ) .",
            "This finding suggests that a high correlation with GPT-4s judgments does not necessarily translate to a high unit test pass rate. A judge model can share the same  relative preferences  as GPT-4 (indicated by strong rank correlation) while lacking the same  calibration  on precise reference cases (very good answers, subtle mistakes, etc.), resulting in poor performance on judgement unit tests. Figure  5  illustrates this difference with Prometheus 2 7b and the finetuned Llama-3 8b: while Prometheus 2 confusion matrix entries are closer to the diagonal, it features more confusions on extreme cases (1, 5 and NaN cases) when compared to the finetuned Llama-3. On the contrary, the finetuned Llama-3 has better exact agreement with GPT-4 on extreme case, but lacks correlation on intermediate cases.",
            "We conduct an ablation experiment by measuring GPT-4s performance on GroUSE with different prompts: removing the ground truth and having the model rate only one answer, removing the justification field, and removing the chain-of-thought field. The results are shown in Table  5 : the best agreement rates are obtained for the prompt without the justification, nonetheless removing the ground truth or the chain-of-thought lowers the performances.",
            "Although Table  5  indicates that the best results are achieved without a justification, we opted to build the dataset of GPT-4 traces with one. This decision is supported by two main reasons: first,  Mukherjee et al. ( 2023 )  show that a smaller model benefit more from GPT-4s traces if they include explanations of its reasoning. Second, the justification enhances the interpretability of the models responses."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Detailed list of unit test types: their goals, which failure modes they cover, the characteristics of the references and answer, as well as the expected notes. The first seven tests cover no failure modes but check that correct answers get the highest marks possible, in various situations.",
        "table": "A3.T6.96",
        "footnotes": [],
        "references": [
            "Table  1  relates the failure modes presented in our work with existing failure modes presented and reported by prior works. To quantify these seven failure modes, we introduce specific evaluation criteria for grounded question answering.  Answer relevancy  assesses the relevance of the information provided in the answer regarding the question, using a Likert scale (1 to 5), which helps to measure  FM1 .  Completeness  also uses a Likert scale to evaluate whether  all  relevant information from the documents is present in the answer, thus measuring  FM3 .  Faithfulness  is a binary score that checks if all facts in the answer are accurate and correctly attributed to the corresponding document, addressing  FM6  and  FM7 . In adversarial cases and when additional information is provided,  Usefulness  is a binary score that determines if the provided additional information is indeed useful and relevant to the question, measuring  FM5 . Usefulness can be considered a form of  soft relevancy  in adversarial cases. Lastly,  Positive Acceptance  and  Negative Rejection  are binary scores indicating a true positive and a true negative respectively in identifying whether the question is answerable, thereby measuring  FM4  and  FM2 . Not all failure modes can occur in all situations: Figure  2  clarifies the conditions under which each metric is defined, depending on whether the references contain an answer, if the answer provides a response, or if it adds related information when it does not provide a direct response.",
            "A detailed listing of the unit tests, including the expected mark for each test, is available in Table  6 .  The queries of the sets are:",
            "We measured the performances of eleven models on GroUSE, half of them closed-source, the other half open-source. For  GPT-4   Achiam et al. ( 2023 ) , Gemini 1.0 Pro  Team et al. ( 2023 ) , Mixtral 8x7b  Jiang et al. ( 2024 ) , Prometheus 2 7b and Prometheus 2 8x7b  Kim et al. ( 2024 ) , we iterated on the prompts, making our best effort to achieve the best possible results on the training set of GroUSE. These engineered prompts are then used to test the other models: the GPT-4 prompt is used for the whole GPT family and the Mixtral 8x7b prompt is used for Mixtral 8x22b  Team ( 2024 )  and the Llama 3 models  MetaAI ( 2024 ) . To engineer a prompt, we begin with a basic instruction and evaluate how many tests in the training set it passes. We then qualitatively analyze the errors and craft a new prompt aimed at eliminating those errors. This iterative process continues until all tests pass or further progress becomes challenging. The amount of prompt tested for each model is visible Figure  6 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Percentage of tests passed for balanced and unbalanced model. The highest score in each column is highlighted in bold.",
        "table": "A8.T7.1",
        "footnotes": [],
        "references": [
            "Table  1  relates the failure modes presented in our work with existing failure modes presented and reported by prior works. To quantify these seven failure modes, we introduce specific evaluation criteria for grounded question answering.  Answer relevancy  assesses the relevance of the information provided in the answer regarding the question, using a Likert scale (1 to 5), which helps to measure  FM1 .  Completeness  also uses a Likert scale to evaluate whether  all  relevant information from the documents is present in the answer, thus measuring  FM3 .  Faithfulness  is a binary score that checks if all facts in the answer are accurate and correctly attributed to the corresponding document, addressing  FM6  and  FM7 . In adversarial cases and when additional information is provided,  Usefulness  is a binary score that determines if the provided additional information is indeed useful and relevant to the question, measuring  FM5 . Usefulness can be considered a form of  soft relevancy  in adversarial cases. Lastly,  Positive Acceptance  and  Negative Rejection  are binary scores indicating a true positive and a true negative respectively in identifying whether the question is answerable, thereby measuring  FM4  and  FM2 . Not all failure modes can occur in all situations: Figure  2  clarifies the conditions under which each metric is defined, depending on whether the references contain an answer, if the answer provides a response, or if it adds related information when it does not provide a direct response.",
            "The prompts used for GPT-4 are available on  Figures   7 ,  8 ,  9  and  10 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Alignment with the ground truth (GPT-4) evaluations on the test set.",
        "table": "A8.T8.1",
        "footnotes": [],
        "references": [
            "The prompts used for GPT-4 are available on  Figures   7 ,  8 ,  9  and  10 ."
        ]
    },
    "global_footnotes": [
        "This threshold is conservative; the largest difference between annotations from two different annotators on the same sample is 0.125 and on average around 0.05"
    ]
}