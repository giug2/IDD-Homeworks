{
    "Ch4.T1": {
        "caption": "Table 4.1: Results the models achieved on the training set. The highlighted cells are the lowest training loss and highest training average precision score that were achieved by the models that we trained.",
        "table": "<table id=\"Ch4.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Ch4.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"Ch4.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">layers</th>\n<th id=\"Ch4.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">hid_size</th>\n<th id=\"Ch4.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fs_out_size</th>\n<th id=\"Ch4.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fc1_hid_size</th>\n<th id=\"Ch4.T1.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fc2_hid_size</th>\n<th id=\"Ch4.T1.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">lr.</th>\n<th id=\"Ch4.T1.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">train_loss</th>\n<th id=\"Ch4.T1.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">train_aps</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Ch4.T1.1.2.1\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T1.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">32</td>\n<td id=\"Ch4.T1.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.1</td>\n<td id=\"Ch4.T1.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">15.23</td>\n<td id=\"Ch4.T1.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.391</td>\n</tr>\n<tr id=\"Ch4.T1.1.3.2\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T1.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T1.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">32</td>\n<td id=\"Ch4.T1.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.01</td>\n<td id=\"Ch4.T1.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">13.45</td>\n<td id=\"Ch4.T1.1.3.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.392</td>\n</tr>\n<tr id=\"Ch4.T1.1.4.3\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T1.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">32</td>\n<td id=\"Ch4.T1.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.001</td>\n<td id=\"Ch4.T1.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">10.20</td>\n<td id=\"Ch4.T1.1.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.387</td>\n</tr>\n<tr id=\"Ch4.T1.1.5.4\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T1.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T1.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">32</td>\n<td id=\"Ch4.T1.1.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.0001</td>\n<td id=\"Ch4.T1.1.5.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.5241</td>\n<td id=\"Ch4.T1.1.5.4.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.741</td>\n</tr>\n<tr id=\"Ch4.T1.1.6.5\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T1.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T1.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T1.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T1.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T1.1.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.1</td>\n<td id=\"Ch4.T1.1.6.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">14.05</td>\n<td id=\"Ch4.T1.1.6.5.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.393</td>\n</tr>\n<tr id=\"Ch4.T1.1.7.6\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T1.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T1.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T1.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T1.1.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T1.1.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.01</td>\n<td id=\"Ch4.T1.1.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">11.43</td>\n<td id=\"Ch4.T1.1.7.6.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.398</td>\n</tr>\n<tr id=\"Ch4.T1.1.8.7\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T1.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T1.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T1.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T1.1.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T1.1.8.7.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.001</td>\n<td id=\"Ch4.T1.1.8.7.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">10.15</td>\n<td id=\"Ch4.T1.1.8.7.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.413</td>\n</tr>\n<tr id=\"Ch4.T1.1.9.8\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T1.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T1.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T1.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T1.1.9.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T1.1.9.8.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.0001</td>\n<td id=\"Ch4.T1.1.9.8.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"background-color:#FFBFBF;padding-bottom:2.15277pt;\"><span id=\"Ch4.T1.1.9.8.7.1\" class=\"ltx_text\" style=\"background-color:#FFBFBF;\">0.463</span></td>\n<td id=\"Ch4.T1.1.9.8.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"background-color:#FFBFBF;padding-bottom:2.15277pt;\"><span id=\"Ch4.T1.1.9.8.8.1\" class=\"ltx_text\" style=\"background-color:#FFBFBF;\">0.802</span></td>\n</tr>\n<tr id=\"Ch4.T1.1.10.9\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">2</td>\n<td id=\"Ch4.T1.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">512</td>\n<td id=\"Ch4.T1.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1024</td>\n<td id=\"Ch4.T1.1.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">512</td>\n<td id=\"Ch4.T1.1.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1024</td>\n<td id=\"Ch4.T1.1.10.9.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.01</td>\n<td id=\"Ch4.T1.1.10.9.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">14.94</td>\n<td id=\"Ch4.T1.1.10.9.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.3912</td>\n</tr>\n<tr id=\"Ch4.T1.1.11.10\" class=\"ltx_tr\">\n<td id=\"Ch4.T1.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">2</td>\n<td id=\"Ch4.T1.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">512</td>\n<td id=\"Ch4.T1.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1024</td>\n<td id=\"Ch4.T1.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">512</td>\n<td id=\"Ch4.T1.1.11.10.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1024</td>\n<td id=\"Ch4.T1.1.11.10.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.0001</td>\n<td id=\"Ch4.T1.1.11.10.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">6.67</td>\n<td id=\"Ch4.T1.1.11.10.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.454</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The first proposed fusion module consists of a GRU that encodes the description, a pre-trained ResNet that outputs a vector representation of the image and two fully connected layers that transform the image feature representation and description encoding to the same vector space. The outputs of the two fully connected layers are combined by point-wise multiplication. To find the best hyperparameter configuration for the fusion module and Siamese neural network we did a small hyperparameter search by training 10 different models. We explored the number of stacked GRU layers (layers), the size of the GRU cell (hid_size), the size of the two fully connected layers (fs_out_size), the size of the first fully connected layer in the Siamese neural network (fc1_hid_size), the size of the second fully connected layer in the Siamese neural network (fc2_hid_size) and the learning rate (lr.). All models were trained for 30 epochs and as an optimization technique, we used early stopping (we stopped the training loop if after 5 epochs the validation loss didn’t improve). Due to the exponential complexity of doing a more in-depth hyperparameter search, we decided that we are going to train our models with mini-batches of 64 samples and use a fixed random seed so that the weights in the neural networks in our models would always have the same random initialization. By doing this we would be able to compare the results of the models more accurately. The results of the hyperparameter search are shown in Table 4.1 and Table 4.2. Table 4.1 shows the Binary Cross Entropy loss and the Average Precision Score. the model achieved on the training set, while Table 4.2 show the same metrics the model achieved on the validation set. We are more interested in the scores the model achieves on the validation set, because the model hasn’t seen the validation data (hasn’t been trained on it), therefore the scores achieved on the validation set give us a more honest outlook on how the model generalizes and how it will perform in the future on unseen data from the real world.",
            "From the tables, we can see that applying dropout right after the point-wise multiplication didn’t improve the model’s performance. Therefore in this section, we conclude that the best model from these experiments is the model with the 8th configuration from Table 4.1. To get a better estimation of the average precision score on the validation set, we trained two models that had the 8th configuration but we used a different random initialization of the weights of the model. After we trained the models we averaged their validation scores, therefore the final average precision score on the validation set is 0.682±0.003plus-or-minus0.6820.003\\textbf{0.682}\\pm 0.003."
        ]
    },
    "Ch4.T2": {
        "caption": "Table 4.2: Results the models achieved on the validation set. The highlighted cells are the lowest validation loss and highest validation average precision score that were achieved by the models that we trained.",
        "table": "<table id=\"Ch4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Ch4.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"Ch4.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">layers</th>\n<th id=\"Ch4.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">hid_size</th>\n<th id=\"Ch4.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fs_out_size</th>\n<th id=\"Ch4.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fc1_hid_size</th>\n<th id=\"Ch4.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fc2_hid_size</th>\n<th id=\"Ch4.T2.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">lr.</th>\n<th id=\"Ch4.T2.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">val_loss</th>\n<th id=\"Ch4.T2.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">val_aps</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Ch4.T2.1.2.1\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T2.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T2.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T2.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">32</td>\n<td id=\"Ch4.T2.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.1</td>\n<td id=\"Ch4.T2.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">10.83</td>\n<td id=\"Ch4.T2.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.392</td>\n</tr>\n<tr id=\"Ch4.T2.1.3.2\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T2.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T2.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T2.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">32</td>\n<td id=\"Ch4.T2.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.01</td>\n<td id=\"Ch4.T2.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">16.79</td>\n<td id=\"Ch4.T2.1.3.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.392</td>\n</tr>\n<tr id=\"Ch4.T2.1.4.3\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T2.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T2.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T2.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">32</td>\n<td id=\"Ch4.T2.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.001</td>\n<td id=\"Ch4.T2.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">7.56</td>\n<td id=\"Ch4.T2.1.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.375</td>\n</tr>\n<tr id=\"Ch4.T2.1.5.4\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T2.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">100</td>\n<td id=\"Ch4.T2.1.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T2.1.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">32</td>\n<td id=\"Ch4.T2.1.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.0001</td>\n<td id=\"Ch4.T2.1.5.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.581</td>\n<td id=\"Ch4.T2.1.5.4.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.668</td>\n</tr>\n<tr id=\"Ch4.T2.1.6.5\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.6.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T2.1.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T2.1.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T2.1.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T2.1.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T2.1.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.1</td>\n<td id=\"Ch4.T2.1.6.5.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">7.74</td>\n<td id=\"Ch4.T2.1.6.5.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.388</td>\n</tr>\n<tr id=\"Ch4.T2.1.7.6\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.7.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T2.1.7.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T2.1.7.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T2.1.7.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T2.1.7.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T2.1.7.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.01</td>\n<td id=\"Ch4.T2.1.7.6.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">11.12</td>\n<td id=\"Ch4.T2.1.7.6.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.401</td>\n</tr>\n<tr id=\"Ch4.T2.1.8.7\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.8.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T2.1.8.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T2.1.8.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T2.1.8.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T2.1.8.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T2.1.8.7.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.001</td>\n<td id=\"Ch4.T2.1.8.7.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">8.76</td>\n<td id=\"Ch4.T2.1.8.7.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.44</td>\n</tr>\n<tr id=\"Ch4.T2.1.9.8\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.9.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T2.1.9.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T2.1.9.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T2.1.9.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T2.1.9.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T2.1.9.8.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.0001</td>\n<td id=\"Ch4.T2.1.9.8.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"background-color:#FFBFBF;padding-bottom:2.15277pt;\"><span id=\"Ch4.T2.1.9.8.7.1\" class=\"ltx_text\" style=\"background-color:#FFBFBF;\">0.587</span></td>\n<td id=\"Ch4.T2.1.9.8.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"background-color:#FFBFBF;padding-bottom:2.15277pt;\"><span id=\"Ch4.T2.1.9.8.8.1\" class=\"ltx_text\" style=\"background-color:#FFBFBF;\">0.684</span></td>\n</tr>\n<tr id=\"Ch4.T2.1.10.9\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.10.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">2</td>\n<td id=\"Ch4.T2.1.10.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">512</td>\n<td id=\"Ch4.T2.1.10.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1024</td>\n<td id=\"Ch4.T2.1.10.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">512</td>\n<td id=\"Ch4.T2.1.10.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1024</td>\n<td id=\"Ch4.T2.1.10.9.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.01</td>\n<td id=\"Ch4.T2.1.10.9.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">10.83</td>\n<td id=\"Ch4.T2.1.10.9.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.392</td>\n</tr>\n<tr id=\"Ch4.T2.1.11.10\" class=\"ltx_tr\">\n<td id=\"Ch4.T2.1.11.10.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">2</td>\n<td id=\"Ch4.T2.1.11.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">512</td>\n<td id=\"Ch4.T2.1.11.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1024</td>\n<td id=\"Ch4.T2.1.11.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">512</td>\n<td id=\"Ch4.T2.1.11.10.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1024</td>\n<td id=\"Ch4.T2.1.11.10.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.0001</td>\n<td id=\"Ch4.T2.1.11.10.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.653</td>\n<td id=\"Ch4.T2.1.11.10.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.516</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The first proposed fusion module consists of a GRU that encodes the description, a pre-trained ResNet that outputs a vector representation of the image and two fully connected layers that transform the image feature representation and description encoding to the same vector space. The outputs of the two fully connected layers are combined by point-wise multiplication. To find the best hyperparameter configuration for the fusion module and Siamese neural network we did a small hyperparameter search by training 10 different models. We explored the number of stacked GRU layers (layers), the size of the GRU cell (hid_size), the size of the two fully connected layers (fs_out_size), the size of the first fully connected layer in the Siamese neural network (fc1_hid_size), the size of the second fully connected layer in the Siamese neural network (fc2_hid_size) and the learning rate (lr.). All models were trained for 30 epochs and as an optimization technique, we used early stopping (we stopped the training loop if after 5 epochs the validation loss didn’t improve). Due to the exponential complexity of doing a more in-depth hyperparameter search, we decided that we are going to train our models with mini-batches of 64 samples and use a fixed random seed so that the weights in the neural networks in our models would always have the same random initialization. By doing this we would be able to compare the results of the models more accurately. The results of the hyperparameter search are shown in Table 4.1 and Table 4.2. Table 4.1 shows the Binary Cross Entropy loss and the Average Precision Score. the model achieved on the training set, while Table 4.2 show the same metrics the model achieved on the validation set. We are more interested in the scores the model achieves on the validation set, because the model hasn’t seen the validation data (hasn’t been trained on it), therefore the scores achieved on the validation set give us a more honest outlook on how the model generalizes and how it will perform in the future on unseen data from the real world.",
            "The Stacked Attention Network fusion module is an extension to the previously proposed fusion module. Instead of combining the outputs of the GRU and the pre-trained convolutional network by point-wise multiplication, SAN uses the attention mechanism to create a more refined joint representation of the description and the image features. Because of time constraints (one epoch lasted 48 hours) we were able to train only one model with this fusion module. Since the 8th configuration from Table 4.2 performed the best in previous experiments, we decided that our model is going to have the same configuration. The only difference is that the attention layers had the same size as the output of the first fusion module."
        ]
    },
    "Ch4.T3": {
        "caption": "Table 4.3: Results the models achieved on the training set.",
        "table": "<table id=\"Ch4.T3.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Ch4.T3.1.1.1\" class=\"ltx_tr\">\n<th id=\"Ch4.T3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">layers</th>\n<th id=\"Ch4.T3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">hid_size</th>\n<th id=\"Ch4.T3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fs_out_size</th>\n<th id=\"Ch4.T3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fc1_hid_size</th>\n<th id=\"Ch4.T3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fc2_hid_size</th>\n<th id=\"Ch4.T3.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">dr</th>\n<th id=\"Ch4.T3.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">train_loss</th>\n<th id=\"Ch4.T3.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">train_aps</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Ch4.T3.1.2.1\" class=\"ltx_tr\">\n<td id=\"Ch4.T3.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T3.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T3.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T3.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T3.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T3.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.2</td>\n<td id=\"Ch4.T3.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.591</td>\n<td id=\"Ch4.T3.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.647</td>\n</tr>\n<tr id=\"Ch4.T3.1.3.2\" class=\"ltx_tr\">\n<td id=\"Ch4.T3.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T3.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T3.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T3.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T3.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T3.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.3</td>\n<td id=\"Ch4.T3.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.6</td>\n<td id=\"Ch4.T3.1.3.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.63</td>\n</tr>\n<tr id=\"Ch4.T3.1.4.3\" class=\"ltx_tr\">\n<td id=\"Ch4.T3.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T3.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T3.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T3.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T3.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T3.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.5</td>\n<td id=\"Ch4.T3.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.624</td>\n<td id=\"Ch4.T3.1.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.587</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "For the following experiments, we explored three different dropout rates (0.2, 0.3 and 0.5) and fixed the learning rate to 0.0001. The results of the experiments can be seen in Tables 4.3 and 4.4."
        ]
    },
    "Ch4.T4": {
        "caption": "Table 4.4: Results the models achieved on the validation set.",
        "table": "<table id=\"Ch4.T4.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Ch4.T4.1.1.1\" class=\"ltx_tr\">\n<th id=\"Ch4.T4.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">layers</th>\n<th id=\"Ch4.T4.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">hid_size</th>\n<th id=\"Ch4.T4.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fs_out_size</th>\n<th id=\"Ch4.T4.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fc1_hid_size</th>\n<th id=\"Ch4.T4.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">fc2_hid_size</th>\n<th id=\"Ch4.T4.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">dr</th>\n<th id=\"Ch4.T4.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">val_loss</th>\n<th id=\"Ch4.T4.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">val_aps</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Ch4.T4.1.2.1\" class=\"ltx_tr\">\n<td id=\"Ch4.T4.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T4.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T4.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T4.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T4.1.2.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T4.1.2.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.2</td>\n<td id=\"Ch4.T4.1.2.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.586</td>\n<td id=\"Ch4.T4.1.2.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.654</td>\n</tr>\n<tr id=\"Ch4.T4.1.3.2\" class=\"ltx_tr\">\n<td id=\"Ch4.T4.1.3.2.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T4.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T4.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T4.1.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T4.1.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T4.1.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.3</td>\n<td id=\"Ch4.T4.1.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.593</td>\n<td id=\"Ch4.T4.1.3.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.643</td>\n</tr>\n<tr id=\"Ch4.T4.1.4.3\" class=\"ltx_tr\">\n<td id=\"Ch4.T4.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">1</td>\n<td id=\"Ch4.T4.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T4.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">256</td>\n<td id=\"Ch4.T4.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">128</td>\n<td id=\"Ch4.T4.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">64</td>\n<td id=\"Ch4.T4.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.5</td>\n<td id=\"Ch4.T4.1.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.617</td>\n<td id=\"Ch4.T4.1.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.605</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "The first proposed fusion module consists of a GRU that encodes the description, a pre-trained ResNet that outputs a vector representation of the image and two fully connected layers that transform the image feature representation and description encoding to the same vector space. The outputs of the two fully connected layers are combined by point-wise multiplication. To find the best hyperparameter configuration for the fusion module and Siamese neural network we did a small hyperparameter search by training 10 different models. We explored the number of stacked GRU layers (layers), the size of the GRU cell (hid_size), the size of the two fully connected layers (fs_out_size), the size of the first fully connected layer in the Siamese neural network (fc1_hid_size), the size of the second fully connected layer in the Siamese neural network (fc2_hid_size) and the learning rate (lr.). All models were trained for 30 epochs and as an optimization technique, we used early stopping (we stopped the training loop if after 5 epochs the validation loss didn’t improve). Due to the exponential complexity of doing a more in-depth hyperparameter search, we decided that we are going to train our models with mini-batches of 64 samples and use a fixed random seed so that the weights in the neural networks in our models would always have the same random initialization. By doing this we would be able to compare the results of the models more accurately. The results of the hyperparameter search are shown in Table 4.1 and Table 4.2. Table 4.1 shows the Binary Cross Entropy loss and the Average Precision Score. the model achieved on the training set, while Table 4.2 show the same metrics the model achieved on the validation set. We are more interested in the scores the model achieves on the validation set, because the model hasn’t seen the validation data (hasn’t been trained on it), therefore the scores achieved on the validation set give us a more honest outlook on how the model generalizes and how it will perform in the future on unseen data from the real world.",
            "For the following experiments, we explored three different dropout rates (0.2, 0.3 and 0.5) and fixed the learning rate to 0.0001. The results of the experiments can be seen in Tables 4.3 and 4.4.",
            "From the tables, we can see that applying dropout right after the point-wise multiplication didn’t improve the model’s performance. Therefore in this section, we conclude that the best model from these experiments is the model with the 8th configuration from Table 4.1. To get a better estimation of the average precision score on the validation set, we trained two models that had the 8th configuration but we used a different random initialization of the weights of the model. After we trained the models we averaged their validation scores, therefore the final average precision score on the validation set is 0.682±0.003plus-or-minus0.6820.003\\textbf{0.682}\\pm 0.003.",
            "The Stacked Attention Network fusion module is an extension to the previously proposed fusion module. Instead of combining the outputs of the GRU and the pre-trained convolutional network by point-wise multiplication, SAN uses the attention mechanism to create a more refined joint representation of the description and the image features. Because of time constraints (one epoch lasted 48 hours) we were able to train only one model with this fusion module. Since the 8th configuration from Table 4.2 performed the best in previous experiments, we decided that our model is going to have the same configuration. The only difference is that the attention layers had the same size as the output of the first fusion module.",
            "In this section, we compare the three models we have experimented with so far. We analyze them based on the average precision scores that they achieve on the test set, the set that we didn’t use to choose the hyperparameter setting for the models that needed hyperparameter tuning. In Table 4.5 we can see the average precision scores for each proposed solution. The RNN + CNN fusion module along with the Siamese Neural Network significantly outperform the baseline model. The Stacked Attention Network performs worse than the RNN + CNN module and we believe that this is because the model wasn’t trained long enough. Therefore in the further discussion, we will omit the Stacked Attention Network module since it doesn’t provide additional information to what we already have with the RNN + CNN model. As a way to illustrate the difference in performance between the baseline and the RNN + CNN model, we have plotted the Precision-Recall curve for both models in Figure 4.10 and as expected the PR curve of the RNN + CNN model has a much larger area under the curve then the PR curve of the baseline model.",
            "Advertisements tend to have different description lengths, so we wanted to investigate how does the model perform when both descriptions in the pair are short, when the descriptions are both very long and when the descriptions are different lengths. We want to see if the models can predict equally well when the advertisements have different lengths because that would mean that the fusion module can capture crucial similarities between the descriptions even though, one description might contain more information then the other. To do that we computed the average length of each pair of descriptions. The maximum average could be 100 (because we limited our descriptions to 100 words) and the minimum could be 1, so we decided to divide the test set into 11 groups. The first group would be pairs that have an average description length of 1 to 9 words, the second group would have a length of 10 to 19 words, and so on until the last group, which is consisted of pairs that have an average length of 100 words. This is because we cut many descriptions in our preprocessing so, this group represents all the long advertisements. Table 4.6 shows the number of pairs that each group contains and Figure 4.11 shows the class distribution within each bucket."
        ]
    },
    "Ch4.T5": {
        "caption": "Table 4.5: In this table we have presented the Average Precision Score that each model has achieved on the test set.",
        "table": "<table id=\"Ch4.T5.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"Ch4.T5.1.2.1\" class=\"ltx_tr\">\n<th id=\"Ch4.T5.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">Model name</th>\n<th id=\"Ch4.T5.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">Average Precision Score on Test set</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"Ch4.T5.1.3.1\" class=\"ltx_tr\">\n<th id=\"Ch4.T5.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">Baseline model</th>\n<td id=\"Ch4.T5.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.48</td>\n</tr>\n<tr id=\"Ch4.T5.1.1\" class=\"ltx_tr\">\n<th id=\"Ch4.T5.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">RNN + CNN model</th>\n<td id=\"Ch4.T5.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\"><math id=\"Ch4.T5.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"0.683\\pm 0.004\" display=\"inline\"><semantics id=\"Ch4.T5.1.1.1.m1.1a\"><mrow id=\"Ch4.T5.1.1.1.m1.1.1\" xref=\"Ch4.T5.1.1.1.m1.1.1.cmml\"><mn id=\"Ch4.T5.1.1.1.m1.1.1.2\" xref=\"Ch4.T5.1.1.1.m1.1.1.2.cmml\">0.683</mn><mo id=\"Ch4.T5.1.1.1.m1.1.1.1\" xref=\"Ch4.T5.1.1.1.m1.1.1.1.cmml\">±</mo><mn id=\"Ch4.T5.1.1.1.m1.1.1.3\" xref=\"Ch4.T5.1.1.1.m1.1.1.3.cmml\">0.004</mn></mrow><annotation-xml encoding=\"MathML-Content\" id=\"Ch4.T5.1.1.1.m1.1b\"><apply id=\"Ch4.T5.1.1.1.m1.1.1.cmml\" xref=\"Ch4.T5.1.1.1.m1.1.1\"><csymbol cd=\"latexml\" id=\"Ch4.T5.1.1.1.m1.1.1.1.cmml\" xref=\"Ch4.T5.1.1.1.m1.1.1.1\">plus-or-minus</csymbol><cn type=\"float\" id=\"Ch4.T5.1.1.1.m1.1.1.2.cmml\" xref=\"Ch4.T5.1.1.1.m1.1.1.2\">0.683</cn><cn type=\"float\" id=\"Ch4.T5.1.1.1.m1.1.1.3.cmml\" xref=\"Ch4.T5.1.1.1.m1.1.1.3\">0.004</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"Ch4.T5.1.1.1.m1.1c\">0.683\\pm 0.004</annotation></semantics></math></td>\n</tr>\n<tr id=\"Ch4.T5.1.4.2\" class=\"ltx_tr\">\n<th id=\"Ch4.T5.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">SAN model</th>\n<td id=\"Ch4.T5.1.4.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-bottom:2.15277pt;\">0.574</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "In this section, we compare the three models we have experimented with so far. We analyze them based on the average precision scores that they achieve on the test set, the set that we didn’t use to choose the hyperparameter setting for the models that needed hyperparameter tuning. In Table 4.5 we can see the average precision scores for each proposed solution. The RNN + CNN fusion module along with the Siamese Neural Network significantly outperform the baseline model. The Stacked Attention Network performs worse than the RNN + CNN module and we believe that this is because the model wasn’t trained long enough. Therefore in the further discussion, we will omit the Stacked Attention Network module since it doesn’t provide additional information to what we already have with the RNN + CNN model. As a way to illustrate the difference in performance between the baseline and the RNN + CNN model, we have plotted the Precision-Recall curve for both models in Figure 4.10 and as expected the PR curve of the RNN + CNN model has a much larger area under the curve then the PR curve of the baseline model."
        ]
    },
    "Ch4.T6": {
        "caption": "Table 4.6: Distribution of the test set pairs in the buckets of average description length. In the first row show the number of the bucket and in the second row we show how many pairs does the bucket contain.",
        "table": "<table id=\"Ch4.T6.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"Ch4.T6.1.1.1\" class=\"ltx_tr\">\n<td id=\"Ch4.T6.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">0</td>\n<td id=\"Ch4.T6.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1</td>\n<td id=\"Ch4.T6.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">2</td>\n<td id=\"Ch4.T6.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">3</td>\n<td id=\"Ch4.T6.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">4</td>\n<td id=\"Ch4.T6.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5</td>\n<td id=\"Ch4.T6.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">6</td>\n<td id=\"Ch4.T6.1.1.1.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">7</td>\n<td id=\"Ch4.T6.1.1.1.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">8</td>\n<td id=\"Ch4.T6.1.1.1.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">9</td>\n<td id=\"Ch4.T6.1.1.1.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">10</td>\n</tr>\n<tr id=\"Ch4.T6.1.2.2\" class=\"ltx_tr\">\n<td id=\"Ch4.T6.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">36531</td>\n<td id=\"Ch4.T6.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">63573</td>\n<td id=\"Ch4.T6.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">41254</td>\n<td id=\"Ch4.T6.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">27039</td>\n<td id=\"Ch4.T6.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">18984</td>\n<td id=\"Ch4.T6.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">14836</td>\n<td id=\"Ch4.T6.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">11684</td>\n<td id=\"Ch4.T6.1.2.2.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">8925</td>\n<td id=\"Ch4.T6.1.2.2.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">6910</td>\n<td id=\"Ch4.T6.1.2.2.10\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">6144</td>\n<td id=\"Ch4.T6.1.2.2.11\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">23809</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Advertisements tend to have different description lengths, so we wanted to investigate how does the model perform when both descriptions in the pair are short, when the descriptions are both very long and when the descriptions are different lengths. We want to see if the models can predict equally well when the advertisements have different lengths because that would mean that the fusion module can capture crucial similarities between the descriptions even though, one description might contain more information then the other. To do that we computed the average length of each pair of descriptions. The maximum average could be 100 (because we limited our descriptions to 100 words) and the minimum could be 1, so we decided to divide the test set into 11 groups. The first group would be pairs that have an average description length of 1 to 9 words, the second group would have a length of 10 to 19 words, and so on until the last group, which is consisted of pairs that have an average length of 100 words. This is because we cut many descriptions in our preprocessing so, this group represents all the long advertisements. Table 4.6 shows the number of pairs that each group contains and Figure 4.11 shows the class distribution within each bucket."
        ]
    }
}