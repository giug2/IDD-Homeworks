{
    "PAPER'S NUMBER OF TABLES": 1,
    "S3.T1": {
        "caption": "Table 1: Relative benchmark results for the proposed method and baseline models.\nNote that we set â€œSSL w/o ğ’Ÿcâ€‹lâ€‹iâ€‹eâ€‹nâ€‹tsubscriptğ’Ÿğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡\\mathcal{D}_{client}â€ as the baseline\n(100%) to calculate the relative performance for other two methods\nin each column. FSSL is equivalent to the FSSL (4x) in Fig. 2. FSSL consistently outperforms SSL w/o ğ’Ÿcâ€‹lâ€‹iâ€‹eâ€‹nâ€‹tsubscriptğ’Ÿğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡\\mathcal{D}_{client} baseline on all data partitions. The improvement, however, is less compared to SSL w/ ğ’Ÿcâ€‹lâ€‹iâ€‹eâ€‹nâ€‹tsubscriptğ’Ÿğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡\\mathcal{D}_{client} when the ğ’Ÿcâ€‹lâ€‹iâ€‹eâ€‹nâ€‹tsubscriptğ’Ÿğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡\\mathcal{D}_{client} is directly used to train the classifier.\n",
        "table": "",
        "footnotes": "\n\n\n\n\n\nMethods\nğ’Ÿğ’Ÿ\\mathcal{D}â„â„\\mathcal{I}\n\nğ’Ÿğ’Ÿ\\mathcal{D}ğ’°ğ’°\\mathcal{U}\nğ’Ÿğ’Ÿ\\mathcal{D}ğ’¯ğ’¯\\mathcal{T}\n\nAUC (%)\nPrecision (%) at recall rğ‘Ÿr\nAUC (%)\nPrecision (%) at recall rğ‘Ÿr\nAUC (%)\nPrecision (%) at recall rğ‘Ÿr\n\nr=0.7ğ‘Ÿ0.7r=0.7\nr=0.8ğ‘Ÿ0.8r=0.8\nr=0.9ğ‘Ÿ0.9r=0.9\nr=0.7ğ‘Ÿ0.7r=0.7\nr=0.8ğ‘Ÿ0.8r=0.8\nr=0.9ğ‘Ÿ0.9r=0.9\nr=0.7ğ‘Ÿ0.7r=0.7\nr=0.8ğ‘Ÿ0.8r=0.8\nr=0.9ğ‘Ÿ0.9r=0.9\n\nSSL w/o ğ’Ÿcâ€‹lâ€‹iâ€‹eâ€‹nâ€‹tsubscriptğ’Ÿğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡\\mathcal{D}_{client}\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n\nFSSL\n4.55â†‘â†‘\\uparrow\n7.41â†‘â†‘\\uparrow\n10.51â†‘â†‘\\uparrow\n18.56â†‘â†‘\\uparrow\n3.15â†‘â†‘\\uparrow\n2.94â†‘â†‘\\uparrow\n7.56â†‘â†‘\\uparrow\n20.30â†‘â†‘\\uparrow\n4.97â†‘â†‘\\uparrow\n5.72â†‘â†‘\\uparrow\n8.59â†‘â†‘\\uparrow\n18.80â†‘â†‘\\uparrow\n\nSSL w/ ğ’Ÿcâ€‹lâ€‹iâ€‹eâ€‹nâ€‹tsubscriptğ’Ÿğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡\\mathcal{D}_{client}\n13.17â†‘â†‘\\uparrow\n14.01â†‘â†‘\\uparrow\n21.60â†‘â†‘\\uparrow\n39.57â†‘â†‘\\uparrow\n16.96â†‘â†‘\\uparrow\n17.71â†‘â†‘\\uparrow\n22.75â†‘â†‘\\uparrow\n52.36â†‘â†‘\\uparrow\n14.87â†‘â†‘\\uparrow\n13.15â†‘â†‘\\uparrow\n23.39â†‘â†‘\\uparrow\n50.83â†‘â†‘\\uparrow\n\n\n",
        "references": [
            "The results of different models benchmarked on ğ’Ÿâ„subscriptğ’Ÿâ„\\mathcal{D}_{\\mathcal{I}}, ğ’Ÿğ’°subscriptğ’Ÿğ’°\\mathcal{D}_{\\mathcal{U}}, and ğ’Ÿğ’¯subscriptğ’Ÿğ’¯\\mathcal{D}_{\\mathcal{T}} are shown in Table 1. The proposed\nmethod consistently outperforms the baseline model SSL w/o ğ’Ÿcâ€‹lâ€‹iâ€‹eâ€‹nâ€‹tsubscriptğ’Ÿğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡\\mathcal{D}_{client}\nin all benchmarks, whereas SSL w/ ğ’Ÿcâ€‹lâ€‹iâ€‹eâ€‹nâ€‹tsubscriptğ’Ÿğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡\\mathcal{D}_{client} yields the\nbest model performance among all three models.\nAlthough the improvement in the overall AUC brought by FL of representations\nis relatively small, the improvements at high recall regions (e.g., 0.6-0.9) are significant.\nSince high recall regions are of practical production interest, our results show\nclear evidence that fine-tuning acoustic event classification model\nin the post-deployment stage through continual learning of representations\nis feasible. The vastly superior performance from SSL w/ ğ’Ÿcâ€‹lâ€‹iâ€‹eâ€‹nâ€‹tsubscriptğ’Ÿğ‘ğ‘™ğ‘–ğ‘’ğ‘›ğ‘¡\\mathcal{D}_{client}\nthe model indicates that when well-annotated data is accessible to centralized\ncomputing resources, conventional supervised learning is still more\neffective in optimizing model weights with respect to fixed learning\ntargets. However, for particular use cases where sensitive data is\ninvolved and the learning algorithm is not allowed to directly interact\nwith large datasets on the cloud, centralized learning algorithms\nis consequently infeasible. In turn, federated learning may be one\nof the few solutions to the task."
        ]
    }
}