{
    "PAPER'S NUMBER OF TABLES": 5,
    "S3.T1": {
        "caption": "Table 1: Zero-Parameter Ratio w.r.t Target Sparsity Level",
        "table": "<table id=\"S3.T1.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.T1.3.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sparsity Level</span></th>\n<td id=\"S3.T1.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.3.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td id=\"S3.T1.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.3.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td id=\"S3.T1.3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.3.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.30</span></td>\n<td id=\"S3.T1.3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.3.1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.40</span></td>\n<td id=\"S3.T1.3.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T1.3.1.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></td>\n</tr>\n<tr id=\"S3.T1.3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S3.T1.3.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Zero-Param Ratio</span></th>\n<td id=\"S3.T1.3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T1.3.1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.136</span></td>\n<td id=\"S3.T1.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T1.3.1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.264</span></td>\n<td id=\"S3.T1.3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T1.3.1.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.384</span></td>\n<td id=\"S3.T1.3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T1.3.1.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.496</span></td>\n<td id=\"S3.T1.3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T1.3.1.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.60</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We prune all variables in each layer except the 1-D vector variable and the convolution layer which has specific utility and few parameters. The target sparsity level is the unified pruning percentage assigned for each variable. If we adopt the whole block pruning that prunes the entire column on ",
                "W",
                "ð‘Š",
                "W",
                ", it will further zero-out the corresponding row in next variable ",
                "W",
                "â€²",
                "superscript",
                "ð‘Š",
                "â€²",
                "W^{\\prime}",
                " within the feed forward module in the Conformer blocks. The actual zero-parameter ratios of to-be-pruned variables are slightly higher than the target sparsity level. The detailed numbers are listed in Table ",
                "1",
                ".",
                "Figure ",
                "4",
                " shows a rise in WERs on 4 Librispeech data subsets with the increase of sparsity levels. We also observed obvious quality degradation when the sparsity level ",
                ">",
                ">",
                " 30% in all pruning schemes. Concretely, 0.2% absolute increase on WER was observed on both Dev and Test evaluation set in our default setting Weight-based column pruning when the sparsity level reached 0.3. As we mentioned in Section ",
                "2.2",
                ", different ",
                "pruning methods",
                " can be used to estimate the importance of variables. We conduct ablation experiments of different measurements on the Librispeech dataset. Our empirical finding in Table ",
                "2",
                " suggests the weight-based score achieves similar WER as other metrics, while it is also the most communication efficient and stable metric. Thus, we rely on the weight-based score as the importance metric. In terms of the ",
                "pruning patterns",
                ", we compare the WERs of several FP variants at sparsity level 50% as illustrated in Figure ",
                "4",
                ". One can observe that column pruning and especially the half-column pruning consistently outperforms the row-based pruning. Thus, all experiments below adopt weight magnitude as pruning method and whole column as pruning pattern. With the subsequent fine-tuning phase, the pruning schedule has very limited effect on the WER, so we further fix the constant sparsity level as the default setting."
            ]
        ]
    },
    "S3.T2": {
        "caption": "Table 2: WERs of different pruning methods",
        "table": "<table id=\"S3.T2.1.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.2.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Pruning Methods</span></th>\n<td id=\"S3.T2.1.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\" colspan=\"5\"><span id=\"S3.T2.1.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER on Test with sparsity level</span></td>\n</tr>\n<tr id=\"S3.T2.1.1.3.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.3.2.1\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.3.2.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.10</span></td>\n<td id=\"S3.T2.1.1.3.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.3.2.2.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.20</span></td>\n<td id=\"S3.T2.1.1.3.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.3.2.3.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.30</span></td>\n<td id=\"S3.T2.1.1.3.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.3.2.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.40</span></td>\n<td id=\"S3.T2.1.1.3.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.3.2.5.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">0.50</span></td>\n</tr>\n<tr id=\"S3.T2.1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.4.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T2.1.1.4.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">weight based</span></th>\n<td id=\"S3.T2.1.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.1.1.4.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.1</span></td>\n<td id=\"S3.T2.1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.1.1.4.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.2</span></td>\n<td id=\"S3.T2.1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.1.1.4.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.3</span></td>\n<td id=\"S3.T2.1.1.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.1.1.4.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.4</span></td>\n<td id=\"S3.T2.1.1.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T2.1.1.4.3.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.5</span></td>\n</tr>\n<tr id=\"S3.T2.1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.5.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T2.1.1.5.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">gradient based</span></th>\n<td id=\"S3.T2.1.1.5.4.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.5.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.2</span></td>\n<td id=\"S3.T2.1.1.5.4.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.5.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.3</span></td>\n<td id=\"S3.T2.1.1.5.4.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.5.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.3</span></td>\n<td id=\"S3.T2.1.1.5.4.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.5.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.3</span></td>\n<td id=\"S3.T2.1.1.5.4.6\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T2.1.1.5.4.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.4</span></td>\n</tr>\n<tr id=\"S3.T2.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T2.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\">\n<span id=\"S3.T2.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">weight</span><math id=\"S3.T2.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S3.T2.1.1.1.1.m1.1a\"><mo mathsize=\"90%\" id=\"S3.T2.1.1.1.1.m1.1.1\" xref=\"S3.T2.1.1.1.1.m1.1.1.cmml\">Ã—</mo><annotation-xml encoding=\"MathML-Content\" id=\"S3.T2.1.1.1.1.m1.1b\"><times id=\"S3.T2.1.1.1.1.m1.1.1.cmml\" xref=\"S3.T2.1.1.1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S3.T2.1.1.1.1.m1.1c\">\\times</annotation></semantics></math><span id=\"S3.T2.1.1.1.1.2\" class=\"ltx_text\" style=\"font-size:90%;\">gradient based</span>\n</th>\n<td id=\"S3.T2.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.1</span></td>\n<td id=\"S3.T2.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.2</span></td>\n<td id=\"S3.T2.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.2</span></td>\n<td id=\"S3.T2.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.3</span></td>\n<td id=\"S3.T2.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T2.1.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.4</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We prune all variables in each layer except the 1-D vector variable and the convolution layer which has specific utility and few parameters. The target sparsity level is the unified pruning percentage assigned for each variable. If we adopt the whole block pruning that prunes the entire column on ",
                "W",
                "ð‘Š",
                "W",
                ", it will further zero-out the corresponding row in next variable ",
                "W",
                "â€²",
                "superscript",
                "ð‘Š",
                "â€²",
                "W^{\\prime}",
                " within the feed forward module in the Conformer blocks. The actual zero-parameter ratios of to-be-pruned variables are slightly higher than the target sparsity level. The detailed numbers are listed in Table ",
                "1",
                ".",
                "Figure ",
                "4",
                " shows a rise in WERs on 4 Librispeech data subsets with the increase of sparsity levels. We also observed obvious quality degradation when the sparsity level ",
                ">",
                ">",
                " 30% in all pruning schemes. Concretely, 0.2% absolute increase on WER was observed on both Dev and Test evaluation set in our default setting Weight-based column pruning when the sparsity level reached 0.3. As we mentioned in Section ",
                "2.2",
                ", different ",
                "pruning methods",
                " can be used to estimate the importance of variables. We conduct ablation experiments of different measurements on the Librispeech dataset. Our empirical finding in Table ",
                "2",
                " suggests the weight-based score achieves similar WER as other metrics, while it is also the most communication efficient and stable metric. Thus, we rely on the weight-based score as the importance metric. In terms of the ",
                "pruning patterns",
                ", we compare the WERs of several FP variants at sparsity level 50% as illustrated in Figure ",
                "4",
                ". One can observe that column pruning and especially the half-column pruning consistently outperforms the row-based pruning. Thus, all experiments below adopt weight magnitude as pruning method and whole column as pruning pattern. With the subsequent fine-tuning phase, the pruning schedule has very limited effect on the WER, so we further fix the constant sparsity level as the default setting."
            ]
        ]
    },
    "S3.T3": {
        "caption": "Table 3: WERs of unified / adaptive sparsity at Sparsity 50%.",
        "table": "<table id=\"S3.T3.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T3.3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T3.3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Exp.</span></th>\n<th id=\"S3.T3.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span id=\"S3.T3.3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></th>\n</tr>\n<tr id=\"S3.T3.3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T3.3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S3.T3.3.1.2.2.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Test</span></th>\n<th id=\"S3.T3.3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S3.T3.3.1.2.2.2.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TestOther</span></th>\n<th id=\"S3.T3.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S3.T3.3.1.2.2.3.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Dev</span></th>\n<th id=\"S3.T3.3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column\"><span id=\"S3.T3.3.1.2.2.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">DevOther</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.3.1.3.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.3.1.3.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T3.3.1.3.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Baseline</span></th>\n<td id=\"S3.T3.3.1.3.1.2\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.3.1.3.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.1</span></td>\n<td id=\"S3.T3.3.1.3.1.3\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.3.1.3.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.9</span></td>\n<td id=\"S3.T3.3.1.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.3.1.3.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.3</span></td>\n<td id=\"S3.T3.3.1.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_t\"><span id=\"S3.T3.3.1.3.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">4.9</span></td>\n</tr>\n<tr id=\"S3.T3.3.1.4.2\" class=\"ltx_tr\">\n<th id=\"S3.T3.3.1.4.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\"><span id=\"S3.T3.3.1.4.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Unified Sparsity</span></th>\n<td id=\"S3.T3.3.1.4.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.3.1.4.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.5</span></td>\n<td id=\"S3.T3.3.1.4.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.3.1.4.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.9</span></td>\n<td id=\"S3.T3.3.1.4.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.3.1.4.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.8</span></td>\n<td id=\"S3.T3.3.1.4.2.5\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T3.3.1.4.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.9</span></td>\n</tr>\n<tr id=\"S3.T3.3.1.5.3\" class=\"ltx_tr\">\n<th id=\"S3.T3.3.1.5.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S3.T3.3.1.5.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Adaptive Sparsity</span></th>\n<td id=\"S3.T3.3.1.5.3.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.3.1.5.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.4</span></td>\n<td id=\"S3.T3.3.1.5.3.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.3.1.5.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.6</span></td>\n<td id=\"S3.T3.3.1.5.3.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.3.1.5.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.6</span></td>\n<td id=\"S3.T3.3.1.5.3.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T3.3.1.5.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.7</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "We propose the adaptive per-layer sparsity to make our method more flexible and dynamic, which allows reallocation of target pruning budgets among layers. Table ",
                "3",
                " demonstrates that, compared to federated pruning with unified sparsity, the adaptive sparsity achieves lower WERs on all evaluation sets with 50% sparsity level."
            ]
        ]
    },
    "S3.T4": {
        "caption": "Table 4: WERs of w/o and w/ Mask Refinement at Sparsity 40%.",
        "table": "<table id=\"S3.T4.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T4.3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T4.3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" rowspan=\"2\"><span id=\"S3.T4.3.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">Exp.</span></th>\n<th id=\"S3.T4.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" colspan=\"4\"><span id=\"S3.T4.3.1.1.1.2.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:90%;\">WER</span></th>\n</tr>\n<tr id=\"S3.T4.3.1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T4.3.1.2.2.1\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T4.3.1.2.2.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Test</span></td>\n<td id=\"S3.T4.3.1.2.2.2\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T4.3.1.2.2.2.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">TestOther</span></td>\n<td id=\"S3.T4.3.1.2.2.3\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T4.3.1.2.2.3.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">Dev</span></td>\n<td id=\"S3.T4.3.1.2.2.4\" class=\"ltx_td ltx_align_center\"><span id=\"S3.T4.3.1.2.2.4.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:90%;\">DevOther</span></td>\n</tr>\n<tr id=\"S3.T4.3.1.3.3\" class=\"ltx_tr\">\n<th id=\"S3.T4.3.1.3.3.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\"><span id=\"S3.T4.3.1.3.3.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">w/o Mask Refine</span></th>\n<th id=\"S3.T4.3.1.3.3.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.3.1.3.3.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.3</span></th>\n<th id=\"S3.T4.3.1.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.3.1.3.3.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.5</span></th>\n<th id=\"S3.T4.3.1.3.3.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.3.1.3.3.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.6</span></th>\n<th id=\"S3.T4.3.1.3.3.5\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\"><span id=\"S3.T4.3.1.3.3.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.7</span></th>\n</tr>\n<tr id=\"S3.T4.3.1.4.4\" class=\"ltx_tr\">\n<th id=\"S3.T4.3.1.4.4.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\"><span id=\"S3.T4.3.1.4.4.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">w/ Mask Refine</span></th>\n<td id=\"S3.T4.3.1.4.4.2\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T4.3.1.4.4.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.3</span></td>\n<td id=\"S3.T4.3.1.4.4.3\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T4.3.1.4.4.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.3</span></td>\n<td id=\"S3.T4.3.1.4.4.4\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T4.3.1.4.4.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.5</span></td>\n<td id=\"S3.T4.3.1.4.4.5\" class=\"ltx_td ltx_align_center ltx_border_bb\"><span id=\"S3.T4.3.1.4.4.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">5.3</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "To actually reduce the model size,\nthe masked regions on server model are eventually zero-out by FP. On the other hand, the ",
                "Mask Refinement",
                " phase introduced in Section ",
                "2.3",
                " maintains the original values for masked regions. The pruned variables are allowed to grow back, leading to higher flexibility and thus, lower WERs as shown in Table ",
                "4",
                "."
            ]
        ]
    },
    "S3.T5": {
        "caption": "Table 5: WERs of federated pruning on the voice search dataset.",
        "table": "<table id=\"S3.T5.3.1\" class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T5.3.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T5.3.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt\"><span id=\"S3.T5.3.1.1.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">Sparsity Level</span></th>\n<td id=\"S3.T5.3.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T5.3.1.1.1.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.0</span></td>\n<td id=\"S3.T5.3.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T5.3.1.1.1.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.10</span></td>\n<td id=\"S3.T5.3.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T5.3.1.1.1.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.20</span></td>\n<td id=\"S3.T5.3.1.1.1.5\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T5.3.1.1.1.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.30</span></td>\n<td id=\"S3.T5.3.1.1.1.6\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T5.3.1.1.1.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.40</span></td>\n<td id=\"S3.T5.3.1.1.1.7\" class=\"ltx_td ltx_align_center ltx_border_tt\"><span id=\"S3.T5.3.1.1.1.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">0.50</span></td>\n</tr>\n<tr id=\"S3.T5.3.1.2.2\" class=\"ltx_tr\">\n<th id=\"S3.T5.3.1.2.2.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"><span id=\"S3.T5.3.1.2.2.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">WER</span></th>\n<td id=\"S3.T5.3.1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T5.3.1.2.2.2.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.4</span></td>\n<td id=\"S3.T5.3.1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T5.3.1.2.2.3.1\" class=\"ltx_text\" style=\"font-size:90%;\">6.7</span></td>\n<td id=\"S3.T5.3.1.2.2.4\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T5.3.1.2.2.4.1\" class=\"ltx_text\" style=\"font-size:90%;\">7.0</span></td>\n<td id=\"S3.T5.3.1.2.2.5\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T5.3.1.2.2.5.1\" class=\"ltx_text\" style=\"font-size:90%;\">7.4</span></td>\n<td id=\"S3.T5.3.1.2.2.6\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T5.3.1.2.2.6.1\" class=\"ltx_text\" style=\"font-size:90%;\">7.9</span></td>\n<td id=\"S3.T5.3.1.2.2.7\" class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_t\"><span id=\"S3.T5.3.1.2.2.7.1\" class=\"ltx_text\" style=\"font-size:90%;\">8.9</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "Finally, we show the results on the large-scale short-form multi-domain dataset. The reduced model is trained on our multi-domain utterances and evaluated on the short-form dataset. Table ",
                "5",
                " demonstrates the WERs on different sparsity levels. We conclude that our model can still achieve comparable performance to the baseline model (with sparsity level ",
                "0.0",
                "0.0",
                "0.0",
                ") on challenging dataset in the low sparsity level setting."
            ]
        ]
    }
}