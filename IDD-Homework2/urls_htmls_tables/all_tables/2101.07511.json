{
    "PAPER'S NUMBER OF TABLES": 3,
    "S3.T1": {
        "caption": "TABLE I: The distribution of training and testing data of X-ray and Ultrasound datasets over different classes.",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Data</th>\n<th id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t\">Class</th>\n<th id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Training Data (80%)</th>\n<th id=\"S3.T1.1.1.1.4\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Test Data (20%)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.2.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S3.T1.1.2.1.1.1\" class=\"ltx_text\">X-ray</span></th>\n<th id=\"S3.T1.1.2.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">COVID-19</th>\n<td id=\"S3.T1.1.2.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">179</td>\n<td id=\"S3.T1.1.2.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">44</td>\n</tr>\n<tr id=\"S3.T1.1.3.2\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Healthy</th>\n<td id=\"S3.T1.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">1072</td>\n<td id=\"S3.T1.1.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">269</td>\n</tr>\n<tr id=\"S3.T1.1.4.3\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"3\"><span id=\"S3.T1.1.4.3.1.1\" class=\"ltx_text\">Ultrasound</span></th>\n<th id=\"S3.T1.1.4.3.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">COVID-19</th>\n<td id=\"S3.T1.1.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">319</td>\n<td id=\"S3.T1.1.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">80</td>\n</tr>\n<tr id=\"S3.T1.1.5.4\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">Healthy</th>\n<td id=\"S3.T1.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">116</td>\n<td id=\"S3.T1.1.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">30</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "For this study, two datasets from different sources one containing chest X-ray [10] and chest ultrasound images [13], are used. We formulated the problem as binary classification, i.e., differentiating between COVID-19 chest images and normal chest images. Each dataset is divided into two parts, i.e., a training set and a testing set using a split of 80% and 20%, respectively. The training portion (i.e., 80%) of each dataset is further divided into different parts, depending upon the number of clients in that cluster. The distribution of training and testing data of X-ray and Ultrasound datasets over different classes is shown in Table I. Moreover, the testing sets from both datasets are merged to develop a joint testing set that will be used by the server for the evaluation of the performance of a shared model that is being trained in a collaborative fashion using CFL.",
            "We further note that the datasets used in this study have inter and intra class variability in terms of image size and quality, contrast and brightness level, and positioning of subjects, an example is shown in Figure 3. This is not surprising as these publicly available databases are not standard datasets for COVID-19 detection, and have been curated from different sources [48]. Moreover, it is evident from Table I that these datasets are highly imbalanced. These limitations make the training of a generalized model more difficult."
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Parameters of clustered federated learning (CFL) experiments.",
        "table": "<table id=\"S4.T2.1\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.1.2.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.2.1.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S4.T2.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Parameter (s)</span></th>\n<td id=\"S4.T2.1.2.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S4.T2.1.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Value (s)</span></td>\n</tr>\n<tr id=\"S4.T2.1.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Communication Rounds</th>\n<td id=\"S4.T2.1.3.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">30, 50, &amp; 100</td>\n</tr>\n<tr id=\"S4.T2.1.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Epochs</th>\n<td id=\"S4.T2.1.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">5 &amp; 10</td>\n</tr>\n<tr id=\"S4.T2.1.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\">Batch Size</th>\n<td id=\"S4.T2.1.5.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16 &amp; 32</td>\n</tr>\n<tr id=\"S4.T2.1.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.1.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Learning Rate</th>\n<td id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><math id=\"S4.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"1e^{-3}\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.m1.1a\"><mrow id=\"S4.T2.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.cmml\"><mn id=\"S4.T2.1.1.1.m1.1.1.2\" xref=\"S4.T2.1.1.1.m1.1.1.2.cmml\">1</mn><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T2.1.1.1.m1.1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.1.cmml\">‚Äã</mo><msup id=\"S4.T2.1.1.1.m1.1.1.3\" xref=\"S4.T2.1.1.1.m1.1.1.3.cmml\"><mi id=\"S4.T2.1.1.1.m1.1.1.3.2\" xref=\"S4.T2.1.1.1.m1.1.1.3.2.cmml\">e</mi><mrow id=\"S4.T2.1.1.1.m1.1.1.3.3\" xref=\"S4.T2.1.1.1.m1.1.1.3.3.cmml\"><mo id=\"S4.T2.1.1.1.m1.1.1.3.3a\" xref=\"S4.T2.1.1.1.m1.1.1.3.3.cmml\">‚àí</mo><mn id=\"S4.T2.1.1.1.m1.1.1.3.3.2\" xref=\"S4.T2.1.1.1.m1.1.1.3.3.2.cmml\">3</mn></mrow></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.m1.1b\"><apply id=\"S4.T2.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1\"><times id=\"S4.T2.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.1\"></times><cn type=\"integer\" id=\"S4.T2.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.2\">1</cn><apply id=\"S4.T2.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T2.1.1.1.m1.1.1.3.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3\">superscript</csymbol><ci id=\"S4.T2.1.1.1.m1.1.1.3.2.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3.2\">ùëí</ci><apply id=\"S4.T2.1.1.1.m1.1.1.3.3.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3.3\"><minus id=\"S4.T2.1.1.1.m1.1.1.3.3.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3.3\"></minus><cn type=\"integer\" id=\"S4.T2.1.1.1.m1.1.1.3.3.2.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3.3.2\">3</cn></apply></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.m1.1c\">1e^{-3}</annotation></semantics></math></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "In order to show the effectiveness of the proposed multi-modal collaborative learning framework for COVID-19 diagnosis, we performed several experiments. On one side, we aim to evaluate and compare the performances of CFL against two baselines, namely (i) specialized FL baseline, and the (ii) multi-modal222By the term multi-modal we mean images acquired using different imagining techniques, i.e., modalities (e.g., X-ray and Ultrasound). conventional FL. Since CFL aims to tackle the convergence issues of conventional FL schemes due to the diverse distribution of the data, the two baselines, we believe, are appropriate options as a comparison benchmark instead of the state of the art methods for COVID-19 diagnosis. We note that due to the limitations of the dataset, we only consider the divergence in distribution of the data in terms of the nature of the data (i.e., the distribution of ultrasound and X-ray images is different). The first baseline shows the best-case scenario, where separated models for each type of imagery, which we termed as specialized models, are trained in a FL environment. The individual models are trained on X-ray and Ultrasound images with a learning rate of 0.00010.00010.0001 and a batch size of 323232 resulting into two separate models one for each modality (i.e., X-ray and Ultrasound). The second baseline represents the experimental setup of a conventional FL environment, where the data is distributed among different clients, and a shared ML model is built in a federated environment. The parameters used in different experiments can be found in Table II."
        ]
    },
    "S4.T3": {
        "caption": "TABLE III: COMPARISON AGAINST THE TWO BASELINES IN TERMS OF PRECISION, RECALL, AND F1-SCORE. Promising results are obtained by CFL, outperforming the conventional FL while slightly lower performance is obtained compared to Central Baseline with the added advantage of improved privacy and data security. ‚àóA separate model is trained in federated learning settings for each modality.",
        "table": "<table id=\"S4.T3.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T3.3.1\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.1.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.3.1.2.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></th>\n<th id=\"S4.T3.3.1.3\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.3.1.3.1\" class=\"ltx_text ltx_font_bold\">Class</span></th>\n<td id=\"S4.T3.3.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S4.T3.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Federated Learning (Specialized<sup id=\"S4.T3.3.1.1.1.1\" class=\"ltx_sup\"><span id=\"S4.T3.3.1.1.1.1.1\" class=\"ltx_text ltx_font_medium\">‚àó</span></sup>)</span></td>\n<td id=\"S4.T3.3.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S4.T3.3.1.4.1\" class=\"ltx_text ltx_font_bold\">Federated Learning (multi-modal)</span></td>\n<td id=\"S4.T3.3.1.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" colspan=\"3\"><span id=\"S4.T3.3.1.5.1\" class=\"ltx_text ltx_font_bold\">Clustered FL (multi-modal)</span></td>\n</tr>\n<tr id=\"S4.T3.3.2.1\" class=\"ltx_tr\">\n<td id=\"S4.T3.3.2.1.1\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.3.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Precision</span></td>\n<td id=\"S4.T3.3.2.1.2\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.3.2.1.2.1\" class=\"ltx_text ltx_font_bold\">Recall</span></td>\n<td id=\"S4.T3.3.2.1.3\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.3.2.1.3.1\" class=\"ltx_text ltx_font_bold\">F1-Score</span></td>\n<td id=\"S4.T3.3.2.1.4\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.3.2.1.4.1\" class=\"ltx_text ltx_font_bold\">Precision</span></td>\n<td id=\"S4.T3.3.2.1.5\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.3.2.1.5.1\" class=\"ltx_text ltx_font_bold\">Recall</span></td>\n<td id=\"S4.T3.3.2.1.6\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.3.2.1.6.1\" class=\"ltx_text ltx_font_bold\">F1-Score</span></td>\n<td id=\"S4.T3.3.2.1.7\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.3.2.1.7.1\" class=\"ltx_text ltx_font_bold\">Precision</span></td>\n<td id=\"S4.T3.3.2.1.8\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.3.2.1.8.1\" class=\"ltx_text ltx_font_bold\">Recall</span></td>\n<td id=\"S4.T3.3.2.1.9\" class=\"ltx_td ltx_align_left ltx_border_r ltx_border_t\"><span id=\"S4.T3.3.2.1.9.1\" class=\"ltx_text ltx_font_bold\">F1-Score</span></td>\n</tr>\n<tr id=\"S4.T3.3.3.2\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.3.2.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.3.3.2.1.1\" class=\"ltx_text ltx_font_bold\">X-ray</span></th>\n<th id=\"S4.T3.3.3.2.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">COVID-19</th>\n<td id=\"S4.T3.3.3.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.73</td>\n<td id=\"S4.T3.3.3.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.82</td>\n<td id=\"S4.T3.3.3.2.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.77</td>\n<td id=\"S4.T3.3.3.2.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.30</td>\n<td id=\"S4.T3.3.3.2.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.68</td>\n<td id=\"S4.T3.3.3.2.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.41</td>\n<td id=\"S4.T3.3.3.2.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.71</td>\n<td id=\"S4.T3.3.3.2.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.82</td>\n<td id=\"S4.T3.3.3.2.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.76</td>\n</tr>\n<tr id=\"S4.T3.3.4.3\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.4.3.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">Healthy</th>\n<td id=\"S4.T3.3.4.3.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.97</td>\n<td id=\"S4.T3.3.4.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.95</td>\n<td id=\"S4.T3.3.4.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.96</td>\n<td id=\"S4.T3.3.4.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.93</td>\n<td id=\"S4.T3.3.4.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.74</td>\n<td id=\"S4.T3.3.4.3.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.82</td>\n<td id=\"S4.T3.3.4.3.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.97</td>\n<td id=\"S4.T3.3.4.3.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.94</td>\n<td id=\"S4.T3.3.4.3.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.96</td>\n</tr>\n<tr id=\"S4.T3.3.5.4\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.5.4.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" rowspan=\"2\"><span id=\"S4.T3.3.5.4.1.1\" class=\"ltx_text ltx_font_bold\">Ultrasound</span></th>\n<th id=\"S4.T3.3.5.4.2\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t\">COVID-19</th>\n<td id=\"S4.T3.3.5.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.97</td>\n<td id=\"S4.T3.3.5.4.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.95</td>\n<td id=\"S4.T3.3.5.4.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.97</td>\n<td id=\"S4.T3.3.5.4.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.94</td>\n<td id=\"S4.T3.3.5.4.7\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.76</td>\n<td id=\"S4.T3.3.5.4.8\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.84</td>\n<td id=\"S4.T3.3.5.4.9\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.93</td>\n<td id=\"S4.T3.3.5.4.10\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.95</td>\n<td id=\"S4.T3.3.5.4.11\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">0.94</td>\n</tr>\n<tr id=\"S4.T3.3.6.5\" class=\"ltx_tr\">\n<th id=\"S4.T3.3.6.5.1\" class=\"ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t\">Healthy</th>\n<td id=\"S4.T3.3.6.5.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.88</td>\n<td id=\"S4.T3.3.6.5.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.93</td>\n<td id=\"S4.T3.3.6.5.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.90</td>\n<td id=\"S4.T3.3.6.5.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.58</td>\n<td id=\"S4.T3.3.6.5.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.87</td>\n<td id=\"S4.T3.3.6.5.7\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.69</td>\n<td id=\"S4.T3.3.6.5.8\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.86</td>\n<td id=\"S4.T3.3.6.5.9\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.80</td>\n<td id=\"S4.T3.3.6.5.10\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">0.83</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            "Table III and Figure 4 provides the experimental results per class and overall (per dataset) results, respectively, in terms of precision, recall, and F1-Score. Since the data set is not balanced, so we believe, alone accuracy is not enough to evaluate the proposed method. For performance evaluation of the three experimental setups (i.e., the two baselines and CFL), we kept the similar experimental setup where we first train the baseline models with a batch size of 16 (for each modality) and then we train the same model in CFL fashion (i.e., using multi-modal settings) with 5 epochs of local training with a batch size of 16. Then we evaluated the collaboratively trained model with the test data from each cluster (modality), i.e., X-ray and Ultrasound. As can be seen in the Figure 4, overall comparable results are observed for multi-modal model trained using CFL compared with the specialized two models trained in a conventional FL environment using X-ray and Ultrasound imagery separately. On the other hand, we can see that CFL performance is considerably better than the performance of multi-modal model trained in a conventional federated learning environment. Moreover, it is evident from the figure that a collaboratively trained model is capable of recognizing the test of images from different modalities without having explicit knowledge about these modalities. Moreover, overall better results are obtained on ultrasound images (Figure 4(b)) compared to X-ray imagery (Figure 4(a)) for all models."
        ]
    }
}