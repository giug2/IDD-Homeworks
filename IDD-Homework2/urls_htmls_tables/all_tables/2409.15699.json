{
    "id_table_1": {
        "caption": "Table 1.  Cost-effectiveness Analysis on LMQA and ODQA (FlexRAG w. SC. stands for FlexRAG with selective compression).",
        "table": "S4.T1.8.8",
        "footnotes": [],
        "references": [
            "To address the above challenges, we propose a novel approach in this paper, called  FlexRAG  (shown in Figure  1 . 4). It transforms the retrieved contexts into compact and more usable forms, which substantially improves the cost-effectiveness of RAG systems.",
            "We first evaluate the primary question answering performance under the default setting, where a uniform compression ratio (CP. Ratio) of 8  \\times   is applied. For LMQA, all retrieved contexts are confined within 32K tokens, allowing them to be fully utilized by the downstream LLM after compression. In contrast, Llama (w. retrieval) must truncate the retrieved contexts to fit within the 4K window of Llama-2, as implemented by Longbench  (Bai et al . ,  2023 ) . We compare two variants of our method: FlexRAG w/o SC, which disables selective compression and uniformly down-samples the retrieved contexts at an interval of 8 tokens, and FlexRAG w. SC, the default method using selective compression. The following observations can be drawn from the experiment results in Table  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2.  Efficiency analysis using CUDA Time and TFLOPs. Compression ratios are varied from 2  \\times   to 16  \\times  .  Experiments are performed on ODQA with EM as the quality metric.",
        "table": "S4.T2.5.5",
        "footnotes": [],
        "references": [
            "The workflow of FlexRAG consists of the following steps (Figure  2 ). First, the retrieved context is tokenized and jointly encoded as a sequence of embeddings, denoted as  E r  e  t  r subscript E r e t r \\mathbf{E}_{retr} bold_E start_POSTSUBSCRIPT italic_r italic_e italic_t italic_r end_POSTSUBSCRIPT , using a specialized context encoder    (  )   \\psi(\\cdot) italic_ (  ) :  E r  e  t  r    ( X r  e  t  r )  subscript E r e t r  subscript X r e t r \\mathbf{E}_{retr}\\leftarrow\\psi(X_{retr}) bold_E start_POSTSUBSCRIPT italic_r italic_e italic_t italic_r end_POSTSUBSCRIPT  italic_ ( italic_X start_POSTSUBSCRIPT italic_r italic_e italic_t italic_r end_POSTSUBSCRIPT ) . Next, the well-encoded embeddings  E r  e  t  r subscript E r e t r \\mathbf{E}_{retr} bold_E start_POSTSUBSCRIPT italic_r italic_e italic_t italic_r end_POSTSUBSCRIPT  are down-scaled by a sampling function    (  )   \\gamma(\\cdot) italic_ (  ) :",
            "As shown in Table  2 , FlexRAG is faster than Llama (w.r.) while improving the RAGs performance simultaneously. With the increasing of compression ratio, FlexRAG becomes even faster, leading to 3.54  \\times   reduction in CUDA time and 3.09  \\times   reduction in TFLOPs at a 16  \\times   compression ratio. Moreover, FlexRAG substantially improves upon Llama (w.r.) in terms of RAG quality, achieving a 10% improvement in ODQA datasets at a 4  \\times   compression ratio."
        ]
    },
    "id_table_3": {
        "caption": "Table 3.  Flexibility analysis on LMQA and ODQA using different compression ratios (1  \\times  , 2  \\times  , 4  \\times  , 8  \\times  ).",
        "table": "S4.T3.7.7",
        "footnotes": [],
        "references": [
            "We make evaluation of four compression ratios:  1  1\\times 1   (a special case without down-sampling),  2  2\\times 2  ,  4  4\\times 4  ,  8  8\\times 8  . Both FlexRAG w. SC and FlexRAG w/o SC are included in our experiment, allowing us to evaluate the impact where selective compression is enabled or disabled. The experiments are performed on both LMQA and ODQA datasets. For LMQA, FlexRAGs compressed contexts may still exceed Llama-2s window size in some cases (unless  8  8\\times 8  ); as a result, truncation will be used when it has to. We utilize Llama (w/o retrieval) and Llama (w. retrieval) as the baselines for comparison. The experiment results are shown in Table  3 , where the following observations can be made.",
            "We further investigate the effect of the number of retrieved documents. In our experiment, we vary the number of retrieved contexts from the top 1 to the top 10 documents returned by the retriever. The results, shown in Figure  3 , lead to the following observations. First, our method (FlexRAG) consistently outperforms the baseline (Llama w. retrieval), and it demonstrates a greater stability, indicating that FlexRAG effectively handles variations in the number of retrieved documents. Second, FlexRAGs performance improves as the number of retrieved documents increases from 1 to 5, knowing that more useful information can be continually introduced. However, beyond this threshold, no further benefits are observed. According to previous studies  (Yoran et al . ,  2023 ) , this plateau can be attributed to increased noise from irrelevant documents. Notably, FlexRAG experiences a much smaller performance decline compared to the baseline, suggesting it is more robust to noise."
        ]
    },
    "id_table_4": {
        "caption": "Table 4.  Flexibility Analysis on LMQA using different importance estimators and compression ratios (HP/LP: high-priority / low-priority contexts. In our experiment, the top 1/16 (1:16) of the retrieved contexts are allocated with the high priority).",
        "table": "S4.T4.17.17",
        "footnotes": [],
        "references": [
            "We make exploration of three alternative compression methods in our experiment. 1) Token w/o SC: this method is identical to FlexRAG w/o SC, operating at the token level with uniform down-sampling at an interval of 8 tokens. 2) Token w. SC: this method performs selective compression through token-level down-sampling, utilizing the likelihood-based importance estimator described in Eq.  4 . 3) Sentence w. SC: this method performs selective compression via sentence-level down-sampling, employing the embedding-based importance estimator as outlined in Eq.  5 . We make further variation for the compression ratio: with an uniform overall compression ratio  8  8\\times 8   and high-priority proportion 1:16, we use two alternative sets of compression ratios for the high-priority (HP.) and low-priority (LP.) group. One is (HP:  1  1\\times 1  , LP:  16  16\\times 16  ), also the default setting in our experiment; the other one is (HP:  2  2\\times 2  , LP:  11  11\\times 11  ). The following observations can be made from the experiment results in Table  4 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5.  Average performance on ODQA datasets with  various retrievers.",
        "table": "S4.T5.1.1",
        "footnotes": [],
        "references": [
            "We make exploration of three alternative compression methods in our experiment. 1) Token w/o SC: this method is identical to FlexRAG w/o SC, operating at the token level with uniform down-sampling at an interval of 8 tokens. 2) Token w. SC: this method performs selective compression through token-level down-sampling, utilizing the likelihood-based importance estimator described in Eq.  4 . 3) Sentence w. SC: this method performs selective compression via sentence-level down-sampling, employing the embedding-based importance estimator as outlined in Eq.  5 . We make further variation for the compression ratio: with an uniform overall compression ratio  8  8\\times 8   and high-priority proportion 1:16, we use two alternative sets of compression ratios for the high-priority (HP.) and low-priority (LP.) group. One is (HP:  1  1\\times 1  , LP:  16  16\\times 16  ), also the default setting in our experiment; the other one is (HP:  2  2\\times 2  , LP:  11  11\\times 11  ). The following observations can be made from the experiment results in Table  4 .",
            "We first analyze the impact of using different retrievers. Beyond BGE-large, which was applied for both training and testing, we explore several alternative retrievers for testing, including BM25  (Robertson et al . ,  2009 ) , LLM-Embedder  (Zhang et al . ,  2023 ) , BGE-base  (Xiao et al . ,  2023 ) , and E5-large  (Wang et al . ,  2022 ) . The results are presented in Table  5 , leading to the following key observations. First, FlexRAG consistently outperforms llama (w. retrieval) across all retrievers tested. Notably, this includes not only BGE-large, but also other retrievers differ from the one employed during training, demonstrating the strong generalizability of FlexRAG. Second, FlexRAG already achieves a superior performance even with a relatively weaker retriever, e.g., BM25. Additionally, its performance improves further when paired with stronger retrievers, like LLM-Embedder and E5-large."
        ]
    },
    "id_table_6": {
        "caption": "Table 6.  Ablation studies of FlexRAG on LMQA and ODQA datasets. Default settings are marked with *.",
        "table": "S4.T6.1.1",
        "footnotes": [],
        "references": [
            "We first examine the significance of the two-stage training paradigm. In addition to the default method where both training stages are applied, we assess the impact of using only one of the stages: either w/o pre-training (i.e., RAG fine-tuning only) or w/o fine-tuning (i.e., pre-training only). The results, presented in Table  6 , indicate that pre-training with unlabeled data (w/o fine-tuning) significantly boosts FlexRAGs performance, as this stage alone already delivers competitive results. The subsequent RAG fine-tuning further enhances the performance, with the default two-stage method achieving the best results in the experiment.",
            "Next, we evaluate the architecture of the compressive encoder, with three alternatives tested: the first 4 layers of Llama-2, the first 8 layers of Llama-2 (default), and the first 12 layers of Llama-2. As shown in Table  6 , the best performance is achieved when the encoder uses the first 8 layers of Llama-2 as its backbone. In comparison, the smaller encoder (first 4 layers) is constrained by its limited expressiveness, while the larger encoder (first 12 layers) introduces greater disparity with the input layer of the downstream LLM. Therefore, selecting the appropriate encoder architecture requires balancing these considerations."
        ]
    },
    "global_footnotes": []
}