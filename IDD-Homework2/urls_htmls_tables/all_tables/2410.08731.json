{
    "id_table_1": {
        "caption": "Table 1:  Data statistics. The GPT-4o tokenizer was used.",
        "table": "S3.T1.7",
        "footnotes": [],
        "references": [
            "Son et al. ( 2024a )  developed KMMLU, a dataset similar to MMLU Hendrycks et al. ( 2021 )  tailored for Korean and cultural contexts.  It includes diverse language understanding tasks along with four legal AI tasks named  criminal-law, law, patent , and  taxation , sourced from the Public Service Aptitude Test and various Korean license exams. 3 3 3 The specific origins of the datasets are not described in the paper.  Many examples pertain to licensure exams such as the Petitioner Police exam, the Real Estate Brokers exam, and the Occupational Safety and Health exam, or cover topics in social science theory and legal terminology.  These tasks are generally less challenging than those found in the bar exam.  In contrast,  kbl  consists of questions from the Korean bar exam and the newly created questions designed to assess comprehensive legal knowledge and in-depth reasoning capability relevant to the practice of law.  Our work also differs in that it exclusively focus on the legal domains, and we have collaborated with legal professionals to develop a pragmatic and verified benchmark.  Additionally, we also assess LLMs in scenarios where they can utilize legal documents.  We have also ensured that there is no overlap between the legal task examples in KMMLU and our dataset by conducting fuzzy matching between the two datasets, with the most significant matches displayed in Tables  13  16  in Appendix.",
            "The Korean legal system, rooted in civil law, incorporates complex historical and cultural aspects unique to Korea. Notably, GPT-4 has not passed the Korean bar exam, despite passing the US bar exam. This highlights that creating a Korean legal benchmark involves more than translating existing benchmarks; it requires developing a new system with a unique ontology.  To ensure the quality of the datasets, we developed the tasks in close corporation with legal professionals, and all the answers have been verified by 8 licensed lawyers. 6 6 6 For two case relevance QA datasets ( rel q ,  rel p ), only a portion of examples were verified due to the difficulty of the tasks. For more details, see Section  3.2  The verification took 26 hours in total reflecting the difficulty (and the quality) of the tasks constructed.  During the quality assurance process, we found freely available data, often created by individuals with semi-expertise, frequently include substantial amounts of errors (up to 21% in our study), highlighting the importance of close collaboration with experts (Section  A.1  covers additional general lessons learned during the dataset creation process).  Representative examples from individual tasks are displayed in Tables  3 ,  4 ,  5  in Appendix.",
            "The legal concept QA dataset ( conc ) comprises questions addressing complex legal concepts.  For instance, from various types of suspensionsuch as suspension of indictment, suspension of sentence, suspension of execution, suspension of collection, suspension of announcementa model needs to select the option that best fits a given definition.  The examples were crafted based on legal terminology reference documents from South Korean courts 7 7 7 https://sldongbu.scourt.go.kr/word/new/WordList.work , definition of legal terms provided by the National Law Information Center 8 8 8 https://www.law.go.kr/LSW/eng/engMain.do , and the Korea Legislation Research Institute 9 9 9 https://www.klri.re.kr/kor/business/bizLawDicKeyword.do . Please See Section  A.2.1  for further information.",
            "Noticing all open-source LLMs and GPT-3.5 show limited performance on the bar exam, we next focus on evaluating more powerful commercial LLMs.  Five commercial LLMsClaude-3-sonnet, Claude-3-opus, Claude-3.5-sonnet, and GPT-4achieve higher performance compared to the strongest open-source model Qwen-72b, with improvement of +1.8, +6.8, +9.8, +11.8 on the knowledge tasks; +1.0, +0.5, +2.6, +1.9 on the two reasoning tasks ( causal ,  cons ); +2.4, +9.9, +11.4, +17 on the 2024 bar exam 2024 (rows 68, and 10).  It shows that although GPT-4 passes the U.S. bar exam and it achieves most competent performance,  there remains significant room for improvement in LLM applications for Korean legal AI tasks.  Here we focus on the 2024 bar exam, which is least likely to have been used in training these LLMs.  Scores for the bar exams from 2010 to 2023, including the professional responsibility QA, are shown in Table  7 ,  8 ,  9 ,  10  in Appendix.",
            "Fig.  1  depicts GPT-4s accuracy on the 2024 bar exam, highlighting performance without RAG (blue), with the statute corpus (orange), with the precedent-corpus, and with both corpora (red).  The left panels display accuracy on Rule-type questions, while the right panels show on Application-type questions."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of various models. The accuracies of individual tasks are shown for  legal concept QA ( conc ), offense component QA ( comp ), statute number and content matching QA ( stat ), statute and query matching QA ( stat q ), statute hallucination QA ( hall ), common legal mistake QA ( mstke ), common legal mistake QA with reasoning ( mstke r ), causal reasoning QA ( causal ), statement consistency QA ( cons ), query and case relevance QA ( rel q ), inter-case relevance QA ( rel p ), and the Korean bar exam ( bar exam ). The average accuracies for 7 knowledge tasks (AVG K ), 2 reasoning tasks ( causal ,  cons , AVG R ), and 3 bar exams (AVG B ) are shown. For the experiments with GPT where the model shows randomness even with temperature    \\rightarrow   0 due to their internal algorithm that automatically raises the temperature until certain threshold satisfied, we repeat either two (the knowledge and the reasoning tasks) or three (the bar exam) times and show their mean values.  prec.  and  stat.  in the bottom 5 rows indicate the precedents and the statutes corpus respectively. Two corpora were used during the RAG experiments.  n/a  indicates the scores cannot be computed due the limitation in the context length. The scores for other open-source models are present in Table  11  in Appendix.",
        "table": "S3.T2.35",
        "footnotes": [],
        "references": [
            "The Korean legal system, rooted in civil law, incorporates complex historical and cultural aspects unique to Korea. Notably, GPT-4 has not passed the Korean bar exam, despite passing the US bar exam. This highlights that creating a Korean legal benchmark involves more than translating existing benchmarks; it requires developing a new system with a unique ontology.  To ensure the quality of the datasets, we developed the tasks in close corporation with legal professionals, and all the answers have been verified by 8 licensed lawyers. 6 6 6 For two case relevance QA datasets ( rel q ,  rel p ), only a portion of examples were verified due to the difficulty of the tasks. For more details, see Section  3.2  The verification took 26 hours in total reflecting the difficulty (and the quality) of the tasks constructed.  During the quality assurance process, we found freely available data, often created by individuals with semi-expertise, frequently include substantial amounts of errors (up to 21% in our study), highlighting the importance of close collaboration with experts (Section  A.1  covers additional general lessons learned during the dataset creation process).  Representative examples from individual tasks are displayed in Tables  3 ,  4 ,  5  in Appendix.",
            "The legal concept QA dataset ( conc ) comprises questions addressing complex legal concepts.  For instance, from various types of suspensionsuch as suspension of indictment, suspension of sentence, suspension of execution, suspension of collection, suspension of announcementa model needs to select the option that best fits a given definition.  The examples were crafted based on legal terminology reference documents from South Korean courts 7 7 7 https://sldongbu.scourt.go.kr/word/new/WordList.work , definition of legal terms provided by the National Law Information Center 8 8 8 https://www.law.go.kr/LSW/eng/engMain.do , and the Korea Legislation Research Institute 9 9 9 https://www.klri.re.kr/kor/business/bizLawDicKeyword.do . Please See Section  A.2.1  for further information.",
            "We evaluate four open-source LLMs and five commercial LLMs across 7 legal knowledge tasks, 4 legal reasoning tasks, and 3 bar exam tasks conducted in 2024 (Table  2 )."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Task examples. The examples are translated to English using GPT-4.",
        "table": "A1.SS2.SSS1.p3.1",
        "footnotes": [],
        "references": [
            "Son et al. ( 2024a )  developed KMMLU, a dataset similar to MMLU Hendrycks et al. ( 2021 )  tailored for Korean and cultural contexts.  It includes diverse language understanding tasks along with four legal AI tasks named  criminal-law, law, patent , and  taxation , sourced from the Public Service Aptitude Test and various Korean license exams. 3 3 3 The specific origins of the datasets are not described in the paper.  Many examples pertain to licensure exams such as the Petitioner Police exam, the Real Estate Brokers exam, and the Occupational Safety and Health exam, or cover topics in social science theory and legal terminology.  These tasks are generally less challenging than those found in the bar exam.  In contrast,  kbl  consists of questions from the Korean bar exam and the newly created questions designed to assess comprehensive legal knowledge and in-depth reasoning capability relevant to the practice of law.  Our work also differs in that it exclusively focus on the legal domains, and we have collaborated with legal professionals to develop a pragmatic and verified benchmark.  Additionally, we also assess LLMs in scenarios where they can utilize legal documents.  We have also ensured that there is no overlap between the legal task examples in KMMLU and our dataset by conducting fuzzy matching between the two datasets, with the most significant matches displayed in Tables  13  16  in Appendix.",
            "The Korean legal system, rooted in civil law, incorporates complex historical and cultural aspects unique to Korea. Notably, GPT-4 has not passed the Korean bar exam, despite passing the US bar exam. This highlights that creating a Korean legal benchmark involves more than translating existing benchmarks; it requires developing a new system with a unique ontology.  To ensure the quality of the datasets, we developed the tasks in close corporation with legal professionals, and all the answers have been verified by 8 licensed lawyers. 6 6 6 For two case relevance QA datasets ( rel q ,  rel p ), only a portion of examples were verified due to the difficulty of the tasks. For more details, see Section  3.2  The verification took 26 hours in total reflecting the difficulty (and the quality) of the tasks constructed.  During the quality assurance process, we found freely available data, often created by individuals with semi-expertise, frequently include substantial amounts of errors (up to 21% in our study), highlighting the importance of close collaboration with experts (Section  A.1  covers additional general lessons learned during the dataset creation process).  Representative examples from individual tasks are displayed in Tables  3 ,  4 ,  5  in Appendix."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Task examples",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": [
            "The Korean legal system, rooted in civil law, incorporates complex historical and cultural aspects unique to Korea. Notably, GPT-4 has not passed the Korean bar exam, despite passing the US bar exam. This highlights that creating a Korean legal benchmark involves more than translating existing benchmarks; it requires developing a new system with a unique ontology.  To ensure the quality of the datasets, we developed the tasks in close corporation with legal professionals, and all the answers have been verified by 8 licensed lawyers. 6 6 6 For two case relevance QA datasets ( rel q ,  rel p ), only a portion of examples were verified due to the difficulty of the tasks. For more details, see Section  3.2  The verification took 26 hours in total reflecting the difficulty (and the quality) of the tasks constructed.  During the quality assurance process, we found freely available data, often created by individuals with semi-expertise, frequently include substantial amounts of errors (up to 21% in our study), highlighting the importance of close collaboration with experts (Section  A.1  covers additional general lessons learned during the dataset creation process).  Representative examples from individual tasks are displayed in Tables  3 ,  4 ,  5  in Appendix."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Task examples",
        "table": "A1.T5.2",
        "footnotes": [],
        "references": [
            "The Korean legal system, rooted in civil law, incorporates complex historical and cultural aspects unique to Korea. Notably, GPT-4 has not passed the Korean bar exam, despite passing the US bar exam. This highlights that creating a Korean legal benchmark involves more than translating existing benchmarks; it requires developing a new system with a unique ontology.  To ensure the quality of the datasets, we developed the tasks in close corporation with legal professionals, and all the answers have been verified by 8 licensed lawyers. 6 6 6 For two case relevance QA datasets ( rel q ,  rel p ), only a portion of examples were verified due to the difficulty of the tasks. For more details, see Section  3.2  The verification took 26 hours in total reflecting the difficulty (and the quality) of the tasks constructed.  During the quality assurance process, we found freely available data, often created by individuals with semi-expertise, frequently include substantial amounts of errors (up to 21% in our study), highlighting the importance of close collaboration with experts (Section  A.1  covers additional general lessons learned during the dataset creation process).  Representative examples from individual tasks are displayed in Tables  3 ,  4 ,  5  in Appendix."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  The mean token length of the Bar exam",
        "table": "A1.T6.1",
        "footnotes": [],
        "references": [
            "Son et al. ( 2024a )  developed KMMLU, a dataset similar to MMLU Hendrycks et al. ( 2021 )  tailored for Korean and cultural contexts.  It includes diverse language understanding tasks along with four legal AI tasks named  criminal-law, law, patent , and  taxation , sourced from the Public Service Aptitude Test and various Korean license exams. 3 3 3 The specific origins of the datasets are not described in the paper.  Many examples pertain to licensure exams such as the Petitioner Police exam, the Real Estate Brokers exam, and the Occupational Safety and Health exam, or cover topics in social science theory and legal terminology.  These tasks are generally less challenging than those found in the bar exam.  In contrast,  kbl  consists of questions from the Korean bar exam and the newly created questions designed to assess comprehensive legal knowledge and in-depth reasoning capability relevant to the practice of law.  Our work also differs in that it exclusively focus on the legal domains, and we have collaborated with legal professionals to develop a pragmatic and verified benchmark.  Additionally, we also assess LLMs in scenarios where they can utilize legal documents.  We have also ensured that there is no overlap between the legal task examples in KMMLU and our dataset by conducting fuzzy matching between the two datasets, with the most significant matches displayed in Tables  13  16  in Appendix."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Comparison of various models on Korean Bar Exam-criminal laws.",
        "table": "A1.T7.1",
        "footnotes": [],
        "references": [
            "Noticing all open-source LLMs and GPT-3.5 show limited performance on the bar exam, we next focus on evaluating more powerful commercial LLMs.  Five commercial LLMsClaude-3-sonnet, Claude-3-opus, Claude-3.5-sonnet, and GPT-4achieve higher performance compared to the strongest open-source model Qwen-72b, with improvement of +1.8, +6.8, +9.8, +11.8 on the knowledge tasks; +1.0, +0.5, +2.6, +1.9 on the two reasoning tasks ( causal ,  cons ); +2.4, +9.9, +11.4, +17 on the 2024 bar exam 2024 (rows 68, and 10).  It shows that although GPT-4 passes the U.S. bar exam and it achieves most competent performance,  there remains significant room for improvement in LLM applications for Korean legal AI tasks.  Here we focus on the 2024 bar exam, which is least likely to have been used in training these LLMs.  Scores for the bar exams from 2010 to 2023, including the professional responsibility QA, are shown in Table  7 ,  8 ,  9 ,  10  in Appendix."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Comparison of various models on the Korean Bar Exam, Civil domain.",
        "table": "A1.T8.1",
        "footnotes": [],
        "references": [
            "Noticing all open-source LLMs and GPT-3.5 show limited performance on the bar exam, we next focus on evaluating more powerful commercial LLMs.  Five commercial LLMsClaude-3-sonnet, Claude-3-opus, Claude-3.5-sonnet, and GPT-4achieve higher performance compared to the strongest open-source model Qwen-72b, with improvement of +1.8, +6.8, +9.8, +11.8 on the knowledge tasks; +1.0, +0.5, +2.6, +1.9 on the two reasoning tasks ( causal ,  cons ); +2.4, +9.9, +11.4, +17 on the 2024 bar exam 2024 (rows 68, and 10).  It shows that although GPT-4 passes the U.S. bar exam and it achieves most competent performance,  there remains significant room for improvement in LLM applications for Korean legal AI tasks.  Here we focus on the 2024 bar exam, which is least likely to have been used in training these LLMs.  Scores for the bar exams from 2010 to 2023, including the professional responsibility QA, are shown in Table  7 ,  8 ,  9 ,  10  in Appendix."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Comparison of various models on the Korean Bar Exam, Public domain.",
        "table": "A1.T9.1",
        "footnotes": [],
        "references": [
            "Noticing all open-source LLMs and GPT-3.5 show limited performance on the bar exam, we next focus on evaluating more powerful commercial LLMs.  Five commercial LLMsClaude-3-sonnet, Claude-3-opus, Claude-3.5-sonnet, and GPT-4achieve higher performance compared to the strongest open-source model Qwen-72b, with improvement of +1.8, +6.8, +9.8, +11.8 on the knowledge tasks; +1.0, +0.5, +2.6, +1.9 on the two reasoning tasks ( causal ,  cons ); +2.4, +9.9, +11.4, +17 on the 2024 bar exam 2024 (rows 68, and 10).  It shows that although GPT-4 passes the U.S. bar exam and it achieves most competent performance,  there remains significant room for improvement in LLM applications for Korean legal AI tasks.  Here we focus on the 2024 bar exam, which is least likely to have been used in training these LLMs.  Scores for the bar exams from 2010 to 2023, including the professional responsibility QA, are shown in Table  7 ,  8 ,  9 ,  10  in Appendix."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Comparison of various models on the Korean Bar Exam, the Responsibility domain. This exam begins from 2010 following the introduction of law schools in South Korea in 2009. As of June 2024, data for the year 2024 is not included as the 15th exam is scheduled for August 2024.",
        "table": "A1.T10.1",
        "footnotes": [],
        "references": [
            "Noticing all open-source LLMs and GPT-3.5 show limited performance on the bar exam, we next focus on evaluating more powerful commercial LLMs.  Five commercial LLMsClaude-3-sonnet, Claude-3-opus, Claude-3.5-sonnet, and GPT-4achieve higher performance compared to the strongest open-source model Qwen-72b, with improvement of +1.8, +6.8, +9.8, +11.8 on the knowledge tasks; +1.0, +0.5, +2.6, +1.9 on the two reasoning tasks ( causal ,  cons ); +2.4, +9.9, +11.4, +17 on the 2024 bar exam 2024 (rows 68, and 10).  It shows that although GPT-4 passes the U.S. bar exam and it achieves most competent performance,  there remains significant room for improvement in LLM applications for Korean legal AI tasks.  Here we focus on the 2024 bar exam, which is least likely to have been used in training these LLMs.  Scores for the bar exams from 2010 to 2023, including the professional responsibility QA, are shown in Table  7 ,  8 ,  9 ,  10  in Appendix."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Comparison of various models.",
        "table": "A1.T11.15",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "Table 12:  Comparison of KMMLU and Korean Bar Exam using 60+ Fuzzy Score - Criminal Law",
        "table": "A1.T12.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "Table 13:  Comparison of KMMLU and Korean Bar Exam using 60+ Fuzzy Score - Civil Law",
        "table": "A1.T13.1",
        "footnotes": [],
        "references": [
            "Son et al. ( 2024a )  developed KMMLU, a dataset similar to MMLU Hendrycks et al. ( 2021 )  tailored for Korean and cultural contexts.  It includes diverse language understanding tasks along with four legal AI tasks named  criminal-law, law, patent , and  taxation , sourced from the Public Service Aptitude Test and various Korean license exams. 3 3 3 The specific origins of the datasets are not described in the paper.  Many examples pertain to licensure exams such as the Petitioner Police exam, the Real Estate Brokers exam, and the Occupational Safety and Health exam, or cover topics in social science theory and legal terminology.  These tasks are generally less challenging than those found in the bar exam.  In contrast,  kbl  consists of questions from the Korean bar exam and the newly created questions designed to assess comprehensive legal knowledge and in-depth reasoning capability relevant to the practice of law.  Our work also differs in that it exclusively focus on the legal domains, and we have collaborated with legal professionals to develop a pragmatic and verified benchmark.  Additionally, we also assess LLMs in scenarios where they can utilize legal documents.  We have also ensured that there is no overlap between the legal task examples in KMMLU and our dataset by conducting fuzzy matching between the two datasets, with the most significant matches displayed in Tables  13  16  in Appendix."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  Comparison of KMMLU and Korean Bar Exam using 60+ Fuzzy Score - Public Law (page 1)",
        "table": "A1.T14.2",
        "footnotes": [],
        "references": []
    },
    "id_table_15": {
        "caption": "Table 15:  Comparison of KMMLU and Korean Bar Exam using 60+ Fuzzy Score - Public Law (page 2)",
        "table": "A1.T15.1",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "Table 16:  Comparison of KMMLU and Korean Bar Exam using 60+ Fuzzy Score - Public Law (page 3)",
        "table": "A1.T16.1",
        "footnotes": [],
        "references": [
            "Son et al. ( 2024a )  developed KMMLU, a dataset similar to MMLU Hendrycks et al. ( 2021 )  tailored for Korean and cultural contexts.  It includes diverse language understanding tasks along with four legal AI tasks named  criminal-law, law, patent , and  taxation , sourced from the Public Service Aptitude Test and various Korean license exams. 3 3 3 The specific origins of the datasets are not described in the paper.  Many examples pertain to licensure exams such as the Petitioner Police exam, the Real Estate Brokers exam, and the Occupational Safety and Health exam, or cover topics in social science theory and legal terminology.  These tasks are generally less challenging than those found in the bar exam.  In contrast,  kbl  consists of questions from the Korean bar exam and the newly created questions designed to assess comprehensive legal knowledge and in-depth reasoning capability relevant to the practice of law.  Our work also differs in that it exclusively focus on the legal domains, and we have collaborated with legal professionals to develop a pragmatic and verified benchmark.  Additionally, we also assess LLMs in scenarios where they can utilize legal documents.  We have also ensured that there is no overlap between the legal task examples in KMMLU and our dataset by conducting fuzzy matching between the two datasets, with the most significant matches displayed in Tables  13  16  in Appendix."
        ]
    },
    "global_footnotes": [
        "The specific origins of the datasets are not described in the paper.",
        "Here, the term reasoning refers to general NLP reasoning task in the legal domain",
        "For two case relevance QA datasets (",
        ",",
        "), only a portion of examples were verified due to the difficulty of the tasks. For more details, see Section",
        "The free legal counselings aim to make legal services accessible to socially disadvantaged groups while stating that the answers provided may not be entirely accurate, as they are based solely on the questions and facts presented by the client.",
        "For example, one of the feedback is It is not possible to determine a violation of the Personal Information Protection Act based solely on the provided facts and questions. However, if the question is changed to inquire about defamation, it can be determined that the offense components are met.",
        "Suwon District Court, Anyang Branch, Judgment dated November 20, 2020, Case No. 202056 (   2020. 11. 20.  202056 )",
        "The Korean bar exam is published annually by the Ministry of Justice of Korea under KOGL Type 1 license. The license permits both commercial and non-commercial use, and allows the creation of derivative works, including modifications, as long as the source is cited.",
        "https://open.law.go.kr/LSO/main.do",
        "Even Qwen2-7b shows most competent performance compared to other open-source Korean LLMs of similar sizes, particularly in Bar examss. We propose two possible explanations for this. As previously reported",
        ", training legal LLMs from scratch can be beneficial for solving difficult tasks. But, most strong open-source Korean LLMs are adapted from English-dedicated LLMs (Llama",
        ", Solar",
        ", Mistral",
        ") by further training with a Korean corpus. There may be knowledge transfer between multilingual corpus. Note that the Qwen series also achieves strong performance in the Chinese legal domain",
        ". Although the Korean legal system differs from Chinese legal systems, basic legal frameworks like IRAC may have corresponding notions and legal terms in each system. It is important to note that the details of the training corpus for many open-source LLMs are not fully disclosed, making it difficult to interpret the origin of their performance."
    ]
}