{
    "S3.T1": {
        "caption": "Table 1: Simple evaluation. w/ rating means the prompt contains ratings and w/o rating is vice.",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T1.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.1\">\n<th class=\"ltx_td ltx_nopad ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S3.T1.1.1.1.1\"><svg height=\"19.07\" overflow=\"visible\" version=\"1.1\" width=\"93.78\"><g transform=\"translate(0,19.07) scale(1,-1)\"><path d=\"M 0,19.07 93.78,0\" stroke=\"#000000\" stroke-width=\"0.4\"/><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"46.89\">\n<span class=\"ltx_inline-block\" id=\"S3.T1.1.1.1.1.pic1.1.1\">\n<span class=\"ltx_inline-block ltx_align_left\" id=\"S3.T1.1.1.1.1.pic1.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.1.1.1.pic1.1.1.1.1\">Method</span>\n</span>\n</span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(54.15,9.61)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject height=\"9.46\" overflow=\"visible\" width=\"39.63\">\n<span class=\"ltx_inline-block\" id=\"S3.T1.1.1.1.1.pic1.2.1\">\n<span class=\"ltx_inline-block ltx_align_right\" id=\"S3.T1.1.1.1.1.pic1.2.1.1\">\n<span class=\"ltx_p\" id=\"S3.T1.1.1.1.1.pic1.2.1.1.1\">Metric</span>\n</span>\n</span></foreignobject></g></g></g></svg></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S3.T1.1.1.1.2\">ROUGE-1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S3.T1.1.1.1.3\">ROUGE-L</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T1.1.1.1.4\">BertScore (mean)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.2.1.1\">GPT-3.5-turbo (w/ rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.2.1.2\">15.99</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T1.1.1.2.1.3\">9.84</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T1.1.1.2.1.4\">41.52</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.1.1.3.2.1\">GPT-3.5-turbo (w/o rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.3.2.2\">16.00</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.3.2.3\">9.81</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.3.2.4\">41.37</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.1.1.4.3.1\">GPT-4o (w/ rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.4.3.2\">12.80</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.4.3.3\">8.47</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.4.3.4\">40.12</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.1.1.5.4.1\">GPT-4o (w/o rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.5.4.2\">15.41</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.5.4.3\">11.22</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.5.4.4\">41.73</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.1.1.6.5.1\">Llama-3-8b (w/ rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.6.5.2\">12.23</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.6.5.3\">8.23</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.6.5.4\">31.30</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.1.1.7.6.1\">Llama-3-8b (w/o rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.7.6.2\">13.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.7.6.3\">9.59</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.7.6.4\">30.46</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.8.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T1.1.1.8.7.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.8.7.1.1\">Review-LLM (w/ rating)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.8.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.8.7.2.1\">31.15</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T1.1.1.8.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.8.7.3.1\">26.88</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T1.1.1.8.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T1.1.1.8.7.4.1\">49.52</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T1.1.1.9.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S3.T1.1.1.9.8.1\">Review-LLM (w/o rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.1.1.9.8.2\">30.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T1.1.1.9.8.3\">26.38</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T1.1.1.9.8.4\">48.56</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Table\u00a01 compares the performance of our method with several baselines and ablations.\nIt is noted that the GPT-3.5-Turbo and GPT-4o are always better than Llama-3-8b, the reason is that the GPT-series models have a larger number of parameters and are pre-trained on massive data, which could learn more general knowledge.\nBesides, we find that some baselines without ratings perform better than with ratings, while our fine-tuning method is the opposite.\nWe argue that this is because the user rating information is further pre-trained in our method while baselines not.\nOverall, our method Review-LLM outperforms all methods (including GPT-3.5-Turbo and GPT-4o) across all metrics, demonstrating the effectiveness of using the item title, review, and rating to personalized fine-tune.",
            "In our method, we employ user rating information to strengthen the model\u2019s understanding of user preferences for different items to achieve more personalized review generation.\nIn this part, we test the performance of the model on the constructed hard testing set.\nThe different model performance is shown in Table\u00a02.\nFrom the results, we can find that all model performance has decreased compared with Table\u00a01.\nIn particular, using Llama3-8b for inference directly, BertScore is reduced to <math alttext=\"26.96\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.1.m1.1\">\n  <semantics id=\"S3.SS4.p1.1.m1.1a\">\n    <mn id=\"S3.SS4.p1.1.m1.1.1\" xref=\"S3.SS4.p1.1.m1.1.1.cmml\">26.96</mn>\n    <annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.1.m1.1b\">\n      <cn id=\"S3.SS4.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.SS4.p1.1.m1.1.1\">26.96</cn>\n    </annotation-xml>\n    <annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.1.m1.1c\">26.96</annotation>\n    <annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.1.m1.1d\">26.96</annotation>\n  </semantics>\n</math>.\nWe argue that this is because the LLMs might be polite, resulting in insufficient negative information captured during generating reviews.\nBesides, methods with ratings outperform methods without ratings on semantic similarity, especially Review-LLM, which further confirms the necessity of fusing the rating information for personalized review generation.\n<semantics id=\"S3.SS4.p1.1.m1.1a\">\n  <mn id=\"S3.SS4.p1.1.m1.1.1\" xref=\"S3.SS4.p1.1.m1.1.1.cmml\">26.96</mn>\n  <annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.1.m1.1b\">\n    <cn id=\"S3.SS4.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.SS4.p1.1.m1.1.1\">26.96</cn>\n  </annotation-xml>\n  <annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.1.m1.1c\">26.96</annotation>\n  <annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.1.m1.1d\">26.96</annotation>\n</semantics>\n<mn id=\"S3.SS4.p1.1.m1.1.1\" xref=\"S3.SS4.p1.1.m1.1.1.cmml\">26.96</mn>\n26.96<annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.1.m1.1b\">\n  <cn id=\"S3.SS4.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.SS4.p1.1.m1.1.1\">26.96</cn>\n</annotation-xml>\n<cn id=\"S3.SS4.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.SS4.p1.1.m1.1.1\">26.96</cn>\n26.96<annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.1.m1.1c\">26.96</annotation>\n26.96<annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.1.m1.1d\">26.96</annotation>\n26.96.\nWe argue that this is because the LLMs might be polite, resulting in insufficient negative information captured during generating reviews.\nBesides, methods with ratings outperform methods without ratings on semantic similarity, especially Review-LLM, which further confirms the necessity of fusing the rating information for personalized review generation."
        ]
    },
    "S3.T2": {
        "caption": "Table 2: Hard evaluation. w/ rating means the prompt contains ratings and w/o rating is vice.",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S3.T2.1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.1\">\n<th class=\"ltx_td ltx_nopad ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S3.T2.1.1.1.1\"><svg height=\"19.07\" overflow=\"visible\" version=\"1.1\" width=\"93.78\"><g transform=\"translate(0,19.07) scale(1,-1)\"><path d=\"M 0,19.07 93.78,0\" stroke=\"#000000\" stroke-width=\"0.4\"/><g class=\"ltx_svg_fog\" transform=\"translate(0,0)\"><g transform=\"translate(0,9.61) scale(1, -1)\"><foreignobject height=\"9.61\" overflow=\"visible\" width=\"46.89\">\n<span class=\"ltx_inline-block\" id=\"S3.T2.1.1.1.1.pic1.1.1\">\n<span class=\"ltx_inline-block ltx_align_left\" id=\"S3.T2.1.1.1.1.pic1.1.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.1.1.1.pic1.1.1.1.1\">Method</span>\n</span>\n</span></foreignobject></g></g><g class=\"ltx_svg_fog\" transform=\"translate(54.15,9.61)\"><g transform=\"translate(0,9.46) scale(1, -1)\"><foreignobject height=\"9.46\" overflow=\"visible\" width=\"39.63\">\n<span class=\"ltx_inline-block\" id=\"S3.T2.1.1.1.1.pic1.2.1\">\n<span class=\"ltx_inline-block ltx_align_right\" id=\"S3.T2.1.1.1.1.pic1.2.1.1\">\n<span class=\"ltx_p\" id=\"S3.T2.1.1.1.1.pic1.2.1.1.1\">Metric</span>\n</span>\n</span></foreignobject></g></g></g></svg></th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S3.T2.1.1.1.2\">ROUGE-1</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S3.T2.1.1.1.3\">ROUGE-L</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S3.T2.1.1.1.4\">BertScore (mean)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S3.T2.1.1.2.1.1\">GPT-3.5-turbo (w/ rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.1.2.1.2\">17.62</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S3.T2.1.1.2.1.3\">10.70</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S3.T2.1.1.2.1.4\">37.45</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T2.1.1.3.2.1\">GPT-3.5-turbo (w/o rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.3.2.2\">16.07</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.3.2.3\">9.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.3.2.4\">37.25</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T2.1.1.4.3.1\">GPT-4o (w/ rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.4.3.2\">16.66</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.4.3.3\">9.86</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.4.3.4\">39.21</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T2.1.1.5.4.1\">GPT-4o (w/o rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.5.4.2\">14.51</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.5.4.3\">8.73</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.5.4.4\">38.64</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.6.5\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T2.1.1.6.5.1\">Llama-3-8b (w/ rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.6.5.2\">13.47</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.6.5.3\">8.05</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.6.5.4\">28.38</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.7.6\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T2.1.1.7.6.1\">Llama-3-8b (w/o rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.7.6.2\">13.11</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.7.6.3\">7.89</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.7.6.4\">26.96</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.8.7\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S3.T2.1.1.8.7.1\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.8.7.1.1\">Review-LLM (w/ rating)</span></th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.8.7.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.8.7.2.1\">21.93</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S3.T2.1.1.8.7.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.8.7.3.1\">16.63</span></td>\n<td class=\"ltx_td ltx_align_center\" id=\"S3.T2.1.1.8.7.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S3.T2.1.1.8.7.4.1\">39.35</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S3.T2.1.1.9.8\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S3.T2.1.1.9.8.1\">Review-LLM (w/o rating)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.1.1.9.8.2\">17.82</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S3.T2.1.1.9.8.3\">13.50</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S3.T2.1.1.9.8.4\">35.89</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "In our method, we employ user rating information to strengthen the model\u2019s understanding of user preferences for different items to achieve more personalized review generation.\nIn this part, we test the performance of the model on the constructed hard testing set.\nThe different model performance is shown in Table\u00a02.\nFrom the results, we can find that all model performance has decreased compared with Table\u00a01.\nIn particular, using Llama3-8b for inference directly, BertScore is reduced to <math alttext=\"26.96\" class=\"ltx_Math\" display=\"inline\" id=\"S3.SS4.p1.1.m1.1\">\n  <semantics id=\"S3.SS4.p1.1.m1.1a\">\n    <mn id=\"S3.SS4.p1.1.m1.1.1\" xref=\"S3.SS4.p1.1.m1.1.1.cmml\">26.96</mn>\n    <annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.1.m1.1b\">\n      <cn id=\"S3.SS4.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.SS4.p1.1.m1.1.1\">26.96</cn>\n    </annotation-xml>\n    <annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.1.m1.1c\">26.96</annotation>\n    <annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.1.m1.1d\">26.96</annotation>\n  </semantics>\n</math>.\nWe argue that this is because the LLMs might be polite, resulting in insufficient negative information captured during generating reviews.\nBesides, methods with ratings outperform methods without ratings on semantic similarity, especially Review-LLM, which further confirms the necessity of fusing the rating information for personalized review generation.\n<semantics id=\"S3.SS4.p1.1.m1.1a\">\n  <mn id=\"S3.SS4.p1.1.m1.1.1\" xref=\"S3.SS4.p1.1.m1.1.1.cmml\">26.96</mn>\n  <annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.1.m1.1b\">\n    <cn id=\"S3.SS4.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.SS4.p1.1.m1.1.1\">26.96</cn>\n  </annotation-xml>\n  <annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.1.m1.1c\">26.96</annotation>\n  <annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.1.m1.1d\">26.96</annotation>\n</semantics>\n<mn id=\"S3.SS4.p1.1.m1.1.1\" xref=\"S3.SS4.p1.1.m1.1.1.cmml\">26.96</mn>\n26.96<annotation-xml encoding=\"MathML-Content\" id=\"S3.SS4.p1.1.m1.1b\">\n  <cn id=\"S3.SS4.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.SS4.p1.1.m1.1.1\">26.96</cn>\n</annotation-xml>\n<cn id=\"S3.SS4.p1.1.m1.1.1.cmml\" type=\"float\" xref=\"S3.SS4.p1.1.m1.1.1\">26.96</cn>\n26.96<annotation encoding=\"application/x-tex\" id=\"S3.SS4.p1.1.m1.1c\">26.96</annotation>\n26.96<annotation encoding=\"application/x-llamapun\" id=\"S3.SS4.p1.1.m1.1d\">26.96</annotation>\n26.96.\nWe argue that this is because the LLMs might be polite, resulting in insufficient negative information captured during generating reviews.\nBesides, methods with ratings outperform methods without ratings on semantic similarity, especially Review-LLM, which further confirms the necessity of fusing the rating information for personalized review generation."
        ]
    }
}