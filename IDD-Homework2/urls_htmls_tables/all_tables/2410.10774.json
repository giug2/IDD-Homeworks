{
    "id_table_1": {
        "caption": "Table 1:  Quantitative comparison for monocular geometry consistency on RealEstate10K test set.",
        "table": "S5.T1.7.7",
        "footnotes": [],
        "references": [
            "Image-to-video generation takes a single image  I 0 subscript I 0 I_{0} italic_I start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT  as input and outputs a video sequence  O 1 ,  , O n subscript O 1  subscript O n O_{1},\\cdots,O_{n} italic_O start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_O start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT . By introducing camera control, the model additionally takes in a sequence of camera information  C 1 ,  , C n subscript C 1  subscript C n C_{1},\\cdots,C_{n} italic_C start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_C start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT , which dictates the desired viewpoint changes for the output sequence. In the multi-view scenario, we extend each batch of the camera control signal and output video sequence to  V V V italic_V  sequences. In the following paragraphs, we present our proposed  Cavia  framework in detail. First, we outline the preliminaries of image-to-video diffusion and describe how camera controllability is introduced in monocular video generation. Then, we elaborate on the model design for multi-view consistent video generation. An overview of our framework is provided in Fig.  1 .",
            "To overcome this issue, we inflate the original 1D temporal attention modules in the SVD network into 3D cross-frame temporal attention modules, allowing for joint modeling of spatial-temporal feature coherence. The inflation operation can be achieved by rearranging the latent features before the attention matrix calculations. Consider the latent features of shape  ( B  V  F  C  H  W ) B V F C H W (B\\hskip 2.5ptV\\hskip 2.5ptF\\hskip 2.5ptC\\hskip 2.5ptH\\hskip 2.5ptW) ( italic_B italic_V italic_F italic_C italic_H italic_W )  where  F F F italic_F  refers to the length of frames and  V V V italic_V  is the number of views, instead of employing 1D attention mechanism on rearranged shape of  ( ( B  V  H  W )  F  C ) B V H W F C ((B\\hskip 2.5ptV\\hskip 2.5ptH\\hskip 2.5ptW)\\hskip 2.5ptF\\hskip 2.5ptC) ( ( italic_B italic_V italic_H italic_W ) italic_F italic_C ) , our inflated attention operates on the rearranged shape of  ( ( B  V )  ( F  H  W )  C ) B V F H W C ((B\\hskip 2.5ptV)\\hskip 2.5pt(F\\hskip 2.5ptH\\hskip 2.5ptW)\\hskip 2.5ptC) ( ( italic_B italic_V ) ( italic_F italic_H italic_W ) italic_C ) , integrating spatial features into the attention matrices. A visualization is provided in Fig.  1 (c).",
            "To improve cross-view consistency in multi-view videos, we aim to encourage information exchange during the generation process. Since our temporal cross-frame attention modules already handle intra-view feature connections within each video sequence, we focus on exchanging inter-view signals through the spatial cross-view modules. Inspired by MVDream  (Shi et al.,  2023b ) , we introduce 3D cross-view attention modules, inflated from the spatial attention blocks of SVD  (Stability,  2023 ) . Specifically, we rearrange the  V  views such that frames at each corresponding timesteps are concatenated before being sent into the attention modules. In detail, we rearrange the latent features from shape  ( B  V  F  C  H  W ) B V F C H W (B\\hskip 2.5ptV\\hskip 2.5ptF\\hskip 2.5ptC\\hskip 2.5ptH\\hskip 2.5ptW) ( italic_B italic_V italic_F italic_C italic_H italic_W )  to  ( ( ( B F ) ( V H W ) C ) (((B\\hskip 2.5ptF)(V\\hskip 2.5ptH\\hskip 2.5ptW)\\hskip 2.5ptC) ( ( ( italic_B italic_F ) ( italic_V italic_H italic_W ) italic_C )  instead of  ( ( ( B V F ) ( H W ) C ) (((B\\hskip 2.5ptV\\hskip 2.5ptF)(H\\hskip 2.5ptW)\\hskip 2.5ptC) ( ( ( italic_B italic_V italic_F ) ( italic_H italic_W ) italic_C ) . A visualization is provided in Fig.  1 (b).",
            "We evaluate the 3D consistency of the generated videos using COLMAP  (Schonberger & Frahm,  2016 ; Schonberger et al.,  2016 ) . COLMAP is widely adopted for 3D reconstruction methods where camera pose estimation is required for in-the-wild images. We configure the COLMAP following previous methods  (Deng et al.,  2022 ; Xu et al.,  2024 )  for best few-view performance. A higher COLMAP error rate indicates poorer 3D consistency in the input images. Motivated by this, we report COLMAP errors as a measure of the 3D consistency of the frames. Each video is retried up to five times to reduce randomness. We randomly sample 1,000 video sequences from RealEstate10K  (Zhou et al.,  2018 )  test set for evaluation. Since we have ground truth 3D scenes, we use the ground truth camera pose sequences as the viewpoint instruction of the video model and compare the generated frames against the ground truth images. Similar to prior works  (He et al.,  2024 ; Xu et al.,  2024 ) , we extract the estimated camera poses and calculate the relative translation and rotation differences. Specifically, given two camera pose sequences, we convert them to relative poses and align the first frames to world origin. We then measure the angular errors in translation and rotation. Unlike previous works  (He et al.,  2024 ; Xu et al.,  2024 )  that calculate the Euclidean distance of translation vectors, we use angular error measurements to ensure the camera pose scales are normalized, addressing scale ambiguity. As shown in Tab.  1 , we calculate the area under the cumulative error curve (AUC) of frames whose rotation and translations are below certain thresholds ( 5  superscript 5 5^{\\circ} 5 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ,  10  superscript 10 10^{\\circ} 10 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ,  20  superscript 20 20^{\\circ} 20 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ). Our method significantly outperforms existing baselines.",
            "To assess the frame perceptual quality, we evaluate visual quality using FID  (Heusel et al.,  2017 )  and FVD  (Unterthiner et al.,  2018 ) . FID and FVD measure the feature-space similarity of two sets of images and videos, respectively. In our case, they quantify the distribution distance between the generated frame sequences and the ground-truth frames. We provide monocular evaluations in Tab.  1  and multi-view evaluations in Tab.  2 . As shown in these tables, our proposed framework enjoys the best visual quality. For both the Real10K and General categories, the ground-truth videos used to calculate these metrics are the video sequences corresponding to the input frames. These video sequences are from the test set split of the datasets and are not seen during training.",
            "We further perform 3D reconstruction on our generated frames. We render our reconstructed 3D Gaussians from an elliptical trajectory consisting of 16 novel views. We provide a side-by-side comparison with the concurrent work CVD  (Kuang et al.,  2024 )  in Fig.  10 . Compared with the results of CVD, our frames are more geometrically consistent and result in clearer 3D reconstruction and fewer floaters. For example, the results from CVD produce floaters on the cupboard regions and generate blurry artifacts for the wall and the TV due to inconsistencies. We provide video comparisons in the supplementary for clearer comparisons. We also provide additional 3D reconstruction results of Cavias generated frames in the supplementary."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Quantitative comparison for 2-view video generation.",
        "table": "S5.T2.8.8",
        "footnotes": [],
        "references": [
            "To improve the models ability to generate object motion in the presence of complex backgrounds, we prepare monocular videos with camera pose annotations similar to CamCo  (Xu et al.,  2024 ) . First, we use Particle-SfM  (Zhao et al.,  2022 )  to estimate the camera poses for randomly sampled frames from videos from InternVid  (Wang et al.,  2023b )  and OpenVid  (Nan et al.,  2024 ) . Inspired by CO3D  (Reizenstein et al.,  2021 )  and CamCo  (Xu et al.,  2024 ) , we remove the videos where SfM fails to register all available frames or produces a point cloud with too few points or too many points. Fig.  2 (a) shows the point count statistics. A point cloud with too few points indicates poor frame registration to a shared 3D representation, while too many points suggest a mostly static scene, which is undesirable as we focus on object motion. Additionally, non-registered frames may indicate potential scene changes. We then apply a rigorous filtering pipeline to ensure the quality of the video samples used for training. This includes filtering based on aesthetic scores, optical character recognition (OCR), and camera motion classification using optical flow. Videos containing detected character regions are aggressively removed. Fig.  2 (b) and (c) present statistics on aesthetic score and camera motion classification results. Videos with low aesthetic scores or those classified as having static camera motion are excluded from the training set. Ultimately, we construct a dataset of 393,000 monocular videos annotated with camera poses. We provide a summary of the data sources used in Fig.  3 . More details and analysis are provided in the appendix.",
            "Alongside evaluating the individual monocular frame pose accuracy using COLMAP-based metrics, we further assess the cross-video consistency of the corresponding frames from generated multi-view videos. We randomly sample 1,000 videos, each with 27 frames, from RealEstate10k  (Zhou et al.,  2018 )  test set and convert each video into a two-view sequence with 14 frames per view. The new camera pose sequences are generated by setting the 14th frame as the world origin and positioning the remaining frames relative to it. The scales of the scenes are normalized so that the maximum distance from the origin is 1. Following CVD  (Kuang et al.,  2024 ) , we adopt SuperGlue  (Sarlin et al.,  2020 )  to find correspondences and estimate the camera poses between each time-aligned set of frames. SuperGlue not only measures angular errors in the rotation and translation but also computes the epipolar error of the matched correspondences. We similarly collect the AUC for frame pairs with rotation and translation errors below specific thresholds ( 5  superscript 5 5^{\\circ} 5 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ,  10  superscript 10 10^{\\circ} 10 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ,  20  superscript 20 20^{\\circ} 20 start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT ). The epipolar errors for the estimated correspondences are summarized to the precision (P) and matching score (MS). As shown in Tab.  2 , our method outperforms baselines greatly. The Real10K category means that the input images are taken from the corresponding RealEstate10K test sequence, while the General means that the input images are taken from 1,000 randomly sampled images in the test split of our monocular video dataset.",
            "To assess the frame perceptual quality, we evaluate visual quality using FID  (Heusel et al.,  2017 )  and FVD  (Unterthiner et al.,  2018 ) . FID and FVD measure the feature-space similarity of two sets of images and videos, respectively. In our case, they quantify the distribution distance between the generated frame sequences and the ground-truth frames. We provide monocular evaluations in Tab.  1  and multi-view evaluations in Tab.  2 . As shown in these tables, our proposed framework enjoys the best visual quality. For both the Real10K and General categories, the ground-truth videos used to calculate these metrics are the video sequences corresponding to the input frames. These video sequences are from the test set split of the datasets and are not seen during training.",
            "Our monocular video filtering pipeline involves filtering according to Particle-SfM output, OCR, aesthetic score, and camera motion. As mentioned in Sec.  4.2 , we first attempt to annotate the camera poses for the video frames using Particle-SfM  (Zhao et al.,  2022 ) . Take InternVid  (Wang et al.,  2023b )  as an example, roughly 10 million video clips are processed and around 3 million samples are successfully processed by Particle-SfM. For each video, we start from the first frame and randomly select a frame stride of 1 or 2. The total number of images sent to Particle-SfM is 32 images. Our point count filtering is empirically implemented as a cut-off at 1,000 points and 40,000 points. Point clouds with too few points are removed due to the concern that the frames are poorly registered. Point clouds with too many points are avoided because their limited object motion. This aggressive filtering results in around 2 million samples for further processing. We then evaluate all the video clips using OCR detection algorithms and remove the samples whose detected text regions are larger then  10  4 superscript 10 4 {10^{-4}} 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT  of the image resolution ( i.e.  6 pixels). This process results in 604,000 samples. The next step is filtering with aesthetic scores and videos with aesthetic score annotations smaller than 4 are removed. 467,000 videos are left after these filtering process. Finally, we employ a camera motion classifier extended from the Open-Sora pipeline 1 1 1 https://github.com/hpcaitech/Open-Sora/tree/main/tools/caption/camera_motion . The main motivation is that optical-flow on consecutive frames can be summarized to a global motion vector, assuming the most parts of the scene is moving in a uniform direction. Optical flow is first obtained using  cv2.calcOpticalFlowFarneback  for each consecutive frame pairs. Then, the magnitudes and directions are calculated via  cv2.cartToPolar . These magnitudes and directions are classified into 8 categories: static, zoom out, zoom in, pan left, tilt up, pan right, tilt down, and unknown. The results of the frame pairs are summarized to obtain the final result of each video clip. When a certain type appears more than 50%, the type for the whole video clip is determined directly. We aggressively classify a video clip as static if any of its frame pairs is categorized into static or unknown. Finally, we obtain 355,000 clips that satisfy our needs. The process is similarly applied to OpenVid  (Nan et al.,  2024 ) s Panda-70M subset  Chen et al. ( 2024a )  and we obtained 38,000 clips. In summary, our monocular video dataset consists of 393,000 clips."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Ablation Studies on each of our introduced modules. w/o Plucker refers to replacing the Plucker coordinate conditioning with one-dimensional conditioning as in MotionCtrl. w/o Cross-frame refers to replacing the Cross-frame attention with vanilla 1D temporal attention. w/o Cross-view refers to replacing the Cross-view attention with vanilla spatial attention. Ours (Static) means the model is only trained on static video datasets. Ours (w/o Mono) means that the model is fine-tuned on synthetic multi-view datasets, but is not trained with monocular video datasets. Ours (Full) means that the model is trained on all available data sources.",
        "table": "A2.T3.8.8",
        "footnotes": [],
        "references": [
            "To improve the models ability to generate object motion in the presence of complex backgrounds, we prepare monocular videos with camera pose annotations similar to CamCo  (Xu et al.,  2024 ) . First, we use Particle-SfM  (Zhao et al.,  2022 )  to estimate the camera poses for randomly sampled frames from videos from InternVid  (Wang et al.,  2023b )  and OpenVid  (Nan et al.,  2024 ) . Inspired by CO3D  (Reizenstein et al.,  2021 )  and CamCo  (Xu et al.,  2024 ) , we remove the videos where SfM fails to register all available frames or produces a point cloud with too few points or too many points. Fig.  2 (a) shows the point count statistics. A point cloud with too few points indicates poor frame registration to a shared 3D representation, while too many points suggest a mostly static scene, which is undesirable as we focus on object motion. Additionally, non-registered frames may indicate potential scene changes. We then apply a rigorous filtering pipeline to ensure the quality of the video samples used for training. This includes filtering based on aesthetic scores, optical character recognition (OCR), and camera motion classification using optical flow. Videos containing detected character regions are aggressively removed. Fig.  2 (b) and (c) present statistics on aesthetic score and camera motion classification results. Videos with low aesthetic scores or those classified as having static camera motion are excluded from the training set. Ultimately, we construct a dataset of 393,000 monocular videos annotated with camera poses. We provide a summary of the data sources used in Fig.  3 . More details and analysis are provided in the appendix.",
            "In this section, we conduct extensive evaluations for ablation studies. We provide video comparisons in the supplementary. We provide thorough quantitative comparisons in Tab.  3  to illustrate the importance of our proposed components. The models are evaluated using RealEstate10K camera trajectories. For the Real10K and General categories, the testing images are from our test set split of RealEstate10K and InternVid, respectively. Our full model enjoys the best perceptual quality and geometric consistency."
        ]
    }
}