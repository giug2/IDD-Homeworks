{
    "id_table_1": {
        "caption": "Table 1:  Data quantity SDG.",
        "table": "S2.T1.1.1",
        "footnotes": [],
        "references": [
            "We use  nemotron-4-340b-instruct   (Nvidia et al.,  2024 )  selected for its open license that allows commercial use. Our process includes deduplication and a decontamination procedure akin to that outlined by  Li et al. ( 2023 ) . Additionally, we conduct syntax checks to eliminate coding problems containing docstrings or solutions from Verilog benchmarks. To ensure further data quality, we discard code solutions that fail these syntax checks and apply self-verification  (Weng et al.,  2023 )  to remove entries where the LLM identifies errors in the solution.  Table   1  shows the quantity of our synthetic data generation (denoted as SDG) after deduplication and filtering, yielding a total of 80.1k fine-tuning examples.",
            "During our training, we observed significant variability in the models pass rate on specific benchmark problems across different checkpoints. We note such variance is different from training instability  (Wortsman et al.,  2023 )  as we observe a stable decrease in the training loss. This variability persists even in the later stages of training, despite using a low learning rate. We illustrate this variability in  Figure   1 . The scatter plot tracks the pass rate for each problem in VerilogEval-Human, with each point representing the pass rate for the same problem across two checkpoints. The size of each point indicates the number of problems with the same pass rates for the two model checkpoints. We further categorize the region into areas where the checkpoints agree on problem difficulty and areas where they do not.",
            "Algorithm   1  outlines the process for generating a Moore FSM with random transitions. State reachability is ensured by first constructing a tree. Legality for state transition is ensured by ensuring each node has an out-degree of  2 w superscript 2 w 2^{w} 2 start_POSTSUPERSCRIPT italic_w end_POSTSUPERSCRIPT  with the input bit width of  w w w italic_w . The result is an FSM where the transitions between states are randomly assigned but conform to the specified input bit width. The algorithm can be easily modified for a Mealy FSM by assigning the output to the edges rather than nodes.",
            "Our fine-tuning training data is comprised of 80.1k LLM synthetic generated data using various prompting methods as described in  Section   2.1 , 28.5k data samples generated correct-by-construction aimed at non-textual representations detailed in  Section   3.1 , and 1.4k carefully filtered data for targeted code repair as outlined in  Section   3.2 . We refer to each data set as  SDG ,  CC , and  Repair , respectively.",
            "Figure   5  displays the pass rates for two consecutive checkpoints of Starcoder2-SDG-CC-Repair on VerilogEval-Human problems, sampled with a temperature of 0.8. Compared to  Figure   1 , the updated model shows significant improvements by (1) moving previously unsolved problems into the solved category, including those with non-textual representations addressed by our correct-by-construction  CC  data, and (2) reducing the number of problems with large pass rate discrepancies, particularly where performance had degraded. The targeted repair data has effectively mitigated the models tendency to repeat common mistakes found in our  Repair  dataset, despite the noise inherent in synthetically generated  SDG  data.",
            "We present our models results on Verilog benchmarks tested with temperatures 0.2 and 0.8. We ablate across different data blends, with  SDG  indicating using LLM synthetic generated data in  Section   2.1 ,  CC  indicating correct-by-construction data targeting non-textual representations in  Section   3.1 , and  Repair  representing our targed code repair dataset in  Section   3.2 .",
            "As shown in Table  12 , a carefully filtered dataset of 1.4k samples achieves comparable performance to a 7.8k dataset. This suggests that merely increasing the dataset size by injecting the same types of errors does not contribute meaningfully to improving model performance.",
            "We conduct a second iteration by generating 2.7k repair data for the model based on the  Repair  data from the first iteration. As shown in   Table   13 , performance mostly saturates after this initial iteration. We suspect that the remaining issues are likely due to significant errors that are challenging to correct.",
            "Table   14  presents the results on code diversity. We sample 20 solutions with temperature of 0.8 for each model. We observe that fine-tuned models generally show a decrease in code diversity for both Text and NonText problems. This reduction is expected, as BLEU and Jaccard metrics account for both correct and incorrect code solutions, and there are often multiple ways to implement a correct solution. When comparing our fine-tuned models with GPT-4o, code diversity is similar for Text problems, but our models exhibit poor diversity for NonText problems. This is anticipated, given that the  CC  training dataset for NonText problems is generated using correct-by-construction methods and follows similar templates for Verilog code. However, our models demonstrate comparable diversity to GPT-4o for Text problems, particularly in TSED metric."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  pass@1 results on VerilogEval sampled with temperature of 0.8.",
        "table": "S2.T2.1.1",
        "footnotes": [],
        "references": [
            "We observe that models underperform on benchmark problems involving non-textual input formats, such as Karnaugh Maps, state-transition diagrams, and waveforms.  Table   2  shows the pass@1 results for the VerilogEval  (Liu et al.,  2023b ) . Additionally, we have identified a subset of 45 questions within VerilogEval-Human that include non-textual representations, termed VerilogEval-NonText. It appears that models like GPT-4o and Starcoder2 struggle with these non-textual formats, likely due to insufficient representation of such data during both pretraining and fine-tuning. Despite our efforts to generate such questions during synthetic data creation, our fine-tuned models still lag in these areas. This outcome is not entirely surprising, given that the LLMs used were also ineffective at generating problems with these representations, complicating the validation of fine-tuning data. These results suggest that merely including non-textual data is insufficient; ensuring the quality and correctness of the data, particularly that the code solutions accurately align with these representations, is crucial.",
            "We generate Verilog code problems and solutions that are correct-by-construction. Our focus is on creating problems and solutions for non-textual representations. The overview of the process for generating non-textual problems and correct-by-construction solutions is illustrated in  Figure   2 .  Table   3  shows the quantity of our correct-by-construction generation data (referred to as CC). To prevent data contamination, we exclude entries that duplicate the data representations of benchmark problems.",
            "Our fine-tuning training data is comprised of 80.1k LLM synthetic generated data using various prompting methods as described in  Section   2.1 , 28.5k data samples generated correct-by-construction aimed at non-textual representations detailed in  Section   3.1 , and 1.4k carefully filtered data for targeted code repair as outlined in  Section   3.2 . We refer to each data set as  SDG ,  CC , and  Repair , respectively.",
            "We present our models results on Verilog benchmarks tested with temperatures 0.2 and 0.8. We ablate across different data blends, with  SDG  indicating using LLM synthetic generated data in  Section   2.1 ,  CC  indicating correct-by-construction data targeting non-textual representations in  Section   3.1 , and  Repair  representing our targed code repair dataset in  Section   3.2 .",
            "As shown in Table  12 , a carefully filtered dataset of 1.4k samples achieves comparable performance to a 7.8k dataset. This suggests that merely increasing the dataset size by injecting the same types of errors does not contribute meaningfully to improving model performance."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Data quantity CC.",
        "table": "S3.T3.1.1",
        "footnotes": [],
        "references": [
            "We generate Verilog code problems and solutions that are correct-by-construction. Our focus is on creating problems and solutions for non-textual representations. The overview of the process for generating non-textual problems and correct-by-construction solutions is illustrated in  Figure   2 .  Table   3  shows the quantity of our correct-by-construction generation data (referred to as CC). To prevent data contamination, we exclude entries that duplicate the data representations of benchmark problems.",
            "Figure   3  illustrates our approach for generating state transition logic in Verilog from a state-transition graph. Our method predominantly employs an out-edge focused strategy for state transitions. Additionally, we incorporate in-edge focused transition logic to address specific challenges encountered in benchmark problems. These benchmarks often involve states represented using one-hot encoding and require rigorous testing of non-default states.",
            "Our fine-tuning training data is comprised of 80.1k LLM synthetic generated data using various prompting methods as described in  Section   2.1 , 28.5k data samples generated correct-by-construction aimed at non-textual representations detailed in  Section   3.1 , and 1.4k carefully filtered data for targeted code repair as outlined in  Section   3.2 . We refer to each data set as  SDG ,  CC , and  Repair , respectively.",
            "We present our models results on Verilog benchmarks tested with temperatures 0.2 and 0.8. We ablate across different data blends, with  SDG  indicating using LLM synthetic generated data in  Section   2.1 ,  CC  indicating correct-by-construction data targeting non-textual representations in  Section   3.1 , and  Repair  representing our targed code repair dataset in  Section   3.2 .",
            "We conduct a second iteration by generating 2.7k repair data for the model based on the  Repair  data from the first iteration. As shown in   Table   13 , performance mostly saturates after this initial iteration. We suspect that the remaining issues are likely due to significant errors that are challenging to correct."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  We compare our models with various baseline models on VerilogEval  (Liu et al.,  2023b ) . We update the results from  Zhao et al. ( 2024 )  with the latest foundational and frontier code models. The  best  results for each model type are highlighted in bold. We use  red  to denote the best model overall and  blue  to indicate the best results among models other than ours.",
        "table": "S4.T4.7.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "In this section, we start with a thorough analysis of fine-tuned large language models (LLMs) applied to Verilog code. We adapt previous approaches for generating synthetic data for general coding to focus on Verilog code. For our pilot study, we only present results based on fine-tuning StarCoder2-15B  (Lozhkov et al.,  2024 ) . Details on experimental settings are the same as in  Section   4 . We assess model performance in Verilog code completion and identify two main issues. First, the models demonstrate notably poor performance when dealing with non-textual elements in the problem statements. Second, the variability in the models pass rates across different benchmark problems and training checkpoints suggests inconsistencies in learning outcomes and model variability.",
            "Table   4  and  Table   5  compare our models with baselines on VerilogEval and RTLLM. We mainly source baseline results from  Zhao et al. ( 2024 ) . For RTLLM we found a large variance with biased pass@5, thus we re-evalaute all models and report unbiased pass@k metric. We further rigorously evaluate the latest foundational and frontier code models, including Llama-3.1  (Dubey et al.,  2024 ) , DeepSeek-Coder-V2  (DeepSeek-AI et al.,  2024 ) , and GPT-4o. Recent foundational and frontier code models already reached competitive performance compared to previous efforts targeting Verilog code generation.",
            "We assess the diversity of the code generated by our models. We measure this diversity using BLEU score, Jaccard similarity, and abstract tree edit distance (TSED) in  Song et al. ( 2024 ) . The VerilogEval-Human problems are categorized into NonText and Text, as described in  Section   A.4 . For each problem, we compute the average code diversity score across sampled codes for the same problem and report the mean score for all problems. For TSED, we use PyVerilog  (Takamaeda-Yamazaki,  2015 )  to extract the abstract syntax tree, and codes that fail syntax checks are excluded from the analysis.",
            "Table   14  presents the results on code diversity. We sample 20 solutions with temperature of 0.8 for each model. We observe that fine-tuned models generally show a decrease in code diversity for both Text and NonText problems. This reduction is expected, as BLEU and Jaccard metrics account for both correct and incorrect code solutions, and there are often multiple ways to implement a correct solution. When comparing our fine-tuned models with GPT-4o, code diversity is similar for Text problems, but our models exhibit poor diversity for NonText problems. This is anticipated, given that the  CC  training dataset for NonText problems is generated using correct-by-construction methods and follows similar templates for Verilog code. However, our models demonstrate comparable diversity to GPT-4o for Text problems, particularly in TSED metric."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Evaluations on RTLLM v1.1  (Lu et al.,  2024 )  using unbiased pass@k metrics. The  best  results for each model type are highlighted in bold. We use  red  to denote the best model overall and  blue  to indicate the best results among models other than ours. We re-evaluate all models (see  Appendix   A  for details).",
        "table": "S4.T5.7.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Table   4  and  Table   5  compare our models with baselines on VerilogEval and RTLLM. We mainly source baseline results from  Zhao et al. ( 2024 ) . For RTLLM we found a large variance with biased pass@5, thus we re-evalaute all models and report unbiased pass@k metric. We further rigorously evaluate the latest foundational and frontier code models, including Llama-3.1  (Dubey et al.,  2024 ) , DeepSeek-Coder-V2  (DeepSeek-AI et al.,  2024 ) , and GPT-4o. Recent foundational and frontier code models already reached competitive performance compared to previous efforts targeting Verilog code generation.",
            "Figure   5  displays the pass rates for two consecutive checkpoints of Starcoder2-SDG-CC-Repair on VerilogEval-Human problems, sampled with a temperature of 0.8. Compared to  Figure   1 , the updated model shows significant improvements by (1) moving previously unsolved problems into the solved category, including those with non-textual representations addressed by our correct-by-construction  CC  data, and (2) reducing the number of problems with large pass rate discrepancies, particularly where performance had degraded. The targeted repair data has effectively mitigated the models tendency to repeat common mistakes found in our  Repair  dataset, despite the noise inherent in synthetically generated  SDG  data."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Ablation study on training data.",
        "table": "S4.T6.1.1",
        "footnotes": [],
        "references": [
            "Table   6  highlights the effectiveness of our generated data fine-tuned on Starcoder2-15B. Our  CC  data enhances the models ability to handle non-textual representations, leading to improved scores on VerilogEval-Human. Our targeted code  Repair  data boosts performance across all benchmarks, suggesting that the model has learned to generalize from code repair tasks and reduce similar errors during code completion.",
            "Figure   6  illustrates the scaling of correct-by-construction ( CC ) data and the fine-tuned Starcoder2-15B pass rate on problems involving non-textual representations. We expanded our testing to include strictly in-distribution test set, with each category containing around 50 problems. The results show that the model can quickly learn and comprehend these non-textual representations with as few as 4k training data samples, with the pass rate steadily improving as more data is provided. Additionally, the model demonstrates the ability to generalize to VerilogEval-NonText benchmark problems. While our models achieve near-perfect scores on KMap and FSM problems, they perform less effectively on Waveforms, suggesting that reverse engineering circuits from waveforms pose a greater challenge."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Ablation study on  Repair  data quality with Starcoder2-15B.",
        "table": "S4.T7.3.1",
        "footnotes": [],
        "references": [
            "The quality control mechanisms integrated into the data generation pipeline are crucial for improving model performance, particularly in correcting minor errors through targeted code repair. To evaluate the impact of these quality controls, we conducted an ablation study in  Table   7 ), where we systematically removed each component of the targeted code repair generation pipeline and assessed the resulting model performance. Specifically, we eliminated the self-consistency checks that validate whether the generated error report effectively guides the LLMs in correcting mistakes. Additionally, we tested the removal of the error report entirely, substituting it with random errors injected into the open-source code by the LLMs. The benchmark results indicate a significant performance drop when these validation processes are excluded. These findings highlight the essential role of both the self-consistency checks and the targeted error report in improving the models ability to correct errors."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Results for our models, across different dataset and temperature on VerilogEval.",
        "table": "A1.T8.1.1",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_9": {
        "caption": "Table 9:  Results for our models, across different dataset and temperature on RTLLM.",
        "table": "A1.T9.1.1",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_10": {
        "caption": "Table 10:  Results on foundational and code models on VerilogEval.",
        "table": "A1.T10.1.1",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_11": {
        "caption": "Table 11:  Results on foundational and code models on RTLLM.",
        "table": "A1.T11.1.1",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_12": {
        "caption": "Table 12:  Scaling  Repair  data.",
        "table": "A1.T12.3.1",
        "footnotes": [],
        "references": [
            "As shown in Table  12 , a carefully filtered dataset of 1.4k samples achieves comparable performance to a 7.8k dataset. This suggests that merely increasing the dataset size by injecting the same types of errors does not contribute meaningfully to improving model performance."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:  Iterative code repair.",
        "table": "A1.T13.1.1",
        "footnotes": [],
        "references": [
            "We conduct a second iteration by generating 2.7k repair data for the model based on the  Repair  data from the first iteration. As shown in   Table   13 , performance mostly saturates after this initial iteration. We suspect that the remaining issues are likely due to significant errors that are challenging to correct."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  Diversity of generated code solutions on VerilogEval-Human sampled with temperature of 0.8. Lower scores indicate higher diversity.",
        "table": "A1.T14.1.1",
        "footnotes": [],
        "references": [
            "Table   14  presents the results on code diversity. We sample 20 solutions with temperature of 0.8 for each model. We observe that fine-tuned models generally show a decrease in code diversity for both Text and NonText problems. This reduction is expected, as BLEU and Jaccard metrics account for both correct and incorrect code solutions, and there are often multiple ways to implement a correct solution. When comparing our fine-tuned models with GPT-4o, code diversity is similar for Text problems, but our models exhibit poor diversity for NonText problems. This is anticipated, given that the  CC  training dataset for NonText problems is generated using correct-by-construction methods and follows similar templates for Verilog code. However, our models demonstrate comparable diversity to GPT-4o for Text problems, particularly in TSED metric."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A1.T14.2.1",
        "footnotes": [],
        "references": []
    }
}