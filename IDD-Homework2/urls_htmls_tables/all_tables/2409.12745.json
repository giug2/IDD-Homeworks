{
    "id_table_1": {
        "caption": "Table 1 :  Results obtained on GSC test set from training a set of MatchboxNet models with real and synthetic data. Each system was trained 5 times using different random seeds, the mean and standard deviation values are presented. Literature results are also reported for reference, but they are not directly comparable since they use an additional silence class.",
        "table": "S2.T1.8",
        "footnotes": [
            "",
            "",
            ""
        ],
        "references": [
            "Given the end-to-end nature of XTTS v2, that can oftentimes produce hallucinations and artefacts, we introduce an ASR filtering scheme, illustrated in Fig.  1 . We keep audio snippets for which the two transcriptions obtained with two distinct large vocabulary ASR systems exactly match the word of interest. The two ASR systems are a fast-conformer transducer-based and a Jasper CTC-based models, both trained for English and available off-the-shelf within the NeMO toolkit  [ 19 ] . Using two ASR systems improved the filtering, compared to using a single one.",
            "Matchboxnet models were trained on three different subsets: either the original speech commands corpus ( Real ), or the synthetic version ( Synth ), or the synthetic version generated with ASR-based filtering ( Synth. (F) ). A total of 50 training epochs with early stopping, a batch size of 128, a dropout of 0.25 and a cosine annealing learning scheduler, starting at 5e-3 and finishing at 5e-12, were used as main training hyperparameters. At each epoch, our models were validated using the real GSC validation set. Furthermore, all our models were similarly tested on the original test set. Accuracy values are reported in Table  1 . As expected, the best results were obtained with the original GSC training data ( Real ), with accuracy values above 98%. Results obtained in the  Synth.  condition, using synthetic data for training, achieved accuracy values of 89-90%. The ASR-based filtering technique led to an accuracy gain of over two percentage points on MatchboxNet models, demonstrating that uncontrolled hallucinations in synthetic training data can contaminate the dataset and degrade model performance. A significant gap still remains between training high-performing models with real, original, speech data and purely synthetic data, generated with XTTS v2 and voice cloning. We may compare these results to other approaches in the literature using synthetic data, namely  [ 17 ] , with the limitation that they used the additional silence class, while we did not. Nevertheless, while the accuracy results remain quite similar ( 92.6%), our largest Matchboxnet model (134 k) is roughly one third smaller than their model sizes, in number of parameters.",
            "The CycleGAN is optimized using a cycle-consistency loss function  L C  C subscript L C C \\mathcal{L}_{CC} caligraphic_L start_POSTSUBSCRIPT italic_C italic_C end_POSTSUBSCRIPT   [ 26 ]  (see Eq.  1 ) that combines the discriminator and generator loss functions (MSE loss function, see equation  2 ) and a cyclic and identity loss with their respective weights. The cycle loss calculates the full reconstruction loss of the same input, enforcing consistency (eq.  3 ). Contrarily, the identity loss encourages the generator to produce outputs that are close to the inputs when the inputs are already in the target domain (Eq.  4 ).",
            "Similarly to section  4.1 , linear classifiers are trained using the same hyperparameters as before, this time using the pre-trained domain adaptation provided by the CycleGAN, with a frozen Generator A. The results, reported in Table  2 , show a small but noticeable improvement in accuracy when using our CycleGAN versus the system trained without this domain adaptation."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Results obtained on the GSC test set from training a set of WavLM-based linear classifiers using real data, synthetic data (without and with filtering) and synthetic data transformed with our CycleGAN. Each system was trained 5 times using different random seeds, the mean and standard deviation values are presented",
        "table": "S4.T2.4",
        "footnotes": [],
        "references": [
            "We trained the classification layer (WavLM-Base-Plus was frozen) over 30 epochs with a fixed learning rate of 5e-3 and a batch size of 128. As we did with the MatchboxNet models, we compare the three scenarios, using either the original GSC speech training data, or synthetic speech, without and with ASR-based filtering. The results, reported in table  2 , show that the performance gap between training a model either on real data or on the filtered synthetic speech has been reduced, compared to the previous experiments using MatchboxNet and MFCCs: 96.11% ( Synth.(F) )  vs.  98.03% ( Real ). The  Synth.(F)  SSL-based linear classifier is also much better that its MatchboxNet counterpart, with an absolute gain of 3.5% in accuracy. Interestingly, the results obtained when using the synthetic data without filtering ( Synth. ) displayed a subpar accuracy of around 83%, showing that hallucinated speech has a negative impact stronger when using SSL features compared to MFCCs.",
            "In order to get some insights between GSC real speech and our synthesized speech material, we visualize, in Fig.  2 ), a 2-d dimension reduction through PCA over two sets of features: i) the 64 Mel-Frequency Cepstral Coefficients (MFCCs), as used by the MatchboxNet models, ii) the 768-d WavLM-Base-Plus feature vector representations,  [ 24 ] . In both cases, statistic pooling over the time dimension was used to reduce sequences into single vectors. The MFCC plot (left plot) shows two superposed clusters, hinting that real speech is less clustered and more scattered than real speech. Surprisingly, with the SSL features, the two data points are much less superposed (right plot). They seem to even be linearly separable, showing that SSL features, even reduced to two dimensions, could be reliably used to differentiate between real and synthetic speech material. Given the impressive results witnessed when using SSL features on different tasks such as speech classification and KWS  [ 23 ] , this clear separation in the two dimensional PCA space comes as unexpected. We draw the hypothesis that domain adaptation between the two types of speech material, in the SSL latent space, could help reduce the remaining performance gap in our experiments.",
            "The CycleGAN is optimized using a cycle-consistency loss function  L C  C subscript L C C \\mathcal{L}_{CC} caligraphic_L start_POSTSUBSCRIPT italic_C italic_C end_POSTSUBSCRIPT   [ 26 ]  (see Eq.  1 ) that combines the discriminator and generator loss functions (MSE loss function, see equation  2 ) and a cyclic and identity loss with their respective weights. The cycle loss calculates the full reconstruction loss of the same input, enforcing consistency (eq.  3 ). Contrarily, the identity loss encourages the generator to produce outputs that are close to the inputs when the inputs are already in the target domain (Eq.  4 ).",
            "Our CycleGAN is trained over 200 epochs using two data batches, one comprised of synthetic data and one of real data, using the 25 words of GSC classified as  unknown  (see  2.2 ). The real and synthetic mini-batches are drawn randomly and independently. The system is optimized with a learning rate of 1e-5, a batch size of 128, a lambda cycle (  c  y  c subscript  c y c \\lambda_{cyc} italic_ start_POSTSUBSCRIPT italic_c italic_y italic_c end_POSTSUBSCRIPT ) of 10.0 and a lambda identity (  i  d subscript  i d \\lambda_{id} italic_ start_POSTSUBSCRIPT italic_i italic_d end_POSTSUBSCRIPT ) value of 0.5. Since our main goal is to make synthetic speech representations closer to real ones, only the generator A is used to perform the domain transformation, as illustrated in Fig.  3 .",
            "Similarly to section  4.1 , linear classifiers are trained using the same hyperparameters as before, this time using the pre-trained domain adaptation provided by the CycleGAN, with a frozen Generator A. The results, reported in Table  2 , show a small but noticeable improvement in accuracy when using our CycleGAN versus the system trained without this domain adaptation.",
            "Despite advancements in the quality of modern TTS systems, PCA analysis on MFCCs  and even more so on SSL features (Fig.  2 )  revealed noticeable differences in cluster size and position between synthetic and real speech data. This indicates that synthetic speech still lacks the variety and diversity inherent in real speech, but also a mismatch between the feature representation spaces, at least with SSL features. While the literature and our experiments show good performance when using SSL speech representations for our task, it was surprising to observe such a distinct separation between synthetic and real speech data. Despite the CycleGAN methodology promoting a small performance gain, a number of questions appear, left for future work. We aim to conduct an in-depth analysis of the SSL features to identify which specific features are most relevant in distinguishing between synthetic and real speech data. We could potentially exclude them in our downstream application, thereby aligning it more closely with real data characteristics. A deeper analysis of the domain adaptation actually achieved by the CycleGAN is essential to evaluate how closely the transformed data distribution aligns with the real data. The modest performance gain obtained with CycleGAN adaptation suggests that the distributions were brought closer together. However, we would like to further quantify this alignment more rigorously. Additionally, exploring other recent domain adaptation methods, such as Flow Matching  [ 27 ] , could provide further insights and potentially enhance the adaptation process."
        ]
    }
}