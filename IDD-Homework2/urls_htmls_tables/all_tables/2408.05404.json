{
    "id_table_1": {
        "caption": "Table 1:  The statistics of MCI dataset.",
        "table": "S4.T1.1",
        "footnotes": [],
        "references": [
            "As a significant phenomenon in natural language, metaphors are prevalent in daily communication, literary works, and scientific discourse. As illustrated in Fig. 1 , Metaphor Components Identification (MCI), which involves accurately identifying and interpreting the components of metaphors (typically including the tenor, vehicle, and ground), holds substantial value for various applications in natural language processing (NLP), such as sentiment analysis, information retrieval, and machine translation  [ 5 ,  13 ,  17 ] . However, the complexity and diversity of metaphors, along with their reliance on context and background knowledge, present significant challenges.",
            "We conduct experiments using the dataset provided by NLPCC2024 Shared Task 9  [ 14 ]  Subtask 2: Metaphor Components Identification 3 3 3 https://github.com/xingweiqu/NLPCC-2024-Shared-Task-9 , which is divided into training, validation, and test sets. The statistical details are shown in Table  1 , where  Single  and  Multiple  refer to the proportion of metaphorical sentences containing single and multiple groups of metaphor components in the dataset, respectively.  Avg.Sub-sents  indicates the average number of sub-sentences in a single metaphorical sentence, with sub-sentences separated by commas or spaces.  Simile  indicates the proportion of simile sentences in the dataset. The training set is selected from the dataset proposed by Shao et al. [ 15 ]  in a ratio of  Single:Multiple = 9:1 . The dataset originates from various highly refined literary forms, including poetry, prose, lyrics, etc., which are not conducive to machine understanding. Notably, illustrated in Table  1 , the expression forms of metaphorical sentences in the validation and test sets differ from those in the training set, being more simplistic. Therefore, during data augmentation pre-training, we applied the Chinese simile corpus 4 4 4 https://github.com/cnunlp/Chinese-Simile-Recognition-Dataset  created by Liu et al. [ 11 ]  to familiarize the model with simpler metaphors and to partially bridge the gap between the training and test sets."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The overall performance of all the compared baselines and our LaiDA.",
        "table": "S4.T2.1",
        "footnotes": [],
        "references": [
            "In the following, we introduce our proposed LaiDA framework. As shown in Fig. 2 , the LaiDA framework consists of four modules: data preprocessing ( 3.2 ), data augmentation pre-training ( 3.3 ), graph attention network encoder ( 3.4 ), and task fine-tuning ( 3.5 ).",
            "As shown in Fig. 2 , the GAT encoder can be subdivided into three units: the construction of positive and negative sample matrices for linguistic features, the graph attention network, and contrastive learning. Each of these units is described in detail below:",
            "In this section, all previous modules are integrated to fine-tune for MCI. The training data constructed from the data preprocessing module ( 3.2 ) is encoded by the GAT encoder ( 3.4 ) into sentence representation rich in linguistic features, and FAISS (Facebook AI Similarity Search) 2 2 2 https://github.com/facebookresearch/faiss  is used to perform approximate nearest neighbor search within the training set, based on these sentence vectors, to retrieve three linguistically similar examples. Subsequently, we followed the instructions that integrated the prompt (Fig.  3 , right panel) and similar examples, using the pre-trained model obtained from the data augmentation pre-training module ( 3.3 ) as the base model to further fine-tune the LLM.",
            "Main Results  The overall performance of all the compared baselines and LaiDA is compared on the test set and reported in Table  2 . LaiDA achieves the highest accuracy compared to several baselines, demonstrating its effectiveness. Interestingly, the application of pre-trained BERT  [ 2 ]  for Euclidean distance similarity retrieval did not outperform random retrieval. The emphasis on similarity and diversity in selecting in-context examples might explain LLM-BERTs underwhelming performance versus LLM-Random, potentially stemming from Euclidean distances inadequacy for similarity assessment and the broader diversity inherent in LLM-Random. Notably, LLM-Mix-tuning, leveraging multi-task learning, exhibits commendable results, partially attributed to the metaphor analysis prior to component selection, which augments the model. Nevertheless, despite doubling the training data, the enhancement is modest.",
            "Ablation Study  To investigate the role of data augmentation and in-context learning in LaiDA, this section presents the results of ablation experiments. As shown in Table  2 , removing the data augmentation pre-training module results in a 0.9% decrease in accuracy. Similarly, removing in-context learning by not embedding any examples in the prompt during supervised fine-tuning leads to a 2.6% decrease. These results confirm the effectiveness of both data augmentation pre-training and in-context learning in LaiDA."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Performance of LaiDA on individual metaphor component.",
        "table": "S5.T4.fig1.1",
        "footnotes": [],
        "references": [
            "In the following, we introduce our proposed LaiDA framework. As shown in Fig. 2 , the LaiDA framework consists of four modules: data preprocessing ( 3.2 ), data augmentation pre-training ( 3.3 ), graph attention network encoder ( 3.4 ), and task fine-tuning ( 3.5 ).",
            "Guided by the prompt shown in the left figure of Fig.  3 , ChatGPT 1 1 1 https://chatgpt.com/  is utilized to generate three distractor triplets  O 1  , O 2  , O 3  subscript superscript O  1 subscript superscript O  2 subscript superscript O  3 O^{\\prime}_{1},O^{\\prime}_{2},O^{\\prime}_{3} italic_O start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_O start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_O start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT  in a small sample dataset that is drawn from the curated original dataset. Subsequently, a dataset  D s  a  m  p  l  e  d subscript D s a m p l e d \\mathbbm{D}_{sampled} blackboard_D start_POSTSUBSCRIPT italic_s italic_a italic_m italic_p italic_l italic_e italic_d end_POSTSUBSCRIPT  based on these processed samples is constructed, and a smaller LLM than ChatGPT is fine-tuned on  D s  a  m  p  l  e  d subscript D s a m p l e d \\mathbbm{D}_{sampled} blackboard_D start_POSTSUBSCRIPT italic_s italic_a italic_m italic_p italic_l italic_e italic_d end_POSTSUBSCRIPT . The fine-tuned model is used to comprehensively process the remaining training data, organizing each datum into a metaphorical sentence with four metaphor component triplet options. Furthermore, randomization is implemented to shuffle the four options, ensuring the correct triplet is randomly distributed among the A, B, C, and D.",
            "The data augmentation pre-training module is aimed at enhancing the models ability to recognize metaphors by learning their features and patterns in advance. We employed the metaphor dataset by Liu et al.  [ 11 ] , annotated with the tenors and vehicles of metaphorical sentences, to fine-tune the LLM under the guidance of the middle prompt depicted in Fig. 3 . While the dataset focuses on similes, a relatively straightforward form of metaphor, it provides valuable initial exposure to metaphors.",
            "In this section, all previous modules are integrated to fine-tune for MCI. The training data constructed from the data preprocessing module ( 3.2 ) is encoded by the GAT encoder ( 3.4 ) into sentence representation rich in linguistic features, and FAISS (Facebook AI Similarity Search) 2 2 2 https://github.com/facebookresearch/faiss  is used to perform approximate nearest neighbor search within the training set, based on these sentence vectors, to retrieve three linguistically similar examples. Subsequently, we followed the instructions that integrated the prompt (Fig.  3 , right panel) and similar examples, using the pre-trained model obtained from the data augmentation pre-training module ( 3.3 ) as the base model to further fine-tune the LLM."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Error analysis in LaiDAs results. The error types are specifically combined into seven types.",
        "table": "S5.T4.fig2.1",
        "footnotes": [],
        "references": [
            "In the following, we introduce our proposed LaiDA framework. As shown in Fig. 2 , the LaiDA framework consists of four modules: data preprocessing ( 3.2 ), data augmentation pre-training ( 3.3 ), graph attention network encoder ( 3.4 ), and task fine-tuning ( 3.5 ).",
            "In this section, all previous modules are integrated to fine-tune for MCI. The training data constructed from the data preprocessing module ( 3.2 ) is encoded by the GAT encoder ( 3.4 ) into sentence representation rich in linguistic features, and FAISS (Facebook AI Similarity Search) 2 2 2 https://github.com/facebookresearch/faiss  is used to perform approximate nearest neighbor search within the training set, based on these sentence vectors, to retrieve three linguistically similar examples. Subsequently, we followed the instructions that integrated the prompt (Fig.  3 , right panel) and similar examples, using the pre-trained model obtained from the data augmentation pre-training module ( 3.3 ) as the base model to further fine-tune the LLM.",
            "Individual Metaphor Component Results  As shown in Table  4 , LaiDA achieves an accuracy above 97% in identifying the Tenor (97.20%) and Vehicle (97.32%), demonstrating its excellent capability in capturing the core elements of metaphors, attributed to their directness and clarity, which facilitate model detection. However, the accuracy in identifying the Ground (94.14%) is slightly lower, indicating its higher difficulty. The Ground, serving as the bridge in metaphors, requires stronger contextual understanding and reasoning abilities from the model due to its subtle and abstract nature, highlighting it as a key area for future optimization.",
            "Error Analysis  Despite LaiDAs remarkable 93.21% accuracy in MCI, a detailed error analysis is undertaken to bolster its performance, as presented in Table  4 . It reveals that the only ground errors are as high as 35.94%, indicating their obscure and variable nature within contexts, consistent with the phenomena reflected in Table  4 . Notably, in samples with tenor or vehicle errors (including T, V, T-G, V-G, T-V-G), ground errors reach 87.89%. More specifically, ground errors account for 82.15% in tenor error samples, while this escalates to 92.61% in vehicle errors, suggesting that tenor or vehicle misidentifications exacerbate ground identification difficulties, with vehicle errors exerting a more profound influence. Grounds, as connectors between tenor and vehicle, rely heavily on their accurate identification. Therefore, LaiDAs future advancements should prioritize enhancing contextual comprehension and refining tenor/vehicle detection for improved ground identification."
        ]
    },
    "global_footnotes": []
}