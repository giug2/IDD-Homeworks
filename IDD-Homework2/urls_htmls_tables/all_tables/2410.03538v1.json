{
    "S2.T1": {
        "caption": "Table 1. Results of online A/B experiments, measured by Active Days and Play Count. Each row indicates the relative improvement with our method over the online baseline, which already includes the SIM (Search-based user Interest Model) model <cite class=\"ltx_cite ltx_citemacro_citep\">(Pi et&#160;al<span class=\"ltx_text\">.</span>, <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.03538v1#bib.bib11\" title=\"\">2020</a>)</cite>. Statistically significant improvement is marked with bold font in the table (p-value &#161; 5%).\n(Pi et\u00a0al., <a class=\"ltx_ref\" href=\"https://arxiv.org/html/2410.03538v1#bib.bib11\" title=\"\">2020</a>)\n2020). Statistically significant improvement is marked with bold font in the table (p-value \u00a1 5%).",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_align_middle\" id=\"S2.T1.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T1.1.1.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.1.1.1\">Method</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S2.T1.1.1.1.2\">Platform A</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" colspan=\"2\" id=\"S2.T1.1.1.1.3\">Platform B</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.2.2\">\n<td class=\"ltx_td\" id=\"S2.T1.1.2.2.1\"/>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.2.2.2\">Active Days</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.2.2.3\">Play Count</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.2.2.4\">Active Days</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S2.T1.1.2.2.5\">Play Count</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.3.3.1\">DreamUMM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.3.3.2\">0.003%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.3.3.3.1\">0.273%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.3.3.4\">0.000%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T1.1.3.3.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.3.3.5.1\">0.287%</span></td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T1.1.4.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.1.4.4.1\">Candidate-DreamUMM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.1.4.4.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.4.4.2.1\">0.037%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.1.4.4.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.4.4.3.1\">0.867%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.1.4.4.4\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.4.4.4.1\">0.050%</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T1.1.4.4.5\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T1.1.4.4.5.1\">0.318%</span></td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "RQ1: User Engagement. Table 1 presents the relative improvements of DreamUMM and Candidate-DreamUMM over the control group in terms of Play Count and Active Days on both platforms. We observe significant gains in both metrics, indicating the effectiveness of our methods in enhancing user engagement. Candidate-DreamUMM consistently outperforms DreamUMM, suggesting its superior ability to capture users\u2019 real-time interests by focusing on the current context. The lifts in Play Count and Active Days demonstrate that our methods can effectively encourage users to consume more videos and visit the platform more frequently."
        ]
    },
    "S2.T2": {
        "caption": "Table 2. The results of our offline video retrieval benchmark demonstrate that our new representation, enhanced by the distillation process of a MLLM, has achieved significant and consistent improvements in precision. Specifically, \u2019HitRate@100\u2019 indicates the mean precision for the top 50 recall videos across a set of 100 query videos, while \u2019HitRate@200\u2019 applies this metric to an expanded set of 200 query videos, underscoring the robustness and reliability of our approach in enhancing retrieval accuracy.",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S2.T2.1\">\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S2.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T2.1.1.1.1\">Method</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T2.1.1.1.2\">HitRate@100</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t\" id=\"S2.T2.1.1.1.3\">HitRate@200</th>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T2.1.1.1.4\"/>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.2.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.2.2.1\">Representation/wo.MLLM</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.2.2.2\">0.730</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S2.T2.1.2.2.3\">0.678</td>\n<td class=\"ltx_td ltx_border_t\" id=\"S2.T2.1.2.2.4\"/>\n</tr>\n<tr class=\"ltx_tr\" id=\"S2.T2.1.3.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T2.1.3.3.1\">Representation/w.MLLM(ours)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T2.1.3.3.2\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.1.3.3.2.1\">0.742</span></td>\n<td class=\"ltx_td ltx_align_center ltx_border_b ltx_border_t\" id=\"S2.T2.1.3.3.3\"><span class=\"ltx_text ltx_font_bold\" id=\"S2.T2.1.3.3.3.1\">0.688</span></td>\n<td class=\"ltx_td ltx_border_b ltx_border_t\" id=\"S2.T2.1.3.3.4\"/>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "RQ3: Representation Quality. Table 2 presents the HitRat@100 and HitRat@200 of our model with MLLM-based representation learning and a variant without MLLM pre-training. Our full model achieves a HitRate@100 of 0.742 and a HitRate@200 of 0.688, significantly outperforming the variant without MLLM pre-training. This demonstrates the effectiveness of leveraging the knowledge encoded in the MLLM to learn informative video representations that align well with human judgments of content similarity."
        ]
    }
}