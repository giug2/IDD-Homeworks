{
    "id_table_1": {
        "caption": "Table 1:  Summary of  temperature-tuned coding benchmark results for LMs with   0.2 absent 0.2 \\mathbf{\\leq 0.2}  bold_0.2 B parameters . Scores annotated with (   \\dagger  ) indicate external model evaluations that we ran using the procedure described in Appendix  B , and all other scores are as reported by model authors. We list models in order of increasing HumanEval pass@1 score.",
        "table": "S3.T1.7",
        "footnotes": [
            ""
        ],
        "references": [
            "Tiny edit sequence LMs have state-of-the-art performance in their model class (Table  1 ).",
            "For smaller LLMs, repeatedly sampling from edit sequence models results in HumanEval coverage that is competitive with GPT-4 models and has similar cumulative cost to sampling once per problem from open-source LLMs like Llama 3.1 405B (Figures  1  and  4 ).",
            "Recall from above that a single program edit computed by the  diff  operator    (  ,  )    \\Delta(\\cdot,\\cdot) roman_ (  ,  )  can consist of any number of deletions and insertions. LintSeq is an algorithm for computing edit sequence re-factorings  D  superscript D  \\mathcal{D^{\\prime}} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  such that all data  ( x ,  y )  D  x subscript  y superscript D  (x,\\bm{\\delta}_{y})\\in\\mathcal{D^{\\prime}} ( italic_x , bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )  caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  have a particular property: every edit in   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  consists of  insertions only . There are two phases in LintSeq: a backward sampling phase that is used to compute program state sequences   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , and a forward edit sequence computation phase that is used to re-express   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  as edit sequences   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT . An illustration of these phases is shown in Figure  2 . Full examples of edit sequences generated with LintSeq are provided in Appendix  E  (Figures  9  and  10 ).",
            "Similar to previous works  (Chen et al.,  2021 ) , we evaluate models by computing zero-shot coverage statistics on code synthesis benchmarks with and without repeated sampling. We expound upon our motivation for evaluating models in this manner in Appendix  B.3.1 .",
            "Using both the code edit re-factorized and baseline instruction datasets obtained in section  3.2 , we run pairs of finetuning experiments with six different models. In each experiment pair, we finetune an LM on both datasets for an equal number of optimizer steps and with the same learning rate schedule, saving intermediate checkpoints throughout finetuning. We run full benchmark evaluations on HumanEval and MBPP on each saved checkpoint 1 1 1 To process the generations of edit sequence LMs into executable programs, we simply resolve each of the predicted code edits one-by-one. This procedure is visualized in Figure  1  and described in Appendix  A.2 . , performing no prompt tuning. Then, we compare the best inference-time scaling behavior, i.e. benchmark coverage (pass@k) as a function of samples (k), obtained by edit sequence finetuning vs standard finetuning for each model. A more detailed description of the computed metrics as well as a full specification of the evaluation and finetuning procedures is provided in Appendices  B  and  D .",
            "We run our first two pairs of finetuning experiments on  TinyCodeLM-150M  and  TinyCodeLM-400M . These models were not pretrained on code synthesis instruction data, nor were they pretrained on any diff-like edit data. Our experimental results are summarized in Tables  1  and  2 , where we compare the temperature-tuned performance of our models to the reported benchmark coverage of existing code LMs at similar parameter scales. We also report the inference-time scaling of benchmark coverage as a function of samples for our finetuned models in Appendix Tables  8  and  9 .",
            "For both the 150M and 400M parameter versions of TinyCodeLM, we find that finetuning LMs to synthesize code with edits via LintSeq data dramatically improves benchmark performance compared to baseline finetuning. Indeed, the edit sequence variants of TinyCodeLM outperform all existing code language models of comparable scale that we are aware of, including AlphaCode  (Li et al.,  2022b ) , Codex  (Chen et al.,  2021 ) , CodeT5+  (Wang et al.,  2023b ) , and the recent SmolLM-Instruct  (Ben Allal et al.,  2024 ) . Our smaller edit sequence-finetuned model is particularly strong for its size, roughly matching or out-performing models with twice as many parameters including the 300M parameter version of Codex and the 302M-parameter version of AlphaCode (Tables  1  and  2 ).",
            "The results above raise a natural question: do performance improvements from finetuning LMs to synthesize code with edit sequences hold for other model scales, architectures, and tokenizers? To test this, we conduct four additional pairs of instruction finetuning experiments on LMs from three model families, Gemma 2, Phi-3, and Llama 3.1, ranging in size from 2.6B to 14B. We employ pretrained-only model weights, if available. The results of these experiments are in Figure  4 , where we plot zero-shot benchmark coverage as a function of samples for instruction finetuned models. Raw coverage scores are reported in Appendix Tables  10  and  11 .",
            "Most of our findings echo those of Section  3.3.1 . Aggregating across sample counts, we find that finetuning models to synthesize code with edits improves overall zero-shot performance on HumanEval and MBPP compared to finetuning on the original data. This suggests that re-factoring code with edit sequences is an architecture- and tokenizer-independent mechanism for improving downstream LM outputs. Furthermore, as shown in Figure  4  and Tables  10  and  11 , we find that the degree by which edit sequence LMs outperform baseline model variants increases with repeated sampling for all tested models, culminating in an average absolute gain in pass@50 of +20% (  plus-or-minus \\pm   3%) on HumanEval and +12% (  plus-or-minus \\pm   2%) on MBPP. This observation confirms the hypothesis posed in Section  2 , showing that training LMs to synthesize code with edits using LintSeq data improves the relationship between cumulative inference-time compute and zero-shot performance.",
            "At pass@1, however, our results are slightly more mixed than in Section  3.3.1 . For Phi-3 models, we observe either no difference or a decrease in score between each pair of model-data variants. One explanation for this is bias: the Phi-3 models have been previously instruction finetuned and were likely to have been trained on standard code data, putting LM variants finetuned to generate edit sequence re-factorized code at a comparative disadvantage due to distributional shift.",
            "Tiny LMs can be efficiently finetuned to synthesize Python programs with edit sequences via LintSeq data. This results in state-of-the-art code benchmark performance for models that can be trained on device (Sections  3.1  and  3.3.1 ).",
            "On HumanEval, the cumulative inference cost of repeatedly sampling from small edit sequence LLMs is similar to sampling once from larger LLMs and yields coverage that is competitive with GPT-4, GPT-4-Omni, and Llama 3.1 405B (Figure  1 , Appendix  F.4 ).",
            "Figure  1  shows a code edit sequence generation from a LintSeq instruction finetuned LM and the corresponding resolved, executable Python program.",
            "The primary goal of this paper is to introduce a method for re-factorizing code synthesis with LMs by finetuning them on synthetic instruction data. As a result, we evaluate all models using minimal prompt formats, performing no prompt tuning (see Figures  9  and  10 ). Examples of the prompt formats that we use during evaluation are shown in Figure  8 .",
            "From a practical perspective, while smaller language models may have weaker representational power than larger models, the representational expressivity of the former may enable them to overtake the latter at fixed computational budgets by leveraging extra compute at inference-time, e.g. generating a larger number of samples per problem and using the provided test cases to check each one for correctness before returning an output  (Brown et al.,  2024 ; Snell et al.,  2024 ) . For example, an LLM that has an 85 % percent \\% %  pass@1 score on an arbitrary task may be more expensive in total serving cost (see Figure  1 ) than a smaller LM with a  90 % percent 90 90\\% 90 %  pass@50 score on the same task. A small LM can only have this property, however, if it exhibits a reliable trade-off between generation quality and inference-time sampling cost across tasks. In other words, its representation must be sufficiently expressive.",
            "Our goal is to probe whether re-parameterizing code synthesis with edit sequences can improve the expressivity of smaller LLM representations, boosting benchmark coverage as a function of samples-per-problem. Hence, we primarily compare finetuned models by evaluating them with the procedures described above on HumanEval and MBPP(+) at a high temperature and a large sample count, computing pass@k for k  { 1 , 5 , 10 , 20 , 50 } absent 1 5 10 20 50 \\ \\in\\{1,5,10,20,50\\}  { 1 , 5 , 10 , 20 , 50 }  with  N = 50 N 50 N=50 italic_N = 50  samples 2 2 2 These are the largest sample counts that are feasible to compute on our hardware given the scope of our experiments.  at temperature 1, top-p 0.95. We compute pass@k statistics with the same procedure as  Chen et al. ( 2021 ) . The results of these evaluations are reported throughout the paper and shown in Figures  4 ,  5  and Tables  8 ,  9 ,  10 ,  11 . In each of these Figures and Tables, we identify the most performant checkpoint from each model-data finetuning run by comparing pass@50 score at temperature 1 on HumanEval and MBPP(+) across checkpoints.",
            "Many existing state-of-the-art code synthesis LMs only report temperature-tuned pass@k scores on HumanEval, including Codex, AlphaCode, and Codegen-Mono  (Chen et al.,  2021 ; Li et al.,  2022b ; Nijkamp et al.,  2022 ) . Thus, in Tables  1  and  2 , we select the single best overall checkpoint of each TinyCodeLM model by averaging HumanEval and MBPP(+) pass@50 score. Then, we temperature-tune each checkpoints pass@1 and pass@10 scores when reporting results. On HumanEval, we test temperatures    { 0.0 , 0.2 , 0.4 , 0.8 , 1.0 }  0.0 0.2 0.4 0.8 1.0 \\tau\\in\\{0.0,0.2,0.4,0.8,1.0\\} italic_  { 0.0 , 0.2 , 0.4 , 0.8 , 1.0 } . On MBPP(+), we sweep over a smaller temperature range,    { 0.0 , 0.1 , 1.0 }  0.0 0.1 1.0 \\tau\\in\\{0.0,0.1,1.0\\} italic_  { 0.0 , 0.1 , 1.0 } . We perform the same temperature tuning procedure when reporting external model benchmark scores as well, i.e. the scores annotated with  (  )  (\\dagger) (  )  in Tables  1  and  2 . When running benchmark evaluations with these external code LMs, we stray from the prompt formatting, generation, and parsing procedures described in Appendices  B.1  and  B.2 ; instead, in the interest of a fair evaluation, we reproduce the conventions reported by model authors to report other scores.",
            "Table  5  displays the data sources that are used to prepare the dataset described in Section  3.2 . These data are pooled and preprocessed into instruction-program pairs by stripping away Markdown formatting and natural language explanations from completions (Figure  9  and  10 ). In our experiments, we use the resultant data to finetune baseline models, comparing their performance to those of LMs finetuned on edit sequences generated with LintSeq from the same set of instruction-program pairs.",
            "In Figure  1 , we plot HumanEval coverage as a function of cumulative inference-time FLOPs, comparing the performance and total cost of repeatedly sampling from our instruction finetuned Phi-3 and Llama 3.1 models vs sampling a single generation per problem from very large models like Llama 3.1 405B  (Dubey et al.,  2024 )  and Nemotron 4 340B  (Adler et al.,  2024 ) .",
            "For our instruction finetuned models, we determine the quantities  T avg-total-tokens-per-sample subscript T avg-total-tokens-per-sample T_{\\text{avg-total-tokens-per-sample}} italic_T start_POSTSUBSCRIPT avg-total-tokens-per-sample end_POSTSUBSCRIPT  by computing token counts over all sets of samples per problem that we obtained to compute the coverage statistics in Figure  4  and Table  10  above. These token statistics are provided in the table below.",
            "We report zero-shot HumanEval coverage for external models by using the evaluation statistics from  Dubey et al. ( 2024 )  (Table 18, column one). To estimate cumulative inference-time FLOPs for these models, we employ the approximation expression above and estimate  T avg-total-tokens-per-sample  200 subscript T avg-total-tokens-per-sample 200 T_{\\text{avg-total-tokens-per-sample}}\\approx 200 italic_T start_POSTSUBSCRIPT avg-total-tokens-per-sample end_POSTSUBSCRIPT  200 , reflecting an ensemble over the per sample token counts of standard Instruct finetuned models shown in Table  12 . Note that this ensembled statistic reflects program generations without chain-of-thought only. As a result, we believe it to be a conservative estimator."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Summary of  temperature-tuned coding benchmark results for  0.2 0.2 \\mathbf{0.2} bold_0.2 B to  0.4 0.4 \\mathbf{0.4} bold_0.4 B parameter language models . Annotations, model order, and evaluation procedure are the same as in Table  1 .",
        "table": "S3.T2.9",
        "footnotes": [
            ""
        ],
        "references": [
            "Recall from above that a single program edit computed by the  diff  operator    (  ,  )    \\Delta(\\cdot,\\cdot) roman_ (  ,  )  can consist of any number of deletions and insertions. LintSeq is an algorithm for computing edit sequence re-factorings  D  superscript D  \\mathcal{D^{\\prime}} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  such that all data  ( x ,  y )  D  x subscript  y superscript D  (x,\\bm{\\delta}_{y})\\in\\mathcal{D^{\\prime}} ( italic_x , bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )  caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  have a particular property: every edit in   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  consists of  insertions only . There are two phases in LintSeq: a backward sampling phase that is used to compute program state sequences   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , and a forward edit sequence computation phase that is used to re-express   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  as edit sequences   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT . An illustration of these phases is shown in Figure  2 . Full examples of edit sequences generated with LintSeq are provided in Appendix  E  (Figures  9  and  10 ).",
            "Once  s s s italic_s  program state sequences   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  have been generated for each  ( x , y )  D x y D (x,y)\\in\\mathcal{D} ( italic_x , italic_y )  caligraphic_D , we run the forward edit computation phase of our algorithm. In this phase, we apply Equation  2  from above to compute an edit sequence   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  for each   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT . Starting from the last program that was added to   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , we use the  diff  operator to compute edits between each pair of consecutive programs in   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  up to the original program  y y y italic_y . Finally, we pair each edit sequence   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  with its instruction  x x x italic_x  (if present) to yield an edit sequence re-factoring  D  superscript D  \\mathcal{D}^{\\prime} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  of  D D \\mathcal{D} caligraphic_D  with size  s  N s N sN italic_s italic_N .",
            "With our baseline dataset prepared, we run LintSeq to generate  s = 5 s 5 s=5 italic_s = 5  synthetic edit trajectory samples for each instruction-program pair. As described in Section  2.4 , we concatenate each synthetic edit trajectory into a single string by interleaving consecutive edits with a special reserved edit token. Inspired by  Muennighoff et al. ( 2024 ) , we do not restrict against edit sequence repetitions. We use the popular Python linter  pylint  to guide edit sampling during generation. Examples of generated edit sequences and experiments testing the effect of varying  s s s italic_s  are in Appendix  E .",
            "Using both the code edit re-factorized and baseline instruction datasets obtained in section  3.2 , we run pairs of finetuning experiments with six different models. In each experiment pair, we finetune an LM on both datasets for an equal number of optimizer steps and with the same learning rate schedule, saving intermediate checkpoints throughout finetuning. We run full benchmark evaluations on HumanEval and MBPP on each saved checkpoint 1 1 1 To process the generations of edit sequence LMs into executable programs, we simply resolve each of the predicted code edits one-by-one. This procedure is visualized in Figure  1  and described in Appendix  A.2 . , performing no prompt tuning. Then, we compare the best inference-time scaling behavior, i.e. benchmark coverage (pass@k) as a function of samples (k), obtained by edit sequence finetuning vs standard finetuning for each model. A more detailed description of the computed metrics as well as a full specification of the evaluation and finetuning procedures is provided in Appendices  B  and  D .",
            "We run our first two pairs of finetuning experiments on  TinyCodeLM-150M  and  TinyCodeLM-400M . These models were not pretrained on code synthesis instruction data, nor were they pretrained on any diff-like edit data. Our experimental results are summarized in Tables  1  and  2 , where we compare the temperature-tuned performance of our models to the reported benchmark coverage of existing code LMs at similar parameter scales. We also report the inference-time scaling of benchmark coverage as a function of samples for our finetuned models in Appendix Tables  8  and  9 .",
            "For both the 150M and 400M parameter versions of TinyCodeLM, we find that finetuning LMs to synthesize code with edits via LintSeq data dramatically improves benchmark performance compared to baseline finetuning. Indeed, the edit sequence variants of TinyCodeLM outperform all existing code language models of comparable scale that we are aware of, including AlphaCode  (Li et al.,  2022b ) , Codex  (Chen et al.,  2021 ) , CodeT5+  (Wang et al.,  2023b ) , and the recent SmolLM-Instruct  (Ben Allal et al.,  2024 ) . Our smaller edit sequence-finetuned model is particularly strong for its size, roughly matching or out-performing models with twice as many parameters including the 300M parameter version of Codex and the 302M-parameter version of AlphaCode (Tables  1  and  2 ).",
            "Most of our findings echo those of Section  3.3.1 . Aggregating across sample counts, we find that finetuning models to synthesize code with edits improves overall zero-shot performance on HumanEval and MBPP compared to finetuning on the original data. This suggests that re-factoring code with edit sequences is an architecture- and tokenizer-independent mechanism for improving downstream LM outputs. Furthermore, as shown in Figure  4  and Tables  10  and  11 , we find that the degree by which edit sequence LMs outperform baseline model variants increases with repeated sampling for all tested models, culminating in an average absolute gain in pass@50 of +20% (  plus-or-minus \\pm   3%) on HumanEval and +12% (  plus-or-minus \\pm   2%) on MBPP. This observation confirms the hypothesis posed in Section  2 , showing that training LMs to synthesize code with edits using LintSeq data improves the relationship between cumulative inference-time compute and zero-shot performance.",
            "To do this, we replace the backwards procedure described in Section  2.2  with exclusively random sampling; during each step of the algorithm, we first sample the number of lines to delete from the current program uniformly at random, before sampling a set of lines with the desired count. Using this linter-ablated version of the algorithm, we generate a new synthetic edit sequence dataset with the same size as the LintSeq dataset used in all previous finetuning experiments, i.e. with  s = 5 s 5 s=5 italic_s = 5  example sequences per sample in the source dataset. The average number of edits per example in this dataset ( E   RandSeqInstruct = 3.9 subscript   E RandSeqInstruct 3.9 \\overline{E}_{\\text{RandSeqInstruct}}=3.9 over  start_ARG italic_E end_ARG start_POSTSUBSCRIPT RandSeqInstruct end_POSTSUBSCRIPT = 3.9 ) is empirically similar to its linter-guided counterpart ( E   LintSeqInstruct = 3.8 subscript   E LintSeqInstruct 3.8 \\overline{E}_{\\text{LintSeqInstruct}}=3.8 over  start_ARG italic_E end_ARG start_POSTSUBSCRIPT LintSeqInstruct end_POSTSUBSCRIPT = 3.8 , see Figure  3 ). We employ the same procedure as the one used in Section  3.3  to instruction finetune TinyCodeLM models on the dataset of randomly sampled edit sequences.",
            "Re-parameterizing code generation with edits has a few immediate benefits. For example, it makes code generation with LLMs much more controllable at the prompt-level (Appendix  A.3 ) and it reduces the cost of predicting useful and correct code insertions with models, since synthetic edit-trained LLMs do not need to be prompted to re-synthesize entire programs from scratch (Section  2.4 ).",
            "Across other tested models from the Phi, Gemma, and Llama families, finetuning on LintSeq data also improves the diversity of zero-shot generations, boosting the inference-time scaling of coverage on HumanEval and MBPP at fixed sample counts (Section  3.3.2 ).",
            "During generation, we continue decoding until an end-of-sequence token is output by an LM. We treat all LM outputs as either Python code or sequences of Python code edits, depending on whether an LM was finetuned on standard instruct or LintSeq instruct data. In the latter case, we post-process outputs by resolving the output edit sequences using the procedure described in Appendix  A.2 .",
            "Many existing state-of-the-art code synthesis LMs only report temperature-tuned pass@k scores on HumanEval, including Codex, AlphaCode, and Codegen-Mono  (Chen et al.,  2021 ; Li et al.,  2022b ; Nijkamp et al.,  2022 ) . Thus, in Tables  1  and  2 , we select the single best overall checkpoint of each TinyCodeLM model by averaging HumanEval and MBPP(+) pass@50 score. Then, we temperature-tune each checkpoints pass@1 and pass@10 scores when reporting results. On HumanEval, we test temperatures    { 0.0 , 0.2 , 0.4 , 0.8 , 1.0 }  0.0 0.2 0.4 0.8 1.0 \\tau\\in\\{0.0,0.2,0.4,0.8,1.0\\} italic_  { 0.0 , 0.2 , 0.4 , 0.8 , 1.0 } . On MBPP(+), we sweep over a smaller temperature range,    { 0.0 , 0.1 , 1.0 }  0.0 0.1 1.0 \\tau\\in\\{0.0,0.1,1.0\\} italic_  { 0.0 , 0.1 , 1.0 } . We perform the same temperature tuning procedure when reporting external model benchmark scores as well, i.e. the scores annotated with  (  )  (\\dagger) (  )  in Tables  1  and  2 . When running benchmark evaluations with these external code LMs, we stray from the prompt formatting, generation, and parsing procedures described in Appendices  B.1  and  B.2 ; instead, in the interest of a fair evaluation, we reproduce the conventions reported by model authors to report other scores.",
            "Table  5  displays the data sources that are used to prepare the dataset described in Section  3.2 . These data are pooled and preprocessed into instruction-program pairs by stripping away Markdown formatting and natural language explanations from completions (Figure  9  and  10 ). In our experiments, we use the resultant data to finetune baseline models, comparing their performance to those of LMs finetuned on edit sequences generated with LintSeq from the same set of instruction-program pairs.",
            "We instruction finetune all models with Microsoft DeepSpeed using the ZeRO++ protocol for stage three sharding. For the largest of these models, we also use CPU parameter offloading to accelerate experiments  (Wang et al.,  2023a ; Ren et al.,  2021 ) . When finetuning models on LintSeq data, we add a new token  <|diff|>  to tokenizers (Section  2.4 ) and resize model embeddings accordingly.",
            "Across all of the finetuning experiments conducted in this paper, we train model-data variants with the same batch size and for an equal number of total optimizer steps. This optimizer step count corresponds to ten epochs of finetuning with the baseline instruction tuning dataset described in Section  3.2 . We save intermediate checkpoints at equal optimizer step intervals in all experiments.",
            "We report zero-shot HumanEval coverage for external models by using the evaluation statistics from  Dubey et al. ( 2024 )  (Table 18, column one). To estimate cumulative inference-time FLOPs for these models, we employ the approximation expression above and estimate  T avg-total-tokens-per-sample  200 subscript T avg-total-tokens-per-sample 200 T_{\\text{avg-total-tokens-per-sample}}\\approx 200 italic_T start_POSTSUBSCRIPT avg-total-tokens-per-sample end_POSTSUBSCRIPT  200 , reflecting an ensemble over the per sample token counts of standard Instruct finetuned models shown in Table  12 . Note that this ensembled statistic reflects program generations without chain-of-thought only. As a result, we believe it to be a conservative estimator."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Architectural and pretraining hyperparameters  of our on device 150M and 400M parameter TinyCodeLM models, pretrained on a mixture of Web text and code for Python understanding.",
        "table": "A3.T3.3",
        "footnotes": [],
        "references": [
            "We show an example of program synthesis dataset statistics before and after LintSeq processing in Figure  3 . In the worst case, re-expressing a program as an edit sequence increases the length of a training example by a token count that is constant in the number of program lines (Appendix  A ).",
            "Similar to previous works  (Chen et al.,  2021 ) , we evaluate models by computing zero-shot coverage statistics on code synthesis benchmarks with and without repeated sampling. We expound upon our motivation for evaluating models in this manner in Appendix  B.3.1 .",
            "Using both the code edit re-factorized and baseline instruction datasets obtained in section  3.2 , we run pairs of finetuning experiments with six different models. In each experiment pair, we finetune an LM on both datasets for an equal number of optimizer steps and with the same learning rate schedule, saving intermediate checkpoints throughout finetuning. We run full benchmark evaluations on HumanEval and MBPP on each saved checkpoint 1 1 1 To process the generations of edit sequence LMs into executable programs, we simply resolve each of the predicted code edits one-by-one. This procedure is visualized in Figure  1  and described in Appendix  A.2 . , performing no prompt tuning. Then, we compare the best inference-time scaling behavior, i.e. benchmark coverage (pass@k) as a function of samples (k), obtained by edit sequence finetuning vs standard finetuning for each model. A more detailed description of the computed metrics as well as a full specification of the evaluation and finetuning procedures is provided in Appendices  B  and  D .",
            "Most of our findings echo those of Section  3.3.1 . Aggregating across sample counts, we find that finetuning models to synthesize code with edits improves overall zero-shot performance on HumanEval and MBPP compared to finetuning on the original data. This suggests that re-factoring code with edit sequences is an architecture- and tokenizer-independent mechanism for improving downstream LM outputs. Furthermore, as shown in Figure  4  and Tables  10  and  11 , we find that the degree by which edit sequence LMs outperform baseline model variants increases with repeated sampling for all tested models, culminating in an average absolute gain in pass@50 of +20% (  plus-or-minus \\pm   3%) on HumanEval and +12% (  plus-or-minus \\pm   2%) on MBPP. This observation confirms the hypothesis posed in Section  2 , showing that training LMs to synthesize code with edits using LintSeq data improves the relationship between cumulative inference-time compute and zero-shot performance.",
            "At pass@1, however, our results are slightly more mixed than in Section  3.3.1 . For Phi-3 models, we observe either no difference or a decrease in score between each pair of model-data variants. One explanation for this is bias: the Phi-3 models have been previously instruction finetuned and were likely to have been trained on standard code data, putting LM variants finetuned to generate edit sequence re-factorized code at a comparative disadvantage due to distributional shift.",
            "To do this, we replace the backwards procedure described in Section  2.2  with exclusively random sampling; during each step of the algorithm, we first sample the number of lines to delete from the current program uniformly at random, before sampling a set of lines with the desired count. Using this linter-ablated version of the algorithm, we generate a new synthetic edit sequence dataset with the same size as the LintSeq dataset used in all previous finetuning experiments, i.e. with  s = 5 s 5 s=5 italic_s = 5  example sequences per sample in the source dataset. The average number of edits per example in this dataset ( E   RandSeqInstruct = 3.9 subscript   E RandSeqInstruct 3.9 \\overline{E}_{\\text{RandSeqInstruct}}=3.9 over  start_ARG italic_E end_ARG start_POSTSUBSCRIPT RandSeqInstruct end_POSTSUBSCRIPT = 3.9 ) is empirically similar to its linter-guided counterpart ( E   LintSeqInstruct = 3.8 subscript   E LintSeqInstruct 3.8 \\overline{E}_{\\text{LintSeqInstruct}}=3.8 over  start_ARG italic_E end_ARG start_POSTSUBSCRIPT LintSeqInstruct end_POSTSUBSCRIPT = 3.8 , see Figure  3 ). We employ the same procedure as the one used in Section  3.3  to instruction finetune TinyCodeLM models on the dataset of randomly sampled edit sequences.",
            "Re-parameterizing code generation with edits has a few immediate benefits. For example, it makes code generation with LLMs much more controllable at the prompt-level (Appendix  A.3 ) and it reduces the cost of predicting useful and correct code insertions with models, since synthetic edit-trained LLMs do not need to be prompted to re-synthesize entire programs from scratch (Section  2.4 ).",
            "Tiny LMs can be efficiently finetuned to synthesize Python programs with edit sequences via LintSeq data. This results in state-of-the-art code benchmark performance for models that can be trained on device (Sections  3.1  and  3.3.1 ).",
            "Across other tested models from the Phi, Gemma, and Llama families, finetuning on LintSeq data also improves the diversity of zero-shot generations, boosting the inference-time scaling of coverage on HumanEval and MBPP at fixed sample counts (Section  3.3.2 ).",
            "Ablating linter-guidance from LintSeq hurts the quality, diversity, and correctness of code synthesized by instruction finetuned models (Section  3.4 ).",
            "Table  5  displays the data sources that are used to prepare the dataset described in Section  3.2 . These data are pooled and preprocessed into instruction-program pairs by stripping away Markdown formatting and natural language explanations from completions (Figure  9  and  10 ). In our experiments, we use the resultant data to finetune baseline models, comparing their performance to those of LMs finetuned on edit sequences generated with LintSeq from the same set of instruction-program pairs.",
            "Across all of the finetuning experiments conducted in this paper, we train model-data variants with the same batch size and for an equal number of total optimizer steps. This optimizer step count corresponds to ten epochs of finetuning with the baseline instruction tuning dataset described in Section  3.2 . We save intermediate checkpoints at equal optimizer step intervals in all experiments."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Pretraining data mix  used to train both TinyCodeLM models. Datasets were tokenized and prepared using HuggingFace and Dolma tooling  (Wolf et al.,  2020 ; Soldaini et al.,  2024 ) .",
        "table": "A3.T4.3",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "For smaller LLMs, repeatedly sampling from edit sequence models results in HumanEval coverage that is competitive with GPT-4 models and has similar cumulative cost to sampling once per problem from open-source LLMs like Llama 3.1 405B (Figures  1  and  4 ).",
            "With our baseline dataset prepared, we run LintSeq to generate  s = 5 s 5 s=5 italic_s = 5  synthetic edit trajectory samples for each instruction-program pair. As described in Section  2.4 , we concatenate each synthetic edit trajectory into a single string by interleaving consecutive edits with a special reserved edit token. Inspired by  Muennighoff et al. ( 2024 ) , we do not restrict against edit sequence repetitions. We use the popular Python linter  pylint  to guide edit sampling during generation. Examples of generated edit sequences and experiments testing the effect of varying  s s s italic_s  are in Appendix  E .",
            "The results above raise a natural question: do performance improvements from finetuning LMs to synthesize code with edit sequences hold for other model scales, architectures, and tokenizers? To test this, we conduct four additional pairs of instruction finetuning experiments on LMs from three model families, Gemma 2, Phi-3, and Llama 3.1, ranging in size from 2.6B to 14B. We employ pretrained-only model weights, if available. The results of these experiments are in Figure  4 , where we plot zero-shot benchmark coverage as a function of samples for instruction finetuned models. Raw coverage scores are reported in Appendix Tables  10  and  11 .",
            "Most of our findings echo those of Section  3.3.1 . Aggregating across sample counts, we find that finetuning models to synthesize code with edits improves overall zero-shot performance on HumanEval and MBPP compared to finetuning on the original data. This suggests that re-factoring code with edit sequences is an architecture- and tokenizer-independent mechanism for improving downstream LM outputs. Furthermore, as shown in Figure  4  and Tables  10  and  11 , we find that the degree by which edit sequence LMs outperform baseline model variants increases with repeated sampling for all tested models, culminating in an average absolute gain in pass@50 of +20% (  plus-or-minus \\pm   3%) on HumanEval and +12% (  plus-or-minus \\pm   2%) on MBPP. This observation confirms the hypothesis posed in Section  2 , showing that training LMs to synthesize code with edits using LintSeq data improves the relationship between cumulative inference-time compute and zero-shot performance.",
            "Re-parameterizing code generation with edits has a few immediate benefits. For example, it makes code generation with LLMs much more controllable at the prompt-level (Appendix  A.3 ) and it reduces the cost of predicting useful and correct code insertions with models, since synthetic edit-trained LLMs do not need to be prompted to re-synthesize entire programs from scratch (Section  2.4 ).",
            "On HumanEval, the cumulative inference cost of repeatedly sampling from small edit sequence LLMs is similar to sampling once from larger LLMs and yields coverage that is competitive with GPT-4, GPT-4-Omni, and Llama 3.1 405B (Figure  1 , Appendix  F.4 ).",
            "Ablating linter-guidance from LintSeq hurts the quality, diversity, and correctness of code synthesized by instruction finetuned models (Section  3.4 ).",
            "Our goal is to probe whether re-parameterizing code synthesis with edit sequences can improve the expressivity of smaller LLM representations, boosting benchmark coverage as a function of samples-per-problem. Hence, we primarily compare finetuned models by evaluating them with the procedures described above on HumanEval and MBPP(+) at a high temperature and a large sample count, computing pass@k for k  { 1 , 5 , 10 , 20 , 50 } absent 1 5 10 20 50 \\ \\in\\{1,5,10,20,50\\}  { 1 , 5 , 10 , 20 , 50 }  with  N = 50 N 50 N=50 italic_N = 50  samples 2 2 2 These are the largest sample counts that are feasible to compute on our hardware given the scope of our experiments.  at temperature 1, top-p 0.95. We compute pass@k statistics with the same procedure as  Chen et al. ( 2021 ) . The results of these evaluations are reported throughout the paper and shown in Figures  4 ,  5  and Tables  8 ,  9 ,  10 ,  11 . In each of these Figures and Tables, we identify the most performant checkpoint from each model-data finetuning run by comparing pass@50 score at temperature 1 on HumanEval and MBPP(+) across checkpoints.",
            "We instruction finetune all models with Microsoft DeepSpeed using the ZeRO++ protocol for stage three sharding. For the largest of these models, we also use CPU parameter offloading to accelerate experiments  (Wang et al.,  2023a ; Ren et al.,  2021 ) . When finetuning models on LintSeq data, we add a new token  <|diff|>  to tokenizers (Section  2.4 ) and resize model embeddings accordingly.",
            "For our instruction finetuned models, we determine the quantities  T avg-total-tokens-per-sample subscript T avg-total-tokens-per-sample T_{\\text{avg-total-tokens-per-sample}} italic_T start_POSTSUBSCRIPT avg-total-tokens-per-sample end_POSTSUBSCRIPT  by computing token counts over all sets of samples per problem that we obtained to compute the coverage statistics in Figure  4  and Table  10  above. These token statistics are provided in the table below."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Instruction data mix  used to prepare the baseline instruction dataset in Section  3.2 .",
        "table": "A4.T5.1",
        "footnotes": [
            ""
        ],
        "references": [
            "Ablating the linter from edit sampling during data generation hurts the downstream quality of programs synthesized by edit sequence models (Figures  5  and  6 ).",
            "In Figure  5 , we compare the temperature  1 1 1 1  inference-time scaling laws on HumanEval and MBPP obtained by finetuning models on randomly sampled vs static error-free edit sequences. Raw model scores are also provided in Appendix  F , Tables  8  and  9 . Ablating linter-guidance results in a decline in benchmark coverage. On HumanEval, linter ablation reduces absolute pass@50 score on TinyCodeLM-150M ( 22.6 %  17.7 % maps-to percent 22.6 percent 17.7 22.6\\%\\mapsto 17.7\\% 22.6 %  17.7 % ) and on TinyCodeLM-400M ( 26.8 %  22.0 % maps-to percent 26.8 percent 22.0 26.8\\%\\mapsto 22.0\\% 26.8 %  22.0 % ). MBPP pass@50 is similarly affected, dropping for both models ( 34.5 %  30.2 %  percent 34.5 percent 30.2 34.5\\%\\to 30.2\\% 34.5 %  30.2 %  and  39.6 %  34.5 % maps-to percent 39.6 percent 34.5 39.6\\%\\mapsto 34.5\\% 39.6 %  34.5 % ). These results suggest that the error-free nature of edits in LintSeq instruction finetuning data does indeed have a positive impact on the coding problem coverage of sampled solutions.",
            "In summary, the error-free nature of the linter-guided edits sampled in LintSeq appears to indeed be important for improving both the quality and diversity of sampled programs (Figure  5 ), as well as the overall correctness (Figure  6 ) of code synthesized by language models trained on edit sequences.",
            "Our goal is to probe whether re-parameterizing code synthesis with edit sequences can improve the expressivity of smaller LLM representations, boosting benchmark coverage as a function of samples-per-problem. Hence, we primarily compare finetuned models by evaluating them with the procedures described above on HumanEval and MBPP(+) at a high temperature and a large sample count, computing pass@k for k  { 1 , 5 , 10 , 20 , 50 } absent 1 5 10 20 50 \\ \\in\\{1,5,10,20,50\\}  { 1 , 5 , 10 , 20 , 50 }  with  N = 50 N 50 N=50 italic_N = 50  samples 2 2 2 These are the largest sample counts that are feasible to compute on our hardware given the scope of our experiments.  at temperature 1, top-p 0.95. We compute pass@k statistics with the same procedure as  Chen et al. ( 2021 ) . The results of these evaluations are reported throughout the paper and shown in Figures  4 ,  5  and Tables  8 ,  9 ,  10 ,  11 . In each of these Figures and Tables, we identify the most performant checkpoint from each model-data finetuning run by comparing pass@50 score at temperature 1 on HumanEval and MBPP(+) across checkpoints.",
            "Table  5  displays the data sources that are used to prepare the dataset described in Section  3.2 . These data are pooled and preprocessed into instruction-program pairs by stripping away Markdown formatting and natural language explanations from completions (Figure  9  and  10 ). In our experiments, we use the resultant data to finetune baseline models, comparing their performance to those of LMs finetuned on edit sequences generated with LintSeq from the same set of instruction-program pairs."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Peak learning rates  used to instruction finetune models.",
        "table": "A4.T6.1",
        "footnotes": [],
        "references": [
            "Ablating the linter from edit sampling during data generation hurts the downstream quality of programs synthesized by edit sequence models (Figures  5  and  6 ).",
            "In Figure  6 , we plot the total proportions of synthesized program samples with at least one static error across finetuned model variants. On both benchmarks, LMs trained on randomly sampled edits (dark grey) appear to generate buggy code with much higher frequency than all other models. Furthermore, on HumanEval, we find that LintSeq models (indigo) synthesize programs with static errors at a higher frequency than baseline models (light grey), despite their higher coverage of benchmark coding problems. This additional finding suggests that model performance gains from LintSeq cannot simply be attributed to improvement in static error frequency across code  training on re-factored code must be helping models write generally better, more diverse programs.",
            "In summary, the error-free nature of the linter-guided edits sampled in LintSeq appears to indeed be important for improving both the quality and diversity of sampled programs (Figure  5 ), as well as the overall correctness (Figure  6 ) of code synthesized by language models trained on edit sequences.",
            "In order to tune the peak learning rates used in each set of model experiments, we run a full sweep    { \\alpha\\in\\{ italic_  { 6e-4, 3e-4, 1e-4, 5e-5, 1e-5, 5e-6 } } \\} }  in the baseline instruction data setting for each model. We select peak learning rate values by tracking the best-achieved downstream benchmark performance across models. The chosen values are displayed in Table  6 . All other finetuning hyperparameters are kept fixed at the settings in Table  7  across experiments."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  All other instruction finetuning settings , re-used across experiments.",
        "table": "A4.T7.1",
        "footnotes": [],
        "references": [
            "We provide a guide to reading Unix-style diffs below in Figure  7 . The diff shown in this figure is computed using the Python library  difflib , which is the implementation that we use to compactly represent edits in our synthetic data generation experiments. Note that the total extra tokens present in an insertion edit sequence representation of a program scales with the number of program lines  L L L italic_L , and can be upper-bounded as  T diff  L  ( ( chars in decorator ) + ( extra chars per line in body ) ) = 16  L subscript T diff  L chars in decorator extra chars per line in body 16 L T_{\\text{diff}}\\leq L\\cdot((\\text{chars in ``decorator''})+(\\text{extra chars % per line in ``body''}))=16L italic_T start_POSTSUBSCRIPT diff end_POSTSUBSCRIPT  italic_L  ( ( chars in decorator ) + ( extra chars per line in body ) ) = 16 italic_L .",
            "During inference, LMs that have been finetuned on LintSeq instruct data will synthesize code via edit sequences, outputting text strings that consist of a sequence of consecutive Python diffs interleaved with newline characters and  <|diff|>  tokens, similar to   Piterbarg et al. ( 2024 ) . Each of these diffs will be structured as shown in Figure  7 , if correctly formatted by the language model.",
            "The structure of Unix-style diffs affects the downstream controllability of code synthesis with models that have been trained on edit sequence re-parameterized programs. As shown in Figure  7 , the first line of every diff is a decorator that describes the location and the numbers of lines changed by the edit. During inference, autoregressive language models that have been trained on Unix-style diffs with this format can be prompted to predict an edit in any desired target location within the program being synthesized by intervening on a model generation.",
            "In order to tune the peak learning rates used in each set of model experiments, we run a full sweep    { \\alpha\\in\\{ italic_  { 6e-4, 3e-4, 1e-4, 5e-5, 1e-5, 5e-6 } } \\} }  in the baseline instruction data setting for each model. We select peak learning rate values by tracking the best-achieved downstream benchmark performance across models. The chosen values are displayed in Table  6 . All other finetuning hyperparameters are kept fixed at the settings in Table  7  across experiments."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  HumanEval  fixed-temperature coverage scaling results achieved by finetuning  TinyCodeLM  models (zero-shot, temperature = 1, top-p = 0.95).",
        "table": "A6.T8.5",
        "footnotes": [],
        "references": [
            "We run our first two pairs of finetuning experiments on  TinyCodeLM-150M  and  TinyCodeLM-400M . These models were not pretrained on code synthesis instruction data, nor were they pretrained on any diff-like edit data. Our experimental results are summarized in Tables  1  and  2 , where we compare the temperature-tuned performance of our models to the reported benchmark coverage of existing code LMs at similar parameter scales. We also report the inference-time scaling of benchmark coverage as a function of samples for our finetuned models in Appendix Tables  8  and  9 .",
            "In Figure  5 , we compare the temperature  1 1 1 1  inference-time scaling laws on HumanEval and MBPP obtained by finetuning models on randomly sampled vs static error-free edit sequences. Raw model scores are also provided in Appendix  F , Tables  8  and  9 . Ablating linter-guidance results in a decline in benchmark coverage. On HumanEval, linter ablation reduces absolute pass@50 score on TinyCodeLM-150M ( 22.6 %  17.7 % maps-to percent 22.6 percent 17.7 22.6\\%\\mapsto 17.7\\% 22.6 %  17.7 % ) and on TinyCodeLM-400M ( 26.8 %  22.0 % maps-to percent 26.8 percent 22.0 26.8\\%\\mapsto 22.0\\% 26.8 %  22.0 % ). MBPP pass@50 is similarly affected, dropping for both models ( 34.5 %  30.2 %  percent 34.5 percent 30.2 34.5\\%\\to 30.2\\% 34.5 %  30.2 %  and  39.6 %  34.5 % maps-to percent 39.6 percent 34.5 39.6\\%\\mapsto 34.5\\% 39.6 %  34.5 % ). These results suggest that the error-free nature of edits in LintSeq instruction finetuning data does indeed have a positive impact on the coding problem coverage of sampled solutions.",
            "The primary goal of this paper is to introduce a method for re-factorizing code synthesis with LMs by finetuning them on synthetic instruction data. As a result, we evaluate all models using minimal prompt formats, performing no prompt tuning (see Figures  9  and  10 ). Examples of the prompt formats that we use during evaluation are shown in Figure  8 .",
            "To evaluate models on MBPP(+), we use the default prompts from the MBPP benchmark dataset, formatted with specification of the target function name and arguments both inside and outside of the natural language instruction, as shown in Figure  8 .",
            "Our goal is to probe whether re-parameterizing code synthesis with edit sequences can improve the expressivity of smaller LLM representations, boosting benchmark coverage as a function of samples-per-problem. Hence, we primarily compare finetuned models by evaluating them with the procedures described above on HumanEval and MBPP(+) at a high temperature and a large sample count, computing pass@k for k  { 1 , 5 , 10 , 20 , 50 } absent 1 5 10 20 50 \\ \\in\\{1,5,10,20,50\\}  { 1 , 5 , 10 , 20 , 50 }  with  N = 50 N 50 N=50 italic_N = 50  samples 2 2 2 These are the largest sample counts that are feasible to compute on our hardware given the scope of our experiments.  at temperature 1, top-p 0.95. We compute pass@k statistics with the same procedure as  Chen et al. ( 2021 ) . The results of these evaluations are reported throughout the paper and shown in Figures  4 ,  5  and Tables  8 ,  9 ,  10 ,  11 . In each of these Figures and Tables, we identify the most performant checkpoint from each model-data finetuning run by comparing pass@50 score at temperature 1 on HumanEval and MBPP(+) across checkpoints."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  MBPP(+)  fixed-temperature coverage scaling results achieved by finetuning  TinyCodeLM  models (zero-shot, temperature = 1, top-p = 0.95).",
        "table": "A6.T9.5",
        "footnotes": [],
        "references": [
            "Recall from above that a single program edit computed by the  diff  operator    (  ,  )    \\Delta(\\cdot,\\cdot) roman_ (  ,  )  can consist of any number of deletions and insertions. LintSeq is an algorithm for computing edit sequence re-factorings  D  superscript D  \\mathcal{D^{\\prime}} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  such that all data  ( x ,  y )  D  x subscript  y superscript D  (x,\\bm{\\delta}_{y})\\in\\mathcal{D^{\\prime}} ( italic_x , bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )  caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  have a particular property: every edit in   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  consists of  insertions only . There are two phases in LintSeq: a backward sampling phase that is used to compute program state sequences   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , and a forward edit sequence computation phase that is used to re-express   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  as edit sequences   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT . An illustration of these phases is shown in Figure  2 . Full examples of edit sequences generated with LintSeq are provided in Appendix  E  (Figures  9  and  10 ).",
            "We run our first two pairs of finetuning experiments on  TinyCodeLM-150M  and  TinyCodeLM-400M . These models were not pretrained on code synthesis instruction data, nor were they pretrained on any diff-like edit data. Our experimental results are summarized in Tables  1  and  2 , where we compare the temperature-tuned performance of our models to the reported benchmark coverage of existing code LMs at similar parameter scales. We also report the inference-time scaling of benchmark coverage as a function of samples for our finetuned models in Appendix Tables  8  and  9 .",
            "In Figure  5 , we compare the temperature  1 1 1 1  inference-time scaling laws on HumanEval and MBPP obtained by finetuning models on randomly sampled vs static error-free edit sequences. Raw model scores are also provided in Appendix  F , Tables  8  and  9 . Ablating linter-guidance results in a decline in benchmark coverage. On HumanEval, linter ablation reduces absolute pass@50 score on TinyCodeLM-150M ( 22.6 %  17.7 % maps-to percent 22.6 percent 17.7 22.6\\%\\mapsto 17.7\\% 22.6 %  17.7 % ) and on TinyCodeLM-400M ( 26.8 %  22.0 % maps-to percent 26.8 percent 22.0 26.8\\%\\mapsto 22.0\\% 26.8 %  22.0 % ). MBPP pass@50 is similarly affected, dropping for both models ( 34.5 %  30.2 %  percent 34.5 percent 30.2 34.5\\%\\to 30.2\\% 34.5 %  30.2 %  and  39.6 %  34.5 % maps-to percent 39.6 percent 34.5 39.6\\%\\mapsto 34.5\\% 39.6 %  34.5 % ). These results suggest that the error-free nature of edits in LintSeq instruction finetuning data does indeed have a positive impact on the coding problem coverage of sampled solutions.",
            "The primary goal of this paper is to introduce a method for re-factorizing code synthesis with LMs by finetuning them on synthetic instruction data. As a result, we evaluate all models using minimal prompt formats, performing no prompt tuning (see Figures  9  and  10 ). Examples of the prompt formats that we use during evaluation are shown in Figure  8 .",
            "Our goal is to probe whether re-parameterizing code synthesis with edit sequences can improve the expressivity of smaller LLM representations, boosting benchmark coverage as a function of samples-per-problem. Hence, we primarily compare finetuned models by evaluating them with the procedures described above on HumanEval and MBPP(+) at a high temperature and a large sample count, computing pass@k for k  { 1 , 5 , 10 , 20 , 50 } absent 1 5 10 20 50 \\ \\in\\{1,5,10,20,50\\}  { 1 , 5 , 10 , 20 , 50 }  with  N = 50 N 50 N=50 italic_N = 50  samples 2 2 2 These are the largest sample counts that are feasible to compute on our hardware given the scope of our experiments.  at temperature 1, top-p 0.95. We compute pass@k statistics with the same procedure as  Chen et al. ( 2021 ) . The results of these evaluations are reported throughout the paper and shown in Figures  4 ,  5  and Tables  8 ,  9 ,  10 ,  11 . In each of these Figures and Tables, we identify the most performant checkpoint from each model-data finetuning run by comparing pass@50 score at temperature 1 on HumanEval and MBPP(+) across checkpoints.",
            "Table  5  displays the data sources that are used to prepare the dataset described in Section  3.2 . These data are pooled and preprocessed into instruction-program pairs by stripping away Markdown formatting and natural language explanations from completions (Figure  9  and  10 ). In our experiments, we use the resultant data to finetune baseline models, comparing their performance to those of LMs finetuned on edit sequences generated with LintSeq from the same set of instruction-program pairs."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  HumanEval  fixed-temperature coverage scaling results achieved by finetuning Gemma 2, Phi-3, and Llama 3.1 models (zero-shot, temperature = 1, top-p = 0.95).",
        "table": "A6.T10.3",
        "footnotes": [],
        "references": [
            "Recall from above that a single program edit computed by the  diff  operator    (  ,  )    \\Delta(\\cdot,\\cdot) roman_ (  ,  )  can consist of any number of deletions and insertions. LintSeq is an algorithm for computing edit sequence re-factorings  D  superscript D  \\mathcal{D^{\\prime}} caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  such that all data  ( x ,  y )  D  x subscript  y superscript D  (x,\\bm{\\delta}_{y})\\in\\mathcal{D^{\\prime}} ( italic_x , bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT )  caligraphic_D start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  have a particular property: every edit in   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  consists of  insertions only . There are two phases in LintSeq: a backward sampling phase that is used to compute program state sequences   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT , and a forward edit sequence computation phase that is used to re-express   y subscript  y \\bm{\\sigma}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT  as edit sequences   y subscript  y \\bm{\\delta}_{y} bold_italic_ start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT . An illustration of these phases is shown in Figure  2 . Full examples of edit sequences generated with LintSeq are provided in Appendix  E  (Figures  9  and  10 ).",
            "The results above raise a natural question: do performance improvements from finetuning LMs to synthesize code with edit sequences hold for other model scales, architectures, and tokenizers? To test this, we conduct four additional pairs of instruction finetuning experiments on LMs from three model families, Gemma 2, Phi-3, and Llama 3.1, ranging in size from 2.6B to 14B. We employ pretrained-only model weights, if available. The results of these experiments are in Figure  4 , where we plot zero-shot benchmark coverage as a function of samples for instruction finetuned models. Raw coverage scores are reported in Appendix Tables  10  and  11 .",
            "Most of our findings echo those of Section  3.3.1 . Aggregating across sample counts, we find that finetuning models to synthesize code with edits improves overall zero-shot performance on HumanEval and MBPP compared to finetuning on the original data. This suggests that re-factoring code with edit sequences is an architecture- and tokenizer-independent mechanism for improving downstream LM outputs. Furthermore, as shown in Figure  4  and Tables  10  and  11 , we find that the degree by which edit sequence LMs outperform baseline model variants increases with repeated sampling for all tested models, culminating in an average absolute gain in pass@50 of +20% (  plus-or-minus \\pm   3%) on HumanEval and +12% (  plus-or-minus \\pm   2%) on MBPP. This observation confirms the hypothesis posed in Section  2 , showing that training LMs to synthesize code with edits using LintSeq data improves the relationship between cumulative inference-time compute and zero-shot performance.",
            "The primary goal of this paper is to introduce a method for re-factorizing code synthesis with LMs by finetuning them on synthetic instruction data. As a result, we evaluate all models using minimal prompt formats, performing no prompt tuning (see Figures  9  and  10 ). Examples of the prompt formats that we use during evaluation are shown in Figure  8 .",
            "Our goal is to probe whether re-parameterizing code synthesis with edit sequences can improve the expressivity of smaller LLM representations, boosting benchmark coverage as a function of samples-per-problem. Hence, we primarily compare finetuned models by evaluating them with the procedures described above on HumanEval and MBPP(+) at a high temperature and a large sample count, computing pass@k for k  { 1 , 5 , 10 , 20 , 50 } absent 1 5 10 20 50 \\ \\in\\{1,5,10,20,50\\}  { 1 , 5 , 10 , 20 , 50 }  with  N = 50 N 50 N=50 italic_N = 50  samples 2 2 2 These are the largest sample counts that are feasible to compute on our hardware given the scope of our experiments.  at temperature 1, top-p 0.95. We compute pass@k statistics with the same procedure as  Chen et al. ( 2021 ) . The results of these evaluations are reported throughout the paper and shown in Figures  4 ,  5  and Tables  8 ,  9 ,  10 ,  11 . In each of these Figures and Tables, we identify the most performant checkpoint from each model-data finetuning run by comparing pass@50 score at temperature 1 on HumanEval and MBPP(+) across checkpoints.",
            "Table  5  displays the data sources that are used to prepare the dataset described in Section  3.2 . These data are pooled and preprocessed into instruction-program pairs by stripping away Markdown formatting and natural language explanations from completions (Figure  9  and  10 ). In our experiments, we use the resultant data to finetune baseline models, comparing their performance to those of LMs finetuned on edit sequences generated with LintSeq from the same set of instruction-program pairs.",
            "For our instruction finetuned models, we determine the quantities  T avg-total-tokens-per-sample subscript T avg-total-tokens-per-sample T_{\\text{avg-total-tokens-per-sample}} italic_T start_POSTSUBSCRIPT avg-total-tokens-per-sample end_POSTSUBSCRIPT  by computing token counts over all sets of samples per problem that we obtained to compute the coverage statistics in Figure  4  and Table  10  above. These token statistics are provided in the table below."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  MBPP(+)  fixed-temperature coverage scaling results achieved by finetuning Gemma 2, Phi-3, and Llama 3.1 models (zero-shot, temperature = 1, top-p = 0.95).",
        "table": "A6.T11.3",
        "footnotes": [],
        "references": [
            "The results above raise a natural question: do performance improvements from finetuning LMs to synthesize code with edit sequences hold for other model scales, architectures, and tokenizers? To test this, we conduct four additional pairs of instruction finetuning experiments on LMs from three model families, Gemma 2, Phi-3, and Llama 3.1, ranging in size from 2.6B to 14B. We employ pretrained-only model weights, if available. The results of these experiments are in Figure  4 , where we plot zero-shot benchmark coverage as a function of samples for instruction finetuned models. Raw coverage scores are reported in Appendix Tables  10  and  11 .",
            "Most of our findings echo those of Section  3.3.1 . Aggregating across sample counts, we find that finetuning models to synthesize code with edits improves overall zero-shot performance on HumanEval and MBPP compared to finetuning on the original data. This suggests that re-factoring code with edit sequences is an architecture- and tokenizer-independent mechanism for improving downstream LM outputs. Furthermore, as shown in Figure  4  and Tables  10  and  11 , we find that the degree by which edit sequence LMs outperform baseline model variants increases with repeated sampling for all tested models, culminating in an average absolute gain in pass@50 of +20% (  plus-or-minus \\pm   3%) on HumanEval and +12% (  plus-or-minus \\pm   2%) on MBPP. This observation confirms the hypothesis posed in Section  2 , showing that training LMs to synthesize code with edits using LintSeq data improves the relationship between cumulative inference-time compute and zero-shot performance.",
            "Our goal is to probe whether re-parameterizing code synthesis with edit sequences can improve the expressivity of smaller LLM representations, boosting benchmark coverage as a function of samples-per-problem. Hence, we primarily compare finetuned models by evaluating them with the procedures described above on HumanEval and MBPP(+) at a high temperature and a large sample count, computing pass@k for k  { 1 , 5 , 10 , 20 , 50 } absent 1 5 10 20 50 \\ \\in\\{1,5,10,20,50\\}  { 1 , 5 , 10 , 20 , 50 }  with  N = 50 N 50 N=50 italic_N = 50  samples 2 2 2 These are the largest sample counts that are feasible to compute on our hardware given the scope of our experiments.  at temperature 1, top-p 0.95. We compute pass@k statistics with the same procedure as  Chen et al. ( 2021 ) . The results of these evaluations are reported throughout the paper and shown in Figures  4 ,  5  and Tables  8 ,  9 ,  10 ,  11 . In each of these Figures and Tables, we identify the most performant checkpoint from each model-data finetuning run by comparing pass@50 score at temperature 1 on HumanEval and MBPP(+) across checkpoints."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  HumanEval total tokens per sample  for finetuned Gemma 2, Phi-3, and Llama 3.1 models (zero-shot, temperature  = 1 absent 1 =1 = 1 , top-p  = 0.95 absent 0.95 =0.95 = 0.95 ). These counts reflect prompt and completion tokens. They are computed using the same samples whose coverage statistics are reported in Table  10 .",
        "table": "A6.EGx1",
        "footnotes": [
            ""
        ],
        "references": [
            "We report zero-shot HumanEval coverage for external models by using the evaluation statistics from  Dubey et al. ( 2024 )  (Table 18, column one). To estimate cumulative inference-time FLOPs for these models, we employ the approximation expression above and estimate  T avg-total-tokens-per-sample  200 subscript T avg-total-tokens-per-sample 200 T_{\\text{avg-total-tokens-per-sample}}\\approx 200 italic_T start_POSTSUBSCRIPT avg-total-tokens-per-sample end_POSTSUBSCRIPT  200 , reflecting an ensemble over the per sample token counts of standard Instruct finetuned models shown in Table  12 . Note that this ensembled statistic reflects program generations without chain-of-thought only. As a result, we believe it to be a conservative estimator."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A6.T12.5",
        "footnotes": [],
        "references": []
    }
}