{
    "id_table_1": {
        "caption": "Table 1:  Comparison of different proportions of global data allocated to each client.   : Completely non-IID setting.",
        "table": "S3.T1.1.1",
        "footnotes": [],
        "references": [
            "Taking a 10-category subset  [ Russakovsky et al.(2015)Russakovsky, Deng, Su, Krause, Satheesh, Ma, Huang, Karpathy, Khosla, Bernstein, et al. ]  of ImageNet as an example, to answer this question, we first demonstrate a typical label distribution of a random client in  Fig.   1  (a). It is evident that the label distribution is severely biased, leading to overfitting on majority classes and difficulty in accurately identifying missing or minority classes. To address this issue, we propose a novel method to recover the biased distribution, ensuring label balance for each client (details will be discussed in subsequent sections). In  Fig.   1  (b), we visualize the label distribution after applying our method. It can be observed that the distribution is now more balanced compared to the initial distribution shown in  Fig.   1  (a). To evaluate the performance of our method, we conduct experiments using the recovered balanced data in  Fig.   1  (c), and our method outperforms FedAvg in both non-IID and IID scenarios. Even in IID scenarios where FedAvg is designed to perform well, our method surpasses it by 15% in terms of accuracy. These results validate that the balanced local distribution contributes to the global performance. So the goal of this paper is to  recover each imbalanced local distribution back to the balanced global distribution without compromising the privacy of other clients .",
            "To validate this assumption, we collect entire data from all clients on the server side and sample different proportions from this balanced dataset to distribute to each client, thus alleviating local data imbalance.  Note that this operation completely violates privacy and is solely used for analysis purposes.  To be specific, we use the Dirichlet distribution  [ Li et al.(2021a)Li, He, and Song ]  with   = 0.01  0.01 \\beta=0.01 italic_ = 0.01  to simulate label distribution skew on ImageFruit  [ Zhang et al.(2023)Zhang, Qi, and Zhao ]  dataset across clients (details can be found in  Sec.   5.1 ), and then we allocate an equal proportion of data from each label of the global dataset to each client, varying the percentage from 5% to 30%. This allocation, sourced from the global dataset, maintains uniformity within each label, thereby the local distribution of each client partially recovers the global distribution.",
            "As depicted in  Tab.   1 , when global data accessibility is absent, FedAvg is trained under a wholly non-IID scenario, resulting in poor accuracies. This starkly emphasizes the substantial repercussions of label distribution skew. Conversely, with an increasing proportion of global data allocation, the accuracy exhibits remarkable enhancement. Specifically, when 30% of the global data is allocated to clients, the final accuracy reaches 55.8% and 57.3% for two tasks. This inspire us to consider:  How can we recover the global data distribution using only local data information without compromising the privacy of other clients?",
            "During the local personalization task, each client obtains the global generalization parameter,  i.e \\bmvaOneDot ,   italic- \\vartheta italic_ , and fine-tunes it on its local dataset  O m subscript O m O_{m} italic_O start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , with the same objective as in  Eq.   3 . Finally, they obtain the local parameters,  i.e \\bmvaOneDot ,   m subscript italic- m \\vartheta_{m} italic_ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , respectively. The overall pseudocode of both generalization and personalization task can be seen in Appendix Algorithm  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Main Results I : Global generalization performance on distribution-based label skew. The best results are highlighted in  bold , and the second best is  underlined .   : We train a centralized model on the entire dataset without distributing data to multiple clients.  TF : Using training-free generative model.  FT : Fine-tuning the generative model adaptively.",
        "table": "S5.T2.116.116.116",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "As shown in  Fig.   2 , our ReGL leverages generative models at the client-side, aiming to recover the global distribution locally. Considering the computational limitations, we propose two approaches: training-free and fine-tuning. In training-free approach, we use Stable Diffusion (SD)  [ Rombach et al.(2022)Rombach, Blattmann, Lorenz, Esser, and Ommer ]  to generate data for each client, thereby complementing the minority and missing classes. However, different datasets exist with domain gaps, causing synthetic data insufficiently aligned with the global distribution ( Fig.   8 ). To tackle this gap, we fine-tune the vanilla SD with adaptive approaches  [ Hu et al.(2021)Hu, Shen, Wallis, Allen-Zhu, Li, Wang, Wang, and Chen ]  using local data, which improves the alignment between the distributions of synthetic and real images. Finally, clients can train their models based on both real and synthetic images.",
            "As shown in  Tab.   2 , with training-free generation, our ReGL consistently outperforms all previous methods. When fine-tuning on local data, the performance is further enhanced, demonstrating the effectiveness of our paradigm. Notably, our adaptive ReGL surpasses FedAvg by significant margins of 48.6% and 40.5% on the ImageFruit and ImageNet100, with   = 0.01  0.01 \\beta=0.01 italic_ = 0.01 . Furthermore, as data heterogeneity increases ( i.e \\bmvaOneDot , with smaller    \\beta italic_ ), our method exhibits greater performance gains compared to previous methods, highlighting the effectiveness in addressing label imbalance.",
            "Moreover, previous works underperform the centralized model by about 35% due to the data heterogeneity among clients, where local optima are generally far from the global optima. In contrast, we recover the global distribution, aligning the local and global optima. As shown in  Tab.   2 , our method achieves accuracies that match the centralized baselines.",
            "We also evaluate the personalization of our ReGL by comparing it with several state-of-the-art Personalized FL methods. As shown in  Tab.   4 , we can observe that most of the state-of-the-art methods outperform the Separate baseline, especially with   = 0.5  0.5 \\beta=0.5 italic_ = 0.5 , which highlights the importance of collaboration among various clients. Additionally, our method outperforms all previous approaches across all datasets. For instance, with   = 0.01  0.01 \\beta=0.01 italic_ = 0.01 , our method surpasses previous methods by an average of 15% across five datasets and by over 30% with   = 0.5  0.5 \\beta=0.5 italic_ = 0.5 . We attribute this result to two key aspects: 1) Excellent personalization is inherently based on robust generalization. As demonstrated in  Tab.   2 , our method achieves the best global generalization, ensuring that each client has a solid foundation for personalized fine-tuning. 2) During fine-tuning, each client can utilize both real and synthetic data. The synthetic data conforms to the same distribution as the real data, significantly augmenting the training dataset."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Main Results II : Generalization performance when dealing with missing classes.",
        "table": "S5.T3.50.50",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Let us consider the global distribution denoted as  P global  ( x , y ) subscript P global x y P_{\\text{global}}(x,y) italic_P start_POSTSUBSCRIPT global end_POSTSUBSCRIPT ( italic_x , italic_y ) , where  x x x italic_x  represents the input data and  y y y italic_y  denotes the corresponding label. Additionally, we have the local distributions of client  i i i italic_i , denoted as  P i  ( x , y ) subscript P i x y P_{i}(x,y) italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x , italic_y ) . Each client  i i i italic_i  is capable of sampling a data point  ( x , y ) x y (x,y) ( italic_x , italic_y )  from its local distribution,  i.e \\bmvaOneDot ,  ( x , y )  P i  ( x , y ) similar-to x y subscript P i x y (x,y)\\sim P_{i}(x,y) ( italic_x , italic_y )  italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x , italic_y ) . These distributions can be expressed as the product of conditional probabilities  P i  ( x  y ) subscript P i conditional x y P_{i}(x\\mid y) italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x  italic_y )  and marginal probabilities  P i  ( y ) subscript P i y P_{i}(y) italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_y ) , yielding  P i  ( x  y )  P i  ( y ) subscript P i conditional x y subscript P i y P_{i}(x\\mid y)P_{i}(y) italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x  italic_y ) italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_y ) . In label distribution skew, the marginal probability  P i  ( y ) subscript P i y P_{i}(y) italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_y )  may vary across different clients while  P i  ( x  y ) subscript P i conditional x y P_{i}(x\\mid y) italic_P start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ( italic_x  italic_y )  remains the same, resulting in local distributions that deviate significantly from the global distribution  P global  ( x , y ) subscript P global x y P_{\\text{global}}(x,y) italic_P start_POSTSUBSCRIPT global end_POSTSUBSCRIPT ( italic_x , italic_y ) . We show the illustration of label distribution skew among clients in  Fig.   3 .",
            "During the local personalization task, each client obtains the global generalization parameter,  i.e \\bmvaOneDot ,   italic- \\vartheta italic_ , and fine-tunes it on its local dataset  O m subscript O m O_{m} italic_O start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , with the same objective as in  Eq.   3 . Finally, they obtain the local parameters,  i.e \\bmvaOneDot ,   m subscript italic- m \\vartheta_{m} italic_ start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT , respectively. The overall pseudocode of both generalization and personalization task can be seen in Appendix Algorithm  1 .",
            "In this paper, we use widely used Dirichlet distribution  [ Yurochkin et al.(2019)Yurochkin, Agarwal, Ghosh, Greenewald, Hoang, and Khazaeni ,  Li et al.(2021a)Li, He, and Song ]  to simulate label distribution skew. Specifically, we sample  p k subscript p k p_{k} italic_p start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT  from  D  i  r N  (  ) D i subscript r N  Dir_{N}(\\beta) italic_D italic_i italic_r start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT ( italic_ )  and allocate a  p k , j subscript p k j p_{k,j} italic_p start_POSTSUBSCRIPT italic_k , italic_j end_POSTSUBSCRIPT  proportion of the instances of class  k k k italic_k  to client  j j j italic_j , where  N N N italic_N  represents the client number, and    \\beta italic_  controls the label imbalance, with a lower    \\beta italic_  indicating a more skewed distribution. We evaluate under   = 0.5  0.5 \\beta=0.5 italic_ = 0.5  and   = 0.01  0.01 \\beta=0.01 italic_ = 0.01  (highly skewed). The distributions are detailed in  Fig.   3 . We also consider an extremely challenging setting: a party with a single label  [ Yu et al.(2020)Yu, Rawat, Menon, and Kumar ,  Li et al.(2022)Li, Diao, Chen, and He ] , where clients have numerous missing classes. This also exists in real world, where we can employ FL to train a speaker recognition model, while each device only has its respective user data  [ Li et al.(2022)Li, Diao, Chen, and He ] .",
            "Tab.   3  indicates that numerous missing classes lead to a catastrophic decline in accuracy for previous methods, which is primarily due to the significant disparity between local and global distributions. Most previous approaches rely on long-tail learning and struggle to effectively tackle the issue of missing classes. In contrast, our method demonstrates robustness by maintaining stable performance and achieving an accuracy approximately 40% higher than that of previous methods. We further compare the T-SNE  [ Van der Maaten and Hinton(2008) ]  visualization of our method and FedAvg on both owning and missing classes. As illustrated in  Fig.   4 , after the update, FedAvg fails to distinguish between samples from owning and missing classes, resulting in poor performance. In contrast, our method leverages a generative model to complement the missing classes for each client, thus recovering each local distribution back to the global distribution and effectively learning discriminative features for samples of missing and owning classes."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Main Results III : Local personalization performance on distribution-based label skew.   : Each model is solely trained on local data without cross-client collaboration.",
        "table": "S5.T4.101.101.101",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Tab.   3  indicates that numerous missing classes lead to a catastrophic decline in accuracy for previous methods, which is primarily due to the significant disparity between local and global distributions. Most previous approaches rely on long-tail learning and struggle to effectively tackle the issue of missing classes. In contrast, our method demonstrates robustness by maintaining stable performance and achieving an accuracy approximately 40% higher than that of previous methods. We further compare the T-SNE  [ Van der Maaten and Hinton(2008) ]  visualization of our method and FedAvg on both owning and missing classes. As illustrated in  Fig.   4 , after the update, FedAvg fails to distinguish between samples from owning and missing classes, resulting in poor performance. In contrast, our method leverages a generative model to complement the missing classes for each client, thus recovering each local distribution back to the global distribution and effectively learning discriminative features for samples of missing and owning classes.",
            "We also evaluate the personalization of our ReGL by comparing it with several state-of-the-art Personalized FL methods. As shown in  Tab.   4 , we can observe that most of the state-of-the-art methods outperform the Separate baseline, especially with   = 0.5  0.5 \\beta=0.5 italic_ = 0.5 , which highlights the importance of collaboration among various clients. Additionally, our method outperforms all previous approaches across all datasets. For instance, with   = 0.01  0.01 \\beta=0.01 italic_ = 0.01 , our method surpasses previous methods by an average of 15% across five datasets and by over 30% with   = 0.5  0.5 \\beta=0.5 italic_ = 0.5 . We attribute this result to two key aspects: 1) Excellent personalization is inherently based on robust generalization. As demonstrated in  Tab.   2 , our method achieves the best global generalization, ensuring that each client has a solid foundation for personalized fine-tuning. 2) During fine-tuning, each client can utilize both real and synthetic data. The synthetic data conforms to the same distribution as the real data, significantly augmenting the training dataset."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Performance under various skewness.   = 0.01  0.01 \\beta=0.01 italic_ = 0.01  indicates highly skewed data, while IID represents no skewness.",
        "table": "S5.T5.6.6",
        "footnotes": [],
        "references": [
            "To validate this assumption, we collect entire data from all clients on the server side and sample different proportions from this balanced dataset to distribute to each client, thus alleviating local data imbalance.  Note that this operation completely violates privacy and is solely used for analysis purposes.  To be specific, we use the Dirichlet distribution  [ Li et al.(2021a)Li, He, and Song ]  with   = 0.01  0.01 \\beta=0.01 italic_ = 0.01  to simulate label distribution skew on ImageFruit  [ Zhang et al.(2023)Zhang, Qi, and Zhao ]  dataset across clients (details can be found in  Sec.   5.1 ), and then we allocate an equal proportion of data from each label of the global dataset to each client, varying the percentage from 5% to 30%. This allocation, sourced from the global dataset, maintains uniformity within each label, thereby the local distribution of each client partially recovers the global distribution.",
            "As shown in  Fig.   5 , we investigate the effect of synthetic image volume on performance with   = 0.5  0.5 \\beta=0.5 italic_ = 0.5 . We generate different numbers of images per class for each client, ranging from 0.5 k k k italic_k  to 4 k k k italic_k . Obviously, as the number of synthetic data increases, the accuracy consistently improves. For example, increasing the number of synthetic data from 0.5 k k k italic_k  to 2 k k k italic_k  results in an accuracy improvement, from 60% to 77%, on the ImageNet100. The accuracy can further improve with more synthetic images, such as 4 k k k italic_k . In this way, our method can significantly enhance performance by generating more training samples without the need for complex design. Considering both efficiency and performance, we set the synthetic data volume per class for each client to 2 k k k italic_k .",
            "To assess the robustness of ReGL on highly skewed data, we conduct experiments on ImageFruit, wherein    \\beta italic_  varies from  0.01 0.01 0.01 0.01  to  0.5 0.5 0.5 0.5 . Additionally, we test the models in an IID (Independent and Identically Distributed) setting, where the data is evenly distributed among the clients. As shown in  Tab.   5 , our method outperforms previous methods by around 15% under the IID setting, which can be attributed to the generation of synthetic images, serving as data augmentation. As the skewness increases, our method remains robust, with performance consistently exceeding 77%, whereas other methods experience a sharp decline in performance, dropping from 75% (IID) to 30% (  = 0.01  0.01 \\beta=0.01 italic_ = 0.01 ). This demonstrates that our method can significantly improve performance in scenarios of extreme label distribution skew."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Performance comparison of different local epochs.",
        "table": "A3.T6.5.5.5",
        "footnotes": [],
        "references": [
            "Our method essentially recovers the heterogeneous local distributions to match the global distribution, thereby eliminating label distribution skew. Here, we verify the data distribution from two perspectives:  1)  We visually compare real images, training-free baseline synthetic images, and adaptive LoRA synthetic images from ImageFruit. As shown in  Fig.   8 , although the synthetic images generated by vanilla SD have correct class information, their styles are very different from the real images. In other words, the global distribution is only partially recovered. In contrast, our LoRA synthetic images have a style similar to the real images, leading to a well-recovered data distribution.  2)  We compare the t-SNE visualization of these distributions, which is presented in  Fig.   6 . We can observe a significant misalignment between the baseline synthetic and real data distributions. In contrast, the distribution of LoRA synthetic images closely match the real distribution, validating the recovery of the global distribution by our method.",
            "Here, we increase the computation load per client in each round by expanding the number of local epochs, which we denote as  E l  o  c  a  l subscript E l o c a l E_{local} italic_E start_POSTSUBSCRIPT italic_l italic_o italic_c italic_a italic_l end_POSTSUBSCRIPT . We conduct numerous experiments on ImageFruit and ImageNet100 datasets, and compare our ReGL with previous algorithms in  Tab.   6 . It is evident that, when  E l  o  c  a  l subscript E l o c a l E_{local} italic_E start_POSTSUBSCRIPT italic_l italic_o italic_c italic_a italic_l end_POSTSUBSCRIPT  is set to 1, the performance of all methods degrades significantly. In this scenario, the number of local updates is too small, resulting in inadequate model training. Nonetheless, our method continues to exhibit competitive performance, achieving an accuracy of 60.7% on the ImageFruit dataset and 60.1% on the ImageNet100 dataset with   = 0.01  0.01 \\beta=0.01 italic_ = 0.01 . As we increase the value of  E l  o  c  a  l subscript E l o c a l E_{local} italic_E start_POSTSUBSCRIPT italic_l italic_o italic_c italic_a italic_l end_POSTSUBSCRIPT , the performance of all methods generally improves. However, excessively large values of  E l  o  c  a  l subscript E l o c a l E_{local} italic_E start_POSTSUBSCRIPT italic_l italic_o italic_c italic_a italic_l end_POSTSUBSCRIPT  can lead to overfitting. Therefore, the optimal choice for our method is 5 epochs per round."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Performance comparison of different number of clients.",
        "table": "A3.T7.6.6.6",
        "footnotes": [],
        "references": [
            "To analyze the effect of the number of clients on performance, we train these methods with different numbers of clients  M M M italic_M  on ImageFruit and ImageNet100 datasets. Specifically, we set  M = { 5 , 50 , 100 } M 5 50 100 M=\\{5,50,100\\} italic_M = { 5 , 50 , 100 }  for the ImageFruit and  M = { 10 , 50 , 100 } M 10 50 100 M=\\{10,50,100\\} italic_M = { 10 , 50 , 100 }  for the ImageNet100, with a skew degree of   = 0.01  0.01 \\beta=0.01 italic_ = 0.01 . As demonstrated in  Tab.   7 , when  M M M italic_M  increases, the performance of all previous methods experiences a significant decline. This decline is especially pronounced when  M = 100 M 100 M=100 italic_M = 100 , where the accuracy of previous methods drops to approximately 10% on ImageFruit and 15% on ImageNet100. We conjecture that as the number of clients increases, there are more skewed local models, leading to a poorer aggregated model. However, our method remains robust as  M M M italic_M  increases, with almost no performance degradation. Based on these results, we can conclude that our method is highly suitable for scenarios involving a large number of clients. In cases where the number of clients is particularly high, for instance, exceeding 100, our method outperforms the previous approaches by about 60%.",
            "Label distribution skew can lead to class inconsistency across clients, so we compare the accuracy of FedAvg and our method for each class before and after a certain local update on the ImageFruit dataset using the   = 0.01  0.01 \\beta=0.01 italic_ = 0.01  setting.  For the sake of display, we generate synthetic images for each class in our method to ensure that the total number of real and synthetic images is 1.3 k k k italic_k , which is slightly different from the settings in our main experiment.  The performance is shown in  Fig.   7 . Here, all local models on the test set have the same test accuracy before local updates, because these local models are equivalent to the global model. First, we can observe that in FedAvg, the local model is restricted to learning samples solely from the majority classes, leading to a sharp decline in accuracy for the remaining classes. This indicates that label distribution skew can lead to a biased model, severely impacting global model performance. While in our method, we eliminate label skew by generating synthetic images for each class, which enables a more effective local model update. The test accuracy of each class improves after local updates,  resulting in a more robust federated learning system under label distribution skew."
        ]
    }
}