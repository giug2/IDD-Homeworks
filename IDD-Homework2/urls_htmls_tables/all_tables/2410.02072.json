{
    "id_table_1": {
        "caption": "Table 1:  Dataset Overview",
        "table": "S4.T1.2",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "The Depth Normal Evaluation and Selection Algorithm (DNESA), as outlined in Algorithm  1 , addresses the challenge of noisy labels commonly present in real-world underwater datasets. DNESA systematically evaluates the quality of pseudo-depth maps and surface normals derived from unlabelled underwater images, leveraging multiple pre-trained models. The algorithm assesses these outputs using a set of predefined metrics and selects the most reliable data to train a lightweight student model. DNESA operates in two primary stages. The first stage involves evaluating the pseudo-labeled data by computing several critical metrics:",
            "The  FindBestDepthNormalPair  function, as detailed in Algorithm  1 , selects the depth-normal pair that achieves the highest combined score. This process ensures that the student model is trained on the highest-quality pseudo-labeled data, which is crucial for achieving reliable 3D perception in underwater environments.",
            "The overall loss for a training set  l l l italic_l  is a combination of the scale- and shift-invariant loss (Equation  9 ) and the gradient matching term (Equation  10 ):",
            "This research used a variety of publicly available underwater image datasets, as detailed in Table  1  to train and evaluate the performance of our proposed method. These datasets represent diverse underwater environments with variations in water clarity, lighting conditions, and object types. The data sets were divided into different training and testing sets. Unlike previous methods, our approach did   not  rely on ground-truth depth maps or surface normals during training, highlighting the strength of our pseudo-labelling technique. Using these data sets, we ensured a robust evaluation of our method for both depth and surface normal estimation in challenging underwater scenarios."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Quantitative comparison results for depth estimation of our proposed method and compared methods on USOD 10K  [ 11 ]  dataset. For   1 subscript  1 \\delta_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,   2 subscript  2 \\delta_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , and   3 subscript  3 \\delta_{3} italic_ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , (   \\uparrow  ) means higher is better. For other metrics, (   \\downarrow  ) means that lower is better.",
        "table": "S4.T2.20",
        "footnotes": [
            ""
        ],
        "references": [
            "The rest of this paper is organised as follows: Section  2  reviews related work on underwater depth estimation and surface normals, focussing on state-of-the-art approaches and their limitations in resource-constrained environments. Section  3  details our proposed methodology, including the hybrid CNN-Transformer architecture and the domain-specific data pipeline. Section  4  presents the experimental setup and evaluates the performance of our model compared to existing methods, highlighting its efficiency and precision. Section  5  provides an in-depth discussion of the impact of the model on marine science tasks and outlines potential applications. Section  6  concludes the paper, summarising the contributions and suggesting directions for future research.",
            "According to all the above analyses, our proposed approach is designed with a focus on both precision and computational efficiency, particularly in the context of real-time applications. The general framework consists of two main steps: (1) generating precise pseudo-depth and surface normals labels from large-scale, unlabelled real images, and (2) training a final student model that balances accuracy with computational efficiency for deployment on edge devices, ensuring robust generalisation across diverse environments, as illustrated in Figure  Fig.   2 .",
            "The encoder, as outlined in Algorithm  2 , is tasked with extracting hierarchical feature representations from the input image, denoted as  x  R H  W  C x superscript R H W C \\mathbf{x}\\in\\mathbb{R}^{H\\times W\\times C} bold_x  blackboard_R start_POSTSUPERSCRIPT italic_H  italic_W  italic_C end_POSTSUPERSCRIPT . The first step involves normalising the input image using the mean    \\mu italic_  and standard deviation    \\sigma italic_  across the dataset:",
            "USOD 10K:  As shown in table  2 , PoseidonNet significantly outperformed all other methods. It achieved the highest accuracy scores across all three thresholds (  1 subscript  1 \\delta_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,   2 subscript  2 \\delta_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , and   3 subscript  3 \\delta_{3} italic_ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) while simultaneously achieving the lowest error rates (Abs Rel, Sq Rel, RMSE, and log10)."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Quantitative comparison results for depth estimation of our proposed method and compared methods on Atlantis  [ 28 ]  dataset. For   1 subscript  1 \\delta_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,   2 subscript  2 \\delta_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , and   3 subscript  3 \\delta_{3} italic_ start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , (   \\uparrow  ) means higher is better. For other metrics, (   \\downarrow  ) means that lower is better.",
        "table": "S4.T3.20",
        "footnotes": [
            ""
        ],
        "references": [
            "The rest of this paper is organised as follows: Section  2  reviews related work on underwater depth estimation and surface normals, focussing on state-of-the-art approaches and their limitations in resource-constrained environments. Section  3  details our proposed methodology, including the hybrid CNN-Transformer architecture and the domain-specific data pipeline. Section  4  presents the experimental setup and evaluates the performance of our model compared to existing methods, highlighting its efficiency and precision. Section  5  provides an in-depth discussion of the impact of the model on marine science tasks and outlines potential applications. Section  6  concludes the paper, summarising the contributions and suggesting directions for future research.",
            "Following the generation of a robust pseudo-labelled real dataset, the next step involves training a lightweight student model. This model is engineered for computational efficiency while preserving the accuracy achieved by the more complex teacher models used in the pseudo-labelling phase. The student model adopts a hybrid architecture that combines Convolutional Neural Networks (CNNs) with Transformers, as depicted in Figure  3 .",
            "The decoder is responsible for reconstructing the output, specifically depth maps and surface normals, from the encoded features  F F \\mathbf{F} bold_F . The decoding process, as outlined in Algorithm  3 , begins with a series of upsampling operations to progressively restore the spatial resolution of the feature maps. For each stage  i i i italic_i , the decoder employs an upsampling convolution, denoted as  Upconv , to increase the resolution of the feature maps, expressed as:",
            "Atlantis:  Similar to its performance on USOD 10K, PoseidonNet demonstrated superior performance on the Atlantis dataset, table  3 , surpassing other methods in both accuracy and error metrics."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Quantitative performance comparison on Sea-thru (D3) dataset  [ 1 ] . Lower scores represent better performance for all metrics.",
        "table": "S4.T4.1",
        "footnotes": [
            ""
        ],
        "references": [
            "The rest of this paper is organised as follows: Section  2  reviews related work on underwater depth estimation and surface normals, focussing on state-of-the-art approaches and their limitations in resource-constrained environments. Section  3  details our proposed methodology, including the hybrid CNN-Transformer architecture and the domain-specific data pipeline. Section  4  presents the experimental setup and evaluates the performance of our model compared to existing methods, highlighting its efficiency and precision. Section  5  provides an in-depth discussion of the impact of the model on marine science tasks and outlines potential applications. Section  6  concludes the paper, summarising the contributions and suggesting directions for future research.",
            "The core of the encoder is organised into three distinct stages, each comprising a combination of convolutional and transformer blocks, as depicted in Figure  4 . The convolutional layers employ dilated convolutions, represented as  Conv dilation subscript Conv dilation \\text{Conv}_{\\text{dilation}} Conv start_POSTSUBSCRIPT dilation end_POSTSUBSCRIPT , to expand the receptive field without a significant increase in computational cost. Specifically, for a convolution operation with kernel size  k k k italic_k  and dilation factor  d d d italic_d , the receptive field is increased by a factor of  d d d italic_d :",
            "Sea-thru:  On the Sea-thru (D3) dataset, PoseidonNet again achieved the best results across all evaluation metrics, further validating its effectiveness in accurate depth estimation for diverse underwater scenes. Table Table  4  provides a detailed breakdown of the results, with PoseidonNet consistently achieving the lowest error rates."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  ibims-1 datset  [ 14 ]  Normal Benchmark comparison. Higher is better for 11.25, 22.5, and 30(   \\uparrow  ), while lower is better for mean, median, and RMS normal error (   \\downarrow  ).",
        "table": "S4.T5.10",
        "footnotes": [
            ""
        ],
        "references": [
            "The rest of this paper is organised as follows: Section  2  reviews related work on underwater depth estimation and surface normals, focussing on state-of-the-art approaches and their limitations in resource-constrained environments. Section  3  details our proposed methodology, including the hybrid CNN-Transformer architecture and the domain-specific data pipeline. Section  4  presents the experimental setup and evaluates the performance of our model compared to existing methods, highlighting its efficiency and precision. Section  5  provides an in-depth discussion of the impact of the model on marine science tasks and outlines potential applications. Section  6  concludes the paper, summarising the contributions and suggesting directions for future research.",
            "Surface Normals Estimation:  Evaluating surface normal estimation in underwater environments presents a unique challenge due to the lack of an established benchmark or dataset specifically designed for this task. To address this limitation, we trained and evaluated PoseidonNet on the iBims-1 dataset  [ 14 ] . Although iBims-1 was not created for underwater images, it serves as a robust benchmark for assessing surface normal estimation techniques. As presented in Table  5 , PoseidonNet achieved competitive results on the iBims-1 benchmark. It performed strongly across key metrics like mean and median errors, as well as accuracy thresholds at various angles.",
            "Figure  5  showcases the outputs of our model trained exclusively on pseudo-labeled real images. The results demonstrate the models robustness in capturing intricate details in both depth and surface normals, particularly in visually complex underwater environments."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Quantitative Comparison Results of Ablation Study on the Benefit of Conv and Transformer Block. The value in  bold  means the best results.",
        "table": "S4.T6.6",
        "footnotes": [],
        "references": [
            "The rest of this paper is organised as follows: Section  2  reviews related work on underwater depth estimation and surface normals, focussing on state-of-the-art approaches and their limitations in resource-constrained environments. Section  3  details our proposed methodology, including the hybrid CNN-Transformer architecture and the domain-specific data pipeline. Section  4  presents the experimental setup and evaluates the performance of our model compared to existing methods, highlighting its efficiency and precision. Section  5  provides an in-depth discussion of the impact of the model on marine science tasks and outlines potential applications. Section  6  concludes the paper, summarising the contributions and suggesting directions for future research.",
            "In Figure  6 , we present a comparative analysis between our model (PoseidonNet), Marigold  [ 12 ] , and Metric3D-V2  [ 25 ] , using open-world images. All models were evaluated using their best publicly available versions from the HuggingFace model hub  [ 9 ] . The results consistently demonstrate that our method delivers more refined and accurate predictions for both depth and surface normals, especially in challenging scenarios where existing models, such as Marigold and Metric3D-V2, tend to exhibit limitations. Our approach demonstrates a noticeable improvement in sharpness and structure preservation, further illustrating its effectiveness in handling real-world imagery.",
            "Convolutional and Transformer Blocks : The study revealed that both the convolutional ( Conv ) and transformer ( Transformer ) blocks are critical to the models overall performance. Removing either block resulted in a significant decrease in accuracy, underscoring the importance of the synergy between these two components. This observation is supported by the quantitative results presented in Table  6 , where the full model, which includes both convolutional and transformer blocks, achieves the best performance."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Quantitative Comparison Results of Ablation Study on the Benefit of dilated convolutions, pooled concatenations and cross-stage connections. The value in  bold  means the best results.",
        "table": "S4.T7.6",
        "footnotes": [],
        "references": [
            "Dilated Convolutions, Pooled Concatenations, and Cross-Stage Connections : Similarly, the removal of any of the following components-dilated convolutions, pooled concatenations, or cross-stage connections-led to a degradation in the models performance. This suggests that each of these architectural elements plays a pivotal role in enhancing depth and surface normal estimation accuracy. The quantitative comparison results for these components are summarised in Table  7 , where the full model consistently outperforms the variants lacking these features."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Comparison Results for Complexity and FPS of the proposed PoseidonNet and compared methods. On a Linux platform using a single NVIDIA GeForce RTX 4090 GPU",
        "table": "S4.T8.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "Beyond accuracy, we also analysed PoseidonNets computational efficiency, a crucial factor for real-time applications and deployment on resource-constrained devices. As illustrated in Table  8 , PoseidonNet demonstrated a notable balance between accuracy and efficiency. It boasts a significantly lower parameter count and requires less memory compared to more complex models like TransDepth. Furthermore, PoseidonNet achieves a high frame rate (FPS), enabling real-time performance with minimal computational resources."
        ]
    }
}