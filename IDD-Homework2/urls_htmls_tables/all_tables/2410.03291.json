{
    "id_table_1": {
        "caption": "Table 1 :   Root mean square error in test, train time, and number of parameters of the meta-models for different context lengths  m m m italic_m .",
        "table": "S3.E6",
        "footnotes": [],
        "references": [
            "The Transformer architecture in  [ 11 ]  has been extended to address the three limitations previously mentioned. In the next paragraphs, we will systematically address each limitation, detailing the specific modifications made to the original architecture. We anticipate the final extended architecture in Fig.  1 , with the following main changes:",
            "Thus, with respect to  [ 11 ] , we modify the final layer of the decoder to output the two vectors:   m + 1 : N subscript  : m 1 N \\mu_{m+1:N} italic_ start_POSTSUBSCRIPT italic_m + 1 : italic_N end_POSTSUBSCRIPT  and   m + 1  R N  m subscript  m 1 superscript R N m \\sigma_{m+1}\\in\\mathbb{R}^{N-m} italic_ start_POSTSUBSCRIPT italic_m + 1 end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N - italic_m end_POSTSUPERSCRIPT  rather than providing a single point estimate vector  y ^ m + 1 : N  R N  m subscript ^ y : m 1 N superscript R N m \\hat{y}_{m+1:N}\\in\\mathbb{R}^{N-m} over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_m + 1 : italic_N end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_N - italic_m end_POSTSUPERSCRIPT , see the upper right block in Fig.  1 . It is worth remarking that the vector of means   m + 1 : N subscript  : m 1 N \\mu_{m+1:N} italic_ start_POSTSUBSCRIPT italic_m + 1 : italic_N end_POSTSUBSCRIPT  corresponds to the point estimate  y ^ m + 1 : N subscript ^ y : m 1 N \\hat{y}_{m+1:N} over^ start_ARG italic_y end_ARG start_POSTSUBSCRIPT italic_m + 1 : italic_N end_POSTSUBSCRIPT  already provided by the previous architecture, while   m + 1 : N subscript  : m 1 N \\sigma_{m+1:N} italic_ start_POSTSUBSCRIPT italic_m + 1 : italic_N end_POSTSUBSCRIPT  gives new insight about the meta-models predictive uncertainty.",
            "To this end, we modify the Transformer architecture by introducing a linear layer that processes the initial conditions, mapping each time step from  R n u + n y superscript R subscript n u subscript n y \\mathbb{R}^{n_{u}+n_{y}} blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT + italic_n start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  to  R d model superscript R subscript d model \\mathbb{R}^{{d_{\\rm model}}} blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT . Similarly, the query input is mapped from  R n u superscript R subscript n u \\mathbb{R}^{n_{u}} blackboard_R start_POSTSUPERSCRIPT italic_n start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  to  R d model superscript R subscript d model \\mathbb{R}^{{d_{\\rm model}}} blackboard_R start_POSTSUPERSCRIPT italic_d start_POSTSUBSCRIPT roman_model end_POSTSUBSCRIPT end_POSTSUPERSCRIPT  through a separate linear layer. The corresponding output sequences are then concatenated to form a single sequence of length  N N N italic_N , where all elements have dimension  d model subscript d model {d_{\\rm model}} italic_d start_POSTSUBSCRIPT roman_model end_POSTSUBSCRIPT . This sequence is combined with positional encoding before being passed into the decoder backbone of the Transformer, as shown in the layers before the decoder in Fig.  1 .",
            "The input-output context sequence  ( u 1 : m , y 1 : m ) subscript u : 1 m subscript y : 1 m (u_{1:m},y_{1:m}) ( italic_u start_POSTSUBSCRIPT 1 : italic_m end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 : italic_m end_POSTSUBSCRIPT )  is divided into  M M M italic_M  non-overlapping patches 1 1 1 For simplicity of exposition, we assume that  m m m italic_m  is multiple of  M M M italic_M . , each of length  L = m M L m M L=\\frac{m}{M} italic_L = divide start_ARG italic_m end_ARG start_ARG italic_M end_ARG  and dimension  n u + n y subscript n u subscript n y n_{u}+n_{y} italic_n start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT + italic_n start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT . The patches are then processed by an RNN which maps each of them into a single vector of dimension  d model subscript d model {d_{\\rm model}} italic_d start_POSTSUBSCRIPT roman_model end_POSTSUBSCRIPT . The resulting  patch embedding  sequence  p 1 : M subscript p : 1 M p_{1:M} italic_p start_POSTSUBSCRIPT 1 : italic_M end_POSTSUBSCRIPT  is then combined with position encoding to account for the temporal order and then fed as input to the encoders backbone (see Fig.  2 ). The modifications of the architecture with respect to  [ 11 ]  corresponds to the bottom-left blocks in Fig.  1 . Thus, the length of the sequence processed by the multi-head attention mechanism processes is reduced by a factor  L L L italic_L , significantly decreasing the computational burden by a factor of  O  ( L 2 ) O superscript L 2 O(L^{2}) italic_O ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) .",
            "The test root mean square error (RMSE), train time, and number of parameters of the different meta-models are reported in Table  1 . Furthermore, the validation RMSE over the iterations of the AdamW optimization algorithm is visualized in Fig.  3 .",
            "To assess robustness of the trained meta-model against a shift in the input distribution, we have generated test datasets from  256 256 256 256  different systems by applying as input a pseudo random binary signal (PRBS), thus with different characteristics with respect to the input used for meta-model training. The case of context length  m = 16000 m 16000 m=16000 italic_m = 16000  is reported. Output trajectories are visualized in Fig  5(a) , for all systems (left panel) and for one particular realization (right panel). The average RMSE is 0.3094, about 2.4x larger than the in-distribution case (see Table  1 ). It is interesting to observe that, as the performance decrease, the estimated uncertainty bands get wider."
        ]
    },
    "id_table_2": {
        "caption": "",
        "table": "S4.T1.3",
        "footnotes": [],
        "references": [
            "The rest of the paper is organized as follows. Section  2  describes the problem setting, analyzing in detail the meta-modeling framework in  [ 11 ]  and the limitations of the architecture introduced in that work. The salient architectural changes introduced in this paper to overcome these limitations are described in  3 . A numerical example is illustrated in Section  4  to demonstrate the effectiveness of the proposed methodology. Conclusions and the direction for future studies are discussed in the Section  5 .",
            "In order to handle Limitation L3, and thus considering long context sequences, we adopt a patching approach tailored to the meta-learning problem introduced in Section  2 , and described in the following paragraphs.",
            "The input-output context sequence  ( u 1 : m , y 1 : m ) subscript u : 1 m subscript y : 1 m (u_{1:m},y_{1:m}) ( italic_u start_POSTSUBSCRIPT 1 : italic_m end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 1 : italic_m end_POSTSUBSCRIPT )  is divided into  M M M italic_M  non-overlapping patches 1 1 1 For simplicity of exposition, we assume that  m m m italic_m  is multiple of  M M M italic_M . , each of length  L = m M L m M L=\\frac{m}{M} italic_L = divide start_ARG italic_m end_ARG start_ARG italic_M end_ARG  and dimension  n u + n y subscript n u subscript n y n_{u}+n_{y} italic_n start_POSTSUBSCRIPT italic_u end_POSTSUBSCRIPT + italic_n start_POSTSUBSCRIPT italic_y end_POSTSUBSCRIPT . The patches are then processed by an RNN which maps each of them into a single vector of dimension  d model subscript d model {d_{\\rm model}} italic_d start_POSTSUBSCRIPT roman_model end_POSTSUBSCRIPT . The resulting  patch embedding  sequence  p 1 : M subscript p : 1 M p_{1:M} italic_p start_POSTSUBSCRIPT 1 : italic_M end_POSTSUBSCRIPT  is then combined with position encoding to account for the temporal order and then fed as input to the encoders backbone (see Fig.  2 ). The modifications of the architecture with respect to  [ 11 ]  corresponds to the bottom-left blocks in Fig.  1 . Thus, the length of the sequence processed by the multi-head attention mechanism processes is reduced by a factor  L L L italic_L , significantly decreasing the computational burden by a factor of  O  ( L 2 ) O superscript L 2 O(L^{2}) italic_O ( italic_L start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ) ."
        ]
    }
}