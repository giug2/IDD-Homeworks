{
    "id_table_1": {
        "caption": "Table 1:  Test dataset names and properties, taken from  Hollmann et al. [ 2023 ] . Here  did  is the OpenML Dataset ID,  d  the number of features,  n  the number of instances, and  k  the number of classes in each dataset.",
        "table": "A2.T1.9",
        "footnotes": [
            ""
        ],
        "references": [
            "We introduce GAMformer (see   Figure   1 ), the first GAM method to estimate shape functions using ICL in a single forward pass. GAMformer distinguishes itself from existing GAM methods by employing a non-parametric, binned representation of shape functions, thus eliminating the need to impose a specific model class. Similar to TabPFN, our model is trained exclusively on large-scale synthetic datasets, yet demonstrates robust performance on real-world data. During training, GAMformer estimates shape functions for each feature based on the training datas features and labels. These estimated functions are then utilized to generate predictions for test data points by summing the shape function values across features. The model is trained end-to-end based on the GAMs predictions, ensuring that it learns to accurately construct shape functions for reliable predictions.",
            "We first provide a high-level overview of how GAMformer works before delving into the details of each of its components. GAMformer follows a two-step approach that first fits a GAM on training data  D train subscript D train D_{\\text{train}} italic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT  and then predicts on test data  x test subscript x test x_{\\text{test}} italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT , as illustrated in Figure  1 . Initially, a transformer estimates shape functions using ICL on the training dataset  D train subscript D train D_{\\text{train}} italic_D start_POSTSUBSCRIPT train end_POSTSUBSCRIPT . Next, predictions are computed by aggregating the shape function values for each test data point  x test subscript x test x_{\\text{test}} italic_x start_POSTSUBSCRIPT test end_POSTSUBSCRIPT . This methodology replaces the traditional data fitting process of GAM variants with a single forward pass of a pre-trained transformer model, eliminating the need for optimization and regularization hyperparameters. We now describe each model component in more detail.",
            "After pretraining GAMformer on the synthetic datasets, we evaluate it on both illustrative and real-world tasks in  4.1  and  4.2 , respectively. Moreover, in  4.3 , we highlight its potential to assist in decision-making in a clinical setting by predicting the mortality rate of patients in the intensive care unit (ICU). We compare to Explainable Boosting Machines (EBMs)  (Lou et al.,  2012 ;  2013 ; Caruana et al.,  2015 )  in terms of estimated shape function quality, as well as to other state-of-the-art tabular classification models such as XGBoost  (Chen and Guestrin,  2016 )  and TabPFN  (Hollmann et al.,  2023 )  in terms of predictive performance. On the downstream datasets, differently from EBM and the other baselines, GAMformer requires  only a single forward pass  of the transformer model to estimate the shape functions and construct prediction on the entire test set, without any parameter updates.",
            "To assess the transferability of pretraining on synthetic data to real-world tabular data, we evaluate GAMformers performance on the test datasets from TabPFN  (Hollmann et al.,  2023 ) , which include up to 2000 datapoints (see  Section   B.1  for dataset details).  Figure   6  reports Critical Diagrams (CD) from  Demsar ( 2006 )  showing the average rank across datasets for each method, with statistically tied methods grouped by horizontal bars. Our method outperforms EBM when using only main effects. With pair effects, both GAMformer* and EBM* show slight improvements, matching XGBoosts performance. We also compare against GAMs from the  mgcv  R  3 3 3 https://www.rdocumentation.org/packages/mgcv/versions/1.9-1/topics/gam  library. mgcv GAM models the relationships between features and output variables by combining parametric and non-parametric terms. The non-parametric components are represented by splines, thus capturing nonlinear relationships. In mgcv GAM the degree of smoothness in every spline is automatically selected using Restricted Maximum Likelihood (REML)  (Wood,  2010 ) .",
            "In this case study, we examine shape functions derived from GAMformer and EBMs (main effects only) using the MIMIC-II dataset  (Lee et al.,  2011a ) , a publicly available critical care dataset for predicting mortality risk based on various demographic and biophysical indicators. Our analysis focuses on four key clinical variables: Age, Heart Rate (HR), PFratio (PaO2/FiO2 ratio), and Glasgow Coma Scale (GCS), as shown in  Figure   7  (remaining variables in  Section   G.1 ). Further results on the MIMIC-III dataset are available in  Section   G.2 .",
            "As test dataset, we used the 30 datasets used in  Hollmann et al. [ 2023 ]  which were obtained from OpenML  [Vanschoren et al.,  2014 ] . These were chosen because they contain up to 2000 samples, 100 features and 10 classes, show in Table  1 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of GAMformer with other GAM variants and full complexity models on various datasets. We report ROC-AUC (%) (higher is better) and the standard error over 10 fold cross-validation. We also report results by pyGAM  [Serven and Brummitt,  2018 ] .",
        "table": "A2.T1.10",
        "footnotes": [
            ""
        ],
        "references": [
            "After pretraining GAMformer on the synthetic datasets, we evaluate it on both illustrative and real-world tasks in  4.1  and  4.2 , respectively. Moreover, in  4.3 , we highlight its potential to assist in decision-making in a clinical setting by predicting the mortality rate of patients in the intensive care unit (ICU). We compare to Explainable Boosting Machines (EBMs)  (Lou et al.,  2012 ;  2013 ; Caruana et al.,  2015 )  in terms of estimated shape function quality, as well as to other state-of-the-art tabular classification models such as XGBoost  (Chen and Guestrin,  2016 )  and TabPFN  (Hollmann et al.,  2023 )  in terms of predictive performance. On the downstream datasets, differently from EBM and the other baselines, GAMformer requires  only a single forward pass  of the transformer model to estimate the shape functions and construct prediction on the entire test set, without any parameter updates.",
            "Linear, binary classification.  We begin by evaluating GAMformer and, for comparison, EBMs on data generated by the linear, binary classification problem  f  ( x 1 , x 2 , x 3 ) = I  ( (  1 )  x 1 + 0  x 2 + x 3 > 0 ) f subscript x 1 subscript x 2 subscript x 3 I 1 subscript x 1 0 subscript x 2 subscript x 3 0 f(x_{1},x_{2},x_{3})=\\mathbb{I}((-1)x_{1}+0x_{2}+x_{3}>0) italic_f ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) = blackboard_I ( ( - 1 ) italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + 0 italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT > 0 ) , where  I I \\mathbb{I} blackboard_I  is the indicator function. We sample 2000 data points uniformly and independently from the interval [-2, 2] and split the data into 1500 training points and 500 test points. The results, shown in  Figure   2 , demonstrate that both GAMformer and EBMs accurately estimate the slopes for each feature and achieve an ROC AUC of 1.0 on the test dataset. However, the shape functions learned by GAMformer are noticeably smoother, suggesting that it may have captured some bias towards smoother models during pretraining. Additionally, we compared the effect of varying the number of datapoints or features in this example on EBMs and GAMformer in  Figure   3 . Our findings indicate that GAMformer consistently outperforms EBMs across various sample sizes and feature counts.",
            "We note that the small difference in performance between XGBoost and GAMformer suggests that the trade-offs in model capacity when choosing a main effects only GAM are often less significant than expected. As a result, the substantial interpretability benefits offered by the GAM model class become even more appealing, making it a viable choice for many applications. We present additional results on five binary classification datasets used by  Chang et al. ( 2021 )  in  Section   B.2 . Despite these datasets falling outside the recommended range of 2000 datapoints, GAMformer still demonstrates comparable performance to more complex models.",
            "In this case study, we examine shape functions derived from GAMformer and EBMs (main effects only) using the MIMIC-II dataset  (Lee et al.,  2011a ) , a publicly available critical care dataset for predicting mortality risk based on various demographic and biophysical indicators. Our analysis focuses on four key clinical variables: Age, Heart Rate (HR), PFratio (PaO2/FiO2 ratio), and Glasgow Coma Scale (GCS), as shown in  Figure   7  (remaining variables in  Section   G.1 ). Further results on the MIMIC-III dataset are available in  Section   G.2 .",
            "To gain a deeper understanding of GAMformers sensitivity to noisy or incorrect labels, we conducted an experiment similar to the one described in  Section   C.2 . We generated 300 data points and randomly perturbed the labels in the train split with increasing probability (75%, 25% train/test split), repeating each experiment 10 times.  Figure   9(b)  illustrates our findings. Once again, we observed that GAMformer exhibits a sensitivity to noisy labels comparable to that of EBMs."
        ]
    },
    "id_table_3": {
        "caption": "",
        "table": "A2.T2.35.35",
        "footnotes": [],
        "references": [
            "After pretraining GAMformer on the synthetic datasets, we evaluate it on both illustrative and real-world tasks in  4.1  and  4.2 , respectively. Moreover, in  4.3 , we highlight its potential to assist in decision-making in a clinical setting by predicting the mortality rate of patients in the intensive care unit (ICU). We compare to Explainable Boosting Machines (EBMs)  (Lou et al.,  2012 ;  2013 ; Caruana et al.,  2015 )  in terms of estimated shape function quality, as well as to other state-of-the-art tabular classification models such as XGBoost  (Chen and Guestrin,  2016 )  and TabPFN  (Hollmann et al.,  2023 )  in terms of predictive performance. On the downstream datasets, differently from EBM and the other baselines, GAMformer requires  only a single forward pass  of the transformer model to estimate the shape functions and construct prediction on the entire test set, without any parameter updates.",
            "Linear, binary classification.  We begin by evaluating GAMformer and, for comparison, EBMs on data generated by the linear, binary classification problem  f  ( x 1 , x 2 , x 3 ) = I  ( (  1 )  x 1 + 0  x 2 + x 3 > 0 ) f subscript x 1 subscript x 2 subscript x 3 I 1 subscript x 1 0 subscript x 2 subscript x 3 0 f(x_{1},x_{2},x_{3})=\\mathbb{I}((-1)x_{1}+0x_{2}+x_{3}>0) italic_f ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT ) = blackboard_I ( ( - 1 ) italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT + 0 italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT + italic_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT > 0 ) , where  I I \\mathbb{I} blackboard_I  is the indicator function. We sample 2000 data points uniformly and independently from the interval [-2, 2] and split the data into 1500 training points and 500 test points. The results, shown in  Figure   2 , demonstrate that both GAMformer and EBMs accurately estimate the slopes for each feature and achieve an ROC AUC of 1.0 on the test dataset. However, the shape functions learned by GAMformer are noticeably smoother, suggesting that it may have captured some bias towards smoother models during pretraining. Additionally, we compared the effect of varying the number of datapoints or features in this example on EBMs and GAMformer in  Figure   3 . Our findings indicate that GAMformer consistently outperforms EBMs across various sample sizes and feature counts.",
            "Classification Boundaries.  We visualize the classification boundaries of GAMformer compared to TabPFN and EBM on the scikit-learn  (Pedregosa et al.,  2011 )  test datasets in  Figure   5 . We find that GAMformer performs similarly to TabPFN and EBMs on most of the example datasets. LA-NAM  (Bouchiat et al.,  2024 )  (main effects only), a Bayesian version of NAMs  (Agarwal et al.,  2021 ) , provides good uncertainty estimates despite exhibiting slightly worse predictive performance. It is worth noting that GAMformer, EBM and LA-NAM struggle with accurately modeling the XOR dataset (bottom row) due to the absence of higher-order feature interaction terms in these models. This is resolved by incorporating second-order effects (EBM   and GAMformer  ; see Section  3.4  for details), allowing them to effectively learn the non-linear decision boundary of the XOR function."
        ]
    }
}