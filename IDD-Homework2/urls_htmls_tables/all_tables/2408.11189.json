{
    "id_table_1": {
        "caption": "Table 1:  Reading with Intent Results across different models. The top of the table shows the performance of the various model families on the base version of the prompt. The middle of the table shows the performance of the same model families on the reading with intent prompt presented in Section  4  using the ground truth intent labels. The third part of the table also uses the reading with intent prompt, but the intent tags are derived from the intent tagging system described in Sections  4.2  and  5 . In this table, we can see that for the Llama2 and Mistral family of models, the reading with intent prompt boosts performance; whereas for the Phi and Qwen family of models, the prompt performance is a bit more varied (but overall performance-boosting).",
        "table": "S5.T1.144",
        "footnotes": [],
        "references": [
            "Although retrieval is a critical portion of the RAG pipeline, it is not the focus of this paper. Therefore, an off-the-shelf SOTA dense retrieval method  (Wang et al.  2021 )  was used. For each question in the dataset the top  200 200 200 200  passages were retrieved as illustrated in Figure  1 . These retrieved passages formed the base from which our dataset was derived.",
            "Figure  1  shows the process for creating each type of sarcastic passage. For the first type of passage, the retrieved passage was inputted into an LLM with a specialized prompt to rewrite the passage sarcastically. For correctly retrieved passages, the prompt included instructions to ensure that the correct facts were not distorted during the rewriting process. For the second type of passage, the process involved two steps. First, the retrieved passage was inputted into an LLM with a prompt specifically designed to distort the facts. In the case of correctly retrieved passages, part of the prompt explicitly directed the LLM to alter the facts related to the ground truth answer, in addition to any other distortions. Once a corpus of fact-distorted passages was created, these were then inputted into the LLM again, this time with a prompt to add a layer of sarcasm, following the same method used for the first type of passage. This approach ensures that the dataset contains both accurate and intentionally misleading sarcastic passages, enabling evaluations of how models handle different types of sarcastic content.",
            "The goal of our work is for LLMs to be cognizant of the emotional intent of the passages they read when trying to answer open-domain questions. To achieve this, we created a dataset with sarcasm-poisoned passages to experiment on (Section  3 ). In this section, we present our framework, Reading with Intent, which enhances the models ability to better understand the connotation of the passages it is processing. In Section  4.1 , we present a prompt-based approach that improves reading accuracy on the datasets presented in Section  3 . In Section  4.2 , we further enhance our prompt-based approach with trained intent tags.",
            "In this section, we present the results of our overall system. Table  1  summarizes the outcomes across the four versions of the retrieval corpus used with the Natural Questions dataset. The NQ dataset refers to the base Natural Questions retrieved passages with no sarcasm poisoning. FS NQ represents the Natural Questions dataset where all non-sarcastic passages were replaced with factually correct sarcastic versions. The PS-M NQ dataset is the version of the dataset where the fact-distorted sarcastic passages were manually inserted next to correctly retrieved passages and factually-correct sarcastic passages are substituted for a portion of the incorrectly retrieved passages. This is the second dataset referred to in Section  3.4 . Lastly, The PS-A NQ dataset is the version of the Natural Questions dataset where the fact-distorted sarcastic passages are inserted into the retrieval corpus and the retriever selects a new set of top-10 passages to input as context for the LLM. This is the last dataset described in Section  3.4 .",
            "Table  1  shows that the Reading with Intent prompt boosts performance across the various datasets for both the Llama2 and Mistral family of models, across model scales. For Llama2, the overall average percent difference was  5.5 % percent 5.5 5.5\\% 5.5 % , with the  7 7 7 7 b model showing an average boost of  8.2 % percent 8.2 8.2\\% 8.2 %  and the  70 70 70 70 b model showing a  2.8 % percent 2.8 2.8\\% 2.8 %  improvement. The Mistral models show an average increase of  3.5 % percent 3.5 3.5\\% 3.5 %  with the  7 7 7 7 b model seeing an average  5 % percent 5 5\\% 5 %  boost and the  8 8 8 8 x 22 22 22 22 b a  1.9 % percent 1.9 1.9\\% 1.9 %  increase. Notably, in these families, smaller models using the Reading with Intent prompt on the PS-M NQ and PS-A NQ datasets performed comparably to larger models with only the base prompt. The non-oracle tags also mostly outperform the base prompt, but more moderately. In the Phi-3 model family, the greatest improvement ( 1.6 % percent 1.6 1.6\\% 1.6 % ) was observed in the Phi-3-Mini model, while the Phi-3-Medium model saw a modest increase, and the Phi-3-Small experienced a performance decline (  2.4 % percent 2.4 -2.4\\% - 2.4 % ). This pattern is similarly seen in the Qwen2 model family where the smallest model had the largest performance boost ( 3.2 % percent 3.2 3.2\\% 3.2 % ), while the larger models saw either no benefit or a slight decline (-0.4%) with the Reading with Intent prompt.",
            "The performance across datasets, shown in Table  1 , is fairly consistent. On average, models performed slightly better on the FS NQ dataset than on the NQ dataset when using either the base prompt or the Reading with Intent prompt with oracle tags. Following these two datasets, models performed worse on the PS-A NQ dataset and even more so on the PS-M NQ dataset across all prompts. These results indicate that models can equally understand the denotative content of a passage, whether or not it is sarcasm-laced. However, consistent with real-world findings, when sarcasm-laced passages contain falsehoods, models often fail to understand the sarcasm as a potential signal of the passages inaccuracy. The consistently poorer performance on the PS-M NQ dataset compared to the PS-A NQ dataset indicates that the specific ordering in the PS-M NQ dataset presents a greater challenge than the naturally retrieved passages from the PS-A NQ dataset. This extra difficulty makes PS-M NQ a good testbed for future Reading with Intent work."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The retrieval results when sarcastic passages are added in the retrieval corpus. S@K indicates the percent of the top-k retrievals are sarcastic passages. Adding sarcastic passages to the retrieval corpus weakens retrieval performance by  3 % percent 3 3\\% 3 % - 6 % percent 6 6\\% 6 % ; however, most sarcastic retrievals that occur seem to be replacing passages that were incorrectly retrieved in the first place.",
        "table": "S5.T2.60",
        "footnotes": [],
        "references": [
            "The goal of our work is for LLMs to be cognizant of the emotional intent of the passages they read when trying to answer open-domain questions. To achieve this, we created a dataset with sarcasm-poisoned passages to experiment on (Section  3 ). In this section, we present our framework, Reading with Intent, which enhances the models ability to better understand the connotation of the passages it is processing. In Section  4.1 , we present a prompt-based approach that improves reading accuracy on the datasets presented in Section  3 . In Section  4.2 , we further enhance our prompt-based approach with trained intent tags.",
            "Without explicitly directing the model to focus on the connotation of the text, the model pays less attention to the connotation. However, by instructing the model to consider the intent behind the text, we can effectively refocus its attention on the connotation of the input it receives. In our proposed system, we explicitly prompt the LLM to pay attention to the textual connotation. This is illustrated in Figure  2  which shows our systems pipeline.",
            "We also analyzed the responses of different retrieval systems (GPL  (Wang et al.  2021 ) , LLM-Embedder  (Zhang et al.  2023 ) , and BGE-M3  (Chen et al.  2024 ) ) to the inclusion of sarcastic passages in the retrieval corpus. Table  2  summarizes the findings, showing that the performance of each retrieval system at Recall@K declines by  3 % percent 3 3\\% 3 % - 6 % percent 6 6\\% 6 %  with the addition of approximately 1 million sarcastic passages. Sarcastic passages are significantly overrepresented in retrievals; despite making up only  4.5 % percent 4.5 4.5\\% 4.5 %  of the corpus, they appear in the top-1 results  9.7 % percent 9.7 9.7\\% 9.7 % - 16.2 % percent 16.2 16.2\\% 16.2 %  of the time, leading to a  2.2 2.2 2.2 2.2 - 3.6 3.6 3.6 3.6 x over-representation. In the top-100, sarcastic passages are overrepresented by a factor of  4 4 4 4 - 8 8 8 8 x, depending on the retriever used. Interestingly, the number of sarcastic retrievals tends to plateau around the top-20, with only incremental changes beyond that point. Further analysis reveals that sarcastic passages retrieved are rarely direct substitutions for their non-sarcastic counterparts. Instead, an entirely different sarcastic passage is often retrieved. Additionally, while it is common for a sarcastic passage to precede or follow a correct retrieval (occurring   90 % similar-to absent percent 90 {\\sim}90\\%  90 %  and   99 % similar-to absent percent 99 {\\sim}99\\%  99 %  of the time, respectively, in the top-100), it is much rarer for these sarcastic passages to be the sarcastic version of the correct passagethis happens only  1.3 % percent 1.3 1.3\\% 1.3 % - 1.9 % percent 1.9 1.9\\% 1.9 %  before insertion and  1.7 % percent 1.7 1.7\\% 1.7 % - 4.1 % percent 4.1 4.1\\% 4.1 %  after insertion in the top-100 results."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Effect of different prompt components on Llama2-7B-chat. The intent prompt is the most significant factor in improving the models ability to read emotionally inflected text. NQ = unmodified passages; FS NQ = all passages retrieved using GPL replaced by sarcastic equivalents; PS-M NQ = sarcastic fact-distorted passages were manually inserted before correctly retrieved passages; PS-A NQ = sarcastic fact-distorted passages were put into the retrieval corpus and GPL was used to pick a new top-10 passages.",
        "table": "S6.T3.16",
        "footnotes": [],
        "references": [
            "The goal of our work is for LLMs to be cognizant of the emotional intent of the passages they read when trying to answer open-domain questions. To achieve this, we created a dataset with sarcasm-poisoned passages to experiment on (Section  3 ). In this section, we present our framework, Reading with Intent, which enhances the models ability to better understand the connotation of the passages it is processing. In Section  4.1 , we present a prompt-based approach that improves reading accuracy on the datasets presented in Section  3 . In Section  4.2 , we further enhance our prompt-based approach with trained intent tags.",
            "GPL  (Wang et al.  2021 )  was used to retrieve the initial passages for poisoning. The top-200 passages for each query in the NQ validation set were retrieved, totaling  971 , 384 971 384 971,384 971 , 384  unique passages. GPL also constructed the dataset described in the last part of Section  3.4 , where fact-distorted sarcastic passages were added back into the retrieval corpus, and passages for the reader model were re-retrieved.",
            "In this section, we present the results of our overall system. Table  1  summarizes the outcomes across the four versions of the retrieval corpus used with the Natural Questions dataset. The NQ dataset refers to the base Natural Questions retrieved passages with no sarcasm poisoning. FS NQ represents the Natural Questions dataset where all non-sarcastic passages were replaced with factually correct sarcastic versions. The PS-M NQ dataset is the version of the dataset where the fact-distorted sarcastic passages were manually inserted next to correctly retrieved passages and factually-correct sarcastic passages are substituted for a portion of the incorrectly retrieved passages. This is the second dataset referred to in Section  3.4 . Lastly, The PS-A NQ dataset is the version of the Natural Questions dataset where the fact-distorted sarcastic passages are inserted into the retrieval corpus and the retriever selects a new set of top-10 passages to input as context for the LLM. This is the last dataset described in Section  3.4 .",
            "The first part of our analysis examines the impact of the different components of the prompt. Our prompt contains two components: the intent reading prompt and the intent tag. As shown in Table  3  adding the intent prompt produces a significantly larger performance boost than adding the intent tag. Simply instructing the model to consider the intent of the passage helps the LLM better interpret the passages connotation. The intent tag also improves predictions by preventing the model from over-interpreting the texts emotional inflection. This effect is noticeable in the base dataset and the PS-A dataset, where there are a fair amount of non-sarcastic passages. Intent tags, in these cases, bridge the performance gap between the full prompt and only the intent reading prompt. Together, these two components significantly increase performance compared to the base prompt.",
            "Manually creating a dataset that is intentionally sarcasm-poisoned or fact-distorted would be a vast and costly undertaking in terms of time, labor, and resources. To overcome this challenge, we used a state-of-the-art LLM, as described in Section 3, to generate the datasets presented in this paper. The Llama-3-70B Instruct model was prompted in specific ways to create sarcastic and fact-distorted passages. Figure  3  shows the prompts used for sarcasm-poisoning and fact-distortion."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Effect of intent tag position on Llama2-7B-chat. Having the intent tag after the passage on average provides a greater boost to performance from the prompt.",
        "table": "S6.T4.8",
        "footnotes": [],
        "references": [
            "The goal of our work is for LLMs to be cognizant of the emotional intent of the passages they read when trying to answer open-domain questions. To achieve this, we created a dataset with sarcasm-poisoned passages to experiment on (Section  3 ). In this section, we present our framework, Reading with Intent, which enhances the models ability to better understand the connotation of the passages it is processing. In Section  4.1 , we present a prompt-based approach that improves reading accuracy on the datasets presented in Section  3 . In Section  4.2 , we further enhance our prompt-based approach with trained intent tags.",
            "GPL  (Wang et al.  2021 )  was used to retrieve the initial passages for poisoning. The top-200 passages for each query in the NQ validation set were retrieved, totaling  971 , 384 971 384 971,384 971 , 384  unique passages. GPL also constructed the dataset described in the last part of Section  3.4 , where fact-distorted sarcastic passages were added back into the retrieval corpus, and passages for the reader model were re-retrieved.",
            "In this section, we present the results of our overall system. Table  1  summarizes the outcomes across the four versions of the retrieval corpus used with the Natural Questions dataset. The NQ dataset refers to the base Natural Questions retrieved passages with no sarcasm poisoning. FS NQ represents the Natural Questions dataset where all non-sarcastic passages were replaced with factually correct sarcastic versions. The PS-M NQ dataset is the version of the dataset where the fact-distorted sarcastic passages were manually inserted next to correctly retrieved passages and factually-correct sarcastic passages are substituted for a portion of the incorrectly retrieved passages. This is the second dataset referred to in Section  3.4 . Lastly, The PS-A NQ dataset is the version of the Natural Questions dataset where the fact-distorted sarcastic passages are inserted into the retrieval corpus and the retriever selects a new set of top-10 passages to input as context for the LLM. This is the last dataset described in Section  3.4 .",
            "The next parameter analyzed was the position of the intent tag. Whether the tag is placed before or after the passage could influence the models ability to correctly read the passages intent. As shown in Table  4 , placing the intent tag after the passage boosts performance by an average of  0.6 % percent 0.6 0.6\\% 0.6 % . This indicates that the model can change its interpretation of a passage after reading it based on the metadata provided.",
            "Figure  4  presents the Reading with Intent prompt described in the main body of the text. The figure breaks down the different components of the prompt as previously described in the paper."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Effect of fact-distorted sarcastic passage insertion position. Inserting sarcastic passages before the correct passage harms the model a lot more than adding the sarcastic passage after the correct passage. However, in both cases the Reading with Intent prompt boosts performance.",
        "table": "S6.T5.4",
        "footnotes": [],
        "references": [
            "The position of the factually distorted passage relative to the correct passage was also varied. Table  5  shows that this positioning significantly impacts performance. Presenting the non-sarcastic, factually correct passage first greatly improves performance compared to when it is presented second. Conversely, placing the correct passage after the factually distorted one greatly decreases performance. This aligns with prior work, such as  (Liu et al.  2024 ) , which suggests that models are more biased toward reading earlier passages. When two passages present similar information side by side, the model is likely to focus more on the two passages, likely looking at the details of the first passage and skim the second passage."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Intent classifier performance on each passage type. Passages that have no fact distortion and are not sarcastic are the base passages used to create the other passages.",
        "table": "S6.T6.4",
        "footnotes": [],
        "references": [
            "Lastly, we evaluated the performance of our intent classifier. We tested the model on  40 , 000 40 000 40,000 40 , 000  randomly selected passages, with  10 , 000 10 000 10,000 10 , 000  each from the fully sarcastic dataset, the fact-distorted dataset, the fact-distorted sarcastic dataset, and the base dataset. Since the classifier was trained on the SARC dataset, there was no concern about overlap between the training and validation passages. Table  6  shows that the model correctly differentiated between sarcastic and non-sarcastic passages  96.9 % percent 96.9 96.9\\% 96.9 %  of the time. Future work will aim to create sarcastic passages that are more challenging to distinguish from non-sarcastic ones."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Randomly sampled QAs from the NQ dataset that Llama2-7B responded to. The rows in green show where our evaluation method corresponds with reality. Rows in red show where our evaluation method does not properly capture reality. The mistakes our evaluation system make are akin to the mistakes an EM evaluation metric would have while being more flexible to long generations.",
        "table": "A2.T7.1",
        "footnotes": [],
        "references": [
            "The LLMs in the Reading with Intent system were prompted in a manner that mimicked how LLMs are typically used in production applications and by end-users. Although the LLM was instructed to be terse, this was only done once, often resulting in the LLM producing complete sentences instead of 1-3 word answers. To evaluate these outputs, we considered an answer correct if the ground truth was contained within the LLMs output. In this appendix, Table  7  provides a sample of LLM outputs alongside the ground truth as a visualization and qualitative sanity check for readers. Rows highlighted in green indicate where our evaluation metric aligns with the expected outcome, while rows in red indicate discrepancies. The mistakes made by our evaluation system are similar to those an Exact Match (EM) scoring system would make. For example, for the query  protein that serves as the precursor molecule for thyroid hormones , the ground truth answer is  Thyroglobulin (TG) , and the inferred answer included  thyroglobulin . However, because the model did not also provide the molecules abbreviation, this answer was marked incorrect  a mistake that would also occur with EM scoring. On the other hand, our evaluation method allows for correct assessment even when the model responds in a complete sentence, as seen with the query  where do the Great Lakes meet the ocean  where the models correct full-sentence answer was marked accurate."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  F1 and EM scores for Reading with Intent across different models and prompt types.",
        "table": "A3.T8.144",
        "footnotes": [],
        "references": [
            "In addition to the evaluation presented in the papers main body, we also prompted the model in a way that hewed more closely to how models are typically prompted in Open-Domain QA research. Here, the prompts were optimized not only to Read with Intent but also to produce answers limited to 1-3 words. This approach, however, appears to make the task more challenging for the model, as it now has two goals to optimize. Table  8  presents the results of these experiments. The general trend for models between datasets remains consistent: FS NQ slightly outperforms NQ (or the performances are close together); both outperform PS-A and PS-M NQ; and PS-A NQ still outperforms PS-M NQ. However, in this scenario, the base prompt slightly outperforms the Reading with Intent prompt, likely because the model is balancing multiple objectivesanswering the question, being brief, and interpreting the intent of the provided passages simultaneously."
        ]
    },
    "global_footnotes": [
        "Citation withheld pending publication."
    ]
}