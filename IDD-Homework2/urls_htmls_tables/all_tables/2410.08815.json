{
    "id_table_1": {
        "caption": "Table 1:  LLM-judged scores and exact matching rate in knowledge-intensive tasks of Loong benchmark. From Set 1 to Set 4, task complexity gradually increases, as reflected by the growing token number of documents. The table show two main conclusions: StructRAG get the SOTA performance in overall metrics. And the more complex the task, the greater the improvement of StructRAG.",
        "table": "S5.T1.1.1",
        "footnotes": [],
        "references": [
            "Motivated by this, we propose  StructRAG , which employs a hybrid information structuring mechanism to construct and utilize structured knowledge in the most suitable format based on task requirements.  As illustrated in Figure  1 , the StructRAG framework consists of three modules designed to sequentially identify the most suitable structure type, construct structured knowledge in that format, and utilize that structured knowledge to infer the final answer.  First, recognizing that different structure types are suited for different tasks, a hybrid structure router is proposed to determine the most appropriate structure type based on the question and document information of the current task.  Second, given that constructing structured knowledge is complex and requires strong comprehension and generation abilities, an LLM-based scattered knowledge structurizer is employed to convert raw documents into structured knowledge in the optimal type.  Finally, since questions in knowledge-intensive reasoning tasks can often be a complex composite problems that are challenging to solve directly, a structured knowledge utilizer is used to perform question decomposition and precise knowledge extraction for more accurate answer inference.",
            "As mentioned, due to badly dispersed information in knowledge-intensive reasoning tasks, the traditional retrieval module in RAG could retrieve chunks containing substantial textual noise, making it difficult for the generation module to extract useful information for inference.  Drawing inspiration from cognitive theories on how humans tackle such tasks, this paper proposes StructRAG, which utilizes a hybrid information structurization mechanism to construct and leverage structured knowledge in its optimal form.  Specifically, as illustrated in Figure  1 , StructRAG first employs a hybrid structure router to identify the most appropriate structure type for the given task, and then employs a scattered knowledge structurizer to transform raw documents into structured knowledge in that format, and finally incorporates a structured knowledge utilizer to break down complex questions into simpler sub-questions, enabling more accurate reasoning on structured knowledge.",
            "In the StructRAG framework described above, the core factor is accurately determining the most suitable structure type based on the input task, and the performance of the hybrid structure router directly influences the overall effectiveness of the framework.  Therefore, to achieve a high-performance router, we propose a training method to enhance the ability of LLMs in identifying the suitable structure type of knowledge.  Specifically, given the strong capabilities of reinforcement learning in decision-making scenarios, we train the router using the DPO algorithm, which achieves results similar to reinforcement learning while avoiding the need for additional reward models. Regarding training data, since there is no existing preference data for the optimal structure type selection task, we design a synthesizing-simulating-judging method to efficiently construct preference pairs for training. A detailed explanation is provided in the following paragraphs, along with examples and prompts in the Appendix  A.1 .",
            "Results compared with baselines are shown in Table  1  and Table  2 , there are two main conclusions:",
            "1) StructRAG is a powerful solution to addressing knowledge-intensive reasoning tasks.   Based on the experimental results in Table  1 , StructRAG outperforms the baselines in most tasks and document length settings, and in the overall metric, StructRAG exceeds all baselines in both LLM score and EM rate. In addition, as shown in Table  2 , StructRAG achieves the best average performance compared to all baselines in Podcast Transcripts. All in all, these experimental findings demonstrate that StructRAG can effectively address knowledge-intensive reasoning tasks and improve a lot compared with previous long-context methods, and different kind of existing powerful RAG techniques.",
            "2) StructRAG is particularly suitable for complex tasks, performance improvement becomes more significant in scenarios with more dispersed information.   Based on the overall performance in Table  1 , the performance comparison between StructRAG and the long-context baseline shows that StructRAG achieves performance improvements of approximately 9, 15, 22, and 23 on Set 1, Set 2, Set 3, and Set 4, respectively. Similarly, comparing StructRAG with RAG shows performance improvements of around 15, 15, and 22 on Set 2, Set 3, and Set 4, respectively. Each set represents the total length of the given documents, with Set 1 being the shortest and Set 4 being the longest. This means that the information needed to answer the questions becomes more dispersed as the length of the documents increases, and making the reasoning process more challenging. Therefore, these results indicate that StructRAG shows more significant improvements over the baselines with longer documents and more scattered information, demonstrating that abilities of our framework to construct and use the optimal type of structured knowledge is especially effective for complex tasks.",
            "According to the experimental results in Table  1 , StructRAG surpasses baselines in general score, but falls short in seven sub-situations for the exact matching rate. Therefore, we analysis some cases that StructRAG method gets high score but fails exact matching.  The reason is mainly about structurization process may alter the textual format of original information. As shown in Table  6 , there are some wording differences between structured knowledge and raw information (e.g. from original  $ 1,308,463  to  138463  in the table). Intuitively this aligns with the common sense, where structurizer is a probabilistic language model rather than rule-based model, thus some possible textual loss may be unavoidable, and output from GraphRAG method also show similar issue."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Win rate of StructRAG vs. baselines on Podcast Transcripts. StructRAG achieves the best average performance compared to all baselines, further conforming effectiveness of framework.",
        "table": "S5.T2.1.1",
        "footnotes": [],
        "references": [
            "Due to the scarcity of training data for selecting the optimal structure type in the current NLP community, we employ a synthesizing-simulating-judging method to construct preference pairs for training the router.  Specifically, as illustrated in Figure  2 , given several manually collected seed tasks that covering the possible structure types, we first use LLMs to synthesize a set of new tasks by the in-context learning method, where each task contains a question and core context for documents.  Then, for each synthetic task, LLMs is employed to simulate the process of addressing this task by structured knowledge in different types, thus getting different simulated solutions.  Finally, a LLM-based judge compares these simulated solutions for solving the task, generating preference pairs regarding the structure types.  Each constructed data entry includes a question, the core contents of documents, the chosen structure type, and the rejected structure type, as expressed as follows:",
            "Results compared with baselines are shown in Table  1  and Table  2 , there are two main conclusions:",
            "1) StructRAG is a powerful solution to addressing knowledge-intensive reasoning tasks.   Based on the experimental results in Table  1 , StructRAG outperforms the baselines in most tasks and document length settings, and in the overall metric, StructRAG exceeds all baselines in both LLM score and EM rate. In addition, as shown in Table  2 , StructRAG achieves the best average performance compared to all baselines in Podcast Transcripts. All in all, these experimental findings demonstrate that StructRAG can effectively address knowledge-intensive reasoning tasks and improve a lot compared with previous long-context methods, and different kind of existing powerful RAG techniques."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Ablation results of three modules. The table shows that all three module are with positive contribution, and the most core module is the hybrid structure router.",
        "table": "S5.T3.1.1",
        "footnotes": [],
        "references": [
            "After identifying the most suitable structure type, StructRAG extracts the textual knowledge scattered across raw documents and reconstructs it into structured knowledge.  This process requires a comprehensive understanding of all raw documents and and precise formatting of the information, making it a challenging and flexible problem.  Therefore, StructRAG employs an LLM-based scattered knowledge structurizer to facilitate the structurization process.  Specifically, as shown in Eq.  3  the structurizer  S S \\mathcal{S} caligraphic_S  takes the question  q q q italic_q , the selected type  t t t italic_t , and each raw document  d ( i ) superscript d i d^{(i)} italic_d start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT  as input, and extract the structured knowledge  k t ( i ) superscript subscript k t i k_{t}^{(i)} italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT  from the document via the powerful understanding and generation ability of LLMs.  In addition, a description of the structured knowledge  k t subscript k t k_{t} italic_k start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is also generated.",
            "where  n n n italic_n  is number of sub-questions,  Q ^ ^ Q \\mathcal{\\hat{Q}} over^ start_ARG caligraphic_Q end_ARG  is set of all sub-questions,  K ^ t subscript ^ K t \\hat{K}_{t} over^ start_ARG italic_K end_ARG start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  is whole precise knowledge for all sub-questions, and  U decompose subscript U decompose \\mathcal{U}_{\\text{decompose}} caligraphic_U start_POSTSUBSCRIPT decompose end_POSTSUBSCRIPT ,  U extract subscript U extract \\mathcal{U}_{\\text{extract}} caligraphic_U start_POSTSUBSCRIPT extract end_POSTSUBSCRIPT  and  U infer subscript U infer \\mathcal{U}_{\\text{infer}} caligraphic_U start_POSTSUBSCRIPT infer end_POSTSUBSCRIPT  are process of decomposition, extraction and inference, respectively. More details about the utilizer are shown in Appendix  A.3 .",
            "To validate the role of each module in StructRAG, we perform ablation experiments. As shown in Table  3 , w/o router refers to random routing, w/o structurizer means using only chunks, and w/o utilizer refers to directly concatenating the structured knowledge with the original question for answer generation. There are following conclusions:"
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Results of only containing structured knowledge in one fixed type. It shows any single fixed type is insufficient, confirming the advance of StructRAG via hybrid information structurization.",
        "table": "S5.F4.fig1.1.1",
        "footnotes": [],
        "references": [
            "The core content  C C C italic_C  is the concentrate of the titles or the first few sentences from each document  d ( i ) superscript d i d^{(i)} italic_d start_POSTSUPERSCRIPT ( italic_i ) end_POSTSUPERSCRIPT .  In our work, there are five candidate structure types for five kinds of knowledge-intensive tasks: table for statistical tasks, graph for long-chain tasks, algorithm for planning tasks, catalogue for summarizing tasks, and chunk for simple single-hop tasks.  Considering the core effect of the router in the overall framework, our work designs a DPO-based training method to develop a router that excels in knowledge type decision, which is detailed in Section  4 .",
            "1) Selecting the optimal type of knowledge based on the task is challenging for raw LLMs without special training.   Based on the experimental results in Table  4 , the router trained based on Qwen2-7B-Instruct model significantly outperforms the 72B model with few-shot setting. This  indicates that LLMs need some special training to get the ability of selecting the optimal structure type based on needs of the task, even when the model scale reaches 72B.",
            "2) The performance of hybrid structure router is with significant relevance with the final performance of StructRAG.   As shown in Figure  4 , we select Qwen2-72B-Instruct (zero-shot) as the weak router, and design a completely random router and a completely incorrect bad router. The curves in the figure clearly show a positive correlation between router accuracy and the overall performance of the StructRAG framework. This further demonstrates that selecting knowledge types that match the task needs for augmentation is crucial in knowledge-intensice reasoning tasks.",
            "Using a single fixed type of knowledge cannot achieve good performance on diverse tasks.   Based on the experimental results in Table  4 , it shows that for both scores and exact matching rate, using a single fixed type performs worse than selecting the optimal structure type for needs of the input task. In comparison, the performance degradation is least when only using chunk, with a reduction from 60.38 to 53.92, in other cases, the performance decline is more significant. This can demonstrate the effectiveness of cognitive fit theory in LLMs, meaning that using structured knowledge in the most suitable type can effectively enhance problem-solving abilities."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "S5.T4.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_6": {
        "caption": "",
        "table": "S5.F6.fig1.1.1",
        "footnotes": [],
        "references": [
            "According to the experimental results in Table  1 , StructRAG surpasses baselines in general score, but falls short in seven sub-situations for the exact matching rate. Therefore, we analysis some cases that StructRAG method gets high score but fails exact matching.  The reason is mainly about structurization process may alter the textual format of original information. As shown in Table  6 , there are some wording differences between structured knowledge and raw information (e.g. from original  $ 1,308,463  to  138463  in the table). Intuitively this aligns with the common sense, where structurizer is a probabilistic language model rather than rule-based model, thus some possible textual loss may be unavoidable, and output from GraphRAG method also show similar issue.",
            "In this section, we report average latency of StructRAG and compare it with the RQ-RAG  (Chan et al.,  2024 )  and GraphRAG  (Edge et al.,  2024 ) . The latency includes two components: The first part is constructing latency, referring to the process of iteratively retrieving chunks for RQ-RAG, constructing graphs for GraphRAG, and determining the optimal knowledge type and constructing corresponding structure for StructRAG. The second part is reading latency, referring to the process of using augmented knowledge to generate final answers. As shown in Table  6 . StructRAG has slightly higher latency compared to RQ-RAG but is obviously faster than GraphRAG. Therefore, StructRAG is a kind of high-performance framework with available implementing speed."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "S5.F6.1.1",
        "footnotes": [],
        "references": [
            "Considering that questions in knowledge-intensive reasoning can be complex combinatorial tasks, breaking them down into multiple simpler sub-questions can leverage the structured knowledge more effectively for reasoning. Therefore, we designed prompts to drive LLMs to achieve question decomposition and precise knowledge extraction, as shown in Figure  7 ."
        ]
    },
    "global_footnotes": [
        "https://github.com/MozerWang/Loong",
        "https://github.com/huggingface/trl",
        "https://pypi.org/project/vllm/",
        "https://github.com/chanchimin/RQ-RAG",
        "https://pypi.org/project/graphrag/"
    ]
}