{
    "S5.T1": {
        "caption": "Table 1. We see more NE gains when using more interaction features. We do expect the number to plateau beyond a certain number of features as a large portion of the impact comes from the top interaction features.",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S5.T1.1.1.2\">Model Architecture</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T1.1.1.1\">NE (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T1.1.1.1.m1.1\"><semantics id=\"S5.T1.1.1.1.m1.1a\"><mo id=\"S5.T1.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T1.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.1.1.1.m1.1b\"><ci id=\"S5.T1.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.1.1.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T1.1.1.1.m1.1d\">&#8595;</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T1.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S5.T1.1.2.1.1\">Siamese Networks</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T1.1.2.1.2\">0.0% (reference)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S5.T1.1.3.2.1\">Siamese + Interaction Tower (1x) + MergeNet</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T1.1.3.2.2\">-0.3%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T1.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S5.T1.1.4.3.1\">Siamese + Interaction Tower (20x) + MergeNet</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T1.1.4.3.2\">-0.45%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "In this ablation study, we aim to capture the benefit of using interaction features and exploring other \u00a1user-ad\u00bf interactions instead of a dot product interaction. To do this, we ablate the interaction features and interaction tower in Figue\u00a03 to validate the importance of these interaction features. We can see the results in Table\u00a01. To understand the scale of importance, we created 2 model version - 1 with <math alttext=\"1x\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.1.m1.1\">\n  <semantics id=\"S5.SS1.p1.1.m1.1a\">\n    <mrow id=\"S5.SS1.p1.1.m1.1.1\" xref=\"S5.SS1.p1.1.m1.1.1.cmml\">\n      <mn id=\"S5.SS1.p1.1.m1.1.1.2\" xref=\"S5.SS1.p1.1.m1.1.1.2.cmml\">1</mn>\n      <mo id=\"S5.SS1.p1.1.m1.1.1.1\" xref=\"S5.SS1.p1.1.m1.1.1.1.cmml\">&#8290;</mo>\n      <mi id=\"S5.SS1.p1.1.m1.1.1.3\" xref=\"S5.SS1.p1.1.m1.1.1.3.cmml\">x</mi>\n    </mrow>\n    <annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.p1.1.m1.1b\">\n      <apply id=\"S5.SS1.p1.1.m1.1.1.cmml\" xref=\"S5.SS1.p1.1.m1.1.1\">\n        <times id=\"S5.SS1.p1.1.m1.1.1.1.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.1\"/>\n        <cn id=\"S5.SS1.p1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.1.m1.1.1.2\">1</cn>\n        <ci id=\"S5.SS1.p1.1.m1.1.1.3.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.3\">&#119909;</ci>\n      </apply>\n    </annotation-xml>\n    <annotation encoding=\"application/x-tex\" id=\"S5.SS1.p1.1.m1.1c\">1x</annotation>\n    <annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.p1.1.m1.1d\">1 italic_x</annotation>\n  </semantics>\n</math> number of interaction features and another with \n<semantics id=\"S5.SS1.p1.1.m1.1a\">\n  <mrow id=\"S5.SS1.p1.1.m1.1.1\" xref=\"S5.SS1.p1.1.m1.1.1.cmml\">\n    <mn id=\"S5.SS1.p1.1.m1.1.1.2\" xref=\"S5.SS1.p1.1.m1.1.1.2.cmml\">1</mn>\n    <mo id=\"S5.SS1.p1.1.m1.1.1.1\" xref=\"S5.SS1.p1.1.m1.1.1.1.cmml\">&#8290;</mo>\n    <mi id=\"S5.SS1.p1.1.m1.1.1.3\" xref=\"S5.SS1.p1.1.m1.1.1.3.cmml\">x</mi>\n  </mrow>\n  <annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.p1.1.m1.1b\">\n    <apply id=\"S5.SS1.p1.1.m1.1.1.cmml\" xref=\"S5.SS1.p1.1.m1.1.1\">\n      <times id=\"S5.SS1.p1.1.m1.1.1.1.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.1\"/>\n      <cn id=\"S5.SS1.p1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.1.m1.1.1.2\">1</cn>\n      <ci id=\"S5.SS1.p1.1.m1.1.1.3.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.3\">&#119909;</ci>\n    </apply>\n  </annotation-xml>\n  <annotation encoding=\"application/x-tex\" id=\"S5.SS1.p1.1.m1.1c\">1x</annotation>\n  <annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.p1.1.m1.1d\">1 italic_x</annotation>\n</semantics>\n<mrow id=\"S5.SS1.p1.1.m1.1.1\" xref=\"S5.SS1.p1.1.m1.1.1.cmml\">\n  <mn id=\"S5.SS1.p1.1.m1.1.1.2\" xref=\"S5.SS1.p1.1.m1.1.1.2.cmml\">1</mn>\n  <mo id=\"S5.SS1.p1.1.m1.1.1.1\" xref=\"S5.SS1.p1.1.m1.1.1.1.cmml\">&#8290;</mo>\n  <mi id=\"S5.SS1.p1.1.m1.1.1.3\" xref=\"S5.SS1.p1.1.m1.1.1.3.cmml\">x</mi>\n</mrow>\n<mn id=\"S5.SS1.p1.1.m1.1.1.2\" xref=\"S5.SS1.p1.1.m1.1.1.2.cmml\">1</mn>\n1<mo id=\"S5.SS1.p1.1.m1.1.1.1\" xref=\"S5.SS1.p1.1.m1.1.1.1.cmml\">&#8290;</mo>\n\u2062<mi id=\"S5.SS1.p1.1.m1.1.1.3\" xref=\"S5.SS1.p1.1.m1.1.1.3.cmml\">x</mi>\nx<annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.p1.1.m1.1b\">\n  <apply id=\"S5.SS1.p1.1.m1.1.1.cmml\" xref=\"S5.SS1.p1.1.m1.1.1\">\n    <times id=\"S5.SS1.p1.1.m1.1.1.1.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.1\"/>\n    <cn id=\"S5.SS1.p1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.1.m1.1.1.2\">1</cn>\n    <ci id=\"S5.SS1.p1.1.m1.1.1.3.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.3\">&#119909;</ci>\n  </apply>\n</annotation-xml>\n<apply id=\"S5.SS1.p1.1.m1.1.1.cmml\" xref=\"S5.SS1.p1.1.m1.1.1\">\n  <times id=\"S5.SS1.p1.1.m1.1.1.1.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.1\"/>\n  <cn id=\"S5.SS1.p1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.1.m1.1.1.2\">1</cn>\n  <ci id=\"S5.SS1.p1.1.m1.1.1.3.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.3\">&#119909;</ci>\n</apply>\n<times id=\"S5.SS1.p1.1.m1.1.1.1.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.1\"/>\n<cn id=\"S5.SS1.p1.1.m1.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.1.m1.1.1.2\">1</cn>\n1<ci id=\"S5.SS1.p1.1.m1.1.1.3.cmml\" xref=\"S5.SS1.p1.1.m1.1.1.3\">&#119909;</ci>\n\ud835\udc65<annotation encoding=\"application/x-tex\" id=\"S5.SS1.p1.1.m1.1c\">1x</annotation>\n1x<annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.p1.1.m1.1d\">1 italic_x</annotation>\n1 italic_x number of interaction features and another with <math alttext=\"20x\" class=\"ltx_Math\" display=\"inline\" id=\"S5.SS1.p1.2.m2.1\">\n  <semantics id=\"S5.SS1.p1.2.m2.1a\">\n    <mrow id=\"S5.SS1.p1.2.m2.1.1\" xref=\"S5.SS1.p1.2.m2.1.1.cmml\">\n      <mn id=\"S5.SS1.p1.2.m2.1.1.2\" xref=\"S5.SS1.p1.2.m2.1.1.2.cmml\">20</mn>\n      <mo id=\"S5.SS1.p1.2.m2.1.1.1\" xref=\"S5.SS1.p1.2.m2.1.1.1.cmml\">&#8290;</mo>\n      <mi id=\"S5.SS1.p1.2.m2.1.1.3\" xref=\"S5.SS1.p1.2.m2.1.1.3.cmml\">x</mi>\n    </mrow>\n    <annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.p1.2.m2.1b\">\n      <apply id=\"S5.SS1.p1.2.m2.1.1.cmml\" xref=\"S5.SS1.p1.2.m2.1.1\">\n        <times id=\"S5.SS1.p1.2.m2.1.1.1.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.1\"/>\n        <cn id=\"S5.SS1.p1.2.m2.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.2.m2.1.1.2\">20</cn>\n        <ci id=\"S5.SS1.p1.2.m2.1.1.3.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.3\">&#119909;</ci>\n      </apply>\n    </annotation-xml>\n    <annotation encoding=\"application/x-tex\" id=\"S5.SS1.p1.2.m2.1c\">20x</annotation>\n    <annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.p1.2.m2.1d\">20 italic_x</annotation>\n  </semantics>\n</math>. We see more NE loss when ablating a stronger model with 20x interaction features. We do expect the number to plateau beyond a certain number of features as a large portion of the impact comes from the top interaction features.\n<semantics id=\"S5.SS1.p1.2.m2.1a\">\n  <mrow id=\"S5.SS1.p1.2.m2.1.1\" xref=\"S5.SS1.p1.2.m2.1.1.cmml\">\n    <mn id=\"S5.SS1.p1.2.m2.1.1.2\" xref=\"S5.SS1.p1.2.m2.1.1.2.cmml\">20</mn>\n    <mo id=\"S5.SS1.p1.2.m2.1.1.1\" xref=\"S5.SS1.p1.2.m2.1.1.1.cmml\">&#8290;</mo>\n    <mi id=\"S5.SS1.p1.2.m2.1.1.3\" xref=\"S5.SS1.p1.2.m2.1.1.3.cmml\">x</mi>\n  </mrow>\n  <annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.p1.2.m2.1b\">\n    <apply id=\"S5.SS1.p1.2.m2.1.1.cmml\" xref=\"S5.SS1.p1.2.m2.1.1\">\n      <times id=\"S5.SS1.p1.2.m2.1.1.1.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.1\"/>\n      <cn id=\"S5.SS1.p1.2.m2.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.2.m2.1.1.2\">20</cn>\n      <ci id=\"S5.SS1.p1.2.m2.1.1.3.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.3\">&#119909;</ci>\n    </apply>\n  </annotation-xml>\n  <annotation encoding=\"application/x-tex\" id=\"S5.SS1.p1.2.m2.1c\">20x</annotation>\n  <annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.p1.2.m2.1d\">20 italic_x</annotation>\n</semantics>\n<mrow id=\"S5.SS1.p1.2.m2.1.1\" xref=\"S5.SS1.p1.2.m2.1.1.cmml\">\n  <mn id=\"S5.SS1.p1.2.m2.1.1.2\" xref=\"S5.SS1.p1.2.m2.1.1.2.cmml\">20</mn>\n  <mo id=\"S5.SS1.p1.2.m2.1.1.1\" xref=\"S5.SS1.p1.2.m2.1.1.1.cmml\">&#8290;</mo>\n  <mi id=\"S5.SS1.p1.2.m2.1.1.3\" xref=\"S5.SS1.p1.2.m2.1.1.3.cmml\">x</mi>\n</mrow>\n<mn id=\"S5.SS1.p1.2.m2.1.1.2\" xref=\"S5.SS1.p1.2.m2.1.1.2.cmml\">20</mn>\n20<mo id=\"S5.SS1.p1.2.m2.1.1.1\" xref=\"S5.SS1.p1.2.m2.1.1.1.cmml\">&#8290;</mo>\n\u2062<mi id=\"S5.SS1.p1.2.m2.1.1.3\" xref=\"S5.SS1.p1.2.m2.1.1.3.cmml\">x</mi>\nx<annotation-xml encoding=\"MathML-Content\" id=\"S5.SS1.p1.2.m2.1b\">\n  <apply id=\"S5.SS1.p1.2.m2.1.1.cmml\" xref=\"S5.SS1.p1.2.m2.1.1\">\n    <times id=\"S5.SS1.p1.2.m2.1.1.1.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.1\"/>\n    <cn id=\"S5.SS1.p1.2.m2.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.2.m2.1.1.2\">20</cn>\n    <ci id=\"S5.SS1.p1.2.m2.1.1.3.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.3\">&#119909;</ci>\n  </apply>\n</annotation-xml>\n<apply id=\"S5.SS1.p1.2.m2.1.1.cmml\" xref=\"S5.SS1.p1.2.m2.1.1\">\n  <times id=\"S5.SS1.p1.2.m2.1.1.1.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.1\"/>\n  <cn id=\"S5.SS1.p1.2.m2.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.2.m2.1.1.2\">20</cn>\n  <ci id=\"S5.SS1.p1.2.m2.1.1.3.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.3\">&#119909;</ci>\n</apply>\n<times id=\"S5.SS1.p1.2.m2.1.1.1.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.1\"/>\n<cn id=\"S5.SS1.p1.2.m2.1.1.2.cmml\" type=\"integer\" xref=\"S5.SS1.p1.2.m2.1.1.2\">20</cn>\n20<ci id=\"S5.SS1.p1.2.m2.1.1.3.cmml\" xref=\"S5.SS1.p1.2.m2.1.1.3\">&#119909;</ci>\n\ud835\udc65<annotation encoding=\"application/x-tex\" id=\"S5.SS1.p1.2.m2.1c\">20x</annotation>\n20x<annotation encoding=\"application/x-llamapun\" id=\"S5.SS1.p1.2.m2.1d\">20 italic_x</annotation>\n20 italic_x. We see more NE loss when ablating a stronger model with 20x interaction features. We do expect the number to plateau beyond a certain number of features as a large portion of the impact comes from the top interaction features."
        ]
    },
    "S5.T2": {
        "caption": "Table 2. K-Means init + fine-tuning on the retrieval model supervision shows minor cluster NE gains demonstrating value in this direction. Both EM and LTC co-trained showed large cluster NE gains. The LTC algorithm is a gradient-descent based clustering algorithm and this aligns with the existing inference stack making it also easier to integrate into production.",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T2.1.1.2\">Clustering Technique</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1\">Cluster NE (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T2.1.1.1.m1.1\"><semantics id=\"S5.T2.1.1.1.m1.1a\"><mo id=\"S5.T2.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T2.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T2.1.1.1.m1.1b\"><ci id=\"S5.T2.1.1.1.m1.1.1.cmml\" xref=\"S5.T2.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T2.1.1.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T2.1.1.1.m1.1d\">&#8595;</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T2.1.2.1.1\">K-Means Clustering</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T2.1.2.1.2\">0.0% (reference)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.3.2.1\">K-Means Init + FineTuning</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.3.2.2\">-0.6%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T2.1.4.3.1\">K-Means Init + EM co-Trained</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T2.1.4.3.2\">-6.2%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T2.1.5.4.1\">LTC co-trained</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T2.1.5.4.2\">-6.5%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "In this ablation study, we aim to capture the benefit of co-training the clustering with the retrieval model. In other words, by making the clustering aware of the retrieval optimization criteria, we believe that it could improve clustering. In the EM style algorithm, we alternate between learning the cluster centroid representation given the assignment and then updating the cluster assignment given the learnt cluster centroid representation. In the LTC algorithm, both the cluster representation and ad representation are jointly optimized in a gradient descent fashion using techniques like curriculum learning to deal with discrete clustering operation. We can see the results in Table\u00a02."
        ]
    },
    "S5.T3": {
        "caption": "Table 3. An ablation study of various components of LTC algorithm. Curriculum learning with variable softmax temperature showed the maximum impact. NE gains from solving cluster collapse techiques like FLOPs regularizer and codebook reset demonstate the generalization benefits of clustering.",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S5.T3.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S5.T3.1.1.2\">Training Technique</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T3.1.1.1\">Cluster NE (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S5.T3.1.1.1.m1.1\"><semantics id=\"S5.T3.1.1.1.m1.1a\"><mo id=\"S5.T3.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S5.T3.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S5.T3.1.1.1.m1.1b\"><ci id=\"S5.T3.1.1.1.m1.1.1.cmml\" xref=\"S5.T3.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T3.1.1.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S5.T3.1.1.1.m1.1d\">&#8595;</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T3.1.2.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S5.T3.1.2.1.1\">K-Means Clustering</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S5.T3.1.2.1.2\">0.0% (reference)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.3.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.3.2.1\">LTC w/o curriculum learning</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.3.2.2\">6.2%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.4.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.4.3.1\">LTC w/o FLOPs regularizer</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.4.3.2\">0.8%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.5.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S5.T3.1.5.4.1\">LTC w/o Codebook reset</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S5.T3.1.5.4.2\">0.4%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T3.1.6.5\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S5.T3.1.6.5.1\">LTC with 2 clustering modules</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S5.T3.1.6.5.2\">-0.2%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "There are numerous factors that bring gains to the LTC algorithm - the curriculum learning makes the LTC algorithm trainable on the discrete clustering operation. The warmup strategy makes the learning from the ad tower more smooth and the flops regularizer and the codebook reset ensure that we don\u2019t see a cluster collapse. As we can see in Table\u00a03, the curriculum learning brings in most of the gains. While the FLOPS regularizer and codebook reset were orirginally added to deal with cluster collapse, it is interesting to see them bring in accuracy (NE) gains as well. We attribute this to the"
        ]
    },
    "S6.T4": {
        "caption": "Table 4. We show that the LTC algorithm can show improvement over K-Means on both NE and Recall@K.",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S6.T4.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T4.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S6.T4.2.2.3\">Technique</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S6.T4.1.1.1\">Cluster NE (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.1.1.1.m1.1\"><semantics id=\"S6.T4.1.1.1.m1.1a\"><mo id=\"S6.T4.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S6.T4.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.1.1.1.m1.1b\"><ci id=\"S6.T4.1.1.1.m1.1.1.cmml\" xref=\"S6.T4.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.1.1.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T4.1.1.1.m1.1d\">&#8595;</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T4.2.2.2\">Recall@K (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T4.2.2.2.m1.1\"><semantics id=\"S6.T4.2.2.2.m1.1a\"><mo id=\"S6.T4.2.2.2.m1.1.1\" stretchy=\"false\" xref=\"S6.T4.2.2.2.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T4.2.2.2.m1.1b\"><ci id=\"S6.T4.2.2.2.m1.1.1.cmml\" xref=\"S6.T4.2.2.2.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T4.2.2.2.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T4.2.2.2.m1.1d\">&#8593;</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T4.2.3.1\">\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T4.2.3.1.1\">K-Means</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T4.2.3.1.2\">0.0% (reference)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T4.2.3.1.3\">100% (reference)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T4.2.4.2\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T4.2.4.2.1\">Co-Trained EM Algorithm</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T4.2.4.2.2\">-4.2%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.2.4.2.3\">106.3%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T4.2.5.3\">\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T4.2.5.3.1\">HSNN with 1 clustering module</td>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T4.2.5.3.2\">-6.5%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T4.2.5.3.3\">110.5%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T4.2.6.4\">\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S6.T4.2.6.4.1\">HSNN with 2 clustering modules</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S6.T4.2.6.4.2\">-6.9%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T4.2.6.4.3\">111.8%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We\u2019ve deployed HSNN using the LTC algorithm (both 1 level and 2 levels of clustering) and tested the improvements over Embedding Based Retrieval (EBR) using an A/B experiment. For all of our experiments, we used the K-Means vector codec in EBR - we didn\u2019t see any improvements in using different codecs or indices. Upon deploying the LTC algorithm in production, we observe a 0.2% improvement over K-Means algorithm in our topline metric. We\u2019ve shared the results in Table\u00a05. For an equivalent offline metrics comparison, we\u2019ve consolidated the offline metrics in Table\u00a04."
        ]
    },
    "S6.T5": {
        "caption": "Table 5. We show that HSNN brings improvement on both accuracy and efficiency over a well-established baseline of EBR. We also show that the gains improve as we increase the number of hierarchical layers.",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S6.T5.2\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T5.2.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S6.T5.2.2.3\">Technique</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt\" id=\"S6.T5.1.1.1\">Relevance (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.1.1.1.m1.1\"><semantics id=\"S6.T5.1.1.1.m1.1a\"><mo id=\"S6.T5.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S6.T5.1.1.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T5.1.1.1.m1.1b\"><ci id=\"S6.T5.1.1.1.m1.1.1.cmml\" xref=\"S6.T5.1.1.1.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T5.1.1.1.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T5.1.1.1.m1.1d\">&#8593;</annotation></semantics></math>)</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T5.2.2.2\">Efficiency (<math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T5.2.2.2.m1.1\"><semantics id=\"S6.T5.2.2.2.m1.1a\"><mo id=\"S6.T5.2.2.2.m1.1.1\" stretchy=\"false\" xref=\"S6.T5.2.2.2.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T5.2.2.2.m1.1b\"><ci id=\"S6.T5.2.2.2.m1.1.1.cmml\" xref=\"S6.T5.2.2.2.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T5.2.2.2.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T5.2.2.2.m1.1d\">&#8593;</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T5.2.3.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T5.2.3.1.1\">Embedding Based Retrieval (EBR)</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" id=\"S6.T5.2.3.1.2\">0% (reference)</td>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T5.2.3.1.3\">0% (reference)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.4.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S6.T5.2.4.2.1\">HSNN with 1 clustering module</th>\n<td class=\"ltx_td ltx_align_center ltx_border_r\" id=\"S6.T5.2.4.2.2\">1.22%</td>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T5.2.4.2.3\">0.10%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T5.2.5.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S6.T5.2.5.3.1\">HSNN with 2 clustering modules</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb ltx_border_r\" id=\"S6.T5.2.5.3.2\">1.52%</td>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T5.2.5.3.3\">0.23%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We\u2019ve deployed HSNN using the LTC algorithm (both 1 level and 2 levels of clustering) and tested the improvements over Embedding Based Retrieval (EBR) using an A/B experiment. For all of our experiments, we used the K-Means vector codec in EBR - we didn\u2019t see any improvements in using different codecs or indices. Upon deploying the LTC algorithm in production, we observe a 0.2% improvement over K-Means algorithm in our topline metric. We\u2019ve shared the results in Table\u00a05. For an equivalent offline metrics comparison, we\u2019ve consolidated the offline metrics in Table\u00a04."
        ]
    },
    "S6.T6": {
        "caption": "Table 6. We show the impact of staleness to the centroid embeddings. This is especially important as the LTC algorithm ensures that the centroid embeddings are available at the same time as the ad embedding without having to run disjoint-training clustering algorithm like K-Means.",
        "table": "<table class=\"ltx_tabular ltx_guessed_headers ltx_align_middle\" id=\"S6.T6.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\" id=\"S6.T6.1.1.2\">Num Hours Stale</th>\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T6.1.1.1\">Cluster NE (<math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T6.1.1.1.m1.1\"><semantics id=\"S6.T6.1.1.1.m1.1a\"><mo id=\"S6.T6.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S6.T6.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T6.1.1.1.m1.1b\"><ci id=\"S6.T6.1.1.1.m1.1.1.cmml\" xref=\"S6.T6.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T6.1.1.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T6.1.1.1.m1.1d\">&#8595;</annotation></semantics></math>)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T6.1.2.1\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t\" id=\"S6.T6.1.2.1.1\">0</th>\n<td class=\"ltx_td ltx_align_center ltx_border_t\" id=\"S6.T6.1.2.1.2\">0.0% (reference)</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.3.2\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.1.3.2.1\">1</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.3.2.2\">0.1%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.4.3\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r\" id=\"S6.T6.1.4.3.1\">2</th>\n<td class=\"ltx_td ltx_align_center\" id=\"S6.T6.1.4.3.2\">0.3%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T6.1.5.4\">\n<th class=\"ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r\" id=\"S6.T6.1.5.4.1\">3</th>\n<td class=\"ltx_td ltx_align_center ltx_border_bb\" id=\"S6.T6.1.5.4.2\">0.7%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "We measure the impact of staleness by measuring the delta between fresh centroid embeddings vs stale centroid embeddings. As we have shown in Table\u00a06, we find that NE when compared to a fresh model worsens over time. It is also worth noting that the cluster assignments are relatively stable compared the the centroid embeddings."
        ]
    }
}