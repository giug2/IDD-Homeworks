{
    "id_table_1": {
        "caption": "Table 1 :  Accuracy (%) and 95% confidence interval of clone in various limited-budget settings. Experiments result for DisGUIDE and CaBaGE are computed over 3 runs. IDEAL  s result are run by a fixed random seed based on the published code. IDEAL   is the query budget adjusted version of IDEAL for a fair comparison (  Sec.   4.1 ).",
        "table": "S4.T1.39",
        "footnotes": [
            ""
        ],
        "references": [
            "Ensemble methods can improve the generalization of neural networks and reduce the high variance properties of the models  [ 5 ] . The general simplicity in implementation and overall improvement of Ensemble Learning methods has resulted in their use across many different machine learning and deep learning fields  [ 35 ] .   Ensemble Learning in Adversarial-Learning : Prior work shows that training a GAN with an ensemble of generators improves performance and the diversity of the generated outputs  [ 10 ,  6 ] . Existing work in data-free model extraction also utilizes the concept of ensemble learning. Rosenthal et al. leverage an ensemble of clones to improve the stability of clones prediction  [ 26 ] . Others deploy two generators as an ensemble and optimize the disagreement of the generator outputs, trying to boost the diversity of the generated outputs  [ 33 ] . In contrast, CaBaGE does not directly compare generator outputs, but relies on the joint optimization process to incentivize ensemble diversity implicitly. In addition, our approach has a negligible increase in computational cost (details in  Sec.   3.1 ).",
            "Fig.   1  is an overview of CaBaGE. CaBaGE three novel components are highlighted in yellow: Generator Ensemble ( Sec.   3.1 ), Selective Query ( Sec.   3.2 ), and Class-Balanced Difficulty-Weighted Replay ( Sec.   3.3 ). We build CaBaGE upon the foundational method introduced by DisGUIDE  [ 26 ] . In DisGUIDE, an attacker trains a generator and two clones in an adversarial-like setting and the final extracted model is derived from the ensemble of the clones.",
            "Similar to DisGUIDE, CaBaGEs extraction process is composed of two phases, shown in  Fig.   1 : (1)  Clone Training , and (2)  Generator Training . Within the given query budget, CaBaGE cycles between these phases. Before entering any phases, CaBaGEs extraction process starts by initializing the  generator ensemble  and the  clone ensemble  from random weights. Afterwards, CaBaGE optimizes the generator ensembles weights in the  Generator Training  phase, while keeping the  clone ensemble s parameters frozen. Conversely, the  Clone Training  phase updates the clone ensemble while keeping the generator ensemble fixed.",
            "In the limited-budget setting, we compare three techniques: IDEAL, DisGUIDE, and CaBaGE. Following IDEALs settings, we use 25K queries for extractions on MNIST, 100K for FMNIST and SVHN, 250K for CIFAR-10 and ImageNet-subset, and 2M for CIFAR-100 and Tiny ImageNet.   Tab.   1  reports the average final accuracies obtained by each respective method. On CIFAR-100 and Tiny ImageNet the clone model architecture is ResNet18, while in all other cases the clone architecture is congruent to the victim model architecture.",
            "Limited-Budget Setting   Tab.   1  shows the extraction results of IDEAL  , DisGUIDE, and CaBaGE in terms of final extracted accuracy and 95% confidence interval under the limited-budget setting, described in  Sec.   4.1 . IDEAL   is the query budget adjusted version of IDEAL, for a fair comparison as described in  Sec.   4.1 . For example, when extracting the MLP victim trained on MNIST, CaBaGE reaches  57.13 % percent 57.13 57.13\\% 57.13 %  accuracy on the test set, outperforms the prior best extraction result of 14.00% by 43.13%. Consistently, CaBaGE outperforms prior work on all victims, on any target dataset.",
            "In  Tab.   7 , we present a performance comparison between DisGUIDE and CaBaGE method while eliminating the effect of replay iterations. The experimental configurations remain consistent with those in  Tab.   1 , except that we increased DisGUIDEs replay iteration from 3 to 12, and denote this method as DisGUIDE  . Based on our findings in  Appendix   0.C , we have observed that increasing the replay iterations can lead to a boost in the final accuracy of the extracted model, although the gains diminish over time. To provide a fair comparison between the two methods, we chose to set DisGUIDEs replay iterations equal to CaBaGEs, and we run all experiments 9 times. The results are shown as mean values of extracted models accuracies, along with standard deviations. Statistical significance is determined using independent t-tests, and the  p p p italic_p  value for each test is recorded (Column  p p p italic_p  val) with three decimal places."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Final clone accuracy comparison with 95% confidence intervals in the relaxed-budget setting. DisGUIDE  [ 26 ]  results are the paper reported accuracies.",
        "table": "S5.T2.8",
        "footnotes": [
            ""
        ],
        "references": [
            "Fig.   1  is an overview of CaBaGE. CaBaGE three novel components are highlighted in yellow: Generator Ensemble ( Sec.   3.1 ), Selective Query ( Sec.   3.2 ), and Class-Balanced Difficulty-Weighted Replay ( Sec.   3.3 ). We build CaBaGE upon the foundational method introduced by DisGUIDE  [ 26 ] . In DisGUIDE, an attacker trains a generator and two clones in an adversarial-like setting and the final extracted model is derived from the ensemble of the clones.",
            "Tab.   2  compares the final accuracy of CaBaGE on CIFAR-10 and CIFAR-100 following DisGUIDE configurations in the relaxed-budget setting for both SL and HL extractions. For SL extractions, when extracting from the ResNet-34 victim trained on CIFAR-100, CaBaGE outperforming DisGUIDE by  6.49 % percent 6.49 6.49\\% 6.49 % , achieving a final accuracy of  75.96 % percent 75.96 75.96\\% 75.96 %  on the test set, which is  97.99 97.99 97.99 97.99  percent of the victim models accuracy ( 77.52 % percent 77.52 77.52\\% 77.52 % ). This underscores that with a higher query budget, CaBaGE offers highly accurate SL model extraction, even on more intricate models trained with complex datasets. CaBaGE also increases the final extracted accuracy on CIFAR-10 by  0.34 % percent 0.34 0.34\\% 0.34 % . Similarly, in the HL setting, the test-set accuracies are increased by  1.35 % percent 1.35 1.35\\% 1.35 %  and  5.73 % percent 5.73 5.73\\% 5.73 %  for extraction processes on CIFAR-10 and CIFAR-100, reaching  89.28 % percent 89.28 89.28\\% 89.28 %  and  64.45 % percent 64.45 64.45\\% 64.45 %  accuracy respectively. Besides the gains of final accuracy of the extracted models, CaBaGE also makes the extraction process more stable, i.e., reduces the fluctuation of results, when the query budget is relaxed.",
            "Fig.   2  compares the fidelity extraction curves of the prior SOTA with CaBaGE. In the next section, we present the accuracy curves of the same runs in  Fig.   3 . The final fidelity values for these runs can be found in  Tab.   6 . Subjectively the accuracy and fidelity results look to be inline with one another, with fidelity values being higher than accuracy.",
            "We explored the impact of both  generator ensemble size  and  generator training iterations  on the CIFAR-100 dataset, and our experiments results are encapsulated in  Fig.   5  and  Fig.   6  respectively. To ensure a fair comparison, our experimental setup solely applies the generator ensemble technique to DisGUIDE, excluding the integration of Selective Query and class-balanced difficulty-weighted replay. The configurations mirror the DisGUIDE setup for CIFAR-100 as detailed in  Tab.   2   [ 26 ] ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 :  Mean number of queries (in millions) to reach prior workDisGUIDEs reported final accuracies with respective 95% confidence intervals. Lower is better.",
        "table": "S5.T3.4",
        "footnotes": [],
        "references": [
            "Ensemble methods can improve the generalization of neural networks and reduce the high variance properties of the models  [ 5 ] . The general simplicity in implementation and overall improvement of Ensemble Learning methods has resulted in their use across many different machine learning and deep learning fields  [ 35 ] .   Ensemble Learning in Adversarial-Learning : Prior work shows that training a GAN with an ensemble of generators improves performance and the diversity of the generated outputs  [ 10 ,  6 ] . Existing work in data-free model extraction also utilizes the concept of ensemble learning. Rosenthal et al. leverage an ensemble of clones to improve the stability of clones prediction  [ 26 ] . Others deploy two generators as an ensemble and optimize the disagreement of the generator outputs, trying to boost the diversity of the generated outputs  [ 33 ] . In contrast, CaBaGE does not directly compare generator outputs, but relies on the joint optimization process to incentivize ensemble diversity implicitly. In addition, our approach has a negligible increase in computational cost (details in  Sec.   3.1 ).",
            "Fig.   1  is an overview of CaBaGE. CaBaGE three novel components are highlighted in yellow: Generator Ensemble ( Sec.   3.1 ), Selective Query ( Sec.   3.2 ), and Class-Balanced Difficulty-Weighted Replay ( Sec.   3.3 ). We build CaBaGE upon the foundational method introduced by DisGUIDE  [ 26 ] . In DisGUIDE, an attacker trains a generator and two clones in an adversarial-like setting and the final extracted model is derived from the ensemble of the clones.",
            "We compare the number of queries required for CaBaGE to reach the final accuracy of the prior work DisGUIDE against its query budgets, and report the result in  Tab.   3 . In all extraction settings, CaBaGE shows better query efficiency compared to DisGUIDE. For SL extraction on CIFAR-100  CaBaGE needs only 2.43 million queries on average to achieve similar accuracy to DisGUIDEs final result, with a 95% confidence interval of 0.15 million, reducing the prior standard by 75.7%. For SL extraction on CIFAR-10 the number of queries needed to reach prior works best is reduced by 3.96 million on average, 19.8% of the total queries.",
            "Fig.   2  compares the fidelity extraction curves of the prior SOTA with CaBaGE. In the next section, we present the accuracy curves of the same runs in  Fig.   3 . The final fidelity values for these runs can be found in  Tab.   6 . Subjectively the accuracy and fidelity results look to be inline with one another, with fidelity values being higher than accuracy.",
            "The plots in  Fig.   3  are in the relaxed budget settings with the exact runs used to generate the CaBaGE main results compared with reproduced runs of DisGUIDE with results inline with the numbers reported in the paper."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 :  Final Accuracy of CaBaGE and ablation settings shown with 95% confidence interval. Independent t-tests are used to determine improvement over the baseline DisGUIDE  , where statistically significant improvements are highlighted in  Blue .",
        "table": "S5.T4.111",
        "footnotes": [],
        "references": [
            "Limited-Budget Setting   Tab.   1  shows the extraction results of IDEAL  , DisGUIDE, and CaBaGE in terms of final extracted accuracy and 95% confidence interval under the limited-budget setting, described in  Sec.   4.1 . IDEAL   is the query budget adjusted version of IDEAL, for a fair comparison as described in  Sec.   4.1 . For example, when extracting the MLP victim trained on MNIST, CaBaGE reaches  57.13 % percent 57.13 57.13\\% 57.13 %  accuracy on the test set, outperforms the prior best extraction result of 14.00% by 43.13%. Consistently, CaBaGE outperforms prior work on all victims, on any target dataset.",
            "We evaluate the individual contribution of Class-Balanced Difficulty-Weighted Replay (CB-DW), Generator Ensemble (GE), and Selective Query by progressively adding them to the baseline, DisGUIDE  , which only differs from DisGUIDE by incrementing the replay iteration from 3 to 12 to match CaBaGE for a fair comparison. We report the averaged extraction accuracy with a 95% confidence interval in  Tab.   4 . Column  DisGUIDE  +CB-DW +CB-DW {}_{\\text{+CB-DW}} start_FLOATSUBSCRIPT +CB-DW end_FLOATSUBSCRIPT  represents DisGUIDE   with CB-DW. Other columns define similar ablations of our technique or our full approach. We run each experiment 9 times and evaluate the statistical significance using independent t-tests against DisGUIDE  .",
            "Figure  4  demonstrates the performance trajectories of the two replays as a function of replay iterations, with the error bars indicating a 95% confidence interval. The upper chart demonstrates that the performance of CaBaGEs replay consistently exceeds that of DisGUIDE and exhibits a positive correlation with replay iterations when extracting the MLP victim on MNIST. Conversely, DisGUIDEs performance first increases then declines with additional iterations. In the lower chart, Nemesis replay marginally trails behind DisGUIDEs replay, and a uniform decrement in performance for both methods is observed as the replay iteration count escalates. These findings indicate that the influence of replay strategies is not uniform across datasets. It is imperative for future research in model extraction to leverage experience replay to scrutinize the differential impacts of replay modalities relative to the victim model and dataset characteristics."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 :  Evaluation of the impact Generator Ensemble Size on computation cost. Extraction result performed by  DisGUIDE  +GE +GE {}_{\\text{+GE}} start_FLOATSUBSCRIPT +GE end_FLOATSUBSCRIPT  following limited-budget setting on the AlexNet victim trained on CIFAR-10. We run each experiment setting 10 times.",
        "table": "S5.T5.10",
        "footnotes": [],
        "references": [
            "We show in  Tab.   5  that under a fixed generator training iteration, the increase in Generator Ensemble size does not lead to a noticeable increase of our implementations runtime. Increasing the generator size from 1 to 4 results in a 6.76% increase in final accuracy, yet the time required for extraction remains approximately the same. In fact, the time is reduced by 6 seconds, which we believe is due to system randomness. This evaluation was performed on an NVIDIA GeForce RTX 2080 Ti with 11 GB of memory and an Intel(R) Xeon(R) Gold 5120 CPU.",
            "We explored the impact of both  generator ensemble size  and  generator training iterations  on the CIFAR-100 dataset, and our experiments results are encapsulated in  Fig.   5  and  Fig.   6  respectively. To ensure a fair comparison, our experimental setup solely applies the generator ensemble technique to DisGUIDE, excluding the integration of Selective Query and class-balanced difficulty-weighted replay. The configurations mirror the DisGUIDE setup for CIFAR-100 as detailed in  Tab.   2   [ 26 ] .",
            "In experiments for evaluating the generator ensemble size, shown in  Fig.   5 ), for both  SL Setting  and  HL Setting , the extracted models final accuracy generally improves with an increase in ensemble size, peaking at a size of 8, which results in  3.48 % percent 3.48 3.48\\% 3.48 %  improvement over base DisGUIDE with a final accuracy of  66.39 % percent 66.39 66.39\\% 66.39 %  . However, expanding the ensemble from 8 to 16 members results in a marginal decline in performance."
        ]
    },
    "id_table_6": {
        "caption": "Table 6 :  Final clone fidelity along with 95% CI. Results from DisGUIDE are from reproduced runs with their codebase.",
        "table": "Pt0.A1.T6.8.8",
        "footnotes": [],
        "references": [
            "Fig.   2  compares the fidelity extraction curves of the prior SOTA with CaBaGE. In the next section, we present the accuracy curves of the same runs in  Fig.   3 . The final fidelity values for these runs can be found in  Tab.   6 . Subjectively the accuracy and fidelity results look to be inline with one another, with fidelity values being higher than accuracy.",
            "We explored the impact of both  generator ensemble size  and  generator training iterations  on the CIFAR-100 dataset, and our experiments results are encapsulated in  Fig.   5  and  Fig.   6  respectively. To ensure a fair comparison, our experimental setup solely applies the generator ensemble technique to DisGUIDE, excluding the integration of Selective Query and class-balanced difficulty-weighted replay. The configurations mirror the DisGUIDE setup for CIFAR-100 as detailed in  Tab.   2   [ 26 ] .",
            "The impact of  generator training iterations per victim query iteration  are shown in  Fig.   6 . In both the  HL Setting  and the  SL Setting , A clear upward trend is observed with increased training iterations. The increments in accuracy for each increasing generator training iteration (   \\beta italic_ ) are as follows. For  HL : an increase of 1.87% from   = 1  1 \\beta=1 italic_ = 1  to   = 2  2 \\beta=2 italic_ = 2 , 0.28% from   = 2  2 \\beta=2 italic_ = 2  to   = 4  4 \\beta=4 italic_ = 4 , and 1.33% from   = 4  4 \\beta=4 italic_ = 4  to   = 8  8 \\beta=8 italic_ = 8 . For  SL : an increase of 1.18% from   = 1  1 \\beta=1 italic_ = 1  to   = 2  2 \\beta=2 italic_ = 2 , 0.85% from   = 2  2 \\beta=2 italic_ = 2  to   = 4  4 \\beta=4 italic_ = 4 , and 0.14% from   = 4  4 \\beta=4 italic_ = 4  to   = 8  8 \\beta=8 italic_ = 8 . The result suggests that more generator training leads to enhanced performance in both HL and SL settings. However, similar to the generator size, we can observe the diminishing returns and it is possible that the increased generator training iterations positive effect on model performance will be reversed beyond a specific threshold."
        ]
    },
    "id_table_7": {
        "caption": "Table 7 :  Independent t-test comparing CaBaGE (using class-balanced difficulty-weighted replay) and DisGUIDE (using original replay) under 12 replay iterations in the setting of various configurations. There are 9 runs for all experiments. accuracy is shown along with 95% confidence interval.",
        "table": "Pt0.A7.T7.68",
        "footnotes": [],
        "references": [
            "In  Tab.   7 , we present a performance comparison between DisGUIDE and CaBaGE method while eliminating the effect of replay iterations. The experimental configurations remain consistent with those in  Tab.   1 , except that we increased DisGUIDEs replay iteration from 3 to 12, and denote this method as DisGUIDE  . Based on our findings in  Appendix   0.C , we have observed that increasing the replay iterations can lead to a boost in the final accuracy of the extracted model, although the gains diminish over time. To provide a fair comparison between the two methods, we chose to set DisGUIDEs replay iterations equal to CaBaGEs, and we run all experiments 9 times. The results are shown as mean values of extracted models accuracies, along with standard deviations. Statistical significance is determined using independent t-tests, and the  p p p italic_p  value for each test is recorded (Column  p p p italic_p  val) with three decimal places."
        ]
    },
    "id_table_8": {
        "caption": "Table 8 :  Final Accuracy of CaBaGE and ablation settings shown with 95% confidence interval. Independent t-tests are used to determine improvement over the baseline DisGUIDE  , where statistically significant improvements are highlighted in  Blue .",
        "table": "Pt0.A8.T8.39",
        "footnotes": [],
        "references": [
            "We show the additional experiments on CIFAR-100 and Tiny-Imagenet for our ablation on the individual contribution of our novel components in  Tab.   8 . All settings are the same as Sec. 5.2 in our paper, except all experiments are run 3 times instead of 9 times on this table due to time constraints. We progressively add our novel components to the baseline, DisGUIDE  , and present the ablations for DisGUIDE  ,  DisGUIDE  +CB-DW +CB-DW {}_{\\text{+CB-DW}} start_FLOATSUBSCRIPT +CB-DW end_FLOATSUBSCRIPT ,  DisGUIDE  +CB-DW+GE +CB-DW+GE {}_{\\text{+CB-DW+GE}} start_FLOATSUBSCRIPT +CB-DW+GE end_FLOATSUBSCRIPT  and CaBaGE."
        ]
    }
}