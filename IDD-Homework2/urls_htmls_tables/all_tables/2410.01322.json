{
    "id_table_1": {
        "caption": "Table 1:  Performance comparison between our method and established supervised SoTA methods on various out-of-distribution (OOD) detection tasks. Our method consistently achieves the best performance across a range of dataset pairings, particularly outperforming on challenging datasets like NINCO and SSB-Hard. It also excels at detecting covariate shifted ID datasets. Key configurations include ViT-B trained with Cross Entropy and the RMDS postprocessor, and DINOv2+MLS, which utilizes training with Linear Probe DINOv2 alongside the MLS postprocessor. For reliable measurements, Forte is run with 10 random seeds.",
        "table": "S4.T2.19",
        "footnotes": [],
        "references": [
            "Table  1  &  2  present the performance comparison between our proposed methods (Forte+SVM, Forte+KDE, and Forte+GMM) and various state of the art supervised and unsupervised techniques for out-of-distribution (OOD) detection techniques. Our methods consistently outperform techniques across all challenging dataset pairings, demonstrating their effectiveness in detecting OOD samples without relying on labeled data.",
            "To simulate this scenario, two public datasets are used for the experiments in Section  4.2 : coronal knee MRI from FastMRI  Zbontar et al. ( 2018 ); Knoll et al. ( 2020 )  with two subsets and Osteoarthritis Initiative (OAI)  Nevitt et al. ( 2006 )  with three subsets. The acquisition parameters, including sequence and fat suppression are detailed in Table  6  and samples are shown in Figure  14 . Treating the FastMRI dataset as in-distribution and assuming that models have been trained on them, Forte is used to determine the next course of action when confronted with the OAI dataset: 1) which subsets of the new dataset can be aligned with the existing subsets / models? 2) To what degree do these subsets diverge from each other? Using the FastMRI FS subset as in-distribution, the two OAI subsets (OAI T1 and OAI MPR) are tested for OOD detection. Similarly, the FastMRI NoFS subset is used as in-distribution and the OAI TSE subset is tested for OOD detection."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of performance figures between our method and various unsupervised baselines for out-of-distribution (OOD) detection tasks, demonstrating superior performance of our method across all challenging dataset pairings. For reliable measurements, Forte is run with 10 random seeds.",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "Table  1  &  2  present the performance comparison between our proposed methods (Forte+SVM, Forte+KDE, and Forte+GMM) and various state of the art supervised and unsupervised techniques for out-of-distribution (OOD) detection techniques. Our methods consistently outperform techniques across all challenging dataset pairings, demonstrating their effectiveness in detecting OOD samples without relying on labeled data.",
            "To generate synthetic data  for assessing distributional robustness, we first employ the Stable Diffusion Img2Img setting  (Rombach et al.,  2022 ) , where a diffusion-based model can generate new images conditioned on an input image and a text prompt. We use the Stable Diffusion 2.0 base model and generate images with varying strength parameters (0.3, 0.5, 0.7, 0.9, 1.0) to control the influence of the input image on the generated output, essentially allowing our real distribution to be prior of controllable strength for the generated distribution. We also use the Stable Diffusion 2.0 text-to-image model to generate new images conditioned on the captions generated by BLIP (Li et al.,  2022 )  for each real image from the reference distribution. This allows us to create images that are semantically similar to the real images but with novel compositions and variations. Finally, we also generate images directly from the class name associated with each real image (e.g., \"a photo of a {monarch butterfly}, in a natural setting\"). This provides a baseline for generating images that capture the essential characteristics of the class without relying on specific input images or captions. The image generation pipeline is implemented using the Hugging Face Transformers  (Wolf et al.,  2020 )  and Diffusers  (von Platen et al.,  2022 )  libraries, which provide high-level APIs for working with pre-trained models. Examples can be found in  Figure 2   Figure 3   Figure 4",
            "To simulate this scenario, two public datasets are used for the experiments in Section  4.2 : coronal knee MRI from FastMRI  Zbontar et al. ( 2018 ); Knoll et al. ( 2020 )  with two subsets and Osteoarthritis Initiative (OAI)  Nevitt et al. ( 2006 )  with three subsets. The acquisition parameters, including sequence and fat suppression are detailed in Table  6  and samples are shown in Figure  14 . Treating the FastMRI dataset as in-distribution and assuming that models have been trained on them, Forte is used to determine the next course of action when confronted with the OAI dataset: 1) which subsets of the new dataset can be aligned with the existing subsets / models? 2) To what degree do these subsets diverge from each other? Using the FastMRI FS subset as in-distribution, the two OAI subsets (OAI T1 and OAI MPR) are tested for OOD detection. Similarly, the FastMRI NoFS subset is used as in-distribution and the OAI TSE subset is tested for OOD detection."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Ablation study investigating the impact of incorporating multiple representation learning techniques for detecting arbitrary superclasses from the ImageNet1k hierarchy, using Forte+GMM.",
        "table": "S4.T4.15",
        "footnotes": [],
        "references": [
            "We extract representations from reference and test images using CLIP  Radford et al. ( 2021b ) , which learns to map images and text into a shared latent space, self-supervised models  Gui et al. ( 2023 )  like ViT-MSN  Assran et al. ( 2022 ) , which predicts similarity between masked views of the same image, and DiNo v2  Oquab et al. ( 2023a ) , which distills knowledge from a teacher to a student network. Combining representations from these diverse models captures distinct aspects of the data via different manifolds and improves robustness of our anomaly detection approach, as we observe later in  Table 3 .",
            "Table  3  presents an ablation study investigating the impact of incorporating multiple representation learning techniques into the Forte framework, when detecting arbitrary superclasses from the ImageNet1k hierarchy. In this challenging task example, the in-distribution data consists of various vehicle classes from the ImageNet dataset, such as Ambulance, Beach wagon, Cab, Convertible, Jeep, Limousine, Minivan, Model t, Racer, and Sportscar. The out-of-distribution data is divided into two categories: Near OOD, which includes other utility vehicle classes like fire engine, garbage truck, pickup, tow truck, trailer truck, minivan, moving van, and police van; and Far OOD, which consists of animal classes such as Egyptian cat, Persian cat, Siamese cat, Tabby cat, Tiger cat, Cougar, Lynx, Cheetah, Jaguar, Leopard, Lion, Snow leopard, and Tiger. Our method is successful in very challenging near-ood situations, and becomes more so when using multiple representations. The combination of all three techniques (CLIP, MSN, and DINOv2) achieves the best overall performance for both Far OOD detection, and comes a close second in Near OOD detection. These findings suggest that leveraging diverse SSL representation techniques within the Forte framework can capture complementary information and enhance the overall OOD detection capabilities.",
            "To generate synthetic data  for assessing distributional robustness, we first employ the Stable Diffusion Img2Img setting  (Rombach et al.,  2022 ) , where a diffusion-based model can generate new images conditioned on an input image and a text prompt. We use the Stable Diffusion 2.0 base model and generate images with varying strength parameters (0.3, 0.5, 0.7, 0.9, 1.0) to control the influence of the input image on the generated output, essentially allowing our real distribution to be prior of controllable strength for the generated distribution. We also use the Stable Diffusion 2.0 text-to-image model to generate new images conditioned on the captions generated by BLIP (Li et al.,  2022 )  for each real image from the reference distribution. This allows us to create images that are semantically similar to the real images but with novel compositions and variations. Finally, we also generate images directly from the class name associated with each real image (e.g., \"a photo of a {monarch butterfly}, in a natural setting\"). This provides a baseline for generating images that capture the essential characteristics of the class without relying on specific input images or captions. The image generation pipeline is implemented using the Hugging Face Transformers  (Wolf et al.,  2020 )  and Diffusers  (von Platen et al.,  2022 )  libraries, which provide high-level APIs for working with pre-trained models. Examples can be found in  Figure 2   Figure 3   Figure 4"
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Performance comparison of Forte+GMM against a CLIP-based baseline, and Gen Eval methods for detecting synthetic images of Golden Retrievers generated using various techniques.",
        "table": "S4.T5.18",
        "footnotes": [],
        "references": [
            "To generate synthetic data  for assessing distributional robustness, we first employ the Stable Diffusion Img2Img setting  (Rombach et al.,  2022 ) , where a diffusion-based model can generate new images conditioned on an input image and a text prompt. We use the Stable Diffusion 2.0 base model and generate images with varying strength parameters (0.3, 0.5, 0.7, 0.9, 1.0) to control the influence of the input image on the generated output, essentially allowing our real distribution to be prior of controllable strength for the generated distribution. We also use the Stable Diffusion 2.0 text-to-image model to generate new images conditioned on the captions generated by BLIP (Li et al.,  2022 )  for each real image from the reference distribution. This allows us to create images that are semantically similar to the real images but with novel compositions and variations. Finally, we also generate images directly from the class name associated with each real image (e.g., \"a photo of a {monarch butterfly}, in a natural setting\"). This provides a baseline for generating images that capture the essential characteristics of the class without relying on specific input images or captions. The image generation pipeline is implemented using the Hugging Face Transformers  (Wolf et al.,  2020 )  and Diffusers  (von Platen et al.,  2022 )  libraries, which provide high-level APIs for working with pre-trained models. Examples can be found in  Figure 2   Figure 3   Figure 4",
            "Frechet Distance (FD),  F  D  F subscript D FD_{\\infty} italic_F italic_D start_POSTSUBSCRIPT  end_POSTSUBSCRIPT , and CMMD scores  provide insights into the distribution shift between real and synthetic images, with generated images moving further away from the real distribution as diffusion strength increases. However, these distribution-level statistics do not provide information about individual images within the distribution, necessitating an OOD detection strategy. Tables  4  &  B  compare the performance of Forte+GMM against a strong CLIP-based baseline on the Golden Retriever and Volleyball classes from ImageNet. For the well-represented Golden Retriever class, Forte+GMM consistently outperforms the baseline across all image generation settings, achieving near-perfect AUROC scores and low FPR95 values for Img2Img with strength parameters 0.9 and 1.0, Caption-based, and Class-based image generation. Lower performance at lower strengths is due to images being too similar to the real distribution. The Volleyball class, part of Hard ImageNet  (Moayeri et al.,  2022 ) , focuses on classes with strong spurious cues. Volleyballs rarely occur alone in ImageNet images, and generating images without appropriate priors results in mode collapse (see  Figure 4 ). Forte+GMM surpasses the CLIP-based baseline in all image generation scenarios, with notable improvements for Img2Img with higher strength parameters, Caption-based, and Class-based image generation (AUROC scores > 97%, FPR95 values < 6%).",
            "To simulate this scenario, two public datasets are used for the experiments in Section  4.2 : coronal knee MRI from FastMRI  Zbontar et al. ( 2018 ); Knoll et al. ( 2020 )  with two subsets and Osteoarthritis Initiative (OAI)  Nevitt et al. ( 2006 )  with three subsets. The acquisition parameters, including sequence and fat suppression are detailed in Table  6  and samples are shown in Figure  14 . Treating the FastMRI dataset as in-distribution and assuming that models have been trained on them, Forte is used to determine the next course of action when confronted with the OAI dataset: 1) which subsets of the new dataset can be aligned with the existing subsets / models? 2) To what degree do these subsets diverge from each other? Using the FastMRI FS subset as in-distribution, the two OAI subsets (OAI T1 and OAI MPR) are tested for OOD detection. Similarly, the FastMRI NoFS subset is used as in-distribution and the OAI TSE subset is tested for OOD detection."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Out-of-distribution (OOD) detection using Forte, applied to medical image datasets. Strong performance by Forte suggests the presence of batch effects and a need for data harmonization. Refer to Appendix  D  for more details on the FastMRI and OAI datasets. For reliable estimation, performance is measured over 10 random seeds.",
        "table": "A4.T6.11",
        "footnotes": [
            ""
        ],
        "references": [
            "Magnetic resonance imaging (MRI) datasets and models present a unique challenge in a high stakes scenario, making robust out-of-distribution (OOD) detection paramount. These datasets, typically acquired under specific study protocols, suffer from batch effects that hinder model generalization even when changes in protocols are minute  (Horng,  2023 ) . The problem is exacerbated by dataset homogeneity within studies and limited dataset sizes in clinical applications, making it impractical to train separate models for each batch. Subtle distribution shifts between datasets of the same subject matter, though not immediately apparent to human observers, can significantly impact model performance. While data augmentation and harmonization methods have been explored  (Hu et al.,  2023 ) , existing metrics for detecting distribution drift and assessing harmonization effectiveness are limited. Our work proposes deploying Forte as a more robust metric to address this gap by effectively differentiating between datasets that may appear similar but have crucial distributional differences. Using public datasets like FastMRI  Zbontar et al. ( 2018 ); Knoll et al. ( 2020 )  and the Osteoarthritis Initiative (OAI)  Nevitt et al. ( 2006 ) , we simulate realistic scenarios where models trained on one dataset (treated as in-distribution) are confronted with another (considered OOD). Table  5  demonstrates the effectiveness of Forte in differentiating the datasets. In other tests over closed-source but real-world hospital data, we observe similar near perfect performance, suggesting that Forte can have zero-shot applications in a variety of other sensitive domains."
        ]
    }
}