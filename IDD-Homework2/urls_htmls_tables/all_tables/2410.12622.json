{
    "id_table_1": {
        "caption": "Table 1:  Studies on synthetic training data generation.  We differentiate our work from previous research based on whether prompts ask the LLM to rewrite examples from real data (alterations) and whether they include additional theoretical information on the construct to be classified. \"-\" indicates that the exact prompt could not be determined from the information provided by the authors. See Appendix  A  for a more detailed overview.",
        "table": "S2.T1.1.1",
        "footnotes": [
            ""
        ],
        "references": [
            "LLMs have been used to create synthetic data since their development. At first, smaller models such as GPT-2 were fine-tuned to generate training examples for specific classification tasks  Bayer_2022 ; Juuti et al. ( 2020 ); Wullach et al. ( 2021 ); Schmidhuber ( 2021 ) . However because fine-tuning state-of-the-art LLMs became more cost-intensive and access to LLMs shifted to APIs, researchers started to explore prompt-based approaches of synthetic data creation  Han et al. ( 2022 ) , which rely on few-shot learning capabilities of LLMs  Brown et al. ( 2020 ) . Under the prompt-based approach, the primary way to steer the data-generating process of the LLM is to add conditioning information to the prompt. Previous work can broadly be categorized by their prompting strategies: (a) whether they add theoretical information on the construct of interest to the prompt, and (b) whether they provide examples of existing data to generate alternations (see Table  1 )",
            "Almost all studies shown in Table  1  use examples randomly drawn from a real-world dataset in their prompt. If they do so, the also instruct the LLM to base the newly generated training examples on the examples provided in the prompt.  Li et al.,  2023  and  Veselovsky et al.,  2023  additionally compared the presence of an example in a prompt to its absence and concluded that providing examples in the prompt increases the performance in downstream classifiers.",
            "Figure  1  visualizes our workflow.",
            "Based on the different prompting strategies identified in Table  1 , we generate different types of synthetic data. The generation of synthetic data was varied according to two factors:  instruction strategy  ( theory-driven  or  naive ) and  generation method  ( alternations of existing data  or  newly generated data ). Both combinations of prompting strategies allow us to obtain labeled instances of synthetic data without human labeling. Below, we further outline the basic ideas behind our taxonomy. See Appendix  C  for the full overview of all prompts used to generate synthetic data according to the combination of both variants."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Descriptive Statistics for Political Topics Study . ER = External Relations, FD = Freedom and Democracy, PS = Political System, EC = Economy, WQ = Welfare and Quality of Life, FS = Fabric of Society, SC = Social Groups. In-domain train and test datasets were subsequently balanced for the analysis.",
        "table": "S2.T1.1.1.1.2.1",
        "footnotes": [],
        "references": [
            "We rely on a two-class (sexism) and a multi-class (political topics) dataset to explore the effect of theory-driven synthetic data for training text classification models. Both studies serve as important case studies because they focus on constructs commonly explored by computational social scientists, who seek to address social science questions through natural language processing methods  (Glavas et al.,  2019 ) . We include both in-domain datasets that are used to train and test the models, as well as OOD datasets used for evaluation only. Table  2  and Table  3  provide an overview of the class distribution for each dataset.",
            "In-Domain:  Figure  2  displays the in-domain model performances across the varying prompting strategies for both studies."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Descriptive Statistics for Sexism Study . S = Sexist, nS = non-Sexist. In-domain train and test datasets were subsequently balanced for the analysis.",
        "table": "S2.T1.1.1.1.3.1",
        "footnotes": [],
        "references": [
            "We rely on a two-class (sexism) and a multi-class (political topics) dataset to explore the effect of theory-driven synthetic data for training text classification models. Both studies serve as important case studies because they focus on constructs commonly explored by computational social scientists, who seek to address social science questions through natural language processing methods  (Glavas et al.,  2019 ) . We include both in-domain datasets that are used to train and test the models, as well as OOD datasets used for evaluation only. Table  2  and Table  3  provide an overview of the class distribution for each dataset.",
            "OOD:  Overall, our findings (Table  5  and Figure  3 ) indicate that the inclusion of synthetic data did only marginally improve  overall  OOD performance.",
            "Overall, our results are twofold. In the political topics study,  we observe a consistent, slight decline in performance as the proportion of synthetic data increases across different prompting strategies . Nonetheless, our results demonstrate that a model trained with just 30 per cent labeled data and 70 per cent synthetic data achieved an F1-score of .60, only  .04 .04 .04 .04  lower than a model trained on only human-labeled data. A model trained exclusively on synthetic data still attained a respectable macro-F1 score of .52. Furthermore, most RoBERTa (X) models achieve significantly higher performance than their RoBERTa (X-labeled subset) baseline models, indicating that the addition of synthetic data helped the model in the training process beyond the labeled data available in the training dataset. In contrast, for the sexism study, including synthetic data did not improve model performance. Across all data subsets, the RoBERTa (X-labeled subset) models trained on only the share of labeled data performed equally well or even outperformed those trained on the combined labeled and synthetic subsets. Figure  3  illustrates that this pattern is consistent for OOD classification."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Best In-domain Performance . Column I refers to  Instruction Strategy , with T = Theory-driven and N = New; Column G refers to the  Generation Type , with N = Naive and A = Alternations. The prompting model refers to the baseline GPT-4o. For the sexism model, we report average performance across four out-of-domain (OOD) test sets. The best-performing models are highlighted in bold.",
        "table": "S2.T1.1.1.1.4.1",
        "footnotes": [],
        "references": [
            "Table  4  and Table  5  show the best-performing models that were trained on datasets with different shares of synthetic data for in-domain and OOD, respectively.",
            "To test the robustness of our findings, Figure  4  displays the differences in performance between the default GPT-3.5-turbo and the Llama-3-70b generation model."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Best OOD Performance . Column I refers to  Instruction Strategy , with T = Theory-driven and N = New; Column G refers to the  Generation Type , with N = Naive and A = Alternations. The prompting model refers to the baseline GPT-4o. For the sexism model, we report average performance across four out-of-domain (OOD) test sets. The best-performing models are highlighted in bold.",
        "table": "S3.T2.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "Table  4  and Table  5  show the best-performing models that were trained on datasets with different shares of synthetic data for in-domain and OOD, respectively.",
            "OOD:  Overall, our findings (Table  5  and Figure  3 ) indicate that the inclusion of synthetic data did only marginally improve  overall  OOD performance."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Impact of Theory-Driven vs. Naive Instruction Strategy . Diff shows the performance gap between models trained on theory-driven and naive synthetic data, with positive values indicating better performance of theory-driven synthetic training data.",
        "table": "S3.T3.1.1",
        "footnotes": [
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "To further investigate the impact of a theory-driven data generation strategy, Table  6  presents the mean differences between models trained on theory-driven data and those trained on synthetic data generated using a naive  instruction strategy .",
            "In the political topics study,  models trained on theory-driven data consistently outperformed the naive ones for both in-domain and OOD classification . As the proportion of synthetic data increased, the performance gap between the two prompting strategies widened, reaching a difference for the RoBERTa (100) models of up to 14% for the in-domain and 12% for the OOD test sets. In the sexism study, synthetic data generally did not benefit the models. This is evident in the lack of significant differences across prompting strategies, where models trained on theory-driven data showed only slightly better performance on the in-domain dataset but worse performance on the OOD dataset (see Table  6 )."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Comparison of Studies on Prompting and Classification Tasks.  Constructs  refers to whether different constructs were evaluated.  Task  indicates whether the classification task varied.  Theory  shows if the study used theory-driven prompts.  Examples  refers to whether examples were included in prompts. Alternations show if the prompt used included instructions to rewrite a given example.  Prompt strategy  indicates if prompt strategies were varied.  Human + Synth.  refers to the combination of human and synthetic data.  OOD Test  signifies out-of-domain testing, and  LLM Direct  compares classification using LLM without prompting. \"-\" indicates that the exact prompt could not be determined from the information provided by the authors.",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "Table 8:  (continued)",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": [
            "For the multi-class dataset, we rely on a dataset of annotated sentences from UK party manifestos between 1964 and 2019 gathered from the Party Manifesto Project  (Lehmann et al.,  2024 ) . Sentences are coded into one of seven political issues, that are  external relations, freedom and democracy, political system, economy, welfare and quality of life, fabric of society  and  social groups . As an OOD dataset, we rely on a dataset by  Osnabrugge et al. ( 2023 )  who coded 4,165 New Zealand parliamentary speeches using the same codebook and classes. Hence, the labels should be comparable despite utilising a different text source and political context in a foreign parliamentary system. See Appendix  8  for an overview of all the political topics and their descriptions."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Prompt Designs",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "Table 10:  Exemplary Survey Items",
        "table": "A1.T7.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table 11:  Prompts Classification",
        "table": "A2.T8.1",
        "footnotes": [],
        "references": []
    },
    "id_table_12": {
        "caption": "",
        "table": "A3.T9.1",
        "footnotes": [],
        "references": []
    },
    "id_table_13": {
        "caption": "",
        "table": "A4.T10.1",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A5.T11.1",
        "footnotes": [],
        "references": []
    }
}