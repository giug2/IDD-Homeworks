{
    "id_table_1": {
        "caption": "Table 1:  The experiments in this benchmark.",
        "table": "S3.E2",
        "footnotes": [],
        "references": [
            "In this paper, we address this gap by presenting a psycholinguistic benchmark study that evaluates the humanlikeness of 20 LLMs. Our benchmark consists of 10 representative psycholinguistic experiments, adapted from  Cai et al. ( 2024 ) , which cover five core linguistic aspects: sound, word, syntax, semantics, and discourse, with two experiments dedicated to each aspect (see  1 ). We collected approximately 50 to 100 responses per item from over 2,000 human participants. Additionally, we gathered 100 responses per item from each of the 20 LLMs, including well-known models such as GPT-4o, GPT-3.5, Llama 2, Llama 3, Llama 3.1, and other state-of-the-art models (see Table  1 ). To quantify humanlikeness, we developed an auto-coding algorithm that efficiently and reliably extracts language use patterns from responses. The humanlikeness metric was then calculated based on the similarity between the response distributions of humans and LLMs, using a comparison of their probability distributions."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The humanlikness score for models in different experiments.",
        "table": "S3.E3",
        "footnotes": [],
        "references": [
            "Experimental Design  To compare human responses with those generated by LLMs, we employed the same 10 psycholinguistic tasks designed for human participants. 20 LLMs (See Table  2 ) were selected for evaluation, including models from prominent families like OpenAIs GPT series (GPT-4o, GPT-3.5), Metas Llama series (Llama 2, Llama 3, Llama 3.1) and Mistral series OpenAI et al. ( 2024 ); Touvron et al. ( 2023 ); AI ( 2024 ) . Each model provided 100 responses per item in each experiment, ensuring that the response data was comparable to the human data. Similar to the human experimental design, LLMs followed a one-trial-per-run paradigm, ensuring that responses were generated independently for each item to prevent context effects. The input format for the LLMs closely mirrored the instructions provided to human participants. Careful modification of human prompts was performed to ensure that task instructions were clear and interpretable by LLMs. This allowed for a direct comparison between human and LLM performance on the same tasks under identical conditions."
        ]
    }
}