{
    "id_table_1": {
        "caption": "Table 1:  EM accuracy of full data (All) and subsets with gold ( Subset Gold subscript Subset Gold \\text{Subset}_{\\text{Gold}} Subset start_POSTSUBSCRIPT Gold end_POSTSUBSCRIPT ) and noisy contexts ( Subset Noisy subscript Subset Noisy \\text{Subset}_{\\text{Noisy}} Subset start_POSTSUBSCRIPT Noisy end_POSTSUBSCRIPT ). The highest score is in  bold , and the second-best is  underlined .",
        "table": "S3.T1.24",
        "footnotes": [],
        "references": [
            "To assess whether the existing contrastive decoding approaches can be utilized in practice,  we extend the setting to situations where the gold-standard context is not guaranteed, specifically in the retrieval-augmented generation (RAG) framework  (Yao et al.,  2022 ; Shi et al.,  2024 ; Izacard et al.,  2023 ) .  In this paper, we demonstrate that existing context-aware contrastive decoding approaches experience performance drops in open-domain question answering, especially when the retrieved context is noisy.  To address this issue, we propose  adaptive contrastive decoding (ACD) , adaptively weighting the contrastive contextual influence on the parametric knowledge, making it suitable for noisy context settings (Figure  1 ).",
            "As shown in Table  1 , ACD outperforms the baselines across all datasets and models within the RAG framework, particularly when considering the full test data (All).  When analyzing the performance by dividing the data into two subsets based on whether the retrieved context is gold ( Subset Gold subscript Subset Gold \\text{Subset}_{\\text{Gold}} Subset start_POSTSUBSCRIPT Gold end_POSTSUBSCRIPT ) or not ( Subset Noisy subscript Subset Noisy \\text{Subset}_{\\text{Noisy}} Subset start_POSTSUBSCRIPT Noisy end_POSTSUBSCRIPT ), ACD achieves either the best or second-best performance.   MICD D subscript MICD D \\text{MICD}_{D} MICD start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT  demonstrates performance comparable to ACD on  Subset Noisy subscript Subset Noisy \\text{Subset}_{\\text{Noisy}} Subset start_POSTSUBSCRIPT Noisy end_POSTSUBSCRIPT .  However, it shows a significant drop on  Subset Gold subscript Subset Gold \\text{Subset}_{\\text{Gold}} Subset start_POSTSUBSCRIPT Gold end_POSTSUBSCRIPT , indicating a tendency to ignore gold context while handling noisy context.  It is notable that both CAD and  MICD F subscript MICD F \\text{MICD}_{F} MICD start_POSTSUBSCRIPT italic_F end_POSTSUBSCRIPT  exhibit a significant drop in their performance under noisy conditions.",
            "AUROC of ACD and MICD D  for three models not reported in Table  2  is reported in Table  10 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  AUROC between    \\alpha italic_  used in each method and the noisiness of the retrieved context.",
        "table": "S4.T2.4.4",
        "footnotes": [],
        "references": [
            "From Figure  2 ,  we observe that ACD outperforms the baselines in  Known-noisy .  Notably, two approaches with adaptively adjusted weight, ACD and  MICD D subscript MICD D \\text{MICD}_{D} MICD start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT , perform well in  Known-noisy ,  while other baselines show a relative strength in  Unknown-gold .  However, these baselines also experience significant performance drops in  Known-noisy , indicating distraction by noisy context despite correctly answering when only the question is provided.  In both cases, ACD demonstrates better performance compared to  MICD D subscript MICD D \\text{MICD}_{D} MICD start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT , overall showing a tendency towards reliability.",
            "Adaptive weights   ACD subscript  ACD \\alpha_{\\text{ACD}} italic_ start_POSTSUBSCRIPT ACD end_POSTSUBSCRIPT  and   MICD subscript  MICD \\alpha_{\\text{MICD}} italic_ start_POSTSUBSCRIPT MICD end_POSTSUBSCRIPT  are extracted at each decoding step and analyzed across three metrics: maximum, average, and the first within the generated sequence.  As an evaluation metric, the area under the receiver operator characteristic curve (AUROC) between    \\alpha italic_  and the noisiness of the retrieved context is measured.  AUROC of each    \\alpha italic_  for  Llama 2-7B  is reported in Table  2 .  Under every metric and dataset, ACD demonstrates a higher AUROC compared to MICD D D {}_{\\text{D}} start_FLOATSUBSCRIPT D end_FLOATSUBSCRIPT .  Aligned with our motivation, when the model is knowledgeable and presented with noisy context,   ACD subscript  ACD \\alpha_{\\text{ACD}} italic_ start_POSTSUBSCRIPT ACD end_POSTSUBSCRIPT  tends to be lower, emphasizing greater reliance on parametric knowledge.  Conversely, when the model lacks knowledge and is provided with gold context,   ACD subscript  ACD \\alpha_{\\text{ACD}} italic_ start_POSTSUBSCRIPT ACD end_POSTSUBSCRIPT  is adjusted to prioritize reliance on the provided context.",
            "AUROC of ACD and MICD D  for three models not reported in Table  2  is reported in Table  10 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Template used in closed-book generation.",
        "table": "A1.T3.1",
        "footnotes": [],
        "references": [
            "With a knowledge conflict QA dataset, NQ-swap  (Longpre et al.,  2022 ) , we verify whether the two decoding methods with dynamic weight, ACD and  MICD D subscript MICD D \\text{MICD}_{D} MICD start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT , can generate context-based responses without considering a conflicting context as a noisy context.  The conflicting context in the NQ-swap dataset is constructed by replacing the answer entity span in the original gold context with a random entity of the same type.  Figure  3  illustrates that ACD consistently exceeds the performance of  MICD D subscript MICD D \\text{MICD}_{D} MICD start_POSTSUBSCRIPT italic_D end_POSTSUBSCRIPT  across all models and achieves results comparable to open-book regular decoding.  The results indicate that the ACDs approach remains effective even in settings where the context is relevant to the question but contradicts the models parametric knowledge.",
            "The templates we use throughout the experiment are in Table  3  and Table  4 . The template used in open-book generation (Table  4 ) is applied to get context-augmented distribution  z t c superscript subscript z t c \\mathbf{z}_{t}^{c} bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT . Also, to obtain  z t subscript z t \\mathbf{z}_{t} bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , the template in Table  3  is used."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Template used in open-book generation.",
        "table": "A1.T4.1",
        "footnotes": [],
        "references": [
            "To assess the impact of   A  C  D subscript  A C D \\alpha_{ACD} italic_ start_POSTSUBSCRIPT italic_A italic_C italic_D end_POSTSUBSCRIPT  on performance, we fix the value of    \\alpha italic_  within a range  [ 0 , 1 ] 0 1 [0,1] [ 0 , 1 ]  and examine whether employing ACD is more effective than optimizing a fixed weight.  In Figure  4 , it can be observed that using a fixed    \\alpha italic_  results in degraded performance compared to ACD.  Increasing the alpha value, which enhances the contextual influence on the output distribution, initially leads to a rise in the EM score.  However, beyond a certain point, further increasing    \\alpha italic_  results in a decline in the EM score.  In scenarios with potential noisy context, a fixed    \\alpha italic_  value may not ensure optimal performance.  Therefore, employing an adaptive weight,   A  C  D subscript  A C D \\alpha_{ACD} italic_ start_POSTSUBSCRIPT italic_A italic_C italic_D end_POSTSUBSCRIPT , to adjust the impact of contextual knowledge based on entropy is crucial for improving overall performance.",
            "The templates we use throughout the experiment are in Table  3  and Table  4 . The template used in open-book generation (Table  4 ) is applied to get context-augmented distribution  z t c superscript subscript z t c \\mathbf{z}_{t}^{c} bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT . Also, to obtain  z t subscript z t \\mathbf{z}_{t} bold_z start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , the template in Table  3  is used."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Recall@100 performance for  Contriever-msmarco",
        "table": "A1.T5.1.1",
        "footnotes": [],
        "references": [
            "To assess performance in the RAG framework, the top-1 context from top-100 contexts retrieved by  Contriever-msmarco   (Izacard et al.,  2022 )  is utilized.  Recall@100 is reported for each dataset in Table  5 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  EM score comparison between ACD (  A  C  D subscript  A C D \\alpha_{ACD} italic_ start_POSTSUBSCRIPT italic_A italic_C italic_D end_POSTSUBSCRIPT ) and ACD with oracle alpha value (  o  r  a  c  l  e subscript  o r a c l e \\alpha_{oracle} italic_ start_POSTSUBSCRIPT italic_o italic_r italic_a italic_c italic_l italic_e end_POSTSUBSCRIPT ).",
        "table": "A3.T6.8.8",
        "footnotes": [],
        "references": [
            "For TriviaQA dataset, the performance of ACD is comparable to   o  r  a  c  l  e subscript  o r a c l e \\alpha_{oracle} italic_ start_POSTSUBSCRIPT italic_o italic_r italic_a italic_c italic_l italic_e end_POSTSUBSCRIPT , with less than 1 point difference (Table  6 ). NQ and PopQA show a difference of approximately 2-3 points, indicating that the method for calculating the    \\alpha italic_  weight could be further enhanced in future research."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Case study on the value of   A  C  D subscript  A C D \\alpha_{ACD} italic_ start_POSTSUBSCRIPT italic_A italic_C italic_D end_POSTSUBSCRIPT  for  Known-noisy  and  Unknown-gold  cases in  Llama2 7B . Each value of entropy without context ( H  ( Y t ) H subscript Y t H(Y_{t}) italic_H ( italic_Y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ) ), entropy with context ( H  ( Y t c ) H superscript subscript Y t c H(Y_{t}^{c}) italic_H ( italic_Y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_c end_POSTSUPERSCRIPT ) ), and   A  C  D subscript  A C D \\alpha_{ACD} italic_ start_POSTSUBSCRIPT italic_A italic_C italic_D end_POSTSUBSCRIPT  is extracted at the first decoding step ( t = 0 t 0 t=0 italic_t = 0 ).",
        "table": "A3.T7.5",
        "footnotes": [],
        "references": [
            "We conduct the case study on   A  C  D subscript  A C D \\alpha_{ACD} italic_ start_POSTSUBSCRIPT italic_A italic_C italic_D end_POSTSUBSCRIPT , examining its value in cases of  Known-noisy  and  Unknown-gold .  Table  7  shows the generations from  Llama2 7B  and how the values of entropy from closed-book generation ( Reg C  l  s subscript Reg C l s \\text{Reg}_{Cls} Reg start_POSTSUBSCRIPT italic_C italic_l italic_s end_POSTSUBSCRIPT ) and open-book generation ( Reg O  p  n subscript Reg O p n \\text{Reg}_{Opn} Reg start_POSTSUBSCRIPT italic_O italic_p italic_n end_POSTSUBSCRIPT ) affect   A  C  D subscript  A C D \\alpha_{ACD} italic_ start_POSTSUBSCRIPT italic_A italic_C italic_D end_POSTSUBSCRIPT  at the first decoding time step."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  EM accuracy of  Known-noisy  case.",
        "table": "A3.T8.12",
        "footnotes": [],
        "references": [
            "For  Known-noisy  and  Unknown-gold , the exact values of EM accuracy on each case are reported in Table  8  and Table  9 , respectively."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  EM accuracy of  Unknown-gold  case.",
        "table": "A3.T9.12",
        "footnotes": [],
        "references": [
            "For  Known-noisy  and  Unknown-gold , the exact values of EM accuracy on each case are reported in Table  8  and Table  9 , respectively."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  AUROC between    \\alpha italic_  used in each method and the noisiness of the retrieved context. The best AUROC is in bold.",
        "table": "A3.T10.10.10",
        "footnotes": [],
        "references": [
            "AUROC of ACD and MICD D  for three models not reported in Table  2  is reported in Table  10 ."
        ]
    },
    "global_footnotes": [
        "Corresponding author.",
        "Wikipedia dump from Dec. 2018."
    ]
}