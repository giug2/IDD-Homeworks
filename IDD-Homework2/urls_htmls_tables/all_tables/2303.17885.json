{
    "PAPER'S NUMBER OF TABLES": 1,
    "S5.T1": {
        "caption": "TABLE I: Computational Complexity of Neural Networks.",
        "table": "<table id=\"S5.T1.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tr id=\"S5.T1.3.4\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.4.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_tt\" colspan=\"2\"><span id=\"S5.T1.3.4.1.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">AWE</span></td>\n<td id=\"S5.T1.3.4.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span id=\"S5.T1.3.4.2.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">CIFAR-10</span></td>\n<td id=\"S5.T1.3.4.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_tt\" colspan=\"2\"><span id=\"S5.T1.3.4.3.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">FMNIST</span></td>\n</tr>\n<tr id=\"S5.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><math id=\"S5.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\hat{d}_{0}\" display=\"inline\"><semantics id=\"S5.T1.1.1.1.m1.1a\"><msub id=\"S5.T1.1.1.1.m1.1.1\" xref=\"S5.T1.1.1.1.m1.1.1.cmml\"><mover accent=\"true\" id=\"S5.T1.1.1.1.m1.1.1.2\" xref=\"S5.T1.1.1.1.m1.1.1.2.cmml\"><mi mathcolor=\"#0000FF\" mathsize=\"80%\" id=\"S5.T1.1.1.1.m1.1.1.2.2\" xref=\"S5.T1.1.1.1.m1.1.1.2.2.cmml\">d</mi><mo mathcolor=\"#0000FF\" mathsize=\"80%\" id=\"S5.T1.1.1.1.m1.1.1.2.1\" xref=\"S5.T1.1.1.1.m1.1.1.2.1.cmml\">^</mo></mover><mn mathcolor=\"#0000FF\" mathsize=\"80%\" id=\"S5.T1.1.1.1.m1.1.1.3\" xref=\"S5.T1.1.1.1.m1.1.1.3.cmml\">0</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.1.1.1.m1.1b\"><apply id=\"S5.T1.1.1.1.m1.1.1.cmml\" xref=\"S5.T1.1.1.1.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.1.1.1.m1.1.1.1.cmml\" xref=\"S5.T1.1.1.1.m1.1.1\">subscript</csymbol><apply id=\"S5.T1.1.1.1.m1.1.1.2.cmml\" xref=\"S5.T1.1.1.1.m1.1.1.2\"><ci id=\"S5.T1.1.1.1.m1.1.1.2.1.cmml\" xref=\"S5.T1.1.1.1.m1.1.1.2.1\">^</ci><ci id=\"S5.T1.1.1.1.m1.1.1.2.2.cmml\" xref=\"S5.T1.1.1.1.m1.1.1.2.2\">ùëë</ci></apply><cn type=\"integer\" id=\"S5.T1.1.1.1.m1.1.1.3.cmml\" xref=\"S5.T1.1.1.1.m1.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.1.1.1.m1.1c\">\\hat{d}_{0}</annotation></semantics></math></td>\n<td id=\"S5.T1.3.3.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.3.4.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">FLOPs</span></td>\n<td id=\"S5.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S5.T1.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"\\hat{d}_{0}\" display=\"inline\"><semantics id=\"S5.T1.2.2.2.m1.1a\"><msub id=\"S5.T1.2.2.2.m1.1.1\" xref=\"S5.T1.2.2.2.m1.1.1.cmml\"><mover accent=\"true\" id=\"S5.T1.2.2.2.m1.1.1.2\" xref=\"S5.T1.2.2.2.m1.1.1.2.cmml\"><mi mathcolor=\"#0000FF\" mathsize=\"80%\" id=\"S5.T1.2.2.2.m1.1.1.2.2\" xref=\"S5.T1.2.2.2.m1.1.1.2.2.cmml\">d</mi><mo mathcolor=\"#0000FF\" mathsize=\"80%\" id=\"S5.T1.2.2.2.m1.1.1.2.1\" xref=\"S5.T1.2.2.2.m1.1.1.2.1.cmml\">^</mo></mover><mn mathcolor=\"#0000FF\" mathsize=\"80%\" id=\"S5.T1.2.2.2.m1.1.1.3\" xref=\"S5.T1.2.2.2.m1.1.1.3.cmml\">0</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.2.2.2.m1.1b\"><apply id=\"S5.T1.2.2.2.m1.1.1.cmml\" xref=\"S5.T1.2.2.2.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.2.2.2.m1.1.1.1.cmml\" xref=\"S5.T1.2.2.2.m1.1.1\">subscript</csymbol><apply id=\"S5.T1.2.2.2.m1.1.1.2.cmml\" xref=\"S5.T1.2.2.2.m1.1.1.2\"><ci id=\"S5.T1.2.2.2.m1.1.1.2.1.cmml\" xref=\"S5.T1.2.2.2.m1.1.1.2.1\">^</ci><ci id=\"S5.T1.2.2.2.m1.1.1.2.2.cmml\" xref=\"S5.T1.2.2.2.m1.1.1.2.2\">ùëë</ci></apply><cn type=\"integer\" id=\"S5.T1.2.2.2.m1.1.1.3.cmml\" xref=\"S5.T1.2.2.2.m1.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.2.2.2.m1.1c\">\\hat{d}_{0}</annotation></semantics></math></td>\n<td id=\"S5.T1.3.3.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.3.5.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">FLOPs</span></td>\n<td id=\"S5.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><math id=\"S5.T1.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"\\hat{d}_{0}\" display=\"inline\"><semantics id=\"S5.T1.3.3.3.m1.1a\"><msub id=\"S5.T1.3.3.3.m1.1.1\" xref=\"S5.T1.3.3.3.m1.1.1.cmml\"><mover accent=\"true\" id=\"S5.T1.3.3.3.m1.1.1.2\" xref=\"S5.T1.3.3.3.m1.1.1.2.cmml\"><mi mathcolor=\"#0000FF\" mathsize=\"80%\" id=\"S5.T1.3.3.3.m1.1.1.2.2\" xref=\"S5.T1.3.3.3.m1.1.1.2.2.cmml\">d</mi><mo mathcolor=\"#0000FF\" mathsize=\"80%\" id=\"S5.T1.3.3.3.m1.1.1.2.1\" xref=\"S5.T1.3.3.3.m1.1.1.2.1.cmml\">^</mo></mover><mn mathcolor=\"#0000FF\" mathsize=\"80%\" id=\"S5.T1.3.3.3.m1.1.1.3\" xref=\"S5.T1.3.3.3.m1.1.1.3.cmml\">0</mn></msub><annotation-xml encoding=\"MathML-Content\" id=\"S5.T1.3.3.3.m1.1b\"><apply id=\"S5.T1.3.3.3.m1.1.1.cmml\" xref=\"S5.T1.3.3.3.m1.1.1\"><csymbol cd=\"ambiguous\" id=\"S5.T1.3.3.3.m1.1.1.1.cmml\" xref=\"S5.T1.3.3.3.m1.1.1\">subscript</csymbol><apply id=\"S5.T1.3.3.3.m1.1.1.2.cmml\" xref=\"S5.T1.3.3.3.m1.1.1.2\"><ci id=\"S5.T1.3.3.3.m1.1.1.2.1.cmml\" xref=\"S5.T1.3.3.3.m1.1.1.2.1\">^</ci><ci id=\"S5.T1.3.3.3.m1.1.1.2.2.cmml\" xref=\"S5.T1.3.3.3.m1.1.1.2.2\">ùëë</ci></apply><cn type=\"integer\" id=\"S5.T1.3.3.3.m1.1.1.3.cmml\" xref=\"S5.T1.3.3.3.m1.1.1.3\">0</cn></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.T1.3.3.3.m1.1c\">\\hat{d}_{0}</annotation></semantics></math></td>\n<td id=\"S5.T1.3.3.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.3.6.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">FLOPs</span></td>\n</tr>\n<tr id=\"S5.T1.3.5\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.5.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.5.1.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">100</span></td>\n<td id=\"S5.T1.3.5.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.5.2.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">13,568</span></td>\n<td id=\"S5.T1.3.5.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.5.3.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">100</span></td>\n<td id=\"S5.T1.3.5.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.5.4.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">7,484</span></td>\n<td id=\"S5.T1.3.5.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.5.5.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">300</span></td>\n<td id=\"S5.T1.3.5.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.5.6.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">19,584</span></td>\n</tr>\n<tr id=\"S5.T1.3.6\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.6.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.6.1.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">300</span></td>\n<td id=\"S5.T1.3.6.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.6.2.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">39,168</span></td>\n<td id=\"S5.T1.3.6.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.6.3.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">200</span></td>\n<td id=\"S5.T1.3.6.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.6.4.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">14,584</span></td>\n<td id=\"S5.T1.3.6.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.6.5.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">400</span></td>\n<td id=\"S5.T1.3.6.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.6.6.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">25,984</span></td>\n</tr>\n<tr id=\"S5.T1.3.7\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.7.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.7.1.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">500</span></td>\n<td id=\"S5.T1.3.7.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.7.2.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">64,768</span></td>\n<td id=\"S5.T1.3.7.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.7.3.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">300</span></td>\n<td id=\"S5.T1.3.7.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.7.4.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">21,684</span></td>\n<td id=\"S5.T1.3.7.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.7.5.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">500</span></td>\n<td id=\"S5.T1.3.7.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.7.6.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">32,384</span></td>\n</tr>\n<tr id=\"S5.T1.3.8\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.8.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.8.1.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">700</span></td>\n<td id=\"S5.T1.3.8.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.8.2.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">90,368</span></td>\n<td id=\"S5.T1.3.8.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.8.3.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">400</span></td>\n<td id=\"S5.T1.3.8.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.8.4.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">28,784</span></td>\n<td id=\"S5.T1.3.8.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.8.5.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">600</span></td>\n<td id=\"S5.T1.3.8.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.8.6.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">38,784</span></td>\n</tr>\n<tr id=\"S5.T1.3.9\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.9.1\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.9.1.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">900</span></td>\n<td id=\"S5.T1.3.9.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.9.2.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">115,968</span></td>\n<td id=\"S5.T1.3.9.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.9.3.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">500</span></td>\n<td id=\"S5.T1.3.9.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.9.4.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">35,884</span></td>\n<td id=\"S5.T1.3.9.5\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.9.5.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">700</span></td>\n<td id=\"S5.T1.3.9.6\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.9.6.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">45,184</span></td>\n</tr>\n<tr id=\"S5.T1.3.10\" class=\"ltx_tr\">\n<td id=\"S5.T1.3.10.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.10.1.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">1024</span></td>\n<td id=\"S5.T1.3.10.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.10.2.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">131,840</span></td>\n<td id=\"S5.T1.3.10.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.10.3.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">600</span></td>\n<td id=\"S5.T1.3.10.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.10.4.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">42,984</span></td>\n<td id=\"S5.T1.3.10.5\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.10.5.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">784</span></td>\n<td id=\"S5.T1.3.10.6\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S5.T1.3.10.6.1\" class=\"ltx_text\" style=\"font-size:80%;color:#0000FF;\">50,560</span></td>\n</tr>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "For the FMNIST dataset, we consider to train a three-layer perceptron with a ",
                "64",
                "64",
                "64",
                "-neuron hidden layer and a ",
                "10",
                "10",
                "10",
                "-neuron output layer, respectively.\nFor the CIFAR-10 dataset, we consider to train a neural network that consists of a 1D-convolutional layer, a ",
                "64",
                "64",
                "64",
                "-neuron hidden layer, and a ",
                "10",
                "10",
                "10",
                "-neuron output layer.\nThe 1D-convolutional layer contains a ",
                "7",
                "√ó",
                "1",
                "7",
                "1",
                "7\\times 1",
                " filter.\nFor the CIFAR-10 dataset, we preprocess the data samples via a pretrained AlexNet ",
                "[",
                "48",
                "]",
                " for the convenience of PCA operations.\nFor the AWE dataset, we consider to train a three-layer perceptron with a ",
                "128",
                "128",
                "128",
                "-neuron hidden layer and a ",
                "6",
                "6",
                "6",
                "-neuron output layer, respectively.\nThe hyperbolic tangent activation function is used at the hidden layer, and the loss function is the softmax cross entropy.\nThe computational complexities for neural networks can be obtained based on ",
                "[",
                "49",
                ", Appendix A.1]",
                ".\nIn this work, we estimate the computational complexity of neural networks for three different datasets and different input sizes via the python package ",
                "thop",
                ".\nThe results are shown in Table ",
                "I",
                ".\nWe observe that the neural network for AWE with ",
                "1",
                ",",
                "024",
                "1",
                "024",
                "1,024",
                "-dimensional input has the highest computational complexity‚Äî",
                "131",
                ",",
                "840",
                "131",
                "840",
                "131,840",
                " FLOPs.\nMultiplying by the number of frames, we obtain the worst-case overall complexity is around ",
                "3.96",
                "√ó",
                "10",
                "9",
                "3.96",
                "superscript",
                "10",
                "9",
                "3.96\\times 10^{9}",
                " FLOPs.\nNote that a Raspberry Pi-4B (4GB) 64-bit has a ",
                "2.02",
                "√ó",
                "10",
                "9",
                "2.02",
                "superscript",
                "10",
                "9",
                "2.02\\times 10^{9}",
                " FLOPs per second per watt",
                "2",
                "2",
                "2",
                "https://web.eece.maine.edu/~vweaver/group/green_machines.html",
                ".\nThe ",
                "3.96",
                "√ó",
                "10",
                "9",
                "3.96",
                "superscript",
                "10",
                "9",
                "3.96\\times 10^{9}",
                " FLOPs requirement can be well handled by current off-the-shelf energy-limited terminals that have ",
                "2.02",
                "√ó",
                "10",
                "9",
                "2.02",
                "superscript",
                "10",
                "9",
                "2.02\\times 10^{9}",
                " FLOPs per second per watt.\nTherefore, we believe that our proposed PCA-WFL and PCA-AWFL algorithms are suitable to be deployed at the off-the-shelf edge terminals in real-life applications."
            ]
        ]
    }
}