{
    "id_table_1": {
        "caption": "Table 1 :  The diffusion model configurations.",
        "table": "S5.T1.4.1",
        "footnotes": [],
        "references": [
            "Most existing dataset distillation approaches  [ 29 ,  28 ,  2 ,  23 ,  18 ,  11 ,  5 ,  31 ]  focused on various matching algorithms and achieved reasonable performance as can be seen in the upper half of Fig  1 . The computation cost may increase exponentially when the number of images per class increase. Furthermore, the quality of synthetic images is often not human-readable. The challenges of reducing computational costs and increasing the image quality are still significant.",
            "To address the problem of generating human-readable distilled synthetic images, we propose using class conditional diffusion models as can be seen in the lower half of Fig  1 . Li et al.  [ 10 ]  applies a diffusion model for dataset distillation of medical images. We propose that this approach could also be beneficial when training on natural images. The quality of these synthetic images relies on the effectiveness of diffusion models. One positive aspect is that many diffusion models can generate realistic synthetic images, such as UViT  [ 1 ]  and GAN-based models  [ 19 ] . Besides generating images with good quality in different class, the diffusion models are capable of capturing the image distribution of the training dataset. Also, we only need to train the diffusion models once, then generate as many synthetic images as needed in advance. This property can significantly reduce the computational cost.",
            "In training, we use the AdamW optimizer  [ 15 ]  with a weight decay of 0.03 for all datasets and betas are set to  ( 0.99 , 0.999 ) 0.99 0.999 (0.99,0.999) ( 0.99 , 0.999 ) . The learning rate is  0.0002 0.0002 0.0002 0.0002 . The batch size is  1024 1024 1024 1024 . In inference, we sample  50 , 000 50 000 50,000 50 , 000  images and calculate the FID every  500 500 500 500  epochs. When training on the CIFAR100 dataset, we initialize the small UViT  [ 1 ]  model by the pretrained CIFAR10 weights excluding the final label layer and finetuning the model for  80 , 000 80 000 80,000 80 , 000  epochs. When training on the TinyImageNet dataset, we initialize the middle UViT  [ 1 ]  model by the pretrained ImageNet weights excluding the final label layer and finetuning the model for  117 , 000 117 000 117,000 117 , 000  epoches. We do not use AutoEncoder to generate latent features because the image size is so small. The diffusion model configurations are listed in Table  1"
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  The UViT performance.",
        "table": "S5.T2.2.2",
        "footnotes": [],
        "references": [
            "The UViT model  [ 1 ]  is designed to leverage the strengths of both Vision Transformers and U-Net architectures  [ 16 ] , providing a robust backbone for diffusion-based generative tasks as shown in Figure  2 . Our UViT consists of the following components:",
            "We train the diffusion models UViT on CIFAR100 and TinyImageNet datasets respectively and the performances are shown in Table  2 . We use FID as the evaluation metric for synthetic image quality and present the FID scores at every  1000 1000 1000 1000  training iterations in Fig  3 ."
        ]
    }
}