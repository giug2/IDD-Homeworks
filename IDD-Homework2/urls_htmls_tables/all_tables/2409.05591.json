{
    "id_table_1": {
        "caption": "Table 1:  Case for Information Seeking with Implicit Query.",
        "table": "S2.T1.1",
        "footnotes": [],
        "references": [
            "To address the above challenge, we propose a novel framework called  MemoRAG , as shown in Figure  1 . This framework introduces an smart interface that connects tasks with the relevant knowledge from a database. For each presented task, MemoRAG prompts its memory module to generate retrieval clues. These clues are essentially drafted answers based on a compressed representation of the database, i.e. the memory. Despite the possible existence of false details, the clues explicitly reveal the underlying information needs for the presented task. Furthermore, they can also correspond directly to the source information in reality. By using these clues as queries, MemoRAG can effectively retrieve the necessary knowledge from the database.",
            "In Table  1 , we illustrate how MemoRAG handles implicit queries. For instance, the input query How does the book convey the theme of love? lacks a direct semantic connection with the content in the relevant database, as the theme of love is not explicitly stated in the raw text. MemoRAG forms a global memory across the entire database and generates key answer clues that facilitate the retrieval of relevant content from the database."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Case for Information Seeking with Distributed Evidence Query.",
        "table": "S2.T2.1",
        "footnotes": [],
        "references": [
            "In MemoRAG, the global memory  X m superscript X m \\bm{{\\mathcal{X}}}^{m} bold_caligraphic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  is used to generate task-specific clues  y y y italic_y . These clues help outline the expected answers  Y Y {\\mathcal{Y}} caligraphic_Y , effectively bridging the gap between the raw input context and the ground-truth answer. Based on these memory-generated clues, any stand-alone retriever can be employed to locate the precise evidence text within the input sequence, as defined in Eq. ( 2 ).",
            "In Table  2 , we demonstrate how MemoRAG handles such a query. For instance, the input query Which year had the peak revenue in the past three years? requires analyzing financial data across multiple years. MemoRAG forms a global memory from the past ten years financial reports of a large company, reformulates the multi-hop query into several specific query, and integrates this information to determine the peak revenue year."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Case for Information Aggregation.",
        "table": "S2.T3.1",
        "footnotes": [],
        "references": [
            "In Table  3 , we illustrate how MemoRAG handles an information aggregation task. For instance, the task is to summarize a government report on city construction. MemoRAG first extracts key points from the report, such as infrastructure development, budget allocations, and future planning, then retrieve detailed content, and aggregates these information to produce a comprehensive summary of the report."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Case for Personalized Assistant.",
        "table": "S2.T4.1",
        "footnotes": [],
        "references": [
            "During the attentive interactions defined by Eq. ( 4 ), we initialize another set of weight matrices  W Q m subscript W superscript Q m {\\bm{W}}_{{\\mathcal{Q}}^{m}} bold_italic_W start_POSTSUBSCRIPT caligraphic_Q start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ,  W K m subscript W superscript K m {\\bm{W}}_{{\\mathcal{K}}^{m}} bold_italic_W start_POSTSUBSCRIPT caligraphic_K start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  and  W V m subscript W superscript V m {\\bm{W}}_{{\\mathcal{V}}^{m}} bold_italic_W start_POSTSUBSCRIPT caligraphic_V start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  on the special purpose of memory formation. Therefore, we have:",
            "In Table  4 , we demonstrate how MemoRAG handles a personalized recommendation query. For example, when asked, Can you recommend a song for me? MemoRAG analyzes the dialogue history of the user, identifying preferences for certain music genres, artists, or eras, and uses this information to suggest a song that fits the users profile."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Case for Conversational Search with Omitted Semantics.",
        "table": "S2.T5.1",
        "footnotes": [],
        "references": [
            "In Table  5 , we illustrate how MemoRAG handles a query with omitted semantics in a conversational search context. By drawing on the full conversational history, MemoRAG ensures accurate and contextually appropriate responses."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Main experiment results on  UltraDomain  using Mistral-7B-Instruct-v0.2-32K as the generator. The evaluation metric is the F1-score, with the best results highlighted in bold and the second-best results underlined. The upward arrow    \\uparrow   indicates the improvement over the second-best results.  ave  ( | C | ) ave C \\text{ave}(|{\\mathcal{C}}|) ave ( | caligraphic_C | )  refers to the average context length, counted in thousands of tokens (K).",
        "table": "S3.T6.28",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": "Table 7:  Main experiment results. The best results of each block are in bold. The evaluation metrics for all dataset would be introduced in Appendix  A . The Memory Model used in these experiments is trained based on Mistral-7B-Instruct-v0.2-32K, which is available at  HuggingFace .",
        "table": "S3.T7.1",
        "footnotes": [],
        "references": [
            "where  Q m superscript Q m {\\mathcal{Q}}^{m} caligraphic_Q start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ,  K m superscript K m {\\mathcal{K}}^{m} caligraphic_K start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT , and  V m superscript V m {\\mathcal{V}}^{m} caligraphic_V start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  refer to the query, key, and value for the memory tokens  x m superscript x m x^{m} italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT .  K cache m subscript superscript K m cache {\\mathcal{K}}^{m}_{\\text{cache}} caligraphic_K start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT cache end_POSTSUBSCRIPT  and  V cache m subscript superscript V m cache {\\mathcal{V}}^{m}_{\\text{cache}} caligraphic_V start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT cache end_POSTSUBSCRIPT  refer to the KV cache of previous memory tokens. We denote the memory tokens as  X m superscript X m \\bm{{\\mathcal{X}}}^{m} bold_caligraphic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  and the conversion process as  X m =  mem  ( X ) superscript X m subscript  mem X \\bm{{\\mathcal{X}}}^{m}=\\Theta_{\\text{mem}}(\\bm{{\\mathcal{X}}}) bold_caligraphic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT = roman_ start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT ( bold_caligraphic_X ) . For  l l l italic_l  raw tokens  { x 1 ,  , x l } subscript x 1  subscript x l \\{x_{1},\\cdots,x_{l}\\} { italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT } , after multiple attentive processes defined by Eq. ( 7 ), they are encoded into hidden states  X [ 0 : l ] = { x 1 ,  , x l , x 1 m ,  , x k m } subscript X delimited-[] : 0 l subscript x 1  subscript x l subscript superscript x m 1  subscript superscript x m k \\bm{{\\mathcal{X}}}_{[0:l]}=\\{\\bm{x}_{1},\\cdots,\\bm{x}_{l},\\bm{x}^{m}_{1},%  \\cdots,\\bm{x}^{m}_{k}\\} bold_caligraphic_X start_POSTSUBSCRIPT [ 0 : italic_l ] end_POSTSUBSCRIPT = { bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , bold_italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , bold_italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , bold_italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT } , where  { x 1 ,  , x l } subscript x 1  subscript x l \\{\\bm{x}_{1},\\cdots,\\bm{x}_{l}\\} { bold_italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , bold_italic_x start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT }  represent the raw tokens hidden states and  { x 1 m ,  , x k m } subscript superscript x m 1  subscript superscript x m k \\{\\bm{x}^{m}_{1},\\cdots,\\bm{x}^{m}_{k}\\} { bold_italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ,  , bold_italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT }  represent the memory tokens hidden states. After the formation of memory, the KV cache of the  l l l italic_l  raw tokens is discarded, similar to the forgetting process in human memory.  After  n n n italic_n  context windows, MemoRAG progressively converts all raw tokens in  X X {\\mathcal{X}} caligraphic_X  into memory tokens. Thus, we have    ( X )   mem  ( X ) = { x 1 , 1 m ,  , x 1 , k m ,  , x n , k m }   X subscript  mem X subscript superscript x m 1 1  subscript superscript x m 1 k  subscript superscript x m n k \\Theta({\\mathcal{X}})\\rightarrow\\Theta_{\\text{mem}}(\\bm{{\\mathcal{X}}})=\\{\\bm{%  x}^{m}_{1,1},\\cdots,\\bm{x}^{m}_{1,k},\\cdots,\\bm{x}^{m}_{n,k}\\} roman_ ( caligraphic_X )  roman_ start_POSTSUBSCRIPT mem end_POSTSUBSCRIPT ( bold_caligraphic_X ) = { bold_italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 , 1 end_POSTSUBSCRIPT ,  , bold_italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 , italic_k end_POSTSUBSCRIPT ,  , bold_italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_n , italic_k end_POSTSUBSCRIPT } , which represents the global memory  X m superscript X m \\bm{{\\mathcal{X}}}^{m} bold_caligraphic_X start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT  formed from the input  X X {\\mathcal{X}} caligraphic_X .",
            "As mentioned above, we initialize another set of weight matrices  W Q m subscript W superscript Q m {\\bm{W}}_{{\\mathcal{Q}}^{m}} bold_italic_W start_POSTSUBSCRIPT caligraphic_Q start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ,  W K m subscript W superscript K m {\\bm{W}}_{{\\mathcal{K}}^{m}} bold_italic_W start_POSTSUBSCRIPT caligraphic_K start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT , and  W V m subscript W superscript V m {\\bm{W}}_{{\\mathcal{V}}^{m}} bold_italic_W start_POSTSUBSCRIPT caligraphic_V start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT end_POSTSUBSCRIPT  for the special purpose of mapping the memory tokens  x m subscript x m x_{m} italic_x start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT  into query, key, and value vectors (as formulated in Eq.  7 ). The newly initialized weight matrices are updated during the training process, while the parameters of the underlying LLM remain frozen.",
            "Table  7  shows the experiment results on three benchmarks, from which we can conclude that MemoRAG generally surpasses all baselines on all datasets, except for a single outlier.   First , for open-domain QA tasks, MemoRAG improves performance over all baselines on all datasets except for en.qa with Llama3 as generator. This verifies that in the comfortable zone of standard RAG, where most queries have explicit information needs, MemoRAG can better locate the expected evidence within the original context, thanks to the memory-generated clues.   Second , most previous RAG methods struggle with tasks that do not involve queries, such as summarization tasks (e.g., MultiNews, GovReport, and en.sum) 3 3 3 In the experiments, we use the summarization instruct as a fake query to facilitate standard RAG to perform summarization. . Our MemoRAG enables the RAG system to generate key points from the input context and retrieve more details to form a comprehensive summary.   Third , for domain-specific tasks (e.g., Financial and Legal), MemoRAG shows significant improvement, indicating MemoRAGs superiority in handling complex tasks with long contexts.   In summary , the results demonstrate that MemoRAG significantly enhances performance over standard RAG methods and other baselines across various datasets and query types. MemoRAGs ability to handle complex and long-context tasks effectively highlights its advantages, particularly in scenarios where standard RAG systems struggle. This consistency across different generators underscores the robustness and general applicability of MemoRAG."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Statistical information of the datasets utilized in this paper.",
        "table": "A1.T8.1",
        "footnotes": [],
        "references": [
            "The objective in Eq. ( 8 ) aims to maximize the generation probability of the next token given the KV cache of the previous memory tokens  { x 1 , 1 m ,  , x i  1 , k i  1 m } subscript superscript x m 1 1  subscript superscript x m i 1 subscript k i 1 \\{x^{m}_{1,1},\\cdots,x^{m}_{i-1,k_{i-1}}\\} { italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 1 , 1 end_POSTSUBSCRIPT ,  , italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i - 1 , italic_k start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT }  and recent raw tokens  { x i , 1 ,  , x i , j  1 } subscript x i 1  subscript x i j 1 \\{x_{i,1},\\cdots,x_{i,j-1}\\} { italic_x start_POSTSUBSCRIPT italic_i , 1 end_POSTSUBSCRIPT ,  , italic_x start_POSTSUBSCRIPT italic_i , italic_j - 1 end_POSTSUBSCRIPT } .",
            "The statistical details of the specialized datasets are provided in Table  8 , while the details of the textbook-based datasets are shown in Table  LABEL:tab:outdomain . Together, these datasets form a comprehensive benchmark that rigorously tests MemoRAGs effectiveness in handling both domain-specific challenges and broader, cross-disciplinary tasks.",
            "For the evaluation dataset, we use a separate set of contexts from the aforementioned datasets as the evaluation context. We follow the same process to generate QA pairs, creating the in-domain evaluation datasets: the Fin dataset (from the financial reports), the Leg dataset (from the legal contracts), and the Mix dataset (from NarrativeQA, Qasper, and HotpotQA). Additionally, we collect college textbooks from 18 domains from  this repository  to construct the out-of-domain evaluation dataset, following the same process. The statistical information of the in-domain datasets is shown in Table  8 , and the statistical information of the out-of-domain datasets is shown in Table  LABEL:tab:outdomain ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Statistical information of the out-of-domain evaluation datasets utilized in this paper.",
        "table": "A1.T9.5",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "In the experiments, we use the summarization instruct as a fake query to facilitate standard RAG to perform summarization."
    ]
}