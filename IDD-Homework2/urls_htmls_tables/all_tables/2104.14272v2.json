{
    "id_table_1": {
        "caption": " A summary of advantages and limitations of various deep learning-based table detection methods that are based on object detection frameworks.",
        "table": "<table id=\"S3.T1.11\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T1.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T1.1.1.1\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T1.1.1.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.1.1.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.1.1.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.1.1.1.1.p1.1.1\" class=\"ltx_text ltx_font_bold\">Literature</span></span>\n</span></span></th>\n<th id=\"S3.T1.1.1.2\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></span>\n</span>\n</th>\n<th id=\"S3.T1.1.1.3\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Highlights</span></span>\n</span>\n</th>\n<th id=\"S3.T1.1.1.4\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S3.T1.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.1.1.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">Limitations</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.2.2.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T1.2.2.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.2.2.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.2.2.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.2.2.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Gelani et al.<span id=\"S3.T1.2.2.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">44</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.2.2.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.2.2.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.2.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN (Section <a href=\"#S3.SS1.SSS1.Px2\" title=\"Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>).</span></span>\n<span id=\"S3.T1.2.2.2.1.2\" class=\"ltx_p\"><span id=\"S3.T1.2.2.2.1.2.1\" class=\"ltx_text\" style=\"font-size:80%;\">Images are transformed and then fed into the Faster R-CNN.</span></span>\n</span>\n</td>\n<td id=\"S3.T1.2.2.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.2.2.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.2.2.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T1.2.2.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> First deep learning based table detection approach on scanned document images, </span>b)<span id=\"S3.T1.2.2.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> Transforming RGB pixels to distance metrics facilitates the object detection algorithm.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T1.2.2.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.2.2.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Extra pre-processing steps involved.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.3.3.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T1.3.3.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.3.3.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.3.3.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.3.3.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">DeCNT<span id=\"S3.T1.3.3.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib46\" title=\"\" class=\"ltx_ref\">46</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.3.3.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.3.3.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.3.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Deformable convolutions implemented in the Faster R-CNN architecture (Section <a href=\"#S3.SS1.SSS1.Px3\" title=\"Deformable Convolutions ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T1.3.3.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.3.3.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.3.3.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The dynamic receptive field of deformable convolutional neural networks help in recognizing various tabular boundaries.</span></span>\n</span>\n</td>\n<td id=\"S3.T1.3.3.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.3.3.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.3.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Deformable convolutions are computationally intensive as compared to traditional convolutions.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.4.4.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T1.4.4.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.4.4.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.4.4.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.4.4.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">DeepDeSRT<span id=\"S3.T1.4.4.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib45\" title=\"\" class=\"ltx_ref\">45</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.4.4.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.4.4.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.4.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN with transfer learning techniques (Section <a href=\"#S3.SS1.SSS1.Px2\" title=\"Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>)</span></span>\n</span>\n</td>\n<td id=\"S3.T1.4.4.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.4.4.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.4.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Simple and effective end-to-end approach to detect tables and structures of the tables.</span></span>\n</span>\n</td>\n<td id=\"S3.T1.4.4.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.4.4.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.4.4.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Not as accurate as compared to other states of the art approaches.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.5.5.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T1.5.5.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.5.5.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.5.5.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.5.5.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">TableBank<span id=\"S3.T1.5.5.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib62\" title=\"\" class=\"ltx_ref\">62</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.5.5.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.5.5.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.5.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN used as a baseline method for a novel dataset (Section <a href=\"#S3.SS1.SSS1.Px2\" title=\"Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T1.5.5.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.5.5.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.5.5.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">This approach presents that by leveraging a large dataset such as TableBank, a simple Faster R-CNN can produce impressive results.</span></span>\n</span>\n</td>\n<td id=\"S3.T1.5.5.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.5.5.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.5.5.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Just a direct application of Faster R-CNN.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.6.6.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T1.6.6.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.6.6.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.6.6.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.6.6.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Sun et al.<span id=\"S3.T1.6.6.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib63\" title=\"\" class=\"ltx_ref\">63</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.6.6.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.6.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.6.6.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.6.6.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Faster R-CNN with locating corners (Section <a href=\"#S3.SS1.SSS1.Px2\" title=\"Faster R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T1.6.6.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.6.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.6.6.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.6.6.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T1.6.6.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> Faster R-CNN is exploited to detect not only tables but the corners of the tabular boundaries as well, </span>b)<span id=\"S3.T1.6.6.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> Novel method produces better results.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T1.6.6.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.6.6.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.6.6.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T1.6.6.4.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> Computationally more extensive because of additional detections, </span>b)<span id=\"S3.T1.6.6.4.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> Post-processing steps such as corners’ refinement are required.</span></span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.7.7.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T1.7.7.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.7.7.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.7.7.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.7.7.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Huang et al.<span id=\"S3.T1.7.7.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib47\" title=\"\" class=\"ltx_ref\">47</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.7.7.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.7.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.7.7.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.7.7.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">YOLO based table detection method (Section <a href=\"#S3.SS1.SSS1.Px4\" title=\"YOLO ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T1.7.7.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.7.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.7.7.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.7.7.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Comparatively, faster and efficient approach.</span></span>\n</span>\n</td>\n<td id=\"S3.T1.7.7.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.7.7.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.7.7.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.7.7.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The proposed method depends on the data driven post-processing techniques.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T1.8.8.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T1.8.8.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.8.8.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.8.8.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.8.8.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">García et al.<span id=\"S3.T1.8.8.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib72\" title=\"\" class=\"ltx_ref\">72</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.8.8.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.8.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.8.8.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.8.8.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Employed Mask R-CNN, YOLO, SSD and RetinaNet to compare fine-tuning techniques (Section <a href=\"#S3.SS1.SSS1.Px5\" title=\"Mask R-CNN, YOLO, SSD and Retina Net ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T1.8.8.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.8.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.8.8.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.8.8.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Presented the benefits of leveraging a closer domain fine-tuning methods for table detection while employing object detection networks.</span></span>\n</span>\n</td>\n<td id=\"S3.T1.8.8.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.8.8.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.8.8.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.8.8.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Still, closed domain fine-tuning is not enough to reach the state-of-the-art results.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T1.9.9.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T1.9.9.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.9.9.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.9.9.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.9.9.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">CascadeTabNet<span id=\"S3.T1.9.9.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">48</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.9.9.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.9.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.9.9.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.9.9.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Employed Cascade Mask R-CNN with an iterative transfer learning approach (Section <a href=\"#S3.SS1.SSS1.Px6\" title=\"Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T1.9.9.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.9.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.9.9.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.9.9.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">This work presents that transformed images with an iterative transfer learning can reduce the dependency of large-scale datasets.</span></span>\n</span>\n</td>\n<td id=\"S3.T1.9.9.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.9.9.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.9.9.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.9.9.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Similar to <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib44\" title=\"\" class=\"ltx_ref\">44</a>]</cite>, extra pre-processing steps are involved in this approach.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T1.10.10.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T1.10.10.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.10.10.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.10.10.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.10.10.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">CDeC-Net<span id=\"S3.T1.10.10.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib49\" title=\"\" class=\"ltx_ref\">49</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.10.10.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.10.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.10.10.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.10.10.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Cascade Mask R-CNN with a deformable composite backbone (Section <a href=\"#S3.SS1.SSS1.Px3\" title=\"Deformable Convolutions ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T1.10.10.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.10.10.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.10.10.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.10.10.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T1.10.10.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> Extensive evaluations on publicly available benchmark datasets for table detection. </span>b)<span id=\"S3.T1.10.10.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> An end-to-end object detection-based framework leveraging composite backbone to produce state-of-the-art results.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T1.10.10.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T1.10.10.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.10.10.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.10.10.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Along with the deformable convolutions, a composite backbone is employed which makes the approach computationally intensive.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T1.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T1.11.11.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S3.T1.11.11.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T1.11.11.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T1.11.11.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T1.11.11.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">GTE<span id=\"S3.T1.11.11.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T1.11.11.2\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.11.11.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.11.11.2.1.1\" class=\"ltx_p\"><span id=\"S3.T1.11.11.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Proposed a generic object detection approach (Section <a href=\"#S3.SS1.SSS1.Px6\" title=\"Cascade Mask R-CNN ‣ III-A1 Object Detection Algorithms ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T1.11.11.3\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T1.11.11.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.11.11.3.1.1\" class=\"ltx_p\"><span id=\"S3.T1.11.11.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a<span id=\"S3.T1.11.11.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\">) An end-to-end technique that can operate on any object detection framework. </span>b<span id=\"S3.T1.11.11.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\">) This work proposed an additional piece-wise constraint loss that benefits the task of table detection.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T1.11.11.4\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S3.T1.11.11.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T1.11.11.4.1.1\" class=\"ltx_p\"><span id=\"S3.T1.11.11.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Since the task of table detection is dependent on cell detections, annotations for cellular boundaries are required.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_2": {
        "caption": "  A summary of advantages and limitations of various table detection methods that operate on other deep learning-based concepts. The bold horizontal line separates the approaches with different architectures. ",
        "table": "<table id=\"S3.T2.6\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T2.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T2.1.1.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T2.1.1.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T2.1.1.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T2.1.1.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T2.1.1.1.1.p1.1.1\" class=\"ltx_text ltx_font_bold\">Literature</span></span>\n</span></span></td>\n<td id=\"S3.T2.1.1.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.2.1.1\" class=\"ltx_p\"><span id=\"S3.T2.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.3.1.1\" class=\"ltx_p\"><span id=\"S3.T2.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Highlights</span></span>\n</span>\n</td>\n<td id=\"S3.T2.1.1.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.1.1.4.1.1\" class=\"ltx_p\"><span id=\"S3.T2.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">Limitations</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T2.2.2.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T2.2.2.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T2.2.2.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T2.2.2.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T2.2.2.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Kavasidis et al.<span id=\"S3.T2.2.2.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib81\" title=\"\" class=\"ltx_ref\">81</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T2.2.2.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.2.2.1.1\" class=\"ltx_p\"><span id=\"S3.T2.2.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Semantic Image Segmentation with saliency concepts (Section <a href=\"#S3.SS1.SSS2\" title=\"III-A2 Semantic Image Segmentation ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>2</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.2.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.2.3.1.1\" class=\"ltx_p\"><span id=\"S3.T2.2.2.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T2.2.2.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> This method poses the task of table detection as saliency detection, </span>b)<span id=\"S3.T2.2.2.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> Dilated convolutions are applied instead of traditional convolutions.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T2.2.2.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.2.2.4.1.1\" class=\"ltx_p\"><span id=\"S3.T2.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Multiple processing steps are required to achieve comparable results.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T2.3.3.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T2.3.3.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T2.3.3.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T2.3.3.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T2.3.3.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">TableNet<span id=\"S3.T2.3.3.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib84\" title=\"\" class=\"ltx_ref\">84</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T2.3.3.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.3.3.2.1.1\" class=\"ltx_p\"><span id=\"S3.T2.3.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Fully Convolutional Networks (Section <a href=\"#S3.SS1.SSS2.Px1\" title=\"Fully Convolutional Networks ‣ III-A2 Semantic Image Segmentation ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>2</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T2.3.3.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.3.3.3.1.1\" class=\"ltx_p\"><span id=\"S3.T2.3.3.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T2.3.3.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> An end-to-end approach for table detection and structure recognition in document images, </span>b)<span id=\"S3.T2.3.3.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> First approach to jointly address the task of table detection and structure recognition with a single method.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T2.3.3.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.3.3.4.1.1\" class=\"ltx_p\"><span id=\"S3.T2.3.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">In the case of table structural extraction, this technique only works on column detection.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T2.4.4.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T2.4.4.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T2.4.4.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T2.4.4.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T2.4.4.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Martin et al.<span id=\"S3.T2.4.4.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib86\" title=\"\" class=\"ltx_ref\">86</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T2.4.4.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.4.4.2.1.1\" class=\"ltx_p\"><span id=\"S3.T2.4.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Graph Neural Network with the line item detection approach. (Section <a href=\"#S3.SS1.SSS3\" title=\"III-A3 Graph Neural Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>3</span></a>)</span></span>\n</span>\n</td>\n<td id=\"S3.T2.4.4.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.4.4.3.1.1\" class=\"ltx_p\"><span id=\"S3.T2.4.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The method shows promising results on the layout-heavy documents such as invoices.</span></span>\n</span>\n</td>\n<td id=\"S3.T2.4.4.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.4.4.4.1.1\" class=\"ltx_p\"><span id=\"S3.T2.4.4.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T2.4.4.4.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> Approach is not evaluated on any publicly available table datasets, </span>b)<span id=\"S3.T2.4.4.4.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> Weak baseline method and no comparisons with other state-of-the-art methods.</span></span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T2.5.5.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T2.5.5.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T2.5.5.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T2.5.5.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T2.5.5.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Riba et al.<span id=\"S3.T2.5.5.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib87\" title=\"\" class=\"ltx_ref\">87</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T2.5.5.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.5.5.2.1.1\" class=\"ltx_p\"><span id=\"S3.T2.5.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Graph Neural Network by leveraging textual attributes through OCR (Section <a href=\"#S3.SS1.SSS3\" title=\"III-A3 Graph Neural Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>3</span></a>)</span></span>\n</span>\n</td>\n<td id=\"S3.T2.5.5.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T2.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.5.5.3.1.1\" class=\"ltx_p\"><span id=\"S3.T2.5.5.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The proposed method leverages more information than just the spatial features.</span></span>\n</span>\n</td>\n<td id=\"S3.T2.5.5.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T2.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.5.5.4.1.1\" class=\"ltx_p\"><span id=\"S3.T2.5.5.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T2.5.5.4.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> This method requires extra annotations apart from the information of tabular area, </span>b)<span id=\"S3.T2.5.5.4.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> No comparisons with other state-of-the-art approaches.</span></span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T2.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T2.6.6.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt\"><span id=\"S3.T2.6.6.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T2.6.6.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T2.6.6.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T2.6.6.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Li et al.<span id=\"S3.T2.6.6.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib88\" title=\"\" class=\"ltx_ref\">88</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T2.6.6.2\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt\">\n<span id=\"S3.T2.6.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.6.6.2.1.1\" class=\"ltx_p\"><span id=\"S3.T2.6.6.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Generative Adversarial Networks and object detection network (Section <a href=\"#S3.SS1.SSS4\" title=\"III-A4 Generative Adversarial Networks ‣ III-A Table Detection ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-A</span>4</span></a>)</span></span>\n</span>\n</td>\n<td id=\"S3.T2.6.6.3\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_tt\">\n<span id=\"S3.T2.6.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.6.6.3.1.1\" class=\"ltx_p\"><span id=\"S3.T2.6.6.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">GAN based approach forces the network to extract similar features for ruling and less-ruled tables.</span></span>\n</span>\n</td>\n<td id=\"S3.T2.6.6.4\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_tt\">\n<span id=\"S3.T2.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T2.6.6.4.1.1\" class=\"ltx_p\"><span id=\"S3.T2.6.6.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Model with generators is vulnerable in document images having diverse tabular layouts.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_3": {
        "caption": ": A summary of advantages and limitations of various deep learning-based methods that have worked on the task of table structure recognition. The bold horizontal line separates the approaches with different architectures.",
        "table": "<table id=\"S3.T3.12\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T3.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T3.1.1.1\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T3.1.1.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.1.1.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.1.1.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.1.1.1.1.p1.1.1\" class=\"ltx_text ltx_font_bold\">Literature</span></span>\n</span></span></th>\n<th id=\"S3.T3.1.1.2\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></span>\n</span>\n</th>\n<th id=\"S3.T3.1.1.3\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Highlights</span></span>\n</span>\n</th>\n<th id=\"S3.T3.1.1.4\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S3.T3.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.1.1.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">Limitations</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T3.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T3.2.2.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.2.2.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.2.2.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.2.2.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.2.2.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Siddiqui et al.<span id=\"S3.T3.2.2.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">50</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.2.2.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.2.2.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.2.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Deformable Convolution with Faster R-CNN (Section <a href=\"#S3.SS2.SSS4\" title=\"III-B4 Deformable and Dilated Convolutions ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>4</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.2.2.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.2.2.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.2.2.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T3.2.2.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> Published another dataset having structural information of tables. </span>b)<span id=\"S3.T3.2.2.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> Deformable convolution allows tackling varied tabular structures.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T3.2.2.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.2.2.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The published work will not work well in case of row/column span in the tables.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T3.3.3.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.3.3.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.3.3.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.3.3.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.3.3.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">CascadeTabNet<span id=\"S3.T3.3.3.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib48\" title=\"\" class=\"ltx_ref\">48</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.3.3.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.3.3.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.3.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Cascade Mask R-CNN with HRNet as a backbone network (Section <a href=\"#S3.SS2.SSS5\" title=\"III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>5</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.3.3.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.3.3.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.3.3.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">An end-to-end approach to directly regress cellular boundaries.</span></span>\n</span>\n</td>\n<td id=\"S3.T3.3.3.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.3.3.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.3.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">An extra post-processing is required to filter tables (with and without) ruling lines.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T3.4.4.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.4.4.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.4.4.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.4.4.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.4.4.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">GTE<span id=\"S3.T3.4.4.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib52\" title=\"\" class=\"ltx_ref\">52</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.4.4.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.4.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.4.4.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.4.4.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Generic object detection approach (Section <a href=\"#S3.SS2.SSS5\" title=\"III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>5</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.4.4.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.4.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.4.4.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.4.4.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">An hierarchical network with an additional novel cluster-based method to recognize tabular structures.</span></span>\n</span>\n</td>\n<td id=\"S3.T3.4.4.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.4.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.4.4.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.4.4.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Final cell structure recognition is conditioned on the precise classification of a table (Graphical ruling lines present or not present).</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T3.5.5.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.5.5.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.5.5.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.5.5.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.5.5.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Hashmi et al.<span id=\"S3.T3.5.5.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib51\" title=\"\" class=\"ltx_ref\">51</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.5.5.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.5.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.5.5.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.5.5.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Mask R-CNN with an Anchor optimization method (Section <a href=\"#S3.SS2.SSS5\" title=\"III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>5</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.5.5.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.5.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.5.5.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.5.5.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Optimized anchors help region proposal networks to converge faster and better.</span></span>\n</span>\n</td>\n<td id=\"S3.T3.5.5.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.5.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.5.5.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.5.5.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">This work depends on the initial pre-processing step of clustering the ground-truth to retrieve suitable anchors.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T3.6.6.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.6.6.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.6.6.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.6.6.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.6.6.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Raja et al.<span id=\"S3.T3.6.6.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib53\" title=\"\" class=\"ltx_ref\">53</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.6.6.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.6.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.6.6.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.6.6.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Mask R-CNN with ResNet-101 as a backbone network (Section <a href=\"#S3.SS2.SSS5\" title=\"III-B5 Object Detection Algorithms ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>5</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.6.6.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.6.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.6.6.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.6.6.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T3.6.6.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> A trainable combination of top-down (cell detection) and bottom-up (structure recognition) is presented. </span>b)<span id=\"S3.T3.6.6.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> An additional alignment loss is proposed to detect cells accurately.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T3.6.6.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.6.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.6.6.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.6.6.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The approach is vulnerable in the case of empty cells.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T3.7.7.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.7.7.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.7.7.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.7.7.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.7.7.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Siddiqui et al.<span id=\"S3.T3.7.7.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib92\" title=\"\" class=\"ltx_ref\">92</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.7.7.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.7.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.7.7.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.7.7.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Fully Convolutional Networks (Section <a href=\"#S3.SS2.SSS1.Px1\" title=\"Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.7.7.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.7.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.7.7.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.7.7.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The proposed Prediction tiling technique minimizes the complexity of the problem of table structure recognition.</span></span>\n</span>\n</td>\n<td id=\"S3.T3.7.7.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.7.7.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.7.7.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.7.7.4.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T3.7.7.4.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> The method relies on the consistency assumption of tabular structures, </span>b)<span id=\"S3.T3.7.7.4.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> In case of overly-segmented rows/columns, extra post-processing steps are required.</span></span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.8.8\" class=\"ltx_tr\">\n<td id=\"S3.T3.8.8.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.8.8.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.8.8.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.8.8.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.8.8.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Tensmeyer et al.<span id=\"S3.T3.8.8.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib101\" title=\"\" class=\"ltx_ref\">101</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.8.8.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.8.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.8.8.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.8.8.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Dilated Convolutions in Fully Convolutional Networks (Section <a href=\"#S3.SS2.SSS4.Px2\" title=\"Dilated Convolutions ‣ III-B4 Deformable and Dilated Convolutions ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>4</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.8.8.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.8.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.8.8.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.8.8.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The system works well both on PDF and scanned document images.</span></span>\n</span>\n</td>\n<td id=\"S3.T3.8.8.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.8.8.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.8.8.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.8.8.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The merging part of the approach is depends on the post-processing heuristics.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.9.9\" class=\"ltx_tr\">\n<td id=\"S3.T3.9.9.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.9.9.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.9.9.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.9.9.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.9.9.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Zou et al.<span id=\"S3.T3.9.9.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib94\" title=\"\" class=\"ltx_ref\">94</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.9.9.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.9.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.9.9.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.9.9.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Fully Convolutional Networks (Section <a href=\"#S3.SS2.SSS1.Px1\" title=\"Fully Convolutional Networks ‣ III-B1 Semantic Image Segmentation ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.9.9.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.9.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.9.9.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.9.9.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T3.9.9.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> Along with segmenting rows and columns, cells are segmented in a table. </span>b)<span id=\"S3.T3.9.9.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> Applying Connected component analysis further improves the results.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T3.9.9.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.9.9.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.9.9.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.9.9.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Handful of post-processing steps involving custom heuristics are required to produce comparative results.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.10.10\" class=\"ltx_tr\">\n<td id=\"S3.T3.10.10.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.10.10.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.10.10.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.10.10.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.10.10.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Qasim et al.<span id=\"S3.T3.10.10.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib96\" title=\"\" class=\"ltx_ref\">96</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.10.10.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.10.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.10.10.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.10.10.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Graph Neural Networks with Convolutional Neural Networks (Section <a href=\"#S3.SS2.SSS2\" title=\"III-B2 Graph Neural Networks ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>2</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.10.10.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.10.10.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.10.10.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.10.10.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T3.10.10.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> The proposed method exploits both the spatial and textual features, </span>b)<span id=\"S3.T3.10.10.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> A novel Monte Carlo based memory efficient training method is also presented in this work.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T3.10.10.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.10.10.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.10.10.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.10.10.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The system is not evaluated on the publicly available table datasets.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.11.11\" class=\"ltx_tr\">\n<td id=\"S3.T3.11.11.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T3.11.11.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.11.11.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.11.11.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.11.11.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Xue et al.<span id=\"S3.T3.11.11.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib99\" title=\"\" class=\"ltx_ref\">99</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.11.11.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.11.11.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.11.11.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.11.11.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Graph Neural Networks with distance based weights (Section <a href=\"#S3.SS2.SSS2.Px1\" title=\"Distance Based Weights ‣ III-B2 Graph Neural Networks ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>2</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.11.11.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.11.11.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.11.11.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.11.11.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The distance-based weight technique resolves the class imbalance problem for the cell relationship network.</span></span>\n</span>\n</td>\n<td id=\"S3.T3.11.11.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T3.11.11.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.11.11.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.11.11.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The method is vulnerable in the case of sparse tables.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T3.12.12\" class=\"ltx_tr\">\n<td id=\"S3.T3.12.12.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S3.T3.12.12.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T3.12.12.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T3.12.12.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T3.12.12.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Khan et al.<span id=\"S3.T3.12.12.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib102\" title=\"\" class=\"ltx_ref\">102</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T3.12.12.2\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.12.12.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.12.12.2.1.1\" class=\"ltx_p\"><span id=\"S3.T3.12.12.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Recurrent Neural Networks (Section <a href=\"#S3.SS2.SSS3\" title=\"III-B3 Recurrent Neural Networks ‣ III-B Table Structural Segmentation ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-B</span>3</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T3.12.12.3\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T3.12.12.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.12.12.3.1.1\" class=\"ltx_p\"><span id=\"S3.T3.12.12.3.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The Bi-directional GRU overcomes the problem of the smaller receptive field of CNNs.</span></span>\n</span>\n</td>\n<td id=\"S3.T3.12.12.4\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S3.T3.12.12.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T3.12.12.4.1.1\" class=\"ltx_p\"><span id=\"S3.T3.12.12.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">A series of pre-processing steps such as binarization, noise removal, and morphological transformation are required.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_4": {
        "caption": " A summary of advantages and limitations of deep learning-based methods that have solely worked on the task of table recognition on scanned document images.",
        "table": "<table id=\"S3.T4.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S3.T4.1.1\" class=\"ltx_tr\">\n<th id=\"S3.T4.1.1.1\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\"><span id=\"S3.T4.1.1.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T4.1.1.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T4.1.1.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T4.1.1.1.1.p1.1.1\" class=\"ltx_text ltx_font_bold\">Literature</span></span>\n</span></span></th>\n<th id=\"S3.T4.1.1.2\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.1.2.1.1\" class=\"ltx_p\"><span id=\"S3.T4.1.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">Method</span></span>\n</span>\n</th>\n<th id=\"S3.T4.1.1.3\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.1.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.1.3.1.1\" class=\"ltx_p\"><span id=\"S3.T4.1.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">Highlights</span></span>\n</span>\n</th>\n<th id=\"S3.T4.1.1.4\" class=\"ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_t\">\n<span id=\"S3.T4.1.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.1.1.4.1.1\" class=\"ltx_p\"><span id=\"S3.T4.1.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">Limitations</span></span>\n</span>\n</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T4.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T4.2.2.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\"><span id=\"S3.T4.2.2.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T4.2.2.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T4.2.2.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T4.2.2.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Zhong et al.<span id=\"S3.T4.2.2.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T4.2.2.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.2.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.2.2.2.1.1\" class=\"ltx_p\"><span id=\"S3.T4.2.2.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Attention based encoder dual decoder (Section <a href=\"#S3.SS3.SSS1\" title=\"III-C1 Encoder-Dual-Decoder ‣ III-C Table Recognition ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-C</span>1</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T4.2.2.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.2.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.2.2.3.1.1\" class=\"ltx_p\"><span id=\"S3.T4.2.2.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T4.2.2.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> Published a large-scale table dataset, </span>b)<span id=\"S3.T4.2.2.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> The approach presents a novel evaluation metrics <span id=\"S3.T4.2.2.3.1.1.1.2.1\" class=\"ltx_text ltx_font_italic\">TEDS</span> to evaluate table recognition methods.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T4.2.2.4\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S3.T4.2.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.2.2.4.1.1\" class=\"ltx_p\"><span id=\"S3.T4.2.2.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The approach is not directly comparable with other state-of-the-art methods.</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S3.T4.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T4.3.3.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\"><span id=\"S3.T4.3.3.1.1\" class=\"ltx_inline-logical-block ltx_align_top\">\n<span id=\"S3.T4.3.3.1.1.p1\" class=\"ltx_para ltx_noindent\">\n<span id=\"S3.T4.3.3.1.1.p1.1\" class=\"ltx_p\"><span id=\"S3.T4.3.3.1.1.p1.1.1\" class=\"ltx_text ltx_font_italic\" style=\"font-size:80%;\">Deng et al.<span id=\"S3.T4.3.3.1.1.p1.1.1.1\" class=\"ltx_text ltx_font_upright\"> <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib109\" title=\"\" class=\"ltx_ref\">109</a>]</cite></span></span></span>\n</span></span></td>\n<td id=\"S3.T4.3.3.2\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.3.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.3.3.2.1.1\" class=\"ltx_p\"><span id=\"S3.T4.3.3.2.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Encoder decoder network presented as the baseline model (Section <a href=\"#S3.SS3.SSS2\" title=\"III-C2 Encoder Decoder Network ‣ III-C Table Recognition ‣ III METHODOLOGIES ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">III-C</span>2</span></a>).</span></span>\n</span>\n</td>\n<td id=\"S3.T4.3.3.3\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S3.T4.3.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.3.3.3.1.1\" class=\"ltx_p\"><span id=\"S3.T4.3.3.3.1.1.1\" class=\"ltx_text ltx_font_bold\" style=\"font-size:80%;\">a)<span id=\"S3.T4.3.3.3.1.1.1.1\" class=\"ltx_text ltx_font_medium\"> Contributed with another large-scale dataset in the field of table understanding, </span>b)<span id=\"S3.T4.3.3.3.1.1.1.2\" class=\"ltx_text ltx_font_medium\"> Challenges in end-to-end table recognition are discussed in the presented work.</span></span></span>\n</span>\n</td>\n<td id=\"S3.T4.3.3.4\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S3.T4.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S3.T4.3.3.4.1.1\" class=\"ltx_p\"><span id=\"S3.T4.3.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">The proposed baseline method is not evaluated on the other publicly available table recognition datasets.</span></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_5": {
        "caption": "Table Datasets. TD denotes Table Detection, TSR is Table Structure Recognition wheras TR is Table Recognition.",
        "table": "<table id=\"S4.T5.3\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T5.3.4.1\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.4.1.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.4.1.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.4.1.1.1.1.1\" class=\"ltx_text ltx_font_bold\">Dataset</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.4.1.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.4.1.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.2.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.4.1.2.1.1.1\" class=\"ltx_text ltx_font_bold\">TD</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.4.1.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.4.1.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.3.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.4.1.3.1.1.1\" class=\"ltx_text ltx_font_bold\">TSR</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.4.1.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.4.1.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.4.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.4.1.4.1.1.1\" class=\"ltx_text ltx_font_bold\">TR</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.4.1.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.4.1.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.4.1.5.1.1.1\" class=\"ltx_text ltx_font_bold\"># Samples</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.4.1.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.4.1.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.4.1.6.1.1.1\" class=\"ltx_text ltx_font_bold\">Image Type</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.4.1.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.4.1.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.4.1.7.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.4.1.7.1.1.1\" class=\"ltx_text ltx_font_bold\">Location</span></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.5.2\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.5.2.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.5.2.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.5.2.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ICDAR-2013 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib66\" title=\"\" class=\"ltx_ref\">66</a>]</cite> (Section <a href=\"#S4.SS1\" title=\"IV-A ICDAR-2013 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-A</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.5.2.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.5.2.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.5.2.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.5.2.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.3.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.5.2.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.5.2.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.4.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.5.2.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.5.2.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.5.2.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">238</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.5.2.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.5.2.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.5.2.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.5.2.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.5.2.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.5.2.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"http://www.tamirhassan.com/html/dataset.html\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">http://www.tamirhassan.com/html/dataset.html</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.6.3\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.6.3.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.6.3.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.6.3.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ICDAR-2017-POD<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib64\" title=\"\" class=\"ltx_ref\">64</a>]</cite> (Section <a href=\"#S4.SS2\" title=\"IV-B ICDAR-2017-POD ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-B</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.6.3.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.6.3.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.6.3.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.6.3.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.3.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.6.3.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.6.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.4.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.6.3.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.6.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.6.3.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.4K</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.6.3.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.6.3.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.6.3.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.6.3.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.6.3.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.6.3.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"http://www.icst.pku.edu.cn/cpdp\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">http://www.icst.pku.edu.cn/cpdp</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.7.4\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.7.4.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.7.4.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.7.4.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">ICDAR-2019<cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib80\" title=\"\" class=\"ltx_ref\">80</a>]</cite> (Section <a href=\"#S4.SS5\" title=\"IV-E ICDAR-2019 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-E</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.7.4.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.7.4.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.7.4.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.7.4.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.3.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.7.4.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.7.4.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.4.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.7.4.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.7.4.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.7.4.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">3.6K</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.7.4.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.7.4.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.7.4.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.7.4.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.7.4.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.7.4.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"https://zenodo.org/record/2649217\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">https://zenodo.org/record/2649217</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.8.5\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.8.5.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.8.5.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.8.5.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.8.5.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">UNLV <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib2\" title=\"\" class=\"ltx_ref\">2</a>]</cite> (Section <a href=\"#S4.SS3\" title=\"IV-C UNLV ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-C</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.8.5.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.8.5.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.8.5.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.8.5.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.8.5.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.8.5.3.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.8.5.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.8.5.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.8.5.4.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.8.5.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.8.5.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.8.5.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.8.5.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">427</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.8.5.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.8.5.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.8.5.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.8.5.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.8.5.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.8.5.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.8.5.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"https://drive.google.com/file/d/1ETq5hhoIgCzzom6yivkokhQ8DoOm6nDs\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">https://drive.google.com/file/d/</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.9.6\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.9.6.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.9.6.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.9.6.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.9.6.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Marmot <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib3\" title=\"\" class=\"ltx_ref\">3</a>]</cite> (Section <a href=\"#S4.SS6\" title=\"IV-F Marmot ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-F</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.9.6.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.9.6.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.9.6.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.9.6.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.9.6.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.9.6.3.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.9.6.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.9.6.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.9.6.4.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.9.6.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.9.6.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.9.6.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.9.6.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">958</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.9.6.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.9.6.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.9.6.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.9.6.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.9.6.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.9.6.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.9.6.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"http://www.icst.pku.edu.cn/cpdp/sjzy/\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">http://www.icst.pku.edu.cn/cpdp/sjzy/</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.10.7\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.10.7.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.10.7.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.10.7.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.10.7.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">UW3 <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib112\" title=\"\" class=\"ltx_ref\">112</a>]</cite> (Section <a href=\"#S4.SS4\" title=\"IV-D UW3 ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-D</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.10.7.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.10.7.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.10.7.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.10.7.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.10.7.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.10.7.3.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.10.7.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.10.7.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.10.7.4.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.10.7.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.10.7.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.10.7.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.10.7.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">165</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.10.7.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.10.7.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.10.7.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.10.7.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.10.7.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.10.7.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.10.7.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"http://tc11.cvc.uab.es/datasets/DFKITGT2010_1/\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">http://tc11.cvc.uab.es/datasets/DFKITGT2010_1/</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.3.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.3.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.3.4.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.3.4.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">TableBank <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib62\" title=\"\" class=\"ltx_ref\">62</a>]</cite> (Section <a href=\"#S4.SS7\" title=\"IV-G TableBank ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-G</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.3.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.3.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.3.5.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.3.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.3.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.3.6.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.3.7\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.3.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.3.7.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.3.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.3.3.3\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.3.3.3.3\" class=\"ltx_p\"><math id=\"S4.T5.1.1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"417\" display=\"inline\"><semantics id=\"S4.T5.1.1.1.1.1.m1.1a\"><mn mathsize=\"80%\" id=\"S4.T5.1.1.1.1.1.m1.1.1\" xref=\"S4.T5.1.1.1.1.1.m1.1.1.cmml\">417</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.1.1.1.1.1.m1.1b\"><cn type=\"integer\" id=\"S4.T5.1.1.1.1.1.m1.1.1.cmml\" xref=\"S4.T5.1.1.1.1.1.m1.1.1\">417</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.1.1.1.1.1.m1.1c\">417</annotation></semantics></math><span id=\"S4.T5.3.3.3.3.3.2\" class=\"ltx_text\" style=\"font-size:80%;\">K<math id=\"S4.T5.2.2.2.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"(TD)\" display=\"inline\"><semantics id=\"S4.T5.2.2.2.2.2.1.m1.1a\"><mrow id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml\"><mo stretchy=\"false\" id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.2\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml\">(</mo><mrow id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml\"><mi id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.2\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.2.cmml\">T</mi><mo lspace=\"0em\" rspace=\"0em\" id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.1\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.1.cmml\">​</mo><mi id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.3\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.3.cmml\">D</mi></mrow><mo stretchy=\"false\" id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.3\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml\">)</mo></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.2.2.2.2.2.1.m1.1b\"><apply id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.cmml\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1\"><times id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.1.cmml\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.1\"></times><ci id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.2.cmml\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.2\">𝑇</ci><ci id=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.3.cmml\" xref=\"S4.T5.2.2.2.2.2.1.m1.1.1.1.1.3\">𝐷</ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.2.2.2.2.2.1.m1.1c\">(TD)</annotation></semantics></math>, <math id=\"S4.T5.3.3.3.3.3.2.m2.1\" class=\"ltx_Math\" alttext=\"145\" display=\"inline\"><semantics id=\"S4.T5.3.3.3.3.3.2.m2.1a\"><mn id=\"S4.T5.3.3.3.3.3.2.m2.1.1\" xref=\"S4.T5.3.3.3.3.3.2.m2.1.1.cmml\">145</mn><annotation-xml encoding=\"MathML-Content\" id=\"S4.T5.3.3.3.3.3.2.m2.1b\"><cn type=\"integer\" id=\"S4.T5.3.3.3.3.3.2.m2.1.1.cmml\" xref=\"S4.T5.3.3.3.3.3.2.m2.1.1\">145</cn></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T5.3.3.3.3.3.2.m2.1c\">145</annotation></semantics></math>K (TSR)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.3.8\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.3.8.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.3.8.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.3.8.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.3.9\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.3.9.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.3.9.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"https://github.com/doc-analysis/TableBank/\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">https://github.com/doc-analysis/TableBank</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.11.8\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.11.8.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.11.8.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.11.8.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.11.8.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">TabStructDB <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib50\" title=\"\" class=\"ltx_ref\">50</a>]</cite> (Section <a href=\"#S4.SS8\" title=\"IV-H TabStructDB ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-H</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.11.8.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.11.8.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.11.8.2.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.11.8.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.11.8.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.11.8.3.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.11.8.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.11.8.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.11.8.4.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.11.8.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.11.8.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.11.8.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.11.8.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">2.4K</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.11.8.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.11.8.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.11.8.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.11.8.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.11.8.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.11.8.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.11.8.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"https://bit.ly/2XonOEx\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">https://bit.ly/2XonOEx</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.12.9\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.12.9.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.12.9.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.12.9.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.12.9.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">TABLE2LATEX <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib109\" title=\"\" class=\"ltx_ref\">109</a>]</cite> (Section <a href=\"#S4.SS9\" title=\"IV-I TABLE2LATEX-450K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-I</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.12.9.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.12.9.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.12.9.2.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.12.9.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.12.9.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.12.9.3.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.12.9.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.12.9.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.12.9.4.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.12.9.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.12.9.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.12.9.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.12.9.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">450K</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.12.9.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.12.9.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.12.9.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.12.9.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.12.9.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.12.9.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.12.9.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"https://github.com/bloomberg/TABLE2LATEX/\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">https://github.com/bloomberg/TABLE2LATEX</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.13.10\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.13.10.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.13.10.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.13.10.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.13.10.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">SciTSR <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib98\" title=\"\" class=\"ltx_ref\">98</a>]</cite> (Section <a href=\"#S4.SS10\" title=\"IV-J SciTSR ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-J</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.13.10.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.13.10.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.13.10.2.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.13.10.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.13.10.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.13.10.3.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.13.10.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.13.10.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.13.10.4.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.13.10.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.13.10.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.13.10.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.13.10.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">15K</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.13.10.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.13.10.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.13.10.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.13.10.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.13.10.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.13.10.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.13.10.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"https://github.com/Academic-Hammer/SciTSR\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">https://github.com/Academic-Hammer/SciTSR</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.14.11\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.14.11.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.14.11.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.14.11.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.14.11.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">DeepFigures <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib4\" title=\"\" class=\"ltx_ref\">4</a>]</cite> (Section <a href=\"#S4.SS11\" title=\"IV-K DeepFigures ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-K</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.14.11.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.14.11.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.14.11.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.14.11.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.14.11.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.14.11.3.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.14.11.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.14.11.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.14.11.4.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.14.11.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.14.11.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.14.11.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.14.11.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">1.4M</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.14.11.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.14.11.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.14.11.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.14.11.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.14.11.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.14.11.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.14.11.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/deepfigures/jcdl-deepfigures-labels.tar.gz\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">https://s3-us-west-2.amazonaws.com/ai2-s2-research-public/</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.15.12\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.15.12.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.15.12.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.15.12.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.15.12.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">RVL-CDIP (Subset) (Section <a href=\"#S4.SS12\" title=\"IV-L RVL-CDIP (Subset) ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-L</span></span></a>) <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib87\" title=\"\" class=\"ltx_ref\">87</a>]</cite></span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.15.12.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.15.12.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.15.12.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.15.12.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.15.12.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.15.12.3.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.15.12.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.15.12.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.15.12.4.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.15.12.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.15.12.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.15.12.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.15.12.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">518</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.15.12.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.15.12.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.15.12.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.15.12.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.15.12.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.15.12.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.15.12.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"https://zenodo.org/record/3257319\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">https://zenodo.org/record/3257319</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.16.13\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.16.13.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.16.13.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.16.13.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.16.13.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">PubTabNet <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib32\" title=\"\" class=\"ltx_ref\">32</a>]</cite> (Section <a href=\"#S4.SS13\" title=\"IV-M PubTabNet ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-M</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.16.13.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.16.13.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.16.13.2.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.16.13.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.16.13.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.16.13.3.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.16.13.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.16.13.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.16.13.4.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.16.13.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.16.13.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.16.13.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.16.13.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">568K</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.16.13.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.16.13.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.16.13.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.16.13.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.16.13.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.16.13.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.16.13.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"https://github.com/ibm-aur-nlp/PubTabNet\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">https://github.com/ibm-aur-nlp/PubTabNet</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.17.14\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.17.14.1\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.17.14.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.17.14.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.17.14.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">IIT-AR-13k <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib113\" title=\"\" class=\"ltx_ref\">113</a>]</cite> (Section <a href=\"#S4.SS14\" title=\"IV-N IIIT-AR-13K ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-N</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.17.14.2\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.17.14.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.17.14.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.17.14.3\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.17.14.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.17.14.3.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.17.14.4\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.17.14.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.17.14.4.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.17.14.5\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.17.14.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.17.14.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.17.14.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">13K</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.17.14.6\" class=\"ltx_td ltx_align_justify ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.17.14.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.17.14.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.17.14.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Scanned</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.17.14.7\" class=\"ltx_td ltx_align_justify ltx_border_t\">\n<span id=\"S4.T5.3.17.14.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.17.14.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"http://cvit.iiit.ac.in/usodi/iiitar13k.php\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">http://cvit.iiit.ac.in/usodi/iiitar13k.php</a></span>\n</span>\n</td>\n</tr>\n<tr id=\"S4.T5.3.18.15\" class=\"ltx_tr\">\n<td id=\"S4.T5.3.18.15.1\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.18.15.1.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.18.15.1.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.18.15.1.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">CamCap <cite class=\"ltx_cite ltx_citemacro_cite\">[<a href=\"#bib.bib21\" title=\"\" class=\"ltx_ref\">21</a>]</cite> (Section <a href=\"#S4.SS15\" title=\"IV-O CamCap ‣ IV Datasets ‣ Current Status and Performance Analysis of Table Recognition in Document Images with Deep Neural Networks\" class=\"ltx_ref\"><span class=\"ltx_text ltx_ref_tag\"><span class=\"ltx_text\">IV-O</span></span></a>)</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.18.15.2\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.18.15.2.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.18.15.2.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.18.15.3\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.18.15.3.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.18.15.3.1.1\" class=\"ltx_p\">✓</span>\n</span>\n</td>\n<td id=\"S4.T5.3.18.15.4\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.18.15.4.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.18.15.4.1.1\" class=\"ltx_p\">✗</span>\n</span>\n</td>\n<td id=\"S4.T5.3.18.15.5\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.18.15.5.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.18.15.5.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.18.15.5.1.1.1\" class=\"ltx_text\" style=\"font-size:90%;\">75</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.18.15.6\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_r ltx_border_t\">\n<span id=\"S4.T5.3.18.15.6.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.18.15.6.1.1\" class=\"ltx_p\"><span id=\"S4.T5.3.18.15.6.1.1.1\" class=\"ltx_text\" style=\"font-size:80%;\">Camera-captured</span></span>\n</span>\n</td>\n<td id=\"S4.T5.3.18.15.7\" class=\"ltx_td ltx_align_justify ltx_border_b ltx_border_t\">\n<span id=\"S4.T5.3.18.15.7.1\" class=\"ltx_inline-block ltx_align_top\">\n<span id=\"S4.T5.3.18.15.7.1.1\" class=\"ltx_p\"><a target=\"_blank\" href=\"http://ispl.snu.ac.kr/~cusisi/dataset.zip\" title=\"\" class=\"ltx_ref ltx_href\" style=\"font-size:80%;\">http://ispl.snu.ac.kr/ cusisi/dataset.zip</a></span>\n</span>\n</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_6": {
        "caption": " Table Detection Performance Comparison. The double horizontal line partitions the results obtained on various datasets. Outstanding results in all the respective datasets are highlighted. For the ICDAR-2019 dataset [80], all of the three approaches are not directly comparable to each other because they report F-Measure on different IOU thresholds. Hence, results on ICDAR-2019 dataset are not highlighted. ",
        "table": "<table id=\"S5.E1\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S5.E1.m1.1\" class=\"ltx_Math\" alttext=\"\\frac{\\text{Predicted area in ground truth}}{\\text{Total area of predicted region}}=\\frac{\\text{TP}}{\\text{TP $+$ FP}}\" display=\"block\"><semantics id=\"S5.E1.m1.1a\"><mrow id=\"S5.E1.m1.1.2\" xref=\"S5.E1.m1.1.2.cmml\"><mfrac id=\"S5.E1.m1.1.2.2\" xref=\"S5.E1.m1.1.2.2.cmml\"><mtext id=\"S5.E1.m1.1.2.2.2\" xref=\"S5.E1.m1.1.2.2.2a.cmml\">Predicted area in ground truth</mtext><mtext id=\"S5.E1.m1.1.2.2.3\" xref=\"S5.E1.m1.1.2.2.3a.cmml\">Total area of predicted region</mtext></mfrac><mo id=\"S5.E1.m1.1.2.1\" xref=\"S5.E1.m1.1.2.1.cmml\">=</mo><mfrac id=\"S5.E1.m1.1.1\" xref=\"S5.E1.m1.1.1.cmml\"><mtext id=\"S5.E1.m1.1.1.3\" xref=\"S5.E1.m1.1.1.3a.cmml\">TP</mtext><mrow id=\"S5.E1.m1.1.1.1\" xref=\"S5.E1.m1.1.1.1c.cmml\"><mtext id=\"S5.E1.m1.1.1.1a\" xref=\"S5.E1.m1.1.1.1c.cmml\">TP </mtext><mo id=\"S5.E1.m1.1.1.1.1.1.m1.1.1\" xref=\"S5.E1.m1.1.1.1.1.1.m1.1.1.cmml\">+</mo><mtext id=\"S5.E1.m1.1.1.1b\" xref=\"S5.E1.m1.1.1.1c.cmml\"> FP</mtext></mrow></mfrac></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E1.m1.1b\"><apply id=\"S5.E1.m1.1.2.cmml\" xref=\"S5.E1.m1.1.2\"><eq id=\"S5.E1.m1.1.2.1.cmml\" xref=\"S5.E1.m1.1.2.1\"></eq><apply id=\"S5.E1.m1.1.2.2.cmml\" xref=\"S5.E1.m1.1.2.2\"><divide id=\"S5.E1.m1.1.2.2.1.cmml\" xref=\"S5.E1.m1.1.2.2\"></divide><ci id=\"S5.E1.m1.1.2.2.2a.cmml\" xref=\"S5.E1.m1.1.2.2.2\"><mtext id=\"S5.E1.m1.1.2.2.2.cmml\" xref=\"S5.E1.m1.1.2.2.2\">Predicted area in ground truth</mtext></ci><ci id=\"S5.E1.m1.1.2.2.3a.cmml\" xref=\"S5.E1.m1.1.2.2.3\"><mtext id=\"S5.E1.m1.1.2.2.3.cmml\" xref=\"S5.E1.m1.1.2.2.3\">Total area of predicted region</mtext></ci></apply><apply id=\"S5.E1.m1.1.1.cmml\" xref=\"S5.E1.m1.1.1\"><divide id=\"S5.E1.m1.1.1.2.cmml\" xref=\"S5.E1.m1.1.1\"></divide><ci id=\"S5.E1.m1.1.1.3a.cmml\" xref=\"S5.E1.m1.1.1.3\"><mtext id=\"S5.E1.m1.1.1.3.cmml\" xref=\"S5.E1.m1.1.1.3\">TP</mtext></ci><ci id=\"S5.E1.m1.1.1.1c.cmml\" xref=\"S5.E1.m1.1.1.1\"><mrow id=\"S5.E1.m1.1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1\"><mtext id=\"S5.E1.m1.1.1.1a.cmml\" xref=\"S5.E1.m1.1.1.1\">TP </mtext><mo id=\"S5.E1.m1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.E1.m1.1.1.1.1.1.m1.1.1\">+</mo><mtext id=\"S5.E1.m1.1.1.1b.cmml\" xref=\"S5.E1.m1.1.1.1\"> FP</mtext></mrow></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E1.m1.1c\">\\frac{\\text{Predicted area in ground truth}}{\\text{Total area of predicted region}}=\\frac{\\text{TP}}{\\text{TP $+$ FP}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(1)</span></td>\n</tr></tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_7": {
        "caption": ": Table Structural Segmentation Performance. Outstanding results are highlighted. Results in the last two rows are not directly comparable with other methods because PDF files are employed instead of document images. ",
        "table": "<table id=\"S5.E2\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S5.E2.m1.1\" class=\"ltx_Math\" alttext=\"\\frac{\\text{Ground truth area in predicted region}}{\\text{Total area of ground truth region}}=\\frac{\\text{TP}}{\\text{TP $+$ FN}}\" display=\"block\"><semantics id=\"S5.E2.m1.1a\"><mrow id=\"S5.E2.m1.1.2\" xref=\"S5.E2.m1.1.2.cmml\"><mfrac id=\"S5.E2.m1.1.2.2\" xref=\"S5.E2.m1.1.2.2.cmml\"><mtext id=\"S5.E2.m1.1.2.2.2\" xref=\"S5.E2.m1.1.2.2.2a.cmml\">Ground truth area in predicted region</mtext><mtext id=\"S5.E2.m1.1.2.2.3\" xref=\"S5.E2.m1.1.2.2.3a.cmml\">Total area of ground truth region</mtext></mfrac><mo id=\"S5.E2.m1.1.2.1\" xref=\"S5.E2.m1.1.2.1.cmml\">=</mo><mfrac id=\"S5.E2.m1.1.1\" xref=\"S5.E2.m1.1.1.cmml\"><mtext id=\"S5.E2.m1.1.1.3\" xref=\"S5.E2.m1.1.1.3a.cmml\">TP</mtext><mrow id=\"S5.E2.m1.1.1.1\" xref=\"S5.E2.m1.1.1.1c.cmml\"><mtext id=\"S5.E2.m1.1.1.1a\" xref=\"S5.E2.m1.1.1.1c.cmml\">TP </mtext><mo id=\"S5.E2.m1.1.1.1.1.1.m1.1.1\" xref=\"S5.E2.m1.1.1.1.1.1.m1.1.1.cmml\">+</mo><mtext id=\"S5.E2.m1.1.1.1b\" xref=\"S5.E2.m1.1.1.1c.cmml\"> FN</mtext></mrow></mfrac></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S5.E2.m1.1b\"><apply id=\"S5.E2.m1.1.2.cmml\" xref=\"S5.E2.m1.1.2\"><eq id=\"S5.E2.m1.1.2.1.cmml\" xref=\"S5.E2.m1.1.2.1\"></eq><apply id=\"S5.E2.m1.1.2.2.cmml\" xref=\"S5.E2.m1.1.2.2\"><divide id=\"S5.E2.m1.1.2.2.1.cmml\" xref=\"S5.E2.m1.1.2.2\"></divide><ci id=\"S5.E2.m1.1.2.2.2a.cmml\" xref=\"S5.E2.m1.1.2.2.2\"><mtext id=\"S5.E2.m1.1.2.2.2.cmml\" xref=\"S5.E2.m1.1.2.2.2\">Ground truth area in predicted region</mtext></ci><ci id=\"S5.E2.m1.1.2.2.3a.cmml\" xref=\"S5.E2.m1.1.2.2.3\"><mtext id=\"S5.E2.m1.1.2.2.3.cmml\" xref=\"S5.E2.m1.1.2.2.3\">Total area of ground truth region</mtext></ci></apply><apply id=\"S5.E2.m1.1.1.cmml\" xref=\"S5.E2.m1.1.1\"><divide id=\"S5.E2.m1.1.1.2.cmml\" xref=\"S5.E2.m1.1.1\"></divide><ci id=\"S5.E2.m1.1.1.3a.cmml\" xref=\"S5.E2.m1.1.1.3\"><mtext id=\"S5.E2.m1.1.1.3.cmml\" xref=\"S5.E2.m1.1.1.3\">TP</mtext></ci><ci id=\"S5.E2.m1.1.1.1c.cmml\" xref=\"S5.E2.m1.1.1.1\"><mrow id=\"S5.E2.m1.1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1\"><mtext id=\"S5.E2.m1.1.1.1a.cmml\" xref=\"S5.E2.m1.1.1.1\">TP </mtext><mo id=\"S5.E2.m1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.E2.m1.1.1.1.1.1.m1.1.1\">+</mo><mtext id=\"S5.E2.m1.1.1.1b.cmml\" xref=\"S5.E2.m1.1.1.1\"> FN</mtext></mrow></ci></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E2.m1.1c\">\\frac{\\text{Ground truth area in predicted region}}{\\text{Total area of ground truth region}}=\\frac{\\text{TP}}{\\text{TP $+$ FN}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(2)</span></td>\n</tr></tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_8": {
        "caption": "I: Table Structural Segmentation Performance on the dataset of ICDAR-2019 [80]. For brevity and clarity, these results are separately presented in this table.",
        "table": "<table id=\"S5.E3\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S5.E3.m1.3\" class=\"ltx_Math\" alttext=\"\\frac{\\text{$2\\times$ Precision $\\times$ Recall}}{\\text{Precision $+$ Recall}}\" display=\"block\"><semantics id=\"S5.E3.m1.3a\"><mfrac id=\"S5.E3.m1.3.3\" xref=\"S5.E3.m1.3.3.cmml\"><mrow id=\"S5.E3.m1.2.2.2\" xref=\"S5.E3.m1.2.2.2c.cmml\"><mrow id=\"S5.E3.m1.1.1.1.1.1.m1.1\" xref=\"S5.E3.m1.1.1.1.1.1.m1.1.cmml\"><mn id=\"S5.E3.m1.1.1.1.1.1.m1.1.1\" xref=\"S5.E3.m1.1.1.1.1.1.m1.1.1.cmml\">2</mn><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S5.E3.m1.1.1.1.1.1.m1.1.2\" xref=\"S5.E3.m1.1.1.1.1.1.m1.1.2.cmml\">×</mo></mrow><mtext id=\"S5.E3.m1.2.2.2a\" xref=\"S5.E3.m1.2.2.2c.cmml\"> Precision </mtext><mo lspace=\"0.222em\" rspace=\"0.222em\" id=\"S5.E3.m1.2.2.2.2.2.m2.1.1\" xref=\"S5.E3.m1.2.2.2.2.2.m2.1.1.cmml\">×</mo><mtext id=\"S5.E3.m1.2.2.2b\" xref=\"S5.E3.m1.2.2.2c.cmml\"> Recall</mtext></mrow><mrow id=\"S5.E3.m1.3.3.3\" xref=\"S5.E3.m1.3.3.3c.cmml\"><mtext id=\"S5.E3.m1.3.3.3a\" xref=\"S5.E3.m1.3.3.3c.cmml\">Precision </mtext><mo id=\"S5.E3.m1.3.3.3.1.1.m1.1.1\" xref=\"S5.E3.m1.3.3.3.1.1.m1.1.1.cmml\">+</mo><mtext id=\"S5.E3.m1.3.3.3b\" xref=\"S5.E3.m1.3.3.3c.cmml\"> Recall</mtext></mrow></mfrac><annotation-xml encoding=\"MathML-Content\" id=\"S5.E3.m1.3b\"><apply id=\"S5.E3.m1.3.3.cmml\" xref=\"S5.E3.m1.3.3\"><divide id=\"S5.E3.m1.3.3.4.cmml\" xref=\"S5.E3.m1.3.3\"></divide><ci id=\"S5.E3.m1.2.2.2c.cmml\" xref=\"S5.E3.m1.2.2.2\"><mrow id=\"S5.E3.m1.2.2.2.cmml\" xref=\"S5.E3.m1.2.2.2\"><mrow id=\"S5.E3.m1.1.1.1.1.1.m1.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.m1.1\"><mn id=\"S5.E3.m1.1.1.1.1.1.m1.1.1.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.m1.1.1\">2</mn><mo id=\"S5.E3.m1.1.1.1.1.1.m1.1.2.cmml\" xref=\"S5.E3.m1.1.1.1.1.1.m1.1.2\">×</mo></mrow><mtext id=\"S5.E3.m1.2.2.2a.cmml\" xref=\"S5.E3.m1.2.2.2\"> Precision </mtext><mo id=\"S5.E3.m1.2.2.2.2.2.m2.1.1.cmml\" xref=\"S5.E3.m1.2.2.2.2.2.m2.1.1\">×</mo><mtext id=\"S5.E3.m1.2.2.2b.cmml\" xref=\"S5.E3.m1.2.2.2\"> Recall</mtext></mrow></ci><ci id=\"S5.E3.m1.3.3.3c.cmml\" xref=\"S5.E3.m1.3.3.3\"><mrow id=\"S5.E3.m1.3.3.3.cmml\" xref=\"S5.E3.m1.3.3.3\"><mtext id=\"S5.E3.m1.3.3.3a.cmml\" xref=\"S5.E3.m1.3.3.3\">Precision </mtext><mo id=\"S5.E3.m1.3.3.3.1.1.m1.1.1.cmml\" xref=\"S5.E3.m1.3.3.3.1.1.m1.1.1\">+</mo><mtext id=\"S5.E3.m1.3.3.3b.cmml\" xref=\"S5.E3.m1.3.3.3\"> Recall</mtext></mrow></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E3.m1.3c\">\\frac{\\text{$2\\times$ Precision $\\times$ Recall}}{\\text{Precision $+$ Recall}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(3)</span></td>\n</tr></tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    },
    "id_table_9": {
        "caption": " Table Recognition Performance. Results mentioned in this table are not directly comparable with each other because different datasets and evaluation metrics have been used.",
        "table": "<table id=\"S5.E4\" class=\"ltx_equation ltx_eqn_table\">\n\n<tbody><tr class=\"ltx_equation ltx_eqn_row ltx_align_baseline\">\n<td class=\"ltx_eqn_cell ltx_eqn_center_padleft\"></td>\n<td class=\"ltx_eqn_cell ltx_align_center\"><math id=\"S5.E4.m1.1\" class=\"ltx_Math\" alttext=\"\\frac{\\text{Area of Overlap region}}{\\text{Area of Union region}}\" display=\"block\"><semantics id=\"S5.E4.m1.1a\"><mfrac id=\"S5.E4.m1.1.1\" xref=\"S5.E4.m1.1.1.cmml\"><mtext id=\"S5.E4.m1.1.1.2\" xref=\"S5.E4.m1.1.1.2a.cmml\">Area of Overlap region</mtext><mtext id=\"S5.E4.m1.1.1.3\" xref=\"S5.E4.m1.1.1.3a.cmml\">Area of Union region</mtext></mfrac><annotation-xml encoding=\"MathML-Content\" id=\"S5.E4.m1.1b\"><apply id=\"S5.E4.m1.1.1.cmml\" xref=\"S5.E4.m1.1.1\"><divide id=\"S5.E4.m1.1.1.1.cmml\" xref=\"S5.E4.m1.1.1\"></divide><ci id=\"S5.E4.m1.1.1.2a.cmml\" xref=\"S5.E4.m1.1.1.2\"><mtext id=\"S5.E4.m1.1.1.2.cmml\" xref=\"S5.E4.m1.1.1.2\">Area of Overlap region</mtext></ci><ci id=\"S5.E4.m1.1.1.3a.cmml\" xref=\"S5.E4.m1.1.1.3\"><mtext id=\"S5.E4.m1.1.1.3.cmml\" xref=\"S5.E4.m1.1.1.3\">Area of Union region</mtext></ci></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S5.E4.m1.1c\">\\frac{\\text{Area of Overlap region}}{\\text{Area of Union region}}</annotation></semantics></math></td>\n<td class=\"ltx_eqn_cell ltx_eqn_center_padright\"></td>\n<td rowspan=\"1\" class=\"ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right\"><span class=\"ltx_tag ltx_tag_equation ltx_align_right\">(4)</span></td>\n</tr></tbody>\n</table>\n",
        "footnotes": [],
        "references": []
    }
}