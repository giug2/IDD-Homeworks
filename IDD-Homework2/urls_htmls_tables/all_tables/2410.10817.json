{
    "id_table_1": {
        "caption": "Table 1 :  Base and human-aligned model performance on semantic segmentation.  Aligned models largely outperform baselines, with DINO-HA achieving the highest performance across models for 4 out of 5 datasets. Note that Pascal VOC, ADE20k, and Cityscapes were included in DINOv2s retrieval pretraining.    \\dagger   indicates best score in the column.",
        "table": "S4.T1.13.13.13",
        "footnotes": [],
        "references": [
            "NIGHTS consists of image triplets varying in  mid-level  information. Images in a triplet roughly share the same semantic content; however, they vary in pose, layout, shape, color, and the number of objects (see Fig.  15  in the Appendix for examples). Thus, the perceptual judgments indicate the shared visual appearance properties, as opposed to requiring higher-level semantic knowledge about the image content.",
            "CLS CLS \\mathrm{CLS} roman_CLS  is of dimension  ( 1 , d ) 1 d (1,d) ( 1 , italic_d )  and  PATCH PATCH \\mathrm{PATCH} roman_PATCH  is  ( s , s , d ) s s d (s,s,d) ( italic_s , italic_s , italic_d )  where  s s s italic_s  is the number of patches along each spatial dimension. We spatially average the patch tokens to get dimension  ( 1 , d ) 1 d (1,d) ( 1 , italic_d ) . We then concatenate the  CLS CLS \\mathrm{CLS} roman_CLS  and pooled patch tokens to get dimension  ( 1 , 2  d ) 1 2 d (1,2d) ( 1 , 2 italic_d ) . We fine-tune the same alignment loss (see Eq.  2 ) on concatenated  CLS CLS \\mathrm{CLS} roman_CLS  and averaged-pooled patch tokens, and train heads for semantic segmentation and depth estimation on the resulting patch embeddings. Note that only experiments reported in Sec.  4.1  use this objective, as they require local features. For all other evaluations, we exclusively use  CLS CLS \\mathrm{CLS} roman_CLS  tokens (see Eq.  2 ) if not mentioned otherwise.",
            "We finetune each backbone using Low-Rank Adaptation (LoRA), which was found to achieve better alignment performance and efficiency than full fine-tuning in  [ 18 ] . For more training and technical details see Sections  C.1  and  C.6  in the Appendix.",
            "Semantic segmentation.  Following the procedure detailed in Section  3.3  and Fig.   2 , we LoRA-tune new backbones with perceptually-aligned CLS and patch tokens. To evaluate segmentation performance, we freeze these backbones and train a single linear layer transforming patch tokens to a segmentation map. We evaluate DINO and DINOv2 on standard segmentation benchmarks in Table  1  and show that human-aligned models boost performance in 16 out of 20 cases. Across all datasets and metrics, human-aligned DINO (denoted as DINO-HA) outperforms the base model and often achieves the highest mIoU and Pixel Accuracy (P.A.) overall. DINOv2-HA also outperforms its nonaligned counterpart on COCO and DAVIS2017.",
            "Given that the perceptual similarity dataset we use for finetuning contains image-level similarity judgments, the consistent improvements that we observe on counting tasks, which requires local object awareness, is somewhat surprising. We hypothesize that the sensitivity of our human-aligned models to object counts may be a byproduct of NIGHTS examples themselves, many of which include image triplets with varying numbers of objects (see Fig.  15  in the Appendix). In terms of human perception, we note that humans consider object count when evaluating image similarity as soon as they develop counting profiency  [ 40 ] . Thus, it is possible that given the prevalence of triplets with object-count variations in NIGHTS, the human annotations naturally capture this counting aware effect in the global image-level labels and propagate this information to the human-aligned models.",
            "In Section  A.2  of the Appendix we find that the performance boost from NIGHTS over other datasets is also consistent across semantic segmentation and depth estimation. Conversely, tuning on NIGHTS fails to improve over other datasets on classification datasets; we further discuss this finding in Sections  5  and  A.1 .",
            "Why does fine-tuning on NIGHTS in particular lead to improvements? We hypothesize that the variations found in BAPPS and THINGS are solely high- or low-level, whereas the mid-level distortions in NIGHTS cover salient features that humans use when making inferences about what they see; these characteristics include style, pose, color, and count (see Fig. 15 ), and largely correlate with the characteristics a model must successfully extract for many computer vision tasks. Previous work  [ 18 ]  found that models fine-tuned on NIGHTS seem to attend to both low-level and semantic attributes. Aligning a feature space to these concepts may be useful for visual tasks requiring both visual and semantic knowledge, such as retrieval, counting, segmentation, etc. This hypothesis may also explain why tuning on NIGHTS hurts performance on fine-grained tasks, in which perceptually similar images may belong to different categories.",
            "In Section  4.5  we show that tuning on NIGHTS leads to larger performance improvements on object counting and instance retrieval than training on other triplet datasets. Here, we show that this finding is consistent across dense prediction tasks as well. We evaluate models trained on NIGHTS, BAPPS, THINGS, and ImageNet  as described in section  4.5   on semantic segmentation and depth estimation. As shown in Fig.  9 -  10 , training on NIGHTS outperforms training on all other datasets.",
            "We additionally run this ablation on a subset of VTAB (see Fig.  11 a). On these classification datasets, base models perform best; the exception is sNORB (pose prediction) for which NIGHTS is best. Amongst perceptual datasets, NIGHTS is sometimes outperformed by BAPPS/ImageNet. This result is consistent with our findings in Section  A.1 , that tuning on NIGHTS often fails to improve classification performance.",
            "Finally, we ablate the  strength  of alignment  i.e. the training loss when fine-tuning on different perceptual datasets. In Fig.  11 b we show the downstream performance on DeepFashion2 when fine-tuning for increasing numbers of steps on different datasets. Tuning on NIGHTS outperforms other datasets over the full training trajectory. Moreover, while performance rises significantly with a small amount of alignment to NIGHTS, it trends down after >1000 steps. This indicates that a small amount of alignment is helpful, however overfitting may harm performance.",
            "We replicate our RAG experiment on OpenFlamingo in Section  4.2  on IDEFICS2, a recently released 8B multimodal model achieving state-of-the-art results across several benchmarks  [ 56 ] . As shown in Fig.  12 , across the same four classification datasets, performance consistently improved when using NIGHTS-tuned models in the RAG pipeline. These results validate our RAG results on OpenFlamingo, suggesting that performance gains from perceptual alignment are not specific to a particular VLM.",
            "See Figures  13  and  14  for additional examples of instance retrieval on DeepFashion2 and object counting on Clevr-count.",
            "See Figure  15  for examples of the triplet datasets we train on in this paper. In each triplet, the reference image (middle) and outlined image are labeled as the similar pair. Perceptually-aligned models in this paper are tuned on NIGHTS triplets (top row), whose image variations encompass a variety of mid-level perceptual attributes including object count, identity, layout, subject pose, and color. In contrast, the datasets studied in our ablation encompass differing image attributes: BAPPS focuses on image patches with low-level distortions such as CNN artifacts and color jitter, THINGS encodes similarity in concept space, and our constructed ImageNet triplets outline class boundaries.",
            "We train models for the dataset ablation section with identical hyperparameters as the human-aligned models (details in Section  C.1  and on the same amount of data as the NIGHTS training set (13,900 triplets). We randomly select these triplets without replacement from the BAPPS and THINGS training sets, and construct the ImageNet triplets by randomly sampling two images from one class and a third image from a different class."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 :  Human-aligned DINO and DINOv2 performance on monocular depth estimation benchmarks.  Note that NYUv2 and SUN-RGBD were included in DINOv2s retrieval pretraining set, yet human-aligned DINOv2 still outperforms the base model on SUN-RGBD. Along with the results on an unseen test data domain (train on NYUv2    \\rightarrow   test on 4D Light Field), these results demonstrate strong generalization performance of models aligned to human perceptual judgments.    \\dagger   indicates best score in the column.",
        "table": "S4.T2.49.49.49",
        "footnotes": [],
        "references": [
            "CLS CLS \\mathrm{CLS} roman_CLS  is of dimension  ( 1 , d ) 1 d (1,d) ( 1 , italic_d )  and  PATCH PATCH \\mathrm{PATCH} roman_PATCH  is  ( s , s , d ) s s d (s,s,d) ( italic_s , italic_s , italic_d )  where  s s s italic_s  is the number of patches along each spatial dimension. We spatially average the patch tokens to get dimension  ( 1 , d ) 1 d (1,d) ( 1 , italic_d ) . We then concatenate the  CLS CLS \\mathrm{CLS} roman_CLS  and pooled patch tokens to get dimension  ( 1 , 2  d ) 1 2 d (1,2d) ( 1 , 2 italic_d ) . We fine-tune the same alignment loss (see Eq.  2 ) on concatenated  CLS CLS \\mathrm{CLS} roman_CLS  and averaged-pooled patch tokens, and train heads for semantic segmentation and depth estimation on the resulting patch embeddings. Note that only experiments reported in Sec.  4.1  use this objective, as they require local features. For all other evaluations, we exclusively use  CLS CLS \\mathrm{CLS} roman_CLS  tokens (see Eq.  2 ) if not mentioned otherwise.",
            "Semantic segmentation.  Following the procedure detailed in Section  3.3  and Fig.   2 , we LoRA-tune new backbones with perceptually-aligned CLS and patch tokens. To evaluate segmentation performance, we freeze these backbones and train a single linear layer transforming patch tokens to a segmentation map. We evaluate DINO and DINOv2 on standard segmentation benchmarks in Table  1  and show that human-aligned models boost performance in 16 out of 20 cases. Across all datasets and metrics, human-aligned DINO (denoted as DINO-HA) outperforms the base model and often achieves the highest mIoU and Pixel Accuracy (P.A.) overall. DINOv2-HA also outperforms its nonaligned counterpart on COCO and DAVIS2017.",
            "Depth estimation.  We follow the evaluation protocol of  [ 44 ,  37 ]  and train a single linear layer on frozen patch tokens to output a depth map with values mapped into 256 uniformly-distributed bins. This head is trained with a scale-invariant log loss introduced in  [ 15 ]  and a scale-invariant gradient-matching term as described in  [ 36 ] . In Table  2 , we report performance on monocular depth estimation and show that human-aligned models outperform base models in 27 out of 36 cases. Consistent with segmentation performance, human-aligned DINO outperforms the base model on all metrics across all datasets, and is often the highest-performing model overall (denoted by    \\dagger  ). We also evaluate out-of-distribution generalization by training a depth head on NYUv2 and evaluating on the 4D Light Field dataset; combined with the performance boost even on datasets that DINOv2 was trained on (NYUv2 and SUN-RGBD), these results demonstrate that human-aligned models have strong generalization capabilities prior to any training for downstream tasks.",
            "In Section  A.2  of the Appendix we find that the performance boost from NIGHTS over other datasets is also consistent across semantic segmentation and depth estimation. Conversely, tuning on NIGHTS fails to improve over other datasets on classification datasets; we further discuss this finding in Sections  5  and  A.1 .",
            "Additionally, a key insight from our dataset ablations is that not all human preferences improve performance. Finetuning on perceptual datasets with solely high-level (THINGS) or low-level (BAPPS) variations hurts performance for many downstream tasks (Section   4.5  and  A.2 ). Similarly,  Muttenthaler et al. [ 43 ]  recently discovered that finetuning vision models on THINGS can hurt downstream task transfer. Due to our evaluation of the synthetically-pretrained SynCLR  [ 55 ] , we attribute the drop in performance to perceptual alignment. Furthermore, by ablating the number of fine-tuning steps in Section  A.2 , we show that overfitting to perceptual judgments may also harm downstream performance.",
            "We replicate our RAG experiment on OpenFlamingo in Section  4.2  on IDEFICS2, a recently released 8B multimodal model achieving state-of-the-art results across several benchmarks  [ 56 ] . As shown in Fig.  12 , across the same four classification datasets, performance consistently improved when using NIGHTS-tuned models in the RAG pipeline. These results validate our RAG results on OpenFlamingo, suggesting that performance gains from perceptual alignment are not specific to a particular VLM.",
            "We assess the effects of perceptual alignment with NIGHTS on two popular CNNs: ResNet50 and ConvNeXt-B, using the same loss described in Section  3.2 . We evaluate on counting and instance retrieval tasks; our results are shown in Tables  7 a-b. Similarly to ViTs, human-aligned CNNs outperform their pretrained counterparts across both tasks. Note that we report results for both ConvNeXt and ResNet, however acknowledge that the accuracies for ResNet are likely too small to draw conclusions from."
        ]
    },
    "id_table_3": {
        "caption": "Table 6 :  Performance on VTAB structured subset.",
        "table": "S4.SS3.8.8.7.7",
        "footnotes": [],
        "references": [
            "Semantic segmentation.  Following the procedure detailed in Section  3.3  and Fig.   2 , we LoRA-tune new backbones with perceptually-aligned CLS and patch tokens. To evaluate segmentation performance, we freeze these backbones and train a single linear layer transforming patch tokens to a segmentation map. We evaluate DINO and DINOv2 on standard segmentation benchmarks in Table  1  and show that human-aligned models boost performance in 16 out of 20 cases. Across all datasets and metrics, human-aligned DINO (denoted as DINO-HA) outperforms the base model and often achieves the highest mIoU and Pixel Accuracy (P.A.) overall. DINOv2-HA also outperforms its nonaligned counterpart on COCO and DAVIS2017.",
            "As illustrated in Fig.  3 , classification accuracy on a wide variety of data domains improves with prompts retrieved by human-aligned models, compared to the original model backbones. Even in out-of-distribution domains such as medical imagery and 2D renders of game scenes, human-aligned models retrieve in-context examples that boost OpenFlamingo classification accuracy. These results suggest that human-aligned models can select more informative examples for in-context learning, thereby boosting the few-shot generalization abilities of a downstream multimodal VLM.",
            "A well-documented limitation of large vision backbones is their performance on compositional tasks: in particular, on object counting  [ 45 ] . We investigate how aligning to perceptual judgments affects performance on counting tasks via the FSC147, CARPK, and Clevr-Count (adapted from the original Clevr dataset by  [ 60 ] ) benchmarks by computer k-Nearest Neighbors accuracy on frozen vision representations. We report results in Table  3  and retrieval visualizations for few ( n = 3 n 3 n=3 italic_n = 3 ) and many ( n = 8 , 10 n 8 10 n=8,10 italic_n = 8 , 10 ) objects in Fig.  4  and find that, across 6 different models, the human-aligned versions outperform their counterparts in 35 out of 36 cases. See the Appendix for full details on the counting experimental setup.",
            "We find that human-aligned models largely lead to worse performance on standard natural tasks. Results on specialized tasks are mixed; for most datasets, human alignment helps or hurts by a marginal amount. Results on structured datasets (Table  6 ) are also mixed, however on particular datasets such as sNORB, dSprites-Orientation, and dSprites-Location, alignment improves performance across backbones. We remark, however, that these structured datasets may have limited evaluation utility. The Clevr-Count task, for instance, has a numerical ground truth (i.e. the number of objects in an image) in which the distance from the correct answer is meaningful; thus it may be ill-suited to classification, and better suited to continuous evaluations such as RMSE or MAE, which we report in Section  4.3 .",
            "We assess the effects of perceptual alignment with NIGHTS on two popular CNNs: ResNet50 and ConvNeXt-B, using the same loss described in Section  3.2 . We evaluate on counting and instance retrieval tasks; our results are shown in Tables  7 a-b. Similarly to ViTs, human-aligned CNNs outperform their pretrained counterparts across both tasks. Note that we report results for both ConvNeXt and ResNet, however acknowledge that the accuracies for ResNet are likely too small to draw conclusions from.",
            "See Figures  13  and  14  for additional examples of instance retrieval on DeepFashion2 and object counting on Clevr-count."
        ]
    },
    "id_table_4": {
        "caption": "Table 7 :  Human-aligned vs. pretrained CNNs, evaluated on counting and instance-retrieval. In most cases, human-aligned CNNs perform better.",
        "table": "S4.SS4.3.3.3",
        "footnotes": [],
        "references": [
            "CLS CLS \\mathrm{CLS} roman_CLS  is of dimension  ( 1 , d ) 1 d (1,d) ( 1 , italic_d )  and  PATCH PATCH \\mathrm{PATCH} roman_PATCH  is  ( s , s , d ) s s d (s,s,d) ( italic_s , italic_s , italic_d )  where  s s s italic_s  is the number of patches along each spatial dimension. We spatially average the patch tokens to get dimension  ( 1 , d ) 1 d (1,d) ( 1 , italic_d ) . We then concatenate the  CLS CLS \\mathrm{CLS} roman_CLS  and pooled patch tokens to get dimension  ( 1 , 2  d ) 1 2 d (1,2d) ( 1 , 2 italic_d ) . We fine-tune the same alignment loss (see Eq.  2 ) on concatenated  CLS CLS \\mathrm{CLS} roman_CLS  and averaged-pooled patch tokens, and train heads for semantic segmentation and depth estimation on the resulting patch embeddings. Note that only experiments reported in Sec.  4.1  use this objective, as they require local features. For all other evaluations, we exclusively use  CLS CLS \\mathrm{CLS} roman_CLS  tokens (see Eq.  2 ) if not mentioned otherwise.",
            "A well-documented limitation of large vision backbones is their performance on compositional tasks: in particular, on object counting  [ 45 ] . We investigate how aligning to perceptual judgments affects performance on counting tasks via the FSC147, CARPK, and Clevr-Count (adapted from the original Clevr dataset by  [ 60 ] ) benchmarks by computer k-Nearest Neighbors accuracy on frozen vision representations. We report results in Table  3  and retrieval visualizations for few ( n = 3 n 3 n=3 italic_n = 3 ) and many ( n = 8 , 10 n 8 10 n=8,10 italic_n = 8 , 10 ) objects in Fig.  4  and find that, across 6 different models, the human-aligned versions outperform their counterparts in 35 out of 36 cases. See the Appendix for full details on the counting experimental setup.",
            "We evaluate all base models and their human-aligned counterparts on the Consumer-to-Shop benchmark of the DeepFashion2 dataset  [ 21 ] . The benchmark consists of 10990 consumer \"in the wild images\" as the query set, and 21438 gallery images with matching clothing items to consumer images. Following the evaluation protocol from  [ 21 ] , we report Top-1, 3, and 5 accuracy in Table  4 . Human aligned models outperform base models by a significant margin across all metrics and backbones (visualized in Fig.  6 . In Fig.  7  we provide qualitative retrieval results. These results agree with prior work showing that training on NIGHTS improves performance in retrieving similar images to queries  [ 18 ] .",
            "Additionally, a key insight from our dataset ablations is that not all human preferences improve performance. Finetuning on perceptual datasets with solely high-level (THINGS) or low-level (BAPPS) variations hurts performance for many downstream tasks (Section   4.5  and  A.2 ). Similarly,  Muttenthaler et al. [ 43 ]  recently discovered that finetuning vision models on THINGS can hurt downstream task transfer. Due to our evaluation of the synthetically-pretrained SynCLR  [ 55 ] , we attribute the drop in performance to perceptual alignment. Furthermore, by ablating the number of fine-tuning steps in Section  A.2 , we show that overfitting to perceptual judgments may also harm downstream performance.",
            "We find that human-aligned models largely lead to worse performance on standard natural tasks. Results on specialized tasks are mixed; for most datasets, human alignment helps or hurts by a marginal amount. Results on structured datasets (Table  6 ) are also mixed, however on particular datasets such as sNORB, dSprites-Orientation, and dSprites-Location, alignment improves performance across backbones. We remark, however, that these structured datasets may have limited evaluation utility. The Clevr-Count task, for instance, has a numerical ground truth (i.e. the number of objects in an image) in which the distance from the correct answer is meaningful; thus it may be ill-suited to classification, and better suited to continuous evaluations such as RMSE or MAE, which we report in Section  4.3 .",
            "In Section  4.5  we show that tuning on NIGHTS leads to larger performance improvements on object counting and instance retrieval than training on other triplet datasets. Here, we show that this finding is consistent across dense prediction tasks as well. We evaluate models trained on NIGHTS, BAPPS, THINGS, and ImageNet  as described in section  4.5   on semantic segmentation and depth estimation. As shown in Fig.  9 -  10 , training on NIGHTS outperforms training on all other datasets.",
            "We replicate our RAG experiment on OpenFlamingo in Section  4.2  on IDEFICS2, a recently released 8B multimodal model achieving state-of-the-art results across several benchmarks  [ 56 ] . As shown in Fig.  12 , across the same four classification datasets, performance consistently improved when using NIGHTS-tuned models in the RAG pipeline. These results validate our RAG results on OpenFlamingo, suggesting that performance gains from perceptual alignment are not specific to a particular VLM.",
            "See Figures  13  and  14  for additional examples of instance retrieval on DeepFashion2 and object counting on Clevr-count."
        ]
    },
    "id_table_5": {
        "caption": "",
        "table": "A1.SS1.7.7.7.7",
        "footnotes": [],
        "references": [
            "NIGHTS consists of image triplets varying in  mid-level  information. Images in a triplet roughly share the same semantic content; however, they vary in pose, layout, shape, color, and the number of objects (see Fig.  15  in the Appendix for examples). Thus, the perceptual judgments indicate the shared visual appearance properties, as opposed to requiring higher-level semantic knowledge about the image content.",
            "Given that the perceptual similarity dataset we use for finetuning contains image-level similarity judgments, the consistent improvements that we observe on counting tasks, which requires local object awareness, is somewhat surprising. We hypothesize that the sensitivity of our human-aligned models to object counts may be a byproduct of NIGHTS examples themselves, many of which include image triplets with varying numbers of objects (see Fig.  15  in the Appendix). In terms of human perception, we note that humans consider object count when evaluating image similarity as soon as they develop counting profiency  [ 40 ] . Thus, it is possible that given the prevalence of triplets with object-count variations in NIGHTS, the human annotations naturally capture this counting aware effect in the global image-level labels and propagate this information to the human-aligned models.",
            "In Section  A.2  of the Appendix we find that the performance boost from NIGHTS over other datasets is also consistent across semantic segmentation and depth estimation. Conversely, tuning on NIGHTS fails to improve over other datasets on classification datasets; we further discuss this finding in Sections  5  and  A.1 .",
            "Why does fine-tuning on NIGHTS in particular lead to improvements? We hypothesize that the variations found in BAPPS and THINGS are solely high- or low-level, whereas the mid-level distortions in NIGHTS cover salient features that humans use when making inferences about what they see; these characteristics include style, pose, color, and count (see Fig. 15 ), and largely correlate with the characteristics a model must successfully extract for many computer vision tasks. Previous work  [ 18 ]  found that models fine-tuned on NIGHTS seem to attend to both low-level and semantic attributes. Aligning a feature space to these concepts may be useful for visual tasks requiring both visual and semantic knowledge, such as retrieval, counting, segmentation, etc. This hypothesis may also explain why tuning on NIGHTS hurts performance on fine-grained tasks, in which perceptually similar images may belong to different categories.",
            "Perceptual alignment does not appear to improve performance for standard image classification tasks such as natural image datasets in the VTAB benchmark  [ 60 ]  (see Tables  5 a-b in the Appendix). This is surprising in light of recent findings that demonstrate downstream task improvements in image classification tasks for human-aligned representations  [ 42 ] . Although it is hard to pinpoint the exact cause, we hypothesize two reasons: First, perceptual judgments at different levels of abstraction may be helpful for different downstream tasks. While the mid-level perceptual judgments in NIGHTS boost performance for retrieval-based and dense prediction tasks, they may not impart a useful inductive bias for standard image classification tasks; high-level semantic associations could simply be better suited for these kinds of tasks. Alternatively, the visual features that humans use to judge similarity may not be appropriately captured in classification accuracy metrics. Some VTAB datasets have numerical ground truth in which the distance from the correct answer is meaningful; thus, it may be ill-suited for classification, and better suited to continuous evaluations, which we report in sections of the paper.",
            "Additionally, a key insight from our dataset ablations is that not all human preferences improve performance. Finetuning on perceptual datasets with solely high-level (THINGS) or low-level (BAPPS) variations hurts performance for many downstream tasks (Section   4.5  and  A.2 ). Similarly,  Muttenthaler et al. [ 43 ]  recently discovered that finetuning vision models on THINGS can hurt downstream task transfer. Due to our evaluation of the synthetically-pretrained SynCLR  [ 55 ] , we attribute the drop in performance to perceptual alignment. Furthermore, by ablating the number of fine-tuning steps in Section  A.2 , we show that overfitting to perceptual judgments may also harm downstream performance.",
            "In Tables  5 a-b, we show classification results on all VTAB datasets for each backbone, and its human-aligned version (denoted with -HA). The VTAB datasets are divided into three categories: Natural, Specialized, and Structured. Natural datasets include standard vision benchmarks; specialized datasets include benchmarks captured through specialized imaging equipment, such as satellites and microscopes; structured datasets include benchmarks that focus on structure and layout, with both naturally-captured and simulated images.",
            "In Section  4.5  we show that tuning on NIGHTS leads to larger performance improvements on object counting and instance retrieval than training on other triplet datasets. Here, we show that this finding is consistent across dense prediction tasks as well. We evaluate models trained on NIGHTS, BAPPS, THINGS, and ImageNet  as described in section  4.5   on semantic segmentation and depth estimation. As shown in Fig.  9 -  10 , training on NIGHTS outperforms training on all other datasets.",
            "See Figure  15  for examples of the triplet datasets we train on in this paper. In each triplet, the reference image (middle) and outlined image are labeled as the similar pair. Perceptually-aligned models in this paper are tuned on NIGHTS triplets (top row), whose image variations encompass a variety of mid-level perceptual attributes including object count, identity, layout, subject pose, and color. In contrast, the datasets studied in our ablation encompass differing image attributes: BAPPS focuses on image patches with low-level distortions such as CNN artifacts and color jitter, THINGS encodes similarity in concept space, and our constructed ImageNet triplets outline class boundaries."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A1.SS1.12.12.5.5",
        "footnotes": [],
        "references": [
            "We finetune each backbone using Low-Rank Adaptation (LoRA), which was found to achieve better alignment performance and efficiency than full fine-tuning in  [ 18 ] . For more training and technical details see Sections  C.1  and  C.6  in the Appendix.",
            "We evaluate all base models and their human-aligned counterparts on the Consumer-to-Shop benchmark of the DeepFashion2 dataset  [ 21 ] . The benchmark consists of 10990 consumer \"in the wild images\" as the query set, and 21438 gallery images with matching clothing items to consumer images. Following the evaluation protocol from  [ 21 ] , we report Top-1, 3, and 5 accuracy in Table  4 . Human aligned models outperform base models by a significant margin across all metrics and backbones (visualized in Fig.  6 . In Fig.  7  we provide qualitative retrieval results. These results agree with prior work showing that training on NIGHTS improves performance in retrieving similar images to queries  [ 18 ] .",
            "We find that human-aligned models largely lead to worse performance on standard natural tasks. Results on specialized tasks are mixed; for most datasets, human alignment helps or hurts by a marginal amount. Results on structured datasets (Table  6 ) are also mixed, however on particular datasets such as sNORB, dSprites-Orientation, and dSprites-Location, alignment improves performance across backbones. We remark, however, that these structured datasets may have limited evaluation utility. The Clevr-Count task, for instance, has a numerical ground truth (i.e. the number of objects in an image) in which the distance from the correct answer is meaningful; thus it may be ill-suited to classification, and better suited to continuous evaluations such as RMSE or MAE, which we report in Section  4.3 ."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.T6.9.9.9",
        "footnotes": [],
        "references": [
            "We evaluate all base models and their human-aligned counterparts on the Consumer-to-Shop benchmark of the DeepFashion2 dataset  [ 21 ] . The benchmark consists of 10990 consumer \"in the wild images\" as the query set, and 21438 gallery images with matching clothing items to consumer images. Following the evaluation protocol from  [ 21 ] , we report Top-1, 3, and 5 accuracy in Table  4 . Human aligned models outperform base models by a significant margin across all metrics and backbones (visualized in Fig.  6 . In Fig.  7  we provide qualitative retrieval results. These results agree with prior work showing that training on NIGHTS improves performance in retrieving similar images to queries  [ 18 ] .",
            "We assess the effects of perceptual alignment with NIGHTS on two popular CNNs: ResNet50 and ConvNeXt-B, using the same loss described in Section  3.2 . We evaluate on counting and instance retrieval tasks; our results are shown in Tables  7 a-b. Similarly to ViTs, human-aligned CNNs outperform their pretrained counterparts across both tasks. Note that we report results for both ConvNeXt and ResNet, however acknowledge that the accuracies for ResNet are likely too small to draw conclusions from."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.T7.2.1.1",
        "footnotes": [],
        "references": [
            "For all three datasets, we apply the same training settings as with the original LoRA-tuning on NIGHTS. See Fig.  8  for dataset ablations on object counting and instance retrieval."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.T7.3.1.1",
        "footnotes": [],
        "references": [
            "In Section  4.5  we show that tuning on NIGHTS leads to larger performance improvements on object counting and instance retrieval than training on other triplet datasets. Here, we show that this finding is consistent across dense prediction tasks as well. We evaluate models trained on NIGHTS, BAPPS, THINGS, and ImageNet  as described in section  4.5   on semantic segmentation and depth estimation. As shown in Fig.  9 -  10 , training on NIGHTS outperforms training on all other datasets."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A3.SS7.p1.2",
        "footnotes": [],
        "references": [
            "In Section  4.5  we show that tuning on NIGHTS leads to larger performance improvements on object counting and instance retrieval than training on other triplet datasets. Here, we show that this finding is consistent across dense prediction tasks as well. We evaluate models trained on NIGHTS, BAPPS, THINGS, and ImageNet  as described in section  4.5   on semantic segmentation and depth estimation. As shown in Fig.  9 -  10 , training on NIGHTS outperforms training on all other datasets."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A3.SS7.p1.3",
        "footnotes": [],
        "references": [
            "We additionally run this ablation on a subset of VTAB (see Fig.  11 a). On these classification datasets, base models perform best; the exception is sNORB (pose prediction) for which NIGHTS is best. Amongst perceptual datasets, NIGHTS is sometimes outperformed by BAPPS/ImageNet. This result is consistent with our findings in Section  A.1 , that tuning on NIGHTS often fails to improve classification performance.",
            "Finally, we ablate the  strength  of alignment  i.e. the training loss when fine-tuning on different perceptual datasets. In Fig.  11 b we show the downstream performance on DeepFashion2 when fine-tuning for increasing numbers of steps on different datasets. Tuning on NIGHTS outperforms other datasets over the full training trajectory. Moreover, while performance rises significantly with a small amount of alignment to NIGHTS, it trends down after >1000 steps. This indicates that a small amount of alignment is helpful, however overfitting may harm performance."
        ]
    },
    "global_footnotes": []
}