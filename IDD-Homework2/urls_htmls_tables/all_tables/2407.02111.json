{
    "PAPER'S NUMBER OF TABLES": 2,
    "S4.T1": {
        "caption": "TABLE I: Small DNN architecture.",
        "table": "<table id=\"S4.T1.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T1.3.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T1.3.4.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t\">Layer</th>\n<th id=\"S4.T1.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Size</th>\n<th id=\"S4.T1.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\">Activation</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T1.1.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.1.1.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Convolutional layer 1</td>\n<td id=\"S4.T1.1.1.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">16 kernels 3<math id=\"S4.T1.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T1.1.1.1.m1.1a\"><mo id=\"S4.T1.1.1.1.m1.1.1\" xref=\"S4.T1.1.1.1.m1.1.1.cmml\">Ã—</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.1.1.1.m1.1b\"><times id=\"S4.T1.1.1.1.m1.1.1.cmml\" xref=\"S4.T1.1.1.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.1.1.1.m1.1c\">\\times</annotation></semantics></math>3</td>\n<td id=\"S4.T1.1.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">ReLu</td>\n</tr>\n<tr id=\"S4.T1.2.2\" class=\"ltx_tr\">\n<td id=\"S4.T1.2.2.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Convolutional layer 2</td>\n<td id=\"S4.T1.2.2.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">64 kernels 3<math id=\"S4.T1.2.2.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T1.2.2.1.m1.1a\"><mo id=\"S4.T1.2.2.1.m1.1.1\" xref=\"S4.T1.2.2.1.m1.1.1.cmml\">Ã—</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.2.2.1.m1.1b\"><times id=\"S4.T1.2.2.1.m1.1.1.cmml\" xref=\"S4.T1.2.2.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.2.2.1.m1.1c\">\\times</annotation></semantics></math>3</td>\n<td id=\"S4.T1.2.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">ReLu</td>\n</tr>\n<tr id=\"S4.T1.3.3\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.3.2\" class=\"ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t\">Convolutional layer 3</td>\n<td id=\"S4.T1.3.3.1\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">128 kernels 3<math id=\"S4.T1.3.3.1.m1.1\" class=\"ltx_Math\" alttext=\"\\times\" display=\"inline\"><semantics id=\"S4.T1.3.3.1.m1.1a\"><mo id=\"S4.T1.3.3.1.m1.1.1\" xref=\"S4.T1.3.3.1.m1.1.1.cmml\">Ã—</mo><annotation-xml encoding=\"MathML-Content\" id=\"S4.T1.3.3.1.m1.1b\"><times id=\"S4.T1.3.3.1.m1.1.1.cmml\" xref=\"S4.T1.3.3.1.m1.1.1\"></times></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T1.3.3.1.m1.1c\">\\times</annotation></semantics></math>3</td>\n<td id=\"S4.T1.3.3.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\">ReLu</td>\n</tr>\n<tr id=\"S4.T1.3.5.1\" class=\"ltx_tr\">\n<td id=\"S4.T1.3.5.1.1\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t\">Fully connected layer</td>\n<td id=\"S4.T1.3.5.1.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">10 neurons</td>\n<td id=\"S4.T1.3.5.1.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\">Softmax</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "In order to better isolate the impact of the FL dynamics on the traitor tracing capabilities of the scheme, the experiments described in this work follow the same configuration as those presented in ",
                "[",
                "17",
                "]",
                ". Specifically, we will consider an image classification task on the MNIST dataset ",
                "[",
                "21",
                "]",
                " with a small DNN architecture, which is described in ",
                "Table",
                "Â ",
                "I",
                ", as a toy example. For all purposes, the output class ",
                "y",
                "i",
                "subscript",
                "ğ‘¦",
                "ğ‘–",
                "y_{i}",
                " will be considered as the highest-value output neuron, regardless of the soft value of the vector."
            ]
        ]
    },
    "S4.T2": {
        "caption": "TABLE II: Number of false negatives for Vanilla WM in 500 random collusions.",
        "table": "<table id=\"S4.T2.3\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S4.T2.3.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.3.4\" class=\"ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"></th>\n<th id=\"S4.T2.1.1.1\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><math id=\"S4.T2.1.1.1.m1.1\" class=\"ltx_Math\" alttext=\"j\\in\\mathcal{C}^{1}\" display=\"inline\"><semantics id=\"S4.T2.1.1.1.m1.1a\"><mrow id=\"S4.T2.1.1.1.m1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.cmml\"><mi id=\"S4.T2.1.1.1.m1.1.1.2\" xref=\"S4.T2.1.1.1.m1.1.1.2.cmml\">j</mi><mo id=\"S4.T2.1.1.1.m1.1.1.1\" xref=\"S4.T2.1.1.1.m1.1.1.1.cmml\">âˆˆ</mo><msup id=\"S4.T2.1.1.1.m1.1.1.3\" xref=\"S4.T2.1.1.1.m1.1.1.3.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.T2.1.1.1.m1.1.1.3.2\" xref=\"S4.T2.1.1.1.m1.1.1.3.2.cmml\">ğ’</mi><mn id=\"S4.T2.1.1.1.m1.1.1.3.3\" xref=\"S4.T2.1.1.1.m1.1.1.3.3.cmml\">1</mn></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.1.1.1.m1.1b\"><apply id=\"S4.T2.1.1.1.m1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1\"><in id=\"S4.T2.1.1.1.m1.1.1.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.1\"></in><ci id=\"S4.T2.1.1.1.m1.1.1.2.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.2\">ğ‘—</ci><apply id=\"S4.T2.1.1.1.m1.1.1.3.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T2.1.1.1.m1.1.1.3.1.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3\">superscript</csymbol><ci id=\"S4.T2.1.1.1.m1.1.1.3.2.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3.2\">ğ’</ci><cn type=\"integer\" id=\"S4.T2.1.1.1.m1.1.1.3.3.cmml\" xref=\"S4.T2.1.1.1.m1.1.1.3.3\">1</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.1.1.1.m1.1c\">j\\in\\mathcal{C}^{1}</annotation></semantics></math></th>\n<th id=\"S4.T2.2.2.2\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><math id=\"S4.T2.2.2.2.m1.1\" class=\"ltx_Math\" alttext=\"j\\in\\mathcal{C}^{2}\" display=\"inline\"><semantics id=\"S4.T2.2.2.2.m1.1a\"><mrow id=\"S4.T2.2.2.2.m1.1.1\" xref=\"S4.T2.2.2.2.m1.1.1.cmml\"><mi id=\"S4.T2.2.2.2.m1.1.1.2\" xref=\"S4.T2.2.2.2.m1.1.1.2.cmml\">j</mi><mo id=\"S4.T2.2.2.2.m1.1.1.1\" xref=\"S4.T2.2.2.2.m1.1.1.1.cmml\">âˆˆ</mo><msup id=\"S4.T2.2.2.2.m1.1.1.3\" xref=\"S4.T2.2.2.2.m1.1.1.3.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.T2.2.2.2.m1.1.1.3.2\" xref=\"S4.T2.2.2.2.m1.1.1.3.2.cmml\">ğ’</mi><mn id=\"S4.T2.2.2.2.m1.1.1.3.3\" xref=\"S4.T2.2.2.2.m1.1.1.3.3.cmml\">2</mn></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.2.2.2.m1.1b\"><apply id=\"S4.T2.2.2.2.m1.1.1.cmml\" xref=\"S4.T2.2.2.2.m1.1.1\"><in id=\"S4.T2.2.2.2.m1.1.1.1.cmml\" xref=\"S4.T2.2.2.2.m1.1.1.1\"></in><ci id=\"S4.T2.2.2.2.m1.1.1.2.cmml\" xref=\"S4.T2.2.2.2.m1.1.1.2\">ğ‘—</ci><apply id=\"S4.T2.2.2.2.m1.1.1.3.cmml\" xref=\"S4.T2.2.2.2.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T2.2.2.2.m1.1.1.3.1.cmml\" xref=\"S4.T2.2.2.2.m1.1.1.3\">superscript</csymbol><ci id=\"S4.T2.2.2.2.m1.1.1.3.2.cmml\" xref=\"S4.T2.2.2.2.m1.1.1.3.2\">ğ’</ci><cn type=\"integer\" id=\"S4.T2.2.2.2.m1.1.1.3.3.cmml\" xref=\"S4.T2.2.2.2.m1.1.1.3.3\">2</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.2.2.2.m1.1c\">j\\in\\mathcal{C}^{2}</annotation></semantics></math></th>\n<th id=\"S4.T2.3.3.3\" class=\"ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\"><math id=\"S4.T2.3.3.3.m1.1\" class=\"ltx_Math\" alttext=\"j\\in\\mathcal{C}^{6}\" display=\"inline\"><semantics id=\"S4.T2.3.3.3.m1.1a\"><mrow id=\"S4.T2.3.3.3.m1.1.1\" xref=\"S4.T2.3.3.3.m1.1.1.cmml\"><mi id=\"S4.T2.3.3.3.m1.1.1.2\" xref=\"S4.T2.3.3.3.m1.1.1.2.cmml\">j</mi><mo id=\"S4.T2.3.3.3.m1.1.1.1\" xref=\"S4.T2.3.3.3.m1.1.1.1.cmml\">âˆˆ</mo><msup id=\"S4.T2.3.3.3.m1.1.1.3\" xref=\"S4.T2.3.3.3.m1.1.1.3.cmml\"><mi class=\"ltx_font_mathcaligraphic\" id=\"S4.T2.3.3.3.m1.1.1.3.2\" xref=\"S4.T2.3.3.3.m1.1.1.3.2.cmml\">ğ’</mi><mn id=\"S4.T2.3.3.3.m1.1.1.3.3\" xref=\"S4.T2.3.3.3.m1.1.1.3.3.cmml\">6</mn></msup></mrow><annotation-xml encoding=\"MathML-Content\" id=\"S4.T2.3.3.3.m1.1b\"><apply id=\"S4.T2.3.3.3.m1.1.1.cmml\" xref=\"S4.T2.3.3.3.m1.1.1\"><in id=\"S4.T2.3.3.3.m1.1.1.1.cmml\" xref=\"S4.T2.3.3.3.m1.1.1.1\"></in><ci id=\"S4.T2.3.3.3.m1.1.1.2.cmml\" xref=\"S4.T2.3.3.3.m1.1.1.2\">ğ‘—</ci><apply id=\"S4.T2.3.3.3.m1.1.1.3.cmml\" xref=\"S4.T2.3.3.3.m1.1.1.3\"><csymbol cd=\"ambiguous\" id=\"S4.T2.3.3.3.m1.1.1.3.1.cmml\" xref=\"S4.T2.3.3.3.m1.1.1.3\">superscript</csymbol><ci id=\"S4.T2.3.3.3.m1.1.1.3.2.cmml\" xref=\"S4.T2.3.3.3.m1.1.1.3.2\">ğ’</ci><cn type=\"integer\" id=\"S4.T2.3.3.3.m1.1.1.3.3.cmml\" xref=\"S4.T2.3.3.3.m1.1.1.3.3\">6</cn></apply></apply></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S4.T2.3.3.3.m1.1c\">j\\in\\mathcal{C}^{6}</annotation></semantics></math></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S4.T2.3.4.1\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.4.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">No further attacks</th>\n<td id=\"S4.T2.3.4.1.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">\n<span id=\"S4.T2.3.4.1.2.1\" class=\"ltx_text ltx_font_bold\">0</span> (<span id=\"S4.T2.3.4.1.2.2\" class=\"ltx_text ltx_font_bold\">0%</span>)</td>\n<td id=\"S4.T2.3.4.1.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">497 (99.4%)</td>\n<td id=\"S4.T2.3.4.1.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">500 (100%)</td>\n</tr>\n<tr id=\"S4.T2.3.5.2\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.5.2.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Fine-tuning</th>\n<td id=\"S4.T2.3.5.2.2\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">421 (84.2%)</td>\n<td id=\"S4.T2.3.5.2.3\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">438 (87.6%)</td>\n<td id=\"S4.T2.3.5.2.4\" class=\"ltx_td ltx_align_center ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">500 (100%)</td>\n</tr>\n<tr id=\"S4.T2.3.6.3\" class=\"ltx_tr\">\n<th id=\"S4.T2.3.6.3.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">Pruning</th>\n<td id=\"S4.T2.3.6.3.2\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">359 (71.8%)</td>\n<td id=\"S4.T2.3.6.3.3\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">412 (82.4%)</td>\n<td id=\"S4.T2.3.6.3.4\" class=\"ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t\" style=\"padding-top:1.25pt;padding-bottom:1.25pt;\">500 (100%)</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": "",
        "references": [
            [
                "The evolution of the main task and watermarking for the different training strategies can be seen in ",
                "Figure",
                "Â ",
                "2",
                ", averaging the metrics of 10 random models out of the 100 data-owners, and indicating the protection threshold after the watermarked models (Vanilla WM, Dropout WM, Dropout & Limited WM) surpass the main task accuracy achievable by data-owners on their own, an average of 85.91%. As the white-box fingerprints are leak-resistant by design, no impact can be seen on ",
                "Figure",
                "Â ",
                "2b",
                ", and the projection is correctly embedded before the protection threshold in any case. For the black-box fingerprint, even though all strategies seem to accurately classify the shared trigger set, for the most part, after the threshold in ",
                "Figure",
                "Â ",
                "2c",
                ", this does not necessarily ensure the traitor tracing capabilities of the scheme. The work in ",
                "[",
                "17",
                "]",
                " also measures the violations to the MA (MAV), defined by the collusion outputting a class that is different to the colludersâ€™ labels, as",
                "where ",
                "ğ’³",
                "ğ’",
                "â€‹",
                "i",
                "subscript",
                "ğ’³",
                "ğ’",
                "ğ‘–",
                "\\mathcal{X}_{\\mathcal{C}i}",
                " is a set of all ",
                "x",
                "j",
                "â€‹",
                "i",
                "subscript",
                "ğ‘¥",
                "ğ‘—",
                "ğ‘–",
                "x_{ji}",
                " for data-owners ",
                "j",
                "âˆˆ",
                "ğ’",
                "ğ‘—",
                "ğ’",
                "j\\in\\mathcal{C}",
                ", and ",
                "[",
                "â‹…",
                "]",
                "delimited-[]",
                "â‹…",
                "[\\cdot]",
                " represents the Iverson bracket. Now, according to the experimental MAV on 10 random collusions ",
                "ğ’",
                "2",
                "superscript",
                "ğ’",
                "2",
                "\\mathcal{C}^{2}",
                " in ",
                "Figure",
                "Â ",
                "2d",
                ", one can see that after reaching 100% accuracy on the triggers, the Vanilla WM ends up overfitting their fingerprints to internal features that do not survive the collusion, and thus, are not compatible with traitor tracing. Additionally, the dropout regularization on the watermarking step seems to have a positive effect on the convergence of the main task in ",
                "Figure",
                "Â ",
                "2a",
                ", while the Vanilla WM has a noticeable impact on the modelsâ€™ accuracy.",
                "Unfortunately, this vulnerability to the collusion attack is not exclusive to our proposed scheme, and one can see a similar effect on ",
                "Figure",
                "Â ",
                "3",
                " if each data-owner ",
                "j",
                "ğ‘—",
                "j",
                " is assigned unique trigger-label pairs ",
                "(",
                "ğ’¯",
                "j",
                ",",
                "x",
                "j",
                ")",
                "subscript",
                "ğ’¯",
                "ğ‘—",
                "subscript",
                "x",
                "ğ‘—",
                "(\\mathcal{T}_{j},\\textbf{x}_{j})",
                ", as done in ",
                "[",
                "9",
                "]",
                ". Considering this, it is more efficient to use the same trigger set ",
                "ğ’¯",
                "ğ’¯",
                "\\mathcal{T}",
                " across all data-owners, which significantly reduces the number of necessary queries before an accusation ",
                "[",
                "17",
                "]",
                ".",
                "Exploring the hypothesis of ",
                "Section",
                "Â ",
                "IV-C",
                ", where black-box triggers in Vanilla WM could potentially be delegated to a small, weak, subset of neurons, ",
                "Figure",
                "Â ",
                "4",
                " shows the histogram of the flattened outputs ",
                "f",
                "c",
                "â€‹",
                "o",
                "â€‹",
                "n",
                "â€‹",
                "v",
                "â€‹",
                "3",
                "subscript",
                "f",
                "ğ‘",
                "ğ‘œ",
                "ğ‘›",
                "ğ‘£",
                "3",
                "\\textbf{f}_{conv3}",
                " of the third convolutional layer for different training strategies, after feeding a given data-ownerâ€™s network with the trigger set ",
                "ğ’¯",
                "ğ’¯",
                "\\mathcal{T}",
                ". It is evident that the number of activations greater than 0.1 for the Vanilla WM is extremely low when compared to the Independent Models, and that dropout regularization aids in the increase of salient neurons, making for a more robust fingerprint.",
                "In terms of the black-box accusation process, ",
                "Figure",
                "Â ",
                "5",
                " shows the minimum number of queries ",
                "t",
                "âˆ—",
                "superscript",
                "ğ‘¡",
                "t^{*}",
                ", out of the available ",
                "m",
                "ğ‘š",
                "m",
                " embedded triggers, required before an accusation can be made on a particular colluder, on 500 collusions randomly chosen from all 100 data-owners. Both Dropout WM and Dropout & Limited WM are able to catch a colluder using less than half of the available triggers, even when pruning 80% of the neurons. The repetition of the watermarking step in Dropout WM seems to have made it more robust to attacks, especially pruning, but depending on the application, the computational overhead may shift the balance in favor of Dropout & Limited WM, as the difference in ",
                "t",
                "âˆ—",
                "superscript",
                "ğ‘¡",
                "t^{*}",
                " is not too extreme. For a guilty data-owner ",
                "j",
                "âˆˆ",
                "ğ’",
                "6",
                "ğ‘—",
                "superscript",
                "ğ’",
                "6",
                "j\\in\\mathcal{C}^{6}",
                " and pruning of the merged model, the average ",
                "t",
                "âˆ—",
                "superscript",
                "ğ‘¡",
                "t^{*}",
                " is 97 and 140 respectively. While the Dropout and Dropout & Limited WM had no false negatives in the generated collusions, the same cannot be said about Vanilla WM, as can be seen in ",
                "Table",
                "Â ",
                "II",
                ". As expected, this strategy was unable to detect a single guilty participant in the majority of cases, unless the models were not attacked at all (i.e. only one guilty participant ",
                "j",
                "âˆˆ",
                "ğ’",
                "1",
                "ğ‘—",
                "superscript",
                "ğ’",
                "1",
                "j\\in\\mathcal{C}^{1}",
                " with no further attacks), showing that the direct implementation of the work in ",
                "[",
                "17",
                "]",
                " is not possible for an FL framework, unless one considers the potential effect of the fingerprint leaks on the MAV.",
                "For the white-box accusation, the projection ",
                "r",
                "j",
                "subscript",
                "ğ‘Ÿ",
                "ğ‘—",
                "r_{j}",
                " of the model weights over the data-owner basis ",
                "ğ’®",
                "ğ’®",
                "\\mathcal{S}",
                " can be seen in ",
                "Figure",
                "Â ",
                "6",
                ", for 500 random collusions of the 100 data-owners. Although there is a slight benefit to the Dropout WM strategy, as the dropout regulatization promotes more neurons to contribute to the alignment of the weights to the data-owner vector ",
                "s",
                "j",
                "subscript",
                "s",
                "ğ‘—",
                "\\textbf{s}_{j}",
                ", this benefit is not too significant on the practicality of the scheme. Still, in the most challenging scenario considered (with guilty data-owners ",
                "j",
                "âˆˆ",
                "ğ’",
                "6",
                "ğ‘—",
                "superscript",
                "ğ’",
                "6",
                "j\\in\\mathcal{C}^{6}",
                " and an additional pruning attack), and setting an accusation threshold of 0.11 to avoid false accusations, all strategies would achieve their ",
                "catch-all",
                " goal in all the cases considered."
            ]
        ]
    }
}