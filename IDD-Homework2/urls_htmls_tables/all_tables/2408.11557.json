{
    "id_table_1": {
        "caption": "Table 1:  Hyperparameters in experiment",
        "table": "S4.T1.1.1",
        "footnotes": [],
        "references": [
            "In our experimental setup, we utilized the Low-Rank Adaptation (LoRA) technique to fine-tune both the Llama2-7b and Llama3-8b models, which is a parameter-efficient fine-tuning method that introduces trainable low-rank matrices into each layer of a pre-trained model, significantly reducing the number of trainable parameters and making the fine-tuning process more efficient and less resource-intensive [ 22 ] . The approach allows for the adaptation of large pre-trained models to specific tasks while preserving their performance. The hyperparameters set in the experiment is shown in Table 1."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Evaluation results of LLM1 for extracting entity",
        "table": "S4.T2.1.1",
        "footnotes": [],
        "references": [
            "LLM1 significantly outperformed the baseline Chat-GPT model in all metrics, and the experimental results are shown in Table 2. For extracting research objects from questions, LLM1 achieved higher scores in BLEU, ROUGE1, METEOR, and BERTScore accuracy. Similarly, for extracting spectral methods mentioned in questions, LLM1 showed excellent performance. LLM1 also had significantly higher accuracy on both tasks.",
            "The accuracy of each method is calculated as follows: If the papers retrieved using the entity keyword-based retrieval method match the actual papers corresponding to the keyword, the accuracy is 100%. Then the average accuracy of all queries in the database is calculated as the final accuracy of each method. The experimental results are shown in Table 2."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Evaluation results of different knowledge retrieve methods",
        "table": "S4.T3.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_4": {
        "caption": "Table 4:  Evaluation results of LLM2 for generating response",
        "table": "S4.T4.1.1",
        "footnotes": [],
        "references": [
            "The performance of LLM2 models (particularly Llama3-8b and Llama2-13b) was evaluated using several key metrics, including BLEU, ROUGE, METEOR, BERTScore, and AI evaluation, and the experimental results are shown in Table 4. These results are compared with the baseline Chat-GPT model. Both Llama2-13b and Llama3-8b models were fine-tuned using the LoRA technique."
        ]
    },
    "global_footnotes": [
        "Corresponding author"
    ]
}