{
    "id_table_1": {
        "caption": "Table 1.  The tasks, benchmarks, methods, and metrics of GraphRAG.",
        "table": "S9.T1.5.5",
        "footnotes": [
            ". https://github.com/WENGSYX/CMCQA",
            "https://www.yelp.com/dataset/"
        ],
        "references": [
            "Graph Retrieval-Augmented Generation (GraphRAG)  (Edge et al . ,  2024 ; Hu et al . ,  2024 ; Mavromatis and Karypis,  2024 )  emerges as an innovative solution to address these challenges. Unlike traditional RAG, GraphRAG retrieves graph elements containing relational knowledge pertinent to a given query from a pre-constructed graph database, as depicted in Figure  1 . These elements may include nodes, triples, paths, or subgraphs, which are utilized to generate responses. GraphRAG considers the interconnections between texts, enabling a more accurate and comprehensive retrieval of relational information. Additionally, graph data, such as knowledge graphs, offer abstraction and summarization of textual data, thereby significantly shortening the length of the input text and mitigating concerns of verbosity. By retrieving subgraphs or graph communities, we can access comprehensive information to effectively address the QFS challenge by capturing the broader context and interconnections within the graph structure.",
            "Organization.  The rest of the survey is organized as follows: Section  2  compares related techniques, while Section  3  outlines the general process of GraphRAG. Sections  5  to  7  categorize the techniques associated with GraphRAGs three stages: G-Indexing, G-Retrieval, and G-Generation. Section  8  introduces the training strategies of retrievers and generators. Section  9  summarizes GraphRAGs downstream tasks, corresponding benchmarks, application domains, evaluation metrics, and industrial GraphRAG systems. Section  10  provides an outlook on future directions. Finally, Section  11  concludes the content of this survey.",
            "As discussed in Section  1 , domain-specific knowledge graphs are crucial for enhancing LLMs in addressing domain-specific questions. These KGs offer specialized knowledge in particular fields, aiding models in gaining deeper insights and a more comprehensive understanding of complex professional relationships.  In the biomedical field, CMeKG 6 6 6 https://cmekg.pcl.ac.cn/  encompasses a wide range of data, including diseases, symptoms, treatments, medications, and relationships between medical concepts. CPubMed-KG 7 7 7 https://cpubmed.openi.org.cn/graph/wiki  is a medical knowledge database in Chinese, building on the extensive repository of biomedical literature in PubMed.  In the movie domain, Wiki-Movies  (Miller et al . ,  2016 )  extracts structured information from Wikipedia articles related to films, compiling data about movies, actors, directors, genres, and other relevant details into a structured format. Additionally,      Jin et al .  ( 2024b )    construct a dataset named GR-Bench, which includes five domain knowledge graphs spanning academic, E-commerce, literature, healthcare, and legal fields.  Furthermore,      He et al .  ( 2024 )    convert triplet-format and JSON files from ExplaGraphs and SceneGraphs into a standard graph format and selects questions requiring 2-hop reasoning from WebQSP to create the universal graph-format dataset GraphQA for evaluating GraphRAG systems.",
            "In this section, we will summarize the downstream tasks, application domains, benchmarks and metrics, and industrial applications related to GraphRAG. Table  1  collects existing GraphRAG techniques, categorizing them by downstream tasks, benchmarks, methods, and evaluation metrics. This table serves as a comprehensive overview, highlighting the various aspects and applications of GraphRAG technologies across different domains.",
            "The benchmarks used to evaluate the performance of the GraphRAG system can be divided into two categories. The first category is the corresponding datasets of downstream tasks. We summarize the benchmarks and papers tested with them according to the classification in Section  9.1 , details of which are shown in Table  1 . The second category consists of benchmarks specifically designed for the GraphRAG systems. These benchmarks usually cover multiple task domains to provide a comprehensive test result. For example, STARK  (Wu et al . ,  2024c )  benchmarks LLM Retrieval on semi-structured knowledge bases covering three domains, including product search, academic paper search, and queries in precision medicine to access the capacity of current GraphRAG systems.      He et al .  ( 2024 )    propose a flexible question-answering benchmark targeting real-world textual graphs, named GraphQA, which is applicable to multiple applications including scene graph understanding, commonsense reasoning, and knowledge graph reasoning. Graph Reasoning Benchmark (GRBENCH)  (Jin et al . ,  2024b )  is constructed to facilitate the research of augmenting LLMs with graphs, which contains 1,740 questions that can be answered with the knowledge from 10 domain graphs. CRAG  (Yang et al . ,  2024b )  provides a structured query dataset, with additional mock APIs to  access information from underlying mock KGs to achieve fair comparison."
        ]
    },
    "global_footnotes": [
        "https://www.wikidata.org/",
        "http://www.freebase.be/",
        "https://www.dbpedia.org/",
        "https://yago-knowledge.org/",
        "https://conceptnet.io/",
        "https://cmekg.pcl.ac.cn/",
        "https://cpubmed.openi.org.cn/graph/wiki",
        ". https://github.com/WENGSYX/CMCQA",
        "https://www.yelp.com/dataset/",
        "https://github.com/microsoft/graphrag",
        "https://docs.llamaindex.ai/en/stable/ examples/index structs/knowledge graph/KnowledgeGraphDemo.html",
        "https://python.langchain.com/docs/use_cases/graph",
        "https://www.nebula-graph.io/posts/graph-RAG",
        "https://github.com/eosphoros-ai/DB-GPT",
        "https://github.com/neo4j/NaLLM",
        "https://github.com/neo4j-labs/llm-graph-builder"
    ]
}