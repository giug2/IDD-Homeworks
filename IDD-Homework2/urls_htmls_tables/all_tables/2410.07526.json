{
    "id_table_1": {
        "caption": "Table 1 :  An illustrative KGL-to-English dictionary.",
        "table": "S3.T1.16.16",
        "footnotes": [],
        "references": [
            "As illustrated in Figure  1 , to bridge this comprehension gap, we introduce an English-KGL dictionary, and the LLM is supposed to assemble new KG sentences using the provided linguistic building blocks. The basic elements of KGL, while different from our natural language, are familiar to the LLM as they are constructed from the pretrained token embeddings. We leverage a context retriever to retrieve the text information and relational information of a KGL token, which transforms the sequential token embeddings of its name into an embedding vector. Subsequently, we update the LLM token embedding layer with new KGL token embeddings. In the scoring layer, we also employ a KG score retriever to supplement the LLM with extra KG relational information for prediction.",
            "As depicted on the left side of Figure  1 , the architecture of a typical LLM can be divided into four main components:",
            "Here,  < < < kgl: Wendee Lee > > >  denotes the definitive KGL token (corresponding to  e i subscript e i e_{i} italic_e start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  in previous sections and Figure  1 ) assigned to the entity  Wendee Lee . We enrich the tokenizers vocabulary with all pertinent KGL tokens, thereby enabling it to translate these KGL tokens into token IDs, which append sequentially to the LLMs original vocabulary range. It is worth noting that we only provide at most one example KGL sentence for each KGL word. Our intention is to introduce the schematics of KGL sentences to the LLM, rather than leveraging augmented KG data for in-context learning. To mitigate potential biases, the example sentences are sampled randomly.",
            "We introduce Algorithm  1  to demonstrate the fine-tuning process of MKGL for KG completion. We first construct input instructions following Instruction  3.1  and tokenize them into IDs. For those in the score of original LLM vocabulary, their embeddings can be looked up from  T T {\\mathbf{T}} bold_T , while those of out of scope will be retrieved by our context retriever  R context subscript R context R_{\\textbf{context}} italic_R start_POSTSUBSCRIPT context end_POSTSUBSCRIPT . After assembling the input embeddings, we feed them to the LLM to obtain output hidden states and then obtain the scores from the score retriever  R score subscript R score R_{\\text{score}} italic_R start_POSTSUBSCRIPT score end_POSTSUBSCRIPT . Finally, we optimize MKGL by minimizing the constrastive loss  L L \\mathcal{L} caligraphic_L . The main hyper-parameter settings are summarized in Table  5 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The KG completion results on FB15k-237 and WN18RR. The best and second-best results are  boldfaced  and  underlined , respectively.    \\uparrow  : higher is better;    \\downarrow  : lower is better. -: unavailable entry.",
        "table": "S3.T2.12.8",
        "footnotes": [],
        "references": [
            "As illustrated in Figure  2 , the first step is to reduce the dimensionality of LLM token embeddings to lower computational demands during text and KG context aggregation. Inspired by LoRA  [ 12 ] , we leverage a projection matrix  W T  R d  r subscript W T superscript R d r {\\mathbf{W}}_{T}\\in\\mathbb{R}^{d\\times r} bold_W start_POSTSUBSCRIPT italic_T end_POSTSUBSCRIPT  blackboard_R start_POSTSUPERSCRIPT italic_d  italic_r end_POSTSUPERSCRIPT  to transform the token embedding matrix  T  R N  d T superscript R N d {\\mathbf{T}}\\in\\mathbb{R}^{N\\times d} bold_T  blackboard_R start_POSTSUPERSCRIPT italic_N  italic_d end_POSTSUPERSCRIPT  into a reduced space  R N  r superscript R N r \\mathbb{R}^{N\\times r} blackboard_R start_POSTSUPERSCRIPT italic_N  italic_r end_POSTSUPERSCRIPT :",
            "The knowledge graph completion results are presented in Table  2 . MKGL outperforms other baselines in nearly all metrics. Notably, MKGL and KICGPT significantly surpass other LLM-based methods, demonstrating the importance of KG relational information. Contrarily, many BERT-based methods fall short against GNN-based methods, suggesting that merely incorporating text information may not yield the anticipated benefits. In summary, the proposed MKGL clearly outshines its counterparts, particularly those founded on commercial LLMs."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  The inductive KG completion results on FB15k-237-ind and WN18RR-ind (v1). The results on other subsets can be found in Appendix  F .",
        "table": "S4.T3.6.6",
        "footnotes": [],
        "references": [
            "To our knowledge, existing LLM-based methods have not addressed the inductive KG completion challenge. We benchmark MKGL against the state-of-the-art inductive methods. Although we can not assess KICGPT  [ 10 ]  due to unavailable source code, it is worth mentioning that our MKGL could potentially augment KICGPT by supplying a candidate list, facilitating seamless integration between the two methods. We present the results in Table  3 . MKGL significantly outperforms all the baseline methods across metrics. As most inductive reasoning methods do not have an embedding module for entities, the proposed MKGL represents the first embedding-based method to deliver state-of-the-art results in inductive KG completion.",
            "The results are shown in Figure  3 . The left section contrasts the sequence prediction results against standard KG completion, revealing only a modest loss in performance. MKGL still outperforms many conventional methods, especially on WN18RR dataset. The right panel displays sample sentences generated by MKGL, illustrating its potential to discover legitimate KGL sentences absent from the existing KG. We observe that WN18RR is more difficult than anticipated as it contains many plausible entities that challenge even an LLMs discernment.",
            "We introduce Algorithm  1  to demonstrate the fine-tuning process of MKGL for KG completion. We first construct input instructions following Instruction  3.1  and tokenize them into IDs. For those in the score of original LLM vocabulary, their embeddings can be looked up from  T T {\\mathbf{T}} bold_T , while those of out of scope will be retrieved by our context retriever  R context subscript R context R_{\\textbf{context}} italic_R start_POSTSUBSCRIPT context end_POSTSUBSCRIPT . After assembling the input embeddings, we feed them to the LLM to obtain output hidden states and then obtain the scores from the score retriever  R score subscript R score R_{\\text{score}} italic_R start_POSTSUBSCRIPT score end_POSTSUBSCRIPT . Finally, we optimize MKGL by minimizing the constrastive loss  L L \\mathcal{L} caligraphic_L . The main hyper-parameter settings are summarized in Table  5 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Ablation studies on FB15k-237 and WN18RR.",
        "table": "S4.T4.16.16",
        "footnotes": [],
        "references": [
            "The practice of incorporating supplementary context information alongside instructional prompts, known as in-context learning (ICL), has proven effective in enhancing performance across many NLP tasks  [ 43 ,  44 ] . However, the concatenation of retrieved context on KGs with the input text can easily exceed the input length constraints of LLMs. Processing such long input sequences remains computationally intensive even with truncation. To address these constraints, we propose an alternative approach to encode context information into compact vector representations. Our experiments in Section  4.6  also demonstrate its superiority in terms of both efficiency and performance.",
            "We conduct ablation studies to assess the importance of each module, as detailed in Table  4 . The unmarked cells indicate that we either substitute the text retrieval module with a learnable embedding module or remove the KG retrieval module. Clearly, the method with complete features achieves best results, while replacing or removing either module significantly impacts performance. Notably, removing the KG retrieval module yields more performance loss on WN18RR, as many entities in this dataset have similar names. For example, there are  14 14 14 14  different entities named call. In this case, incorporating with KG information becomes necessary.",
            "We examine the computational efficiency of our method (MKGL) relative to in-context baselines. Specifically, we develop several variants: LLM randomly-initialized new entity token embeddings (NewToken), LLM with KGL context from 1-hop neighbors (NewToken (1-hop)), LLM with KGL context from 2-hop neighbors (NewToken (2-hop)), and MKGL without score retriever (MKGL w/o SR). The results are shown in Figure  4 . MKGL surpasses all alternatives in performance. NewToken variants slightly lag behind MKGL w/o SR, but notably, our proposed methods demand fewer trainable parameters than NewToken variants. By encoding all context information within KGL token embeddings, the average input length is significantly reduced, which decreases training time considerably. Moreover, MKGL supports larger batch sizes during both training and inference phases, enhancing computational efficiency."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Hyper-parameter settings in the main experiments.",
        "table": "A4.T5.3.1",
        "footnotes": [],
        "references": [
            "We introduce Algorithm  1  to demonstrate the fine-tuning process of MKGL for KG completion. We first construct input instructions following Instruction  3.1  and tokenize them into IDs. For those in the score of original LLM vocabulary, their embeddings can be looked up from  T T {\\mathbf{T}} bold_T , while those of out of scope will be retrieved by our context retriever  R context subscript R context R_{\\textbf{context}} italic_R start_POSTSUBSCRIPT context end_POSTSUBSCRIPT . After assembling the input embeddings, we feed them to the LLM to obtain output hidden states and then obtain the scores from the score retriever  R score subscript R score R_{\\text{score}} italic_R start_POSTSUBSCRIPT score end_POSTSUBSCRIPT . Finally, we optimize MKGL by minimizing the constrastive loss  L L \\mathcal{L} caligraphic_L . The main hyper-parameter settings are summarized in Table  5 .",
            "We present additional examples of KGL modeling in Table  5 , which demonstrates that MKGL can not only generate KGL sentences seen during training but also produce previously unseen triplets within the testing set."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Dataset statistics.",
        "table": "A5.T6.1.1",
        "footnotes": [],
        "references": [
            "The practice of incorporating supplementary context information alongside instructional prompts, known as in-context learning (ICL), has proven effective in enhancing performance across many NLP tasks  [ 43 ,  44 ] . However, the concatenation of retrieved context on KGs with the input text can easily exceed the input length constraints of LLMs. Processing such long input sequences remains computationally intensive even with truncation. To address these constraints, we propose an alternative approach to encode context information into compact vector representations. Our experiments in Section  4.6  also demonstrate its superiority in terms of both efficiency and performance.",
            "We use the following benchmark datasets to evaluate the performance of MKGL, and summarize the statistics in Table  6 :",
            "We conduct experiments to analyze the influence of layer numbers in the KGL retrievers. The results are illustrated in Figure  6 . Clearly, increasing the number of layers enhances performance across all datasets and metrics. Additionally, we observe that a small number of layers (i.e.,  2 2 2 2 ) significantly impairs performance."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  The detailed inductive KG completion results, where v1-v4 represent four different subsets.",
        "table": "A5.T7.24.24",
        "footnotes": [],
        "references": [
            "We present detailed results on all inductive KG completion benchmarks in Table  7 , where MKGL consistently and significantly outperforms all state-of-the-art baselines.",
            "We conduct experiments to explore the impact of different encoders in the retrievers. The results are depicted in Figure  7 . We find that the MKGL is not highly sensitive to the choice of encoders. The performance when using GAT  [ 50 ]  is slightly lower than when using PNA."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Detailed results of Table 2. The KG completion results on FB15k-237 and WN18RR. The best and second-best results are  boldfaced  and  underlined , respectively.    \\uparrow  : higher is better;    \\downarrow  : lower is better. -: unavailable entry.",
        "table": "A6.T8.20.16",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": []
}