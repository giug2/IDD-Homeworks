{
    "S4.T1": {
        "caption": "Table 1: Test Accuracy of RNNs, BRNNs, LSTM",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S4.T1.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.1\">RNNs-Related Models</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S4.T1.1.1.1.2\">Test Accuracy</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S4.T1.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.1\">RNNs</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S4.T1.1.2.1.2\">53.33%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.1\">BRNNs</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S4.T1.1.3.2.2\">48.89%</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S4.T1.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.1.4.3.1\">LSTM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S4.T1.1.4.3.2\">51.11%</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Table 1 compares the test accuracy of three RNNs-related models: RNNs, BRNNs, and LSTM. Surprisingly, RNNs achieved the highest test accuracy at 53.33%. Initially, we anticipated that LSTM would perform the best. However, the results shown in Table 1 may be influenced by our small datasets, which contain only 900 samples in total\u2014720 for training and 90 each for evaluation and testing."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Test Accuracy of RNNs, BRNNs, LSTM between 900 and Augmented Data",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S5.T2.1\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.1.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.1\">RNNs-Related Models</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.2\">Test Accuracy(Original)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S5.T2.1.1.1.3\">Test Accuracy(Augmented)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S5.T2.1.2.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.2.1.1\">RNNs</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.2.1.2\">53.33%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S5.T2.1.2.1.3\">64.03% &#8593;</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.3.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.3.2.1\">BRNNs</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.3.2.2\">48.89%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S5.T2.1.3.2.3\">58.33% &#8593;</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S5.T2.1.4.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.1\">LSTM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.2\">51.11%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S5.T2.1.4.3.3\">65.97% &#8593;</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "Through the three plots about evaluation loss versus folds numbers shown in Figure 10, we can notice that reaching the 5th fold, BRNNs show the lowest evaluation loss, then LSTM, then RNNs. From Table 2 below, we can notice that the test performance for all three models improved. From the original data to augmented data, the test accuracy improved by 20%, 19%, and 30% for RNNs, BRNNs, and LSTM, respectively. We fixed our model while applying the modifications to our dataset.",
            "In our analysis, represented through three plots of evaluation loss versus fold numbers, the BRNNs demonstrated the lowest evaluation loss by the 5th fold, followed by LSTM, and RNNs. As shown in Table 2, transitioning from the original to the augmented dataset significantly improved test accuracy: RNNs by 20%, BRNNs by 19%, and LSTMs by 30%. Throughout this evaluation, our model architecture remained constant, highlighting that improvements were solely due to the augmented dataset\u2019s enhanced quality and diversity. This confirms that dataset augmentation effectively boosts model performance."
        ]
    },
    "S6.T3": {
        "caption": "Table 3: Test Accuracy of RNNs, BRNNs, LSTM between 900 and 14,000 Data",
        "table": "<table class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\" id=\"S6.T3.3\">\n<thead class=\"ltx_thead\">\n<tr class=\"ltx_tr\" id=\"S6.T3.3.4.1\">\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T3.3.4.1.1\">RNNs-Related Models</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T3.3.4.1.2\">Test Accuracy (900)</th>\n<th class=\"ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt\" id=\"S6.T3.3.4.1.3\">Test Accuracy (14,000)</th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr class=\"ltx_tr\" id=\"S6.T3.1.1\">\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T3.1.1.2\">RNNs</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T3.1.1.3\">53.33%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_t\" id=\"S6.T3.1.1.1\">50.18% <math alttext=\"\\downarrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T3.1.1.1.m1.1\"><semantics id=\"S6.T3.1.1.1.m1.1a\"><mo id=\"S6.T3.1.1.1.m1.1.1\" stretchy=\"false\" xref=\"S6.T3.1.1.1.m1.1.1.cmml\">&#8595;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.1.1.1.m1.1b\"><ci id=\"S6.T3.1.1.1.m1.1.1.cmml\" xref=\"S6.T3.1.1.1.m1.1.1\">&#8595;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.1.1.1.m1.1c\">\\downarrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T3.1.1.1.m1.1d\">&#8595;</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.2.2\">\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.2.2.2\">BRNNs</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.2.2.3\">48.89%</td>\n<td class=\"ltx_td ltx_align_left\" id=\"S6.T3.2.2.1\">53.97% <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T3.2.2.1.m1.1\"><semantics id=\"S6.T3.2.2.1.m1.1a\"><mo id=\"S6.T3.2.2.1.m1.1.1\" stretchy=\"false\" xref=\"S6.T3.2.2.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.2.2.1.m1.1b\"><ci id=\"S6.T3.2.2.1.m1.1.1.cmml\" xref=\"S6.T3.2.2.1.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.2.2.1.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T3.2.2.1.m1.1d\">&#8593;</annotation></semantics></math>\n</td>\n</tr>\n<tr class=\"ltx_tr\" id=\"S6.T3.3.3\">\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S6.T3.3.3.2\">LSTM</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S6.T3.3.3.3\">51.11%</td>\n<td class=\"ltx_td ltx_align_left ltx_border_bb\" id=\"S6.T3.3.3.1\">52.61% <math alttext=\"\\uparrow\" class=\"ltx_Math\" display=\"inline\" id=\"S6.T3.3.3.1.m1.1\"><semantics id=\"S6.T3.3.3.1.m1.1a\"><mo id=\"S6.T3.3.3.1.m1.1.1\" stretchy=\"false\" xref=\"S6.T3.3.3.1.m1.1.1.cmml\">&#8593;</mo><annotation-xml encoding=\"MathML-Content\" id=\"S6.T3.3.3.1.m1.1b\"><ci id=\"S6.T3.3.3.1.m1.1.1.cmml\" xref=\"S6.T3.3.3.1.m1.1.1\">&#8593;</ci></annotation-xml><annotation encoding=\"application/x-tex\" id=\"S6.T3.3.3.1.m1.1c\">\\uparrow</annotation><annotation encoding=\"application/x-llamapun\" id=\"S6.T3.3.3.1.m1.1d\">&#8593;</annotation></semantics></math>\n</td>\n</tr>\n</tbody>\n</table>\n\n",
        "footnotes": [],
        "references": [
            "From Table 3 we can clearly see the test accuracy of RNNs decreased as the dataset becomes larger, whereas both Bidirectional RNNs and LSTM have an increased test accuracy as we have more data samples for training the models. For a larger dataset, we have BRNNs as the model with the best performance among the three, where LSTM leads the second place, and lastly the RNNs model. This proved our hypothesis that larger dataset performs better on more complex models, whereas RNNs cannot capture many complex details within larger datasets."
        ]
    }
}