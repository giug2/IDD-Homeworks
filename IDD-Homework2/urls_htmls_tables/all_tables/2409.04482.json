{
    "id_table_1": {
        "caption": "Table 1:  Quantitative results of continual learning. We calculate the PSNR at different training stages for each scene. For the first column, the bracketed texts indicate the name of  n t  h superscript n t h n^{th} italic_n start_POSTSUPERSCRIPT italic_t italic_h end_POSTSUPERSCRIPT  scene being trained for model  M n subscript M n M_{n} italic_M start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT . For the other column, bracketed values indicate the difference compared to previous stage.",
        "table": "S3.T1.19.19",
        "footnotes": [],
        "references": [
            "The radiance fields of different 3D scenes have significant domain gaps, so it is hard to adapt all parameters of a trained model to a coming up new scene, resulting in a degraded performance in either the previous scene or the new scene. Therefore, we propose to factorize the conventional MLP of the NeRF model into a set of scene-specific weight matrixes (SSWMs) and a cross-scene weight matrix (CSWM), which linearly combined with the SSWMs. Besides, to design a more memory-efficient network architecture for continual learning NeRF, we take inspiration from HyperNetworks  [ HDL16 ]  that use a single hypernetwork to generate the SSWMs for different scenes. Fig. 1  illustrates the overall flow of SCARF. Now, we introduce the details of the factorization of NeRF.",
            "We are the first work to continually learn multiple scenes of NeRF, to the best of our knowledge. So, our work focuses on whether continual learning of NeRF can maintain performance and whether the model parameters keep memory efficiency with the increasing number of coming scenes. We first evaluate our model on the NeRF-Synthetic dataset. The scenes (Hotdog, Lego, Ship, and Chair) are input into the model in sequence, and the ground truth label of previous scenes is unavailable when training the new scene. Tab. 1  shows the quantitative results of continual learning NeRF for 4 scenes in the NeRF-Synthetic dataset with PSNR and SSIM. Fig. 2  shows the corresponding qualitative results of the previous 3 scenes. We can see that high quality results can be achieved for previously learned scenes."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Comparison of the quantitative results of common continual learning methods combined with NeRF.",
        "table": "S3.T2.2",
        "footnotes": [],
        "references": [
            "c M t  1 i superscript subscript c subscript M t 1 i \\mathbf{c}_{M_{t-1}}^{i} bold_c start_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  and   M t  1 i superscript subscript  subscript M t 1 i \\mathbf{\\sigma}_{M_{t-1}}^{i} italic_ start_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  are the density and radiance random sampled from  M t  1 subscript M t 1 M_{t-1} italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  in scene  S i subscript S i S_{i} italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , while  c M t i ^ ^ superscript subscript c subscript M t i \\hat{\\mathbf{c}_{M_{t}}^{i}} over^ start_ARG bold_c start_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG  and   M t i ^ ^ superscript subscript  subscript M t i \\hat{\\mathbf{\\sigma}_{M_{t}}^{i}} over^ start_ARG italic_ start_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG  are corresponding the density and radiance sampled from  M t subscript M t M_{t} italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  in scene  S i subscript S i S_{i} italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT .    \\alpha italic_  is a hyperparameter balancing the weights of two regular terms. Moreover, we constrain the rendered pixel (using Eq. 2 )  C  ( r ) M t i ^ ^ C superscript subscript r subscript M t i \\hat{\\mathbf{C(r)}_{M_{t}}^{i}} over^ start_ARG bold_C ( bold_r ) start_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT end_ARG  from model  M t subscript M t M_{t} italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  close to  C  ( r ) M t  1 i C superscript subscript r subscript M t 1 i \\mathbf{C(r)}_{M_{t-1}}^{i} bold_C ( bold_r ) start_POSTSUBSCRIPT italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  from model  M t  1 subscript M t 1 M_{t-1} italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT . Unlike PVD, USD achieves the knowledge distillation of the NeRF model in a single stage. To this end, taking inspiration from multi-task learning  [ KGC17 ] , we introduce the uncertain loss and use the learnable self-balancing parameters to distill both the density-radiance field and the RGB pixel simultaneously. Two learnable parameters   1 subscript  1 \\beta_{1} italic_ start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT  and   2 subscript  2 \\beta_{2} italic_ start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT  are introduced to weigh the uncertainty and balance the weight between these two loss functions. Due to the large domain gap between  L c   i superscript subscript L c  i \\mathcal{L}_{\\mathbf{c\\sigma}}^{i} caligraphic_L start_POSTSUBSCRIPT bold_c italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT  and  L C  ( r ) i superscript subscript L C r i L_{C(r)}^{i} italic_L start_POSTSUBSCRIPT italic_C ( italic_r ) end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , we found that this is very necessary. Moreover, we observe that since the density field is sparse, the random sampling distillation strategy leads to sampling a large number of blank regions (positions where the corresponding density tends to zero), making it unable to distill previous knowledge into new models efficiently. Therefore, for the loss  L c   i superscript subscript L c  i \\mathcal{L}_{\\mathbf{c\\sigma}}^{i} caligraphic_L start_POSTSUBSCRIPT bold_c italic_ end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT , we pre-extract an explicit density occupancy grid for each learned scene and only sample the radiance field and on the surface, which the density   i subscript  i \\sigma_{i} italic_ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT  greater than a threshold    \\tau italic_ . Specifically,",
            "We are the first work to continually learn multiple scenes of NeRF, to the best of our knowledge. So, our work focuses on whether continual learning of NeRF can maintain performance and whether the model parameters keep memory efficiency with the increasing number of coming scenes. We first evaluate our model on the NeRF-Synthetic dataset. The scenes (Hotdog, Lego, Ship, and Chair) are input into the model in sequence, and the ground truth label of previous scenes is unavailable when training the new scene. Tab. 1  shows the quantitative results of continual learning NeRF for 4 scenes in the NeRF-Synthetic dataset with PSNR and SSIM. Fig. 2  shows the corresponding qualitative results of the previous 3 scenes. We can see that high quality results can be achieved for previously learned scenes.",
            "Tab. 2  shows the results of the quantitative analyses on LLFF and TanksAndTemple datasets, while Fig. 3  shows qualitative results of continual learning in four scenes of NeRF-Synthetic dataset. In contrast to traditional methods of knowledge distillation, we can learn new scenes while preserving the rendering quality of previous learned scenes. Besides, Fig.  4  and Fig.  5  show qualitative results of continual learning on TanksAndTemple (5 scenes) and LLFF (8 scenes) datasets, respectively."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Comparison with recent methods related to composition and compression NeRF. Our method achieves comparable rendering results with the most minimal model size for multiple scenes while enabling compression (CP) and continual learning (CL). Metrics (PSNR and SSIM) are averaged over the eight scenes. \"SCARF with HI\" indicates that the historical images of learned scenes is always accessible, which demonstrates the compression capability of our model.",
        "table": "S4.T3.3",
        "footnotes": [
            "",
            "",
            "",
            "",
            "",
            ""
        ],
        "references": [
            "We found that memory replay, a common solution for continual learning, on NeRF, i.e., direct distilling the knowledge from  M t  1 subscript M t 1 M_{t-1} italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  to  M t subscript M t M_{t} italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT  for previous 3D scenes, as shown in Fig. 3 , causes severe performance degradation. A recent work  [ FXW  23 ]  introduces a Progressive Volume Distillation (PVD) strategy to achieve the conversions of a single scene between different NeRF architectures, e.g., from iNGP  [ MESK22 ]  model to TensoRF  [ CXG  22 ]  model. However, we found that since PVD is a multi-stage distillation, it cannot be applied to the continual learning of multiple 3D scenes. Furthermore, for memory-compact multi-scene representation, PVD is overfitted in a single scene and thus fails to maintain high-quality rendering results of multiple scenes. So, we introduce a single-stage NeRF knowledge distillation method for continual learning NeRF to distill the knowledge of previous scenes from  M t  1 subscript M t 1 M_{t-1} italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  to  M t subscript M t M_{t} italic_M start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT , called Uncertain Surface radiance field Distillation (USD). PVD has shown that distilling the radiance field (radiance and density) and the rendered RGB pixels preserves better rendering quality in NeRF than directly distilling the network parameters. We similarly distill the knowledge from the model  M t  1 subscript M t 1 M_{t-1} italic_M start_POSTSUBSCRIPT italic_t - 1 end_POSTSUBSCRIPT  in the radiance field level and RGB pixel level, i.e.:",
            "Tab. 2  shows the results of the quantitative analyses on LLFF and TanksAndTemple datasets, while Fig. 3  shows qualitative results of continual learning in four scenes of NeRF-Synthetic dataset. In contrast to traditional methods of knowledge distillation, we can learn new scenes while preserving the rendering quality of previous learned scenes. Besides, Fig.  4  and Fig.  5  show qualitative results of continual learning on TanksAndTemple (5 scenes) and LLFF (8 scenes) datasets, respectively.",
            "We also compare our method with some recent works on composable and compression NeRF in Tab. 3 . We focus on the continual learning capability to facilitate practical applications, but not boosting the rendering quality over the previous state-of-the-art. Although the rendering performance of the proposed method is not the best, the memory-efficient model design with the extra continual learning capability is unique and enables various applications."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  Experiments on the impact of the number of learned scenes on rendering quality. \"CLSD\" refers to \"Continuous Learning in Single Dataset ,\" which involves continual learning within a single datasetspecifically, 5 scenes for the TanksAndTemples (T&T) dataset, and 8 scenes each for the LLFF and NeRF-Synthetic datasets. Conversely, \"CLCD\" stands for \" Continuous Learning Cross Dataset\", where learning occurs across the aforementioned datasets, encompassing a total of 21 scenes.",
        "table": "S4.T4.1.1",
        "footnotes": [],
        "references": [
            "As our work is the first to focus on continuous NeRF learning across multiple scenes, there is currently no established baseline for quantitatively analyzing how the number of learned scenes affects the rendering quality of previously learned scenes. We conduct two experiments to quantitatively explore the upper limit of learned scenes: continuous learning across datasets and repeated continuous learning within a single dataset using data augmentation. For continuous learning across datasets, we sequentially trained on the LLFF dataset (containing 8 scenes), the TanksAndTemples dataset (containing 5 scenes), and the NeRF-Synthetic dataset (containing 8 scenes), resulting in a total of 21 scenes. As a baseline for comparison, we also provide results of continuous learning within a single dataset. Tab. 4  demonstrates that as the number of scenes increases from 5 or 8 to 21, the rendering quality remains comparable. Additionally, we perform data augmentation on the NeRF-Synthetic dataset to generate multiple group of datasets with domain gaps but that should achieve similar rendering quality (PSNR) after training. This strategy allows us to continuous learning these augmented datasets group by group and observe changes in rendering quality, thus quantifying the robustness of SCARF to the increasing number of learning scenes. For each scene in the NeRF-Synthetic dataset, we apply the following transformations sequentially: flip the X-axis, flip the Y-axis, flip the Z-axis, swap the X-axis and Y-axis, swap the Y-axis and Z-axis, and swap the X-axis and Z-axis. Consequently, 6 new scenes with domain gaps are generated for each scene, producing 6 groups totaling 48 scenes. We learn each group of scenes sequentially and calculate the average metrics for all learned scenes. As shown in Tab. 5 , even with 48 learned scenes, each NeRF still demonstrates well rendering performance. This success is attributed to the disentanglement of scene-specific matrices and the cross-scene matrix, where the cross-scene matrix has learned generalizable features capable of handling various domains.",
            "Tab. 2  shows the results of the quantitative analyses on LLFF and TanksAndTemple datasets, while Fig. 3  shows qualitative results of continual learning in four scenes of NeRF-Synthetic dataset. In contrast to traditional methods of knowledge distillation, we can learn new scenes while preserving the rendering quality of previous learned scenes. Besides, Fig.  4  and Fig.  5  show qualitative results of continual learning on TanksAndTemple (5 scenes) and LLFF (8 scenes) datasets, respectively."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Experiments on the impact of the number of learned scenes on rendering quality with data augmentation for NeRF-Synthetic dataset. For each experimental group, we generated 8 novel scenes from the original dataset and performed continual learning on these augmented scenes.",
        "table": "S4.T5.3",
        "footnotes": [],
        "references": [
            "As our work is the first to focus on continuous NeRF learning across multiple scenes, there is currently no established baseline for quantitatively analyzing how the number of learned scenes affects the rendering quality of previously learned scenes. We conduct two experiments to quantitatively explore the upper limit of learned scenes: continuous learning across datasets and repeated continuous learning within a single dataset using data augmentation. For continuous learning across datasets, we sequentially trained on the LLFF dataset (containing 8 scenes), the TanksAndTemples dataset (containing 5 scenes), and the NeRF-Synthetic dataset (containing 8 scenes), resulting in a total of 21 scenes. As a baseline for comparison, we also provide results of continuous learning within a single dataset. Tab. 4  demonstrates that as the number of scenes increases from 5 or 8 to 21, the rendering quality remains comparable. Additionally, we perform data augmentation on the NeRF-Synthetic dataset to generate multiple group of datasets with domain gaps but that should achieve similar rendering quality (PSNR) after training. This strategy allows us to continuous learning these augmented datasets group by group and observe changes in rendering quality, thus quantifying the robustness of SCARF to the increasing number of learning scenes. For each scene in the NeRF-Synthetic dataset, we apply the following transformations sequentially: flip the X-axis, flip the Y-axis, flip the Z-axis, swap the X-axis and Y-axis, swap the Y-axis and Z-axis, and swap the X-axis and Z-axis. Consequently, 6 new scenes with domain gaps are generated for each scene, producing 6 groups totaling 48 scenes. We learn each group of scenes sequentially and calculate the average metrics for all learned scenes. As shown in Tab. 5 , even with 48 learned scenes, each NeRF still demonstrates well rendering performance. This success is attributed to the disentanglement of scene-specific matrices and the cross-scene matrix, where the cross-scene matrix has learned generalizable features capable of handling various domains.",
            "Tab. 2  shows the results of the quantitative analyses on LLFF and TanksAndTemple datasets, while Fig. 3  shows qualitative results of continual learning in four scenes of NeRF-Synthetic dataset. In contrast to traditional methods of knowledge distillation, we can learn new scenes while preserving the rendering quality of previous learned scenes. Besides, Fig.  4  and Fig.  5  show qualitative results of continual learning on TanksAndTemple (5 scenes) and LLFF (8 scenes) datasets, respectively."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Ablation studies of our method. PSNR and SSIM metrics are averaged over the eight scenes from the NeRF-Synthetic dataset, which are continually learned.",
        "table": "S4.T6.9.9",
        "footnotes": [],
        "references": [
            "Our ablation studies validate the algorithms design choice on the NeRF-Synthetic dataset in Tab. 6 . We implement continual learning of 8 scenes on the NeRF-Synthetic dataset. Rows 1-4 show our choice of dim of random noise  z z z italic_z  and hyperparameter  K K K italic_K . Only using 8 dims of the random noise  z z z italic_z  reduces performance, but increasing the number of the dims of random noise  z z z italic_z  to 32 does not improve performance. A larger dim coefficient matrix  C C C italic_C  can provide greater flexibility for modeling. However, larger  K K K italic_K  increases network parameters. So, we choose  K = 21 K 21 K=21 italic_K = 21  as a trade-off. Row 5 demonstrates the performance will drop sharply without the coefficient matrix  C C C italic_C . Whats more, we find that parameter generator  G ( . ) G(.) italic_G ( . )  not only reduces the number of parameters in the model but also improves the quality of the rendering in row 6. In rows 7-10, we also take continual learning without using the loss of  L c   subscript L c  \\mathcal{L}_{\\mathbf{c\\sigma}} caligraphic_L start_POSTSUBSCRIPT bold_c italic_ end_POSTSUBSCRIPT ,  L C  ( r ) subscript L C r \\mathcal{L}_{C(r)} caligraphic_L start_POSTSUBSCRIPT italic_C ( italic_r ) end_POSTSUBSCRIPT , uncertain learnable weight, or surface occupy grid distillation. The experimental results show that pixel-level distillation (with  L C  ( r ) subscript L C r \\mathcal{L}_{C(r)} caligraphic_L start_POSTSUBSCRIPT italic_C ( italic_r ) end_POSTSUBSCRIPT ) is the most crucial factor, as it efficiently optimizes multiple 3D sampling points along a ray like NeRF. Additionally, surface distillation of radiance and density at the 3D level is essential for further improving rendering quality.",
            "Furthermore, we demonstrate the ablation studies on the order of continual learning. As shown in Tab. 7 , conducting multiple sets of experiments that swapped the order in which scenes were learned, we found that the rendering quality (PSNR) of the latest scene would be better. However, after training about two scenes, the rendering quality of the previous scenes stabilizes. Moreover, as shown in Fig. 6 , the USD strategy made knowledge distillation more efficient and improved the RGB and depth accuracy of previous scenes."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Ablation studies on the order in which each scene is learned earlier. \"H\", \"L\", \"S\", and \"C\" are scenes of \"Hotdog\", \"Lego\", \"Ship\", and \"Chair\", respectively. The first column of the table shows the continual learning order.",
        "table": "S4.T7.28.28",
        "footnotes": [],
        "references": [
            "Furthermore, we demonstrate the ablation studies on the order of continual learning. As shown in Tab. 7 , conducting multiple sets of experiments that swapped the order in which scenes were learned, we found that the rendering quality (PSNR) of the latest scene would be better. However, after training about two scenes, the rendering quality of the previous scenes stabilizes. Moreover, as shown in Fig. 6 , the USD strategy made knowledge distillation more efficient and improved the RGB and depth accuracy of previous scenes."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  Ablation studies on the design choices on the decoder. Metrics (PSNR and SSIM) are averaged over the eight scenes on the NeRF-Synthetic dataset, which are continually learned.",
        "table": "S4.T8.2.2",
        "footnotes": [],
        "references": [
            "At first, we tried to generate the decoders parameters from another hypernetwork, just as we did with the encoder. As shown in Tab. 8 , we find that such a change does not improve the quality of the rendering. We considered that since the parameters of the encoder were generated through a global parameter generation network, in which the encoded high-dimensional features are generalizable, the use of a global decoder across scenes is sufficient. For simplicity yet efficient purposes, we use a global decoder."
        ]
    },
    "id_table_9": {
        "caption": "Table A:  Quantitative results of continual learning on TanksAndTemples dataset. We calculate the PSNR and SSIM after continual learning five scenes in sequence.",
        "table": "A4.T1.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_10": {
        "caption": "Table B:  Quantitative results of continual learning on LLFF dataset. We calculate the PSNR and SSIM after continual learning eight scenes in sequence.",
        "table": "A4.T2.1.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table C:  Quantitative results of continual learning on NeRF-Synthetic dataset. We calculate the PSNR and SSIM after continual learning eight scenes in sequence.",
        "table": "A4.T3.1.1",
        "footnotes": [],
        "references": []
    }
}