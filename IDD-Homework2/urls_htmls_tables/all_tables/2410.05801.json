{
    "id_table_1": {
        "caption": "Table 1:  Verification Criteria",
        "table": "S2.T1.8",
        "footnotes": [],
        "references": [
            "Nevertheless, previous RAG methods still confront numerous factual issues, which may be attributed to the following two aspects (Figure  1 ):",
            "To provide a comprehensive understanding of CoV-RAG, we present the inference in Algorithm  1 .",
            "Retrieval Augmented Generation  Following Equation ( 1 ), the retriever  R R \\mathit{R} italic_R  retrieves references  k k \\mathcal{k} caligraphic_k  based on the question  x x \\mathcal{x} caligraphic_x   Liu et al. ( 2023 ) . Then, the model of CoV-RAG  M M \\mathit{M} italic_M  predicts an answer  y ^ ^ y \\mathcal{\\hat{y}} over^ start_ARG caligraphic_y end_ARG  using both the question and the references.",
            "Neg. RAG Augmentation : To enhance the diversity and robustness of the training data, we utilize ChatGPT to synthesize additional negative answers on criteria in Table  1  from  D 2 subscript D 2 \\mathit{D_{2}} italic_D start_POSTSUBSCRIPT italic_2 end_POSTSUBSCRIPT . The main types of negative answers included:",
            "Step 2: Verification Data Synthesis  Based on criteria in Table  1 , GPT-4 assesses  D 2  superscript subscript D 2  \\mathit{{D_{2}}^{\\prime}} italic_D start_POSTSUBSCRIPT italic_2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  provided by step 1, producing both negative and positive RAG data with rationale, and continues updating  D 2  superscript subscript D 2  \\mathit{D_{2}}^{\\prime} italic_D start_POSTSUBSCRIPT italic_2 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT  end_POSTSUPERSCRIPT  with chain-of-verification data. For example:",
            "In the context of Question-Answering (QA) tasks based on the Retrieval-Augmented Generation (RAG) framework, we have designed a set of actions aimed at enabling the model to introspect and evaluate the effectiveness of the retrieved references and the answers generated by the generator. Further details can be found in Table  9 , Table  10 , Table  11 , Table  12 .",
            "An example of retrieved references from CoV-RAG is shown in Table  13 .",
            "An example of Question Answering from CoV-RAG is shown in Table  14 .",
            "An example of Verification for Question Answering in CoV-RAG is shown in Table  15 .",
            "An example of Multi-Iteration Question Answering in CoV-RAG is shown in Table  16 .",
            "To enhance the assessment of the quality of our Question-Answer system, we conducted an Automatic Evaluation to evaluate the quality of our responses across multiple scoring dimensions. As shown in Table  18 , GPT-4 was employed to compare and rank our method (CoV-RAG) against WebGLM in GLM-10b and Llama2-13b based on various scoring criteria, ranging from superior to inferior. The final ranking is shown in Table  3 , and a case is shown in Table  17 ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  The table presents accuracy for RAG methods, including naive GPT3, Rewrite-Retrieve-Read(RRR), RAG with ChatGPT, Self-RAG, Perplexity.ai, WebGLM, and CoV-RAG. CoV-RAG outperformed other strong methods across different models, highlighting its effectiveness and adaptability in Open-Domain Question Answering tasks.",
        "table": "S2.T2.1",
        "footnotes": [],
        "references": [
            "To alleviate the aforementioned issues  Neeman et al. ( 2022 ); Mallen et al. ( 2023 ) , we present \"Retrieving, Rethinking, and Revising: The Chain-of-Verification Can Improve Retrieval Augmented Generation (CoV-RAG)\". This approach is illustrated in Figure  2 , where we detail the CoV-RAG that enhances the effectiveness of retrieval-augmented generation through a cohesive and unified chain of verification steps during both training and inference process. Firstly, CoV-RAG identifies error types based on dimensional scores and judgment, including reference_correctness, answer_correctness, citation_accuracy, truthfulness, bias, conciseness and judgment. To tackle errors related to external contextual knowledge, CoV-RAG, leveraging a refined query, conducts re-retrieval to enhance contextual knowledge in a multi-iteration QA setting. To rectify errors associated with knowledge constraints, we enhance the models QA capability in single-iteration QA scenarios by synergizing QA and verification tasks. This involves introducing the chain of verification during QA training, thereby incorporating negative samples of QA and elucidating the reasons for their errors by verification into the training regimen for generative models.",
            "As depicted in Figure  2 , model CoV-RAG, is composed of two foundational elements: the generator, and the chain-of-verification(CoV). By integrating CoV, we introduce a novel mechanism for enhancing the factuality and consistency in RAG.",
            "Our experiments validate CoV-RAGs effectiveness and adaptability, as shown in Table  2  and Figure  4 .",
            "Effectiveness  CoV-RAG outperforms popular methods, including naive LLMs (GPT-3), RAG models (ChatGPT with the same retrieval, Perplexity.ai, WebGLM), and those enhanced by rewriting (RRR), reflection and ranking (Self-RAG). This superiority is demonstrated across four datasets in open-domain question-answering tasks (Table  2 ). Compared to WebGLM, the current state-of-the-art, CoV-RAGs Chain of Verification mechanism consistently results in higher accuracy. Notably, CoV-RAG with ChatGLM2-6b achieved 72.2% accuracy, surpassing WebGLM with Vicuna-13b at 71.1%, demonstrating CoV-RAGs superior performance across different model sizes.",
            "We trained Llama2-13b with the same inputs (question + retrieval + answer) and different outputs of CoV-RAG dataset. Following Section  2.3 , the outputs for the RAG task were the same, but the verify task outputs were different: w/ Chain (score->judge->revise) and w/o Chain (direct revise). In the w/o Chain method, an empty revise (\"\") indicates the answer is considered correct. The w/ Chain method demonstrated superior performance.",
            "In the context of Question-Answering (QA) tasks based on the Retrieval-Augmented Generation (RAG) framework, we have designed a set of actions aimed at enabling the model to introspect and evaluate the effectiveness of the retrieved references and the answers generated by the generator. Further details can be found in Table  9 , Table  10 , Table  11 , Table  12 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Rankings of various methods (CoV-RAG-S: CoV-RAG in Single-Iteration) evaluated by GPT-4 across Citation (Cite), Correctness (Corr), Truthfulness (Trut), Bias, and Conciseness (Conc). Lower scores indicate higher rankings.",
        "table": "S4.T3.1",
        "footnotes": [],
        "references": [
            "CoV-RAG enhances an LM  M M \\mathit{M} italic_M  in RAG to generate answers with chain of verification, incorporating preferences and their rationale (see Figure  3 ). For the training data preparation, we divide the vanilla RAG training dataset  Liu et al. ( 2023 )  into two equal parts:  D 1 subscript D 1 \\mathit{D_{1}} italic_D start_POSTSUBSCRIPT italic_1 end_POSTSUBSCRIPT  (for RAG task) and  D 2 subscript D 2 \\mathit{D_{2}} italic_D start_POSTSUBSCRIPT italic_2 end_POSTSUBSCRIPT  (for verification task). The training involves:",
            "Result  As depicted in Table  3 , our method surpasses others in all dimensions. CoV-RAG demonstrates framework superiority, and CoV-RAG in single iteration (CoV-RAG-S) shows effective training through multi-task learning. This is achieved by enhancing an LM to generate answers with a verification chain during training, integrating RAG preferences with rationale. Details of the GPT-4 evaluation are in Appendix  G .",
            "We trained Llama2-13b with the same inputs (question + retrieval + answer) and different outputs of CoV-RAG dataset. Following Section  2.3 , the outputs for the RAG task were the same, but the verify task outputs were different: w/ Chain (score->judge->revise) and w/o Chain (direct revise). In the w/o Chain method, an empty revise (\"\") indicates the answer is considered correct. The w/ Chain method demonstrated superior performance.",
            "An example of retrieved references from CoV-RAG is shown in Table  13 .",
            "To enhance the assessment of the quality of our Question-Answer system, we conducted an Automatic Evaluation to evaluate the quality of our responses across multiple scoring dimensions. As shown in Table  18 , GPT-4 was employed to compare and rank our method (CoV-RAG) against WebGLM in GLM-10b and Llama2-13b based on various scoring criteria, ranging from superior to inferior. The final ranking is shown in Table  3 , and a case is shown in Table  17 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  High-quality rates (hqr) for various methods evaluated by GPT-4 with manual verification across Citation (Cite), Correctness (Corr), Truthfulness (Trut), Bias, and Conciseness (Conc).",
        "table": "S4.T4.1",
        "footnotes": [],
        "references": [
            "Verification/Rewriting Augmented RAG  This group includes RAG enhanced by verification or rewriting, such as Self-RAG 7 7 7 https://huggingface.co/selfrag/selfrag_llama2_13b Asai et al. ( 2023a )  with the best-performing Llama2-13b, RRR 8 8 8 https://github.com/langchain_ai/.../rewrite.ipynb Ma et al. ( 2023 )  with ChatGPT(gpt-4-1106-preview), and models trained on CoV-RAG with various parameters and types. Additionally, we conducted detailed experiments on verification, including single-turn RAG with/without reflection (Figure  4 ), rewriting position (before or after RAG, Table  5 ), and the influence of chain-type verification (direct rewriting or chained rewriting such as scoring -> judgement -> rewriting, Table  6 ).",
            "Retrieval  CoV-RAG employs a two-stage retrieval Liu et al. ( 2023 ) : coarse-grained web search (Chrome) and fine-grained LLM-augmented retrieval. Additionally, to validate adaptability across retrieval tools, we also utilize Bing Search, as detailed in Section  4.4 .",
            "Our experiments validate CoV-RAGs effectiveness and adaptability, as shown in Table  2  and Figure  4 .",
            "Adaptability  We evaluated model size and version effects by comparing WebGLM, CoV-RAG-S (single iteration without re-retrieval), and CoV-RAG across various models: Llama2-13b/7b, Vicuna-13b/7b, and ChatGLM2-6b (Figure  4 ). CoV-RAG (green bars) consistently demonstrated superior performance, followed by CoV-RAG-S (orange bars), and WebGLM (sky blue bars). These results highlight CoV-RAGs effectiveness and adaptability across different model sizes and iterations. CoV-RAG-S uses the same inference process as vanilla RAG (Question -> Retrieve -> Generate) but enhances the model by incorporating both positive and negative RAG preferences with their rationales. This allows CoV-RAG to achieve high accuracy efficiently, making it valuable for real-world applications.",
            "Analysis  We aim to validate the proposed verification criteria through a rigorous evaluation of RAG methods to understand the distribution of error types, reflected in the high-quality rates in Table  4 . These rates are based on GPT-4s scores, where high-quality samples have a score of 1 for citation, correctness, and truthfulness, bias below 0.3, and conciseness above 0.5. Manual sampling confirmed that GPT-4s scoring accuracy exceeds 95%, demonstrating its reliability and mapping error types to low high-quality rates, validating the proposed score criteria.",
            "An example of Question Answering from CoV-RAG is shown in Table  14 ."
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Ablation study of revision position in RAG on accuracy. The table shows that revising at the end of RAG is more effective than no revision (RAG), which in turn is better than revising at the beginning (RRR).",
        "table": "S4.T5.1",
        "footnotes": [],
        "references": [
            "Verification/Rewriting Augmented RAG  This group includes RAG enhanced by verification or rewriting, such as Self-RAG 7 7 7 https://huggingface.co/selfrag/selfrag_llama2_13b Asai et al. ( 2023a )  with the best-performing Llama2-13b, RRR 8 8 8 https://github.com/langchain_ai/.../rewrite.ipynb Ma et al. ( 2023 )  with ChatGPT(gpt-4-1106-preview), and models trained on CoV-RAG with various parameters and types. Additionally, we conducted detailed experiments on verification, including single-turn RAG with/without reflection (Figure  4 ), rewriting position (before or after RAG, Table  5 ), and the influence of chain-type verification (direct rewriting or chained rewriting such as scoring -> judgement -> rewriting, Table  6 ).",
            "End-Revise consistently outperformed other methods across all datasets in Table  5 . Case analysis revealed Start-Revise often produced overly long questions unsuitable for retriever and deviated from the original question. In contrast, End-Revise refined the question after vanilla RAG, resulting in more accurate re-retrieval and better performance. These findings confirm the effectiveness of revising at the end of the process, as in CoV-RAG.",
            "An example of Verification for Question Answering in CoV-RAG is shown in Table  15 ."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  Ablation study of methods with and without the CoV module. Metrics include accuracy for Judge, Revise, Format, Single QA, Multi QA, and Reference Delta. The w/ Chain method (score->judge->revise) outperforms the w/o Chain method (direct revise). Reference delta measures the difference in retrieval accuracy before and after applying the revision mechanism.",
        "table": "S4.T6.1",
        "footnotes": [],
        "references": [
            "Verification/Rewriting Augmented RAG  This group includes RAG enhanced by verification or rewriting, such as Self-RAG 7 7 7 https://huggingface.co/selfrag/selfrag_llama2_13b Asai et al. ( 2023a )  with the best-performing Llama2-13b, RRR 8 8 8 https://github.com/langchain_ai/.../rewrite.ipynb Ma et al. ( 2023 )  with ChatGPT(gpt-4-1106-preview), and models trained on CoV-RAG with various parameters and types. Additionally, we conducted detailed experiments on verification, including single-turn RAG with/without reflection (Figure  4 ), rewriting position (before or after RAG, Table  5 ), and the influence of chain-type verification (direct rewriting or chained rewriting such as scoring -> judgement -> rewriting, Table  6 ).",
            "In Table  6 , the w/ Chain method outperformed the w/o Chain across all metrics, including judgement accuracy, revising, and RAG performance in both single and multi-iteration settings. Additionally, CoV-RAG (w/ Chain) achieved greater increases in reference accuracy with re-retrieval, as measured by the reference delta. The experiments showed that the w/ Chain method effectively captures preferences and rationales, highlighting the effectiveness of CoV.",
            "An example of Multi-Iteration Question Answering in CoV-RAG is shown in Table  16 ."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Retrieval accuracy of single-iteration and multi-iteration of CoV-RAG using Bing and Chrome.",
        "table": "S5.T7.1",
        "footnotes": [],
        "references": [
            "We evaluated the improvement of CoV-RAG in retrieval accuracy with two retriever tools (Bing and Chrome) in Table  7 . Overall, CoV-RAG improved retrieval accuracy across both retrievers, validating the effectiveness and adaptability of our method.",
            "To enhance the assessment of the quality of our Question-Answer system, we conducted an Automatic Evaluation to evaluate the quality of our responses across multiple scoring dimensions. As shown in Table  18 , GPT-4 was employed to compare and rank our method (CoV-RAG) against WebGLM in GLM-10b and Llama2-13b based on various scoring criteria, ranging from superior to inferior. The final ranking is shown in Table  3 , and a case is shown in Table  17 ."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  A list of instructions that we use for QA and verification task. Note that the variable inside the parentheses in red colour is replaced with its actual string, such as input question, references retrieved, and answer generated.",
        "table": "A1.T8.1",
        "footnotes": [],
        "references": [
            "There are two tasks in our CoV-RAG, Question Answering(QA) Task and verification task. Details for Instructions we use for QA and verification are shown in Table  8 . Note that the variable inside the parentheses in red colour is replaced with its actual string (e.g., input question, references retrieved, and answer generated).",
            "To enhance the assessment of the quality of our Question-Answer system, we conducted an Automatic Evaluation to evaluate the quality of our responses across multiple scoring dimensions. As shown in Table  18 , GPT-4 was employed to compare and rank our method (CoV-RAG) against WebGLM in GLM-10b and Llama2-13b based on various scoring criteria, ranging from superior to inferior. The final ranking is shown in Table  3 , and a case is shown in Table  17 ."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  Negative QA Example1",
        "table": "A2.T9.1",
        "footnotes": [],
        "references": [
            "In the context of Question-Answering (QA) tasks based on the Retrieval-Augmented Generation (RAG) framework, we have designed a set of actions aimed at enabling the model to introspect and evaluate the effectiveness of the retrieved references and the answers generated by the generator. Further details can be found in Table  9 , Table  10 , Table  11 , Table  12 ."
        ]
    },
    "id_table_10": {
        "caption": "Table 10:  Negative QA Example2",
        "table": "A2.T10.1",
        "footnotes": [],
        "references": [
            "In the context of Question-Answering (QA) tasks based on the Retrieval-Augmented Generation (RAG) framework, we have designed a set of actions aimed at enabling the model to introspect and evaluate the effectiveness of the retrieved references and the answers generated by the generator. Further details can be found in Table  9 , Table  10 , Table  11 , Table  12 ."
        ]
    },
    "id_table_11": {
        "caption": "Table 11:  Negative QA Example3",
        "table": "A2.T11.1",
        "footnotes": [],
        "references": [
            "In the context of Question-Answering (QA) tasks based on the Retrieval-Augmented Generation (RAG) framework, we have designed a set of actions aimed at enabling the model to introspect and evaluate the effectiveness of the retrieved references and the answers generated by the generator. Further details can be found in Table  9 , Table  10 , Table  11 , Table  12 ."
        ]
    },
    "id_table_12": {
        "caption": "Table 12:  Negative QA Example4",
        "table": "A2.T12.1",
        "footnotes": [],
        "references": [
            "In the context of Question-Answering (QA) tasks based on the Retrieval-Augmented Generation (RAG) framework, we have designed a set of actions aimed at enabling the model to introspect and evaluate the effectiveness of the retrieved references and the answers generated by the generator. Further details can be found in Table  9 , Table  10 , Table  11 , Table  12 ."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:  Retrieval Example",
        "table": "A3.T13.1",
        "footnotes": [],
        "references": [
            "An example of retrieved references from CoV-RAG is shown in Table  13 ."
        ]
    },
    "id_table_14": {
        "caption": "Table 14:  Question Answer Example",
        "table": "A4.T14.1",
        "footnotes": [],
        "references": [
            "An example of Question Answering from CoV-RAG is shown in Table  14 ."
        ]
    },
    "id_table_15": {
        "caption": "Table 15:  Verification Example",
        "table": "A5.T15.1",
        "footnotes": [],
        "references": [
            "An example of Verification for Question Answering in CoV-RAG is shown in Table  15 ."
        ]
    },
    "id_table_16": {
        "caption": "Table 16:  Details of Multi-Iteration CoV-RAG",
        "table": "A6.T16.1",
        "footnotes": [],
        "references": [
            "An example of Multi-Iteration Question Answering in CoV-RAG is shown in Table  16 ."
        ]
    },
    "id_table_17": {
        "caption": "Table 17:  Case of Winner Evaluation by GPT-4",
        "table": "A7.T17.1",
        "footnotes": [],
        "references": [
            "To enhance the assessment of the quality of our Question-Answer system, we conducted an Automatic Evaluation to evaluate the quality of our responses across multiple scoring dimensions. As shown in Table  18 , GPT-4 was employed to compare and rank our method (CoV-RAG) against WebGLM in GLM-10b and Llama2-13b based on various scoring criteria, ranging from superior to inferior. The final ranking is shown in Table  3 , and a case is shown in Table  17 ."
        ]
    },
    "id_table_18": {
        "caption": "Table 18:  Instructions of Automatic Evaluation for RAG by GPT-4",
        "table": "A7.T18.1",
        "footnotes": [],
        "references": [
            "To enhance the assessment of the quality of our Question-Answer system, we conducted an Automatic Evaluation to evaluate the quality of our responses across multiple scoring dimensions. As shown in Table  18 , GPT-4 was employed to compare and rank our method (CoV-RAG) against WebGLM in GLM-10b and Llama2-13b based on various scoring criteria, ranging from superior to inferior. The final ranking is shown in Table  3 , and a case is shown in Table  17 ."
        ]
    },
    "id_table_19": {
        "caption": "Table 19:  Instruction of Automatic Evaluation for Revise by GPT-4",
        "table": "A7.T19.1",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "Typically,",
        "depends on if the revised question",
        "is non-empty. For practical time costs,",
        "can use the values (0.27, (correct 0.26, bias 0.7, truthfulness 0.92), False, Not",
        "), derived through cross-validation on the validation set."
    ]
}