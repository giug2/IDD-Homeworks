{
    "id_table_1": {
        "caption": "Table 1 .  Summary of the dataset.",
        "table": "S3.T1.5",
        "footnotes": [],
        "references": [
            "The list of RFs used in the AIMEN system is presented in Fig.  1 . AIMEN integrates 34 different RFs in its prediction and explanation approach, including the ones used to develop and test the FRI. Clinical datasets often have limitations, such as small data size, inadequate number of samples for a specific category, or incomplete data. These challenges can make learning from these datasets difficult for most supervised learning systems. This paper addresses these challenges by providing a systematic data generation and evaluation approach. AIMEN has three major components as shown in Fig.  2 : a data generation module, a classification pipeline, and a counterfactual explanation (CE) tool that provides what-if scenarios for changing abnormal labor to normal labor.",
            "Verify the goodness of  S S \\mathcal{S} caligraphic_S  using distribution gap    ( f , S , D , D t  e  s  t )  f S D subscript D t e s t \\delta(f,\\mathcal{S},\\mathcal{D},\\mathcal{D}_{test}) italic_ ( italic_f , caligraphic_S , caligraphic_D , caligraphic_D start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT )  given by Equations  1 ,  2 , and  3 .",
            "Data was collected from 1462 patients. The recorded RFs include preexisting maternal conditions such as diabetes, hypertension, and cholesterol. The fetal, obstetrical, and delivery RFs and EFM data were collected. EFM features include the absence of FHR accelerations, abnormal baseline FHR, and excessive uterine activity. This datasets full list of RFs can be found in Mamun et al.  (Mamun et al . ,  2023 ) . A summary of some numeric features of the dataset is available in Table  1 . Five cases were excluded because of missing RFs and the dataset was prepared with the remaining 1457 cases."
        ]
    },
    "id_table_2": {
        "caption": "Table 2 .  Comparison of the performance of the downstream classification task using different backbones of the AIMEN system. In this experiment, AIMEN with MLP_v5 backbone achieved the best performance, as judged by all of the metrics.",
        "table": "S4.T2.4",
        "footnotes": [],
        "references": [
            "The list of RFs used in the AIMEN system is presented in Fig.  1 . AIMEN integrates 34 different RFs in its prediction and explanation approach, including the ones used to develop and test the FRI. Clinical datasets often have limitations, such as small data size, inadequate number of samples for a specific category, or incomplete data. These challenges can make learning from these datasets difficult for most supervised learning systems. This paper addresses these challenges by providing a systematic data generation and evaluation approach. AIMEN has three major components as shown in Fig.  2 : a data generation module, a classification pipeline, and a counterfactual explanation (CE) tool that provides what-if scenarios for changing abnormal labor to normal labor.",
            "Verify the goodness of  S S \\mathcal{S} caligraphic_S  using distribution gap    ( f , S , D , D t  e  s  t )  f S D subscript D t e s t \\delta(f,\\mathcal{S},\\mathcal{D},\\mathcal{D}_{test}) italic_ ( italic_f , caligraphic_S , caligraphic_D , caligraphic_D start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT )  given by Equations  1 ,  2 , and  3 .",
            "This solution can be realized with a neonatal risk modeling system made up of an EFM to support data collection and three additional components: a data-generating tool for augmenting training data, a classifier for risk analysis, and an explainable AI component for providing counterfactual explanations of abnormal predictions. An overview of the training and evaluation pipeline is presented in Fig.  2 .",
            "Five different neural networks were tested as the backbone of the AIMEN system. The summary of their performance is presented in Table  2 . Each backbone was trained and tested five times on different training and test sets and average performance was reported. The default AIMEN (uses MLP_v5 backbone) achieves the best result on all performance metrics, as reported in Table  2 .",
            "The default decision threshold chosen throughout this paper is 0.5, meaning, the output probability of the classifier is   \\geq   0.5, a case is classified as abnormal, otherwise, normal. In Table  2 , we can see that the AIMEN v5 system has a sensitivity of 0.516 when the decision threshold is 0.5. To check how the systems performance changes with different decision thresholds, we plot the receiver operating characteristic (ROC) curve and the classification performance of the system in Fig.  6 . From this figure, the physicians can decide which decision threshold is suitable for labeling an example as abnormal."
        ]
    },
    "id_table_3": {
        "caption": "Table 3 .  Evaluation of the CEs after using different architectures. In this case, the dimension of the feature vectors is 34, which should be considered while interpreting the results.",
        "table": "S4.T3.18",
        "footnotes": [],
        "references": [
            "AIMEN overcomes the challenge of small datasets by generating useful synthetic data and validating their quality. AIMENs default data generation module is a Conditional Tabular Generative Adversarial Network (CTGAN)  (Xu et al . ,  2019 ) . We also propose two other variations of AIMEN based on the data generation method. One is the ADASYN-based framework AIMEN_ADASYN and the other is a CTGAN-based framework with silhouette score  (Shahapure and Nicholas,  2020 )  restrictions called Restricted AIMEN (R-AIMEN). Silhouette score is a metric that determines the separability of different clusters. A higher overall silhouette score means that in general the samples of the same class are placed close to one another and samples from the opposite classes are placed far from one another in the hyperspace. The R-AIMEN models are described in detail in Section  3.3  and Section  4.6 . We evaluate the quality of the generated data based on the difference between validation loss and test loss, referred to as the distribution gap.",
            "Verify the goodness of  S S \\mathcal{S} caligraphic_S  using distribution gap    ( f , S , D , D t  e  s  t )  f S D subscript D t e s t \\delta(f,\\mathcal{S},\\mathcal{D},\\mathcal{D}_{test}) italic_ ( italic_f , caligraphic_S , caligraphic_D , caligraphic_D start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT )  given by Equations  1 ,  2 , and  3 .",
            "Fully-connected neural networks were employed for the classification step. Five different forms of multilayer perceptions (MLP) were tested as the backbones for AIMEN. They are named MLP_v1 to MLP_v5. The architecture of the MLP_v5 model is shown in Fig.  3 . This network has eight fully connected layers including the output layer. The default AIMEN uses an ensemble of MLP_v5 neural networks but the backbone can be changed to any other option from MLP_v1 to MLP_v4. Based on the performance of the validation sets, weighted voting was performed among the ensemble members to calculate the output during inference.",
            "We present two counterfactual examples produced by our methods in Fig.  7 . The CEs are evaluated based on the average distance and the average sparsity in our experiments. The average distance is the average Euclidean distance between the normalized real example and the corresponding normalized counterfactual example pairs. The average sparsity is the average number of variables that need to be changed to flip the prediction from abnormal class to normal class. We present a summary of this evaluation in Table  3 . The feature dimension of the dataset is 34. An average distance of 0.33 and an average sparsity of 2.50 means that with this method, on average, a CE is located 0.33 units away from a real example in the 34-dimensional hyperspace, and on average 2.5 out of the 34 attributes need to be changed for an abnormal class example to convert to a normal class example."
        ]
    },
    "id_table_4": {
        "caption": "Table 4 .  Performance metrics of different classifiers on predicting abnormal delivery cases with prenatal features. All models were trained with Adam optimizer and cross-entropy loss function. All results of this table are evaluated on real and unseen test data. The unrestricted AIMEN systems performance is compared with the restricted AIMEN (R-AIMEN) systems. R-AIMEN systems set a condition on the generated data so that a minimum silhouette score is ensured among the generated data. In these experiments, the MLP_v5 backbone was used. All models in this table were trained for up to 1000 epochs with early stopping enabled with a learning rate of 0.0001. The 8-fold cross-validation method was used in all the models in this table. F1 +  and F1 -  are the F1 scores for the abnormal class and normal class respectively. Avg F1 is the macro average F1 score of both classes.",
        "table": "S4.T4.6",
        "footnotes": [],
        "references": [
            "AIMEN overcomes the challenge of small datasets by generating useful synthetic data and validating their quality. AIMENs default data generation module is a Conditional Tabular Generative Adversarial Network (CTGAN)  (Xu et al . ,  2019 ) . We also propose two other variations of AIMEN based on the data generation method. One is the ADASYN-based framework AIMEN_ADASYN and the other is a CTGAN-based framework with silhouette score  (Shahapure and Nicholas,  2020 )  restrictions called Restricted AIMEN (R-AIMEN). Silhouette score is a metric that determines the separability of different clusters. A higher overall silhouette score means that in general the samples of the same class are placed close to one another and samples from the opposite classes are placed far from one another in the hyperspace. The R-AIMEN models are described in detail in Section  3.3  and Section  4.6 . We evaluate the quality of the generated data based on the difference between validation loss and test loss, referred to as the distribution gap.",
            "Synthetic data generation was employed with both CTGAN and ADASYN and overall CTGAN generated data were more helpful for the downstream task. In Fig.  4 , we compare different methods of data generation. Data generation with CTGAN allows specifying the categorical variables and the generated values for those variables will be integer values of 0 and 1. For the numerical variables, however, by default, CTGAN generates data that is out of the range seen in training data. For example, labor hours are present in the training data with only integer values   \\geq   0. But CTGAN also generated examples with negative values. We conducted multiple rounds of experiments where i) we allowed those negative values to be used for training the classifiers, or ii) we replaced any negative value with 0, as a negative value for a duration does not make sense. Allowing negative values in the generated data for training the models made the downstream task more accurate, as shown in Fig.  4 a. This may be because labor starts before a mother comes to the hospital. We would like to emphasize that all the test set results reported in this paper were obtained by evaluating the models on real and unseen data.",
            "The default AIMEN model uses CTGAN to generate synthetic data without any restriction. On the other hand, the restricted models require the synthetic data to satisfy the condition that the average silhouette score of the two clusters (positive and negative) must increase from the previous iteration or it has to be higher than 0.3. This restriction makes the data more easily separable. However, in Table  4 , it can be seen that this restriction reduces the performance of the classifier based on the average F1 score. The reason may be that this restriction increases the distance between the distribution of the training data and the distribution of the test data because we are only using real data in the test set and this restriction may not follow the true behavior of the data. We developed another system where a requirement of silhouette score on the synthetic data for the negative class was applied but the positive class samples were generated freely. The goal was to increase the sensitivity of the classifier by giving the positive synthetic data more freedom than the negative synthetic data. If we look at the results, we see that the sensitivity of R-AIMEN with negative class restriction (0.579) is in fact higher than that of R-AIMEN with both class restrictions (0.474) but overall the unrestricted AIMEN has the highest sensitivity score (0.632) among these three.",
            "The average F1 scores reported in Table  4  show that the unrestricted AIMEN has the highest score (0.784) among all the models. From these results, we conclude that synthetic data helps increase the performance of a model but it is important to ensure that the distributions of the training and test data are similar after data augmentation."
        ]
    },
    "id_table_5": {
        "caption": "Table 5 .  Validation loss and test loss of the AIMEN and R-AIMEN models. Three different restrictions with silhouette scores were evaluated: no restriction, restriction on the negative class, and restriction on both classes. The unrestricted AIMEN had the best distribution gap, which means the generated data was closer to the test data than other methods. Suppose, the test loss is  L t  e  s  t subscript L t e s t L_{test} italic_L start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT  and the average validation loss is  L v  a  l subscript L v a l L_{val} italic_L start_POSTSUBSCRIPT italic_v italic_a italic_l end_POSTSUBSCRIPT . Then, distribution gap was calculated by  L t  e  s  t  L v  a  l L v  a  l  100 subscript L t e s t subscript L v a l subscript L v a l 100 \\frac{L_{test}-L_{val}}{L_{val}}\\times 100 divide start_ARG italic_L start_POSTSUBSCRIPT italic_t italic_e italic_s italic_t end_POSTSUBSCRIPT - italic_L start_POSTSUBSCRIPT italic_v italic_a italic_l end_POSTSUBSCRIPT end_ARG start_ARG italic_L start_POSTSUBSCRIPT italic_v italic_a italic_l end_POSTSUBSCRIPT end_ARG  100 .",
        "table": "S4.T5.8",
        "footnotes": [],
        "references": [
            "When a model performs well on both the training and validation metrics, it indicates that it can learn representations and generalize which prepares it well for unseen data. In Fig.  5 , we can see that our model achieves macro average F1 scores over 0.9 in most of the training and validation set experiments. This finding indicates that the model is not underfitting or overfitting.",
            "One challenge of these experiments is that the training and validation sets have both real and synthetic data. So, if the synthetic data does not represent the distribution of the real data, the performance on the validation set may not equate to the performance on the test set. We therefore tested the results when all the examples were from real data in Fig.  5 . We can see that the model achieved an accuracy of 0.789, a sensitivity of 0.632, and a macro average F1 score of 0.784 on a balanced test set of real data. One challenge of evaluating the method on the test set was the small data size. As we had only 112 abnormal class examples in the whole dataset and a large part of them were used in training and data generation, we had to exclude them from the test set. The test set had 38 examples: 19 normal and 19 abnormal. The confusion matrix of Fig.  5  shows that the model identified 12 of the 19 positive class examples, corresponding to a sensitivity of 0.632 while identifying 18 out of 19 negative class examples, corresponding to a specificity of 0.947.",
            "Finally, voting rights are given to only these classifiers with a macro average F1 score  > > >  0.7 on the corresponding validation set. In the case of Fig.  5 , the model for Fold 4 was therefore excluded from voting when evaluating the test set.",
            "Finally, we compared the validation and test losses to determine the distribution gap, defined as the relative difference between the average validation loss and the average test loss. In Table  5 , it can be seen that the distribution gap is lowest in the unrestricted AIMEN. The minimum best validation loss or average validation loss is achieved when the silhouette score is applied to both classes. However, better validation loss does not necessarily translate to better test loss. Applying a silhouette score restriction makes the synthetic data more easily separable, hence the validation loss is lower. However, in this way, the model fails to learn some of the distinctive features of the data as the restricted synthetic data does not follow the true distribution of the data because the real data does not have to be easily separable in general. That is why, despite better validation metrics, R-AIMEN models could not achieve test metrics as good as AIMENs. Hence, the distribution gap is lowest in the unrestricted AIMEN."
        ]
    }
}