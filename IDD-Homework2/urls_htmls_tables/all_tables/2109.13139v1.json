{
    "S3.T1": {
        "caption": "Table 1: Results showing test-std and test-dev accuracy scores of our model (trained on train+val+vg) and ablated versions over different datasets. MULAN achieves state-of-the-art on both benchmarks.",
        "table": "<table id=\"S3.T1.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S3.T1.1.1.1\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.1.1.1\" class=\"ltx_td ltx_align_right ltx_border_tt\">Model</td>\n<td id=\"S3.T1.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T1.1.1.1.2.1\" class=\"ltx_text ltx_font_italic\">test-std</span></td>\n<td id=\"S3.T1.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S3.T1.1.1.1.3.1\" class=\"ltx_text ltx_font_italic\">test-dev</span></td>\n</tr>\n<tr id=\"S3.T1.1.2.2\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.2.2.1\" class=\"ltx_td ltx_align_right ltx_border_t\">MULAN (multimodal)</td>\n<td id=\"S3.T1.1.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S3.T1.1.2.2.2.1\" class=\"ltx_text ltx_font_bold\">73.98%</span></td>\n<td id=\"S3.T1.1.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S3.T1.1.2.2.3.1\" class=\"ltx_text ltx_font_bold\">73.72%</span></td>\n</tr>\n<tr id=\"S3.T1.1.3.3\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.3.3.1\" class=\"ltx_td ltx_align_right\">Text only (TSM)</td>\n<td id=\"S3.T1.1.3.3.2\" class=\"ltx_td ltx_align_right\">73.77%</td>\n<td id=\"S3.T1.1.3.3.3\" class=\"ltx_td ltx_align_right\">73.52%</td>\n</tr>\n<tr id=\"S3.T1.1.4.4\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.4.4.1\" class=\"ltx_td ltx_align_right\">Image only (MDS)</td>\n<td id=\"S3.T1.1.4.4.2\" class=\"ltx_td ltx_align_right\">73.67%</td>\n<td id=\"S3.T1.1.4.4.3\" class=\"ltx_td ltx_align_right\">73.39%</td>\n</tr>\n<tr id=\"S3.T1.1.5.5\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.5.5.1\" class=\"ltx_td ltx_align_right\">No Integration</td>\n<td id=\"S3.T1.1.5.5.2\" class=\"ltx_td ltx_align_right\">73.65%</td>\n<td id=\"S3.T1.1.5.5.3\" class=\"ltx_td ltx_align_right\">73.39%</td>\n</tr>\n<tr id=\"S3.T1.1.6.6\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.6.6.1\" class=\"ltx_td ltx_align_right ltx_border_t\">Li et al. (2020)</td>\n<td id=\"S3.T1.1.6.6.2\" class=\"ltx_td ltx_align_right ltx_border_t\">73.82%</td>\n<td id=\"S3.T1.1.6.6.3\" class=\"ltx_td ltx_align_right ltx_border_t\">73.61%</td>\n</tr>\n<tr id=\"S3.T1.1.7.7\" class=\"ltx_tr\">\n<td id=\"S3.T1.1.7.7.1\" class=\"ltx_td ltx_align_right ltx_border_bb\">Jiang et al. (2020)</td>\n<td id=\"S3.T1.1.7.7.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">72.71%</td>\n<td id=\"S3.T1.1.7.7.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">72.59%</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "Table¬†1 shows results of our MULAN model along with current state-of-the-art approaches and ablations of our method.\nOur method outperforms the current state of the art¬†Li et¬†al. (2020), reaching accuracy scores of 73.98% on test-std and 73.72% on test-dev, as compared to 73.82% and 73.61%.\nOur model also uses approximately 80% less trainable parameters than ¬†Li et¬†al. (2020).\nNotably, we observe a systematic increase in performance as a result of human-like attention integration.\nOur base model without any integration reaches 73.65% on test-std.\nWhile the integration of human-like attention on only images (73.67%) or text (73.77% on test-std) can lead to an increase in performance, our full MULAN model employing multimodal integration is the best performing approach. This further underlines the importance of jointly integrating human-like attention on both text and images for the VQA task, which is inherently multimodal."
        ]
    },
    "S5.T2": {
        "caption": "Table 2: Layer-wise integration ablation study results, on test-std. We integrate human-like attention at different layer combinations. TSM question attention weights are integrated into encoder SA modules, MDS image attention weights into decoder SA modules.",
        "table": "<table id=\"S5.T2.1\" class=\"ltx_tabular ltx_centering ltx_align_middle\">\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T2.1.1.1\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.1.1.1\" class=\"ltx_td ltx_align_right ltx_border_tt\">TSM</td>\n<td id=\"S5.T2.1.1.1.2\" class=\"ltx_td ltx_align_right ltx_border_tt\">MDS</td>\n<td id=\"S5.T2.1.1.1.3\" class=\"ltx_td ltx_align_right ltx_border_tt\"><span id=\"S5.T2.1.1.1.3.1\" class=\"ltx_text ltx_font_italic\">test-std</span></td>\n</tr>\n<tr id=\"S5.T2.1.2.2\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.2.2.1\" class=\"ltx_td ltx_align_right ltx_border_t\">1</td>\n<td id=\"S5.T2.1.2.2.2\" class=\"ltx_td ltx_align_right ltx_border_t\">2</td>\n<td id=\"S5.T2.1.2.2.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T2.1.2.2.3.1\" class=\"ltx_text ltx_font_bold\">(ours) 73.98%</span></td>\n</tr>\n<tr id=\"S5.T2.1.3.3\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.3.3.1\" class=\"ltx_td ltx_align_right\">2</td>\n<td id=\"S5.T2.1.3.3.2\" class=\"ltx_td ltx_align_right\">2</td>\n<td id=\"S5.T2.1.3.3.3\" class=\"ltx_td ltx_align_right\">73.64%</td>\n</tr>\n<tr id=\"S5.T2.1.4.4\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.4.4.1\" class=\"ltx_td ltx_align_right\">1, 3, 5</td>\n<td id=\"S5.T2.1.4.4.2\" class=\"ltx_td ltx_align_right\">2</td>\n<td id=\"S5.T2.1.4.4.3\" class=\"ltx_td ltx_align_right\">73.73%</td>\n</tr>\n<tr id=\"S5.T2.1.5.5\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.5.5.1\" class=\"ltx_td ltx_align_right\">1</td>\n<td id=\"S5.T2.1.5.5.2\" class=\"ltx_td ltx_align_right\">1‚Äì6</td>\n<td id=\"S5.T2.1.5.5.3\" class=\"ltx_td ltx_align_right\">71.55%</td>\n</tr>\n<tr id=\"S5.T2.1.6.6\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.6.6.1\" class=\"ltx_td ltx_align_right\">1‚Äì3</td>\n<td id=\"S5.T2.1.6.6.2\" class=\"ltx_td ltx_align_right\">2</td>\n<td id=\"S5.T2.1.6.6.3\" class=\"ltx_td ltx_align_right\">73.49%</td>\n</tr>\n<tr id=\"S5.T2.1.7.7\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.7.7.1\" class=\"ltx_td ltx_align_right\">1‚Äì6</td>\n<td id=\"S5.T2.1.7.7.2\" class=\"ltx_td ltx_align_right\">2</td>\n<td id=\"S5.T2.1.7.7.3\" class=\"ltx_td ltx_align_right\">73.73%</td>\n</tr>\n<tr id=\"S5.T2.1.8.8\" class=\"ltx_tr\">\n<td id=\"S5.T2.1.8.8.1\" class=\"ltx_td ltx_align_right ltx_border_bb\">1‚Äì6</td>\n<td id=\"S5.T2.1.8.8.2\" class=\"ltx_td ltx_align_right ltx_border_bb\">2‚Äì6</td>\n<td id=\"S5.T2.1.8.8.3\" class=\"ltx_td ltx_align_right ltx_border_bb\">73.50%</td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "We evaluated the integration of human-like attention on questions and images for different layers in the MCAN encoder-decoder architecture (see Table¬†2). We investigated the integration of TSM outputs into different SA layers of the encoder, and the integration of MDS outputs into different SA modules of the decoder.\nAmong all investigated combinations, the initial integration into the first layer of the encoder and the second layer of the decoder performed best (73.98% accuracy).\nIntegrating TSM predictions into the second encoder layer decreased the overall accuracy to 73.64%, which is in line with the reasoning discussed in¬†Brunner et¬†al. (2020), where with layer depth feature embeddings are increasingly mixed and therefore less attributable to the input word token at the same position. The TSM predicts attention weights for specific word tokens.\nWe further investigated the integration of TSM and MDS predictions at multiple layers in the MCAN architecture. However all options resulted in decreased performance in comparison to MULAN.\nOur results indicate that early integration of human-like attention at a single point for both text and image is optimal for the VQA task."
        ]
    },
    "S5.T3": {
        "caption": "Table 3: Performance on VQAv2 val split in terms of per-question-type accuracy of the proposed multimodal integration method (MULAN) and the unimodal ablations text-only or image-only and no integration of human attention (No Integration). Because the online evaluation of VQAv2 only returns overall accuracy, we cannot obtain fine-grained accuracy for test-std or test-dev. A star indicates statistically significant pùëùp at p<0.05ùëù0.05p<0.05.",
        "table": "<table id=\"S5.T3.5\" class=\"ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle\">\n<thead class=\"ltx_thead\">\n<tr id=\"S5.T3.5.1.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.1.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_tt\">Question type</th>\n<th id=\"S5.T3.5.1.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt\">Bin Size</th>\n<th id=\"S5.T3.5.1.1.3\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">No Integration</th>\n<th id=\"S5.T3.5.1.1.4\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">text-only</th>\n<th id=\"S5.T3.5.1.1.5\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\">image-only</th>\n<th id=\"S5.T3.5.1.1.6\" class=\"ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_tt\"><span id=\"S5.T3.5.1.1.6.1\" class=\"ltx_text ltx_font_bold\">MULAN</span></th>\n</tr>\n</thead>\n<tbody class=\"ltx_tbody\">\n<tr id=\"S5.T3.5.2.1\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.2.1.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t\">reading</th>\n<th id=\"S5.T3.5.2.1.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t\">31‚ÄâK</th>\n<td id=\"S5.T3.5.2.1.3\" class=\"ltx_td ltx_align_right ltx_border_t\"><span id=\"S5.T3.5.2.1.3.1\" class=\"ltx_text ltx_font_bold\">42.46</span></td>\n<td id=\"S5.T3.5.2.1.4\" class=\"ltx_td ltx_align_right ltx_border_t\">42.28</td>\n<td id=\"S5.T3.5.2.1.5\" class=\"ltx_td ltx_align_right ltx_border_t\">42.40</td>\n<td id=\"S5.T3.5.2.1.6\" class=\"ltx_td ltx_align_right ltx_border_t\">42.30</td>\n</tr>\n<tr id=\"S5.T3.5.3.2\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.3.2.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">activity recognition</th>\n<th id=\"S5.T3.5.3.2.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">15‚ÄâK</th>\n<td id=\"S5.T3.5.3.2.3\" class=\"ltx_td ltx_align_right\">74.55</td>\n<td id=\"S5.T3.5.3.2.4\" class=\"ltx_td ltx_align_right\">74.72</td>\n<td id=\"S5.T3.5.3.2.5\" class=\"ltx_td ltx_align_right\">74.59</td>\n<td id=\"S5.T3.5.3.2.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.3.2.6.1\" class=\"ltx_text ltx_font_bold\">75.01</span></td>\n</tr>\n<tr id=\"S5.T3.5.4.3\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.4.3.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">positional reasoning</th>\n<th id=\"S5.T3.5.4.3.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">26‚ÄâK</th>\n<td id=\"S5.T3.5.4.3.3\" class=\"ltx_td ltx_align_right\">61.74</td>\n<td id=\"S5.T3.5.4.3.4\" class=\"ltx_td ltx_align_right\">61.97</td>\n<td id=\"S5.T3.5.4.3.5\" class=\"ltx_td ltx_align_right\">61.85</td>\n<td id=\"S5.T3.5.4.3.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.4.3.6.1\" class=\"ltx_text ltx_font_bold\">62.01</span></td>\n</tr>\n<tr id=\"S5.T3.5.5.4\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.5.4.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">object recognition</th>\n<th id=\"S5.T3.5.5.4.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">28‚ÄâK</th>\n<td id=\"S5.T3.5.5.4.3\" class=\"ltx_td ltx_align_right\">82.59</td>\n<td id=\"S5.T3.5.5.4.4\" class=\"ltx_td ltx_align_right\">82.50</td>\n<td id=\"S5.T3.5.5.4.5\" class=\"ltx_td ltx_align_right\">82.49</td>\n<td id=\"S5.T3.5.5.4.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.5.4.6.1\" class=\"ltx_text ltx_font_bold\">82.68</span></td>\n</tr>\n<tr id=\"S5.T3.5.6.5\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.6.5.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">counting</th>\n<th id=\"S5.T3.5.6.5.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">24‚ÄâK</th>\n<td id=\"S5.T3.5.6.5.3\" class=\"ltx_td ltx_align_right\">59.77</td>\n<td id=\"S5.T3.5.6.5.4\" class=\"ltx_td ltx_align_right\">59.70</td>\n<td id=\"S5.T3.5.6.5.5\" class=\"ltx_td ltx_align_right\">59.44</td>\n<td id=\"S5.T3.5.6.5.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.6.5.6.1\" class=\"ltx_text ltx_font_bold\">59.82</span></td>\n</tr>\n<tr id=\"S5.T3.5.7.6\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.7.6.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">object presence</th>\n<th id=\"S5.T3.5.7.6.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">17‚ÄâK</th>\n<td id=\"S5.T3.5.7.6.3\" class=\"ltx_td ltx_align_right\">86.45</td>\n<td id=\"S5.T3.5.7.6.4\" class=\"ltx_td ltx_align_right\">86.47</td>\n<td id=\"S5.T3.5.7.6.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.7.6.5.1\" class=\"ltx_text ltx_font_bold\">86.59</span></td>\n<td id=\"S5.T3.5.7.6.6\" class=\"ltx_td ltx_align_right\">86.57</td>\n</tr>\n<tr id=\"S5.T3.5.8.7\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.8.7.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">scene recognition</th>\n<th id=\"S5.T3.5.8.7.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">15‚ÄâK</th>\n<td id=\"S5.T3.5.8.7.3\" class=\"ltx_td ltx_align_right\">79.19</td>\n<td id=\"S5.T3.5.8.7.4\" class=\"ltx_td ltx_align_right\">79.10</td>\n<td id=\"S5.T3.5.8.7.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.8.7.5.1\" class=\"ltx_text ltx_font_bold\">79.20</span></td>\n<td id=\"S5.T3.5.8.7.6\" class=\"ltx_td ltx_align_right\">79.19</td>\n</tr>\n<tr id=\"S5.T3.5.9.8\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.9.8.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">sentiment understanding</th>\n<th id=\"S5.T3.5.9.8.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">14‚ÄâK</th>\n<td id=\"S5.T3.5.9.8.3\" class=\"ltx_td ltx_align_right\">83.59</td>\n<td id=\"S5.T3.5.9.8.4\" class=\"ltx_td ltx_align_right\">83.77</td>\n<td id=\"S5.T3.5.9.8.5\" class=\"ltx_td ltx_align_right\">83.53</td>\n<td id=\"S5.T3.5.9.8.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.9.8.6.1\" class=\"ltx_text ltx_font_bold\">83.92</span></td>\n</tr>\n<tr id=\"S5.T3.5.10.9\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.10.9.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">color</th>\n<th id=\"S5.T3.5.10.9.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">25‚ÄâK</th>\n<td id=\"S5.T3.5.10.9.3\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.10.9.3.1\" class=\"ltx_text ltx_font_bold\">80.56</span></td>\n<td id=\"S5.T3.5.10.9.4\" class=\"ltx_td ltx_align_right\">80.52</td>\n<td id=\"S5.T3.5.10.9.5\" class=\"ltx_td ltx_align_right\">80.31</td>\n<td id=\"S5.T3.5.10.9.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.10.9.6.1\" class=\"ltx_text ltx_font_bold\">80.56</span></td>\n</tr>\n<tr id=\"S5.T3.5.11.10\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.11.10.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">attribute</th>\n<th id=\"S5.T3.5.11.10.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">4‚ÄâK</th>\n<td id=\"S5.T3.5.11.10.3\" class=\"ltx_td ltx_align_right\">69.36</td>\n<td id=\"S5.T3.5.11.10.4\" class=\"ltx_td ltx_align_right\">69.09</td>\n<td id=\"S5.T3.5.11.10.5\" class=\"ltx_td ltx_align_right\">69.37</td>\n<td id=\"S5.T3.5.11.10.6\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.11.10.6.1\" class=\"ltx_text ltx_font_bold\">69.65</span></td>\n</tr>\n<tr id=\"S5.T3.5.12.11\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.12.11.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">utility affordance</th>\n<th id=\"S5.T3.5.12.11.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">11‚ÄâK</th>\n<td id=\"S5.T3.5.12.11.3\" class=\"ltx_td ltx_align_right\">66.33</td>\n<td id=\"S5.T3.5.12.11.4\" class=\"ltx_td ltx_align_right\">66.40</td>\n<td id=\"S5.T3.5.12.11.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.12.11.5.1\" class=\"ltx_text ltx_font_bold\">66.64</span></td>\n<td id=\"S5.T3.5.12.11.6\" class=\"ltx_td ltx_align_right\">66.42</td>\n</tr>\n<tr id=\"S5.T3.5.13.12\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.13.12.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row\">sport recognition</th>\n<th id=\"S5.T3.5.13.12.2\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r\">6‚ÄâK</th>\n<td id=\"S5.T3.5.13.12.3\" class=\"ltx_td ltx_align_right\">85.39</td>\n<td id=\"S5.T3.5.13.12.4\" class=\"ltx_td ltx_align_right\">85.38</td>\n<td id=\"S5.T3.5.13.12.5\" class=\"ltx_td ltx_align_right\"><span id=\"S5.T3.5.13.12.5.1\" class=\"ltx_text ltx_font_bold\">85.93</span></td>\n<td id=\"S5.T3.5.13.12.6\" class=\"ltx_td ltx_align_right\">85.60</td>\n</tr>\n<tr id=\"S5.T3.5.14.13\" class=\"ltx_tr\">\n<th id=\"S5.T3.5.14.13.1\" class=\"ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_t\">Overall VQAv2 <span id=\"S5.T3.5.14.13.1.1\" class=\"ltx_text ltx_font_italic\">val</span> Accuracy:</th>\n<th id=\"S5.T3.5.14.13.2\" class=\"ltx_td ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t\"></th>\n<td id=\"S5.T3.5.14.13.3\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">70.06</td>\n<td id=\"S5.T3.5.14.13.4\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">70.09</td>\n<td id=\"S5.T3.5.14.13.5\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\">70.03</td>\n<td id=\"S5.T3.5.14.13.6\" class=\"ltx_td ltx_align_right ltx_border_bb ltx_border_t\"><span id=\"S5.T3.5.14.13.6.1\" class=\"ltx_text ltx_font_bold\">70.28*</span></td>\n</tr>\n</tbody>\n</table>\n",
        "footnotes": "",
        "references": [
            "To obtain a deeper understanding of our improvements over baseline approaches, we categorized question types into 12 fine-grained bins, similarly to Kafle and Kanan (2017).\nTable¬†3 shows a detailed breakdown of accuracy results by category type.\nWe used the validation set, rather than the test set, since we needed access to the ground truth annotations to calculate the per-category accuracy. For the same reason, we can only perform the paired t-test on the full validation set. As can be seen, all ablated models obtain inferior performance to our full model on the validation set (statistically significant at the 0.05 level).\nFor most categories, MULAN achieves the highest accuracy.\nMoreover, in comparison to the baseline, our method is the best performing one in 10 out of 12 categories with especially clear improvements in activity recognition and sentiment analysis categories.\nMULAN expectedly reduces accuracy on reading questions that other models can most likely only answer by bias exploitation, and improves on small bins like attribute.\nThe distances between the models are small in absolute terms, but given the vastly different bin sizes the relative improvements are large.\nThis underlines the robustness of improvements with human-like attention integration and, in particular, multimodal integration."
        ]
    }
}