{
    "id_table_1": {
        "caption": "Table 3:  Detailed breakdown of human evaluation: faithfulness to entity.",
        "table": "S0.F1.8",
        "footnotes": [],
        "references": [
            "Despite the growing attention to factuality issues in large language models, significantly less focus has been placed on the factual accuracy of image generation models. Existing benchmarks for text-to-image generation predominantly assess alignment with general textual descriptions  (Lin et al.,  2015 ) , compliance with image-editing instructions  (Ku et al.,  2024 ) , or adherence to specified spatial relationships  (Gokhale et al.,  2022 ) .  These benchmarks fall short in evaluating how well models can generate images that accurately reproduce the precise visual details of real-world entities, objects, and scenes grounded in trustworthy knowledge sources (see examples in Figure  1 ).",
            "Selection of image-entity alignment metrics.  We conduct an ablation study to select the most appropriate image-entity alignment metric in Table  1 . Two popular visual features are tested for calculating cosine similarity scores between reference and generated images:  CLIP-Image   (Radford et al.,  2021 )  and  DINO   (Oquab et al.,  2024 ) .",
            "In the example below, Custom-Diff and DreamBooth, show reduced instruction-following compared to their backbone model, SD. In particular, Custom-Diff struggles to create novel compositions like placing the target entity next to a giant sandcastle.  In contrast, the backbone models excel in instruction-following and entity faithfulness, likely because Rolls-Royce Phantom Drophead Coupe is well-represented in their training data. The retrieval-augmented models underperform due to over-relying on the reference images and knowledge forgetting during fine-tuning on small support sets.  These results suggest that the success of retrieval-augmented methods highly depends on the entity domain and customization approach.  More results can be found in Section  A.1 .",
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively.",
            "Figure  11  shows the results for the insect domain. For the prompt, Satyrium liparops sitting at the beach with a view of the sea, all models correctly generate the beach scene as instructed. However, the backbone models hallucinate the insects appearance, generating a completely different insect, while the retrieval models accurately depict the visual details of the Satyrium liparops. For the prompt, Promachus hinei wearing sunglasses, the backbone models demonstrate strong instruction-following ability, with Imagen and Imagen-3 correctly composing the sunglasses on the insect. In contrast, the retrieval models fail to generate this composition. However, the backbone models do not generate an accurate representation of the insect itself, while the retrieval models correctly generate the entity.",
            "Figure  13  presents the results for the plant domain. In the example, An impressionistic painting of Cirsium andersonii, none of the models effectively follow the prompt in generating the impressionistic painting, highlighting the challenge of rendering specific artistic styles. However, the retrieval models generate the entity more accurately than the backbone models. In another example, Penstemon rydbergii on a rustic wooden table next to a rose plant, although the generated entity lacks the fine details of the reference, the backbone models successfully capture the composition of next to a rose, demonstrating their stronger instruction-following ability."
        ]
    },
    "id_table_2": {
        "caption": "Table 4:  Detailed breakdown of human evaluation: instruction-following.",
        "table": "S5.T1.1",
        "footnotes": [],
        "references": [
            "To fill this critical gap in evaluating whether the image generation models can reproduce visual world knowledge, we introduce  Kitten , a benchmark dataset and evaluation suite specifically designed to assess how well these models can generate visually accurate representations of real-world entities grounded in trust-worthy knowledge sources. Unlike prior benchmarks that primarily focus on aesthetics, general text alignment, or commonsense reasoning,  Kitten  leverages prompts derived from visual entities documented in Wikipedia  (Hu et al.,  2023a ) , a widely recognized and trustworthy knowledge base, and evaluates real-world entities over 8 visual domains ( i.e. , aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport in Figure  2 ). This ensures that the generated images are evaluated based on comparison with reliable and verifiable visual information crowdsourced by the large internet population. Moreover, we have developed a comprehensive set of human evaluation criteria that emphasize the precise and accurate visual depiction of these entities, capturing subtle yet critical details essential for visual correctness. By directly assessing the fidelity of entities in the generated images against established knowledge,  Kitten  aims to promote the evaluation of world knowledge for image generation models.",
            "To create a diverse set of prompts focusing on faithfulness to knowledge-grounded concepts, we select several categories of entities from the OVEN-Wiki  (Hu et al.,  2023a )  dataset as the basis for the image generation prompts. Figure  2  shows the process of creating the benchmark. We select entities from eight specialized domains, including human-made objects (aircraft, vehicle, landmark, cuisine), natural species (flower, plant, insect), and human activities (sport).",
            "Impact of backbone model improvements.   The results indicate that improvements to base models alone can enhance both instruction-following and faithfulness. For example, Flux outperforms its predecessor SD by 0.23, and Imagen-3 surpasses its predecessor Imagen by 0.35 in faithfulness. However, these faithfulness improvements are still minor compared to those achieved by retrieval-augmented methods, such as DreamBooth, which shows a 0.57 improvement over SD. This suggests that simply enhancing the text-to-image model is not sufficient; additional methods like retrieval augmentation are necessary to achieve higher levels of faithfulness.  Interestingly, we observe that the Imagen family, Imagen (2.82) and Imagen-3 (3.17), consistently achieve higher faithfulness scores compared to the SD family, SD (2.51) and Flux (2.74). Detailed human evaluation scores across domains can be found in Section  A.2 .",
            "Correlation between automatic metrics and human evaluation.   To quantify the consistency between automatic metrics and human evaluation, we computed Pearson and Spearman correlations in Table  2 . CLIP-Text and CLIP-Image show moderate alignment with user evaluations.",
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively.",
            "To quantify the consistency between automatic metrics and human evaluation results, we computed Pearson and Spearman correlations in Table  2 . CLIP-Text and CLIP-Image show moderate alignment with user evaluations.  Here, we provide a detailed breakdown of the correlation across eight domains in Table  6 .  While these metrics show some alignment with human evaluations, discrepancies remain in certain categories. Notably, the correlation between CLIP-T and the instruction-following score in the landmark domain, as well as the correlation between CLIP-I and the faithfulness score in the plant domain, are negative. This underscores the limitations of automatic metrics in fully capturing human judgment.  Although DINO demonstrates stronger alignment with user evaluations of faithfulness, correlation variability persists across categories. For instance, correlations are lower in the vehicle and landmark domains, highlighting the need for more accurate automatic metrics to better reflect human evaluations and assess model performance.",
            "We present the design of the  Kitten  benchmark, which focuses on evaluating faithfulness to knowledge-grounded concepts in Figure  2  and Section  3.2 .  To ensure diversity, we select entities from eight specialized domains and construct diverse prompts in each domain for evaluation. For each entity, we collect a set of support images as inputs to assess retrieval-augmented models, where support images are used to enhance the models predictions.  In addition, we collect a set of evaluation images for conducting human evaluation.  A detailed breakdown of the data statistics is provided in Table  7 ."
        ]
    },
    "id_table_3": {
        "caption": "Table 5:  Detailed breakdown of automatic metrics.",
        "table": "S5.T2.1",
        "footnotes": [],
        "references": [
            "We design a set of human evaluations targeting the visual fidelity of the target entity in the generated image. We propose to decompose the evaluation into two different aspects: 1) faithfulness to the prompt entity; and 2) adherence to prompt instructions beyond the prompt entity. This design allows the human raters to focus on two clearly defined criteria. It also enables more informative analysis and comparison of different models, as our results show that there are often trade-offs between these two aspects. We show the reference images of the prompt entity but also encourage human raters to conduct their own research to verify the faithfulness of the entity. We use the average of 5 raters for each generated image as the final human evaluation score. The human annotation interface is shown in Figure  3 . We also include open-ended questions to gather qualitative feedback. Full rater instructions are shown in Section  A.6 .",
            "Performance across domains.   Figure  5  shows that the performance of each method is domain-dependent.  Retrieval-augmented models generally achieve higher image-entity alignment scores than backbone models in the insect, landmark, and plant domains.  These domains contain less frequent terms in common image datasets (e.g., LAION), meaning that these visual concepts are not well-represented in the base models parameters. The retrieval-augmented models perform better by incorporating reference images during inference. In contrast, the retrieval-augmented method, Custom-Diff, performs worse than its base model, SD, in the cuisine and sport domains.  These domains contain common terms, such as snowboarding and guacamole, which the SD model has well-memorized. The Custom-Diff models performance degrades, potentially due to fine-tuning on a smaller reference set.  This variability suggests that the effectiveness of a retrieval-augmented method may be influenced by the nature of the domain-specific content, as well as the model and customization method used. The results indicate that the optimal choice of retrieval-augmented method remains an open question and can be task or domain-specific.  Detailed automatic metrics across domains can be found in Section  A.3 .",
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively.",
            "Figure  13  presents the results for the plant domain. In the example, An impressionistic painting of Cirsium andersonii, none of the models effectively follow the prompt in generating the impressionistic painting, highlighting the challenge of rendering specific artistic styles. However, the retrieval models generate the entity more accurately than the backbone models. In another example, Penstemon rydbergii on a rustic wooden table next to a rose plant, although the generated entity lacks the fine details of the reference, the backbone models successfully capture the composition of next to a rose, demonstrating their stronger instruction-following ability.",
            "In Figure  4  (Top), we present the human evaluation results for backbone and retrieval-augmented text-to-image models, comparing their performance in terms of entity faithfulness and instruction-following accuracy.  We provide a detailed breakdown of the human evaluation results across eight domains of the  Kitten  benchmark in Table  3  and Table  4 .",
            "We present the design of the  Kitten  benchmark, which focuses on evaluating faithfulness to knowledge-grounded concepts in Figure  2  and Section  3.2 .  To ensure diversity, we select entities from eight specialized domains and construct diverse prompts in each domain for evaluation. For each entity, we collect a set of support images as inputs to assess retrieval-augmented models, where support images are used to enhance the models predictions.  In addition, we collect a set of evaluation images for conducting human evaluation.  A detailed breakdown of the data statistics is provided in Table  7 ."
        ]
    },
    "id_table_4": {
        "caption": "Table 6:  Per-category correlation between automatic metrics and human evaluation.",
        "table": "S5.F6.16",
        "footnotes": [],
        "references": [
            "For each entity in the selected categories, we collect a set of reference images from Wikipedia for human evaluation. We also provide a support set of entity images, so that we can evaluate retrieval-augmented models that can take advantage of external knowledge sources (see discussions on evaluated models in Section  4 ). Detailed statistics of the data can be found in Section  A.5 .",
            "Improvement in faithfulness and reduction in instruction-following.   Figure  4  (Top) shows that retrieval-augmented modelsCustom-Diff, DreamBooth, and Instruct-Imagengenerally exhibit much better faithfulness to the entity compared to their base models, SD and Imagen. This is because these models can incorporate reference images during testing, allowing them to generate visual concepts that are not well-represented in the base models parameters.  However, we find that retrieval-augmented models tend to have reduced instruction-following capabilities compared to their backbone models.",
            "Improvement in image-entity alignment and reduction in image-text alignment.   Figure  4  (Bottom) shows that retrieval models increase the image-entity alignment score while decreasing the image-text alignment compared to their respective base models. In particular, Instruct-Imagen stands out with a substantial increase in the image-entity score (0.386   \\rightarrow  0.582) and the most significant drop in the image-text score (0.329   \\rightarrow  0.307).  These observations align with the human evaluation, where retrieval-augmented models show improved faithfulness in entity but reduced instruction-following.  In addition, this trend is consistent with observations in recent works  (Materzynska et al.,  2023 ) , which indicate that models incorporating additional inputs, such as reference images, tend to have lower image-text alignment scores compared to base models. This is due to a trade-off between aligning with the text and with the images. With longer training times, alignment to reference images improves, while alignment to the text decreases.",
            "Compared to CLIP-I metrics, DINO demonstrates stronger alignment with user evaluations of faithfulness, with increasing Pearson (0.239   \\rightarrow  0.510) and Spearman (0.340   \\rightarrow  0.504) correlations. This suggests that DINO is more effective at measuring faithfulness than CLIP-Image.  However, the variability in correlations varies across categories, such as the negative correlation between CLIP-I and faithfulness in the plant domain, highlighting that more precise evaluation metrics are still needed to accurately reflect human judgments.  Detailed correlation scores across domains can be found in Section  A.4 .",
            "We present the visual results in Figure  6 .  In the above example, the backbone modelsSD, Flux, and Imagenexhibit lower faithfulness to the entity, with mismatches in the tail fin, engine, and wing shapes compared to the reference image. In contrast, retrieval-augmented models like Custom-Diff, DreamBooth, and Instruct-Imagen, which use support images during testing, show better visual alignment with the target. Surprisingly, Imagen-3, despite relying only on text prompts, generates accurate visual details due to its strong language understanding capability. These observations align with the evaluation results in Figure  4 .",
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively.",
            "In Figure  4  (Top), we present the human evaluation results for backbone and retrieval-augmented text-to-image models, comparing their performance in terms of entity faithfulness and instruction-following accuracy.  We provide a detailed breakdown of the human evaluation results across eight domains of the  Kitten  benchmark in Table  3  and Table  4 .",
            "In Figure  4  (Bottom) and Figure  5 , we present the automatic metric evaluation for backbone and retrieval-augmented text-to-image models, including image-text alignment and image-entity alignment scores.  Here, we include a detailed breakdown of the automatic metrics across eight domains of the  Kitten  benchmark in Table  5 . Specifically, we include a detailed ablation study of the image-entity alignment metric using cosine similarity scores based on two popular image features: the CLIP-Image and the DINO features."
        ]
    },
    "id_table_5": {
        "caption": "Table 7:  Statistics of  Kitten  benchmark.",
        "table": "A1.F7.24",
        "footnotes": [],
        "references": [
            "For each entity in the selected categories, we collect a set of reference images from Wikipedia for human evaluation. We also provide a support set of entity images, so that we can evaluate retrieval-augmented models that can take advantage of external knowledge sources (see discussions on evaluated models in Section  4 ). Detailed statistics of the data can be found in Section  A.5 .",
            "Performance across domains.   Figure  5  shows that the performance of each method is domain-dependent.  Retrieval-augmented models generally achieve higher image-entity alignment scores than backbone models in the insect, landmark, and plant domains.  These domains contain less frequent terms in common image datasets (e.g., LAION), meaning that these visual concepts are not well-represented in the base models parameters. The retrieval-augmented models perform better by incorporating reference images during inference. In contrast, the retrieval-augmented method, Custom-Diff, performs worse than its base model, SD, in the cuisine and sport domains.  These domains contain common terms, such as snowboarding and guacamole, which the SD model has well-memorized. The Custom-Diff models performance degrades, potentially due to fine-tuning on a smaller reference set.  This variability suggests that the effectiveness of a retrieval-augmented method may be influenced by the nature of the domain-specific content, as well as the model and customization method used. The results indicate that the optimal choice of retrieval-augmented method remains an open question and can be task or domain-specific.  Detailed automatic metrics across domains can be found in Section  A.3 .",
            "In Figure  4  (Bottom) and Figure  5 , we present the automatic metric evaluation for backbone and retrieval-augmented text-to-image models, including image-text alignment and image-entity alignment scores.  Here, we include a detailed breakdown of the automatic metrics across eight domains of the  Kitten  benchmark in Table  5 . Specifically, we include a detailed ablation study of the image-entity alignment metric using cosine similarity scores based on two popular image features: the CLIP-Image and the DINO features."
        ]
    },
    "id_table_6": {
        "caption": "",
        "table": "A1.F8.24",
        "footnotes": [],
        "references": [
            "We design a set of human evaluations targeting the visual fidelity of the target entity in the generated image. We propose to decompose the evaluation into two different aspects: 1) faithfulness to the prompt entity; and 2) adherence to prompt instructions beyond the prompt entity. This design allows the human raters to focus on two clearly defined criteria. It also enables more informative analysis and comparison of different models, as our results show that there are often trade-offs between these two aspects. We show the reference images of the prompt entity but also encourage human raters to conduct their own research to verify the faithfulness of the entity. We use the average of 5 raters for each generated image as the final human evaluation score. The human annotation interface is shown in Figure  3 . We also include open-ended questions to gather qualitative feedback. Full rater instructions are shown in Section  A.6 .",
            "We present the visual results in Figure  6 .  In the above example, the backbone modelsSD, Flux, and Imagenexhibit lower faithfulness to the entity, with mismatches in the tail fin, engine, and wing shapes compared to the reference image. In contrast, retrieval-augmented models like Custom-Diff, DreamBooth, and Instruct-Imagen, which use support images during testing, show better visual alignment with the target. Surprisingly, Imagen-3, despite relying only on text prompts, generates accurate visual details due to its strong language understanding capability. These observations align with the evaluation results in Figure  4 .",
            "To quantify the consistency between automatic metrics and human evaluation results, we computed Pearson and Spearman correlations in Table  2 . CLIP-Text and CLIP-Image show moderate alignment with user evaluations.  Here, we provide a detailed breakdown of the correlation across eight domains in Table  6 .  While these metrics show some alignment with human evaluations, discrepancies remain in certain categories. Notably, the correlation between CLIP-T and the instruction-following score in the landmark domain, as well as the correlation between CLIP-I and the faithfulness score in the plant domain, are negative. This underscores the limitations of automatic metrics in fully capturing human judgment.  Although DINO demonstrates stronger alignment with user evaluations of faithfulness, correlation variability persists across categories. For instance, correlations are lower in the vehicle and landmark domains, highlighting the need for more accurate automatic metrics to better reflect human evaluations and assess model performance."
        ]
    },
    "id_table_7": {
        "caption": "",
        "table": "A1.F9.24",
        "footnotes": [],
        "references": [
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively.",
            "We present the design of the  Kitten  benchmark, which focuses on evaluating faithfulness to knowledge-grounded concepts in Figure  2  and Section  3.2 .  To ensure diversity, we select entities from eight specialized domains and construct diverse prompts in each domain for evaluation. For each entity, we collect a set of support images as inputs to assess retrieval-augmented models, where support images are used to enhance the models predictions.  In addition, we collect a set of evaluation images for conducting human evaluation.  A detailed breakdown of the data statistics is provided in Table  7 ."
        ]
    },
    "id_table_8": {
        "caption": "",
        "table": "A1.F10.24",
        "footnotes": [],
        "references": [
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively."
        ]
    },
    "id_table_9": {
        "caption": "",
        "table": "A1.F11.24",
        "footnotes": [],
        "references": [
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively.",
            "Figure  9  presents the results for the cuisine domain. For the prompt, A Wakame dish with a cherry flower on top of it, retrieval-based models such as Custom-Diff, DreamBooth, and Instruct-Imagen successfully generate the entity Wakame. However, Custom-Diff fails to capture the composition of the cherry flower. In contrast, the backbone modelsImagen, Flux, and Imagen-3correctly generate the cherry flower, but the representation of Wakame does not align with its real-world appearance. In the example prompt, photo of a Hot and sour soup, the backbone models introduce incorrect ingredients that significantly deviate from a real-world hot and sour soup, demonstrating how these models hallucinate entities instead of reproducing real-world knowledge accurately."
        ]
    },
    "id_table_10": {
        "caption": "",
        "table": "A1.F12.24",
        "footnotes": [],
        "references": [
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively."
        ]
    },
    "id_table_11": {
        "caption": "",
        "table": "A1.F13.24",
        "footnotes": [],
        "references": [
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively.",
            "Figure  11  shows the results for the insect domain. For the prompt, Satyrium liparops sitting at the beach with a view of the sea, all models correctly generate the beach scene as instructed. However, the backbone models hallucinate the insects appearance, generating a completely different insect, while the retrieval models accurately depict the visual details of the Satyrium liparops. For the prompt, Promachus hinei wearing sunglasses, the backbone models demonstrate strong instruction-following ability, with Imagen and Imagen-3 correctly composing the sunglasses on the insect. In contrast, the retrieval models fail to generate this composition. However, the backbone models do not generate an accurate representation of the insect itself, while the retrieval models correctly generate the entity."
        ]
    },
    "id_table_12": {
        "caption": "",
        "table": "A1.F14.24",
        "footnotes": [],
        "references": [
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively."
        ]
    },
    "id_table_13": {
        "caption": "",
        "table": "A1.T3.7",
        "footnotes": [],
        "references": [
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively.",
            "Figure  13  presents the results for the plant domain. In the example, An impressionistic painting of Cirsium andersonii, none of the models effectively follow the prompt in generating the impressionistic painting, highlighting the challenge of rendering specific artistic styles. However, the retrieval models generate the entity more accurately than the backbone models. In another example, Penstemon rydbergii on a rustic wooden table next to a rose plant, although the generated entity lacks the fine details of the reference, the backbone models successfully capture the composition of next to a rose, demonstrating their stronger instruction-following ability."
        ]
    },
    "id_table_14": {
        "caption": "",
        "table": "A1.T4.7",
        "footnotes": [],
        "references": [
            "We present additional qualitative examples evaluating the backbone modelsSD, Imagen, Flux, and Imagen-3as well as the retrieval modelsCustom-Diff, DreamBooth, and Instruct-Imagenacross various domains in the  Kitten  benchmark. The results for the aircraft, vehicle, cuisine, flower, insect, landmark, plant, and sport domains are shown in Figure  7 , Figure  8 , Figure  9 , Figure  10 , Figure  11 , Figure  12 , Figure  13 , Figure  14 , respectively."
        ]
    },
    "id_table_15": {
        "caption": "",
        "table": "A1.T5.21",
        "footnotes": [],
        "references": []
    },
    "id_table_16": {
        "caption": "",
        "table": "A1.T6.1",
        "footnotes": [],
        "references": []
    },
    "id_table_17": {
        "caption": "",
        "table": "A1.T7.3",
        "footnotes": [],
        "references": []
    },
    "global_footnotes": [
        "Equal contribution"
    ]
}