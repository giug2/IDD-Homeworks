{
    "id_table_1": {
        "caption": "Table 1:  Attributes sampled for the attributed prompt string generation.",
        "table": "S1.F2.1",
        "footnotes": [
            ""
        ],
        "references": [
            "In this paper, we introduce  StyleDistance , a novel method for training stronger, content-independent style embeddings which leverages synthetic parallel text examples generated by a large language model (LLM)  (OpenAI et al.,  2024 ) . By creating near-exact paraphrases with controlled stylistic variations, we produce positive and negative examples across 40 distinct style features. This synthetic dataset, which we call  SynthStel , enables more precise contrastive learning (visualized in Figure  1 ) and is more robust to the content leakage inherent in existing datasets. We evaluate our method on both human and automated benchmarks, measuring the content-independence, quality, and utility of  StyleDistance  embeddings.",
            "There is no predefined set of style features, and what features are considered to describe style vs. content can vary across different studies  (Jin et al.,  2022 ) . For this work, we select 40 style features across 7 broad categories (visualized in Figure  3 ) which have been addressed in different works on text style  (Tausczik and Pennebaker,  2010 ; Kang and Hovy,  2019 ; Wegmann and Nguyen,  2021 ; Jin et al.,  2022 ; Patel et al.,  2023 ) . Specifically, we select features for which it is possible to generate both positive and negative examples (e.g., formal/informal, passive/active voice). Since some features can blur the line between style and content (e.g., usage of sarcasm), it might be difficult to generate perfectly parallel positive and negative pairs, with the same content. For these features, we control the generation as much as possible with the aim to obtain near-exact paraphrases. Furthermore, some style features may be impossible to fully remove from a sentence in order to generate a negative example (e.g., usage of articles). For these, we aim for the positive example to contain the feature with higher frequency than the negative example. For more details on all the selected style features, see Appendix  A . While these 40 features may not cover the infinite number of styles that may exist, we believe they can serve to learn more primitive features (e.g., use of contractions, use of long words, use of formal style) which may help generalize to more complex styles involving these features (e.g., professorial style). We discuss and test this generalization assumption in Section  5.1 .",
            "Yu et al. ( 2023 )  found that LLMs struggle with diversity when prompted to generate text examples. We use their proposed attributed prompt (AttrPrompt) method to ensure generations are sufficiently diverse and varied across basic attributes, such as Sentence Length and Type of Sentence. The method randomly selects values from a defined set of attributes to be included in the prompt, which serve as conditioning for the text generation. In Table  1 , we showcase the attributes we sample from in our attributed prompt in order to vary our generations. See Appendix  B.2  for details on our full attributed prompt and inference parameters.",
            "For the Topic attribute, we sample fine-grained distinct topics for each generation from the C4 corpus  (Raffel et al.,  2020 ) . We do this by extracting a random sentence from a random document in C4, and we then use a zero-shot prompt (given in Appendix  B.1 ) with GPT-4 to identify the fine-grained topic of that sentence. We employ several heuristics to select sentences from C4 that have desired characteristics: written in English, sufficiently long (greater than 32 words), and consist of natural text rather than formatting text found in some C4 documents. We provide an implementation of these heuristics in our supplementary materials.",
            "After generating 100 pairs of positive and negative examples for each style feature, we construct feature-specific triplets as follows: We select an anchor ( a a a italic_a ) and a positive example ( p p p italic_p ) from different pairs available for a feature, ensuring that the two examples are  identical in style  but not in content. For the Usage of Active Voice example in Figure  1 ,  a a a italic_a  and  p p p italic_p  are two active sentences ( I adored ... ,  I observed... ) on different topics. As a negative example ( n n n italic_n ), we use the paraphrase of either  a a a italic_a  or  p p p italic_p  which does not contain the feature; therefore,  n n n italic_n  is always different in style. In the example in Figure  1 ,  n n n italic_n  is the paraphrase of the anchor in passive voice ( ...adored by me ).",
            "We train two versions of our model.  StyleDistance Synth subscript StyleDistance Synth \\textsc{StyleDistance}_{\\textsc{Synth}} StyleDistance start_POSTSUBSCRIPT Synth end_POSTSUBSCRIPT  is fine-tuned only on the synthetic triplets described in Section  4.1 . We also train a version using the synthetic triplets for data augmentation ( StyleDistance ). In this case, our training set is comprised of 50% natural datai.e. the triplets used to train the  Wegmann et al. ( 2022 )  model 3 3 3 We use the  train-conversation  split.  and 50% synthetic data. For the augmented model, we hypothesize that mixing in these perfectly parallel synthetic examples will help regularize the model, and discourage it from representing content-related features in favor of style-related ones offering the potential advantages of both approaches: (1) enhanced content-independence, and (2) the ability to capture niche style features in the natural data. We provide a visualization of the learned embedding space of our model after contrastive training using UMAP  (McInnes et al.,  2018 )  in Appendix  E .",
            "We reproduce the TinyStyler procedure with the exact dataset and hyperparameters in the original paper. We make only one modification: replacing Wegmann embeddings with  StyleDistance  in the generation and filtering steps. We include the formality transfer evaluation results in Table  12 . In these automatic evaluations, the  StyleDistance  conditioned model performs comparably. We additionally include examples comparing model output in Table  LABEL:table:tinystyler-outputs ."
        ]
    },
    "id_table_2": {
        "caption": "Table 2:  Results of the human and automatic evaluations of our synthetic dataset.",
        "table": "S3.T1.1",
        "footnotes": [],
        "references": [
            "The most common objective when training text embeddings is to place texts with similar semantics close together in the embedding space  (Reimers and Gurevych,  2019 ) . Style representations, by contrast, aim to embed texts with similar writing styles near each other and texts with different styles far apart, regardless of their semantic content  (Wegmann et al.,  2022 ) . Embeddings are usually trained via contrastive learning, with triplets consisting of an anchor text, a positive text (which should be embedded closely to the anchor), and a negative text (which should be embedded far from the anchor)  (Goldberger et al.,  2004 ; Khosla et al.,  2020 ; Schroff et al.,  2015 ) . Existing approaches often use social media datasets with the assumption that all writing by the same author shares a similar style and that texts by different authors exhibit dissimilar styles  (Wegmann et al.,  2022 ) .These methods also attempt to minimize content representation in the resulting embeddings. They select a text from the same author on a  different topic  as the positive example, and a text from a different author on the  same topic  as the negative example, approximating topic similarity using subreddit or conversation metadata. However, these methods are limited by the imperfect nature of data acquired under such assumptions. For example, the same author may write about the same topic even in different subreddits. As a result, these imperfect contrastive triplets do not explicitly control for content, leading to style embeddings with weak content-independence. The content leakage caused by such proxy objectives (illustrated in Figure  2 ) can undermine the effectiveness of style representations in tasks that require strict separation between style and content, such as stylistic analysis, authorship tasks, style transfer steering, and automatic style transfer evaluation. To overcome this, a more controlled approach to style contrastive learning is necessary.",
            "Yu et al. ( 2023 )  found that LLMs struggle with diversity when prompted to generate text examples. We use their proposed attributed prompt (AttrPrompt) method to ensure generations are sufficiently diverse and varied across basic attributes, such as Sentence Length and Type of Sentence. The method randomly selects values from a defined set of attributes to be included in the prompt, which serve as conditioning for the text generation. In Table  1 , we showcase the attributes we sample from in our attributed prompt in order to vary our generations. See Appendix  B.2  for details on our full attributed prompt and inference parameters.",
            "We compute a baseline for these scores with natural data from the dataset of sentence pairs in  Wegmann and Nguyen ( 2021 ) . The results of these evaluations can be found in Table  2 . Our generated examples fare well in all these aspects: they are topically diverse and fluent, and the similarity inside each pair of positive/negative examples is high. An additional (less direct) evaluation of the quality of our synthetic dataset is proposed in Section  5 , where we evaluate the  StyleDistance  embeddings that we train on this dataset.",
            "We list all style features selected for our synthetic dataset below along with the positive and negative prompts (used for constructing a full prompt for generating positive and negative examples as shown in Appendix  B.2 ) and definitions (used to help define the style feature to human annotators in the annotation interface in Appendix  C ).",
            "The fine-grained topic is then used as part of the attributed prompt described in Section  B.2  to ensure diversity in the generations.",
            "We reproduce the TinyStyler procedure with the exact dataset and hyperparameters in the original paper. We make only one modification: replacing Wegmann embeddings with  StyleDistance  in the generation and filtering steps. We include the formality transfer evaluation results in Table  12 . In these automatic evaluations, the  StyleDistance  conditioned model performs comparably. We additionally include examples comparing model output in Table  LABEL:table:tinystyler-outputs ."
        ]
    },
    "id_table_3": {
        "caption": "Table 3:  Accuracy on the STEL/STEL-or-Content (S-o-C) tasks.  StyleDistance  leads on both tasks among representations trained for content-independence, and  StyleDistance Synth subscript StyleDistance Synth \\textsc{StyleDistance}_{\\textsc{Synth}} StyleDistance start_POSTSUBSCRIPT Synth end_POSTSUBSCRIPT  generalizes remarkably well to real text data despite being trained only on synthetic data.",
        "table": "S3.T2.1",
        "footnotes": [],
        "references": [
            "There is no predefined set of style features, and what features are considered to describe style vs. content can vary across different studies  (Jin et al.,  2022 ) . For this work, we select 40 style features across 7 broad categories (visualized in Figure  3 ) which have been addressed in different works on text style  (Tausczik and Pennebaker,  2010 ; Kang and Hovy,  2019 ; Wegmann and Nguyen,  2021 ; Jin et al.,  2022 ; Patel et al.,  2023 ) . Specifically, we select features for which it is possible to generate both positive and negative examples (e.g., formal/informal, passive/active voice). Since some features can blur the line between style and content (e.g., usage of sarcasm), it might be difficult to generate perfectly parallel positive and negative pairs, with the same content. For these features, we control the generation as much as possible with the aim to obtain near-exact paraphrases. Furthermore, some style features may be impossible to fully remove from a sentence in order to generate a negative example (e.g., usage of articles). For these, we aim for the positive example to contain the feature with higher frequency than the negative example. For more details on all the selected style features, see Appendix  A . While these 40 features may not cover the infinite number of styles that may exist, we believe they can serve to learn more primitive features (e.g., use of contractions, use of long words, use of formal style) which may help generalize to more complex styles involving these features (e.g., professorial style). We discuss and test this generalization assumption in Section  5.1 .",
            "In their paper,  Wegmann and Nguyen ( 2021 )  provide a STEL and STEL-or-Content evaluation benchmark over five features with curated natural data. We test our models on this benchmark and present the results in Table  3 . Our results are consistent with results reported by  Wegmann and Nguyen ( 2021 ) , who showed that even untrained models like  roberta-base  can capture style information well, resulting in stronger STEL performance than any of their fine-tuned models. However, the more challenging STEL-or-Content task, which better tests content-independence, shows that only models specifically trained for content-independence are able to capture style features better than content features. Our results indicate that the embeddings generated with our  StyleDistance  approach lead over other style representations on both the STEL and STEL-or-Content tasks. Interestingly, we find that  StyleDistance Synth subscript StyleDistance Synth \\textsc{StyleDistance}_{\\textsc{Synth}} StyleDistance start_POSTSUBSCRIPT Synth end_POSTSUBSCRIPT  captures style remarkably well, and manages to generalize to the natural text examples in the evaluation benchmark despite only being trained on our synthetic contrastive triplets. We conclude using synthetic parallel examples during training makes the model more content-independent and helps better capture style."
        ]
    },
    "id_table_4": {
        "caption": "Table 4:  We evaluate how well  StyleDistance Synth subscript StyleDistance Synth \\textsc{StyleDistance}_{\\textsc{Synth}} StyleDistance start_POSTSUBSCRIPT Synth end_POSTSUBSCRIPT  embeddings generalize to unseen style features by ablating features from the synthetic training dataset under three conditions: In-Domain, Out-of-Domain, Out-of-Distribution. We evaluate their performance on the STEL and STEL-or-Content (S-o-C) tasks.",
        "table": "S4.T3.1.1",
        "footnotes": [
            ""
        ],
        "references": [
            "We train two versions of our model.  StyleDistance Synth subscript StyleDistance Synth \\textsc{StyleDistance}_{\\textsc{Synth}} StyleDistance start_POSTSUBSCRIPT Synth end_POSTSUBSCRIPT  is fine-tuned only on the synthetic triplets described in Section  4.1 . We also train a version using the synthetic triplets for data augmentation ( StyleDistance ). In this case, our training set is comprised of 50% natural datai.e. the triplets used to train the  Wegmann et al. ( 2022 )  model 3 3 3 We use the  train-conversation  split.  and 50% synthetic data. For the augmented model, we hypothesize that mixing in these perfectly parallel synthetic examples will help regularize the model, and discourage it from representing content-related features in favor of style-related ones offering the potential advantages of both approaches: (1) enhanced content-independence, and (2) the ability to capture niche style features in the natural data. We provide a visualization of the learned embedding space of our model after contrastive training using UMAP  (McInnes et al.,  2018 )  in Appendix  E .",
            "In an ablation study, we test the ability of our training approach to produce embeddings that can generalize to unseen style features which are not present in the synthetic dataset. We conduct this evaluation by ablating features from the data used to train  StyleDistance Synth subscript StyleDistance Synth \\textsc{StyleDistance}_{\\textsc{Synth}} StyleDistance start_POSTSUBSCRIPT Synth end_POSTSUBSCRIPT  under three conditions and show the results in Table  4 . In the  In-Domain  condition, all 40 style features are included. In the  Out-of-Domain  condition, we exclude synthetic examples corresponding to the five style features in the STEL/STEL-or-Content benchmark  4 4 4 In our 40 features, there are two separate features for emoji and text emoticons (:-D) so we exclude 6 total features for this condition instead of 5 features, resulting in 34 features. . In the  Out-of-Distribution  condition, we further exclude examples for any features similar or indirectly related to the five evaluated features. Details on the exact 15 style features ablated can be found in Appendix  H . We compare the Out-of-Domain and Out-of-Distribution performance of  StyleDistance Synth subscript StyleDistance Synth \\textsc{StyleDistance}_{\\textsc{Synth}} StyleDistance start_POSTSUBSCRIPT Synth end_POSTSUBSCRIPT  on the two tasks to its In-Domain performance, obtained when it was trained on data for all 40 style features. Even in the challenging Out-of-Distribution condition,  StyleDistance Synth subscript StyleDistance Synth \\textsc{StyleDistance}_{\\textsc{Synth}} StyleDistance start_POSTSUBSCRIPT Synth end_POSTSUBSCRIPT  retains 50% of its performance on the challenging STEL-or-Content task (see the Retained Perf. column). This study indicates our training approach generalizes reasonably well to out-of-domain style features which can be composed from style features selected for generation and, to some extent, even to out-of-distribution style features fully outside the selected set.",
            "For the generalization experiment and ablation results we demonstrate in Table  4 , we list the style features ablated (removed from the training data) for the Out-of-Domain and Out-of-Distribution conditions.      Out-of-Domain :"
        ]
    },
    "id_table_5": {
        "caption": "Table 5:  Results obtained by LISA and the  Wegmann et al. ( 2022 )  embeddings on STEL and STEL-or-Content instances created from the test split of our  SynthStel  dataset. See Appendix  G  for full results.",
        "table": "S4.T4.3.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "There is no predefined set of style features, and what features are considered to describe style vs. content can vary across different studies  (Jin et al.,  2022 ) . For this work, we select 40 style features across 7 broad categories (visualized in Figure  3 ) which have been addressed in different works on text style  (Tausczik and Pennebaker,  2010 ; Kang and Hovy,  2019 ; Wegmann and Nguyen,  2021 ; Jin et al.,  2022 ; Patel et al.,  2023 ) . Specifically, we select features for which it is possible to generate both positive and negative examples (e.g., formal/informal, passive/active voice). Since some features can blur the line between style and content (e.g., usage of sarcasm), it might be difficult to generate perfectly parallel positive and negative pairs, with the same content. For these features, we control the generation as much as possible with the aim to obtain near-exact paraphrases. Furthermore, some style features may be impossible to fully remove from a sentence in order to generate a negative example (e.g., usage of articles). For these, we aim for the positive example to contain the feature with higher frequency than the negative example. For more details on all the selected style features, see Appendix  A . While these 40 features may not cover the infinite number of styles that may exist, we believe they can serve to learn more primitive features (e.g., use of contractions, use of long words, use of formal style) which may help generalize to more complex styles involving these features (e.g., professorial style). We discuss and test this generalization assumption in Section  5.1 .",
            "We compute a baseline for these scores with natural data from the dataset of sentence pairs in  Wegmann and Nguyen ( 2021 ) . The results of these evaluations can be found in Table  2 . Our generated examples fare well in all these aspects: they are topically diverse and fluent, and the similarity inside each pair of positive/negative examples is high. An additional (less direct) evaluation of the quality of our synthetic dataset is proposed in Section  5 , where we evaluate the  StyleDistance  embeddings that we train on this dataset.",
            "Our previous experiments demonstrate that training on our synthetic dataset yields strong style embeddings. Next, we investigate whether the synthetic dataset can be used for an entirely different purpose: to probe which specific style features are captured by existing style representations. Our  SynthStel  dataset allows for the creation of synthetic STEL and STEL-or-Content task instances across a range of 40 style featuresmuch broader than the  Wegmann and Nguyen ( 2021 )  benchmark where five features were addressed. We use the test split of  SynthStel  to generate task instances for probing. We examine whether LISA vectors and the  Wegmann et al. ( 2022 )  style embeddings capture these 40 style features, which  StyleDistance  models are directly trained to represent. We show average results over all 40 features in Table  5 . Per feature results are provided in Appendix  G . Our findings reveal only moderate coverage by LISA and  Wegmann et al. ( 2022 ) , with high variance depending on the evaluated feature (e.g., Usage of Nominalizations is poorly captured, with a near-zero STEL-or-Content score for both models). We calculate the mean squared error (MSE) between the STEL and STEL-or-Content scores for the real and synthetic task instances across the five features in the real benchmark, finding an average MSE of 0.039. This small MSE value shows that using synthetic data for probing can reasonably serve to assess which style features are represented by a model without need for manual example curation.",
            "We compare the embedding space of  Wegmann et al. ( 2022 )  and  StyleDistance  on informal/formal texts from GYAFC 6 6 6  Grammarlys Yahoo Answers Formality Corpus which contains 110K informal/formal sentence pairs.   (Rao and Tetreault,  2018 )  in Figure  5  below."
        ]
    },
    "id_table_6": {
        "caption": "Table 6:  ROC-AUC results on the PAN 2011-2015 Authorship Verification (AV) shared tasks.",
        "table": "S5.T5.1",
        "footnotes": [
            ""
        ],
        "references": [
            "We first test our style embeddings on the authorship verification (AV) task  (Koppel and Winter,  2014 ) . Given two documents by unknown authors, the goal of the authorship verification (AV) task is to determine whether they were written by the same author, based on their stylistic similarities and differences  (Kocher and Savoy,  2017 ) . We use a series of AV shared task datasets released by PAN in 2011-2015  (Argamon and Juola,  2011 ; Juola and Stamatatos,  2013 ; Stamatatos et al.,  2014 ,  2015 ) 5 5 5 PAN is the Plagiarism Analysis, Authorship Identification, and Near-Duplicate Detection workshop. No AV shared task was proposed in 2012. (URL:  https://pan.webis.de ) . Since the two documents may be about different topics, good content-independent style representations would be expected to perform better in the AV task than embeddings that capture content. We calculate the cosine similarity of the two documents using our tested style embedding models with no fine-tuning, to measure their off-the-shelf ability to identify whether two documents were written by the same author and report results using the standard ROC-AUC metric used in AV. In Table  6 , we compare the performance of  StyleDistance  embeddings against LISA and the  Wegmann et al. ( 2022 )  style embeddings. On average,  StyleDistance  outperforms the other representations on AV, demonstrating its effectiveness in representing style."
        ]
    },
    "id_table_7": {
        "caption": "Table 7:  Accuracy results on style transfer evaluation.",
        "table": "S5.T6.1.1",
        "footnotes": [
            ""
        ],
        "references": [
            "Patel et al. ( 2022 )  proposed the LUAR embedding model as an automatic measure for style transfer accuracy. This approach was effective and was subsequently adopted for style transfer evaluation  (Liu et al.,  2024 ; Horvitz et al.,  2023 ,  2024 ) . However, the LUAR model considers both style and content, hence confounding two aspects of style transfer evaluationaccuracy and meaning preservationwhich are typically measured separately. Content-independent style representations would be a better measure for this task. We use the same evaluation dataset of 675 task instances used by  Patel et al. ( 2022 )  where given an example text of a target authors style, the task is to discriminate which of two texts (a style transfer output and another actual text by the target author) is written by the target author. We show our results on this task in Table  7 .  StyleDistance  proves to be a more effective discriminator than all models, including LUAR. All models surpass human performance in distinguishing style transfer outputs. Since automatic style transfer evaluation typically includes a separate score for meaning preservation, using a model like LUAR (which is not content-independent) for measuring style transfer accuracy undermines the rigor of the style transfer accuracy metric. We find that a robust content-independent model like  StyleDistance  may enhance automatic style transfer evaluation by: (1) acting as a stronger discriminator, and (2) ensuring style transfer accuracy is assessed independently of meaning preservation."
        ]
    },
    "id_table_8": {
        "caption": "Table 8:  A demonstration of TinyStyler conditioned on  StyleDistance  embeddings.",
        "table": "S5.T7.1",
        "footnotes": [
            ""
        ],
        "references": [
            "Previous systems, like TinyStyler, have leveraged style embeddings to steer style transfer  (Horvitz et al.,  2024 ) . While the original TinyStyler system rewrites text by conditioning on  Wegmann et al. ( 2022 )  embeddings, we demonstrate that  StyleDistance  provides an alternative, and reproduce TinyStyler with  StyleDistance  embeddings. We showcase an example of an output in Table  8  with more details and results in Appendix  I . We will make this version of TinyStyler available as a resource. With this result, we demonstrate  StyleDistance  can be used as a simple drop-in replacement for downstream applications in systems where weaker style representations have been previously used."
        ]
    },
    "id_table_9": {
        "caption": "Table 9:  The style features selected for synthetic data generation in this work.",
        "table": "S5.T8.2",
        "footnotes": [
            ""
        ],
        "references": []
    },
    "id_table_10": {
        "caption": "Table 10:  Hyperparameters selected for contrastive learning training experiments.",
        "table": "A1.T9.1",
        "footnotes": [],
        "references": []
    },
    "id_table_11": {
        "caption": "Table 11:  STEL and STEL-or-Content results for top-performing models on the  SynthStel  test split. This table shows the performance variations and coverage of LISA and  Wegmann et al. ( 2022 )  embeddings for the 40 different style features found in the dataset. After training  StyleDistance  models on the  SynthStel  train split, we observe strong coverage of these 40 style features as expected, demonstrating the successful distillation of the LLMs strong style knowledge into a more efficient representation model  (Hinton et al.,  2015 ) .",
        "table": "A4.T10.1",
        "footnotes": [
            "",
            ""
        ],
        "references": []
    },
    "id_table_12": {
        "caption": "Table 12:  We reproduce the automatic formality transfer evaluation procedure from TinyStyler  Horvitz et al. ( 2024 )  on the GYAFC dataset  Rao and Tetreault ( 2018 ) .   F  absent F \\rightarrow F  italic_F  corresponds to formal transfer, and   I  absent I \\rightarrow I  italic_I  corresponds to informal transfer.",
        "table": "A7.T11.1.1",
        "footnotes": [
            "",
            ""
        ],
        "references": [
            "We reproduce the TinyStyler procedure with the exact dataset and hyperparameters in the original paper. We make only one modification: replacing Wegmann embeddings with  StyleDistance  in the generation and filtering steps. We include the formality transfer evaluation results in Table  12 . In these automatic evaluations, the  StyleDistance  conditioned model performs comparably. We additionally include examples comparing model output in Table  LABEL:table:tinystyler-outputs ."
        ]
    },
    "id_table_13": {
        "caption": "Table 13:  Outputs from TinyStyler, conditioned on Wegmann embeddings and  StyleDistance  embeddings.",
        "table": "A9.T12.10",
        "footnotes": [],
        "references": []
    },
    "id_table_14": {
        "caption": "",
        "table": "A9.T13.4",
        "footnotes": [],
        "references": []
    }
}