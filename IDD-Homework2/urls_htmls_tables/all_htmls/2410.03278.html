<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>What do Large Language Models Need for Machine Translation Evaluation?</title>
<!--Generated on Wed Oct  9 12:06:09 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2410.03278v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S1" title="In What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S2" title="In What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S3" title="In What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4" title="In What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS1" title="In 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS2" title="In 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Zero-shot Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS3" title="In 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>CoT Prompting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS4" title="In 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Few-shot Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS5" title="In 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Model Selection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS6" title="In 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS6.SSS0.Px1" title="In 4.6 Experimental Setup ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title">Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS6.SSS0.Px2" title="In 4.6 Experimental Setup ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title">Formatting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS6.SSS0.Px3" title="In 4.6 Experimental Setup ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title">Evaluation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5" title="In What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.SS1" title="In 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.SS2" title="In 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Zero-shot Inference</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.SS2.SSS0.Px1" title="In 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title">Comparing results among LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.SS2.SSS0.Px2" title="In 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title">Comparing with the baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.SS2.SSS0.Px3" title="In 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title">Comparison among Templates</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.SS2.SSS0.Px4" title="In 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title">Results among different language pairs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.SS3" title="In 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>CoT and Few-shot Inference</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.SS4" title="In 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Discussion</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S6" title="In What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion and Future Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#A1" title="In What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span> Pearson’s <math alttext="r" class="ltx_Math" display="inline"><semantics><mi>r</mi><annotation-xml encoding="MathML-Content"><ci>𝑟</ci></annotation-xml><annotation encoding="application/x-tex">r</annotation><annotation encoding="application/x-llamapun">italic_r</annotation></semantics></math> and Kendall’s <math alttext="\tau" class="ltx_Math" display="inline"><semantics><mi>τ</mi><annotation-xml encoding="MathML-Content"><ci>𝜏</ci></annotation-xml><annotation encoding="application/x-tex">\tau</annotation><annotation encoding="application/x-llamapun">italic_τ</annotation></semantics></math> Correlation Scores</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document">
<h1 class="ltx_title ltx_title_document">What do Large Language Models Need for 
<br class="ltx_break"/>Machine Translation Evaluation?</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shenbin Qian<sup class="ltx_sup" id="id1.1.id1">1</sup>, Archchana Sindhujan<sup class="ltx_sup" id="id2.2.id2">2</sup><sup class="ltx_sup" id="id3.3.id3">*</sup>, Minnie Kabra<sup class="ltx_sup" id="id4.4.id4">3</sup><sup class="ltx_sup" id="id5.5.id5">*</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id6.6.id6">Diptesh Kanojia<sup class="ltx_sup" id="id6.6.id6.1">2</sup>, Constantin Orăsan<sup class="ltx_sup" id="id6.6.id6.2">1</sup>, Tharindu Ranasinghe<sup class="ltx_sup" id="id6.6.id6.3">4</sup> and Frédéric Blain<sup class="ltx_sup" id="id6.6.id6.4">5</sup></span>
<br class="ltx_break"/><sup class="ltx_sup" id="id7.7.id7">1</sup>Centre for Translation Studies, University of Surrey, United Kingdom, 
<br class="ltx_break"/><sup class="ltx_sup" id="id8.8.id8">2</sup>Institute for People-Centred AI, University of Surrey, United Kingdom, 
<br class="ltx_break"/><sup class="ltx_sup" id="id9.9.id9">3</sup>Independent Researcher, India,
<br class="ltx_break"/><sup class="ltx_sup" id="id10.10.id10">4</sup>Lancaster University, United Kingdom,
<sup class="ltx_sup" id="id11.11.id11">5</sup>Tilburg University, The Netherlands 
<br class="ltx_break"/>{s.qian, a.sindhujan, d.kanojia, c.orasan}@surrey.ac.uk, minniekabra@gmail.com, 
<br class="ltx_break"/>t.ranasinghe@lancaster.ac.uk, f.l.g.blain@tilburguniversity.edu
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id12.id1">Leveraging large language models (LLMs) for various natural language processing tasks has led to superlative claims about their performance. For the evaluation of machine translation (MT), existing research shows that LLMs are able to achieve results comparable to fine-tuned multilingual pre-trained language models. In this paper, we explore what translation information, such as the source, reference, translation errors and annotation guidelines, is needed for LLMs to evaluate MT quality. In addition, we
investigate prompting techniques such as zero-shot, Chain of Thought (CoT) and few-shot prompting for eight language pairs covering high-, medium- and low-resource languages, leveraging varying LLM variants. Our findings indicate the importance of reference translations for
an LLM-based evaluation. While larger models do not necessarily fare better, they tend to benefit more from CoT prompting, than smaller models. We also observe that LLMs do not always provide a numerical score when generating evaluations, which poses a question on their reliability for the task. Our work presents a comprehensive analysis for resource-constrained and training-less LLM-based evaluation of machine translation. We release the accrued prompt templates, code and data publicly for reproducibility<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/surrey-nlp/LLM4MT_eval" title="">https://github.com/surrey-nlp/LLM4MT_eval</a></span></span></span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<div class="ltx_block ltx_align_bottom" id="p1.1">
<p class="ltx_p" id="p1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1.1">What do Large Language Models Need for 
<br class="ltx_break"/>Machine Translation Evaluation?</span></p>
<br class="ltx_break ltx_centering"/>
<p class="ltx_p ltx_align_center" id="p1.1.2" style="width:433.6pt;"><span class="ltx_text ltx_inline-block" id="p1.1.2.1" style="width:0.0pt;">
<span class="ltx_tabular ltx_align_top" id="p1.1.2.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="p1.1.2.1.1.1.1">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.1.1.1.1">Shenbin Qian<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.1">1</sup>, Archchana Sindhujan<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.2">2</sup><sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.3">*</sup>, Minnie Kabra<sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.4">3</sup><sup class="ltx_sup" id="p1.1.2.1.1.1.1.1.1.5">*</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.2.2">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="p1.1.2.1.1.2.2.1.1">Diptesh Kanojia<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.1">2</sup>, Constantin Orăsan<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.2">1</sup>, Tharindu Ranasinghe<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.3">4</sup> and Frédéric Blain<sup class="ltx_sup" id="p1.1.2.1.1.2.2.1.1.4">5</sup></span></span></span>
<span class="ltx_tr" id="p1.1.2.1.1.3.3">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.3.3.1"><sup class="ltx_sup" id="p1.1.2.1.1.3.3.1.1">1</sup>Centre for Translation Studies, University of Surrey, United Kingdom,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.4.4">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.4.4.1"><sup class="ltx_sup" id="p1.1.2.1.1.4.4.1.1">2</sup>Institute for People-Centred AI, University of Surrey, United Kingdom,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.5.5">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.5.5.1"><sup class="ltx_sup" id="p1.1.2.1.1.5.5.1.1">3</sup>Independent Researcher, India,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.6.6">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.6.6.1"><sup class="ltx_sup" id="p1.1.2.1.1.6.6.1.1">4</sup>Lancaster University, United Kingdom,
<sup class="ltx_sup" id="p1.1.2.1.1.6.6.1.2">5</sup>Tilburg University, The Netherlands</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.7.7">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.7.7.1">{s.qian, a.sindhujan, d.kanojia, c.orasan}@surrey.ac.uk, minniekabra@gmail.com,</span></span>
<span class="ltx_tr" id="p1.1.2.1.1.8.8">
<span class="ltx_td ltx_align_center" id="p1.1.2.1.1.8.8.1">t.ranasinghe@lancaster.ac.uk, f.l.g.blain@tilburguniversity.edu</span></span>
</span>
</span></span></p>
<br class="ltx_break ltx_centering"/>
</div>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">footnotetext: </span>Both authors contributed equally to this work.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Recent surge in the use of large language models (LLMs) for natural language processing (NLP) tasks like question answering <cite class="ltx_cite ltx_citemacro_citep">(Kocoń et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib15" title="">2023</a>; Tan et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib35" title="">2023</a>)</cite> has taken strides, and significantly improved their applications to other downstream tasks such as machine translation (MT), text summarization, information retrieval and <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">etc.</span>, due to advancements in natural language understanding capabilities, contextual awareness, and a versatile knowledge base <cite class="ltx_cite ltx_citemacro_citep">(Kocmi and Federmann, <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib14" title="">2023b</a>; Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib43" title="">2023</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib42" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">For automatic evaluation of MT quality, traditional approaches use metrics such as BLEU <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib20" title="">2002</a>)</cite>, BLEURT <cite class="ltx_cite ltx_citemacro_citep">(Sellam et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib27" title="">2020</a>)</cite> or BERTScore <cite class="ltx_cite ltx_citemacro_citep">(Zhang* et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib41" title="">2020</a>)</cite> to compare MT output with a reference translation. When references are not available, quality estimation (QE) methods such as fine-tuning multilingual pre-trained language models (PTLMs) on human evaluation data like Direct Assessment (DA) scores <cite class="ltx_cite ltx_citemacro_citep">(Graham et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib9" title="">2013</a>)</cite> are often used to predict estimated scores to approximate human evaluation <cite class="ltx_cite ltx_citemacro_citep">(Specia et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib33" title="">2018</a>)</cite>. Recent studies leverage prompting techniques and <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">instruct</span> LLMs to output a score for translation quality, claim to achieve promising results <cite class="ltx_cite ltx_citemacro_citep">(Kocmi and Federmann, <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib14" title="">2023b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib13" title="">a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, there exists no systematic exploration of what translation information LLMs need for quality evaluation, and whether different prompting techniques, such as Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(Wei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib39" title="">2024</a>)</cite> or few-shot prompting, can help boost the performance of LLMs. To that end, we conduct this investigation to systematically explore the ability of LLMs in quality evaluation in a training-less scenario. Our contributions can be summarized as:</p>
</div>
<div class="ltx_para" id="S1.p4">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We investigate what translation information, <span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">i.e.</span>, source, reference, translation errors and annotation guidelines LLMs need to evaluate translation for <math alttext="8" class="ltx_Math" display="inline" id="S1.I1.i1.p1.1.m1.1"><semantics id="S1.I1.i1.p1.1.m1.1a"><mn id="S1.I1.i1.p1.1.m1.1.1" xref="S1.I1.i1.p1.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S1.I1.i1.p1.1.m1.1b"><cn id="S1.I1.i1.p1.1.m1.1.1.cmml" type="integer" xref="S1.I1.i1.p1.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.p1.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.p1.1.m1.1d">8</annotation></semantics></math> language pairs covering high-, medium- and low-resource languages.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We explore different ways of prompting, <span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">i.e.</span>, zero-shot, CoT and few-shot prompting for LLMs to evaluate MT quality. Our code, data and prompts are released with the paper.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We compare our prompting methods with fine-tuning of encoder-based multilingual PTLMs and find LLM performance still lags behind.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">Our analyses of the results on various prompt templates indicate that
references are important for accurate translation evaluation with LLMs, and while larger models are not always better, they tend to benefit more from CoT prompting than smaller model variants.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The rest of the paper is structured as follows: Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S2" title="2 Related Work ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">2</span></a> discusses relevant work in quality evaluation, while Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S3" title="3 Data ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">3</span></a> introduces the dataset we utilize for this work. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4" title="4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">4</span></a> describes the prompting methods and the baselines with the experimental setup. Results and discussion are presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5" title="5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">5</span></a>. Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S6" title="6 Conclusion and Future Work ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">6</span></a> concludes our study and outlines future directions.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Traditional automatic MT quality evaluation metrics such as BLEU, BLEURT and BERTScore compare the MT output to one or several references, whilst metrics like Translation Error Rate (TER) <cite class="ltx_cite ltx_citemacro_citep">(Snover et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib29" title="">2006</a>)</cite> are based on the number of edits required for MT output to become reference, and neither takes semantic variations into account.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Training supervised machine learning systems on human-annotated data based on metrics such as DA or Multi-dimensional Quality Metrics (MQM) <cite class="ltx_cite ltx_citemacro_citep">(Lommel et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib18" title="">2014</a>)</cite> can help predict translation quality without any references <cite class="ltx_cite ltx_citemacro_cite">Deoghare et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib6" title="">2023b</a>)</cite>. <cite class="ltx_cite ltx_citemacro_citet">Ranasinghe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib22" title="">2020b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib23" title="">2021</a>)</cite> proposed the TransQuest framework to utilize the source text and MT output only and fine-tune XLM-RoBERTa-large <cite class="ltx_cite ltx_citemacro_citep">(Conneau et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib4" title="">2020</a>)</cite> to predict a DA score as an estimation of translation quality. COMET <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib25" title="">2020</a>; Stewart et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib34" title="">2020</a>; Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib26" title="">2022b</a>)</cite> was proposed initially to incorporate references along with the source and MT output to train multilingual PTLMs for quality evaluation, but later it also supported reference-less evaluation. <cite class="ltx_cite ltx_citemacro_citet">Wan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib37" title="">2022</a>)</cite> proposed a unified translation evaluation framework that could include source or reference or both as input for quality evaluation. Various approaches achieved promising results in the QE shared task of the Conference on Machine Translation (WMT) <cite class="ltx_cite ltx_citemacro_citep">(Specia et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib31" title="">2020</a>, <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib32" title="">2021</a>; Zerva et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib40" title="">2022</a>; Blain et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib3" title="">2023</a>)</cite>, however, most require supervision and training <cite class="ltx_cite ltx_citemacro_citep">(Deoghare et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib5" title="">2023a</a>; Kanojia et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib12" title="">2021</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">The advent of LLMs prompted its application to translation quality evaluation. <cite class="ltx_cite ltx_citemacro_citet">Kocmi and Federmann (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib14" title="">2023b</a>)</cite> proposed a zero-shot prompting technique, called GEMBA for DA score prediction using GPT-4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib19" title="">2024</a>)</cite>, claiming LLMs can achieve performance comparable to state-of-the-art models fine-tuned on multilingual data. Based on the GEMBA prompt, <cite class="ltx_cite ltx_citemacro_citet">Fernandes et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib7" title="">2023</a>)</cite> proposed to use LLMs for both DA score prediction and error categorization via fine-tuning to achieve more fine-grained evaluation. Previous research focused on whether LLMs can be better translation evaluators than state-of-the-art models. To the best of our knowledge, only <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib10" title="">2024</a>)</cite> investigated how LLMs leverage the source and reference for quality evaluation. However, they only perform zero-shot prompting for three language pairs. Our work comprehensively examines factors such as translation errors and annotation guidelines across eight language pairs, eight prompt templates, and three different prompting techniques.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Data</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.2">We utilized the DA score prediction data released with WMT22 QE shared task <cite class="ltx_cite ltx_citemacro_cite">Zerva et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib40" title="">2022</a>)</cite>. This dataset includes the source (mainly from news articles), MT output (from different MT engines) and (post-edited) human references for eight language pairs, <span class="ltx_text ltx_font_italic" id="S3.p1.2.1">i.e.,</span> English-German (<span class="ltx_text ltx_font_bold" id="S3.p1.2.2">EN-DE</span>), English-Marathi (<span class="ltx_text ltx_font_bold" id="S3.p1.2.3">EN-MR</span>), English-Chinese (<span class="ltx_text ltx_font_bold" id="S3.p1.2.4">EN-ZH</span>), Estonian-English (<span class="ltx_text ltx_font_bold" id="S3.p1.2.5">ET-EN</span>), Nepali-English (<span class="ltx_text ltx_font_bold" id="S3.p1.2.6">NE-EN</span>), Romanian-English (<span class="ltx_text ltx_font_bold" id="S3.p1.2.7">RO-EN</span>), Russian-English (<span class="ltx_text ltx_font_bold" id="S3.p1.2.8">RU-EN</span>) and Sinhala-English (<span class="ltx_text ltx_font_bold" id="S3.p1.2.9">SI-EN</span>). For each source-MT pair, the dataset contains a DA score ranging from <math alttext="0" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><mn id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><cn id="S3.p1.1.m1.1.1.cmml" type="integer" xref="S3.p1.1.m1.1.1">0</cn></annotation-xml></semantics></math> to <math alttext="100" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><mn id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml">100</mn><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><cn id="S3.p1.2.m2.1.1.cmml" type="integer" xref="S3.p1.2.m2.1.1">100</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">100</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">100</annotation></semantics></math>, rated by human annotators for quality assessment.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">To include annotated errors in the MT output into our prompts, we obtained word-level QE data from WMT22, where tokens of the MT output have sequence labels with either “OK” or “BAD” indicating translation quality at word level. This dataset also involves the above <math alttext="8" class="ltx_Math" display="inline" id="S3.p2.1.m1.1"><semantics id="S3.p2.1.m1.1a"><mn id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><cn id="S3.p2.1.m1.1.1.cmml" type="integer" xref="S3.p2.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S3.p2.1.m1.1d">8</annotation></semantics></math> language pairs, which contains the source, MT output and the tags for translation quality. For each MT output, we extracted the tokens that were tagged as “BAD” as error words.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Since source-MT segments from sentence-level QE data might differ from those of word-level, we compared each source-MT pair in the two datasets and used the overlapping as the main resource of our research. It includes source, MT output, reference translations and error words for the <math alttext="8" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mn id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">8</mn><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><cn id="S3.p3.1.m1.1.1.cmml" type="integer" xref="S3.p3.1.m1.1.1">8</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">8</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">8</annotation></semantics></math> language pairs, covering high-, medium- and low-resource languages. We present different prompt templates in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS2" title="4.2 Zero-shot Prompting ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">4.2</span></a> to selectively include source, reference and error words to test what translation information LLMs need for quality evaluation.</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.3">We split the data into training, validation, and test sets in proportions of <math alttext="80\%" class="ltx_Math" display="inline" id="S3.p4.1.m1.1"><semantics id="S3.p4.1.m1.1a"><mrow id="S3.p4.1.m1.1.1" xref="S3.p4.1.m1.1.1.cmml"><mn id="S3.p4.1.m1.1.1.2" xref="S3.p4.1.m1.1.1.2.cmml">80</mn><mo id="S3.p4.1.m1.1.1.1" xref="S3.p4.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.1.m1.1b"><apply id="S3.p4.1.m1.1.1.cmml" xref="S3.p4.1.m1.1.1"><csymbol cd="latexml" id="S3.p4.1.m1.1.1.1.cmml" xref="S3.p4.1.m1.1.1.1">percent</csymbol><cn id="S3.p4.1.m1.1.1.2.cmml" type="integer" xref="S3.p4.1.m1.1.1.2">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.1.m1.1c">80\%</annotation><annotation encoding="application/x-llamapun" id="S3.p4.1.m1.1d">80 %</annotation></semantics></math>, <math alttext="10\%" class="ltx_Math" display="inline" id="S3.p4.2.m2.1"><semantics id="S3.p4.2.m2.1a"><mrow id="S3.p4.2.m2.1.1" xref="S3.p4.2.m2.1.1.cmml"><mn id="S3.p4.2.m2.1.1.2" xref="S3.p4.2.m2.1.1.2.cmml">10</mn><mo id="S3.p4.2.m2.1.1.1" xref="S3.p4.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.2.m2.1b"><apply id="S3.p4.2.m2.1.1.cmml" xref="S3.p4.2.m2.1.1"><csymbol cd="latexml" id="S3.p4.2.m2.1.1.1.cmml" xref="S3.p4.2.m2.1.1.1">percent</csymbol><cn id="S3.p4.2.m2.1.1.2.cmml" type="integer" xref="S3.p4.2.m2.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.2.m2.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S3.p4.2.m2.1d">10 %</annotation></semantics></math>, and <math alttext="10\%" class="ltx_Math" display="inline" id="S3.p4.3.m3.1"><semantics id="S3.p4.3.m3.1a"><mrow id="S3.p4.3.m3.1.1" xref="S3.p4.3.m3.1.1.cmml"><mn id="S3.p4.3.m3.1.1.2" xref="S3.p4.3.m3.1.1.2.cmml">10</mn><mo id="S3.p4.3.m3.1.1.1" xref="S3.p4.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p4.3.m3.1b"><apply id="S3.p4.3.m3.1.1.cmml" xref="S3.p4.3.m3.1.1"><csymbol cd="latexml" id="S3.p4.3.m3.1.1.1.cmml" xref="S3.p4.3.m3.1.1.1">percent</csymbol><cn id="S3.p4.3.m3.1.1.2.cmml" type="integer" xref="S3.p4.3.m3.1.1.2">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p4.3.m3.1c">10\%</annotation><annotation encoding="application/x-llamapun" id="S3.p4.3.m3.1d">10 %</annotation></semantics></math> respectively. We inferenced with LLMs on the test set to obtain evaluation results. Training and validation sets were used to sample examples for few-shot learning (see Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS4" title="4.4 Few-shot Learning ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">4.4</span></a>). The size of the test set for each language pair can be seen in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S3.T1" title="Table 1 ‣ 3 Data ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.8" style="width:213.4pt;height:20.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-80.1pt,7.7pt) scale(0.57104252034067,0.57104252034067) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.8.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.8.8.9.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S3.T1.8.8.9.1.1">LP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.8.8.9.1.2">EN-DE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.8.8.9.1.3">EN-MR</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.8.8.9.1.4">EN-ZH</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.8.8.9.1.5">ET-EN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.8.8.9.1.6">NE-EN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.8.8.9.1.7">RO-EN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.8.8.9.1.8">RU-EN</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.8.8.9.1.9">SI-EN</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.8.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S3.T1.8.8.8.9">N</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.1.1.1.1"><math alttext="891" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mn id="S3.T1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.m1.1.1.cmml">891</mn><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><cn id="S3.T1.1.1.1.1.m1.1.1.cmml" type="integer" xref="S3.T1.1.1.1.1.m1.1.1">891</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">891</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">891</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.2.2.2"><math alttext="2598" class="ltx_Math" display="inline" id="S3.T1.2.2.2.2.m1.1"><semantics id="S3.T1.2.2.2.2.m1.1a"><mn id="S3.T1.2.2.2.2.m1.1.1" xref="S3.T1.2.2.2.2.m1.1.1.cmml">2598</mn><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.2.m1.1b"><cn id="S3.T1.2.2.2.2.m1.1.1.cmml" type="integer" xref="S3.T1.2.2.2.2.m1.1.1">2598</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.2.m1.1c">2598</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.2.m1.1d">2598</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.3.3.3.3"><math alttext="890" class="ltx_Math" display="inline" id="S3.T1.3.3.3.3.m1.1"><semantics id="S3.T1.3.3.3.3.m1.1a"><mn id="S3.T1.3.3.3.3.m1.1.1" xref="S3.T1.3.3.3.3.m1.1.1.cmml">890</mn><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.3.m1.1b"><cn id="S3.T1.3.3.3.3.m1.1.1.cmml" type="integer" xref="S3.T1.3.3.3.3.m1.1.1">890</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.3.m1.1c">890</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.3.m1.1d">890</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.4.4.4.4"><math alttext="897" class="ltx_Math" display="inline" id="S3.T1.4.4.4.4.m1.1"><semantics id="S3.T1.4.4.4.4.m1.1a"><mn id="S3.T1.4.4.4.4.m1.1.1" xref="S3.T1.4.4.4.4.m1.1.1.cmml">897</mn><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.4.m1.1b"><cn id="S3.T1.4.4.4.4.m1.1.1.cmml" type="integer" xref="S3.T1.4.4.4.4.m1.1.1">897</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.4.m1.1c">897</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.4.m1.1d">897</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.5.5.5.5"><math alttext="761" class="ltx_Math" display="inline" id="S3.T1.5.5.5.5.m1.1"><semantics id="S3.T1.5.5.5.5.m1.1a"><mn id="S3.T1.5.5.5.5.m1.1.1" xref="S3.T1.5.5.5.5.m1.1.1.cmml">761</mn><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.5.m1.1b"><cn id="S3.T1.5.5.5.5.m1.1.1.cmml" type="integer" xref="S3.T1.5.5.5.5.m1.1.1">761</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.5.m1.1c">761</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.5.m1.1d">761</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.6.6.6.6"><math alttext="867" class="ltx_Math" display="inline" id="S3.T1.6.6.6.6.m1.1"><semantics id="S3.T1.6.6.6.6.m1.1a"><mn id="S3.T1.6.6.6.6.m1.1.1" xref="S3.T1.6.6.6.6.m1.1.1.cmml">867</mn><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.6.m1.1b"><cn id="S3.T1.6.6.6.6.m1.1.1.cmml" type="integer" xref="S3.T1.6.6.6.6.m1.1.1">867</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.6.m1.1c">867</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.6.m1.1d">867</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.7.7.7"><math alttext="900" class="ltx_Math" display="inline" id="S3.T1.7.7.7.7.m1.1"><semantics id="S3.T1.7.7.7.7.m1.1a"><mn id="S3.T1.7.7.7.7.m1.1.1" xref="S3.T1.7.7.7.7.m1.1.1.cmml">900</mn><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.7.7.m1.1b"><cn id="S3.T1.7.7.7.7.m1.1.1.cmml" type="integer" xref="S3.T1.7.7.7.7.m1.1.1">900</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.7.7.m1.1c">900</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.7.7.m1.1d">900</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.8.8.8.8"><math alttext="343" class="ltx_Math" display="inline" id="S3.T1.8.8.8.8.m1.1"><semantics id="S3.T1.8.8.8.8.m1.1a"><mn id="S3.T1.8.8.8.8.m1.1.1" xref="S3.T1.8.8.8.8.m1.1.1.cmml">343</mn><annotation-xml encoding="MathML-Content" id="S3.T1.8.8.8.8.m1.1b"><cn id="S3.T1.8.8.8.8.m1.1.1.cmml" type="integer" xref="S3.T1.8.8.8.8.m1.1.1">343</cn></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.8.8.8.8.m1.1c">343</annotation><annotation encoding="application/x-llamapun" id="S3.T1.8.8.8.8.m1.1d">343</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>The number of instances (N) in the <span class="ltx_text ltx_font_bold" id="S3.T1.10.1">test set</span> for each language pair (LP)</figcaption>
</figure>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This section presents the baselines, and our zero-shot, CoT and few-shot prompting methods.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Baselines</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We utilize TransQuest and COMET, two widely used reference-less and reference-based<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>COMET also supports reference-less evaluation.</span></span></span> QE frameworks as baselines. For TransQuest, we employed the fine-tuned MonoTransQuest models proposed by <cite class="ltx_cite ltx_citemacro_citet">Ranasinghe et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib21" title="">2020a</a>)</cite> on each language pair except EN-MR. For EN-MR, we used the English to any model released with TransQuest. For COMET, we utilized a fine-tuned multilingual model “Unbabel/wmt22-comet-da” <cite class="ltx_cite ltx_citemacro_citep">(Rei et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib24" title="">2022a</a>)</cite> for all language pairs as it does not have models for each language pair.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Zero-shot Prompting</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">For LLMs to predict translation quality, our prompt includes 1) instructions to perform the task such as “Score the following translation”, and 2) translation information such as source or reference.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Since <cite class="ltx_cite ltx_citemacro_citet">Kocmi and Federmann (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib14" title="">2023b</a>)</cite> have shown that their prompt template can achieve state-of-the-art performance using GPT-4, we mainly followed their template to create our prompt instruction as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.F1" title="Figure 1 ‣ 4.2 Zero-shot Prompting ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">1</span></a>. We used it as our base template and changed slightly different translation information to test what is needed for LLMs to evaluate MT quality. We constructed prompt <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Template 1</span> containing “source + MT output” as translation information, <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.2">Template 2</span> “MT output + reference”, <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.3">Template 3</span> “source + MT output + reference” (exact GEMBA prompt), <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.4">Template 4</span> “source + MT output + error words”, <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.5">Template 5</span> “source + MT output + reference + error words”.</p>
</div>
<figure class="ltx_figure" id="S4.F1">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" id="S4.F1.1" style="width:341.4pt;">
<p class="ltx_p" id="S4.F1.1.1">Score the following translation from {source_lang} to {target_lang} with respect to the {source/human_reference/error_words} on a continuous scale from 0 to 100, where score of zero means "no meaning preserved" and score of one hundred means "perfect meaning and grammar".</p>
<p class="ltx_p" id="S4.F1.1.2"><span class="ltx_text ltx_font_bold" id="S4.F1.1.2.1">{translation_information}</span></p>
<p class="ltx_p" id="S4.F1.1.3">Score:</p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Base Prompt Template</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">We augmented the base prompt with summarized guidelines used during human evaluation, as <span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">Template 6</span> to test if this could help LLMs evaluate MT quality. These guidelines instruct evaluators to give a DA score by considering multiple factors including accuracy, contextual understanding, grammar, syntax and overall readability.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>CoT Prompting</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Apart from the translation information and guidelines added in the prompt, we also tested whether CoT prompting could improve LLMs’ performance by utilizing reasoning-based steps for quality evaluation. We devised <span class="ltx_text ltx_font_bold" id="S4.SS3.p1.1.1">Template 7</span> which includes two-step prompts to score MT quality, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.F2" title="Figure 2 ‣ 4.3 CoT Prompting ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">2</span></a>. In the first prompt, we give translation information (including source, MT output and reference) to the LLM and ask it to analyze step by step where the machine translation is different from the reference. In the second prompt, we instruct the LLM to score the machine translation based on its previous output, <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.2">i.e.,</span> the analysis of machine translation based on reference. Instruction to output a score in JSON format is given to ensure it produces the score first, like other templates.</p>
</div>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" id="S4.F2.1" style="width:341.4pt;">
<p class="ltx_p" id="S4.F2.1.1">Prompt 1:</p>
<p class="ltx_p" id="S4.F2.1.2">You are going to evaluate the quality for {language_pair} translation. You need to think step by step. First read the following source, machine translation and reference translation. Analyze where the machine translation is different from the reference translation.</p>
<p class="ltx_p" id="S4.F2.1.3">Source: {source_sentence}</p>
<p class="ltx_p" id="S4.F2.1.4">Machine translation: {target_sentence}</p>
<p class="ltx_p" id="S4.F2.1.5">Reference translation: {reference_translation}</p>
<p class="ltx_p" id="S4.F2.1.6">Prompt 2:</p>
<p class="ltx_p" id="S4.F2.1.7">A large language model did an evaluation of machine translation quality for the {source_language} sentence, which is given as below: {output_from_Prompt1}</p>
<p class="ltx_p" id="S4.F2.1.8">Based on your analysis, score the machine translation quality on a continuous scale from 0 to 100, where score of zero means "no meaning preserved" and score of one hundred means "perfect meaning and grammar". Provide the score strictly in JSON format.</p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Prompt Template 7</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Few-shot Learning</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.9">In addition to zero-shot and CoT prompting, we also added <math alttext="5" class="ltx_Math" display="inline" id="S4.SS4.p1.1.m1.1"><semantics id="S4.SS4.p1.1.m1.1a"><mn id="S4.SS4.p1.1.m1.1.1" xref="S4.SS4.p1.1.m1.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.1.m1.1b"><cn id="S4.SS4.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS4.p1.1.m1.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.1.m1.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.1.m1.1d">5</annotation></semantics></math> examples based on Template 3, to show how human annotators score machine translations from <math alttext="0-100" class="ltx_Math" display="inline" id="S4.SS4.p1.2.m2.1"><semantics id="S4.SS4.p1.2.m2.1a"><mrow id="S4.SS4.p1.2.m2.1.1" xref="S4.SS4.p1.2.m2.1.1.cmml"><mn id="S4.SS4.p1.2.m2.1.1.2" xref="S4.SS4.p1.2.m2.1.1.2.cmml">0</mn><mo id="S4.SS4.p1.2.m2.1.1.1" xref="S4.SS4.p1.2.m2.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.2.m2.1.1.3" xref="S4.SS4.p1.2.m2.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.2.m2.1b"><apply id="S4.SS4.p1.2.m2.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1"><minus id="S4.SS4.p1.2.m2.1.1.1.cmml" xref="S4.SS4.p1.2.m2.1.1.1"></minus><cn id="S4.SS4.p1.2.m2.1.1.2.cmml" type="integer" xref="S4.SS4.p1.2.m2.1.1.2">0</cn><cn id="S4.SS4.p1.2.m2.1.1.3.cmml" type="integer" xref="S4.SS4.p1.2.m2.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.2.m2.1c">0-100</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.2.m2.1d">0 - 100</annotation></semantics></math>. We split the training and validation sets into 5 buckets for each language pair according to the score ranges of <math alttext="0-20" class="ltx_Math" display="inline" id="S4.SS4.p1.3.m3.1"><semantics id="S4.SS4.p1.3.m3.1a"><mrow id="S4.SS4.p1.3.m3.1.1" xref="S4.SS4.p1.3.m3.1.1.cmml"><mn id="S4.SS4.p1.3.m3.1.1.2" xref="S4.SS4.p1.3.m3.1.1.2.cmml">0</mn><mo id="S4.SS4.p1.3.m3.1.1.1" xref="S4.SS4.p1.3.m3.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.3.m3.1.1.3" xref="S4.SS4.p1.3.m3.1.1.3.cmml">20</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.3.m3.1b"><apply id="S4.SS4.p1.3.m3.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1"><minus id="S4.SS4.p1.3.m3.1.1.1.cmml" xref="S4.SS4.p1.3.m3.1.1.1"></minus><cn id="S4.SS4.p1.3.m3.1.1.2.cmml" type="integer" xref="S4.SS4.p1.3.m3.1.1.2">0</cn><cn id="S4.SS4.p1.3.m3.1.1.3.cmml" type="integer" xref="S4.SS4.p1.3.m3.1.1.3">20</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.3.m3.1c">0-20</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.3.m3.1d">0 - 20</annotation></semantics></math>, <math alttext="21-40" class="ltx_Math" display="inline" id="S4.SS4.p1.4.m4.1"><semantics id="S4.SS4.p1.4.m4.1a"><mrow id="S4.SS4.p1.4.m4.1.1" xref="S4.SS4.p1.4.m4.1.1.cmml"><mn id="S4.SS4.p1.4.m4.1.1.2" xref="S4.SS4.p1.4.m4.1.1.2.cmml">21</mn><mo id="S4.SS4.p1.4.m4.1.1.1" xref="S4.SS4.p1.4.m4.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.4.m4.1.1.3" xref="S4.SS4.p1.4.m4.1.1.3.cmml">40</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.4.m4.1b"><apply id="S4.SS4.p1.4.m4.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1"><minus id="S4.SS4.p1.4.m4.1.1.1.cmml" xref="S4.SS4.p1.4.m4.1.1.1"></minus><cn id="S4.SS4.p1.4.m4.1.1.2.cmml" type="integer" xref="S4.SS4.p1.4.m4.1.1.2">21</cn><cn id="S4.SS4.p1.4.m4.1.1.3.cmml" type="integer" xref="S4.SS4.p1.4.m4.1.1.3">40</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.4.m4.1c">21-40</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.4.m4.1d">21 - 40</annotation></semantics></math>, <math alttext="41-60" class="ltx_Math" display="inline" id="S4.SS4.p1.5.m5.1"><semantics id="S4.SS4.p1.5.m5.1a"><mrow id="S4.SS4.p1.5.m5.1.1" xref="S4.SS4.p1.5.m5.1.1.cmml"><mn id="S4.SS4.p1.5.m5.1.1.2" xref="S4.SS4.p1.5.m5.1.1.2.cmml">41</mn><mo id="S4.SS4.p1.5.m5.1.1.1" xref="S4.SS4.p1.5.m5.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.5.m5.1.1.3" xref="S4.SS4.p1.5.m5.1.1.3.cmml">60</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.5.m5.1b"><apply id="S4.SS4.p1.5.m5.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1"><minus id="S4.SS4.p1.5.m5.1.1.1.cmml" xref="S4.SS4.p1.5.m5.1.1.1"></minus><cn id="S4.SS4.p1.5.m5.1.1.2.cmml" type="integer" xref="S4.SS4.p1.5.m5.1.1.2">41</cn><cn id="S4.SS4.p1.5.m5.1.1.3.cmml" type="integer" xref="S4.SS4.p1.5.m5.1.1.3">60</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.5.m5.1c">41-60</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.5.m5.1d">41 - 60</annotation></semantics></math>, <math alttext="61-80" class="ltx_Math" display="inline" id="S4.SS4.p1.6.m6.1"><semantics id="S4.SS4.p1.6.m6.1a"><mrow id="S4.SS4.p1.6.m6.1.1" xref="S4.SS4.p1.6.m6.1.1.cmml"><mn id="S4.SS4.p1.6.m6.1.1.2" xref="S4.SS4.p1.6.m6.1.1.2.cmml">61</mn><mo id="S4.SS4.p1.6.m6.1.1.1" xref="S4.SS4.p1.6.m6.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.6.m6.1.1.3" xref="S4.SS4.p1.6.m6.1.1.3.cmml">80</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.6.m6.1b"><apply id="S4.SS4.p1.6.m6.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1"><minus id="S4.SS4.p1.6.m6.1.1.1.cmml" xref="S4.SS4.p1.6.m6.1.1.1"></minus><cn id="S4.SS4.p1.6.m6.1.1.2.cmml" type="integer" xref="S4.SS4.p1.6.m6.1.1.2">61</cn><cn id="S4.SS4.p1.6.m6.1.1.3.cmml" type="integer" xref="S4.SS4.p1.6.m6.1.1.3">80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.6.m6.1c">61-80</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.6.m6.1d">61 - 80</annotation></semantics></math>, <math alttext="81-100" class="ltx_Math" display="inline" id="S4.SS4.p1.7.m7.1"><semantics id="S4.SS4.p1.7.m7.1a"><mrow id="S4.SS4.p1.7.m7.1.1" xref="S4.SS4.p1.7.m7.1.1.cmml"><mn id="S4.SS4.p1.7.m7.1.1.2" xref="S4.SS4.p1.7.m7.1.1.2.cmml">81</mn><mo id="S4.SS4.p1.7.m7.1.1.1" xref="S4.SS4.p1.7.m7.1.1.1.cmml">−</mo><mn id="S4.SS4.p1.7.m7.1.1.3" xref="S4.SS4.p1.7.m7.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.7.m7.1b"><apply id="S4.SS4.p1.7.m7.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1"><minus id="S4.SS4.p1.7.m7.1.1.1.cmml" xref="S4.SS4.p1.7.m7.1.1.1"></minus><cn id="S4.SS4.p1.7.m7.1.1.2.cmml" type="integer" xref="S4.SS4.p1.7.m7.1.1.2">81</cn><cn id="S4.SS4.p1.7.m7.1.1.3.cmml" type="integer" xref="S4.SS4.p1.7.m7.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.7.m7.1c">81-100</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.7.m7.1d">81 - 100</annotation></semantics></math>. We randomly sampled <math alttext="1" class="ltx_Math" display="inline" id="S4.SS4.p1.8.m8.1"><semantics id="S4.SS4.p1.8.m8.1a"><mn id="S4.SS4.p1.8.m8.1.1" xref="S4.SS4.p1.8.m8.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.8.m8.1b"><cn id="S4.SS4.p1.8.m8.1.1.cmml" type="integer" xref="S4.SS4.p1.8.m8.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.8.m8.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.8.m8.1d">1</annotation></semantics></math> example from each range. The selected <math alttext="5" class="ltx_Math" display="inline" id="S4.SS4.p1.9.m9.1"><semantics id="S4.SS4.p1.9.m9.1a"><mn id="S4.SS4.p1.9.m9.1.1" xref="S4.SS4.p1.9.m9.1.1.cmml">5</mn><annotation-xml encoding="MathML-Content" id="S4.SS4.p1.9.m9.1b"><cn id="S4.SS4.p1.9.m9.1.1.cmml" type="integer" xref="S4.SS4.p1.9.m9.1.1">5</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p1.9.m9.1c">5</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p1.9.m9.1d">5</annotation></semantics></math> examples for each language pair were given before the instruction for scoring as a prefix (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.F3" title="Figure 3 ‣ 4.4 Few-shot Learning ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">3</span></a>) of the base prompt in Figure <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.F1" title="Figure 1 ‣ 4.2 Zero-shot Prompting ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">1</span></a>. We call this prompt <span class="ltx_text ltx_font_bold" id="S4.SS4.p1.9.1">Template 8</span>.</p>
</div>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" id="S4.F3.1" style="width:341.4pt;">
<p class="ltx_p" id="S4.F3.1.1">You are going to evaluate the quality of machine translation given the source, machine translation and reference translation. The followings are examples of scoring translation quality.</p>
<p class="ltx_p" id="S4.F3.1.2">{5_examples}</p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Prefix for the base prompt to create Template 8</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Model Selection</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">We chose 6 models from a variety of open-source LLMs according to their size, popularity and type such as mixture of expert (MoE) <cite class="ltx_cite ltx_citemacro_citep">(Shazeer et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib28" title="">2017</a>)</cite> and dense models, and based on our compute capability. For 7-billion-parameter models, we selected Llama-2-7B from Meta <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib36" title="">2023</a>)</cite>, Gemma-7B from Google <cite class="ltx_cite ltx_citemacro_citep">(Gemma Team et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib8" title="">2024</a>)</cite> and OpenChat3.5 which was trained with mixed-quality data using Conditional Reinforcement Learning from Human Feedback <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib38" title="">2024</a>)</cite>. For 13-billion-parameter models, we opted for Llama-2-13B and Qwen1.5-14B which was specifically tested on a diverse set of 12 languages and showed impressive multilingual capabilities <cite class="ltx_cite ltx_citemacro_citep">(Bai et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib2" title="">2023</a>)</cite>. We also included the Mixtral-8x7B model <cite class="ltx_cite ltx_citemacro_citep">(Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib11" title="">2024</a>)</cite> as our MoE model, but due to the limit of our compute capability, we used the activation-aware weight quantized (AWQ) version <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib17" title="">2024</a>)</cite>. For all <math alttext="6" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mn id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">6</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><cn id="S4.SS5.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS5.p1.1.m1.1.1">6</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">6</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">6</annotation></semantics></math> selected models, we used the instruction-tuned version, <span class="ltx_text ltx_font_italic" id="S4.SS5.p1.1.1">i.e.,</span> the chat model, for zero-shot, CoT and few-shot inference. Additionally, we experimented with TowerLLM <cite class="ltx_cite ltx_citemacro_citep">(Alves et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib1" title="">2024</a>)</cite> for EN-ZH via HuggingFace<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/Unbabel/TowerInstruct-7B-v0.1" title="">https://huggingface.co/Unbabel/TowerInstruct-7B-v0.1</a></span></span></span>, but results are not discussed in the paper because the model output is mostly identical to the input for most instances (see Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#A1" title="Appendix A Pearson’s 𝑟 and Kendall’s 𝜏 Correlation Scores ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Experimental Setup</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.3">All our experiments were run using 1 <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS6.p1.1.m1.1"><semantics id="S4.SS6.p1.1.m1.1a"><mo id="S4.SS6.p1.1.m1.1.1" xref="S4.SS6.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.1.m1.1b"><times id="S4.SS6.p1.1.m1.1.1.cmml" xref="S4.SS6.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p1.1.m1.1d">×</annotation></semantics></math> NVIDIA A100 40G, 1 <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS6.p1.2.m2.1"><semantics id="S4.SS6.p1.2.m2.1a"><mo id="S4.SS6.p1.2.m2.1.1" xref="S4.SS6.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.2.m2.1b"><times id="S4.SS6.p1.2.m2.1.1.cmml" xref="S4.SS6.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p1.2.m2.1d">×</annotation></semantics></math> A40, and 2 <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS6.p1.3.m3.1"><semantics id="S4.SS6.p1.3.m3.1a"><mo id="S4.SS6.p1.3.m3.1.1" xref="S4.SS6.p1.3.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS6.p1.3.m3.1b"><times id="S4.SS6.p1.3.m3.1.1.cmml" xref="S4.SS6.p1.3.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.p1.3.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.p1.3.m3.1d">×</annotation></semantics></math> RTX A5000 GPUs, for different LLM variants. We used vLLM <cite class="ltx_cite ltx_citemacro_citep">(Kwon et al., <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib16" title="">2023</a>)</cite> to save inference time. Detailed settings for hyperparameters, formatting and evaluation metrics are provided below.</p>
</div>
<section class="ltx_paragraph" id="S4.SS6.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Hyperparameters</h4>
<div class="ltx_para" id="S4.SS6.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS6.SSS0.Px1.p1.3">We chose the default hyperparameter settings in vLLM for all our experiments, <span class="ltx_text ltx_font_italic" id="S4.SS6.SSS0.Px1.p1.3.1">i.e.,</span> <math alttext="0.8" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS6.SSS0.Px1.p1.1.m1.1a"><mn id="S4.SS6.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS6.SSS0.Px1.p1.1.m1.1.1.cmml">0.8</mn><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS0.Px1.p1.1.m1.1b"><cn id="S4.SS6.SSS0.Px1.p1.1.m1.1.1.cmml" type="float" xref="S4.SS6.SSS0.Px1.p1.1.m1.1.1">0.8</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS0.Px1.p1.1.m1.1c">0.8</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.SSS0.Px1.p1.1.m1.1d">0.8</annotation></semantics></math> as temperature<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>We also experimented with <math alttext="0" class="ltx_Math" display="inline" id="footnote4.m1.1"><semantics id="footnote4.m1.1b"><mn id="footnote4.m1.1.1" xref="footnote4.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="footnote4.m1.1c"><cn id="footnote4.m1.1.1.cmml" type="integer" xref="footnote4.m1.1.1">0</cn></annotation-xml></semantics></math> temperature, but we did not observe huge differences in terms of score distribution.</span></span></span>, <math alttext="0.95" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px1.p1.2.m2.1"><semantics id="S4.SS6.SSS0.Px1.p1.2.m2.1a"><mn id="S4.SS6.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS6.SSS0.Px1.p1.2.m2.1.1.cmml">0.95</mn><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS0.Px1.p1.2.m2.1b"><cn id="S4.SS6.SSS0.Px1.p1.2.m2.1.1.cmml" type="float" xref="S4.SS6.SSS0.Px1.p1.2.m2.1.1">0.95</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS0.Px1.p1.2.m2.1c">0.95</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.SSS0.Px1.p1.2.m2.1d">0.95</annotation></semantics></math> for top<math alttext="\_p" class="ltx_Math" display="inline" id="S4.SS6.SSS0.Px1.p1.3.m3.1"><semantics id="S4.SS6.SSS0.Px1.p1.3.m3.1a"><mrow id="S4.SS6.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS6.SSS0.Px1.p1.3.m3.1.1.cmml"><mi id="S4.SS6.SSS0.Px1.p1.3.m3.1.1.2" mathvariant="normal" xref="S4.SS6.SSS0.Px1.p1.3.m3.1.1.2.cmml">_</mi><mo id="S4.SS6.SSS0.Px1.p1.3.m3.1.1.1" xref="S4.SS6.SSS0.Px1.p1.3.m3.1.1.1.cmml">⁢</mo><mi id="S4.SS6.SSS0.Px1.p1.3.m3.1.1.3" xref="S4.SS6.SSS0.Px1.p1.3.m3.1.1.3.cmml">p</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS6.SSS0.Px1.p1.3.m3.1b"><apply id="S4.SS6.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S4.SS6.SSS0.Px1.p1.3.m3.1.1"><times id="S4.SS6.SSS0.Px1.p1.3.m3.1.1.1.cmml" xref="S4.SS6.SSS0.Px1.p1.3.m3.1.1.1"></times><ci id="S4.SS6.SSS0.Px1.p1.3.m3.1.1.2.cmml" xref="S4.SS6.SSS0.Px1.p1.3.m3.1.1.2">_</ci><ci id="S4.SS6.SSS0.Px1.p1.3.m3.1.1.3.cmml" xref="S4.SS6.SSS0.Px1.p1.3.m3.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS6.SSS0.Px1.p1.3.m3.1c">\_p</annotation><annotation encoding="application/x-llamapun" id="S4.SS6.SSS0.Px1.p1.3.m3.1d">_ italic_p</annotation></semantics></math>. The input sequence length was chosen as 1024 for zero-shot and CoT inference and 3000 for few-shot inference.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS6.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Formatting</h4>
<div class="ltx_para" id="S4.SS6.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS6.SSS0.Px2.p1.1">As chat models were fine-tuned on certain formats to interact with humans, it is suggested to use the specific format that was used to train the model while inferencing. As vLLM does not support formatting natively, we formatted all our prompt templates before they were fed into the models based on the format of each model in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S4.SS5" title="4.5 Model Selection ‣ 4 Methodology ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">4.5</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS6.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Evaluation</h4>
<div class="ltx_para" id="S4.SS6.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS6.SSS0.Px3.p1.1">Since LLMs usually output a score and some explanations about their evaluation, we used regular expression to extract the score from the LLM output. Spearman correlation<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Pearson’s <math alttext="r" class="ltx_Math" display="inline" id="footnote5.m1.1"><semantics id="footnote5.m1.1b"><mi id="footnote5.m1.1.1" xref="footnote5.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="footnote5.m1.1c"><ci id="footnote5.m1.1.1.cmml" xref="footnote5.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m1.1d">r</annotation><annotation encoding="application/x-llamapun" id="footnote5.m1.1e">italic_r</annotation></semantics></math> and Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="footnote5.m2.1"><semantics id="footnote5.m2.1b"><mi id="footnote5.m2.1.1" xref="footnote5.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="footnote5.m2.1c"><ci id="footnote5.m2.1.1.cmml" xref="footnote5.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote5.m2.1d">\tau</annotation><annotation encoding="application/x-llamapun" id="footnote5.m2.1e">italic_τ</annotation></semantics></math> are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#A1" title="Appendix A Pearson’s 𝑟 and Kendall’s 𝜏 Correlation Scores ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">A</span></a>.</span></span></span> <cite class="ltx_cite ltx_citemacro_citep">(Spearman, <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib30" title="">1904</a>)</cite> was used to evaluate how the predicted scores are correlated with the (mean of) human annotated scores. However, not all LLMs would output a score for all the instances. Sometimes, LLMs failed to score the input translation. In such cases, we dropped these instances (denoted as D in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T3" title="Table 3 ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">3</span></a>) during the process of correlation calculation, but they were <span class="ltx_text ltx_font_italic" id="S4.SS6.SSS0.Px3.p1.1.1">noted as a metric for robustness</span>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results and Discussion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section displays our baseline and prompting results using existing QE frameworks and LLMs.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Baselines</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T2" title="Table 2 ‣ 5.1 Baselines ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">2</span></a> shows our baseline results using TransQuest and COMET. Since TransQuest models were fine-tuned on data from each language pair with the exception of EN-MR, the Spearman correlation scores of these reference-less models, are higher than those of reference-based COMET models. Except EN-DE and EN-MR, the correlation scores for most language pairs are relatively high.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.3" style="width:170.7pt;height:174.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(6.0pt,-6.1pt) scale(1.07519013184538,1.07519013184538) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.1.1.1.1">LP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.1.1.1.2">TransQuest</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T2.3.1.1.1.3">COMET</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.3.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.2.1.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.2.1.2">0.3811</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.3.1.2.1.3">0.3579</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.3.2">
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.3.2.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.3.2.2">0.2489</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.3.2.3">0.5135</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.4.3">
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.4.3.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.4.3.2">0.6360</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.4.3.3">0.5410</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.5.4">
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.5.4.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.5.4.2">0.8148</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.5.4.3">0.7018</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.6.5">
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.6.5.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.6.5.2">0.8034</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.6.5.3">0.6393</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.7.6">
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.7.6.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.7.6.2">0.8739</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.7.6.3">0.7699</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.8.7">
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.8.7.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.8.7.2">0.8252</td>
<td class="ltx_td ltx_align_center" id="S5.T2.3.1.8.7.3">0.6482</td>
</tr>
<tr class="ltx_tr" id="S5.T2.3.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.9.8.1">SI-EN</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.9.8.2">0.7233</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.3.1.9.8.3">0.5874</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Spearman <math alttext="\rho" class="ltx_Math" display="inline" id="S5.T2.2.m1.1"><semantics id="S5.T2.2.m1.1b"><mi id="S5.T2.2.m1.1.1" xref="S5.T2.2.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T2.2.m1.1c"><ci id="S5.T2.2.m1.1.1.cmml" xref="S5.T2.2.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.m1.1d">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.m1.1e">italic_ρ</annotation></semantics></math> achieved by models using TransQuest and COMET on each language pair (LP).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Zero-shot Inference</h3>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.6" style="width:426.8pt;height:962.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-10.0pt,22.6pt) scale(0.955144907984883,0.955144907984883) ;">
<table class="ltx_tabular ltx_align_middle" id="S5.T3.6.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.6.6.7.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S5.T3.6.6.7.1.1" rowspan="2"><span class="ltx_text" id="S5.T3.6.6.7.1.1.1">LP</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T3.6.6.7.1.2">T1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T3.6.6.7.1.3">T2</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T3.6.6.7.1.4">T3</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T3.6.6.7.1.5">T4</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T3.6.6.7.1.6">T5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T3.6.6.7.1.7">T6</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.6">
<td class="ltx_td ltx_align_center" id="S5.T3.1.1.1.1"><math alttext="\rho" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.m1.1a"><mi id="S5.T3.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.m1.1d">italic_ρ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.6.7">D</td>
<td class="ltx_td ltx_align_center" id="S5.T3.2.2.2.2"><math alttext="\rho" class="ltx_Math" display="inline" id="S5.T3.2.2.2.2.m1.1"><semantics id="S5.T3.2.2.2.2.m1.1a"><mi id="S5.T3.2.2.2.2.m1.1.1" xref="S5.T3.2.2.2.2.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.m1.1b"><ci id="S5.T3.2.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.2.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.2.m1.1d">italic_ρ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.6.8">D</td>
<td class="ltx_td ltx_align_center" id="S5.T3.3.3.3.3"><math alttext="\rho" class="ltx_Math" display="inline" id="S5.T3.3.3.3.3.m1.1"><semantics id="S5.T3.3.3.3.3.m1.1a"><mi id="S5.T3.3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.3.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.m1.1b"><ci id="S5.T3.3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.3.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.3.m1.1d">italic_ρ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.6.9">D</td>
<td class="ltx_td ltx_align_center" id="S5.T3.4.4.4.4"><math alttext="\rho" class="ltx_Math" display="inline" id="S5.T3.4.4.4.4.m1.1"><semantics id="S5.T3.4.4.4.4.m1.1a"><mi id="S5.T3.4.4.4.4.m1.1.1" xref="S5.T3.4.4.4.4.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.4.m1.1b"><ci id="S5.T3.4.4.4.4.m1.1.1.cmml" xref="S5.T3.4.4.4.4.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.4.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.4.4.m1.1d">italic_ρ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.6.10"><span class="ltx_text ltx_font_bold" id="S5.T3.6.6.6.10.1">D</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.5.5.5.5"><math alttext="\rho" class="ltx_Math" display="inline" id="S5.T3.5.5.5.5.m1.1"><semantics id="S5.T3.5.5.5.5.m1.1a"><mi id="S5.T3.5.5.5.5.m1.1.1" xref="S5.T3.5.5.5.5.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.5.m1.1b"><ci id="S5.T3.5.5.5.5.m1.1.1.cmml" xref="S5.T3.5.5.5.5.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.5.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.5.5.5.m1.1d">italic_ρ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.6.11"><span class="ltx_text ltx_font_bold" id="S5.T3.6.6.6.11.1">D</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.6.6"><math alttext="\rho" class="ltx_Math" display="inline" id="S5.T3.6.6.6.6.m1.1"><semantics id="S5.T3.6.6.6.6.m1.1a"><mi id="S5.T3.6.6.6.6.m1.1.1" xref="S5.T3.6.6.6.6.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.6.m1.1b"><ci id="S5.T3.6.6.6.6.m1.1.1.cmml" xref="S5.T3.6.6.6.6.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.6.m1.1c">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.6.6.6.m1.1d">italic_ρ</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.6.12">D</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.8.2">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="13" id="S5.T3.6.6.8.2.1">OpenChat3.5</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.9.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.6.6.9.3.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.2">0.2258</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.4">0.2209</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.5">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.6">0.2849</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.7">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.8">0.2599</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.9">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.10">0.2812</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.11">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.12"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T3.6.6.9.3.12.1">0.2960</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.9.3.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.10.4">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.10.4.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.2">0.2295</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.3">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.4">0.3110</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.5">9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.10.4.6.1">0.3546</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.8">0.3347</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.10">0.3565</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.12">0.3446</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.10.4.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.11.5">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.11.5.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.2">0.2722</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.4">0.2603</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.5">4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.11.5.6.1">0.3995</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.8">0.3002</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.10">0.3333</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.12">0.3635</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.11.5.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.12.6">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.12.6.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.2">0.5402</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.4">0.5798</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.5">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.6"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T3.6.6.12.6.6.1">0.6980</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.8">0.5879</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.10">0.6700</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.12">0.6925</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.12.6.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.13.7">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.13.7.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.2">0.3784</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.3">9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.4">0.4855</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.5">25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.6">0.5937</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.8">0.5008</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.10">0.5832</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.12"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T3.6.6.13.7.12.1">0.6073</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.13.7.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.14.8">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.14.8.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.2">0.4712</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.3">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.4">0.5669</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.5">25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.6">0.7294</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.8">0.6900</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.10">0.7096</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.12"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T3.6.6.14.8.12.1">0.7385</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.14.8.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.15.9">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.15.9.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.2">0.5714</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.4">0.5320</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.5">13</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.6"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T3.6.6.15.9.6.1">0.6066</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.8">0.5494</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.10">0.5322</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.12">0.5938</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.15.9.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.16.10">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.16.10.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.2">0.4120</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.3">4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.4">0.4201</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.5">7</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.6"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T3.6.6.16.10.6.1">0.6034</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.8">0.4364</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.10">0.5990</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.12">0.5963</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.16.10.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.17.11">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="13" id="S5.T3.6.6.17.11.1">Llama-2-7B</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.18.12">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.6.6.18.12.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.2">0.0663</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.3">5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.4">0.0397</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.5">18</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.6">0.0876</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.7">73</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.8">-0.0166</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.9">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.10">0.0887</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.11">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.18.12.12.1">0.0957</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.18.12.13">27</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.19.13">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.19.13.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.2">0.0417</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.3">26</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.4">0.0024</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.5">85</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.19.13.6.1">0.1255</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.7">377</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.8">0.0154</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.9">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.10">0.0861</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.11">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.12">0.0943</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.19.13.13">140</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.20.14">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.20.14.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.20.14.2.1">0.0956</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.3">4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.4">0.0553</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.5">15</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.6">0.0946</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.7">86</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.8">0.0273</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.9">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.10">0.0607</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.11">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.12">0.0791</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.20.14.13">47</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.21.15">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.21.15.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.2">0.0439</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.3">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.4">0.1643</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.5">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.21.15.6.1">0.3715</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.7">54</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.8">-0.0431</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.9">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.10">0.2527</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.12">0.3319</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.21.15.13">31</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.22.16">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.22.16.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.2">0.1825</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.3">47</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.4">0.1018</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.5">7</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.6">0.2207</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.7">85</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.8">0.0461</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.9">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.10">0.2026</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.11">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.22.16.12.1">0.2629</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.22.16.13">26</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.23.17">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.23.17.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.2">0.3068</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.4">0.1322</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.5">4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.23.17.6.1">0.4514</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.7">50</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.8">-0.0059</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.9">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.10">0.2444</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.12">0.3619</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.23.17.13">20</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.24.18">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.24.18.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.2">0.1718</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.3">13</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.4">0.1389</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.5">44</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.24.18.6.1">0.4253</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.7">64</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.8">-0.0081</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.9">15</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.10">0.2170</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.11">12</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.12">0.2404</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.24.18.13">24</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.25.19">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.25.19.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.2">0.0801</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.3">7</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.4">-0.0238</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.5">11</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.6">0.2212</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.7">36</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.8">0.0639</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.9">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.10">0.2288</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.11">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.25.19.12.1">0.2530</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.25.19.13">18</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.26.20">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="13" id="S5.T3.6.6.26.20.1">Gemma-7B</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.27.21">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.6.6.27.21.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.2">0.1516</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.4">0.1241</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.5">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.6">0.1624</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.7">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.8">0.1074</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.9">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.27.21.10.1">0.1856</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.11">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.12">0.1820</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.27.21.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.28.22">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.28.22.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.28.22.2.1">0.3070</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.4">0.2332</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.5">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.6">0.1479</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.8">0.1529</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.10">0.2177</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.12">0.1815</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.28.22.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.29.23">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.29.23.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.2">0.2046</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.4">0.1362</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.5">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.6">0.1805</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.29.23.8.1">0.2734</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.10">0.2444</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.12">0.2342</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.29.23.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.30.24">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.30.24.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.2">0.3490</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.4">0.4074</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.5">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.6">0.3772</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.8">0.4125</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.30.24.10.1">0.5552</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.12">0.5169</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.30.24.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.31.25">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.31.25.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.2">0.3329</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.4">0.2732</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.5">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.6">0.2921</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.8">0.3439</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.31.25.10.1">0.4098</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.31.25.12.1">0.4098</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.31.25.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.32.26">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.32.26.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.32.26.2.1">0.6238</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.4">0.4393</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.5">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.6">0.4429</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.8">0.5858</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.10">0.5816</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.12">0.5911</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.32.26.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.33.27">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.33.27.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.2">0.3265</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.3">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.4">0.3697</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.5">13</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.6">0.4399</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.8">0.3709</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.10">0.4450</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.33.27.12.1">0.5012</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.33.27.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.34.28">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.34.28.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.2">0.2740</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.4">0.2610</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.5">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.6">0.3519</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.8">0.2816</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.34.28.10.1">0.3980</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.12">0.3741</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.34.28.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.35.29">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="13" id="S5.T3.6.6.35.29.1">Llama-2-13B</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.36.30">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.6.6.36.30.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.2">-0.0062</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.3">535</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.4">-0.0092</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.5">83</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.6">0.0316</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.7">118</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.8">0.0716</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.9">10</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.36.30.10.1">0.1161</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.11">8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.12">0.1061</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.36.30.13">123</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.37.31">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.37.31.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.2">0.0229</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.3">201</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.4">-0.0692</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.5">282</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.6">0.0685</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.7">224</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.8">0.0193</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.9">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.37.31.10.1">0.1051</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.11">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.12">0.1044</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.37.31.13">483</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.38.32">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.38.32.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.2">0.0002</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.3">104</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.4">0.0032</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.5">78</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.38.32.6.1">0.1412</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.7">118</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.8">0.0821</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.9">5</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.10">0.0967</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.11">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.12">0.0974</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.38.32.13">206</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.39.33">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.39.33.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.2">0.2159</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.3">268</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.4">0.0973</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.5">84</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.6">0.4042</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.7">54</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.8">0.2196</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.9">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.10">0.3755</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.11">5</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.39.33.12.1">0.4392</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.39.33.13">123</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.40.34">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.40.34.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.2">0.0890</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.3">78</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.4">0.2337</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.5">42</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.40.34.6.1">0.3178</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.7">76</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.8">0.1175</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.9">4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.10">0.1259</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.11">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.12">0.2895</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.40.34.13">138</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.41.35">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.41.35.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.2">0.2787</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.3">417</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.4">0.2484</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.5">67</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.6">0.4616</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.7">50</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.8">0.2661</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.10">0.3224</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.41.35.12.1">0.5102</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.41.35.13">133</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.42.36">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.42.36.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.2">0.3931</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.3">216</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.4">0.1298</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.5">99</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.6">0.4074</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.7">64</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.8">0.3328</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.9">25</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.10">0.3076</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.11">26</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.42.36.12.1">0.4422</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.42.36.13">105</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.43.37">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.43.37.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.2">0.0152</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.3">79</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.4">0.3020</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.5">28</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.6">0.2669</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.7">32</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.8">0.0498</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.9">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.10">0.0928</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.11">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.43.37.12.1">0.3659</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.43.37.13">58</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.44.38">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="13" id="S5.T3.6.6.44.38.1">Qwen1.5-14B</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.45.39">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.6.6.45.39.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.2">0.1363</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.3">16</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.4">0.2286</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.5">27</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.6">0.2182</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.7">12</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.8">0.1579</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.9">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.10">0.2245</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.11">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.45.39.12.1">0.2359</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.45.39.13">20</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.46.40">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.46.40.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.2">0.3011</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.3">16</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.4"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T3.6.6.46.40.4.1">0.3647</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.5">48</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.6">0.3131</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.7">12</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.8">0.2151</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.10">0.2838</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.12">0.3033</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.46.40.13">17</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.47.41">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.47.41.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.2">0.3758</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.3">68</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.4">0.2500</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.5">30</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.6">0.4131</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.7">11</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.8">0.3166</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.10">0.3504</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.11">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.12"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T3.6.6.47.41.12.1">0.4367</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.47.41.13">18</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.48.42">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.48.42.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.2">0.4836</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.3">86</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.4">0.5240</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.5">132</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.6">0.6467</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.7">26</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.8">0.4741</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.10">0.5483</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.48.42.12.1">0.6516</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.48.42.13">34</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.49.43">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.49.43.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.2">0.3485</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.3">213</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.4">0.4777</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.5">268</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.49.43.6.1">0.5114</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.7">33</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.8">0.3349</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.10">0.4466</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.11">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.12">0.4651</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.49.43.13">29</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.50.44">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.50.44.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.2">0.2201</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.3">124</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.4">0.5161</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.5">124</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.50.44.6.1">0.7200</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.7">17</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.8">0.5569</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.10">0.5790</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.12">0.6992</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.50.44.13">18</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.51.45">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.51.45.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.2">0.5157</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.3">27</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.4">0.5196</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.5">96</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.51.45.6.1">0.5597</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.7">12</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.8">0.4743</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.10">0.5397</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.11">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.12">0.5547</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.51.45.13">18</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.52.46">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.52.46.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.2">0.3828</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.3">71</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.4">0.4691</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.5">94</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.52.46.6.1">0.5936</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.7">7</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.8">0.2769</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.10">0.4091</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.11">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.12">0.5427</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.52.46.13">16</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.53.47">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="13" id="S5.T3.6.6.53.47.1">Mixtral-8x7B-AWQ</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.54.48">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.6.6.54.48.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.2">0.0870</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.3">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.4">0.0607</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.5">4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.54.48.6.1">0.2631</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.7">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.8">0.1572</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.9">2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.10">0.1930</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.11">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.12">0.2309</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.54.48.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.55.49">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.55.49.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.2">0.1067</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.3">19</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.4">0.0799</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.5">22</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.6">0.1825</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.7">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.8">0.0872</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.9">8</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.55.49.10.1">0.2078</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.11">8</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.12">0.1936</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.55.49.13">1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.56.50">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.56.50.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.2">0.3390</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.3">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.4">0.1253</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.5">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.56.50.6.1">0.3720</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.8">0.2104</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.10">0.2746</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.11">5</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.12">0.3434</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.56.50.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.57.51">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.57.51.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.2">0.3128</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.3">9</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.4">0.2081</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.5">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.57.51.6.1">0.6229</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.7">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.8">0.3499</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.9">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.10">0.4338</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.11">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.12">0.5903</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.57.51.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.58.52">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.58.52.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.2">0.4019</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.3">7</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.4">0.4025</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.5">8</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.58.52.6.1">0.4891</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.8">0.1212</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.9">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.10">0.2279</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.11">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.12">0.4684</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.58.52.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.59.53">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.59.53.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.2">0.3204</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.3">13</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.4">0.2557</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.5">16</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.59.53.6.1">0.6526</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.8">0.4053</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.9">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.10">0.4404</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.11">2</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.12">0.6164</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.59.53.13">1</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.60.54">
<td class="ltx_td ltx_align_left" id="S5.T3.6.6.60.54.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.2">0.4750</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.3">4</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.4">0.3742</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.5">13</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.60.54.6.1">0.5831</span></td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.8">0.4160</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.9">1</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.10">0.5022</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.11">3</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.12">0.5528</td>
<td class="ltx_td ltx_align_center" id="S5.T3.6.6.60.54.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T3.6.6.61.55">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.6.6.61.55.1">SI-EN</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.2">0.2652</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.3">6</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.4">0.2139</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.5">9</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.6.6.61.55.6.1">0.4563</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.7">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.8">0.0966</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.9">2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.10">0.2220</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.11">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.12">0.4124</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.6.6.61.55.13">1</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Spearman <math alttext="\rho" class="ltx_Math" display="inline" id="S5.T3.8.m1.1"><semantics id="S5.T3.8.m1.1b"><mi id="S5.T3.8.m1.1.1" xref="S5.T3.8.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T3.8.m1.1c"><ci id="S5.T3.8.m1.1.1.cmml" xref="S5.T3.8.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.m1.1d">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T3.8.m1.1e">italic_ρ</annotation></semantics></math> correlation scores achieved by zero-shot inference using <span class="ltx_text ltx_font_bold" id="S5.T3.10.1">Templates 1-6 (T1-6)</span> on various open-source LLMs for each language pair (LP). D -&gt; rows dropped as LLM generated output without a score. The underlined scores represent the best result among templates, while the bold represent the best among LLMs.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T3" title="Table 3 ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">3</span></a> shows our zero-shot results of Templates 1 to 6 for open-source LLMs including OpenChat3.5, Llama-2-7B, Gemma7B, Llama-2-13B, Qwen1.5-14B and Mixtral-8x7B-AWQ.</p>
</div>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Comparing results among LLMs</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.3">We observe that OpenChat3.5 achieved the highest Spearman correlation scores for most language pairs, despite having only <math alttext="7" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS2.SSS0.Px1.p1.1.m1.1a"><mn id="S5.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.1.m1.1b"><cn id="S5.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS2.SSS0.Px1.p1.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.1.m1.1c">7</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.1.m1.1d">7</annotation></semantics></math> billion parameters – roughly half the size of Llama-2-13B and Qwen1.5-14B. It excelled not only in Spearman scores but also in consistently providing valid quality evaluation scores, with very few dropped rows. Among the 6 models, Llama-2 (both the <math alttext="7" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S5.SS2.SSS0.Px1.p1.2.m2.1a"><mn id="S5.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.2.m2.1b"><cn id="S5.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS2.SSS0.Px1.p1.2.m2.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.2.m2.1c">7</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.2.m2.1d">7</annotation></semantics></math> and <math alttext="13" class="ltx_Math" display="inline" id="S5.SS2.SSS0.Px1.p1.3.m3.1"><semantics id="S5.SS2.SSS0.Px1.p1.3.m3.1a"><mn id="S5.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S5.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS0.Px1.p1.3.m3.1b"><cn id="S5.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" type="integer" xref="S5.SS2.SSS0.Px1.p1.3.m3.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS0.Px1.p1.3.m3.1c">13</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS0.Px1.p1.3.m3.1d">13</annotation></semantics></math> billion variants) performed poorly in generating evaluations with valid scores. Many rows were dropped, and the Spearman scores were low, indicating a weak correlation with the true scores. The MoE model, Mixtral-8x7B-AWQ, did not outperform OpenChat3.5 on most language pairs and prompt templates for our task.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Comparing with the baselines</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">We find that models fine-tuned for each language pair by TransQuest, performed much better than the zero-shot prompting results for all language pairs. For some language pairs like ET-EN, NE-EN and RO-EN, our best zero-shot prompting results (Template 6 of OpenChat3.5 in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T3" title="Table 3 ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">3</span></a>) were comparable to the reference-based models fine-tuned on multilingual data using COMET. For some other pairs like SI-EN, our best zero-shot prompting results were even slightly better than the COMET models.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="S5.F4.g1" src="extracted/5907453/high.png" width="580"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Density plots of the predicted (red) and true (mean) DA scores (blue) for high-resource language pairs <span class="ltx_text ltx_font_italic" id="S5.F4.2.1">i.e.</span>, EN-DE (left) and EN-ZH (right).</figcaption>
</figure>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="S5.F5.g1" src="extracted/5907453/low.png" width="580"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Density plots of the predicted (red) and true (mean) DA scores (blue) for medium- and low-resource language pairs <span class="ltx_text ltx_font_italic" id="S5.F5.2.1">i.e.</span>, RO-EN (left) and SI-EN (right).</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Comparison among Templates</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p1.1">When we fix the model variable as OpenChat3.5, we can compare the performance of different prompt templates. Looking at the OpenChat3.5 results in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T3" title="Table 3 ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that LLM performance is generally better when the source and reference are included in the prompt, as in Templates 3, 5, and 6, compared to prompts without them, such as Templates 1 and 2. This pattern holds true for other LLMs such as the LLama-2 models and Gemma, as shown in the table. Notably, the Spearman scores are obviously higher when the source is incorporated into the prompt, as seen by comparing Templates 2 and 3. This suggests that the source is an essential component for evaluating MT quality using LLMs, contrary to the results in <cite class="ltx_cite ltx_citemacro_citet">Huang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#bib.bib10" title="">2024</a>)</cite>, who indicate that the source provides a negative impact.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p2.1">Our results (on Templates 4, 5, 6) suggest that including error words and annotation guidelines does not consistently help LLMs evaluate MT quality across different language pairs when compared to using just the plain GEMBA prompt (Template 3). For most language pairs like EN-ZH, ET-EN, RU-EN, and SI-EN, Template 3 had the highest correlation with human judgments. However, removing reference translations (Template 4) clearly lowered correlation scores, highlighting their importance for accurate MT evaluation.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p3">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p3.1">Although incorporating error words does not seem to improve performance, they are surprisingly useful in helping LLMs provide scores in their outputs. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T3" title="Table 3 ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">3</span></a>,
there are fewer dropped rows when using Templates 4 and 5, which include error words. Outputs from Templates 4 and 5 are the most stable across models, unlike other templates that are more model-dependent.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Results among different language pairs</h4>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p1.1">For high-resource language pairs like EN-DE and EN-ZH, correlation scores tend to be lower than those of medium- and low-resource pairs such as NE-EN, RO-EN, and RU-EN. This pattern holds true across most models, including the fine-tuned ones from TransQuest and COMET.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p2.1">To further investigate the reasons, we selected EN-DE and EN-ZH as high-resource language pairs, and RO-EN and SI-EN as medium- and low-resource language pairs. We plotted the distributions of the predicted (from OpenChat3.5) <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS0.Px4.p2.1.1">vs</span> true scores (mean of all annotators) as shown in Figures <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.F4" title="Figure 4 ‣ Comparing with the baselines ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.F5" title="Figure 5 ‣ Comparing with the baselines ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">5</span></a>. For high-resource language pairs, the predictions are skewed towards higher DA scores. Well-trained MT systems, due to abundant resources, tend to produce high-quality translations, leading to higher DA scores. However, LLM-based evaluation systems may amplify these imbalanced distributions and are more likely to predict scores within the high range.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.3" style="width:426.8pt;height:145.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.0pt,17.1pt) scale(0.810105298755143,0.810105298755143) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T4.3.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T4.3.1.1.1.1.1">LP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.3.1.1.1.2">OpenChat3.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.3.1.1.1.3">Llama-2-7B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.3.1.1.1.4">Gemma-7B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.3.1.1.1.5">Llama-2-13B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.3.1.1.1.6">Qwen1.5-14B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T4.3.1.1.1.7">Mixtral-8x7B-AWQ</th>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.1">T7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.2">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.3">T7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.4">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.5">T7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.6">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.7">T7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.8">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.9">T7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.10">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.11">T7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.1.2.2.12">T3</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.3.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.2">0.2433</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.1.3.1.3.1">0.2849</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.4">-0.0353</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.3.1.5.1">0.0876</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.6">0.0048</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.3.1.7.1">0.1624</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.3.1.8.1">0.0345</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.9">0.0316</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.3.1.10.1">0.2388</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.11">0.2182</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.12">0.2213</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.1.3.1.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.3.1.13.1">0.2631</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.4.2">
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.2">0.2937</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.1.4.2.3.1">0.3546</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.4">-0.0021</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.4.2.5.1">0.1255</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.6">0.0859</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.4.2.7.1">0.1479</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.4.2.8.1">0.0804</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.9">0.0685</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.4.2.10.1">0.3455</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.11">0.3131</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.4.2.12.1">0.1906</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.4.2.13">0.1825</td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.5.3">
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.2">0.3324</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.1.5.3.3.1">0.3995</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.4">0.0354</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.5.3.5.1">0.0946</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.6">0.1609</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.5.3.7.1">0.1805</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.8">0.0703</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.5.3.9.1">0.1412</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.10">0.3429</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.5.3.11.1">0.4131</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.12">0.2479</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.5.3.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.5.3.13.1">0.3720</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.6.4">
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.2">0.6110</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.1.6.4.3.1">0.6980</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.4">0.1459</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.6.4.5.1">0.3715</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.6">0.3191</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.6.4.7.1">0.3772</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.8">0.2558</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.6.4.9.1">0.4042</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.10">0.5845</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.6.4.11.1">0.6467</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.12">0.4628</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.6.4.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.6.4.13.1">0.6229</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.7.5">
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.2">0.5160</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.1.7.5.3.1">0.5937</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.4">0.1363</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.7.5.5.1">0.2207</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.7.5.6.1">0.3221</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.7">0.2921</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.7.5.8.1">0.3315</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.9">0.3178</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.10">0.4791</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.7.5.11.1">0.5114</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.12">0.4373</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.7.5.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.7.5.13.1">0.4891</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.8.6">
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.2">0.7175</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.1.8.6.3.1">0.7294</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.4">0.1859</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.8.6.5.1">0.4514</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.8.6.6.1">0.4550</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.7">0.4429</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.8">0.3403</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.8.6.9.1">0.4616</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.10">0.7019</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.8.6.11.1">0.7200</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.12">0.6360</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.8.6.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.8.6.13.1">0.6526</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.9.7">
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.2">0.5317</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.1.9.7.3.1">0.6066</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.4">0.1618</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.9.7.5.1">0.4253</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.6">0.2979</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.9.7.7.1">0.4399</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.8">0.2519</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.9.7.9.1">0.4074</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.10">0.5203</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.9.7.11.1">0.5597</span></td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.12">0.5191</td>
<td class="ltx_td ltx_align_center" id="S5.T4.3.1.9.7.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.9.7.13.1">0.5831</span></td>
</tr>
<tr class="ltx_tr" id="S5.T4.3.1.10.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.1">SI-EN</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.2">0.5124</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.1.10.8.3.1">0.6034</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.4">0.1818</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.10.8.5.1">0.2212</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.6">0.2808</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.10.8.7.1">0.3519</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.10.8.8.1">0.2854</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.9">0.2669</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.10">0.4680</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.10.8.11.1">0.5936</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.3.1.10.8.12.1">0.4691</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.1.10.8.13">0.4563</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Spearman <math alttext="\rho" class="ltx_Math" display="inline" id="S5.T4.2.m1.1"><semantics id="S5.T4.2.m1.1b"><mi id="S5.T4.2.m1.1.1" xref="S5.T4.2.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T4.2.m1.1c"><ci id="S5.T4.2.m1.1.1.cmml" xref="S5.T4.2.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.m1.1d">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.m1.1e">italic_ρ</annotation></semantics></math> correlation scores achieved using <span class="ltx_text ltx_font_bold" id="S5.T4.6.1">Template 7 (T7)</span>, the CoT prompt template, on various LLMs for each language pair (LP). Results of <span class="ltx_text ltx_font_bold" id="S5.T4.7.2">Template 3 (T3)</span> from Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T3" title="Table 3 ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">3</span></a> are listed here for reference. The underlined scores represent better result between the templates, while the bold represent the best among LLMs.</figcaption>
</figure>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.3" style="width:426.8pt;height:145.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-50.0pt,17.1pt) scale(0.810105298755143,0.810105298755143) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.3.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S5.T5.3.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T5.3.1.1.1.1.1">LP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.3.1.1.1.2">OpenChat3.5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.3.1.1.1.3">Llama-2-7B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.3.1.1.1.4">Gemma-7B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.3.1.1.1.5">Llama-2-13B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.3.1.1.1.6">Qwen1.5-14B</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="S5.T5.3.1.1.1.7">Mixtral-8x7B-AWQ</th>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.1">T8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.2">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.3">T8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.4">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.5">T8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.6">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.7">T8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.8">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.9">T8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.10">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.11">T8</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.1.2.2.12">T3</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.3.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.2">0.1756</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T5.3.1.3.1.3.1">0.2849</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.4">0.0327</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.3.1.5.1">0.0876</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.6">0.0343</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.3.1.7.1">0.1624</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.3.1.8.1">0.0655</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.9">0.0316</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.10">0.1804</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.3.1.11.1">0.2182</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.12">0.2155</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.1.3.1.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.3.1.13.1">0.2631</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.4.2">
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.2">0.2543</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T5.3.1.4.2.3.1">0.3546</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.4">0.0078</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.4.2.5.1">0.1255</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.4.2.6.1">0.1651</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.7">0.1479</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.8">0.0159</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.4.2.9.1">0.068</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.10">0.2706</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.4.2.11.1">0.3131</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.4.2.12.1">0.2410</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.4.2.13">0.1825</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.5.3">
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.2">0.2801</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.5.3.3.1">0.3995</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.4">0.0283</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.5.3.5.1">0.0946</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.5.3.6.1">0.1831</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.7">0.1805</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.8">0.0875</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.5.3.9.1">0.1412</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.10">0.2946</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.11"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T5.3.1.5.3.11.1">0.4131</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.12">0.2970</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.5.3.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.5.3.13.1">0.3720</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.6.4">
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.2">0.5779</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T5.3.1.6.4.3.1">0.6980</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.4">-0.0026</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.6.4.5.1">0.3715</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.6.4.6.1">0.4134</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.7">0.3772</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.8">0.2328</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.6.4.9.1">0.4042</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.10">0.4320</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.6.4.11.1">0.6467</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.12">0.5566</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.6.4.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.6.4.13.1">0.6229</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.7.5">
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.2">0.4621</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T5.3.1.7.5.3.1">0.5937</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.4">0.1428</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.7.5.5.1">0.2207</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.7.5.6.1">0.3117</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.7">0.2921</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.8">0.1907</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.7.5.9.1">0.3178</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.10">0.3349</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.7.5.11.1">0.5114</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.7.5.12.1">0.5143</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.7.5.13">0.4891</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.8.6">
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.2">0.6881</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T5.3.1.8.6.3.1">0.7294</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.4">0.0405</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.8.6.5.1">0.4514</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.8.6.6.1">0.4693</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.7">0.4429</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.8">0.2574</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.8.6.9.1">0.4616</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.10">0.4498</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.8.6.11.1">0.7200</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.8.6.12.1">0.6712</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.8.6.13">0.6526</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.9.7">
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.2">0.5774</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T5.3.1.9.7.3.1">0.6066</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.4">0.1680</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.9.7.5.1">0.4253</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.6">0.2531</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.9.7.7.1">0.4399</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.8">0.1951</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.9.7.9.1">0.4074</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.10">0.4798</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.9.7.11.1">0.5597</span></td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.12">0.5239</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.1.9.7.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.9.7.13.1">0.5831</span></td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.1.10.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.1">SI-EN</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.2">0.4277</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.3"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T5.3.1.10.8.3.1">0.6034</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.4">0.0352</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.10.8.5.1">0.2212</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.6">0.3048</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.10.8.7.1">0.3519</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.8">0.1368</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.10.8.9.1">0.2669</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.10">0.4207</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.10.8.11.1">0.5936</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.1.10.8.12.1">0.4642</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T5.3.1.10.8.13">0.4563</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Spearman <math alttext="\rho" class="ltx_Math" display="inline" id="S5.T5.2.m1.1"><semantics id="S5.T5.2.m1.1b"><mi id="S5.T5.2.m1.1.1" xref="S5.T5.2.m1.1.1.cmml">ρ</mi><annotation-xml encoding="MathML-Content" id="S5.T5.2.m1.1c"><ci id="S5.T5.2.m1.1.1.cmml" xref="S5.T5.2.m1.1.1">𝜌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.m1.1d">\rho</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.m1.1e">italic_ρ</annotation></semantics></math> correlation scores achieved using <span class="ltx_text ltx_font_bold" id="S5.T5.6.1">Template 8 (T8)</span>, the few-shot prompt template, on various LLMs for each language pair (LP). Results of <span class="ltx_text ltx_font_bold" id="S5.T5.7.2">Template 3 (T3)</span> from Table 3 are listed here for reference. The underlined scores represent better result between the templates, while the bold represent the best among LLMs.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p3">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p3.1">In contrast, for medium- and low-resource language pairs, there are fewer resources for training MT systems. As a result, low-quality translations (with low DA scores) are better represented than in high-resource pairs. Quality evaluation systems can better recognize low-quality translations and produce a more balanced score distribution. This imbalance in the score representation could be the reason why predicted DA scores for high-resource languages are less correlated with true scores than for medium- and low-resource pairs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>CoT and Few-shot Inference</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.2">Tables <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T4" title="Table 4 ‣ Results among different language pairs ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T5" title="Table 5 ‣ Results among different language pairs ‣ 5.2 Zero-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">5</span></a> show results of CoT (Template 7) and 5-shot inference (Template 8) together with the results of Template 3 for the 6 selected LLMs. Dropped rows for the two templates are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T6" title="Table 6 ‣ 5.3 CoT and Few-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">6</span></a>. Both Templates 7 and 8 were built upon Template 3, <span class="ltx_text ltx_font_italic" id="S5.SS3.p1.2.1">i.e.</span>, including the source, MT output and reference. We expect the model performance to be improved when more reasoning steps or evaluation examples were given. However, for <math alttext="7" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><mn id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><cn id="S5.SS3.p1.1.m1.1.1.cmml" type="integer" xref="S5.SS3.p1.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">7</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">7</annotation></semantics></math> billion parameter variants, CoT prompting resulted in worse performance, as Spearman correlation scores of Template 7 were obviously lower than those of Template 3. For the larger <math alttext="13" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><mn id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml">13</mn><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><cn id="S5.SS3.p1.2.m2.1.1.cmml" type="integer" xref="S5.SS3.p1.2.m2.1.1">13</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">13</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">13</annotation></semantics></math> billion parameter variants, results were mixed for different language pairs. For language pairs such as EN-DE and EN-MR, CoT prompting improved the performance in the prediction of DA scores. This indicates that CoT may work better on larger models than smaller models. While CoT prompting did not consistently improve model performance as measured by the Spearman correlation scores, it shows relatively more consistent output than other prompt templates. Table <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S5.T6" title="Table 6 ‣ 5.3 CoT and Few-shot Inference ‣ 5 Results and Discussion ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">6</span></a> suggests that fewer rows were dropped when using Template 7, especially for Llama-2 models.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Interestingly, 5-shot inference results are not better than zero-shot results, posing a question on context utilization by LLMs. Performance varies on the LLMs and the specific language pairs. This could relate to the language data available for training these LLMs, as well as the quality of the evaluation examples chosen for different languages pairs.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T6.1" style="width:369.9pt;height:197.7pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(16.5pt,-8.8pt) scale(1.09812609259547,1.09812609259547) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_tt" id="S5.T6.1.1.1.1.1" rowspan="2"><span class="ltx_text" id="S5.T6.1.1.1.1.1.1">LP</span></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T6.1.1.1.1.2">OpenChat3.5</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T6.1.1.1.1.3">Llama-2-7B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T6.1.1.1.1.4">Gemma-7B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T6.1.1.1.1.5">Llama-2-13B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T6.1.1.1.1.6">Qwen1.5-14B</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S5.T6.1.1.1.1.7">Mixtral-8x7B-AWQ</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.2.2">
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.1">T7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.2">T8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.3">T7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.4">T8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.5">T7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.6">T8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.7">T7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.8">T8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.9">T7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.10">T8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.11">T7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.2.2.12">T8</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S5.T6.1.1.3.3.1">EN-DE</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.3">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.4">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.5">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.6">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.7">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.8">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.9">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.10">3</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.11">9</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.12">0</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.3.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T6.1.1.4.4.1">EN-MR</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.2">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.4">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.5">7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.6">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.8">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.10">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.11">8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.12">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.4.13">2</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.5.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T6.1.1.5.5.1">EN-ZH</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.2">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.4">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.5">21</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.6">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.7">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.8">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.10">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.11">24</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.12">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.5.5.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T6.1.1.6.6.1">ET-EN</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.2">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.4">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.5">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.6">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.8">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.10">38</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.11">22</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.12">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.6.6.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.7.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T6.1.1.7.7.1">NE-EN</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.2">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.4">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.5">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.6">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.8">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.10">36</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.11">23</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.12">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.7.7.13">2</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T6.1.1.8.8.1">RO-EN</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.2">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.4">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.5">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.6">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.7">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.8">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.9">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.10">27</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.11">69</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.12">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.8.8.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S5.T6.1.1.9.9.1">RU-EN</th>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.2">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.3">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.4">4</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.5">7</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.6">4</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.7">0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.8">3</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.9">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.10">17</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.11">29</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.12">1</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.9.9.13">0</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S5.T6.1.1.10.10.1">SI-EN</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.2">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.3">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.4">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.5">2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.6">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.7">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.8">0</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.9">2</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.10">4</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.11">5</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.12">1</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.1.1.10.10.13">0</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span><span class="ltx_text ltx_font_bold" id="S5.T6.6.1">D</span>ropped rows for <span class="ltx_text ltx_font_bold" id="S5.T6.7.2">Template 7 (T7)</span> and <span class="ltx_text ltx_font_bold" id="S5.T6.8.3">Template 8 (T8)</span>, <span class="ltx_text ltx_font_italic" id="S5.T6.9.4">i.e.</span>, the CoT and few-shot prompt templates, using various LLMs for each language pair (LP).</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Discussion</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">Based on our results, <span class="ltx_text ltx_font_italic" id="S5.SS4.p1.1.1">Template 3</span>, which includes the source, MT output and reference, but excludes error words and detailed guidelines, performed the best in terms of Spearman correlation scores. Prompting with CoT and few-shot learning may yield better results for larger models, but more experiments are needed to confirm this.</p>
</div>
<div class="ltx_para" id="S5.SS4.p2">
<p class="ltx_p" id="S5.SS4.p2.2">While larger language models often perform better, our results show that a <math alttext="7" class="ltx_Math" display="inline" id="S5.SS4.p2.1.m1.1"><semantics id="S5.SS4.p2.1.m1.1a"><mn id="S5.SS4.p2.1.m1.1.1" xref="S5.SS4.p2.1.m1.1.1.cmml">7</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.1.m1.1b"><cn id="S5.SS4.p2.1.m1.1.1.cmml" type="integer" xref="S5.SS4.p2.1.m1.1.1">7</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.1.m1.1c">7</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.1.m1.1d">7</annotation></semantics></math>-billion parameter model outperformed other models for most language pairs. Surprisingly, even much smaller COMET models fine-tuned on multilingual data, rather than data for specific language pairs, usually outperformed our LLM prompting results. However, due to the high computational cost, we could not test models with <math alttext="70" class="ltx_Math" display="inline" id="S5.SS4.p2.2.m2.1"><semantics id="S5.SS4.p2.2.m2.1a"><mn id="S5.SS4.p2.2.m2.1.1" xref="S5.SS4.p2.2.m2.1.1.cmml">70</mn><annotation-xml encoding="MathML-Content" id="S5.SS4.p2.2.m2.1b"><cn id="S5.SS4.p2.2.m2.1.1.cmml" type="integer" xref="S5.SS4.p2.2.m2.1.1">70</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.p2.2.m2.1c">70</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.p2.2.m2.1d">70</annotation></semantics></math> billion or more parameters.</p>
</div>
<div class="ltx_para" id="S5.SS4.p3">
<p class="ltx_p" id="S5.SS4.p3.1">Different models excel at various language pairs while struggling with others. Even for a single model, performance fluctuates across different language pairs. This variability could stem from whether a language is considered high-resource, but further research is necessary to understand the underlying causes.</p>
</div>
<div class="ltx_para" id="S5.SS4.p4">
<p class="ltx_p" id="S5.SS4.p4.1">Our experiments with prompting LLMs for translation evaluation reveal that these models are often inconsistent in <span class="ltx_text ltx_font_italic" id="S5.SS4.p4.1.1">generating numerical scores</span>. In most cases, LLMs tend to generate scores accompanied by lengthy and unstructured explanations. While using regular expressions for extraction can be helpful, it is not always reliable. For models like Llama-2, we observed numerous instances where LLMs failed to produce a valid score. Our empirical findings demonstrate that employing CoT prompting or incorporating error words into the prompt can enhance the consistency of the model outputs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion and Future Work</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we explored what translation information is needed for LLMs to evaluate MT quality. We conducted a comprehensive investigation into different prompting techniques such as zero-shot, CoT and few-shot prompting using different translation information for 8 language pairs and 6 LLMs of different sizes and types. Our findings suggest that the source, MT output and reference are essential compared to other information such as translation errors for quality evaluation. Larger models may not necessarily perform better than smaller models, but CoT prompting works better on larger than smaller model variants. We also observe that LLMs do not always provide a numerical score when generating evaluations, which makes their assessments less reliable. For future research, we plan to explore whether fine-tuning LLMs could improve their performance in quality evaluation. We also plan to thoroughly investigate error explainability of LLMs using MQM and other fine-grained error identification techniques. These future studies can inform downstream error correction through automatic post-editing, contributing to a more comprehensive evaluation and correction framework.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Limitations and Ethical Considerations</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">Our results were achieved on a limited number of LLMs which are mostly smaller than 14 billion parameters due to the constraints of our computational capabilities. Larger models may perform differently in this translation evaluation task. The examples used in the few-shot scenario were randomly sampled since we do not have the knowledge to prepare good-quality examples for all language pairs. Results might be different if these examples were carefully chosen by native speakers.</p>
</div>
<div class="ltx_para" id="Sx1.p2">
<p class="ltx_p" id="Sx1.p2.1">Our experiments in the paper were conducted solely on publicly available datasets as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2410.03278v2#S3" title="3 Data ‣ What do Large Language Models Need for Machine Translation Evaluation?"><span class="ltx_text ltx_ref_tag">3</span></a>, requiring no ethical approval.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">We would like to thank the European Association for Machine Translation (EAMT) for funding QE data curation of Indic languages used in this paper (UoS/RN0580).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Alves et al. (2024)</span>
<span class="ltx_bibblock">
Duarte Miguel Alves, José Pombal, Nuno M Guerreiro, Pedro Henrique Martins, João Alves, Amin Farajian, Ben Peters, Ricardo Rei, Patrick Fernandes, Sweta Agrawal, Pierre Colombo, José G. C. de Souza, and Andre Martins. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=EHPns3hVkj" title="">Tower: An Open Multilingual Large Language Model for Translation-Related Tasks</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">First Conference on Language Modeling</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al. (2023)</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, Keming Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xingxuan Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2309.16609" title="">Qwen Technical Report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blain et al. (2023)</span>
<span class="ltx_bibblock">
Frederic Blain, Chrysoula Zerva, Ricardo Rei, Nuno M. Guerreiro, Diptesh Kanojia, José G. C. de Souza, Beatriz Silva, Tânia Vaz, Yan Jingxuan, Fatemeh Azadi, Constantin Orasan, and André Martins. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.52" title="">Findings of the WMT 2023 shared task on quality estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 629–653, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conneau et al. (2020)</span>
<span class="ltx_bibblock">
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.747" title="">Unsupervised cross-lingual representation learning at scale</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 8440–8451, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deoghare et al. (2023a)</span>
<span class="ltx_bibblock">
Sourabh Deoghare, Paramveer Choudhary, Diptesh Kanojia, Tharindu Ranasinghe, Pushpak Bhattacharyya, and Constantin Orăsan. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-acl.585" title="">A multi-task learning framework for quality estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 9191–9205, Toronto, Canada. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deoghare et al. (2023b)</span>
<span class="ltx_bibblock">
Sourabh Deoghare, Diptesh Kanojia, Fred Blain, Tharindu Ranasinghe, and Pushpak Bhattacharyya. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.findings-emnlp.115" title="">Quality estimation-assisted automatic post-editing</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Findings of the Association for Computational Linguistics: EMNLP 2023</em>, pages 1686–1698, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fernandes et al. (2023)</span>
<span class="ltx_bibblock">
Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André Martins, Graham Neubig, Ankush Garg, Jonathan Clark, Markus Freitag, and Orhan Firat. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.100" title="">The devil is in the errors: Leveraging large language models for fine-grained machine translation evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 1066–1083, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemma Team et al. (2024)</span>
<span class="ltx_bibblock">
Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Léonard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Amélie Héliou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Clément Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Mikuła, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem,
Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Clément Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2403.08295" title="">Gemma: Open Models Based on Gemini Research and Technology</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Preprint</em>, arXiv:2403.08295.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Graham et al. (2013)</span>
<span class="ltx_bibblock">
Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. 2013.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/W13-2305" title="">Continuous measurement scales in human evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</em>, pages 33–41, Sofia, Bulgaria. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. (2024)</span>
<span class="ltx_bibblock">
Xu Huang, Zhirui Zhang, Xiang Geng, Yichao Du, Jiajun Chen, and Shujian Huang. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2024.findings-acl.211" title="">Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Findings of the Association for Computational Linguistics ACL 2024</em>, pages 3546–3562, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2024)</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2401.04088" title="">Mixtral of Experts</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Preprint</em>, arXiv:2401.04088.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kanojia et al. (2021)</span>
<span class="ltx_bibblock">
Diptesh Kanojia, Marina Fomicheva, Tharindu Ranasinghe, Frédéric Blain, Constantin Orăsan, and Lucia Specia. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.67" title="">Pushing the right buttons: Adversarial evaluation of quality estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 625–638, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi and Federmann (2023a)</span>
<span class="ltx_bibblock">
Tom Kocmi and Christian Federmann. 2023a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2023.wmt-1.64" title="">GEMBA-MQM: Detecting translation quality error spans with GPT-4</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Proceedings of the Eighth Conference on Machine Translation</em>, pages 768–775, Singapore. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocmi and Federmann (2023b)</span>
<span class="ltx_bibblock">
Tom Kocmi and Christian Federmann. 2023b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2023.eamt-1.19" title="">Large language models are state-of-the-art evaluators of translation quality</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 24th Annual Conference of the European Association for Machine Translation</em>, pages 193–203, Tampere, Finland. European Association for Machine Translation.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kocoń et al. (2023)</span>
<span class="ltx_bibblock">
Jan Kocoń, Igor Cichecki, Oliwier Kaszyca, Mateusz Kochanek, Dominika Szydło, Joanna Baran, Julita Bielaniewicz, Marcin Gruza, Arkadiusz Janz, Kamil Kanclerz, Anna Kocoń, Bartłomiej Koptyra, Wiktoria Mieleszczenko-Kowszewicz, Piotr Miłkowski, Marcin Oleksy, Maciej Piasecki, Łukasz Radliński, Konrad Wojtasik, Stanisław Woźniak, and Przemysław Kazienko. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1016/j.inffus.2023.101861" title="">ChatGPT: Jack of all trades, master of none</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Information Fusion</em>, 99:101861.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. (2023)</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1145/3600006.3613165" title="">Efficient Memory Management for Large Language Model Serving with PagedAttention</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 29th Symposium on Operating Systems Principles</em>, SOSP ’23, page 611–626, New York, NY, USA. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2024)</span>
<span class="ltx_bibblock">
Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://proceedings.mlsys.org/paper_files/paper/2024/file/42a452cbafa9dd64e9ba4aa95cc1ef21-Paper-Conference.pdf" title="">AWQ: Activation-aware Weight Quantization for On-Device LLM Compression and Acceleration</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of Machine Learning and Systems</em>, volume 6, pages 87–100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lommel et al. (2014)</span>
<span class="ltx_bibblock">
Arle Richard Lommel, Aljoscha Burchardt, and Hans Uszkoreit. 2014.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.5565/rev/tradumatica.77" title="">Multidimensional Quality Metrics: A Flexible System for Assessing Translation Quality</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Tradumàtica: tecnologies de la traducció</em>, 0:455–463.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al. (2024)</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo
Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan
Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,
Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia
Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2303.08774" title="">GPT-4 Technical Report</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.3115/1073083.1073135" title="">Bleu: a method for automatic evaluation of machine translation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pages 311–318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranasinghe et al. (2020a)</span>
<span class="ltx_bibblock">
Tharindu Ranasinghe, Constantin Orasan, and Ruslan Mitkov. 2020a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.122" title="">TransQuest at WMT2020: Sentence-level direct assessment</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 1049–1055, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranasinghe et al. (2020b)</span>
<span class="ltx_bibblock">
Tharindu Ranasinghe, Constantin Orasan, and Ruslan Mitkov. 2020b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.coling-main.445" title="">TransQuest: Translation quality estimation with cross-lingual transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 28th International Conference on Computational Linguistics</em>, pages 5070–5081, Barcelona, Spain (Online). International Committee on Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ranasinghe et al. (2021)</span>
<span class="ltx_bibblock">
Tharindu Ranasinghe, Constantin Orasan, and Ruslan Mitkov. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2021.acl-short.55" title="">An exploratory analysis of multilingual word-level quality estimation with cross-lingual transformers</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)</em>, pages 434–440, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2022a)</span>
<span class="ltx_bibblock">
Ricardo Rei, José G. C. de Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and André F. T. Martins. 2022a.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.52" title="">COMET-22: Unbabel-IST 2022 submission for the metrics shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 578–585, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2020)</span>
<span class="ltx_bibblock">
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.emnlp-main.213" title="">COMET: A neural framework for MT evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 2685–2702, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rei et al. (2022b)</span>
<span class="ltx_bibblock">
Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022b.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.60" title="">CometKiwi: IST-unbabel 2022 submission for the quality estimation shared task</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 634–645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sellam et al. (2020)</span>
<span class="ltx_bibblock">
Thibault Sellam, Dipanjan Das, and Ankur Parikh. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2020.acl-main.704" title="">BLEURT: Learning robust metrics for text generation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 7881–7892, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shazeer et al. (2017)</span>
<span class="ltx_bibblock">
Noam Shazeer, Azalia Mirhoseini*, Krzysztof Maziarz*, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=B1ckMDqlg" title=""> Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snover et al. (2006)</span>
<span class="ltx_bibblock">
Matthew Snover, Bonnie Dorr, Rich Schwartz, Linnea Micciulla, and John Makhoul. 2006.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2006.amta-papers.25" title="">A study of translation edit rate with targeted human annotation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers</em>, pages 223–231, Cambridge, Massachusetts, USA. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spearman (1904)</span>
<span class="ltx_bibblock">
Charles Spearman. 1904.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="http://www.jstor.org/stable/1412159?origin=JSTOR-pdf" title="">The Proof and Measurement of Association between Two Things</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">The American Journal of Psychology</em>, 15:72–101.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Specia et al. (2020)</span>
<span class="ltx_bibblock">
Lucia Specia, Frédéric Blain, Marina Fomicheva, Erick Fonseca, Vishrav Chaudhary, Francisco Guzmán, and André F. T. Martins. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.wmt-1.79" title="">Findings of the WMT 2020 shared task on quality estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the Fifth Conference on Machine Translation</em>, pages 743–764, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Specia et al. (2021)</span>
<span class="ltx_bibblock">
Lucia Specia, Frédéric Blain, Marina Fomicheva, Chrysoula Zerva, Zhenhao Li, Vishrav Chaudhary, and André F. T. Martins. 2021.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2021.wmt-1.71" title="">Findings of the WMT 2021 shared task on quality estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Proceedings of the Sixth Conference on Machine Translation</em>, pages 684–725, Online. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Specia et al. (2018)</span>
<span class="ltx_bibblock">
Lucia Specia, Carolina Scarton, and Gustavo Henrique Paetzold. 2018.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1007/978-3-031-02168-8_1" title=""><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1.1">Quality Estimation for Machine Translation</em></a>.

</span>
<span class="ltx_bibblock">Spinger, Cham, Germany.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Stewart et al. (2020)</span>
<span class="ltx_bibblock">
Craig Stewart, Ricardo Rei, Catarina Farinha, and Alon Lavie. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2020.amta-user.4" title="">COMET - deploying a new state-of-the-art MT evaluation metric in production</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track)</em>, pages 78–109, Virtual. Association for Machine Translation in the Americas.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan et al. (2023)</span>
<span class="ltx_bibblock">
Yiming Tan, Dehai Min, Yu Li, Wenbo Li, Nan Hu, Yongrui Chen, and Guilin Qi. 2023.

</span>
<span class="ltx_bibblock">Can ChatGPT Replace Traditional KBQA Models? An In-Depth Analysis of the Question Answering Performance of the GPT LLM Family.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">The Semantic Web – ISWC 2023</em>, pages 348–367, Cham. Springer Nature Switzerland.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2307.09288" title="">Llama 2: Open Foundation and Fine-Tuned Chat Models</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Preprint</em>, arXiv:2307.09288.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wan et al. (2022)</span>
<span class="ltx_bibblock">
Yu Wan, Dayiheng Liu, Baosong Yang, Haibo Zhang, Boxing Chen, Derek Wong, and Lidia Chao. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.18653/v1/2022.acl-long.558" title="">UniTE: Unified translation evaluation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 8117–8127, Dublin, Ireland. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2024)</span>
<span class="ltx_bibblock">
Guan Wang, Sijie Cheng, Xianyuan Zhan, Xiangang Li, Sen Song, and Yang Liu. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=AOJyfhWYHf" title="">OpenChat: Advancing Open-source Language Models with Mixed-Quality Data</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">The Twelfth International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al. (2024)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2024.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 36th International Conference on Neural Information Processing Systems</em>, NIPS ’22, Red Hook, NY, USA. Curran Associates Inc.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zerva et al. (2022)</span>
<span class="ltx_bibblock">
Chrysoula Zerva, Frédéric Blain, Ricardo Rei, Piyawat Lertvittayakumjorn, José G. C. de Souza, Steffen Eger, Diptesh Kanojia, Duarte Alves, Constantin Orăsan, Marina Fomicheva, André F. T. Martins, and Lucia Specia. 2022.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://aclanthology.org/2022.wmt-1.3" title="">Findings of the WMT 2022 shared task on quality estimation</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the Seventh Conference on Machine Translation (WMT)</em>, pages 69–99, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang* et al. (2020)</span>
<span class="ltx_bibblock">
Tianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://openreview.net/forum?id=SkeHuCVFDr" title="">BERTScore: Evaluating Text Generation with BERT</a>.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">International Conference on Learning Representations</em>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2024)</span>
<span class="ltx_bibblock">
Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang, Kathleen McKeown, and Tatsunori B. Hashimoto. 2024.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://doi.org/10.1162/tacl_a_00632" title="">Benchmarking Large Language Models for News Summarization</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Transactions of the Association for Computational Linguistics</em>, 12:39–57.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2023)</span>
<span class="ltx_bibblock">
Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zhicheng Dou, and Ji-Rong Wen. 2023.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_href" href="https://arxiv.org/abs/2308.07107" title="">Large Language Models for Information Retrieval: A Survey</a>.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Preprint</em>, arXiv:2308.07107.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span> Pearson’s <math alttext="r" class="ltx_Math" display="inline" id="A1.1.m1.1"><semantics id="A1.1.m1.1b"><mi id="A1.1.m1.1.1" xref="A1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.1.m1.1c"><ci id="A1.1.m1.1.1.cmml" xref="A1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.1.m1.1d">r</annotation><annotation encoding="application/x-llamapun" id="A1.1.m1.1e">italic_r</annotation></semantics></math> and Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="A1.2.m2.1"><semantics id="A1.2.m2.1b"><mi id="A1.2.m2.1.1" xref="A1.2.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.2.m2.1c"><ci id="A1.2.m2.1.1.cmml" xref="A1.2.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.2.m2.1d">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.2.m2.1e">italic_τ</annotation></semantics></math> Correlation Scores</h2>
<figure class="ltx_table" id="A1.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A1.T7.16" style="width:455.2pt;height:632.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-135.4pt,187.9pt) scale(0.627104790287068,0.627104790287068) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A1.T7.16.16">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T7.16.16.17.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T7.16.16.17.1.1" rowspan="2"><span class="ltx_text" id="A1.T7.16.16.17.1.1.1">LP</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.16.16.17.1.2">T1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.16.16.17.1.3">T2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.16.16.17.1.4">T3</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.16.16.17.1.5">T4</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.16.16.17.1.6">T5</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.16.16.17.1.7">T6</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.16.16.17.1.8">T7</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="2" id="A1.T7.16.16.17.1.9">T8</th>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.16">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.1.1.1.1"><math alttext="r" class="ltx_Math" display="inline" id="A1.T7.1.1.1.1.m1.1"><semantics id="A1.T7.1.1.1.1.m1.1a"><mi id="A1.T7.1.1.1.1.m1.1.1" xref="A1.T7.1.1.1.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.T7.1.1.1.1.m1.1b"><ci id="A1.T7.1.1.1.1.m1.1.1.cmml" xref="A1.T7.1.1.1.1.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.1.1.1.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="A1.T7.1.1.1.1.m1.1d">italic_r</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.2.2.2.2"><math alttext="\tau" class="ltx_Math" display="inline" id="A1.T7.2.2.2.2.m1.1"><semantics id="A1.T7.2.2.2.2.m1.1a"><mi id="A1.T7.2.2.2.2.m1.1.1" xref="A1.T7.2.2.2.2.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.2.2.2.2.m1.1b"><ci id="A1.T7.2.2.2.2.m1.1.1.cmml" xref="A1.T7.2.2.2.2.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.2.2.2.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.T7.2.2.2.2.m1.1d">italic_τ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.3.3.3.3"><math alttext="r" class="ltx_Math" display="inline" id="A1.T7.3.3.3.3.m1.1"><semantics id="A1.T7.3.3.3.3.m1.1a"><mi id="A1.T7.3.3.3.3.m1.1.1" xref="A1.T7.3.3.3.3.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.T7.3.3.3.3.m1.1b"><ci id="A1.T7.3.3.3.3.m1.1.1.cmml" xref="A1.T7.3.3.3.3.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.3.3.3.3.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="A1.T7.3.3.3.3.m1.1d">italic_r</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.4.4.4.4"><math alttext="\tau" class="ltx_Math" display="inline" id="A1.T7.4.4.4.4.m1.1"><semantics id="A1.T7.4.4.4.4.m1.1a"><mi id="A1.T7.4.4.4.4.m1.1.1" xref="A1.T7.4.4.4.4.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.4.4.4.4.m1.1b"><ci id="A1.T7.4.4.4.4.m1.1.1.cmml" xref="A1.T7.4.4.4.4.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.4.4.4.4.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.T7.4.4.4.4.m1.1d">italic_τ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.5.5.5.5"><math alttext="r" class="ltx_Math" display="inline" id="A1.T7.5.5.5.5.m1.1"><semantics id="A1.T7.5.5.5.5.m1.1a"><mi id="A1.T7.5.5.5.5.m1.1.1" xref="A1.T7.5.5.5.5.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.T7.5.5.5.5.m1.1b"><ci id="A1.T7.5.5.5.5.m1.1.1.cmml" xref="A1.T7.5.5.5.5.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.5.5.5.5.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="A1.T7.5.5.5.5.m1.1d">italic_r</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.6.6.6.6"><math alttext="\tau" class="ltx_Math" display="inline" id="A1.T7.6.6.6.6.m1.1"><semantics id="A1.T7.6.6.6.6.m1.1a"><mi id="A1.T7.6.6.6.6.m1.1.1" xref="A1.T7.6.6.6.6.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.6.6.6.6.m1.1b"><ci id="A1.T7.6.6.6.6.m1.1.1.cmml" xref="A1.T7.6.6.6.6.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.6.6.6.6.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.T7.6.6.6.6.m1.1d">italic_τ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.7.7.7.7"><math alttext="r" class="ltx_Math" display="inline" id="A1.T7.7.7.7.7.m1.1"><semantics id="A1.T7.7.7.7.7.m1.1a"><mi id="A1.T7.7.7.7.7.m1.1.1" xref="A1.T7.7.7.7.7.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.T7.7.7.7.7.m1.1b"><ci id="A1.T7.7.7.7.7.m1.1.1.cmml" xref="A1.T7.7.7.7.7.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.7.7.7.7.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="A1.T7.7.7.7.7.m1.1d">italic_r</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.8.8.8.8"><math alttext="\tau" class="ltx_Math" display="inline" id="A1.T7.8.8.8.8.m1.1"><semantics id="A1.T7.8.8.8.8.m1.1a"><mi id="A1.T7.8.8.8.8.m1.1.1" xref="A1.T7.8.8.8.8.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.8.8.8.8.m1.1b"><ci id="A1.T7.8.8.8.8.m1.1.1.cmml" xref="A1.T7.8.8.8.8.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.8.8.8.8.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.T7.8.8.8.8.m1.1d">italic_τ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.9.9.9.9"><math alttext="r" class="ltx_Math" display="inline" id="A1.T7.9.9.9.9.m1.1"><semantics id="A1.T7.9.9.9.9.m1.1a"><mi id="A1.T7.9.9.9.9.m1.1.1" xref="A1.T7.9.9.9.9.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.T7.9.9.9.9.m1.1b"><ci id="A1.T7.9.9.9.9.m1.1.1.cmml" xref="A1.T7.9.9.9.9.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.9.9.9.9.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="A1.T7.9.9.9.9.m1.1d">italic_r</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.10.10.10.10"><math alttext="\tau" class="ltx_Math" display="inline" id="A1.T7.10.10.10.10.m1.1"><semantics id="A1.T7.10.10.10.10.m1.1a"><mi id="A1.T7.10.10.10.10.m1.1.1" xref="A1.T7.10.10.10.10.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.10.10.10.10.m1.1b"><ci id="A1.T7.10.10.10.10.m1.1.1.cmml" xref="A1.T7.10.10.10.10.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.10.10.10.10.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.T7.10.10.10.10.m1.1d">italic_τ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.11.11.11.11"><math alttext="r" class="ltx_Math" display="inline" id="A1.T7.11.11.11.11.m1.1"><semantics id="A1.T7.11.11.11.11.m1.1a"><mi id="A1.T7.11.11.11.11.m1.1.1" xref="A1.T7.11.11.11.11.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.T7.11.11.11.11.m1.1b"><ci id="A1.T7.11.11.11.11.m1.1.1.cmml" xref="A1.T7.11.11.11.11.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.11.11.11.11.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="A1.T7.11.11.11.11.m1.1d">italic_r</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.12.12.12.12"><math alttext="\tau" class="ltx_Math" display="inline" id="A1.T7.12.12.12.12.m1.1"><semantics id="A1.T7.12.12.12.12.m1.1a"><mi id="A1.T7.12.12.12.12.m1.1.1" xref="A1.T7.12.12.12.12.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.12.12.12.12.m1.1b"><ci id="A1.T7.12.12.12.12.m1.1.1.cmml" xref="A1.T7.12.12.12.12.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.12.12.12.12.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.T7.12.12.12.12.m1.1d">italic_τ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.13.13.13.13"><math alttext="r" class="ltx_Math" display="inline" id="A1.T7.13.13.13.13.m1.1"><semantics id="A1.T7.13.13.13.13.m1.1a"><mi id="A1.T7.13.13.13.13.m1.1.1" xref="A1.T7.13.13.13.13.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.T7.13.13.13.13.m1.1b"><ci id="A1.T7.13.13.13.13.m1.1.1.cmml" xref="A1.T7.13.13.13.13.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.13.13.13.13.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="A1.T7.13.13.13.13.m1.1d">italic_r</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.14.14.14.14"><math alttext="\tau" class="ltx_Math" display="inline" id="A1.T7.14.14.14.14.m1.1"><semantics id="A1.T7.14.14.14.14.m1.1a"><mi id="A1.T7.14.14.14.14.m1.1.1" xref="A1.T7.14.14.14.14.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.14.14.14.14.m1.1b"><ci id="A1.T7.14.14.14.14.m1.1.1.cmml" xref="A1.T7.14.14.14.14.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.14.14.14.14.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.T7.14.14.14.14.m1.1d">italic_τ</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.15.15.15.15"><math alttext="r" class="ltx_Math" display="inline" id="A1.T7.15.15.15.15.m1.1"><semantics id="A1.T7.15.15.15.15.m1.1a"><mi id="A1.T7.15.15.15.15.m1.1.1" xref="A1.T7.15.15.15.15.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.T7.15.15.15.15.m1.1b"><ci id="A1.T7.15.15.15.15.m1.1.1.cmml" xref="A1.T7.15.15.15.15.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.15.15.15.15.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="A1.T7.15.15.15.15.m1.1d">italic_r</annotation></semantics></math></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="A1.T7.16.16.16.16"><math alttext="\tau" class="ltx_Math" display="inline" id="A1.T7.16.16.16.16.m1.1"><semantics id="A1.T7.16.16.16.16.m1.1a"><mi id="A1.T7.16.16.16.16.m1.1.1" xref="A1.T7.16.16.16.16.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.16.16.16.16.m1.1b"><ci id="A1.T7.16.16.16.16.m1.1.1.cmml" xref="A1.T7.16.16.16.16.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.16.16.16.16.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.T7.16.16.16.16.m1.1d">italic_τ</annotation></semantics></math></th>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.18.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="17" id="A1.T7.16.16.18.2.1"><span class="ltx_text ltx_font_bold" id="A1.T7.16.16.18.2.1.1">OpenChat3.5</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.16.16.19.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.2">0.2048</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.3">0.1613</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.4">0.2157</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.5">0.1556</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.6">0.2932</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.7">0.2153</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.8">0.2305</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.9">0.1956</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.10">0.3094</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.11">0.2135</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.12">0.3246</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.13">0.2251</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.14">0.2180</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.15">0.1746</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.16">0.2179</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.19.1.17">0.1279</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.20.2">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.2">0.2551</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.3">0.2031</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.4">0.2774</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.5">0.2023</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.6">0.5192</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.7">0.2757</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.8">0.3351</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.9">0.2560</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.10">0.4463</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.11">0.2711</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.12">0.4919</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.13">0.2654</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.14">0.3669</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.15">0.2173</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.16">0.3529</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.20.2.17">0.1791</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.21.3">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.2">0.2921</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.3">0.2267</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.4">0.2655</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.5">0.1894</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.6">0.4001</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.7">0.2905</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.8">0.3063</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.9">0.2191</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.10">0.3702</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.11">0.2406</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.12">0.3865</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.13">0.2617</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.14">0.3487</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.15">0.2364</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.16">0.2971</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.21.3.17">0.1938</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.22.4">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.2">0.5474</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.3">0.4107</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.4">0.5966</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.5">0.4418</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.6">0.6776</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.7">0.5249</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.8">0.5753</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.9">0.4341</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.10">0.6617</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.11">0.5043</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.12">0.6679</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.13">0.5213</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.14">0.5937</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.15">0.4452</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.16">0.5610</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.22.4.17">0.4146</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.23.5">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.2">0.3606</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.3">0.2705</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.4">0.4968</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.5">0.3617</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.6">0.6185</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.7">0.4364</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.8">0.5447</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.9">0.3649</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.10">0.6114</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.11">0.4314</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.12">0.6246</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.13">0.4478</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.14">0.5547</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.15">0.3849</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.16">0.4809</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.23.5.17">0.3287</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.24.6">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.2">0.4312</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.3">0.3664</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.4">0.5307</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.5">0.3971</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.6">0.7891</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.7">0.5642</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.8">0.7353</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.9">0.5359</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.10">0.2487</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.11">0.5516</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.12">0.7983</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.13">0.5716</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.14">0.7634</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.15">0.5433</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.16">0.7491</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.24.6.17">0.5097</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.25.7">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.2">0.5893</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.3">0.4714</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.4">0.4760</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.5">0.3630</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.6">0.6655</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.7">0.4761</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.8">0.4957</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.9">0.4290</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.10">0.5926</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.11">0.4183</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.12">0.6643</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.13">0.4630</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.14">0.6016</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.15">0.4009</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.16">0.6413</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.25.7.17">0.4199</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.26.8">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.2">0.4060</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.3">0.3060</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.4">0.4351</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.5">0.3135</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.6">0.6001</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.7">0.4492</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.8">0.4434</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.9">0.3147</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.10">0.5957</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.11">0.4449</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.12">0.5920</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.13">0.4395</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.14">0.5139</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.15">0.3811</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.16">0.4165</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.26.8.17">0.2987</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.27.9">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="17" id="A1.T7.16.16.27.9.1"><span class="ltx_text ltx_font_bold" id="A1.T7.16.16.27.9.1.1">Llama-2-7B</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.28.10">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.2">0.0436</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.3">0.0344</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.4">0.0861</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.5">0.0642</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.6">0.0216</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.7">0.0662</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.8">0.0494</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.9">-0.0110</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.10">0.0132</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.11">0.0693</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.12">-0.0185</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.13">0.0686</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.14">-0.0225</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.15">-0.0259</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.16">-0.0914</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.28.10.17">0.0235</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.29.11">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.2">0.0763</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.3">0.0597</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.4">0.0481</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.5">0.0361</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.6">-0.0484</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.7">0.0931</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.8">0.0271</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.9">0.0122</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.10">0.0127</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.11">0.0647</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.12">0.0447</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.13">0.0686</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.14">-0.0096</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.15">-0.0017</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.16">0.0222</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.29.11.17">0.0056</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.30.12">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.2">0.0818</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.3">0.0623</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.4">0.0478</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.5">0.0361</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.6">0.0938</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.7">0.0712</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.8">-0.0069</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.9">0.0208</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.10">0.0121</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.11">0.0459</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.12">0.0470</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.13">0.0579</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.14">0.0393</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.15">0.0261</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.16">0.0368</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.30.12.17">0.0194</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.31.13">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.2">0.1231</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.3">0.0978</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.4">0.2589</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.5">0.1976</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.6">0.0452</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.7">0.2805</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.8">0.0382</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.9">-0.0301</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.10">0.0593</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.11">0.1894</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.12">0.0574</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.13">0.2418</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.14">0.1224</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.15">0.1065</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.16">-0.0529</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.31.13.17">-0.0023</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.32.14">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.2">0.2156</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.3">0.1680</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.4">0.1729</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.5">0.1308</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.6">0.1288</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.7">0.1670</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.8">0.0617</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.9">0.0344</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.10">0.0164</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.11">0.1503</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.12">0.0182</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.13">0.1939</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.14">0.1348</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.15">0.0995</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.16">0.0311</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.32.14.17">0.0994</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.33.15">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.2">0.2662</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.3">0.2072</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.4">0.2364</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.5">0.1807</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.6">0.0886</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.7">0.3404</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.8">-0.0514</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.9">-0.0068</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.10">0.0571</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.11">0.1772</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.12">0.0030</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.13">0.2622</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.14">0.0718</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.15">0.1358</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.16">-0.0357</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.33.15.17">0.0285</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.34.16">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.2">0.2342</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.3">0.1891</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.4">0.2084</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.5">0.1564</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.6">0.3123</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.7">0.3234</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.8">0.0273</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.9">-0.0040</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.10">0.0270</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.11">0.1632</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.12">0.0030</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.13">0.1759</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.14">0.1879</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.15">0.1182</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.16">0.0531</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.34.16.17">0.1167</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.35.17">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.2">0.1295</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.3">0.1019</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.4">0.0382</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.5">0.0300</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.6">0.1345</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.7">0.1638</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.8">0.0852</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.9">0.0428</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.10">0.1594</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.11">0.1713</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.12">0.1287</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.13">0.1867</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.14">0.1719</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.15">0.1339</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.16">0.0725</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.35.17.17">0.0250</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.36.18">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="17" id="A1.T7.16.16.36.18.1"><span class="ltx_text ltx_font_bold" id="A1.T7.16.16.36.18.1.1">Gemma-7B</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.37.19">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.2">0.1217</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.3">0.0984</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.4">0.1578</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.5">0.1239</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.6">0.1622</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.7">0.1280</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.8">0.0994</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.9">0.0844</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.10">0.1696</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.11">0.1436</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.12">0.1693</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.13">0.1424</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.14">0.0193</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.15">0.0040</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.16">-0.0066</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.37.19.17">0.0244</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.38.20">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.2">0.1745</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.3">0.1428</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.4">0.2018</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.5">0.1599</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.6">0.2114</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.7">0.1189</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.8">0.2576</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.9">0.1256</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.10">0.2634</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.11">0.1747</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.12">0.2024</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.13">0.1468</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.14">0.1379</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.15">0.0662</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.16">0.0456</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.38.20.17">0.1182</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.39.21">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.2">0.2724</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.3">0.2121</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.4">0.2037</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.5">0.1581</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.6">0.1100</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.7">0.1406</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.8">0.2622</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.9">0.2156</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.10">0.2091</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.11">0.1876</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.12">0.2171</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.13">0.1826</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.14">0.1516</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.15">0.1228</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.16">0.0372</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.39.21.17">0.1274</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.40.22">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.2">0.3837</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.3">0.3003</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.4">0.4749</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.5">0.3721</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.6">0.3452</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.7">0.2954</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.8">0.3922</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.9">0.3237</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.10">0.5009</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.11">0.4294</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.12">0.4522</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.13">0.3979</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.14">0.3483</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.15">0.2382</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.16">0.1742</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.40.22.17">0.2927</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.41.23">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.2">0.3794</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.3">0.3002</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.4">0.3242</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.5">0.2595</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.6">0.2635</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.7">0.2244</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.8">0.3204</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.9">0.2658</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.10">0.3677</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.11">0.3191</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.12">0.3399</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.13">0.3111</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.14">0.3051</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.15">0.2425</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.16">0.0448</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.41.23.17">0.2217</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.42.24">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.2">0.5852</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.3">0.4552</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.4">0.4672</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.5">0.3662</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.6">0.4127</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.7">0.3473</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.8">0.6256</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.9">0.4585</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.10">0.5523</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.11">0.4558</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.12">0.5630</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.13">0.4600</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.14">0.5137</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.15">0.3468</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.16">0.1365</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.42.24.17">0.3430</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.43.25">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.2">0.4205</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.3">0.3294</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.4">0.4479</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.5">0.3448</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.6">0.3331</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.7">0.3452</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.8">0.3223</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.9">0.2900</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.10">0.4826</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.11">0.3497</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.12">0.4614</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.13">0.3965</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.14">0.3384</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.15">0.2273</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.16">0.0591</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.43.25.17">0.1829</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.44.26">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.2">0.2902</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.3">0.2298</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.4">0.2876</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.5">0.2245</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.6">0.3705</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.7">0.2737</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.8">0.2831</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.9">0.2162</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.10">0.3879</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.11">0.3104</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.12">0.3740</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.13">0.2862</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.14">0.2883</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.15">0.2086</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.16">0.0476</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.44.26.17">0.2169</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.45.27">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="17" id="A1.T7.16.16.45.27.1"><span class="ltx_text ltx_font_bold" id="A1.T7.16.16.45.27.1.1">Llama-2-13B</span>
</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.46.28">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.2">0.0612</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.3">0.0436</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.4">0.0111</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.5">0.0082</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.6">0.0527</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.7">0.0229</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.8">0.0467</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.9">0.0534</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.10">0.0246</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.11">0.0874</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.12">0.0386</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.13">0.0735</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.14">0.0337</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.15">0.0266</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.16">0.0018</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.46.28.17">0.0487</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.47.29">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.2">-0.0025</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.3">-0.0021</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.4">-0.0324</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.5">-0.0248</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.6">0.026</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.7">0.0501</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.8">-0.0062</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.9">0.0143</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.10">0.0341</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.11">0.0824</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.12">0.0506</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.13">0.0757</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.14">0.0957</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.15">0.0607</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.16">0.0349</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.47.29.17">0.0120</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.48.30">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.2">0.0085</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.3">0.0067</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.4">0.0086</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.5">0.0067</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.6">0.0889</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.7">0.1033</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.8">-0.0201</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.9">0.0593</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.10">0.0496</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.11">0.0744</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.12">0.0486</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.13">0.0699</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.14">0.0455</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.15">0.0521</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.16">0.0657</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.48.30.17">0.0628</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.49.31">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.2">0.2339</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.3">0.1821</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.4">0.1992</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.5">0.1440</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.6">0.3263</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.7">0.3051</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.8">0.0239</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.9">0.1603</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.10">0.0542</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.11">0.2841</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.12">0.088</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.13">0.3112</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.14">0.2104</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.15">0.1875</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.16">-0.031</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.49.31.17">0.1707</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.50.32">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.2">0.0619</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.3">0.0492</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.4">0.3039</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.5">0.2229</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.6">0.2466</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.7">0.2359</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.8">0.0538</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.9">0.0842</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.10">0.0865</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.11">0.0933</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.12">0.038</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.13">0.2045</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.14">0.3043</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.15">0.2448</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.16">0.0385</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.50.32.17">0.1406</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.51.33">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.2">0.3048</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.3">0.2332</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.4">0.2916</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.5">0.2141</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.6">0.4189</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.7">0.3422</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.8">0.0302</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.9">0.1975</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.10">0.0698</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.11">0.2414</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.12">0.0463</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.13">0.3666</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.14">0.3420</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.15">0.2538</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.16">-0.0267</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.51.33.17">0.1936</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.52.34">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.2">0.404</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.3">0.3118</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.4">0.1973</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.5">0.1442</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.6">0.3916</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.7">0.3021</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.8">0.0843</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.9">0.2463</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.10">0.0562</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.11">0.2331</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.12">0.3697</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.13">0.3148</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.14">0.2766</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.15">0.1899</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.16">0.0645</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.52.34.17">0.1493</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.53.35">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.2">0.0295</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.3">0.0212</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.4">0.3414</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.5">0.2501</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.6">0.2160</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.7">0.1992</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.8">0.0726</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.9">0.0342</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.10">-0.063</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.11">0.0731</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.12">0.0705</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.13">0.2622</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.14">0.2478</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.15">0.2116</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.16">0.0827</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.53.35.17">0.1034</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.54.36">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="17" id="A1.T7.16.16.54.36.1"><span class="ltx_text ltx_font_bold" id="A1.T7.16.16.54.36.1.1">Qwen1.5-14B</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.55.37">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.2">0.1555</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.3">0.1219</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.4">0.2543</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.5">0.1975</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.6">0.1504</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.7">0.1625</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.8">0.0717</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.9">0.1173</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.10">0.0143</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.11">0.1652</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.12">0.2114</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.13">0.1746</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.14">0.2089</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.15">0.1777</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.16">0.0254</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.55.37.17">0.1365</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.56.38">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.2">0.2572</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.3">0.2053</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.4">0.3300</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.5">0.2634</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.6">0.4550</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.7">0.2395</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.8">0.0407</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.9">0.1551</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.10">0.0161</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.11">0.2120</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.12">0.4507</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.13">0.2332</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.14">0.4174</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.15">0.2648</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.16">0.1059</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.56.38.17">0.2114</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.57.39">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.2">0.3839</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.3">0.2914</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.4">0.2615</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.5">0.1976</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.6">0.3655</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.7">0.3028</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.8">0.0557</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.9">0.2245</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.10">0.0297</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.11">0.2497</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.12">0.4011</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.13">0.3212</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.14">0.3580</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.15">0.2479</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.16">0.0038</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.57.39.17">0.2154</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.58.40">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.2">0.5134</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.3">0.3867</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.4">0.5896</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.5">0.4510</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.6">0.5923</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.7">0.4796</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.8">0.0769</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.9">0.3477</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.10">-0.0069</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.11">0.4063</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.12">0.1954</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.13">0.4871</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.14">0.5215</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.15">0.4336</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.16">0.0487</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.58.40.17">0.3170</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.59.41">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.2">0.3265</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.3">0.2376</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.4">0.5208</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.5">0.3857</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.6">0.5149</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.7">0.3741</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.8">0.1401</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.9">0.2391</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.10">0.1960</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.11">0.3282</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.12">0.4868</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.13">0.3338</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.14">0.4712</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.15">0.3578</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.16">0.0484</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.59.41.17">0.2395</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.60.42">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.2">0.5609</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.3">0.4387</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.4">0.5621</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.5">0.4348</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.6">0.7127</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.7">0.5527</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.8">0.0985</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.9">0.4176</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.10">0.0466</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.11">0.4385</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.12">0.7293</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.13">0.5345</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.14">0.7369</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.15">0.5479</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.16">0.1085</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.60.42.17">0.3300</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.61.43">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.2">0.5047</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.3">0.3982</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.4">0.4997</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.5">0.3979</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.6">0.6002</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.7">0.4332</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.8">0.1075</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.9">0.3620</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.10">0.1665</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.11">0.4161</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.12">0.6100</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.13">0.4282</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.14">0.5546</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.15">0.3957</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.16">0.3920</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.61.43.17">0.3728</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.62.44">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.1">SI-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.2">0.3693</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.3">0.2669</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.4">0.5096</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.5">0.3773</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.6">0.5820</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.7">0.4366</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.8">0.0609</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.9">0.2002</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.10">0.1472</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.11">0.3012</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.12">0.5249</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.13">0.3997</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.14">0.4340</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.15">0.3537</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.16">0.1646</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.62.44.17">0.3033</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.63.45">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="17" id="A1.T7.16.16.63.45.1"><span class="ltx_text ltx_font_bold" id="A1.T7.16.16.63.45.1.1">Mixtral-8x7B-Instruct</span></td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.64.46">
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.1">EN-DE</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.2">0.1980</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.3">0.1444</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.4">0.1658</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.5">0.1195</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.6">0.0189</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.7">0.1959</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.8">-0.0161</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.9">0.1137</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.10">-0.0713</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.11">0.1388</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.12">0.0404</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.13">0.1709</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.14">-0.0827</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.15">0.1625</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.16">0.2556</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.16.16.64.46.17">0.1631</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.65.47">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.1">EN-MR</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.2">0.1819</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.3">0.1405</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.4">0.2115</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.5">0.1539</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.6">0.0186</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.7">0.1394</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.8">0.0140</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.9">0.0634</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.10">0.0230</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.11">0.1493</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.12">0.0707</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.13">0.1469</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.14">0.0025</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.15">0.1428</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.16">0.0299</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.65.47.17">0.1917</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.66.48">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.1">EN-ZH</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.2">0.3324</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.3">0.2424</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.4">0.2927</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.5">0.2109</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.6">0.0890</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.7">0.2687</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.8">-0.0062</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.9">0.1496</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.10">0.0654</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.11">0.1926</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.12">-0.0061</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.13">0.2474</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.14">0.2443</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.15">0.1813</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.16">0.3079</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.66.48.17">0.2200</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.67.49">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.1">ET-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.2">0.4748</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.3">0.3531</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.4">0.5552</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.5">0.4095</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.6">0.2111</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.7">0.4614</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.8">0.0380</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.9">0.2517</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.10">0.0738</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.11">0.3171</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.12">0.1534</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.13">0.4336</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.14">0.2656</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.15">0.3399</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.16">0.0567</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.67.49.17">0.4052</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.68.50">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.1">NE-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.2">0.3909</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.3">0.2872</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.4">0.4215</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.5">0.3070</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.6">0.1190</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.7">0.3572</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.8">0.0543</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.9">0.0842</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.10">-0.0219</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.11">0.1629</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.12">0.1416</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.13">0.3409</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.14">0.4209</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.15">0.3223</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.16">0.4284</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.68.50.17">0.3824</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.69.51">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.1">RO-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.2">0.5576</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.3">0.4344</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.4">0.5416</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.5">0.4107</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.6">0.0427</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.7">0.5000</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.8">0.0748</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.9">0.3024</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.10">0.0991</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.11">0.3196</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.12">0.0947</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.13">0.4727</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.14">0.6549</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.15">0.4879</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.16">0.2054</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.69.51.17">0.5154</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.70.52">
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.1">RU-EN</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.2">0.4823</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.3">0.3630</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.4">0.4514</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.5">0.3422</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.6">0.1760</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.7">0.4510</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.8">0.0854</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.9">0.3055</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.10">0.1199</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.11">0.3767</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.12">0.1844</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.13">0.4257</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.14">0.5816</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.15">0.3913</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.16">0.0212</td>
<td class="ltx_td ltx_align_center" id="A1.T7.16.16.70.52.17">0.3984</td>
</tr>
<tr class="ltx_tr" id="A1.T7.16.16.71.53">
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.1">SI-EN</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.2">0.3352</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.3">0.2415</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.4">0.4459</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.5">0.3223</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.6">0.0961</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.7">0.3343</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.8">0.0072</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.9">0.0691</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.10">0.0934</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.11">0.1555</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.12">0.1099</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.13">0.2997</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.14">0.4255</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.15">0.3461</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.16">0.4334</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.16.16.71.53.17">0.3441</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Pearson’s <math alttext="r" class="ltx_Math" display="inline" id="A1.T7.19.m1.1"><semantics id="A1.T7.19.m1.1b"><mi id="A1.T7.19.m1.1.1" xref="A1.T7.19.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="A1.T7.19.m1.1c"><ci id="A1.T7.19.m1.1.1.cmml" xref="A1.T7.19.m1.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.19.m1.1d">r</annotation><annotation encoding="application/x-llamapun" id="A1.T7.19.m1.1e">italic_r</annotation></semantics></math> and Kendall’s <math alttext="\tau" class="ltx_Math" display="inline" id="A1.T7.20.m2.1"><semantics id="A1.T7.20.m2.1b"><mi id="A1.T7.20.m2.1.1" xref="A1.T7.20.m2.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.20.m2.1c"><ci id="A1.T7.20.m2.1.1.cmml" xref="A1.T7.20.m2.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.20.m2.1d">\tau</annotation><annotation encoding="application/x-llamapun" id="A1.T7.20.m2.1e">italic_τ</annotation></semantics></math> correlation scores achieved using Templates 1-8 (T1-8) on various
open-source LLMs for each language pair (LP).</figcaption>
</figure>
<div class="ltx_para" id="A1.p1">
<span class="ltx_ERROR undefined" id="A1.p1.1">{CJK*}</span>
<p class="ltx_p" id="A1.p1.2">UTF8gbsn</p>
</div>
<figure class="ltx_figure" id="A1.F6">
<div class="ltx_block ltx_minipage ltx_align_center ltx_align_middle" id="A1.F6.1" style="width:341.4pt;">
<p class="ltx_p" id="A1.F6.1.1"><span class="ltx_text ltx_font_bold" id="A1.F6.1.1.1">Model input</span> (before formatting):</p>
<p class="ltx_p" id="A1.F6.1.2">Score the following translation from English to Chinese with respect to the human reference on a continuous scale from 0 to 100, where score of zero means "no meaning preserved" and score of one hundred means "perfect meaning and grammar".\nEnglish source: The last conquistador then rides on with his sword drawn.\nChinese human reference: 最后的征服者随后举着剑前进。\nChinese translation: 最后的征服者骑着他的剑继续前进.\nScore:</p>
<p class="ltx_p" id="A1.F6.1.3"><span class="ltx_text ltx_font_bold" id="A1.F6.1.3.1">Model output:</span></p>
<p class="ltx_p" id="A1.F6.1.4">&lt;|im_start|&gt;user\nScore the following translation from English to Chinese with respect to the human reference on a continuous scale from 0 to 100, where score of zero means "no meaning preserved" and score of one hundred means "perfect meaning and grammar". \nEnglish source: The last conquistador then rides on with his sword drawn.\nChinese human reference: 最后的征服者随后举着剑前进。\nChinese translation: 最后的征服者骑着他的剑继续前进.\nScore:&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n最后的征服者骑着他的剑继续前进.</p>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>An example of the TowerLLM output for scoring English-to-Chinese translation using Template 3 via HuggingFace. The output was generated in March, 2024.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  9 12:06:09 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
