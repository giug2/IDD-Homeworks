<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles</title>
<!--Generated on Tue Sep 10 12:04:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Large language model (LLM),  Vision Language Model (VLM),  scenario generation,  prompt engineering.
" lang="en" name="keywords"/>
<base href="/html/2409.06450v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S1" title="In Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S2" title="In Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">RELATED WORKS</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S2.SS1" title="In II RELATED WORKS ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Scenario-based testing</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S2.SS2" title="In II RELATED WORKS ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">Scenario Generation with LLMs</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S3" title="In Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S3.SS1" title="In III Method ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Pipeline</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S3.SS2" title="In III Method ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Scenario Prompt Engineering</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S3.SS3" title="In III Method ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">LLM-driven Scenario Generation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S3.SS4" title="In III Method ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">RAG for scenarios</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4" title="In Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experiments</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.SS1" title="In IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Controllable realistic scenarios</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.SS2" title="In IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Controllable challenging scenarios</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.SS3" title="In IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Effectiveness of RAG Module</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.SS4" title="In IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Effectiveness of OmniTester with crash report</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.SS5" title="In IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-E</span> </span><span class="ltx_text ltx_font_italic">Ablation Study</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S5" title="In Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Conclusion and Future work</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#A1" title="In Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Prompt Examples</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qiujing Lu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Automation, Tsinghua University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuanhan Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Automation, Tsinghua University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiwei Jiang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Automation, Tsinghua University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guangming Zhao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Research Institute for Road Safety of the Ministry of Public Security, Beijing, China
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingyue Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Research Institute for Road Safety of the Ministry of Public Security, Beijing, China
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuo Feng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Automation, Tsinghua University
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The generation of corner cases has become increasingly crucial for efficiently testing autonomous vehicles prior to road deployment. However, existing methods struggle to accommodate diverse testing requirements and often lack the ability to generalize to unseen situations, thereby reducing the convenience and usability of the generated scenarios. A method that facilitates easily controllable scenario generation for efficient autonomous vehicles (AV) testing with realistic and challenging situations is greatly needed. To address this, we proposed OmniTester: a multimodal Large Language Model (LLM) based framework that fully leverages the extensive world knowledge and reasoning capabilities of LLMs. OmniTester is designed to generate realistic and diverse scenarios within a simulation environment, offering a robust solution for testing and evaluating AVs. In addition to prompt engineering, we employ tools from Simulation of Urban Mobility to simplify the complexity of codes generated by LLMs. Furthermore, we incorporate Retrieval-Augmented Generation and a self-improvement mechanism to enhance the LLM’s understanding of scenarios, thereby increasing its ability to produce more realistic scenes. In the experiments, we demonstrated the controllability and realism of our approaches in generating three types of challenging and complex scenarios. Additionally, we showcased its effectiveness in reconstructing new scenarios described in crash report, driven by the generalization capability of LLMs.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Large language model (LLM), Vision Language Model (VLM), scenario generation, prompt engineering.

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Scenario-based testing is vital in the development of autonomous driving, as it allows AVs to be tested in specially crafted scenarios inside simulation. This method is essential for evaluating performance, identifying weaknesses, and ensuring safety— key factors in assessing AVs before road testing. However, the scenario libraries currently in use, primarily sourced from real-world data, fall short due to the scarcity of corner cases. To make things worse, as the intelligence level of autonomous driving improves, the rarity of critical events becomes increasingly problematic, exacerbating what is referred to as the ”curse of rarity” <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib1" title="">1</a>]</cite>. The insufficient testing coverage and inefficiencies throughout the testing process is hindering enhancements in AV safety. As such, the efficient generation of testing scenarios is of critical importance. Various techniques has been explored, such as dense deep reinforcement learning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib2" title="">2</a>]</cite> and adversarial framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib3" title="">3</a>]</cite>. Yet, considering the diverse testing requirements and different development stages in AV development, effectively designing and generating suitable scenarios to meet the testing needs remains an unresolved challenge.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Many efforts have been made in this area, primarily focusing on generating challenging scenarios based on predefined functions or well-defined search algorithms. However, there has been limited exploration into developing an effective control mechanism that allows flexible management of scenario generation based on requirement descriptions. This is particularly important because developers often conceptualize scenarios in abstract terms while simulations require precise configurations for execution. For instance, developers might envision turning scenarios, while simulation requires detailed road geometry, precise initial vehicle placements and behaviors for each turn. Enhancing the controllability of the scenario generation system based on abstract descriptions can bridge the gap between developers and scenario-based testing, making it a more viable tool and expediting the path to efficient performance evaluation for AV systems.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">However, building such text-conditioned generation mechanism is challenging as it demands modeling capacity ranging from static elements in road structures to agent behaviors and map between narrative language to detailed configurations. Additionally, it must ensure the conformity to user requests, maintain he diversity of the road network, and preserve the fidelity of the generated scenarios. Such generation process requires a high level of intelligence, as it involves not only understanding the control signal but also reasoning from the request to design then generate the desired testing scenarios. The emergence of LLMs and Vision-Language Models (VLM), trained with vast amounts of data from the internet, has demonstrated remarkable intelligence, encompassing learning, reasoning, linguistic capabilities, human-like communication, and complex thinking abilities. There have been extensive explorations into their applications in fields such as medicine, education, finance, and engineering. Notably, OpenAI’s CodeX <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib4" title="">4</a>]</cite> and DeepMind’s AlphaCode <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib5" title="">5</a>]</cite> has demonstrated promising coding capabilities, MathPrompter <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib6" title="">6</a>]</cite> has shown advancements in mathematical reasoning, and SceneCraft <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib7" title="">7</a>]</cite> has excelled in creating narrative experiences.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Motivated by these impressive advancements in large language models (LLMs), this paper aims to explore how the coding, narrative, and reasoning capacities of LLMs, along with the extensive knowledge acquired from the internet, can be harnessed to bridge the gap between developers and the simulation systems. Building such a simulation tool with a robust text-conditioned mechanism presents several challenges: Firstly, the realistic and accurate generation of road networks and vehicle movements from text is inherently complex and demanding. Unlike generating vivid images or videos from text, as seen with SORA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib8" title="">8</a>]</cite> and Kling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib9" title="">9</a>]</cite>, this task demands a higher degree of accuracy beyond mere visual realism. The challenge lies in generating domain-specific, complex topological structures that adhere to precise spatial relationships. Secondly, hallucinations and errors inherent in LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib10" title="">10</a>]</cite> can lead to failures in generating reasonable road networks or meaningful vehicle dynamics, especially in less common testing scenarios. Ensuring precise control based on text input remains a significant challenge and requires further research. Thirdly, given that LLMs are trained on general corpora, a significant challenge is how to make LLMs quickly understand generation requests for new scenarios and produce specific outputs while continuously learning with minimal supplied information.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these challenges and fully harness the inherent intelligent capabilities of LLMs, we introduced OmniTester as shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">1</span></a>, which proposes a fully automated pipeline that responds to user requests, effectively generating the desired scenarios to test the targeted functionality of autonomous vehicles. OmniTester generates realistic and diverse scenarios containing road geometries that closely mimic real-world environments through prompt engineering and the integration of tools from Simulation of Urban Mobility(SUMO), an open-source traffic simulation package <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib11" title="">11</a>]</cite>. Meanwhile, OmniTester uses self-improvement mechanisms designed to continuously enhance LLM performance, reducing hallucination and inherent errors. It also uses a Retrieval-Augmented Generation (RAG)-based generation mechanism <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib12" title="">12</a>]</cite> that automatically queries and extracts external knowledge to adapt to the operational design domain requirements. To the best of our knowledge, OmniTester is the first system to generate road structures and vehicles solely based on user requests, offering exceptional controllability and flexibility, powered by RAG and self-improvement techniques.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The rest of this paper is organized as follows: related works on scenario generation and the application of LLMs in the autonomous driving domain is reviewed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S2" title="II RELATED WORKS ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">II</span></a>. Section <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S3" title="III Method ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">III</span></a> details the design and implementation of OmniTester, with a focus on the prompt engineering techniques and RAG implementation. In the section <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4" title="IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">IV</span></a>, we showcase the various scenarios generated by OmniTester, and evaluate the effectiveness of the system’s key components. Additionally, we include a case study featuring scenario generation from a crash report as an application of our system. Finally, conclusions are presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S5" title="V Conclusion and Future work ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">V</span></a>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="157" id="S1.F1.g1" src="extracted/5841084/fig/LLM_overview2.jpg" width="240"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The LLM generation framework of OmniTester</figcaption>
</figure>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="309" id="S1.F2.g1" src="extracted/5841084/fig/LLMGenerationframework2.png" width="568"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Dataflow within OmniTester: The Interpreter, RAG module, Net Generator, Vehicle Generator and LLM Evaluator are activated upon the user’s request.</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">RELATED WORKS</span>
</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.4.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.5.2">Scenario-based testing</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Safety is the major factor holding back the widespread deployment of AVs, extensive efforts have been made to identify and address unsafe components through rigorous testing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib15" title="">15</a>]</cite>. Compared to road testing, scenario-based testing aims to offer more efficient, targeted assessments with better coverage of corner cases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib16" title="">16</a>]</cite>. This approach tests AVs in specially crafted scenarios that highlight safety-critical situations. However, designing and generating these scenarios remains challenging, and various methods have been developed to address this. One straightforward method is replaying the logged behavior of all vehicles in the scene, which ensures realistic behavior for each agent. Nevertheless, this method has limitations because the entire environment cannot adapt to the new behaviors of AVs, and it is restricted to existing collected scenarios, resulting in unreliable evaluation outcomes with limited coverage.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Various methods have been explored for generating novel challenging scenarios for testing, such as combination-based, worst-case, adaptive scenario generation. The combination-based scenario generation approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib17" title="">17</a>]</cite> decomposes scenarios into several basic scenario units and constructs complex scenarios by permuting and combining these basic units. The worst-case scenario generation approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib18" title="">18</a>]</cite> uses the human steering angle as a design variable, optimizing it to produce the most challenging scenarios for AVs. The adaptive scenario generation approach <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib19" title="">19</a>]</cite> calibrates the AV surrogate model adaptively to explore and determine the model’s performance boundaries, generating representative scenarios for safety evaluation. However, these methods can only provide a limited range of scenarios and fail to model the agents within these scenarios realistically. As a result, they do not capture the full complexity of real-world situations.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Data-driven models have also been explored for scenario building, where they are used to model key components of the scenario, ranging from agent behaviors to the road network within the simulation. For instance, NeuralNDE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib20" title="">20</a>]</cite> uses a Transformer-based network with safety mapping to provide realistic agent behaviors, achieving distributional-level similarity to real-world distributions. STRVE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib21" title="">21</a>]</cite> learns a graph-based conditional VAE as traffic prior, optimizing each agent’s behavior to provoke collisions with a rule-based AV planner. SLEDGE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib22" title="">22</a>]</cite> employs Diffusion Transformers to jointly generate lanes and agents, serving as the initial state for traffic simulation. RealGen <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib23" title="">23</a>]</cite> uses an encoder-decoder architecture and retrieval-based in-context learning to synthesize realistic traffic scenarios. However, these models primarily focus on generating realistic scenarios based on provided datasets, the challenge of synthesizing novel, challenging situations and effectively controlling the generated context for specific testing purposes remains unsolved.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.4.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.5.2">Scenario Generation with LLMs</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Multimodal large language models have been utilized in multiple aspects of autonomous driving systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib24" title="">24</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib25" title="">25</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib26" title="">26</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib27" title="">27</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib28" title="">28</a>]</cite> explored the application of LLM (GPT3.5/4) and VLM (GPT4) on the autonomous driving systems. These works seek to leverage the reasoning, interpretive, and memorization capabilities of large pretrained models to comprehend the driving environment in a human-like manner and to make informed driving decisions when confronted with complex scenarios. Similarly, multimodal large language models have been utilized for scenario generation. ChatScene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib29" title="">29</a>]</cite> uses LLM to query a given database written in Scenic code <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib30" title="">30</a>]</cite> for generating scenarios based on user queries. By utilizing the reasoning and understanding capabilities of LLMs, it achieves fine performance. However, the Scenic code snippets that contribute to the main scene setup are drawn from predefined library, limiting its generalization ability and effectiveness in generating new and challenging scenes. ChatSim <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib31" title="">31</a>]</cite> utilizes LLMs to collaboratively edit and generate photorealistic 3D driving scene simulations through natural language commands, enabling efficient and interactive scene modifications. However, it limits modifications to existing scenes, lacking the capability to generate entirely new scenes from scratch.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">LEADE <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib32" title="">32</a>]</cite> proposed a scenario generation approach that utilizes LLM-enhanced adaptive evolutionary search to create safety-critical and diverse test scenarios for autonomous driving systems testing. However, it fails to model safety-violation scenarios caused by anomalous actions from agents. CTG++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib33" title="">33</a>]</cite> introduces the use of the LLM to transform a user’s query about safety-critical scenarios into the corresponding differentiable loss function of a diffusion model to generate the query-compliant trajectories. However, it can only manipulate agent behaviors within a given road map with specified initial locations. LLMScenario <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib34" title="">34</a>]</cite> utilizes LLM to generate short trajectories for agents based on minimal scenario descriptions, facilitating scenario engineering. However, its application is currently limited to highway scenarios, and further exploration is needed to extend its use to more complex environments.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">As a significant and evolving area of research, the challenge of enhancing controllability and generalization in generating complex situations remain inadequately addressed. Our work aims to address this challenge by generating road structures and vehicles from scratch with a multimodal LLM-driven, text-conditioned pipeline.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Method</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In this section, we delineate the OmniTester system, the multimodal LLM-driven tool designed for text-conditioned scenario generation. Within this system, we introduce several techniques to enhance LLM’s understanding and generation capabilities for scenarios. We begin by introducing the overall generation framework, followed by an explanation of the prompt engineering techniques utilized for scenario generation. Subsequently, we provide an in-depth explanation of the Net Generator and Vehicle Generator components. Finally, we introduce the RAG mechanism.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.4.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.5.2">Pipeline</span>
</h3>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="192" id="S3.F3.g1" src="extracted/5841084/fig/NetGeneratorv2.png" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>
Generate a road XML file through a two-step process: first, use a properly prompted LLM to directly generate the node and edge files, then use SUMO to convert these into the corresponding net file, all in XML format. </figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">The generation framework is illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S1.F2" title="Figure 2 ‣ I Introduction ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">2</span></a>. The process of creating a set of scenarios starts when the user submits specific testing requests, such as ”Generate 5 scenarios with a fork.” Then an LLM is invoked to elaborate on the scenario details through descriptive means, such as ”the main road splits into two at a slight angle.. ” for the 5 scenarios requested. Once the scenario descriptions are complete, a Net Generator, powered by the LLM, constructs a corresponding road network to match the outlined scenarios. After the road network is successfully generated, another LLM agent is used to configure the vehicle dynamics involved in the scenarios.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Subsequently, an evaluator based on LLM is deployed to assess whether the scenario generated aligns with the user’s specified testing intention, focusing on background vehicle (BV) behaviors. At the same time, a RAG module can be optionally activated to refine the LLM’s comprehension of the intended road category. In instances where the behavior of the AV or BV does not meet the desired criteria, feedback is directly incorporated into the prompt. This prompts the LLM to regenerate the scenario, incorporating the evaluator’s reasoning in the revised output. This iterative process ensures that the generated scenarios are both accurate and consistent with the user’s initial testing objectives.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.4.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.5.2">Scenario Prompt Engineering</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The LLM’s ability to comprehend the scenarios both geometrically and programmatically, will greatly impact the controllability and realism of the generated scenarios. We utilized various prompt techniques to integrate domain knowledge, generate more structured outputs, and maximize the reasoning capabilities of LLMs, while minimizing the hallucination. Chain-of-Thought (CoT) prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib35" title="">35</a>]</cite> motivates the LLM to articulate its reasoning process before providing the final answer. We designed specific prompts to break down the generation task into a series of subproblems, leading the model to tackle these sequentially to achieve the ultimate solution. For example, in network generation, we utilize a multi-step process to aid the model in understanding user requests. Initially, we direct the model to summarize the task in the “Description” section and then guide it to ”step by step explain your reasoning process” in the “Reasoning” section. Refer to the prompt example in Appendix 1. This method not only helps the LLM to better comprehend and analyze the testing requirements but also encourages rational thought with minimal hallucination. For further evidence of the importance of this component, see the ablation study in section <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.SS5" title="IV-E Ablation Study ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">IV-E</span></span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Secondly, we specify the output requirements for the LLM. We instruct the LLM to generate the correct format of node and edge files by providing detailed formatting guidelines. Additionally, we impose additional constraints on road length and design rules to enhance the realism of the output network.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Considering the complexity of output, the varying number of vehicles, and the heterogeneous information (such as edge and time information), we use the few-shot prompting technique <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib36" title="">36</a>]</cite> to help the model understand the context and generate the desired format of the response with a higher success rate. Please refer to the Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#A1" title="Appendix A Prompt Examples ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">A</span></a> for detailed prompt samples.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.4.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.5.2">LLM-driven Scenario Generation</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">We decompose scenario generation into two stages: road network generation and agents’ routes generation. Road structure is important since testing on accurate and varied road structures ensures that AVs comply with road safety regulations and can respond appropriately to diverse road conditions. In our implementation for road generation, a properly prompted LLM-based Interpreter is employed to generate a detailed description for the whole scenario, then SUMO compatible Node and Edge file defined in XML format is generated based on the description.. Lastly, SUMO tool is utilized to convert them into single net file defined in XML format, representing the entire road network. See Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S3.F3" title="Figure 3 ‣ III-A Pipeline ‣ III Method ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">3</span></a> for an illustration and more detailed examples in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4" title="IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">IV</span></a>. This design is not confined to SUMO or its XML formats. Since road network naturally represents a graph structure, it can be represented by other structured languages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib37" title="">37</a>]</cite> and processed by graph tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib38" title="">38</a>]</cite>, compatible to other simulators such as MATSim <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib39" title="">39</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Considering close interactions with other vehicles are among the most challenging scenarios for AVs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib40" title="">40</a>]</cite>, we utilize LLM to create such scenarios by generating specific routes for the vehicles involved. The intelligence of LLMs is leveraged to set up suitable spatial and temporal positions of vehicles and AVs within a given scenario. Specifically, LLM is called with specially designed prompts to generate trips for the desired number of vehicles as described by the interpreter. This generator is tailored to generate AV and BV’s departing and arriving edges as well as departure time, according to the test goal and the road structure. Once these parameters are established, detailed routes, including all the edges the vehicle will pass, are computed using the duarouter tool <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib41" title="">41</a>]</cite>, which optimizes the routes based on the road network and traffic conditions.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">An additional LLM is used to efficiently correct unreachable routes initially generated by the vehicle generator. Through joint debugging with the Network Conversion tool netconvert <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib42" title="">42</a>]</cite>, the error messages provided by netconvert offer detailed instructions on what went wrong and what can be corrected. Based on the detailed feedback, the LLM-based generator updates the trips generated for vehicles. See Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.F4" title="Figure 4 ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">4</span></a> for the detailed pipeline applied. With the help of this self-improvement mechanism, all the routes can be configured successfully.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.4.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.5.2">RAG for scenarios</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">The process begins with the extraction of road geometry from OpenStreetMap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib43" title="">43</a>]</cite>, a collaborative project that provides freely accessible, editable geographic data globally. This platform is utilized to obtain detailed and up-to-date road geometries of specified regions of interest, serving as the foundational dataset for providing open-world knowledge about road structures. The extracted road geometries are then converted into a network model using netconvert. This model is represented in the net.xml format, which is compatible with SUMO.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">Following the network model creation, a bird’s-eye view (BEV) image of the traffic scenario is generated using the graphical user interface of SUMO (sumo-gui). This image captures the layout and other relevant road network attributes. The BEV image serves as an input to an LLM, which processes the visual information and generates descriptive text detailing the road geometry and network characteristics. This step is critical as it bridges the gap between visual data and textual representation, facilitating easier interpretation and further processing of network characteristics in textual form. Once each textual description generated by the LLM, it is transformed into vector embeddings using OpenAI’s embedding tools, text-embedding-ada-002 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib44" title="">44</a>]</cite>. It converts the text into a high-dimensional space that captures the semantic features of the text numerically. These embeddings are then stored in a database, based on Chroma DB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib45" title="">45</a>]</cite>, enabling efficient retrieval and comparison based on semantic similarity.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">During the retrieval phase, the description of the target scenario is converted into an embedding using the same embedding tool. This embedding serves as the basis for identifying the most semantically similar road geometry stored in the database, achieved through comparison of embeddings. This method of similarity assessment ensures that the selected road geometry closely aligns with the specific requirements of the target scenario. See Section <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4" title="IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">IV</span></a> for more details.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1">Once the appropriate road geometry is identified, the corresponding node and edge files are utilized as additional inputs (prompts) in the scenario generation process. This approach leverages in-context learning, where the scenario generator adjusts and refines its output by incorporating the specific contextual details provided by the node and edge files. This integration enhances the accuracy and relevance of the generated traffic scenarios, tailored to the particular characteristics of the road network.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experiments</span>
</h2>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="258" id="S4.F4.g1" src="extracted/5841084/fig/llm_self_improvement.jpg" width="275"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Self-improvement feedback loops for route generation. </figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="434" id="S4.F5.g1" src="extracted/5841084/fig/reality_roadnet.jpg" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Sampled realistic intersections generated by OmniTester are presented. Left: Description generated by Interpreter; Middle: SUMO visualized road structure from the net XML file; Right: Similar roads found in the real world. </figcaption>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we demonstrate the effectiveness of our Multimodal LLM framework for scenario generation. Firstly, we show that the system can generate diverse scenarios to cover various situations while maintaining high realism, based on user’s testing requests. Secondly, we highlight the difficulty of these scenarios by testing an LLM-driven AV within them. Thirdly, we showcase the effectiveness of RAG module. To demonstrate our system’s controllability and generalization capabilities in generating scenarios with more detailed descriptions, we introduce a case study where our OmniTester generates similar risky situations purely based on text descriptions from crash reports. Lastly, we validate the importance of the key design elements in this framework through a thorough ablation study.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.4.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.5.2">Controllable realistic scenarios</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Controllable scenario generation requires high conformality to user requests and realistic outputs. In our experiments, we observed that with proper prompting, the LLM demonstrates strong intelligence and high performance in this task. One detailed example of a generated road network, along with comprehensive descriptions provided by the interpreter, can be seen in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S3.F3" title="Figure 3 ‣ III-A Pipeline ‣ III Method ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">3</span></a>. The interpreter successfully produces detailed descriptions, including the intersection type (T-shape), road layout, and number of lanes for each direction, as well as the road name and traffic conditions for the roads. Based on this description, Net Generator intelligently places nodes to match the details, and generates edges defined on top of these nodes with lanes and connections that align with the description. As illustrated, it not only connects the nodes correctly to create a layout that matches the description but also generates comments describing the edges’ direction and corresponding names for human readability.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">In evaluating the performance of OmniTester systematically, we first measure the controllability of the whole system, which can be divided into two logical levels, road structure and agents related descriptions. We evaluated how the generated results are matching the user request and interpreter’s descriptions from these two perspectives. Several accuracy metrics are computed for a quantitative evaluation of the alignment between requests and generated results.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The accuracy of the scene type evaluates whether the generated network’s scene category aligns with the user’s request or the detailed scene description provided by the interpreter. The number of lanes/vehicles measures whether the specified count in the description matches the generated result.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">We strictly count one generation pass as failure if the output does not follow required output format or generated net cannot be recognized by SUMO. Success rate is averaged over number of scenarios. Three different testing requests, each with 10 generate scenarios, are compared: general road scenarios, intersections, and fork scenarios. As seen from Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.T1" title="TABLE I ‣ IV-A Controllable realistic scenarios ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">I</span></a>, OmniTester has a high success rate in generating accurate types of road networks. Additionally, it achieves 100% accuracy in generating the desired number of vehicles according to the interpreter’s description for most cases. However, some confusion regarding the exact number of lanes for a specified road shown up when generating road networks, which might be due to inherent hallucinations in the language model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib46" title="">46</a>]</cite>. Moreover, the generation failures are mainly due to incorrect formats in the keywords (like extra ”#” tags or additional ”:”), leading to the inability to parse the node and edge files from the long text response. Considering the complexity of generating these files, the success rate for a single-pass generation is reasonable high, and correct outputs often appear through regenerations.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Next, we calculate the mean and standard deviation of the number of total lanes, edges, route length as well as number of vehicles in the generated scenario to measure the complexity of generated scenarios. As seen from Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.T2" title="TABLE II ‣ IV-A Controllable realistic scenarios ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">II</span></a>, these metrics span a wide range across different types of roads, indicating diverse scenario generation. Furthermore, compared to fork scenarios, intersections exhibit greater variations in route length and the number of lanes, which is reasonable given the complexity and diverse types of intersections. A detailed scatter plot for edges and total length can also be seen in the left figure of Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.F9" title="Figure 9 ‣ IV-C Effectiveness of RAG Module ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">9</span></a>, illustrating that they span a wide range and cover diverse situations.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">Some sampled results are visualized in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.F5" title="Figure 5 ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">5</span></a>. For the request of generating intersection scenarios, LLM produces typical Y-shape (top), four-way (Middle) and T-shape (Bottom) intersections. As shown, Interpreter not only generates the layout clearly, from the general direction to main segments, but also provides very detailed information such as pedestrian facility, curb ramps and sight distances so on. In the case of the Y-shape intersection, the generated description includes critical features, specifically noting the ”split at an acute angle” and providing turn radii information. This level of detail ensures that the physical layout and functional aspects are accurately represented. For the four-way intersection, the interpreter describes the moving directions of the intersecting roads with precision, ensuring that the layout is practical and aligns with typical four-way intersection configurations. This includes information on lane assignments and possible traffic flows, which are essential for realistic scenario generation. Regarding the T-shape intersection, the interpreter covers traffic conditions and splitting directions comprehensively. It describes how the traffic flows from the main road into the side road and vice versa, including details about signal controls. The final net xml outputs from SUMO in these cases match the description from the layout to the number of lanes in each segment. As shown, OmniTester is capable of generating detailed narratives for intersections with high realism and can also produce road structures on demand.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Conformity of command</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1">Accuracy</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2">General Scenarios</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.3">Intersection</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.1.1.4">Fork</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.2.1">Scene Type</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.3">0.9</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.2.2.4">1</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.3.3.1">Number of lanes</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.3.3.4">0.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.4">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.4.4.1">Number of vehicles</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.4.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.4.3">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.1.4.4.4">0.9</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.5.5.1">Success rate</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.5.5.2">0.7</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.5.5.3">0.8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.5.5.4">0.8</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Diversityof generated scenarios</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.12">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.12.13.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.12.13.1.1">Scenario</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.12.13.1.2">General Scenarios</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.12.13.1.3">Intersection</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T2.12.13.1.4">Fork</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.3.3.4">#Lanes</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.1"><math alttext="19.3\pm 8.93" class="ltx_Math" display="inline" id="S4.T2.1.1.1.m1.1"><semantics id="S4.T2.1.1.1.m1.1a"><mrow id="S4.T2.1.1.1.m1.1.1" xref="S4.T2.1.1.1.m1.1.1.cmml"><mn id="S4.T2.1.1.1.m1.1.1.2" xref="S4.T2.1.1.1.m1.1.1.2.cmml">19.3</mn><mo id="S4.T2.1.1.1.m1.1.1.1" xref="S4.T2.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S4.T2.1.1.1.m1.1.1.3" xref="S4.T2.1.1.1.m1.1.1.3.cmml">8.93</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.1.1.1.m1.1b"><apply id="S4.T2.1.1.1.m1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.1.1.1.m1.1.1.1.cmml" xref="S4.T2.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.1.1.1.m1.1.1.2.cmml" type="float" xref="S4.T2.1.1.1.m1.1.1.2">19.3</cn><cn id="S4.T2.1.1.1.m1.1.1.3.cmml" type="float" xref="S4.T2.1.1.1.m1.1.1.3">8.93</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.1.1.1.m1.1c">19.3\pm 8.93</annotation><annotation encoding="application/x-llamapun" id="S4.T2.1.1.1.m1.1d">19.3 ± 8.93</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.2.2.2"><math alttext="23.50\pm 8.91" class="ltx_Math" display="inline" id="S4.T2.2.2.2.m1.1"><semantics id="S4.T2.2.2.2.m1.1a"><mrow id="S4.T2.2.2.2.m1.1.1" xref="S4.T2.2.2.2.m1.1.1.cmml"><mn id="S4.T2.2.2.2.m1.1.1.2" xref="S4.T2.2.2.2.m1.1.1.2.cmml">23.50</mn><mo id="S4.T2.2.2.2.m1.1.1.1" xref="S4.T2.2.2.2.m1.1.1.1.cmml">±</mo><mn id="S4.T2.2.2.2.m1.1.1.3" xref="S4.T2.2.2.2.m1.1.1.3.cmml">8.91</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.2.2.2.m1.1b"><apply id="S4.T2.2.2.2.m1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1"><csymbol cd="latexml" id="S4.T2.2.2.2.m1.1.1.1.cmml" xref="S4.T2.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.2.2.2.m1.1.1.2.cmml" type="float" xref="S4.T2.2.2.2.m1.1.1.2">23.50</cn><cn id="S4.T2.2.2.2.m1.1.1.3.cmml" type="float" xref="S4.T2.2.2.2.m1.1.1.3">8.91</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.2.2.2.m1.1c">23.50\pm 8.91</annotation><annotation encoding="application/x-llamapun" id="S4.T2.2.2.2.m1.1d">23.50 ± 8.91</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.3.3.3"><math alttext="10.50\pm 5.25" class="ltx_Math" display="inline" id="S4.T2.3.3.3.m1.1"><semantics id="S4.T2.3.3.3.m1.1a"><mrow id="S4.T2.3.3.3.m1.1.1" xref="S4.T2.3.3.3.m1.1.1.cmml"><mn id="S4.T2.3.3.3.m1.1.1.2" xref="S4.T2.3.3.3.m1.1.1.2.cmml">10.50</mn><mo id="S4.T2.3.3.3.m1.1.1.1" xref="S4.T2.3.3.3.m1.1.1.1.cmml">±</mo><mn id="S4.T2.3.3.3.m1.1.1.3" xref="S4.T2.3.3.3.m1.1.1.3.cmml">5.25</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.3.3.3.m1.1b"><apply id="S4.T2.3.3.3.m1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1"><csymbol cd="latexml" id="S4.T2.3.3.3.m1.1.1.1.cmml" xref="S4.T2.3.3.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.3.3.3.m1.1.1.2.cmml" type="float" xref="S4.T2.3.3.3.m1.1.1.2">10.50</cn><cn id="S4.T2.3.3.3.m1.1.1.3.cmml" type="float" xref="S4.T2.3.3.3.m1.1.1.3">5.25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.3.3.3.m1.1c">10.50\pm 5.25</annotation><annotation encoding="application/x-llamapun" id="S4.T2.3.3.3.m1.1d">10.50 ± 5.25</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.6.6.4">#Edges</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.4.4.1"><math alttext="10.80\pm 2.75" class="ltx_Math" display="inline" id="S4.T2.4.4.1.m1.1"><semantics id="S4.T2.4.4.1.m1.1a"><mrow id="S4.T2.4.4.1.m1.1.1" xref="S4.T2.4.4.1.m1.1.1.cmml"><mn id="S4.T2.4.4.1.m1.1.1.2" xref="S4.T2.4.4.1.m1.1.1.2.cmml">10.80</mn><mo id="S4.T2.4.4.1.m1.1.1.1" xref="S4.T2.4.4.1.m1.1.1.1.cmml">±</mo><mn id="S4.T2.4.4.1.m1.1.1.3" xref="S4.T2.4.4.1.m1.1.1.3.cmml">2.75</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.4.4.1.m1.1b"><apply id="S4.T2.4.4.1.m1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.4.4.1.m1.1.1.1.cmml" xref="S4.T2.4.4.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.4.4.1.m1.1.1.2.cmml" type="float" xref="S4.T2.4.4.1.m1.1.1.2">10.80</cn><cn id="S4.T2.4.4.1.m1.1.1.3.cmml" type="float" xref="S4.T2.4.4.1.m1.1.1.3">2.75</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.4.4.1.m1.1c">10.80\pm 2.75</annotation><annotation encoding="application/x-llamapun" id="S4.T2.4.4.1.m1.1d">10.80 ± 2.75</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.5.5.2"><math alttext="12.0\pm 4.71" class="ltx_Math" display="inline" id="S4.T2.5.5.2.m1.1"><semantics id="S4.T2.5.5.2.m1.1a"><mrow id="S4.T2.5.5.2.m1.1.1" xref="S4.T2.5.5.2.m1.1.1.cmml"><mn id="S4.T2.5.5.2.m1.1.1.2" xref="S4.T2.5.5.2.m1.1.1.2.cmml">12.0</mn><mo id="S4.T2.5.5.2.m1.1.1.1" xref="S4.T2.5.5.2.m1.1.1.1.cmml">±</mo><mn id="S4.T2.5.5.2.m1.1.1.3" xref="S4.T2.5.5.2.m1.1.1.3.cmml">4.71</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.5.5.2.m1.1b"><apply id="S4.T2.5.5.2.m1.1.1.cmml" xref="S4.T2.5.5.2.m1.1.1"><csymbol cd="latexml" id="S4.T2.5.5.2.m1.1.1.1.cmml" xref="S4.T2.5.5.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.5.5.2.m1.1.1.2.cmml" type="float" xref="S4.T2.5.5.2.m1.1.1.2">12.0</cn><cn id="S4.T2.5.5.2.m1.1.1.3.cmml" type="float" xref="S4.T2.5.5.2.m1.1.1.3">4.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.5.5.2.m1.1c">12.0\pm 4.71</annotation><annotation encoding="application/x-llamapun" id="S4.T2.5.5.2.m1.1d">12.0 ± 4.71</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.6.6.3"><math alttext="6.60\pm 2.80" class="ltx_Math" display="inline" id="S4.T2.6.6.3.m1.1"><semantics id="S4.T2.6.6.3.m1.1a"><mrow id="S4.T2.6.6.3.m1.1.1" xref="S4.T2.6.6.3.m1.1.1.cmml"><mn id="S4.T2.6.6.3.m1.1.1.2" xref="S4.T2.6.6.3.m1.1.1.2.cmml">6.60</mn><mo id="S4.T2.6.6.3.m1.1.1.1" xref="S4.T2.6.6.3.m1.1.1.1.cmml">±</mo><mn id="S4.T2.6.6.3.m1.1.1.3" xref="S4.T2.6.6.3.m1.1.1.3.cmml">2.80</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.6.6.3.m1.1b"><apply id="S4.T2.6.6.3.m1.1.1.cmml" xref="S4.T2.6.6.3.m1.1.1"><csymbol cd="latexml" id="S4.T2.6.6.3.m1.1.1.1.cmml" xref="S4.T2.6.6.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.6.6.3.m1.1.1.2.cmml" type="float" xref="S4.T2.6.6.3.m1.1.1.2">6.60</cn><cn id="S4.T2.6.6.3.m1.1.1.3.cmml" type="float" xref="S4.T2.6.6.3.m1.1.1.3">2.80</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.6.6.3.m1.1c">6.60\pm 2.80</annotation><annotation encoding="application/x-llamapun" id="S4.T2.6.6.3.m1.1d">6.60 ± 2.80</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.9.9">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.9.9.4">Route Length</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.7.7.1"><math alttext="335.98\pm 132.22" class="ltx_Math" display="inline" id="S4.T2.7.7.1.m1.1"><semantics id="S4.T2.7.7.1.m1.1a"><mrow id="S4.T2.7.7.1.m1.1.1" xref="S4.T2.7.7.1.m1.1.1.cmml"><mn id="S4.T2.7.7.1.m1.1.1.2" xref="S4.T2.7.7.1.m1.1.1.2.cmml">335.98</mn><mo id="S4.T2.7.7.1.m1.1.1.1" xref="S4.T2.7.7.1.m1.1.1.1.cmml">±</mo><mn id="S4.T2.7.7.1.m1.1.1.3" xref="S4.T2.7.7.1.m1.1.1.3.cmml">132.22</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.7.7.1.m1.1b"><apply id="S4.T2.7.7.1.m1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.7.7.1.m1.1.1.1.cmml" xref="S4.T2.7.7.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.7.7.1.m1.1.1.2.cmml" type="float" xref="S4.T2.7.7.1.m1.1.1.2">335.98</cn><cn id="S4.T2.7.7.1.m1.1.1.3.cmml" type="float" xref="S4.T2.7.7.1.m1.1.1.3">132.22</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.7.7.1.m1.1c">335.98\pm 132.22</annotation><annotation encoding="application/x-llamapun" id="S4.T2.7.7.1.m1.1d">335.98 ± 132.22</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.8.8.2"><math alttext="386.98\pm 162.2" class="ltx_Math" display="inline" id="S4.T2.8.8.2.m1.1"><semantics id="S4.T2.8.8.2.m1.1a"><mrow id="S4.T2.8.8.2.m1.1.1" xref="S4.T2.8.8.2.m1.1.1.cmml"><mn id="S4.T2.8.8.2.m1.1.1.2" xref="S4.T2.8.8.2.m1.1.1.2.cmml">386.98</mn><mo id="S4.T2.8.8.2.m1.1.1.1" xref="S4.T2.8.8.2.m1.1.1.1.cmml">±</mo><mn id="S4.T2.8.8.2.m1.1.1.3" xref="S4.T2.8.8.2.m1.1.1.3.cmml">162.2</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.8.8.2.m1.1b"><apply id="S4.T2.8.8.2.m1.1.1.cmml" xref="S4.T2.8.8.2.m1.1.1"><csymbol cd="latexml" id="S4.T2.8.8.2.m1.1.1.1.cmml" xref="S4.T2.8.8.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.8.8.2.m1.1.1.2.cmml" type="float" xref="S4.T2.8.8.2.m1.1.1.2">386.98</cn><cn id="S4.T2.8.8.2.m1.1.1.3.cmml" type="float" xref="S4.T2.8.8.2.m1.1.1.3">162.2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.8.8.2.m1.1c">386.98\pm 162.2</annotation><annotation encoding="application/x-llamapun" id="S4.T2.8.8.2.m1.1d">386.98 ± 162.2</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.9.9.3"><math alttext="297.03\pm 142" class="ltx_Math" display="inline" id="S4.T2.9.9.3.m1.1"><semantics id="S4.T2.9.9.3.m1.1a"><mrow id="S4.T2.9.9.3.m1.1.1" xref="S4.T2.9.9.3.m1.1.1.cmml"><mn id="S4.T2.9.9.3.m1.1.1.2" xref="S4.T2.9.9.3.m1.1.1.2.cmml">297.03</mn><mo id="S4.T2.9.9.3.m1.1.1.1" xref="S4.T2.9.9.3.m1.1.1.1.cmml">±</mo><mn id="S4.T2.9.9.3.m1.1.1.3" xref="S4.T2.9.9.3.m1.1.1.3.cmml">142</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.9.9.3.m1.1b"><apply id="S4.T2.9.9.3.m1.1.1.cmml" xref="S4.T2.9.9.3.m1.1.1"><csymbol cd="latexml" id="S4.T2.9.9.3.m1.1.1.1.cmml" xref="S4.T2.9.9.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.9.9.3.m1.1.1.2.cmml" type="float" xref="S4.T2.9.9.3.m1.1.1.2">297.03</cn><cn id="S4.T2.9.9.3.m1.1.1.3.cmml" type="integer" xref="S4.T2.9.9.3.m1.1.1.3">142</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.9.9.3.m1.1c">297.03\pm 142</annotation><annotation encoding="application/x-llamapun" id="S4.T2.9.9.3.m1.1d">297.03 ± 142</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T2.12.12">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T2.12.12.4">#Vehicles</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.10.10.1"><math alttext="10.6\pm 5.43" class="ltx_Math" display="inline" id="S4.T2.10.10.1.m1.1"><semantics id="S4.T2.10.10.1.m1.1a"><mrow id="S4.T2.10.10.1.m1.1.1" xref="S4.T2.10.10.1.m1.1.1.cmml"><mn id="S4.T2.10.10.1.m1.1.1.2" xref="S4.T2.10.10.1.m1.1.1.2.cmml">10.6</mn><mo id="S4.T2.10.10.1.m1.1.1.1" xref="S4.T2.10.10.1.m1.1.1.1.cmml">±</mo><mn id="S4.T2.10.10.1.m1.1.1.3" xref="S4.T2.10.10.1.m1.1.1.3.cmml">5.43</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.10.10.1.m1.1b"><apply id="S4.T2.10.10.1.m1.1.1.cmml" xref="S4.T2.10.10.1.m1.1.1"><csymbol cd="latexml" id="S4.T2.10.10.1.m1.1.1.1.cmml" xref="S4.T2.10.10.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.10.10.1.m1.1.1.2.cmml" type="float" xref="S4.T2.10.10.1.m1.1.1.2">10.6</cn><cn id="S4.T2.10.10.1.m1.1.1.3.cmml" type="float" xref="S4.T2.10.10.1.m1.1.1.3">5.43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.10.10.1.m1.1c">10.6\pm 5.43</annotation><annotation encoding="application/x-llamapun" id="S4.T2.10.10.1.m1.1d">10.6 ± 5.43</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.11.11.2"><math alttext="6.9\pm 2.95" class="ltx_Math" display="inline" id="S4.T2.11.11.2.m1.1"><semantics id="S4.T2.11.11.2.m1.1a"><mrow id="S4.T2.11.11.2.m1.1.1" xref="S4.T2.11.11.2.m1.1.1.cmml"><mn id="S4.T2.11.11.2.m1.1.1.2" xref="S4.T2.11.11.2.m1.1.1.2.cmml">6.9</mn><mo id="S4.T2.11.11.2.m1.1.1.1" xref="S4.T2.11.11.2.m1.1.1.1.cmml">±</mo><mn id="S4.T2.11.11.2.m1.1.1.3" xref="S4.T2.11.11.2.m1.1.1.3.cmml">2.95</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.11.11.2.m1.1b"><apply id="S4.T2.11.11.2.m1.1.1.cmml" xref="S4.T2.11.11.2.m1.1.1"><csymbol cd="latexml" id="S4.T2.11.11.2.m1.1.1.1.cmml" xref="S4.T2.11.11.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.11.11.2.m1.1.1.2.cmml" type="float" xref="S4.T2.11.11.2.m1.1.1.2">6.9</cn><cn id="S4.T2.11.11.2.m1.1.1.3.cmml" type="float" xref="S4.T2.11.11.2.m1.1.1.3">2.95</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.11.11.2.m1.1c">6.9\pm 2.95</annotation><annotation encoding="application/x-llamapun" id="S4.T2.11.11.2.m1.1d">6.9 ± 2.95</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T2.12.12.3"><math alttext="7.3\pm 4.33" class="ltx_Math" display="inline" id="S4.T2.12.12.3.m1.1"><semantics id="S4.T2.12.12.3.m1.1a"><mrow id="S4.T2.12.12.3.m1.1.1" xref="S4.T2.12.12.3.m1.1.1.cmml"><mn id="S4.T2.12.12.3.m1.1.1.2" xref="S4.T2.12.12.3.m1.1.1.2.cmml">7.3</mn><mo id="S4.T2.12.12.3.m1.1.1.1" xref="S4.T2.12.12.3.m1.1.1.1.cmml">±</mo><mn id="S4.T2.12.12.3.m1.1.1.3" xref="S4.T2.12.12.3.m1.1.1.3.cmml">4.33</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T2.12.12.3.m1.1b"><apply id="S4.T2.12.12.3.m1.1.1.cmml" xref="S4.T2.12.12.3.m1.1.1"><csymbol cd="latexml" id="S4.T2.12.12.3.m1.1.1.1.cmml" xref="S4.T2.12.12.3.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T2.12.12.3.m1.1.1.2.cmml" type="float" xref="S4.T2.12.12.3.m1.1.1.2">7.3</cn><cn id="S4.T2.12.12.3.m1.1.1.3.cmml" type="float" xref="S4.T2.12.12.3.m1.1.1.3">4.33</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T2.12.12.3.m1.1c">7.3\pm 4.33</annotation><annotation encoding="application/x-llamapun" id="S4.T2.12.12.3.m1.1d">7.3 ± 4.33</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.4.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.5.2">Controllable challenging scenarios</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">RandomTrip <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib47" title="">47</a>]</cite> from the SUMO tool generates a set of random trips for a given network by randomly selecting source and destination edges for vehicles appearing at a specified arrival rate. We used it as a baseline since it provides sufficient coverage of possible scenarios. To ensure a fair comparison with the LLM-generated scenarios, we use a slightly higher arrival rate and then randomly delete vehicles to match the number of vehicles in the generated scenarios, maintaining a consistent level of crowding.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">To evaluate the challenging level of the generated scenarios, an LLM-based AV is used to navigate inside these scenarios. The poorer its performance, the more challenging the scenarios are considered. We adopt the performance metrics from Limism++ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib48" title="">48</a>]</cite> to quantifying the AV’s performance. The driving score is computed as weighted score for ride comfort, driving efficiency, and driving safety. The route completion value is the ratio of the completed route length by the driver agent to the total route length of the preset route. For more details, see Limism++.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Challenges posed by LLM based vehicle generator for fork scenario </figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.10">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.10.11.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.10.11.1.1">Scenario</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.10.11.1.2">Ours</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.10.11.1.3">RandomTrip</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.2.2.3">Route completion</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.1"><math alttext="0.42\pm 0.50" class="ltx_Math" display="inline" id="S4.T3.1.1.1.m1.1"><semantics id="S4.T3.1.1.1.m1.1a"><mrow id="S4.T3.1.1.1.m1.1.1" xref="S4.T3.1.1.1.m1.1.1.cmml"><mn id="S4.T3.1.1.1.m1.1.1.2" xref="S4.T3.1.1.1.m1.1.1.2.cmml">0.42</mn><mo id="S4.T3.1.1.1.m1.1.1.1" xref="S4.T3.1.1.1.m1.1.1.1.cmml">±</mo><mn id="S4.T3.1.1.1.m1.1.1.3" xref="S4.T3.1.1.1.m1.1.1.3.cmml">0.50</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.1.1.1.m1.1b"><apply id="S4.T3.1.1.1.m1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.1.1.1.m1.1.1.1.cmml" xref="S4.T3.1.1.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.1.1.1.m1.1.1.2.cmml" type="float" xref="S4.T3.1.1.1.m1.1.1.2">0.42</cn><cn id="S4.T3.1.1.1.m1.1.1.3.cmml" type="float" xref="S4.T3.1.1.1.m1.1.1.3">0.50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.1.1.1.m1.1c">0.42\pm 0.50</annotation><annotation encoding="application/x-llamapun" id="S4.T3.1.1.1.m1.1d">0.42 ± 0.50</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.2.2.2"><math alttext="0.72\pm 0.46" class="ltx_Math" display="inline" id="S4.T3.2.2.2.m1.1"><semantics id="S4.T3.2.2.2.m1.1a"><mrow id="S4.T3.2.2.2.m1.1.1" xref="S4.T3.2.2.2.m1.1.1.cmml"><mn id="S4.T3.2.2.2.m1.1.1.2" xref="S4.T3.2.2.2.m1.1.1.2.cmml">0.72</mn><mo id="S4.T3.2.2.2.m1.1.1.1" xref="S4.T3.2.2.2.m1.1.1.1.cmml">±</mo><mn id="S4.T3.2.2.2.m1.1.1.3" xref="S4.T3.2.2.2.m1.1.1.3.cmml">0.46</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.2.2.2.m1.1b"><apply id="S4.T3.2.2.2.m1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.2.2.2.m1.1.1.1.cmml" xref="S4.T3.2.2.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.2.2.2.m1.1.1.2.cmml" type="float" xref="S4.T3.2.2.2.m1.1.1.2">0.72</cn><cn id="S4.T3.2.2.2.m1.1.1.3.cmml" type="float" xref="S4.T3.2.2.2.m1.1.1.3">0.46</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.2.2.2.m1.1c">0.72\pm 0.46</annotation><annotation encoding="application/x-llamapun" id="S4.T3.2.2.2.m1.1d">0.72 ± 0.46</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.4.4.3">Driving score</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.3.3.1"><math alttext="40.22\pm 36.54" class="ltx_Math" display="inline" id="S4.T3.3.3.1.m1.1"><semantics id="S4.T3.3.3.1.m1.1a"><mrow id="S4.T3.3.3.1.m1.1.1" xref="S4.T3.3.3.1.m1.1.1.cmml"><mn id="S4.T3.3.3.1.m1.1.1.2" xref="S4.T3.3.3.1.m1.1.1.2.cmml">40.22</mn><mo id="S4.T3.3.3.1.m1.1.1.1" xref="S4.T3.3.3.1.m1.1.1.1.cmml">±</mo><mn id="S4.T3.3.3.1.m1.1.1.3" xref="S4.T3.3.3.1.m1.1.1.3.cmml">36.54</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.3.3.1.m1.1b"><apply id="S4.T3.3.3.1.m1.1.1.cmml" xref="S4.T3.3.3.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.3.3.1.m1.1.1.1.cmml" xref="S4.T3.3.3.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.3.3.1.m1.1.1.2.cmml" type="float" xref="S4.T3.3.3.1.m1.1.1.2">40.22</cn><cn id="S4.T3.3.3.1.m1.1.1.3.cmml" type="float" xref="S4.T3.3.3.1.m1.1.1.3">36.54</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.3.3.1.m1.1c">40.22\pm 36.54</annotation><annotation encoding="application/x-llamapun" id="S4.T3.3.3.1.m1.1d">40.22 ± 36.54</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.4.2"><math alttext="63.34\pm 27.43" class="ltx_Math" display="inline" id="S4.T3.4.4.2.m1.1"><semantics id="S4.T3.4.4.2.m1.1a"><mrow id="S4.T3.4.4.2.m1.1.1" xref="S4.T3.4.4.2.m1.1.1.cmml"><mn id="S4.T3.4.4.2.m1.1.1.2" xref="S4.T3.4.4.2.m1.1.1.2.cmml">63.34</mn><mo id="S4.T3.4.4.2.m1.1.1.1" xref="S4.T3.4.4.2.m1.1.1.1.cmml">±</mo><mn id="S4.T3.4.4.2.m1.1.1.3" xref="S4.T3.4.4.2.m1.1.1.3.cmml">27.43</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.4.4.2.m1.1b"><apply id="S4.T3.4.4.2.m1.1.1.cmml" xref="S4.T3.4.4.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.4.4.2.m1.1.1.1.cmml" xref="S4.T3.4.4.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.4.4.2.m1.1.1.2.cmml" type="float" xref="S4.T3.4.4.2.m1.1.1.2">63.34</cn><cn id="S4.T3.4.4.2.m1.1.1.3.cmml" type="float" xref="S4.T3.4.4.2.m1.1.1.3">27.43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.4.4.2.m1.1c">63.34\pm 27.43</annotation><annotation encoding="application/x-llamapun" id="S4.T3.4.4.2.m1.1d">63.34 ± 27.43</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T3.6.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.6.6.3">Total score</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.5.5.1"><math alttext="31.09\pm 38.74" class="ltx_Math" display="inline" id="S4.T3.5.5.1.m1.1"><semantics id="S4.T3.5.5.1.m1.1a"><mrow id="S4.T3.5.5.1.m1.1.1" xref="S4.T3.5.5.1.m1.1.1.cmml"><mn id="S4.T3.5.5.1.m1.1.1.2" xref="S4.T3.5.5.1.m1.1.1.2.cmml">31.09</mn><mo id="S4.T3.5.5.1.m1.1.1.1" xref="S4.T3.5.5.1.m1.1.1.1.cmml">±</mo><mn id="S4.T3.5.5.1.m1.1.1.3" xref="S4.T3.5.5.1.m1.1.1.3.cmml">38.74</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.5.5.1.m1.1b"><apply id="S4.T3.5.5.1.m1.1.1.cmml" xref="S4.T3.5.5.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.5.5.1.m1.1.1.1.cmml" xref="S4.T3.5.5.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.5.5.1.m1.1.1.2.cmml" type="float" xref="S4.T3.5.5.1.m1.1.1.2">31.09</cn><cn id="S4.T3.5.5.1.m1.1.1.3.cmml" type="float" xref="S4.T3.5.5.1.m1.1.1.3">38.74</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.5.5.1.m1.1c">31.09\pm 38.74</annotation><annotation encoding="application/x-llamapun" id="S4.T3.5.5.1.m1.1d">31.09 ± 38.74</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.6.6.2"><math alttext="55.14\pm 37.26" class="ltx_Math" display="inline" id="S4.T3.6.6.2.m1.1"><semantics id="S4.T3.6.6.2.m1.1a"><mrow id="S4.T3.6.6.2.m1.1.1" xref="S4.T3.6.6.2.m1.1.1.cmml"><mn id="S4.T3.6.6.2.m1.1.1.2" xref="S4.T3.6.6.2.m1.1.1.2.cmml">55.14</mn><mo id="S4.T3.6.6.2.m1.1.1.1" xref="S4.T3.6.6.2.m1.1.1.1.cmml">±</mo><mn id="S4.T3.6.6.2.m1.1.1.3" xref="S4.T3.6.6.2.m1.1.1.3.cmml">37.26</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.6.6.2.m1.1b"><apply id="S4.T3.6.6.2.m1.1.1.cmml" xref="S4.T3.6.6.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.6.6.2.m1.1.1.1.cmml" xref="S4.T3.6.6.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.6.6.2.m1.1.1.2.cmml" type="float" xref="S4.T3.6.6.2.m1.1.1.2">55.14</cn><cn id="S4.T3.6.6.2.m1.1.1.3.cmml" type="float" xref="S4.T3.6.6.2.m1.1.1.3">37.26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.6.6.2.m1.1c">55.14\pm 37.26</annotation><annotation encoding="application/x-llamapun" id="S4.T3.6.6.2.m1.1d">55.14 ± 37.26</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T3.8.8">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.8.8.3">Use Time</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.7.7.1"><math alttext="67.90\pm 36.33" class="ltx_Math" display="inline" id="S4.T3.7.7.1.m1.1"><semantics id="S4.T3.7.7.1.m1.1a"><mrow id="S4.T3.7.7.1.m1.1.1" xref="S4.T3.7.7.1.m1.1.1.cmml"><mn id="S4.T3.7.7.1.m1.1.1.2" xref="S4.T3.7.7.1.m1.1.1.2.cmml">67.90</mn><mo id="S4.T3.7.7.1.m1.1.1.1" xref="S4.T3.7.7.1.m1.1.1.1.cmml">±</mo><mn id="S4.T3.7.7.1.m1.1.1.3" xref="S4.T3.7.7.1.m1.1.1.3.cmml">36.33</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.7.7.1.m1.1b"><apply id="S4.T3.7.7.1.m1.1.1.cmml" xref="S4.T3.7.7.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.7.7.1.m1.1.1.1.cmml" xref="S4.T3.7.7.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.7.7.1.m1.1.1.2.cmml" type="float" xref="S4.T3.7.7.1.m1.1.1.2">67.90</cn><cn id="S4.T3.7.7.1.m1.1.1.3.cmml" type="float" xref="S4.T3.7.7.1.m1.1.1.3">36.33</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.7.7.1.m1.1c">67.90\pm 36.33</annotation><annotation encoding="application/x-llamapun" id="S4.T3.7.7.1.m1.1d">67.90 ± 36.33</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.8.8.2"><math alttext="89.71\pm 76.56" class="ltx_Math" display="inline" id="S4.T3.8.8.2.m1.1"><semantics id="S4.T3.8.8.2.m1.1a"><mrow id="S4.T3.8.8.2.m1.1.1" xref="S4.T3.8.8.2.m1.1.1.cmml"><mn id="S4.T3.8.8.2.m1.1.1.2" xref="S4.T3.8.8.2.m1.1.1.2.cmml">89.71</mn><mo id="S4.T3.8.8.2.m1.1.1.1" xref="S4.T3.8.8.2.m1.1.1.1.cmml">±</mo><mn id="S4.T3.8.8.2.m1.1.1.3" xref="S4.T3.8.8.2.m1.1.1.3.cmml">76.56</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.8.8.2.m1.1b"><apply id="S4.T3.8.8.2.m1.1.1.cmml" xref="S4.T3.8.8.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.8.8.2.m1.1.1.1.cmml" xref="S4.T3.8.8.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.8.8.2.m1.1.1.2.cmml" type="float" xref="S4.T3.8.8.2.m1.1.1.2">89.71</cn><cn id="S4.T3.8.8.2.m1.1.1.3.cmml" type="float" xref="S4.T3.8.8.2.m1.1.1.3">76.56</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.8.8.2.m1.1c">89.71\pm 76.56</annotation><annotation encoding="application/x-llamapun" id="S4.T3.8.8.2.m1.1d">89.71 ± 76.56</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S4.T3.10.10">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T3.10.10.3">Success rate</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.9.9.1"><math alttext="0.50\pm 0.53" class="ltx_Math" display="inline" id="S4.T3.9.9.1.m1.1"><semantics id="S4.T3.9.9.1.m1.1a"><mrow id="S4.T3.9.9.1.m1.1.1" xref="S4.T3.9.9.1.m1.1.1.cmml"><mn id="S4.T3.9.9.1.m1.1.1.2" xref="S4.T3.9.9.1.m1.1.1.2.cmml">0.50</mn><mo id="S4.T3.9.9.1.m1.1.1.1" xref="S4.T3.9.9.1.m1.1.1.1.cmml">±</mo><mn id="S4.T3.9.9.1.m1.1.1.3" xref="S4.T3.9.9.1.m1.1.1.3.cmml">0.53</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.9.9.1.m1.1b"><apply id="S4.T3.9.9.1.m1.1.1.cmml" xref="S4.T3.9.9.1.m1.1.1"><csymbol cd="latexml" id="S4.T3.9.9.1.m1.1.1.1.cmml" xref="S4.T3.9.9.1.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.9.9.1.m1.1.1.2.cmml" type="float" xref="S4.T3.9.9.1.m1.1.1.2">0.50</cn><cn id="S4.T3.9.9.1.m1.1.1.3.cmml" type="float" xref="S4.T3.9.9.1.m1.1.1.3">0.53</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.9.9.1.m1.1c">0.50\pm 0.53</annotation><annotation encoding="application/x-llamapun" id="S4.T3.9.9.1.m1.1d">0.50 ± 0.53</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T3.10.10.2"><math alttext="0.80\pm 0.42" class="ltx_Math" display="inline" id="S4.T3.10.10.2.m1.1"><semantics id="S4.T3.10.10.2.m1.1a"><mrow id="S4.T3.10.10.2.m1.1.1" xref="S4.T3.10.10.2.m1.1.1.cmml"><mn id="S4.T3.10.10.2.m1.1.1.2" xref="S4.T3.10.10.2.m1.1.1.2.cmml">0.80</mn><mo id="S4.T3.10.10.2.m1.1.1.1" xref="S4.T3.10.10.2.m1.1.1.1.cmml">±</mo><mn id="S4.T3.10.10.2.m1.1.1.3" xref="S4.T3.10.10.2.m1.1.1.3.cmml">0.42</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.T3.10.10.2.m1.1b"><apply id="S4.T3.10.10.2.m1.1.1.cmml" xref="S4.T3.10.10.2.m1.1.1"><csymbol cd="latexml" id="S4.T3.10.10.2.m1.1.1.1.cmml" xref="S4.T3.10.10.2.m1.1.1.1">plus-or-minus</csymbol><cn id="S4.T3.10.10.2.m1.1.1.2.cmml" type="float" xref="S4.T3.10.10.2.m1.1.1.2">0.80</cn><cn id="S4.T3.10.10.2.m1.1.1.3.cmml" type="float" xref="S4.T3.10.10.2.m1.1.1.3">0.42</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.T3.10.10.2.m1.1c">0.80\pm 0.42</annotation><annotation encoding="application/x-llamapun" id="S4.T3.10.10.2.m1.1d">0.80 ± 0.42</annotation></semantics></math></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.T3" title="TABLE III ‣ IV-B Controllable challenging scenarios ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">III</span></a>, our LLM-based vehicle generator creates more challenging BV routes, resulting in significantly lower success rates and performance scores for AVs.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="158" id="S4.F6.g1" src="extracted/5841084/fig/llm_av.png" width="269"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Left: Scenarios generated by RandomTrip, where AV and BV appear randomly from entrances at random times. Right: Challenging situation created by OmniTester, where AV and BV routes are meticulously designed to generate dense interactions around AV and near the junction.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Two sampled test cases for forking and merging situations are shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.F6" title="Figure 6 ‣ IV-B Controllable challenging scenarios ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">6</span></a>. In the top scenario, within the same junction, the randomly distributed vehicles leave an empty lane for AV to drive through, with no other vehicles in nearby lanes. AV is passing the junction smoothly with no interaction with other vehicles, making this testing scenario less effective in testing interaction capabilities. In contrast, OmniTester creates a crowded traffic situation near the junction, requiring AV to perform a lane change before passing through. This challenging scenario ultimately results in a collision with BV2, revealing inherent flaws in the lane change strategies of this LLM-driven AV during dense traffic conditions. In the bottom scenario, at the T-shaped junction, the left-turning AV encounters no incoming vehicles to interact with in the RandomTrip setup. In contrast, OmniTester creates a complex interaction situation with a car following AV, a vehicle leading AV, and another vehicle driving towards the intersection as AV is turning. This complexity challenges AV’s reasoning and decision-making processes, making it easier to reveal inherent flaws in this LLM-driven AV.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.4.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.5.2">Effectiveness of RAG Module</span>
</h3>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="163" id="S4.F7.g1" src="extracted/5841084/fig/rag_demo1.png" width="586"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Left: Freeway Off-ramp retrieved from database
Right: Generated road network with a similar layout. </figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">To generate diverse freeway ramp scenario, we pulled several on/off ramp map from OpenStreetMap and added into the database. Upon user request, it can generate realistic ramp scenarios with assistance from the data retrieved from this database. Instead of using code, text, or picture formats of the road network on the fly for assisting generation, a detailed description of the road structure is prepared to provide distilled information. The generated description ensures that the Net Generator receives sufficient information to construct a similar new road structure that matches the layout, segment details, and directional flow of the original.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">As illustrated in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S1.F2" title="Figure 2 ‣ I Introduction ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">2</span></a>, the text description is generated using a VLM-based summarizer. Powered by VLM’s vision and spatial reasoning capabilities, layout information can be extracted from the picture. The node and edge files offer detailed information about the road structure, including length, number of lanes, and the shape of each curved edge. With this abundant information, the summarizer intelligently selects key features based on knowledge from the open world and hints injected from the prompt. It then summarizes all the important information into the final description, which contains the general layout, main segment locations, moving directions, connectivity, and even detailed descriptions of curve shapes. Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.F7" title="Figure 7 ‣ IV-C Effectiveness of RAG Module ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">7</span></a> contains an example illustrating how the retrieved example’s description guided the generation of freeway off-ramp road structures. As shown, it describes a freeway off-ramp comprising three segments, covering the main freeway road and the trunk for exiting the freeway. Besides detailing the layout, segments, and number of lanes for each edge, it provides a thorough description of the curve shape. Using this example in the prompt, the Interpreter can generate a similar description for the Net Generator, preserving the detailed curve information. The final generated network shares a similar shape to the provided example, especially the curvature of the offramp.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="187" id="S4.F8.g1" src="extracted/5841084/fig/crash_report3.png" width="592"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>
Based on the crash report, our framework can generate the road network using all the provided information. It can also infer the vehicles’ driving routes before the accident, thereby reconstructing the entire scenario.</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="S4.F9.g1" src="extracted/5841084/fig/net_scatter.png" width="473"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>
Left: Diverse road networks generated with varying numbers of edges, segment angles, and total lengths. Right: Similar road networks with the same number of edges due to the removal of the interpreter component.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.4.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.5.2">Effectiveness of OmniTester with crash report</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">A direct application of our OmniTester framework is reconstructing scenarios similar to those described in text. This allows for the generation of multiple challenging testing scenarios that can be used for stress testing AV systems. For example, by varying the departure time for each vehicle or changing the driving strategies, a wide range of testing scenarios near risky situations can be generated.</p>
</div>
<div class="ltx_para" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">As an example, we use one case from crash reports <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#bib.bib49" title="">49</a>]</cite> to demonstrate how to reconstruct similar dangerous situations for testing a vehicle’s reaction. Specifically, instead of using the Interpreter, we directly use the scenario description from crash report for Net Generator as well as the Vehicle Generator. As shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.F8" title="Figure 8 ‣ IV-C Effectiveness of RAG Module ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">8</span></a>, the text is extracted directly from a crash report, describing a crash event that occurred near an intersection. It includes the moving direction for the roads and their speed limits in km/h. The Net Generator not only generates the exact geometry as described in the report but also automatically converts the speed unit in SUMO’s default speed unit of m/s without error. For the Vehicle Generator, according to the reasoning process present, it can extract each vehicle’s property (such as brand and color) and infer their relative positions based only on crash evolution process: ”As Vehicle 1 approached the intersection, it slowed for Vehicle 3, which was attempting to make a right-hand turn. Vehicle 2 braked, but was unable to stop, causing the frontal plane of Vehicle 2 to strike the rear plane of Vehicle 1. After the rear impact from Vehicle 2, Vehicle 1 then continued forward and struck the rear plane of Vehicle 3 with its front plane.”. Eventually, the system can reconstruct the routes with corresponding departing time to ensure the relative positions match and similar scenario can be reconstructed.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS5.4.1.1">IV-E</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS5.5.2">Ablation Study</span>
</h3>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>Ablation study: Change of success rate</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T4.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.1.1.1">Metrics</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T4.1.1.1.2">Success rate*</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.2.1.1">Ours</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.2.1.2">1</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.3.2.1">without netconvert</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.3.2.2">0</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.4.3.1">without interpreter</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.1.4.3.2">1</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T4.1.5.4.1">without reasoning section</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T4.1.5.4.2">0.4</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.T4" title="TABLE IV ‣ IV-E Ablation Study ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">IV</span></a>, the success rate of the generation process (with a maximum of 3 attempts per scene) drops from 1 to 0 for the generation process when the netconvert component is removed. Several recurrent errors occur, indicating a fundamental inability to generate the network directly. For example, the LLM’s response includes unknown keywords in the net file, such as ”limitedTurnSpeed=true,” resulting in unsuccessful parsing into SUMO. Additionally, the generated net XML is missing junctions and connections and contains incorrect connections, leading to errors like ”An unknown lane.”</p>
</div>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">After removing the interpreter, which is responsible for generating detailed narrative descriptions for each scenario, the LLM fails to produce diverse network structures in one pass. It consistently generates 4 nodes with minor spatial variations and 6 edges defined by the same nodes. As shown in Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#S4.F9" title="Figure 9 ‣ IV-C Effectiveness of RAG Module ‣ IV Experiments ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">9</span></a>, compared to the left figure, the right figure lacks diversity in network structures. This demonstrates that diversity is significantly supported by the multi-stage generation process.</p>
</div>
<div class="ltx_para" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">After removing the CoT mechanism, the observed degradation in accuracy can be primarily attributed to three prevalent issues. The first issue arises from the omission of critical attributes within the definition of elements. For instance, the absence of the ”shape” attribute within the definition of the ”lane” element results in SUMO’s inability to accurately load and interpret the lane configurations. The second issue stems from the use of attribute values that fall outside the specified enumeration. For example, the ”spreadType” attribute in the ”edge” element, which indicates how to calculate the lane geometry from the edge geometry, was incorrectly assigned the value ”left,” although the permissible values are limited to ”right,” ”center,” and ”roadCenter.” The third issue is the utilization of undeclared attributes within elements. For instance, the generated XML file references an incorrect XML Schema, resulting in the ”function” attribute within the ”edge” element being undeclared.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Conclusion and Future work</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We present a scenario generation framework based on LLM and VLM models. This is the first system to generate road structures and vehicles based on user requests with high controllability and flexibility, powered by RAG and self-improvement techniques. We demonstrate that a traffic flow simulator can serve as an efficient tool for LLM to generate complex and diverse testing scenarios with properly designed prompts. A structured definition written in XML, combined with image-based representation and narrative description, effectively represents a scenario. Lastly, self-improvement through effective feedback can enhance scenario generation.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">Current endeavors are still in their early stages, and utilizing multimodal large language models and the vast array of knowledge learned within these models to create more complex scenarios remains an open question. One direction for future work is to generate more realistic BV behaviors which could be controlled by deep neural networks (DNNs) or another LLM model. Another direction is to model more elements within the scenario based on detailed descriptions. By pursuing this path, we aim to generate controllable world models that enhance the realism and complexity of simulated environments, making testing more targeted and efficient.</p>
</div>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompt Examples</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">Prompts are crucial in enhancing the reasoning and generation capabilities of LLMs. Fig.<a class="ltx_ref" href="https://arxiv.org/html/2409.06450v1#A1.F10" title="Figure 10 ‣ Appendix A Prompt Examples ‣ Multimodal Large Language Model Driven Scenario Testing for Autonomous Vehicles"><span class="ltx_text ltx_ref_tag">10</span></a> demonstrate the prompts used in the Net Generator and Vehicle Generator of OmniTester. Each prompt includes several parts: a summary of the generation task, the steps guiding the generation process, and the desired format of the output. For the Net generator, the prompt additionally includes several road constraints to ensure the generated node and edge files are well-defined XML format. For Vehicle Generator, the prompt includes explanations for challenging situations.</p>
</div>
<figure class="ltx_figure" id="A1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="268" id="A1.1.g1" src="extracted/5841084/fig/prompt_eg1.png" width="287"/>
</figure>
<figure class="ltx_figure" id="A1.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="308" id="A1.F10.g1" src="extracted/5841084/fig/prompt_eg2.png" width="287"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>
The system prompts for Net Generator and Vehicle Generator of OmniTester.</figcaption>
</figure>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
H. X. Liu and S. Feng, “Curse of rarity for autonomous vehicles,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">nature communications</em>, vol. 15, no. 1, p. 4808, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
S. Feng, H. Sun, X. Yan, H. Zhu, Z. Zou, S. Shen, and H. X. Liu, “Dense reinforcement learning for safety validation of autonomous vehicles,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Nature</em>, vol. 615, no. 7953, pp. 620–627, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
J. Wang, A. Pun, J. Tu, S. Manivasagam, A. Sadat, S. Casas, M. Ren, and R. Urtasun, “Advsim: Generating safety-critical scenarios for self-driving vehicles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2021, pp. 9909–9918.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. D. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph, G. Brockman <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">et al.</em>, “Evaluating large language models trained on code,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">arXiv preprint arXiv:2107.03374</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling, F. Gimeno, A. Dal Lago <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et al.</em>, “Competition-level code generation with alphacode,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">Science</em>, vol. 378, no. 6624, pp. 1092–1097, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Imani, L. Du, and H. Shrivastava, “Mathprompter: Mathematical reasoning using large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2303.05398</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
V. Kumaran, J. Rowe, B. Mott, and J. Lester, “Scenecraft: Automating interactive narrative scene generation in digital games with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</em>, vol. 19, no. 1, 2023, pp. 86–96.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh, “Video generation models as world simulators,” 2024. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://openai.com/research/video-generation-models-as-world-simulators</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
K. Technology. (n.d.) Kling. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://kling.kuaishou.com/en</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Y. Xiao and W. Y. Wang, “On hallucination and predictive uncertainty in conditional language generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2103.15025</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Flötteröd, R. Hilbrich, L. Lücken, J. Rummel, P. Wagner, and E. Wiessner, “Microscopic Traffic Simulation using SUMO,” in <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">2018 21st International Conference on Intelligent Transportation Systems (ITSC)</em>, pp. 2575–2582. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ieeexplore.ieee.org/abstract/document/8569938</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel, S. Riedel, and D. Kiela, “Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Advances in Neural Information Processing Systems</em>, vol. 33.   Curran Associates, Inc., pp. 9459–9474. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
P. Zhang, B. Zhu, J. Zhao, T. Fan, and Y. Sun, “Performance Evaluation Method for Automated Driving System in Logical Scenario,” vol. 5, no. 3, pp. 299–310. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://link.springer.com/10.1007/s42154-022-00191-3</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
S. Zhao, J. Duan, S. Wu, X. Gu, C. Li, K. Yin, and H. Wang, “Genetic Algorithm-Based SOTIF Scenario Construction for Complex Traffic Flow,” vol. 6, no. 4, pp. 531–546. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://link.springer.com/10.1007/s42154-023-00251-2</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
F. Fahrenkrog, S. Reithinger, B. Gülsen, and F. Raisch, “European Research Project’s Contributions to a Safer Automated Road Traffic,” vol. 6, no. 4, pp. 521–530. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://link.springer.com/10.1007/s42154-023-00250-3</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D. Nalic, T. Mihalj, M. Bäumler, M. Lehmann, A. Eichberger, and S. Bernsteiner, “Scenario based testing of automated driving systems: A literature survey,” in <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">FISITA web Congress</em>, vol. 10, 2020, p. 1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
J. Zhou and L. del Re, “Reduced complexity safety testing for adas &amp; adf,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">IFAC-PapersOnLine</em>, vol. 50, no. 1, pp. 5985–5990, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
D. Jung, D. Jung, C. Jeong, Y. Kou, and H. Peng, “Worst case scenarios generation and its application on driving,” SAE Technical Paper, Tech. Rep., 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
G. E. Mullins, P. G. Stankiewicz, R. C. Hawthorne, and S. K. Gupta, “Adaptive generation of challenging scenarios for testing and evaluation of autonomous vehicles,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Journal of Systems and Software</em>, vol. 137, pp. 197–215, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
X. Yan, Z. Zou, S. Feng, H. Zhu, H. Sun, and H. X. Liu, “Learning naturalistic driving environment with statistical realism,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Nature communications</em>, vol. 14, no. 1, p. 2037, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
D. Rempe, J. Philion, L. J. Guibas, S. Fidler, and O. Litany, “Generating useful accident-prone driving scenarios via a learned traffic prior,” in <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em>.   New Orleans, LA, USA: IEEE, Jun. 2022, p. 17284–17294. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://ieeexplore.ieee.org/document/9880074/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
K. Chitta, D. Dauner, and A. Geiger, “Sledge: Synthesizing simulation environments for driving agents with generative models,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2403.17933</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
W. Ding, Y. Cao, D. Zhao, C. Xiao, and M. Pavone, “Realgen: Retrieval augmented generation for controllable traffic scenarios,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2312.13303</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
C. Cui, Y. Ma, X. Cao, W. Ye, Y. Zhou, K. Liang, J. Chen, J. Lu, Z. Yang, K.-D. Liao <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et al.</em>, “A survey on multimodal large language models for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2024, pp. 958–979.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
D. Fu, X. Li, L. Wen, M. Dou, P. Cai, B. Shi, and Y. Qiao, “Drive like a human: Rethinking autonomous driving with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2024, pp. 910–919.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
H. Sha, Y. Mu, Y. Jiang, L. Chen, C. Xu, P. Luo, S. E. Li, M. Tomizuka, W. Zhan, and M. Ding, “Languagempc: Large language models as decision makers for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2310.03026</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Z. Xu, Y. Zhang, E. Xie, Z. Zhao, Y. Guo, K. K. Wong, Z. Li, and H. Zhao, “Drivegpt4: Interpretable end-to-end autonomous driving via large language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2310.01412</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
W. Wang, J. Xie, C. Hu, H. Zou, J. Fan, W. Tong, Y. Wen, S. Wu, H. Deng, Z. Li <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">et al.</em>, “Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.2.2">arXiv preprint arXiv:2312.09245</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
J. Zhang, C. Xu, and B. Li, “Chatscene: Knowledge-enabled safety-critical scenario generation for autonomous vehicles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 15 459–15 469.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
D. J. Fremont, T. Dreossi, S. Ghosh, X. Yue, A. L. Sangiovanni-Vincentelli, and S. A. Seshia, “Scenic: a language for scenario specification and scene generation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 40th ACM SIGPLAN conference on programming language design and implementation</em>, 2019, pp. 63–78.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Y. Wei, Z. Wang, Y. Lu, C. Xu, C. Liu, H. Zhao, S. Chen, and Y. Wang, “Editable scene simulation for autonomous driving via collaborative llm-agents,” in <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 15 077–15 087.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
H. Tian, X. Han, G. Wu, Y. Zhou, S. Li, J. Wei, D. Ye, W. Wang, and T. Zhang, “An llm-enhanced multi-objective evolutionary search for autonomous driving test scenario generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2406.10857</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Z. Zhong, D. Rempe, Y. Chen, B. Ivanovic, Y. Cao, D. Xu, M. Pavone, and B. Ray, “Language-guided traffic simulation via scene-level diffusion,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Conference on Robot Learning</em>.   PMLR, 2023, pp. 144–177.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
C. Chang, S. Wang, J. Zhang, J. Ge, and L. Li, “Llmscenario: Large language model driven scenario generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">IEEE Transactions on Systems, Man, and Cybernetics: Systems</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">et al.</em>, “Chain-of-thought prompting elicits reasoning in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.2.2">Advances in neural information processing systems</em>, vol. 35, pp. 24 824–24 837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">et al.</em>, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2">Advances in neural information processing systems</em>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
The GraphML File Format. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">http://graphml.graphdrawing.org/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
A. Hagberg, P. J. Swart, and D. A. Schult, “Exploring network structure, dynamics, and function using NetworkX.” [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.osti.gov/biblio/960616</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
The Multi-Agent Transport Simulation MATSim. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://library.oapen.org/handle/20.500.12657/32162</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Intelligent driving intelligence test for autonomous vehicles with naturalistic and adversarial environment — Nature Communications. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.nature.com/articles/s41467-021-21007-8</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
SUMO - Simulation of Urban MObility. (2018) Sumo documentation: duarouter. Accessed: 2024-06-24. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://sumo.dlr.de/docs/duarouter.html</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Netconvert - SUMO Documentation. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://sumo.dlr.de/docs/netconvert.html</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
OpenStreetMap Foundation. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://osmfoundation.org/w/index.php?title=Main_Page&amp;oldid=12663</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
A. Neelakantan, T. Xu, R. Puri, A. Radford, J. M. Han, J. Tworek, Q. Yuan, N. Tezak, J. W. Kim, C. Hallacy <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">et al.</em>, “Text and code embeddings by contrastive pre-training,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.2.2">arXiv preprint arXiv:2201.10005</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
C. Developers. (2024) Chroma database. Accessed: 2024-06-24. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://www.trychroma.com/</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Z. Xu, S. Jain, and M. Kankanhalli, “Hallucination is inevitable: An innate limitation of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2401.11817</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
SUMO - Simulation of Urban MObility. (2018) Sumo documentation: Randomtrips. Accessed: 2024-06-24. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://sumo.dlr.de/docs/Tools/Trip.html</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
D. Fu, W. Lei, L. Wen, P. Cai, S. Mao, M. Dou, B. Shi, and Y. Qiao, “Limsim++: A closed-loop platform for deploying multimodal llms in autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2402.01246</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
NHTSA, “Nhtsa crash viewer,” 2023, accessed: 2024-07-03. [Online]. Available: <span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://crashviewer.nhtsa.dot.gov/</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 10 12:04:00 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
