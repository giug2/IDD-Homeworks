<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization</title>
<!--Generated on Fri Jul 19 20:55:19 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Promotional post,  mindmap,  caption,  image,  exploration,  customization,  ideation." lang="en" name="keywords"/>
<base href="/html/2407.14928v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S1" title="In Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S2" title="In Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S2.SS1" title="In 2. Related Work ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Example-based Design and Ideation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S2.SS2" title="In 2. Related Work ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Image and Caption Recommendations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S2.SS3" title="In 2. Related Work ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Promotional Post Design Systems</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S3" title="In Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Formative Study</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S3.SS1" title="In 3. Formative Study ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S3.SS2" title="In 3. Formative Study ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Formulating Design Goals</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S3.SS2.SSS1" title="In 3.2. Formulating Design Goals ‣ 3. Formative Study ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.1 </span><span class="ltx_text ltx_font_bold">R1: Help explore images and captions in diverse dimensions</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S3.SS2.SSS2" title="In 3.2. Formulating Design Goals ‣ 3. Formative Study ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.2 </span><span class="ltx_text ltx_font_bold">R2: Support context-aware exploration considering brand messages and user-specified design constraints</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S3.SS2.SSS3" title="In 3.2. Formulating Design Goals ‣ 3. Formative Study ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.3 </span><span class="ltx_text ltx_font_bold">R3: Flexible fusion of various images and captions to customize designs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S3.SS2.SSS4" title="In 3.2. Formulating Design Goals ‣ 3. Formative Study ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2.4 </span><span class="ltx_text ltx_font_bold">R4: Design a mind-map layout to organize ideas and track the thought process.</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S4" title="In Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Influencer Design Overview</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S4.SS1" title="In 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>User Interface and Blocks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S4.SS2" title="In 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Usage Scenario</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S5" title="In Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Influencer System</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S5.SS1" title="In 5. Influencer System ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Dataset Building</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S5.SS2" title="In 5. Influencer System ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Image and Caption Recommendation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S5.SS2.SSS1" title="In 5.2. Image and Caption Recommendation ‣ 5. Influencer System ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span><span class="ltx_text ltx_font_bold">Image Recommendation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S5.SS2.SSS2" title="In 5.2. Image and Caption Recommendation ‣ 5. Influencer System ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span><span class="ltx_text ltx_font_bold">Caption Recommendation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S5.SS3" title="In 5. Influencer System ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Context-Aware Exploration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S5.SS4" title="In 5. Influencer System ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Image and Caption Fusion</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S5.SS4.SSS1" title="In 5.4. Image and Caption Fusion ‣ 5. Influencer System ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.1 </span><span class="ltx_text ltx_font_bold">Text-based Fusion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S5.SS4.SSS2" title="In 5.4. Image and Caption Fusion ‣ 5. Influencer System ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4.2 </span><span class="ltx_text ltx_font_bold">Image-based Fusion</span></span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S6" title="In Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>User Evaluation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S6.SS1" title="In 6. User Evaluation ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Participants</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S6.SS2" title="In 6. User Evaluation ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Tasks and Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S6.SS3" title="In 6. User Evaluation ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Procedure</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S7" title="In Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S7.SS1" title="In 7. Results ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Quantitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S7.SS2" title="In 7. Results ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Qualitative Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S7.SS3" title="In 7. Results ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span>Expert Assessment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S8" title="In Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S8.SS1" title="In 8. Discussion ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Examples and Context as Key Components in Design</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S8.SS2" title="In 8. Discussion ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Flexible Fusion and Generation with Personalization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S8.SS3" title="In 8. Discussion ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Trade-off between Automation and Autonomy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S8.SS4" title="In 8. Discussion ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.4 </span>Limitations and Future Work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S9" title="In Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xuye Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-5876-7229" title="ORCID identifier">0000-0001-5876-7229</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">University of Waterloo</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2">200 University Ave W</span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3">Waterloo</span><span class="ltx_text ltx_affiliation_state" id="id4.4.id4">Ontario</span><span class="ltx_text ltx_affiliation_country" id="id5.5.id5">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xuye.liu@uwaterloo.ca">xuye.liu@uwaterloo.ca</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Annie Sun
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id6.1.id1">University of Waterloo</span><span class="ltx_text ltx_affiliation_streetaddress" id="id7.2.id2">200 University Ave W</span><span class="ltx_text ltx_affiliation_city" id="id8.3.id3">Waterloo</span><span class="ltx_text ltx_affiliation_state" id="id9.4.id4">Ontario</span><span class="ltx_text ltx_affiliation_country" id="id10.5.id5">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:a34sun@uwaterloo.ca">a34sun@uwaterloo.ca</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Pengcheng An
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id11.1.id1">Southern University of Science and Technology</span><span class="ltx_text ltx_affiliation_city" id="id12.2.id2">Shenzhen</span><span class="ltx_text ltx_affiliation_state" id="id13.3.id3">Guangdong</span><span class="ltx_text ltx_affiliation_country" id="id14.4.id4">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:anpc@sustech.edu.cn">anpc@sustech.edu.cn</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tengfei Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id15.1.id1">Stony Brook University</span><span class="ltx_text ltx_affiliation_city" id="id16.2.id2">Stony Brook</span><span class="ltx_text ltx_affiliation_state" id="id17.3.id3">New York</span><span class="ltx_text ltx_affiliation_country" id="id18.4.id4">United States</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:tengfei.ma@stonybrook.edu">tengfei.ma@stonybrook.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jian Zhao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-5008-4319" title="ORCID identifier">0000-0001-5008-4319</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">University of Waterloo</span><span class="ltx_text ltx_affiliation_streetaddress" id="id20.2.id2">200 University Ave W</span><span class="ltx_text ltx_affiliation_city" id="id21.3.id3">Waterloo</span><span class="ltx_text ltx_affiliation_state" id="id22.4.id4">Ontario</span><span class="ltx_text ltx_affiliation_country" id="id23.5.id5">Canada</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jianzhao@uwaterloo.ca">jianzhao@uwaterloo.ca</a>
</span></span></span>
</div>
<div class="ltx_dates">(2023)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id24.id1">Creating promotional posts on social platforms enables everyday users to disseminate their creative outcomes, engage in community exchanges, or generate additional income from micro-businesses. However, creating eye-catching posts combining both original, appealing images and articulate, effective captions can be rather challenging and time-consuming for everyday users who are mostly design novices.
We propose Influencer, an interactive tool to assist novice creators in crafting high-quality promotional post designs, achieving quick design ideation and unencumbered content creation through AI. Within Influencer, we contribute a multi-dimensional recommendation framework that allows users to intuitively generate new ideas through example-based image and caption recommendation. Further, Influencer implements a holistic promotional post design system that supports context-aware image and caption exploration considering brand messages and user-specified design constraints, flexible fusion of various images and captions, and a mind-map-like layout for thinking tracking and post-recording.
We evaluated Influencer with 12 design enthusiasts through an in-lab user study by comparing it to a baseline combining Google Search + Figma.
Quantitative and qualitative results demonstrate that Influencer is effective in assisting design novices to generate ideas as well as creative and diverse promotional posts with user-friendly interaction. </p>
</div>
<div class="ltx_keywords">Promotional post, mindmap, caption, image, exploration, customization, ideation.
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmcopyright</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2023</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_price" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">price: </span>15.00</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Interactive systems and tools</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Artificial intelligence</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Applied computing Arts and humanities</span></span></span>
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="345" id="S0.F1.g1" src="extracted/5743646/Figure/Teaser.png" width="598"/></div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>A design novice uses Influencer to ideate and make promotional posts to promote her homemade juice. (A) The user starts by input the topic in Influencer via a text block and explores the related images and captions in various directions. (B) Context-aware exploration is supported which updates the image and caption recommendation based on a brand/product image or message. (C) Various materials (i.e., image and text) can be flexibly fused to make a new image or caption. (D) Influencer allows the user to not only easily create harmonious promotional posts but also quickly obtain multiple post alternatives. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_figure_panel undefined" id="S0.F1.1">\Description</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel" id="S0.F1.2">The figure is a pipeline illustrating the promotional post design process in Influencer, which is fully described in the text.</p>
</div>
</div>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Creating promotional posts on social platforms enables everyday users to share their creativity and engage in micro-entrepreneurship, an emerging economic component representing flexible, small-scale business ventures that offer substantial growth potential <cite class="ltx_cite ltx_citemacro_citep">(Richards, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib58" title="">2006</a>)</cite>.
By creating promotional posts on social media, “micro-entrepreneurs” could showcase their products or services (such as selling second-hand items, homemade cakes <cite class="ltx_cite ltx_citemacro_citep">(Metcalf, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib48" title="">1973</a>)</cite>), engage with target customers, and build their online presence <cite class="ltx_cite ltx_citemacro_citep">(Adekunle and Kajumba, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib2" title="">2020</a>; Menon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib46" title="">2019</a>)</cite>.
This helps individuals, such as low-income groups or college students, to generate additional income for improving their life qualities, thus contributing to the resilience of the society <cite class="ltx_cite ltx_citemacro_citep">(Lessa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib42" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Promotional post needs to be crafted with an eye-catching design and packed with informative content to boost brand visibility, engage customers effectively, and ultimately pave the way for higher conversions and sales <cite class="ltx_cite ltx_citemacro_citep">(Hummell, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib30" title="">2006</a>)</cite>.
Hence, a successful promotional post requires high-level designer skills. Namely, designers need to iteratively create engaging visuals and effective messages, and thoughtfully blend the two into a harmonious whole.
</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Therefore, for design novices and everyday users, creating promotional posts is challenging in several aspects.
First, in the ideation stage, since users often draw inspirations from existing images <cite class="ltx_cite ltx_citemacro_citep">(Gonçalves et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib26" title="">2014</a>)</cite>, it is challenging for them to create novel alternatives from familiar visual narrations <cite class="ltx_cite ltx_citemacro_citep">(Truong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib68" title="">2006</a>)</cite>.
Previous research indicates that exploring examples helps users better understand their design direction and facilitates their ideation step from previous creations than starting from scratch <cite class="ltx_cite ltx_citemacro_citep">(Eckert and Stacey, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib21" title="">2000</a>; Bonnardel and Marmèche, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib8" title="">2005</a>; Siangliulue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib61" title="">2015a</a>)</cite>.
Thus, many users rely on existing image search engines (e.g., Google Image Search, Pinterest) to collect examples during the ideation stage; however, this process can be very time-consuming <cite class="ltx_cite ltx_citemacro_citep">(Siangliulue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib62" title="">2015b</a>)</cite>.
Researchers have developed tools (e.g., MetaMap <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib35" title="">2021</a>)</cite>, PopBlend <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib72" title="">2023</a>)</cite>) to support example exploration in the design ideation process, but the search results (design examples) cannot be flexibly edited or modified.
However, users still need to rely on additional retouching tools (e.g., Adobe Photoshop<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.adobe.com/ca/products/photoshop.html" title="">https://www.adobe.com/ca/products/photoshop.html</a></span></span></span>, Figma<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.figma.com/" title="">https://www.figma.com/</a></span></span></span>) to create and refine their designs.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Moreover, the coordination between images and captions is essential for creating coherent and persuasive promotional posts that effectively convey the intended brand message and campaign information.
However, matching the image to the message, or vice versa, can be a challenge in practice, due to the particular themes of the designed post. Without readily usable image resources, users often need to manually create an image to satisfy design constraints and align them with the theme <cite class="ltx_cite ltx_citemacro_citep">(Horphet and Srijongjai, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib29" title="">2020</a>)</cite>.
Similarly, users need to iterate the caption appropriately to match the image for accurate conveyance of the intended message.
These design hurdles escalate the threshold of promotional post design for non-professionals.
While there exists some image editing tools (e.g., Adobe Photoshop) and image captioning tools (e.g., jina<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jina.ai/" title="">https://jina.ai/</a></span></span></span>), they are either targeted at professional users or lack the flexibility and integrativeness for a streamlined unified workflow: e.g., images and captions need to be created separately and combined manually at a later stage. Furthermore, such a complex, tedious and time-consuming process makes it difficult for novice users to create a sufficient number of design alternatives to compare and evolve their post, which is essential for having a good design outcome.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address the challenges, we propose Influencer (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S0.F1" title="Figure 1 ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>), an AI-empowered interactive tool for users who do not have professional design skills but have needs to create promotional posts in their daily lives (e.g., individual creators, college students, small business operators, product managers, freelancers).
Drawing design goals from a formative study with five professionals, Influencer has been designed to streamline the promotional post design process, featuring four design modules: ideation, context-aware exploration, iterative customization, and delivery of multiple alternatives.
Specifically, it provides: 1) an example-based multi-dimensional recommendation framework to facilitate the exploration of related seed captions and images to inspire design ideas, 2) a context-aware exploration module that enables users to add complex design constraints including the requirement of matching color schema of brand image or product message, 3) flexible fusion for various design materials via LLMs and Generative Image AI models, and 4) a mind-map layout to organize ideas, track thought process, and present multiple design alternatives.
To evaluate Influencer, we conducted a controlled experiment with 12 design novices comparing our system with a baseline resembling the current workflow (Google + Figma). The results indicate that Influencer’s features were appreciated and the system in effective in helping users generate more creative and higher-quality promotional posts.
In summary, we make the following contributions:
</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">A comprehensive AI-infused pipeline for effective ideation and generation of promotional posts, which integrates multi-dimensional recommendation, context-aware exploration, and conceptual fusion mechanisms, for both images and captions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A novel interactive canvas-based tool enabled by the pipeline, Influencer, which facilitates users with designing promotional posts and organizing them in a mind-map-like layout to track their thought processes.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">A comparative evaluation of Influencer with design novices and gained insight into whether and how the tool is effective in supporting promotional post design.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Example-based Design and Ideation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Past research indicates that exploring a wide range of design examples enables designers to gain the potential approaches for implementing their ideas <cite class="ltx_cite ltx_citemacro_citep">(Eckert and Stacey, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib21" title="">2000</a>)</cite>. An effective way to generate novel ideas is to utilize example-based exploration rather than starting from scratch <cite class="ltx_cite ltx_citemacro_citep">(Siangliulue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib61" title="">2015a</a>)</cite>.
By getting timely inspiration from these examples, designers can generate a lot more ideas than only relying on brainstorming. When individuals heavily rely on existing knowledge and examples before generating new ideas <cite class="ltx_cite ltx_citemacro_citep">(Jansson and Smith, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib33" title="">1991</a>)</cite>, they may encounter design fixation.
In addition, simply providing irrelevant inspirations can not effectively address this issue and can even influence design efficiency <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib12" title="">2017</a>)</cite>.
Hence, it is crucial to maintain a balance between interrelatedness and diversity in example-based recommendations to foster designers’ creativity <cite class="ltx_cite ltx_citemacro_citep">(Perttula and Sipilä, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib52" title="">2007</a>)</cite>.
Psychologists have investigated the relationship between human creativity and association <cite class="ltx_cite ltx_citemacro_citep">(Gough, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib27" title="">1976</a>; Merten and Fischer, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib47" title="">1999</a>)</cite>. They find that generating creative ideas demands a strong capability for finding an association, rather than only imitating past work <cite class="ltx_cite ltx_citemacro_citep">(Brown, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib9" title="">2008</a>)</cite>.
Based on previous examples, experienced designers are more adept at using analogical reasoning to link different concepts than novices <cite class="ltx_cite ltx_citemacro_citep">(Bonnardel and Marmèche, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib8" title="">2005</a>)</cite>.
Thus, novices need more robust support to innovate and combine concepts, which is also essential for promotional post design.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">To facilitate this process, researchers have published free association datasets to simplify the ideation process among the general public <cite class="ltx_cite ltx_citemacro_citep">(De Deyne et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib19" title="">2019</a>)</cite>. The small world project<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://smallworldofwords.org/en/project/home" title="">https://smallworldofwords.org/en/project/home</a></span></span></span> is currently the largest dataset for word associations in English with more than 12,000 cue words, supporting users’ conceptual level ideation. It demonstrated the importance of facilitating the ideation process at the concept level in promotional post design <cite class="ltx_cite ltx_citemacro_citep">(Bonnardel and Marmèche, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib8" title="">2005</a>; Jansson and Smith, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib33" title="">1991</a>)</cite>.
Tools have also been proposed to leverage the example-based approach to facilitate ideation in design activities such as generating compound icons <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib77" title="">2020</a>)</cite> and visual metaphors <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib35" title="">2021</a>)</cite>.
Our work is grounded by the findings in the above literature and proposes a novel multi-dimensional ideation method by analyzing the characteristics of images and captions.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Image and Caption Recommendations</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Promotional post designs serve as powerful means for marketing and editorial graphics, enabling designers to capture audiences and transmit messages in an engaging and symbolic manner <cite class="ltx_cite ltx_citemacro_citep">(Khan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib36" title="">2021</a>)</cite>.
Such designs require the seamless integration of a promotional image and its caption <cite class="ltx_cite ltx_citemacro_citep">(Tiggemann, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib65" title="">2022</a>)</cite>.
Jeong’s pioneering work <cite class="ltx_cite ltx_citemacro_citep">(Jeong, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib34" title="">2008</a>)</cite> mentions that posts with visual metaphors are more persuasive compared to posts with literal (non-metaphorical) images.
Therefore, we decide to use visual metaphors as our main image resource in our promotional post design system. We here introduce prior work on recommendations regarding images and captions.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">To ease the promotional post design process, various tools have been developed to assist designers in crafting high-quality images.
It is generally required to identify the analogy between two images to have a better chance of finding related images <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib35" title="">2021</a>)</cite>.
The most common correlation between images is semantic relevance, but an analogy may also involve other topics and categories besides semantic relevance.
Apart from semantic analogy, both color <cite class="ltx_cite ltx_citemacro_citep">(Kim and Suk, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib37" title="">2017</a>)</cite> and object <cite class="ltx_cite ltx_citemacro_citep">(Peterson, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib53" title="">2019</a>)</cite> play important roles in establishing metaphorical relationships. Lucero et al. <cite class="ltx_cite ltx_citemacro_citep">(Lucero, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib44" title="">2009</a>)</cite> found color distribution creates a specific atmosphere for the audience.
Prior research has explored the semantic meanings of colors <cite class="ltx_cite ltx_citemacro_citep">(Jahanian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib32" title="">2017</a>; Kim and Suk, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib37" title="">2017</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib74" title="">2018</a>)</cite>, aiming to establish links between colors and semantics. Researchers also highlighted the usefulness of color features in representing objects and conveying underlying semantic messages <cite class="ltx_cite ltx_citemacro_citep">(Kim and Suk, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib37" title="">2017</a>)</cite>. Besides, objects are also important when recommending related images, and designers tend to seek similar elements with similar objects for potential image exploration <cite class="ltx_cite ltx_citemacro_citep">(Gkiouzepas and Hogg, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib25" title="">2011</a>)</cite>.
Therefore, in our work, we seek to use a three-dimensional recommendation framework (semantic, color, and object) for image exploration in the ideation step.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Moreover, prior research has shown that creating interesting and related captions can improve interaction and the sharing of content on promotional post design <cite class="ltx_cite ltx_citemacro_citep">(Jaakonmäki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib31" title="">2017</a>; Yu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib76" title="">2024</a>)</cite>. Traditional caption generation methods <cite class="ltx_cite ltx_citemacro_citep">(Ren et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib57" title="">2024</a>; Srivatsan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib63" title="">2023</a>)</cite> for posts mainly use natural language techniques to create captions just from the picture, not considering the post’s actual use and focusing only on one aspect.
However, these methods do not fully meet the varied requirements of different types of posts.
We propose a novel framework that employs a multidimensional strategy for caption recommendation in post design.
We separate captions in promotional posts into three main topics: activity, product, and advertisement.
Each of these themes serves a distinct purpose in engaging the audience and driving desired actions.
Activity-focused promotional posts entail content that centers on engaging actions or events.
They aim to grab the audience’s attention by showcasing active happenings, events, or efforts linked to a brand or group <cite class="ltx_cite ltx_citemacro_citep">(Schultz, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib59" title="">2017</a>)</cite>.
Product-related promotional posts seek to convey the attributes, advantages, and value of particular products or services. This marketing strategy aims to attract potential consumers and encourage purchase decisions <cite class="ltx_cite ltx_citemacro_citep">(Banerjee, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib6" title="">2009</a>)</cite>. Advertisement-focused promotional posts encompass broader marketing and branding initiatives <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib38" title="">2021</a>)</cite>. They aim to promote the brand as a whole, establish its identity, and convey its unique selling propositions.
In our work, we incorporate caption recommendation in three dimensions (activity, product, advertisement) in Influencer to facilitate the caption exploration in promotional post design.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Promotional Post Design Systems</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">There are a variety of off-the-shelf tools that support post design with different focuses, such as professional image editing (e.g., Adobe Photoshop), collaborative UI design (e.g., Figma), and user-friendly graphic design (e.g., Canva). While all these tools provide powerful post image editing features from template-level to object-level, they have a steep learning curve for novice designers and require much manual user effort. Thus, the research community has been exploring AI-driven technologies for design assistance.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">In particular to the design of promotional posts, customization techniques are widely explored to facilitate and accelerate promotional post design.
Most of the existing research has primarily focused on image customization in the post design scenario.
Common image customization techniques include masking and editing specific areas, generating new images based on a similar prompt context, and regenerating parts of images for variations <cite class="ltx_cite ltx_citemacro_citep">(Ramesh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib56" title="">2021</a>)</cite>.
More recent works include AnyDoor <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib14" title="">2023a</a>)</cite> which used a diffusion-based deep learning method to support object-level image customization. Some works also try to incorporate text or image concepts to customize target images.
For example, Chen et al. <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib13" title="">2023b</a>)</cite> enabled various prompt inputs by incorporating a dual-branch conditioning mechanism to customize images in different concepts.
Further, Kumari et al. <cite class="ltx_cite ltx_citemacro_citep">(Kumari et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib39" title="">2023</a>)</cite> supported composing multiple concepts to generate images based on the given texts.
On the other hand, beyond image customization, the generation and customization of captions for promotional posts also facilitate the promotional post design process.
Large Language Models (LLM) including ChatGPT <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib49" title="">2023a</a>)</cite>, Llama <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib67" title="">2023</a>)</cite>, and Claude <cite class="ltx_cite ltx_citemacro_citep">(Anthropic, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib4" title="">2023</a>)</cite> have been proposed to customize caption by rewriting the prompt based on user’s need.
More recent studies have used Context Sequence Memory Network to customize descriptive captions and predict hashtags based on the query images <cite class="ltx_cite ltx_citemacro_citep">(Chunseong Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib18" title="">2017</a>; Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib51" title="">2018</a>)</cite>.
In this work, we integrate multiple customization techniques such as regenerating images with similar prompt context and mask editing images into our tool to improve the promotional image design process.
We also consider a simplified and practical interaction technique for promotional post customization, enabling users to conceptually customize captions and images through drag-and-drop selected images or texts as context prompts on a mind map.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Additionally, researchers have proposed various intelligent tools to help novices automate parts of the design workflow. For example, there has been a variety of work in domain-specific applications with compound design tasks. Yin et al. <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib75" title="">2013</a>)</cite> developed a system for automatically generating magazine-like visual summaries from traditional social media posts for efficient mobile browsing.
Qiang et al. <cite class="ltx_cite ltx_citemacro_citep">(ting Qiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib66" title="">2017</a>)</cite> introduced a graphical model that learns to generate scientific posters from research papers.
Tyagi et al. <cite class="ltx_cite ltx_citemacro_citep">(Tyagi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib69" title="">2022</a>)</cite> proposed a tool to generate infographics from a user sketch.
Shi et al. <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib60" title="">2023</a>)</cite> developed a holistic color authoring system that supports 2D palette extraction, theme-aware, and spatial-sensitive color.
Zhao et al. <cite class="ltx_cite ltx_citemacro_citep">(Zhao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib77" title="">2020</a>)</cite> created an automated system that generates icon suggestions by considering semantics, style, and space to support compound icon design.
Recent work in aiding creative design <cite class="ltx_cite ltx_citemacro_citep">(Bae et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib5" title="">2020</a>)</cite> also adopts mind-maps to mimic the process of making associations among design materials.
However, all of the above tools mainly focus on the ideation step of design, requiring designers to switch to other tools for the actual design work. Also, none of the above tools accommodates user-specified prompt or context images (e.g. brand image) based on design constraints, which are essential to maintain consistency and effectively communicate with the target audience.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Formative Study</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our primary target audience is design novices such as product managers, marketers, small business operators, freelance creators, or anyone who needs to quickly make attractive promotional posts without professional training.
To better understand their needs, we conducted a formative study, and here we describe its setup and yielded design goals that have guided the development of Influencer.
</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Setup</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We recruited five professionals (two women and three men, aged 20-30), including two design researchers in the IT company, a marketer experienced in designing promotional posts, and two software engineers who collaborate with product managers, marketers, and designers on product promotion.
The study included a 30-minute semi-structured interview with each participant.
The interview questions covered how they would normally design promotional posts, including how they got their inspiration, and how they would craft and iterate the posts. They were also asked to identify difficulties they encountered throughout the design process and raise their needs for support.
The interviews were audio recorded and then transcribed for further analysis. All the authors used the open-coding method to independently conduct a thematic analysis of the interview transcripts and notes. Then, all the authors reviewed and synthesized the results to ensure a comprehensive understanding of the findings.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Formulating Design Goals</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Based on our formative study, we distill the following design goals to inform our development of Influencer. We refer to the participants in our interviews as E#.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.1. </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.1.1">R1: Help explore images and captions in diverse dimensions</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS1.p1">
<p class="ltx_p" id="S3.SS2.SSS1.p1.1">Designers create new ideas through association and recombination from previous examples <cite class="ltx_cite ltx_citemacro_citep">(Bonnardel and Marmèche, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib8" title="">2005</a>)</cite>. Traditional methods to collect inspiring examples of images and captions for the ideation stage include searching online resources from search engines (e.g., Google Search), which is usually cumbersome and time-consuming. The participants confirmed that they all seek inspiration from others’ works, including content, color schema, style, and composition. Exposure to rich information often sparks new ideas <cite class="ltx_cite ltx_citemacro_citep">(Leonard and Rayport, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib41" title="">1997</a>)</cite>. However, when talking about searching exemplar images and captions for inspiration, two design novices (E1, E3) reported that they did not have effective techniques for using search engines for this purpose. Four participants (E1-2, E4-5) suggested a diverse example-based image recommendation would be desirable in this task.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p2">
<p class="ltx_p" id="S3.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p2.1.1">R1.1: Image recommendation.</span>
Image recommendation plays a pivotal role in inspiring visual design for promotional posts by offering a wide array of imagery to draw inspiration. However, traditional methods lack multi-dimensional exploration of the image beyond basic keywords or tags, overlooking crucial aspects such as object, content, and semantic meaning. E1 mentioned: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.2">“Searching images on the search engine results in many irrelevant or useless images. I often spend a lot of time on image selection.”</span>
Moreover, creativity researchers <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib35" title="">2021</a>)</cite> demonstrated that multi-dimensional image recommendation can help explore visual metaphors including semantics, color, and shape, which was confirmed by E2. In addition to images with similar semantics and color, designers desire images that share similar objects. However, they are not particularly inclined towards images with similar shapes.
E3 commented: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p2.1.3">“I find the images with similar objects more useful than just similar shapes. It’s about making that connection and conveying the right message in our promotional stuff. Shapes alone don’t tell the story.”</span> The participants’ feedback suggests that an example-based multi-dimensional recommendations (i.e., semantic, color, and object) should be incorporated into the image recommendation module.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS1.p3">
<p class="ltx_p" id="S3.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS1.p3.1.1">R1.2: Caption recommendation.</span>
The caption in a promotional post is greatly determined by its design intention and usage scenarios <cite class="ltx_cite ltx_citemacro_citep">(Jaakonmäki et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib31" title="">2017</a>)</cite>. For instance, E4 said, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p3.1.2">“In a promotional post, captions should set the scene, provide context, and align with the campaign’s message. It’s about understanding that a caption serves multiple roles.”</span>
E5 expressed frustrations towards the traditional approach to caption recommendation and creation for promotional posts: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS1.p3.1.3">“Simple captions aren’t enough to make our products stand out, engage our audience, and tell a great ad story; we need captions that truly speak to every aspect and make our content stand out.”</span> Considering the influence of context and its role in shaping the captions in the post, a system should establish a multi-dimensional understanding of the design intention (such as the product, activity, and advertisement of the promotion). Thus, multi-dimensional caption recommendations could ease the users’ creative process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.2. </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.1.1">R2: Support context-aware exploration considering brand messages and user-specified design constraints</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS2.p1">
<p class="ltx_p" id="S3.SS2.SSS2.p1.1">Complex post design requirements can be overwhelming for design novices <cite class="ltx_cite ltx_citemacro_citep">(Evans, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib22" title="">2004</a>)</cite>. Existing off-the-shelf images and caption recommendation tools can offer some inspiration to users <cite class="ltx_cite ltx_citemacro_citep">(Feng and Lapata, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib23" title="">2012</a>; Choi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib16" title="">2023</a>; Gal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib24" title="">2022</a>; Handayani, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib28" title="">2015</a>)</cite>. However, there are two unaddressed opportunities: 1) understanding the semantics of the image, and 2) accommodating user-specified design constraints, such as color schemes and related product or branding requirements. Although it was shown in R1 that using a multi-dimensional recommendation method can effectively inspire design novices with new ideas, such a method falls short when designers have specific design constraints such as requiring the right color schema to fit with the brand or product image or making sure captions fit the brand’s values. Thus, a more advanced image and caption recommendation/modification mechanism is needed for iterative ideation.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p2">
<p class="ltx_p" id="S3.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p2.1.1">R2.1: Image-aware.</span>
When given specific images for material exploration in promotional post design, it is essential to engage in iterative ideation to ensure the exploration aligns with these images.
E2 confirmed that <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p2.1.2">“It’s crucial to update material exploration from multiple dimensions like semantics, color schemes, objects, and usage scenarios based on the given images. This can save time in searching for materials that are compatible with the provided images.”</span> Thus, if given related images, context-aware exploration should be conducted, ensuring that the exploration is updated to align with these images.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS2.p3">
<p class="ltx_p" id="S3.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS2.p3.1.1">R2.2: Prompt-aware.</span>
A promotional post design often has different demands based on different goals, such as advertising, entertaining, and informing <cite class="ltx_cite ltx_citemacro_citep">(Batra and Keller, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib7" title="">2016</a>)</cite>. Designing different prompts based on varying needs to update material exploration may help designers engage in more directed design efforts. E4 emphasized this requirement in our formative study: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS2.p3.1.2">“Using prompts efficiently helps narrow down the search to exactly what you need, so you don’t have to start your search over for different requirements.”</span>
Therefore, material exploration for promotional post design needs prompts to support iterative ideation by conveying user-specific demands.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.3. </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.1.1">R3: Flexible fusion of various images and captions to customize designs</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS3.p1">
<p class="ltx_p" id="S3.SS2.SSS3.p1.1">Four participants suggested that it would be efficient to conceptually fuse different elements via dragging and dropping. <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p1.1.1">“It’s like using one element as a prompt background to customize another element on the canvas, making it easy for designers to create prompts and upload images to customize the target image or caption through conceptual fusion.”</span>
Users have various preferences and expectations in different scenarios of the promotional post design processes. Hence, it is necessary to provide users with the right amount of customization in the mind-map interface. The tool should make customization easier by automating tedious steps and offering creative fusion options so that they can rapidly enable design iterations and deliver design alternatives. According to our formative study, we identified the following key aspects related to material fusion.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p2">
<p class="ltx_p" id="S3.SS2.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p2.1.1">R3.1: Image-caption fusion.</span>
In promotional post design, an artful fusion of images with precisely crafted text messages is essential. This dynamic combination allows for the creation of a compelling narrative, enhancing audience engagement and guiding their perception of the product or message being promoted. As E5 mentioned: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p2.1.2">“Pairing the right caption or prompt with a thoughtfully chosen image can create marketing magic.”</span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p3">
<p class="ltx_p" id="S3.SS2.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p3.1.1">R3.2: Image-image fusion.</span>
Strategically integrating a variety of images is a powerful technique for crafting visually stunning promotional posts. By combining different images, a story can be woven, emotions can be evoked, or a product’s unique features can be effectively highlighted. E2 emphasized: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p3.1.2">“Images engaging in a visual dialogue on the canvas is key. It’s about selecting images that complement each other, conveying a unified message to the audience.”</span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p4">
<p class="ltx_p" id="S3.SS2.SSS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p4.1.1">R3.3: Prompt-caption fusion.</span>
The collaboration of a well-crafted prompt with a compelling caption yields an impactful promotional post. The prompt acts as a guide, steering the direction of the caption and ensuring it aligns with the intended message or brand voice. E1 echoed, <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p4.1.2">“The prompt is like a catalyst for crafting the perfect caption. It triggers creativity and ensures the caption hits the mark, captivating attention and driving the desired response from the audience.”</span></p>
</div>
<div class="ltx_para" id="S3.SS2.SSS3.p5">
<p class="ltx_p" id="S3.SS2.SSS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS3.p5.1.1">R3.4: Prompt-image fusion.</span>
Many design novices struggle with how to edit images to achieve their desired outcomes. They often lack design expertise and are not proficient with effective image customization with tools. As E2 mentioned: <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS3.p5.1.2">“Design novices like us don’t really know how to proceed with the editing to get the desired effect. If I want to change the background or the main color of one image, I don’t know how to use related design tools to implement it.”</span> Hence, the tool should allow users to employ the prompt to customize the image by easily ataching the text prompt to the target image.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.2.4. </span><span class="ltx_text ltx_font_bold" id="S3.SS2.SSS4.1.1">R4: Design a mind-map layout to organize ideas and track the thought process.</span>
</h4>
<div class="ltx_para" id="S3.SS2.SSS4.p1">
<p class="ltx_p" id="S3.SS2.SSS4.p1.1">According to our interview, designers can easily get lost in iterative exploration of images and captions during ideation without tracking their exploration path. Canvas lets designers freely create by allowing them to make, move, track, and link elements anywhere <cite class="ltx_cite ltx_citemacro_citep">(Staiano, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib64" title="">2022</a>)</cite>.
Previous work also indicates that tracking the thinking path with mind-maps during exploration is important for ideation <cite class="ltx_cite ltx_citemacro_citep">(Dong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib20" title="">2021</a>)</cite>. E3 suggested that <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS4.p1.1.1">“mind-map is one way to connect and recombine materials to generate new ideas.”</span>
Maintaining a thorough history of their creative process is a crucial design principle that aids in fostering creativity. E1, E2, and E4 suggested that <span class="ltx_text ltx_font_italic" id="S3.SS2.SSS4.p1.1.2">“When updating material exploration by uploading images or creating prompt texts, using links to show that an update has been made. It’s like marking our trail, so we know where we’ve been.
The same goes for the flexible blending feature; a link means we modified the target material using flexible blending.
”</span>
Thus, the tool should support thought process tracking according to users’ design actions.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Influencer Design Overview</h2>
<figure class="ltx_figure" id="S4.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="524" id="S4.F2.g1" src="x1.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Influencer provides three basic types of interactive blocks on its mind-map canvas. Clicking <span class="ltx_text ltx_font_italic" id="S4.F2.4.1">Create New Text Block</span> adds (A) an empty text block. Clicking <span class="ltx_text ltx_font_italic" id="S4.F2.5.2">Upload Image</span> generates (B) an empty image block. Clicking <span class="ltx_text ltx_font_italic" id="S4.F2.6.3">Create Post</span> produces (C) a blank post blockthat can fuse selected images and captions.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F2.7">\Description</span></div>
</div>
</figure>
<figure class="ltx_figure" id="S4.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="290" id="S4.F3.g1" src="extracted/5743646/Figure/System_Pipeline.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>System architecture and workflow of Influencer. From an input topic, the system first analyzes the input to return searched images (H) and the user can select a seed image from the search results. At the same time, the Caption Recommendation (A) digests the input topic and recommends captions from three dimensions (i.e., product, activity, advertisement). Next, Influencer can conduct image recommendation (B) based on the seed image in three dimensions (i.e., semantic, object, color) and suggest images for each dimension (G). It also supports context-aware exploration (C) based on a brand image or product/brand message to find materials aligning with the brand. Besides, Influencer allows users to customize the images via Regenerate or Mast Edit (E) and conceptually fuse design materials (i.e., images and captions) with an image or prompt reference (D). Together, it generates an aesthetically pleasing promotional post design with harmonious content. A user can further iteratively explore more design recommendations (A, B, G) and generate multiple design alternatives to share on social media (F).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F3.1">\Description</span></div>
</div>
</figure>
<figure class="ltx_figure" id="S4.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="334" id="S4.F4.g1" src="x2.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Ideation with Influencer: (A) From a text block, a panel providing three options (<span class="ltx_text ltx_font_italic" id="S4.F4.8.1">Generate Images</span>, <span class="ltx_text ltx_font_italic" id="S4.F4.9.2">Generate Captions</span>, <span class="ltx_text ltx_font_italic" id="S4.F4.10.3">Generate Post</span>) for users to ideate captions or images and generate posts; (B) An assembly of image search results based on users’ input topic; (C) From an image block, a panel providing four options for users to customize (<span class="ltx_text ltx_font_italic" id="S4.F4.11.4">Regenerate</span>, <span class="ltx_text ltx_font_italic" id="S4.F4.12.5">Mask</span>), conduct image recommendation (<span class="ltx_text ltx_font_italic" id="S4.F4.13.6">More Images</span>), or produce post directly (<span class="ltx_text ltx_font_italic" id="S4.F4.14.7">Generate Post</span>); (D) A panel providing related semantic keywords to help users explore diverse materials; (E) An image recommendation block displaying results in three dimensions (i.e., semantic, object, color); (F) A caption recommendation block displaying results in three dimensions (i.e., product, activity, and advertisement).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F4.15">\Description</span></div>
</div>
</figure>
<figure class="ltx_figure" id="S4.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="377" id="S4.F5.g1" src="x3.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>Context-Aware exploration with Influencer: (A) Uploading a brand image and dragging to update the target image recommendation block and caption recommendation block; (B) Creating a text block with product message and dragging to update the target image recommendation block and caption recommendation block; (C) Updated image recommendation block; (D) Updated caption recommendation block.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F5.1">\Description</span></div>
</div>
</figure>
<figure class="ltx_figure" id="S4.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="456" id="S4.F6.g1" src="x4.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>Customization with Influencer: In the <span class="ltx_text ltx_font_italic" id="S4.F6.3.1">Edit</span> mode, an image (A) can be modified with “Mask” and “Regenerate” for customization (B). “Regenerate” produces a semantically similar new image (C). “Mask” allows for framing the part to customize with a prompt (D) and thus produces a new image (E).
In the <span class="ltx_text ltx_font_italic" id="S4.F6.4.2">Fusion</span> mode, images and/or text can be freely combined by dragging an image or text block with a prompt to the target image or caption (F, G, H, I).
</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F6.5">\Description</span></div>
</div>
</figure>
<figure class="ltx_figure" id="S4.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="904" id="S4.F7.g1" src="x5.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Post generation: A post can be generated based on a selected image (A) or caption (B) by clicking <span class="ltx_text ltx_font_italic" id="S4.F7.2.1">Generate Post</span>. Alternatively, a blank post block can be created to combine the selected image and caption (C).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.F7.3">\Description</span></div>
</div>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Based on the aforementioned design goals, we developed Influencer, an interactive, AI-empowered, and canvas-based tool that helps design novices in four main steps of promotional post design (i.e., ideation, context-aware exploration, customization, and post generation), all through a set of simple interactive blocks (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F2" title="Figure 2 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>).
<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F3" title="Figure 3 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 3</span></a> shows an overview of the system backend and workflow, which consists of five main modules: (A) <span class="ltx_text ltx_font_italic" id="S4.p1.1.1">Caption Recommendation</span>, (B) <span class="ltx_text ltx_font_italic" id="S4.p1.1.2">Image Recommendation</span>, (C) <span class="ltx_text ltx_font_italic" id="S4.p1.1.3">Context-Aware Exploration Mechanism</span>, (D) <span class="ltx_text ltx_font_italic" id="S4.p1.1.4">Flexible Fusion Mechanism</span>, and (E) <span class="ltx_text ltx_font_italic" id="S4.p1.1.5">Image Regenerate and Mast Edit Module</span>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">In particular, as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>, users’ material (image and caption) searching process is supported through related concepts based on their search input. After identifying an image or caption of interest, users can further explore the recommendation images based on three features: semantics, color, and object, and explore the recommendation captions in three contexts: product, activity, and advertisement (<span class="ltx_text ltx_font_bold" id="S4.p2.1.1">R1</span>). As shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F5" title="Figure 5 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>, users can conduct context-aware exploration for captions and images by dragging brand messages (i.e., brand image, product message, and brand value) to the caption and image recommendation block (<span class="ltx_text ltx_font_bold" id="S4.p2.1.2">R2</span>). Users can further contextually customize design materials by fusing any images and captions during the promotional post design, as shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a> (<span class="ltx_text ltx_font_bold" id="S4.p2.1.3">R3</span>). All the activities are conducted on a mind-map which records the thinking path generated by users for a quick recollection (<span class="ltx_text ltx_font_bold" id="S4.p2.1.4">R4</span>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>User Interface and Blocks</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The user interface of Influencer is based on an interactive canvas allowing users to create three types of blocks: text block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F2" title="Figure 2 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.A), image block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F2" title="Figure 2 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.B), and post block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F2" title="Figure 2 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.C), as well as offering two types of recommendation blocks: image and caption.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">The image block provides four options for users to further explore, customize, or utilize images in their promotional post design: 1) “Regenerate Image” (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.B) to produce a similar image with the same context,
2) “More Images”(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.B) to browse recommended images based on the selected image in three dimensions (i.e., semantics, color, object) in a image recommendation block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.E), which can further obtain images along the branches (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.C),
3) “Mask Edit” (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.B) to edit an image by drawing a mask and providing a prompt for generating a new image,
and 4) “Generate Post”(<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F7" title="Figure 7 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>.A) to craft a post in a post block directly based on the current image.
To minimize user interaction for efficiency, Influencer apply every user selected or uploaded images with those four options to streamline any design activities (i.e., ideation, context-aware exploration, customization, and post generation) from the block.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The text block provides three options (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.A) for users to ideate captions or images and generate post during the design process:
1) “Generate Images” to obtain an assembly of image search results based on the given topic,
2) “Generate Captions” to explore recommended captions in three context dimensions (i.e., product, activity, advertisement) in a caption recommendation block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.F), which can keep expanding along any branch,
and 3) “Generate Post” to directly produce the post based on their input text in a post block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F7" title="Figure 7 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>.B).</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">In Influencer, images or text can be as context to help users do further exploration. Users can create a text block writing brand message or the requirement of the promotional post design and drag it to the image (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.E) or caption recommendation block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.F). In this way, users can get new image or caption recommendation aligning with the prompt written in the text block. Also, users can create an image block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F2" title="Figure 2 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>) and upload the context image like brand image and drag it to the image (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.E) or caption recommendation block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.F). Then user will get a new image and caption recommendation block aligning with his uploaded image with the similar semantic meaning and color schema. At the same time, users can also drag a text block or an image block and connect it to any target image block, text block for conceptual customization.
Such customization and context-aware exploration are shown in orange arrows to differentiate from the users’ initial exploration paths that are shown in blue arrows.
Furthermore, users have the option to not only generate posts based on a single image or caption but also select their preferred image and caption directly on the canvas and fuse them to a post block for production.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Usage Scenario</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Before diving into our approach and details, we explain how the design novices use Influencer via a simple usage scenario.
Suppose Crystal is a micro-entrepreneur who owns a small grocery store, and she needs to promote a new product, homemade orange juice.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Crystal launches Influencer. She first creates a Text Block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F2" title="Figure 2 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>.A) on the canvas and wants to explore images related to her promotional topic: “homemade juice.” She clicks the “…” button that shows a list of exploration and post generation options (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.A).
From there, she clicks the “Generate Images” button (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.A) to create a search image block (B), consisting of a set of images related to the given topic.
After identifying an image of interest, she further explores the recommendation images based on 3-dimensional features (semantics, color, and object) by clicking on the “More Image” button (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.C). After clicking this button, Influencer requests her to select a related semantic keyword from the semantic keyword panel (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.D) (<span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">R1</span>). Crystal wants the focus of the promotion to be on health. Then she click Health from (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.D) and create an image recommendation block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.E) for more images in health semantic meaning.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Crystal looks at the images from the image recommendation block and finds a satisfied image in (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>) but thinks <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.1">“The image’s meaning is closely related to my topic, but I need more alternatives with similar meaning and want to change the type of juice shown.”</span>
Next, she click the “Regenerate” (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.B) to generate an image with the same semantic meaning (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.C). She also wants to try to click “Mask Edit” button (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.D) to get more alternatives. After clicking “Mask Edit” button, she clicks “Mask” button from (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.D) to frame out the mask she wants to replace and writes the prompt in (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.D). Then she clicks the “Generate” button to generate a new image in (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.E).</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">At the same time, Crystal wants to further explore other alternative paths of design, since she does not have sufficient design training and wants to see if there are better options. Then she tries another path to explore useful captions for her promotional post.
She thus clicks the “Generate Captions” button (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.A) to obtain a caption recommendation block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F4" title="Figure 4 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 4</span></a>.F).
She checks this caption recommendation block and finds one caption suitable for her post.
She clicks this caption which generates a text block including this caption and edits the text based on her preference (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F7" title="Figure 7 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>.C). Then, she directly generates a post by clicking the “Generate Post” button (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F7" title="Figure 7 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>.C).
Through all the above actions, she creates multiple post alternatives for her design and she is quite satisfied (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F7" title="Figure 7 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>).</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">However, she thinks the promotional image needs to align with the brand image (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F5" title="Figure 5 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.A) of her grocery and some new descriptions of the promoted product (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F5" title="Figure 5 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.B).
Using brand images or new descriptions directly to find materials may ignore previous searches and context, potentially leading to recommendations that do not match the original intent and needs.
Therefore, Crystal implements a context-aware exploration to find more materials for post design based on the previous search. She links the brand images and the text block with new product descriptions to the caption and image recommendation block.
Influencer returns newly recommended images and captions in the image (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.C) and caption recommendation block (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F5" title="Figure 5 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 5</span></a>.D) from multiple dimensions which align with the color scheme, object, and semantic meaning of the brand image and the new product descriptions. <span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.1">“Nice! I got new materials for the post design!”</span> Crystal and feels happy about the efficiency.
Using similar interactions above, she continues to customize and generate some alternative designs (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F7" title="Figure 7 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 7</span></a>.ABC) (<span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.2">R2</span>).</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">Crystal is satisfied with the content of the current image design but feels styles are less appealing when compared with another image despite not liking its content(e.g., the left two images in<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.F).
She wants to combine these two images; thus she connects this image as a background to that seed image for conceptual fusion (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.F).
A new image is then generated with a style similar to the background image and the same semantic meaning as the seed image (<span class="ltx_text ltx_font_bold" id="S4.SS2.p6.1.1">R3</span>). <span class="ltx_text ltx_font_italic" id="S4.SS2.p6.1.2">“Looks better!”</span> she thinks.
If there is no satisfactory image on the canvas, she can also create a text block with the style description and drag it to the seed image for the above conceptual fusion. For example, she drags a text block with the text of “orange tree as background” to the seed image, Influencer generates a new image featuring an orange tree background while retaining the orange juice bottle from the seed image (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.G).
Besides, Influencer supports conceptual fusion on captions by dragging existing images or text blocks with prompts to customize target captions (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.HI).
Thus, Crystal drags a text block with a prompt suggesting “write the caption more energetic” in the target caption (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F6" title="Figure 6 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 6</span></a>.I). She takes a look at the customized caption: <span class="ltx_text ltx_font_italic" id="S4.SS2.p6.1.3">“Awesome! That was quick. This caption is exactly what I needed!”</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1">During the process, Crystal finds thinking path very useful. When she get lost in the promotional post design process, the tracked thinking path can remind her of the previous exploration and customization (<span class="ltx_text ltx_font_bold" id="S4.SS2.p7.1.1">R4</span>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Influencer System</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we describe the backend of the Influencer system in detail, particularly on the implementation of the multi-dimensional recommendation, context-aware exploration, and multiple fusion mechanisms (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S4.F3" title="Figure 3 ‣ 4. Influencer Design Overview ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Dataset Building</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Previous work emphasizes the importance of using associations from previous examples to create new ideas <cite class="ltx_cite ltx_citemacro_citep">(Wilkenfeld and Ward, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib73" title="">2001</a>; Bonnardel and Marmèche, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib8" title="">2005</a>)</cite>. Besides, based on previous work and our formative study results, we find that the creation of promotional images involves analogy between all kinds of information <cite class="ltx_cite ltx_citemacro_citep">(Carroll, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib10" title="">1994</a>)</cite>, including semantic <cite class="ltx_cite ltx_citemacro_citep">(Petridis and Chilton, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib54" title="">2019</a>; Phillips and McQuarrie, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib55" title="">2004</a>)</cite>, color<cite class="ltx_cite ltx_citemacro_citep">(Kim and Suk, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib37" title="">2017</a>)</cite>, and object <cite class="ltx_cite ltx_citemacro_citep">(Gkiouzepas and Hogg, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib25" title="">2011</a>)</cite>. To build the post image dataset with reasonable associations between each example, we performed various preprocessing steps to prepare our dataset.
Specifically, we use Small World Dataset <cite class="ltx_cite ltx_citemacro_citep">(De Deyne et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib19" title="">2019</a>)</cite> to get a keyword association dataset and collect a Post Image dataset following Kang et al. <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib35" title="">2021</a>)</cite>’s approach with further refinement.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Key Association dataset.</span>
The Small World Dataset <cite class="ltx_cite ltx_citemacro_citep">(De Deyne et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib19" title="">2019</a>)</cite> is the largest English free word association resource, containing over 12,000 cue words. This dataset captures how people remember and recall concepts through word associations <cite class="ltx_cite ltx_citemacro_citep">(Ma, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib45" title="">2013</a>)</cite>. It includes responses from 100 participants who provided three associated words for each cue, leading to a comprehensive collection of word associations. Then association strengths were calculated based on the frequency of each response relative to the total number of responses <cite class="ltx_cite ltx_citemacro_citep">(Cattle and Ma, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib11" title="">2017</a>)</cite>. We then formed the keyword association dataset as a directed graph, with forward associations (from responses to cues) and association strengths as weights. It includes words within two association distances from the topic words, creating a dataset with 7,407 words.</p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Post Image Collection.</span> Inspired by <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib35" title="">2021</a>)</cite>, we used Pinterest as our main image source for novice designers since it is the most popular website for them to find image examples. Kang et al. <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib35" title="">2021</a>)</cite> built a keyword-image dataset utilizing the keyword association dataset collected before with 4,861 descriptive words and 76,686 images where images mainly focus on the advertising creative images. Since advertising creative images are a key source for promotional post design <cite class="ltx_cite ltx_citemacro_citep">(Vanolo, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib70" title="">2008</a>)</cite>, we updated the dataset based on feedback from our formative study to include object dimension which is more valued by users. This helps design novices have a clearer idea and examples of how to create promotional posts. To identify related objects within our dataset, we fine-tuned the MMDetection model using SAHI <cite class="ltx_cite ltx_citemacro_citep">(Laurer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib40" title="">2022</a>)</cite> and applied it to each image in the dataset sourced from <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib35" title="">2021</a>)</cite>. The image-object relationship tables were added to the dataset to help designers to explore.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Image and Caption Recommendation</h3>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.1.1">Image Recommendation</span>
</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">Influencer recommends images from three dimensions (i.e., semantics, color, and object) to provide related and diverse examples, as detailed below.
Therefore, we extract the corresponding features from a seed image and get the top-ranked images. Due to space limitations of the block, we display the top four images in each recommended dimension and allow designers to scroll left and right to view more images.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.p2.1.1">(a) Semantic.</span> We propose distinct methods for semantic image recommendation based on two types of images: those available in the dataset and those uploaded by users. For images within the dataset, we have already identified highly related concept words through the computation of concreteness and imageability scores (ranging from 0 to 1) sourced from <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib43" title="">2014</a>)</cite>, collected through human annotation and synonym expansion. Our approach prioritizes words associated strongly with perceptible concepts and evoking mental imagery. According to our formative study, designers mentioned the importance of incorporating user-uploaded images to enrich the exploration of design materials in promotional post design. To address this, we leverage the state-of-the-art image captioning model in framework OFA <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib71" title="">2022</a>)</cite> to extract key information from the uploaded image. Additionally, we fine-tune the cutting-edge text classification models mDeBERTa-v3 <cite class="ltx_cite ltx_citemacro_citep">(Laurer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib40" title="">2022</a>)</cite> to identify the highly related concept words corresponding to the uploaded image. After getting the highly related concepts, the top four images are randomly selected from the qualified candidates. By doing so, we increase the diversity of the returned images when the semantic correlation remains the same.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p3">
<p class="ltx_p" id="S5.SS2.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.p3.1.1">(b) Color.</span> Our approach involves suggesting images with analogous colors by analyzing the dominant color from the image. ColorThief<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://github.com/lokesh/color-thief</span></span></span> is adopted to retrieve the main theme color of the image. To ensure a speedy search, we search images from related concepts.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p4">
<p class="ltx_p" id="S5.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS1.p4.1.1">(c) Object.</span> Influencer recommends images with similar objects based on object detection. To facilitate the search speed and make sure that the image has some semantic relationship with the original image, we only search images in the related concepts. MMDetection models leveraging SAHI <cite class="ltx_cite ltx_citemacro_citep">(Akyon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib3" title="">2022</a>)</cite> is used to extract the main object and retrieve the top probable items from the image. The top four images with similar objects are then recommended. The reason of using the SAHI framework is that there are many small but important objects in the promotional image based on our interview.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span><span class="ltx_text ltx_font_bold" id="S5.SS2.SSS2.1.1">Caption Recommendation</span>
</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">Addtionally, Influencer recommends captions from three dimensions (product, activity, advertisement) to provide related and diverse examples. Specifically, activity-focused posts emphasize engaging actions or events, product-focused posts describe the features and benefits of goods or services, and advertisement-focused posts address broader marketing and branding efforts.
Based on the analysis of promotional posts on Instagram <cite class="ltx_cite ltx_citemacro_citep">(Chu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib17" title="">2017</a>)</cite>, there are five themes in total including product, activity, advertisement, text, and others. Based on their statistic of categories and the results of our formative study, we recommend our captions in three contexts (i.e., product, activity, and advertisement). We use GPT-4 to generate caption recommendations based on the user input prompt <math alttext="T_{p}" class="ltx_Math" display="inline" id="S5.SS2.SSS2.p1.1.m1.1"><semantics id="S5.SS2.SSS2.p1.1.m1.1a"><msub id="S5.SS2.SSS2.p1.1.m1.1.1" xref="S5.SS2.SSS2.p1.1.m1.1.1.cmml"><mi id="S5.SS2.SSS2.p1.1.m1.1.1.2" xref="S5.SS2.SSS2.p1.1.m1.1.1.2.cmml">T</mi><mi id="S5.SS2.SSS2.p1.1.m1.1.1.3" xref="S5.SS2.SSS2.p1.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS2.p1.1.m1.1b"><apply id="S5.SS2.SSS2.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS2.SSS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS2.SSS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.2">𝑇</ci><ci id="S5.SS2.SSS2.p1.1.m1.1.1.3.cmml" xref="S5.SS2.SSS2.p1.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS2.p1.1.m1.1c">T_{p}</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS2.p1.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> with the following prompt:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="S5.SS2.SSS2.p1.2">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,ICAgIFByb21wdDogR2VuZXJhdGUgdGhyZWUgcHJvbW90aW9uYWwgY2FwdGlvbnMgZm9yIGVhY2ggZGltZW5zaW9uIChwcm9kdWN0LCBhY3Rpdml0eSwgYWR2ZXJ0aXNlbWVudCkgYmFzZWQgb24gdGhlIGdpdmVuIHRleHQ6IDxUX3A+LiBwbGVhc2UgYWxzbyBoaWdobGlnaHQgdGhlIGtleXdvcmRzIHdpdGggYXN0ZXJpc2tzIGFuZCBrZWVwIHJlbmRlcmluZyBpY29ucyBpbiBlYWNoIGNhcHRpb24u">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.1" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.2" style="font-size:80%;">Prompt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.3" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.4" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.5" style="font-size:80%;">Generate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.6" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.7" style="font-size:80%;">three</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.8" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.9" style="font-size:80%;">promotional</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.10" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.11" style="font-size:80%;">captions</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.12" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.13" style="font-size:80%;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.14" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.15" style="font-size:80%;">each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.16" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.17" style="font-size:80%;">dimension</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.18" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.19" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.20" style="font-size:80%;">product</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.21" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.22" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.23" style="font-size:80%;">activity</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.24" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.25" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.26" style="font-size:80%;">advertisement</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.27" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.28" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.29" style="font-size:80%;">based</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.30" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.31" style="font-size:80%;">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.32" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.33" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.34" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.35" style="font-size:80%;">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.36" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.37" style="font-size:80%;">text</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.38" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.39" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.40" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.41" style="font-size:80%;">T_p</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.42" style="font-size:80%;">&gt;.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.43" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.44" style="font-size:80%;">please</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.45" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.46" style="font-size:80%;">also</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.47" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.48" style="font-size:80%;">highlight</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.49" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.50" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.51" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.52" style="font-size:80%;">keywords</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.53" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.54" style="font-size:80%;">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.55" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.56" style="font-size:80%;">asterisks</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.57" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.58" style="font-size:80%;">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.59" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.60" style="font-size:80%;">keep</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.61" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.62" style="font-size:80%;">rendering</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.63" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.64" style="font-size:80%;">icons</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.65" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.66" style="font-size:80%;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.67" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.68" style="font-size:80%;">each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.69" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.70" style="font-size:80%;">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.71" style="font-size:80%;">.</span>
</div>
</div>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Context-Aware Exploration</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.4">In addition to image/caption recommendation from scratch, Influencer further supports context-aware exploration which is built upon the image or text block dragged by the front-end. Since this task is to help designers update their exploration results for images or captions based on the provided materials (i.e., brand image, product message, etc.), where the goal is to learn the mapping from provided materials to target exploration block. As we stated in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S3.SS2.SSS2" title="3.2.2. R2: Support context-aware exploration considering brand messages and user-specified design constraints ‣ 3.2. Formulating Design Goals ‣ 3. Formative Study ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">3.2.2</span></a>, existing off-the-shell recommendation tools cannot fulfill our customization requirements. Our goal is to recommend new images or captions reflecting the user-specified theme and design constraints extracted from provided materials, while most existing models are trained only to recognize general features without distinguishing specific dimensions for each image or text, nor considering the context of previous searches and selected seed images or captions. Based on the findings of our formative study, users find it useful for recording their explored history. The purpose of this feature is to update the image or caption recommendation based on the given context, created from the seed images <math alttext="I_{s}" class="ltx_Math" display="inline" id="S5.SS3.p1.1.m1.1"><semantics id="S5.SS3.p1.1.m1.1a"><msub id="S5.SS3.p1.1.m1.1.1" xref="S5.SS3.p1.1.m1.1.1.cmml"><mi id="S5.SS3.p1.1.m1.1.1.2" xref="S5.SS3.p1.1.m1.1.1.2.cmml">I</mi><mi id="S5.SS3.p1.1.m1.1.1.3" xref="S5.SS3.p1.1.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.1.m1.1b"><apply id="S5.SS3.p1.1.m1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.1.m1.1.1.1.cmml" xref="S5.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p1.1.m1.1.1.2.cmml" xref="S5.SS3.p1.1.m1.1.1.2">𝐼</ci><ci id="S5.SS3.p1.1.m1.1.1.3.cmml" xref="S5.SS3.p1.1.m1.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.1.m1.1c">I_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.1.m1.1d">italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and seed text <math alttext="T_{s}" class="ltx_Math" display="inline" id="S5.SS3.p1.2.m2.1"><semantics id="S5.SS3.p1.2.m2.1a"><msub id="S5.SS3.p1.2.m2.1.1" xref="S5.SS3.p1.2.m2.1.1.cmml"><mi id="S5.SS3.p1.2.m2.1.1.2" xref="S5.SS3.p1.2.m2.1.1.2.cmml">T</mi><mi id="S5.SS3.p1.2.m2.1.1.3" xref="S5.SS3.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.2.m2.1b"><apply id="S5.SS3.p1.2.m2.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.2.m2.1.1.1.cmml" xref="S5.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p1.2.m2.1.1.2.cmml" xref="S5.SS3.p1.2.m2.1.1.2">𝑇</ci><ci id="S5.SS3.p1.2.m2.1.1.3.cmml" xref="S5.SS3.p1.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.2.m2.1c">T_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.2.m2.1d">italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. Also, we refer the image context as <math alttext="I_{c}" class="ltx_Math" display="inline" id="S5.SS3.p1.3.m3.1"><semantics id="S5.SS3.p1.3.m3.1a"><msub id="S5.SS3.p1.3.m3.1.1" xref="S5.SS3.p1.3.m3.1.1.cmml"><mi id="S5.SS3.p1.3.m3.1.1.2" xref="S5.SS3.p1.3.m3.1.1.2.cmml">I</mi><mi id="S5.SS3.p1.3.m3.1.1.3" xref="S5.SS3.p1.3.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.3.m3.1b"><apply id="S5.SS3.p1.3.m3.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.3.m3.1.1.1.cmml" xref="S5.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p1.3.m3.1.1.2.cmml" xref="S5.SS3.p1.3.m3.1.1.2">𝐼</ci><ci id="S5.SS3.p1.3.m3.1.1.3.cmml" xref="S5.SS3.p1.3.m3.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.3.m3.1c">I_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.3.m3.1d">italic_I start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> and the text context as <math alttext="T_{c}" class="ltx_Math" display="inline" id="S5.SS3.p1.4.m4.1"><semantics id="S5.SS3.p1.4.m4.1a"><msub id="S5.SS3.p1.4.m4.1.1" xref="S5.SS3.p1.4.m4.1.1.cmml"><mi id="S5.SS3.p1.4.m4.1.1.2" xref="S5.SS3.p1.4.m4.1.1.2.cmml">T</mi><mi id="S5.SS3.p1.4.m4.1.1.3" xref="S5.SS3.p1.4.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p1.4.m4.1b"><apply id="S5.SS3.p1.4.m4.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS3.p1.4.m4.1.1.1.cmml" xref="S5.SS3.p1.4.m4.1.1">subscript</csymbol><ci id="S5.SS3.p1.4.m4.1.1.2.cmml" xref="S5.SS3.p1.4.m4.1.1.2">𝑇</ci><ci id="S5.SS3.p1.4.m4.1.1.3.cmml" xref="S5.SS3.p1.4.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p1.4.m4.1c">T_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p1.4.m4.1d">italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>. Four main tasks should be considered to implement:</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.14"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.14.1">(a) Text Context-Aware Exploration of Images.</span> To achieve text context-aware exploration of images, we utilize text prompts <math alttext="T_{c}" class="ltx_Math" display="inline" id="S5.SS3.p2.1.m1.1"><semantics id="S5.SS3.p2.1.m1.1a"><msub id="S5.SS3.p2.1.m1.1.1" xref="S5.SS3.p2.1.m1.1.1.cmml"><mi id="S5.SS3.p2.1.m1.1.1.2" xref="S5.SS3.p2.1.m1.1.1.2.cmml">T</mi><mi id="S5.SS3.p2.1.m1.1.1.3" xref="S5.SS3.p2.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.1.m1.1b"><apply id="S5.SS3.p2.1.m1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.1.m1.1.1.1.cmml" xref="S5.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p2.1.m1.1.1.2.cmml" xref="S5.SS3.p2.1.m1.1.1.2">𝑇</ci><ci id="S5.SS3.p2.1.m1.1.1.3.cmml" xref="S5.SS3.p2.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.1.m1.1c">T_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> to update the image recommendations block. For instance, users can modify the recommendation context, such as searching for similar images with different colors or alternate objects, ensuring a diverse and visually appealing array of options. To ensure we can update the image recommendation reasonably, we first summarize the keyword list into those three dimensions (semantic, color, object) in the collected dataset. Then, we use an advanced image captioning model ofa_imagecaption_coco_large_en <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib71" title="">2022</a>)</cite> to extract the image description <math alttext="D_{i}" class="ltx_Math" display="inline" id="S5.SS3.p2.2.m2.1"><semantics id="S5.SS3.p2.2.m2.1a"><msub id="S5.SS3.p2.2.m2.1.1" xref="S5.SS3.p2.2.m2.1.1.cmml"><mi id="S5.SS3.p2.2.m2.1.1.2" xref="S5.SS3.p2.2.m2.1.1.2.cmml">D</mi><mi id="S5.SS3.p2.2.m2.1.1.3" xref="S5.SS3.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.2.m2.1b"><apply id="S5.SS3.p2.2.m2.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.2.m2.1.1.1.cmml" xref="S5.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p2.2.m2.1.1.2.cmml" xref="S5.SS3.p2.2.m2.1.1.2">𝐷</ci><ci id="S5.SS3.p2.2.m2.1.1.3.cmml" xref="S5.SS3.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.2.m2.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from seed image <math alttext="I_{s}" class="ltx_Math" display="inline" id="S5.SS3.p2.3.m3.1"><semantics id="S5.SS3.p2.3.m3.1a"><msub id="S5.SS3.p2.3.m3.1.1" xref="S5.SS3.p2.3.m3.1.1.cmml"><mi id="S5.SS3.p2.3.m3.1.1.2" xref="S5.SS3.p2.3.m3.1.1.2.cmml">I</mi><mi id="S5.SS3.p2.3.m3.1.1.3" xref="S5.SS3.p2.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.3.m3.1b"><apply id="S5.SS3.p2.3.m3.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.3.m3.1.1.1.cmml" xref="S5.SS3.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p2.3.m3.1.1.2.cmml" xref="S5.SS3.p2.3.m3.1.1.2">𝐼</ci><ci id="S5.SS3.p2.3.m3.1.1.3.cmml" xref="S5.SS3.p2.3.m3.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.3.m3.1c">I_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.3.m3.1d">italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and combine it with the text context <math alttext="T_{c}" class="ltx_Math" display="inline" id="S5.SS3.p2.4.m4.1"><semantics id="S5.SS3.p2.4.m4.1a"><msub id="S5.SS3.p2.4.m4.1.1" xref="S5.SS3.p2.4.m4.1.1.cmml"><mi id="S5.SS3.p2.4.m4.1.1.2" xref="S5.SS3.p2.4.m4.1.1.2.cmml">T</mi><mi id="S5.SS3.p2.4.m4.1.1.3" xref="S5.SS3.p2.4.m4.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.4.m4.1b"><apply id="S5.SS3.p2.4.m4.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.4.m4.1.1.1.cmml" xref="S5.SS3.p2.4.m4.1.1">subscript</csymbol><ci id="S5.SS3.p2.4.m4.1.1.2.cmml" xref="S5.SS3.p2.4.m4.1.1.2">𝑇</ci><ci id="S5.SS3.p2.4.m4.1.1.3.cmml" xref="S5.SS3.p2.4.m4.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.4.m4.1c">T_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.4.m4.1d">italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> to form a new contextual prompt <math alttext="P_{t}^{I}" class="ltx_Math" display="inline" id="S5.SS3.p2.5.m5.1"><semantics id="S5.SS3.p2.5.m5.1a"><msubsup id="S5.SS3.p2.5.m5.1.1" xref="S5.SS3.p2.5.m5.1.1.cmml"><mi id="S5.SS3.p2.5.m5.1.1.2.2" xref="S5.SS3.p2.5.m5.1.1.2.2.cmml">P</mi><mi id="S5.SS3.p2.5.m5.1.1.2.3" xref="S5.SS3.p2.5.m5.1.1.2.3.cmml">t</mi><mi id="S5.SS3.p2.5.m5.1.1.3" xref="S5.SS3.p2.5.m5.1.1.3.cmml">I</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.5.m5.1b"><apply id="S5.SS3.p2.5.m5.1.1.cmml" xref="S5.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.5.m5.1.1.1.cmml" xref="S5.SS3.p2.5.m5.1.1">superscript</csymbol><apply id="S5.SS3.p2.5.m5.1.1.2.cmml" xref="S5.SS3.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.5.m5.1.1.2.1.cmml" xref="S5.SS3.p2.5.m5.1.1">subscript</csymbol><ci id="S5.SS3.p2.5.m5.1.1.2.2.cmml" xref="S5.SS3.p2.5.m5.1.1.2.2">𝑃</ci><ci id="S5.SS3.p2.5.m5.1.1.2.3.cmml" xref="S5.SS3.p2.5.m5.1.1.2.3">𝑡</ci></apply><ci id="S5.SS3.p2.5.m5.1.1.3.cmml" xref="S5.SS3.p2.5.m5.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.5.m5.1c">P_{t}^{I}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.5.m5.1d">italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT</annotation></semantics></math> = <math alttext="D_{i}" class="ltx_Math" display="inline" id="S5.SS3.p2.6.m6.1"><semantics id="S5.SS3.p2.6.m6.1a"><msub id="S5.SS3.p2.6.m6.1.1" xref="S5.SS3.p2.6.m6.1.1.cmml"><mi id="S5.SS3.p2.6.m6.1.1.2" xref="S5.SS3.p2.6.m6.1.1.2.cmml">D</mi><mi id="S5.SS3.p2.6.m6.1.1.3" xref="S5.SS3.p2.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.6.m6.1b"><apply id="S5.SS3.p2.6.m6.1.1.cmml" xref="S5.SS3.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.6.m6.1.1.1.cmml" xref="S5.SS3.p2.6.m6.1.1">subscript</csymbol><ci id="S5.SS3.p2.6.m6.1.1.2.cmml" xref="S5.SS3.p2.6.m6.1.1.2">𝐷</ci><ci id="S5.SS3.p2.6.m6.1.1.3.cmml" xref="S5.SS3.p2.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.6.m6.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.6.m6.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> + under the context of + <math alttext="T_{c}" class="ltx_Math" display="inline" id="S5.SS3.p2.7.m7.1"><semantics id="S5.SS3.p2.7.m7.1a"><msub id="S5.SS3.p2.7.m7.1.1" xref="S5.SS3.p2.7.m7.1.1.cmml"><mi id="S5.SS3.p2.7.m7.1.1.2" xref="S5.SS3.p2.7.m7.1.1.2.cmml">T</mi><mi id="S5.SS3.p2.7.m7.1.1.3" xref="S5.SS3.p2.7.m7.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.7.m7.1b"><apply id="S5.SS3.p2.7.m7.1.1.cmml" xref="S5.SS3.p2.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.7.m7.1.1.1.cmml" xref="S5.SS3.p2.7.m7.1.1">subscript</csymbol><ci id="S5.SS3.p2.7.m7.1.1.2.cmml" xref="S5.SS3.p2.7.m7.1.1.2">𝑇</ci><ci id="S5.SS3.p2.7.m7.1.1.3.cmml" xref="S5.SS3.p2.7.m7.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.7.m7.1c">T_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.7.m7.1d">italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>.
Next, we run the text classification models mDeBERTa-v3 <cite class="ltx_cite ltx_citemacro_citep">(Laurer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib40" title="">2022</a>)</cite> to categorize the contextual prompt <math alttext="P_{t}^{I}" class="ltx_Math" display="inline" id="S5.SS3.p2.8.m8.1"><semantics id="S5.SS3.p2.8.m8.1a"><msubsup id="S5.SS3.p2.8.m8.1.1" xref="S5.SS3.p2.8.m8.1.1.cmml"><mi id="S5.SS3.p2.8.m8.1.1.2.2" xref="S5.SS3.p2.8.m8.1.1.2.2.cmml">P</mi><mi id="S5.SS3.p2.8.m8.1.1.2.3" xref="S5.SS3.p2.8.m8.1.1.2.3.cmml">t</mi><mi id="S5.SS3.p2.8.m8.1.1.3" xref="S5.SS3.p2.8.m8.1.1.3.cmml">I</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.8.m8.1b"><apply id="S5.SS3.p2.8.m8.1.1.cmml" xref="S5.SS3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.8.m8.1.1.1.cmml" xref="S5.SS3.p2.8.m8.1.1">superscript</csymbol><apply id="S5.SS3.p2.8.m8.1.1.2.cmml" xref="S5.SS3.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.8.m8.1.1.2.1.cmml" xref="S5.SS3.p2.8.m8.1.1">subscript</csymbol><ci id="S5.SS3.p2.8.m8.1.1.2.2.cmml" xref="S5.SS3.p2.8.m8.1.1.2.2">𝑃</ci><ci id="S5.SS3.p2.8.m8.1.1.2.3.cmml" xref="S5.SS3.p2.8.m8.1.1.2.3">𝑡</ci></apply><ci id="S5.SS3.p2.8.m8.1.1.3.cmml" xref="S5.SS3.p2.8.m8.1.1.3">𝐼</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.8.m8.1c">P_{t}^{I}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.8.m8.1d">italic_P start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_I end_POSTSUPERSCRIPT</annotation></semantics></math> in three dimensions (<span class="ltx_text ltx_font_italic" id="S5.SS3.p2.14.2">e.g.,</span> semantic, object, color), where the semantic class set is the semantic keyword list <math alttext="L_{s}" class="ltx_Math" display="inline" id="S5.SS3.p2.9.m9.1"><semantics id="S5.SS3.p2.9.m9.1a"><msub id="S5.SS3.p2.9.m9.1.1" xref="S5.SS3.p2.9.m9.1.1.cmml"><mi id="S5.SS3.p2.9.m9.1.1.2" xref="S5.SS3.p2.9.m9.1.1.2.cmml">L</mi><mi id="S5.SS3.p2.9.m9.1.1.3" xref="S5.SS3.p2.9.m9.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.9.m9.1b"><apply id="S5.SS3.p2.9.m9.1.1.cmml" xref="S5.SS3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.9.m9.1.1.1.cmml" xref="S5.SS3.p2.9.m9.1.1">subscript</csymbol><ci id="S5.SS3.p2.9.m9.1.1.2.cmml" xref="S5.SS3.p2.9.m9.1.1.2">𝐿</ci><ci id="S5.SS3.p2.9.m9.1.1.3.cmml" xref="S5.SS3.p2.9.m9.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.9.m9.1c">L_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.9.m9.1d">italic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>, the color class set is color keyword list <math alttext="L_{c}" class="ltx_Math" display="inline" id="S5.SS3.p2.10.m10.1"><semantics id="S5.SS3.p2.10.m10.1a"><msub id="S5.SS3.p2.10.m10.1.1" xref="S5.SS3.p2.10.m10.1.1.cmml"><mi id="S5.SS3.p2.10.m10.1.1.2" xref="S5.SS3.p2.10.m10.1.1.2.cmml">L</mi><mi id="S5.SS3.p2.10.m10.1.1.3" xref="S5.SS3.p2.10.m10.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.10.m10.1b"><apply id="S5.SS3.p2.10.m10.1.1.cmml" xref="S5.SS3.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.10.m10.1.1.1.cmml" xref="S5.SS3.p2.10.m10.1.1">subscript</csymbol><ci id="S5.SS3.p2.10.m10.1.1.2.cmml" xref="S5.SS3.p2.10.m10.1.1.2">𝐿</ci><ci id="S5.SS3.p2.10.m10.1.1.3.cmml" xref="S5.SS3.p2.10.m10.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.10.m10.1c">L_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.10.m10.1d">italic_L start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>,
and the object class set is object keyword list <math alttext="L_{o}" class="ltx_Math" display="inline" id="S5.SS3.p2.11.m11.1"><semantics id="S5.SS3.p2.11.m11.1a"><msub id="S5.SS3.p2.11.m11.1.1" xref="S5.SS3.p2.11.m11.1.1.cmml"><mi id="S5.SS3.p2.11.m11.1.1.2" xref="S5.SS3.p2.11.m11.1.1.2.cmml">L</mi><mi id="S5.SS3.p2.11.m11.1.1.3" xref="S5.SS3.p2.11.m11.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.11.m11.1b"><apply id="S5.SS3.p2.11.m11.1.1.cmml" xref="S5.SS3.p2.11.m11.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.11.m11.1.1.1.cmml" xref="S5.SS3.p2.11.m11.1.1">subscript</csymbol><ci id="S5.SS3.p2.11.m11.1.1.2.cmml" xref="S5.SS3.p2.11.m11.1.1.2">𝐿</ci><ci id="S5.SS3.p2.11.m11.1.1.3.cmml" xref="S5.SS3.p2.11.m11.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.11.m11.1c">L_{o}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.11.m11.1d">italic_L start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>. Then, it returns the target keywords <math alttext="W_{s}" class="ltx_Math" display="inline" id="S5.SS3.p2.12.m12.1"><semantics id="S5.SS3.p2.12.m12.1a"><msub id="S5.SS3.p2.12.m12.1.1" xref="S5.SS3.p2.12.m12.1.1.cmml"><mi id="S5.SS3.p2.12.m12.1.1.2" xref="S5.SS3.p2.12.m12.1.1.2.cmml">W</mi><mi id="S5.SS3.p2.12.m12.1.1.3" xref="S5.SS3.p2.12.m12.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.12.m12.1b"><apply id="S5.SS3.p2.12.m12.1.1.cmml" xref="S5.SS3.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.12.m12.1.1.1.cmml" xref="S5.SS3.p2.12.m12.1.1">subscript</csymbol><ci id="S5.SS3.p2.12.m12.1.1.2.cmml" xref="S5.SS3.p2.12.m12.1.1.2">𝑊</ci><ci id="S5.SS3.p2.12.m12.1.1.3.cmml" xref="S5.SS3.p2.12.m12.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.12.m12.1c">W_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.12.m12.1d">italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="W_{c}" class="ltx_Math" display="inline" id="S5.SS3.p2.13.m13.1"><semantics id="S5.SS3.p2.13.m13.1a"><msub id="S5.SS3.p2.13.m13.1.1" xref="S5.SS3.p2.13.m13.1.1.cmml"><mi id="S5.SS3.p2.13.m13.1.1.2" xref="S5.SS3.p2.13.m13.1.1.2.cmml">W</mi><mi id="S5.SS3.p2.13.m13.1.1.3" xref="S5.SS3.p2.13.m13.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.13.m13.1b"><apply id="S5.SS3.p2.13.m13.1.1.cmml" xref="S5.SS3.p2.13.m13.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.13.m13.1.1.1.cmml" xref="S5.SS3.p2.13.m13.1.1">subscript</csymbol><ci id="S5.SS3.p2.13.m13.1.1.2.cmml" xref="S5.SS3.p2.13.m13.1.1.2">𝑊</ci><ci id="S5.SS3.p2.13.m13.1.1.3.cmml" xref="S5.SS3.p2.13.m13.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.13.m13.1c">W_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.13.m13.1d">italic_W start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="W_{o}" class="ltx_Math" display="inline" id="S5.SS3.p2.14.m14.1"><semantics id="S5.SS3.p2.14.m14.1a"><msub id="S5.SS3.p2.14.m14.1.1" xref="S5.SS3.p2.14.m14.1.1.cmml"><mi id="S5.SS3.p2.14.m14.1.1.2" xref="S5.SS3.p2.14.m14.1.1.2.cmml">W</mi><mi id="S5.SS3.p2.14.m14.1.1.3" xref="S5.SS3.p2.14.m14.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p2.14.m14.1b"><apply id="S5.SS3.p2.14.m14.1.1.cmml" xref="S5.SS3.p2.14.m14.1.1"><csymbol cd="ambiguous" id="S5.SS3.p2.14.m14.1.1.1.cmml" xref="S5.SS3.p2.14.m14.1.1">subscript</csymbol><ci id="S5.SS3.p2.14.m14.1.1.2.cmml" xref="S5.SS3.p2.14.m14.1.1.2">𝑊</ci><ci id="S5.SS3.p2.14.m14.1.1.3.cmml" xref="S5.SS3.p2.14.m14.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p2.14.m14.1c">W_{o}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p2.14.m14.1d">italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>. Last, we locate these keywords from those three dimensions and update the image recommendations accordingly.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.3"><span class="ltx_text ltx_font_bold" id="S5.SS3.p3.3.1">(b) Text Context-Aware Exploration of Captions.</span> Text is also helpful in updating the caption recommendations, enabling users to explore highly related captions that resonate with the promotional goals. By using text context <math alttext="T_{c}" class="ltx_Math" display="inline" id="S5.SS3.p3.1.m1.1"><semantics id="S5.SS3.p3.1.m1.1a"><msub id="S5.SS3.p3.1.m1.1.1" xref="S5.SS3.p3.1.m1.1.1.cmml"><mi id="S5.SS3.p3.1.m1.1.1.2" xref="S5.SS3.p3.1.m1.1.1.2.cmml">T</mi><mi id="S5.SS3.p3.1.m1.1.1.3" xref="S5.SS3.p3.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.1.m1.1b"><apply id="S5.SS3.p3.1.m1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.1.m1.1.1.1.cmml" xref="S5.SS3.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p3.1.m1.1.1.2.cmml" xref="S5.SS3.p3.1.m1.1.1.2">𝑇</ci><ci id="S5.SS3.p3.1.m1.1.1.3.cmml" xref="S5.SS3.p3.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.1.m1.1c">T_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, users can update the caption recommendations across various contexts, like altering the target product or changing the types of activities associated with the promotional post. To implement this feature, we leverage GPT4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib50" title="">2023b</a>)</cite> and use a prompt derived from the seed text <math alttext="T_{s}" class="ltx_Math" display="inline" id="S5.SS3.p3.2.m2.1"><semantics id="S5.SS3.p3.2.m2.1a"><msub id="S5.SS3.p3.2.m2.1.1" xref="S5.SS3.p3.2.m2.1.1.cmml"><mi id="S5.SS3.p3.2.m2.1.1.2" xref="S5.SS3.p3.2.m2.1.1.2.cmml">T</mi><mi id="S5.SS3.p3.2.m2.1.1.3" xref="S5.SS3.p3.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.2.m2.1b"><apply id="S5.SS3.p3.2.m2.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.2.m2.1.1.1.cmml" xref="S5.SS3.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p3.2.m2.1.1.2.cmml" xref="S5.SS3.p3.2.m2.1.1.2">𝑇</ci><ci id="S5.SS3.p3.2.m2.1.1.3.cmml" xref="S5.SS3.p3.2.m2.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.2.m2.1c">T_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.2.m2.1d">italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> and text context <math alttext="T_{c}" class="ltx_Math" display="inline" id="S5.SS3.p3.3.m3.1"><semantics id="S5.SS3.p3.3.m3.1a"><msub id="S5.SS3.p3.3.m3.1.1" xref="S5.SS3.p3.3.m3.1.1.cmml"><mi id="S5.SS3.p3.3.m3.1.1.2" xref="S5.SS3.p3.3.m3.1.1.2.cmml">T</mi><mi id="S5.SS3.p3.3.m3.1.1.3" xref="S5.SS3.p3.3.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p3.3.m3.1b"><apply id="S5.SS3.p3.3.m3.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p3.3.m3.1.1.1.cmml" xref="S5.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p3.3.m3.1.1.2.cmml" xref="S5.SS3.p3.3.m3.1.1.2">𝑇</ci><ci id="S5.SS3.p3.3.m3.1.1.3.cmml" xref="S5.SS3.p3.3.m3.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p3.3.m3.1c">T_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p3.3.m3.1d">italic_T start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> to update the caption recommendation block across three dimensions: product, activity, and advertisement. The prompt is shown below:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="S5.SS3.p3.4">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,ICAgIFByb21wdDogR2VuZXJhdGUgdGhyZWUgcHJvbW90aW9uYWwgY2FwdGlvbnMgZm9yIGVhY2ggZGltZW5zaW9uIChwcm9kdWN0LCBhY3Rpdml0eSwgYWR2ZXJ0aXNlbWVudCkgYmFzZWQgb24gdGhlIGdpdmVuIHRleHQ6IDxUX3M+IGFuZCBmb2xsb3dpbmcgdGhlIGdpdmVuIHByb21wdCA8VF9jPi4gcGxlYXNlIGFsc28gaGlnaGxpZ2h0IHRoZSBrZXl3b3JkcyB3aXRoIGFzdGVyaXNrcyBhbmQga2VlcCByZW5kZXJpbmcgaWNvbnMgaW4gZWFjaCBjYXB0aW9uLg==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.1" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.2" style="font-size:80%;">Prompt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.3" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.4" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.5" style="font-size:80%;">Generate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.6" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.7" style="font-size:80%;">three</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.8" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.9" style="font-size:80%;">promotional</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.10" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.11" style="font-size:80%;">captions</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.12" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.13" style="font-size:80%;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.14" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.15" style="font-size:80%;">each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.16" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.17" style="font-size:80%;">dimension</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.18" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.19" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.20" style="font-size:80%;">product</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.21" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.22" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.23" style="font-size:80%;">activity</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.24" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.25" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.26" style="font-size:80%;">advertisement</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.27" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.28" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.29" style="font-size:80%;">based</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.30" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.31" style="font-size:80%;">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.32" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.33" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.34" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.35" style="font-size:80%;">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.36" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.37" style="font-size:80%;">text</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.38" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.39" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.40" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.41" style="font-size:80%;">T_s</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.42" style="font-size:80%;">&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.43" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.44" style="font-size:80%;">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.45" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.46" style="font-size:80%;">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.47" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.48" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.49" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.50" style="font-size:80%;">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.51" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.52" style="font-size:80%;">prompt</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.53" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.54" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.55" style="font-size:80%;">T_c</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.56" style="font-size:80%;">&gt;.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.57" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.58" style="font-size:80%;">please</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.59" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.60" style="font-size:80%;">also</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.61" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.62" style="font-size:80%;">highlight</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.63" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.64" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.65" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.66" style="font-size:80%;">keywords</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.67" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.68" style="font-size:80%;">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.69" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.70" style="font-size:80%;">asterisks</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.71" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.72" style="font-size:80%;">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.73" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.74" style="font-size:80%;">keep</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.75" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.76" style="font-size:80%;">rendering</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.77" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.78" style="font-size:80%;">icons</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.79" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.80" style="font-size:80%;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.81" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.82" style="font-size:80%;">each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.83" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.84" style="font-size:80%;">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.85" style="font-size:80%;">.</span>
</div>
</div>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.12"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.12.1">(c) Image Context-Aware Exploration of Images.</span> Incorporating image recommendations based on user-uploaded personal or brand logo images is a valuable feature. Users can upload images of personal interest or brand logos to enhance image recommendation results. For instance, searching for similar images that resonate with the brand’s color scheme or share common objects with the brand image ensures consistency and a cohesive visual representation. To extract related keywords from the image <math alttext="I_{c}" class="ltx_Math" display="inline" id="S5.SS3.p4.1.m1.1"><semantics id="S5.SS3.p4.1.m1.1a"><msub id="S5.SS3.p4.1.m1.1.1" xref="S5.SS3.p4.1.m1.1.1.cmml"><mi id="S5.SS3.p4.1.m1.1.1.2" xref="S5.SS3.p4.1.m1.1.1.2.cmml">I</mi><mi id="S5.SS3.p4.1.m1.1.1.3" xref="S5.SS3.p4.1.m1.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><apply id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.1.m1.1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p4.1.m1.1.1.2.cmml" xref="S5.SS3.p4.1.m1.1.1.2">𝐼</ci><ci id="S5.SS3.p4.1.m1.1.1.3.cmml" xref="S5.SS3.p4.1.m1.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">I_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.1.m1.1d">italic_I start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, we use MMDetection models leveraging with SAHI <cite class="ltx_cite ltx_citemacro_citep">(Akyon et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib3" title="">2022</a>)</cite> to get the object keyword <math alttext="W_{o}" class="ltx_Math" display="inline" id="S5.SS3.p4.2.m2.1"><semantics id="S5.SS3.p4.2.m2.1a"><msub id="S5.SS3.p4.2.m2.1.1" xref="S5.SS3.p4.2.m2.1.1.cmml"><mi id="S5.SS3.p4.2.m2.1.1.2" xref="S5.SS3.p4.2.m2.1.1.2.cmml">W</mi><mi id="S5.SS3.p4.2.m2.1.1.3" xref="S5.SS3.p4.2.m2.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.2.m2.1b"><apply id="S5.SS3.p4.2.m2.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.2.m2.1.1.1.cmml" xref="S5.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p4.2.m2.1.1.2.cmml" xref="S5.SS3.p4.2.m2.1.1.2">𝑊</ci><ci id="S5.SS3.p4.2.m2.1.1.3.cmml" xref="S5.SS3.p4.2.m2.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.2.m2.1c">W_{o}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.2.m2.1d">italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math>, use ColorThief<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pypi.org/project/colorthief/" title="">https://pypi.org/project/colorthief/</a></span></span></span> to extract color keyword <math alttext="W_{c}" class="ltx_Math" display="inline" id="S5.SS3.p4.3.m3.1"><semantics id="S5.SS3.p4.3.m3.1a"><msub id="S5.SS3.p4.3.m3.1.1" xref="S5.SS3.p4.3.m3.1.1.cmml"><mi id="S5.SS3.p4.3.m3.1.1.2" xref="S5.SS3.p4.3.m3.1.1.2.cmml">W</mi><mi id="S5.SS3.p4.3.m3.1.1.3" xref="S5.SS3.p4.3.m3.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.3.m3.1b"><apply id="S5.SS3.p4.3.m3.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.3.m3.1.1.1.cmml" xref="S5.SS3.p4.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p4.3.m3.1.1.2.cmml" xref="S5.SS3.p4.3.m3.1.1.2">𝑊</ci><ci id="S5.SS3.p4.3.m3.1.1.3.cmml" xref="S5.SS3.p4.3.m3.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.3.m3.1c">W_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.3.m3.1d">italic_W start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>, and use image captioning model ofa_imagecaption_coco_large_en <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib71" title="">2022</a>)</cite> and text classification model mDeBERTa-v3 <cite class="ltx_cite ltx_citemacro_citep">(Laurer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib40" title="">2022</a>)</cite> to get related semantic keywords <math alttext="W_{s}" class="ltx_Math" display="inline" id="S5.SS3.p4.4.m4.1"><semantics id="S5.SS3.p4.4.m4.1a"><msub id="S5.SS3.p4.4.m4.1.1" xref="S5.SS3.p4.4.m4.1.1.cmml"><mi id="S5.SS3.p4.4.m4.1.1.2" xref="S5.SS3.p4.4.m4.1.1.2.cmml">W</mi><mi id="S5.SS3.p4.4.m4.1.1.3" xref="S5.SS3.p4.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.4.m4.1b"><apply id="S5.SS3.p4.4.m4.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.4.m4.1.1.1.cmml" xref="S5.SS3.p4.4.m4.1.1">subscript</csymbol><ci id="S5.SS3.p4.4.m4.1.1.2.cmml" xref="S5.SS3.p4.4.m4.1.1.2">𝑊</ci><ci id="S5.SS3.p4.4.m4.1.1.3.cmml" xref="S5.SS3.p4.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.4.m4.1c">W_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.4.m4.1d">italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> from the semantic keyword list <math alttext="L_{s}" class="ltx_Math" display="inline" id="S5.SS3.p4.5.m5.1"><semantics id="S5.SS3.p4.5.m5.1a"><msub id="S5.SS3.p4.5.m5.1.1" xref="S5.SS3.p4.5.m5.1.1.cmml"><mi id="S5.SS3.p4.5.m5.1.1.2" xref="S5.SS3.p4.5.m5.1.1.2.cmml">L</mi><mi id="S5.SS3.p4.5.m5.1.1.3" xref="S5.SS3.p4.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.5.m5.1b"><apply id="S5.SS3.p4.5.m5.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.5.m5.1.1.1.cmml" xref="S5.SS3.p4.5.m5.1.1">subscript</csymbol><ci id="S5.SS3.p4.5.m5.1.1.2.cmml" xref="S5.SS3.p4.5.m5.1.1.2">𝐿</ci><ci id="S5.SS3.p4.5.m5.1.1.3.cmml" xref="S5.SS3.p4.5.m5.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.5.m5.1c">L_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.5.m5.1d">italic_L start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. Note that to consider the exploring history, we also extract semantic keywords <math alttext="W_{b}" class="ltx_Math" display="inline" id="S5.SS3.p4.6.m6.1"><semantics id="S5.SS3.p4.6.m6.1a"><msub id="S5.SS3.p4.6.m6.1.1" xref="S5.SS3.p4.6.m6.1.1.cmml"><mi id="S5.SS3.p4.6.m6.1.1.2" xref="S5.SS3.p4.6.m6.1.1.2.cmml">W</mi><mi id="S5.SS3.p4.6.m6.1.1.3" xref="S5.SS3.p4.6.m6.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.6.m6.1b"><apply id="S5.SS3.p4.6.m6.1.1.cmml" xref="S5.SS3.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.6.m6.1.1.1.cmml" xref="S5.SS3.p4.6.m6.1.1">subscript</csymbol><ci id="S5.SS3.p4.6.m6.1.1.2.cmml" xref="S5.SS3.p4.6.m6.1.1.2">𝑊</ci><ci id="S5.SS3.p4.6.m6.1.1.3.cmml" xref="S5.SS3.p4.6.m6.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.6.m6.1c">W_{b}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.6.m6.1d">italic_W start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> from the seed image <math alttext="I_{s}" class="ltx_Math" display="inline" id="S5.SS3.p4.7.m7.1"><semantics id="S5.SS3.p4.7.m7.1a"><msub id="S5.SS3.p4.7.m7.1.1" xref="S5.SS3.p4.7.m7.1.1.cmml"><mi id="S5.SS3.p4.7.m7.1.1.2" xref="S5.SS3.p4.7.m7.1.1.2.cmml">I</mi><mi id="S5.SS3.p4.7.m7.1.1.3" xref="S5.SS3.p4.7.m7.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.7.m7.1b"><apply id="S5.SS3.p4.7.m7.1.1.cmml" xref="S5.SS3.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.7.m7.1.1.1.cmml" xref="S5.SS3.p4.7.m7.1.1">subscript</csymbol><ci id="S5.SS3.p4.7.m7.1.1.2.cmml" xref="S5.SS3.p4.7.m7.1.1.2">𝐼</ci><ci id="S5.SS3.p4.7.m7.1.1.3.cmml" xref="S5.SS3.p4.7.m7.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.7.m7.1c">I_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.7.m7.1d">italic_I start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. The recommended images in the semantic dimension will be prioritized if they contain both keywords <math alttext="W_{b}" class="ltx_Math" display="inline" id="S5.SS3.p4.8.m8.1"><semantics id="S5.SS3.p4.8.m8.1a"><msub id="S5.SS3.p4.8.m8.1.1" xref="S5.SS3.p4.8.m8.1.1.cmml"><mi id="S5.SS3.p4.8.m8.1.1.2" xref="S5.SS3.p4.8.m8.1.1.2.cmml">W</mi><mi id="S5.SS3.p4.8.m8.1.1.3" xref="S5.SS3.p4.8.m8.1.1.3.cmml">b</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.8.m8.1b"><apply id="S5.SS3.p4.8.m8.1.1.cmml" xref="S5.SS3.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.8.m8.1.1.1.cmml" xref="S5.SS3.p4.8.m8.1.1">subscript</csymbol><ci id="S5.SS3.p4.8.m8.1.1.2.cmml" xref="S5.SS3.p4.8.m8.1.1.2">𝑊</ci><ci id="S5.SS3.p4.8.m8.1.1.3.cmml" xref="S5.SS3.p4.8.m8.1.1.3">𝑏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.8.m8.1c">W_{b}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.8.m8.1d">italic_W start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="W_{s}" class="ltx_Math" display="inline" id="S5.SS3.p4.9.m9.1"><semantics id="S5.SS3.p4.9.m9.1a"><msub id="S5.SS3.p4.9.m9.1.1" xref="S5.SS3.p4.9.m9.1.1.cmml"><mi id="S5.SS3.p4.9.m9.1.1.2" xref="S5.SS3.p4.9.m9.1.1.2.cmml">W</mi><mi id="S5.SS3.p4.9.m9.1.1.3" xref="S5.SS3.p4.9.m9.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.9.m9.1b"><apply id="S5.SS3.p4.9.m9.1.1.cmml" xref="S5.SS3.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.9.m9.1.1.1.cmml" xref="S5.SS3.p4.9.m9.1.1">subscript</csymbol><ci id="S5.SS3.p4.9.m9.1.1.2.cmml" xref="S5.SS3.p4.9.m9.1.1.2">𝑊</ci><ci id="S5.SS3.p4.9.m9.1.1.3.cmml" xref="S5.SS3.p4.9.m9.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.9.m9.1c">W_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.9.m9.1d">italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. Similarly, In the color and object dimension, recommendations are updated based on <math alttext="W_{c}" class="ltx_Math" display="inline" id="S5.SS3.p4.10.m10.1"><semantics id="S5.SS3.p4.10.m10.1a"><msub id="S5.SS3.p4.10.m10.1.1" xref="S5.SS3.p4.10.m10.1.1.cmml"><mi id="S5.SS3.p4.10.m10.1.1.2" xref="S5.SS3.p4.10.m10.1.1.2.cmml">W</mi><mi id="S5.SS3.p4.10.m10.1.1.3" xref="S5.SS3.p4.10.m10.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.10.m10.1b"><apply id="S5.SS3.p4.10.m10.1.1.cmml" xref="S5.SS3.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.10.m10.1.1.1.cmml" xref="S5.SS3.p4.10.m10.1.1">subscript</csymbol><ci id="S5.SS3.p4.10.m10.1.1.2.cmml" xref="S5.SS3.p4.10.m10.1.1.2">𝑊</ci><ci id="S5.SS3.p4.10.m10.1.1.3.cmml" xref="S5.SS3.p4.10.m10.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.10.m10.1c">W_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.10.m10.1d">italic_W start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="W_{o}" class="ltx_Math" display="inline" id="S5.SS3.p4.11.m11.1"><semantics id="S5.SS3.p4.11.m11.1a"><msub id="S5.SS3.p4.11.m11.1.1" xref="S5.SS3.p4.11.m11.1.1.cmml"><mi id="S5.SS3.p4.11.m11.1.1.2" xref="S5.SS3.p4.11.m11.1.1.2.cmml">W</mi><mi id="S5.SS3.p4.11.m11.1.1.3" xref="S5.SS3.p4.11.m11.1.1.3.cmml">o</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.11.m11.1b"><apply id="S5.SS3.p4.11.m11.1.1.cmml" xref="S5.SS3.p4.11.m11.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.11.m11.1.1.1.cmml" xref="S5.SS3.p4.11.m11.1.1">subscript</csymbol><ci id="S5.SS3.p4.11.m11.1.1.2.cmml" xref="S5.SS3.p4.11.m11.1.1.2">𝑊</ci><ci id="S5.SS3.p4.11.m11.1.1.3.cmml" xref="S5.SS3.p4.11.m11.1.1.3">𝑜</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.11.m11.1c">W_{o}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.11.m11.1d">italic_W start_POSTSUBSCRIPT italic_o end_POSTSUBSCRIPT</annotation></semantics></math> keywords and we also prioritize images that include the semantic keywords <math alttext="W_{s}" class="ltx_Math" display="inline" id="S5.SS3.p4.12.m12.1"><semantics id="S5.SS3.p4.12.m12.1a"><msub id="S5.SS3.p4.12.m12.1.1" xref="S5.SS3.p4.12.m12.1.1.cmml"><mi id="S5.SS3.p4.12.m12.1.1.2" xref="S5.SS3.p4.12.m12.1.1.2.cmml">W</mi><mi id="S5.SS3.p4.12.m12.1.1.3" xref="S5.SS3.p4.12.m12.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.12.m12.1b"><apply id="S5.SS3.p4.12.m12.1.1.cmml" xref="S5.SS3.p4.12.m12.1.1"><csymbol cd="ambiguous" id="S5.SS3.p4.12.m12.1.1.1.cmml" xref="S5.SS3.p4.12.m12.1.1">subscript</csymbol><ci id="S5.SS3.p4.12.m12.1.1.2.cmml" xref="S5.SS3.p4.12.m12.1.1.2">𝑊</ci><ci id="S5.SS3.p4.12.m12.1.1.3.cmml" xref="S5.SS3.p4.12.m12.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.12.m12.1c">W_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.12.m12.1d">italic_W start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. Last, we update the image recommendation block based on the keywords we find in three dimensions.</p>
</div>
<div class="ltx_para" id="S5.SS3.p5">
<p class="ltx_p" id="S5.SS3.p5.4"><span class="ltx_text ltx_font_bold" id="S5.SS3.p5.4.1">(d) Image Context-Aware Exploration of Captions.</span> Another useful contextual exploration feature is optimizing caption recommendations by incorporating personal interested images or company logo images. By uploading such images, users can re-explore caption recommendations across three contexts. This ensures that the promotional post’s captions are in line with the user’s preferences, the company’s branding strategy, and the intended promotional message. Specifically, we use the image captioning model ofa_imagecaption_coco_large_en <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib71" title="">2022</a>)</cite> to extract the information <math alttext="D_{i}" class="ltx_Math" display="inline" id="S5.SS3.p5.1.m1.1"><semantics id="S5.SS3.p5.1.m1.1a"><msub id="S5.SS3.p5.1.m1.1.1" xref="S5.SS3.p5.1.m1.1.1.cmml"><mi id="S5.SS3.p5.1.m1.1.1.2" xref="S5.SS3.p5.1.m1.1.1.2.cmml">D</mi><mi id="S5.SS3.p5.1.m1.1.1.3" xref="S5.SS3.p5.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.1.m1.1b"><apply id="S5.SS3.p5.1.m1.1.1.cmml" xref="S5.SS3.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS3.p5.1.m1.1.1.1.cmml" xref="S5.SS3.p5.1.m1.1.1">subscript</csymbol><ci id="S5.SS3.p5.1.m1.1.1.2.cmml" xref="S5.SS3.p5.1.m1.1.1.2">𝐷</ci><ci id="S5.SS3.p5.1.m1.1.1.3.cmml" xref="S5.SS3.p5.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.1.m1.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from the context image <math alttext="I_{c}" class="ltx_Math" display="inline" id="S5.SS3.p5.2.m2.1"><semantics id="S5.SS3.p5.2.m2.1a"><msub id="S5.SS3.p5.2.m2.1.1" xref="S5.SS3.p5.2.m2.1.1.cmml"><mi id="S5.SS3.p5.2.m2.1.1.2" xref="S5.SS3.p5.2.m2.1.1.2.cmml">I</mi><mi id="S5.SS3.p5.2.m2.1.1.3" xref="S5.SS3.p5.2.m2.1.1.3.cmml">c</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.2.m2.1b"><apply id="S5.SS3.p5.2.m2.1.1.cmml" xref="S5.SS3.p5.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS3.p5.2.m2.1.1.1.cmml" xref="S5.SS3.p5.2.m2.1.1">subscript</csymbol><ci id="S5.SS3.p5.2.m2.1.1.2.cmml" xref="S5.SS3.p5.2.m2.1.1.2">𝐼</ci><ci id="S5.SS3.p5.2.m2.1.1.3.cmml" xref="S5.SS3.p5.2.m2.1.1.3">𝑐</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.2.m2.1c">I_{c}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT</annotation></semantics></math>. Then, we write the following prompt based on <math alttext="D_{i}" class="ltx_Math" display="inline" id="S5.SS3.p5.3.m3.1"><semantics id="S5.SS3.p5.3.m3.1a"><msub id="S5.SS3.p5.3.m3.1.1" xref="S5.SS3.p5.3.m3.1.1.cmml"><mi id="S5.SS3.p5.3.m3.1.1.2" xref="S5.SS3.p5.3.m3.1.1.2.cmml">D</mi><mi id="S5.SS3.p5.3.m3.1.1.3" xref="S5.SS3.p5.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.3.m3.1b"><apply id="S5.SS3.p5.3.m3.1.1.cmml" xref="S5.SS3.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS3.p5.3.m3.1.1.1.cmml" xref="S5.SS3.p5.3.m3.1.1">subscript</csymbol><ci id="S5.SS3.p5.3.m3.1.1.2.cmml" xref="S5.SS3.p5.3.m3.1.1.2">𝐷</ci><ci id="S5.SS3.p5.3.m3.1.1.3.cmml" xref="S5.SS3.p5.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.3.m3.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.3.m3.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and the seed text <math alttext="T_{s}" class="ltx_Math" display="inline" id="S5.SS3.p5.4.m4.1"><semantics id="S5.SS3.p5.4.m4.1a"><msub id="S5.SS3.p5.4.m4.1.1" xref="S5.SS3.p5.4.m4.1.1.cmml"><mi id="S5.SS3.p5.4.m4.1.1.2" xref="S5.SS3.p5.4.m4.1.1.2.cmml">T</mi><mi id="S5.SS3.p5.4.m4.1.1.3" xref="S5.SS3.p5.4.m4.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS3.p5.4.m4.1b"><apply id="S5.SS3.p5.4.m4.1.1.cmml" xref="S5.SS3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS3.p5.4.m4.1.1.1.cmml" xref="S5.SS3.p5.4.m4.1.1">subscript</csymbol><ci id="S5.SS3.p5.4.m4.1.1.2.cmml" xref="S5.SS3.p5.4.m4.1.1.2">𝑇</ci><ci id="S5.SS3.p5.4.m4.1.1.3.cmml" xref="S5.SS3.p5.4.m4.1.1.3">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p5.4.m4.1c">T_{s}</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p5.4.m4.1d">italic_T start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> to update the caption recommendation in three dimensions by leveraging the GPT4. The prompt is shown below:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="S5.SS3.p5.5">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,ICAgIFByb21wdDogR2VuZXJhdGUgdGhyZWUgcHJvbW90aW9uYWwgY2FwdGlvbnMgZm9yIGVhY2ggZGltZW5zaW9uIChwcm9kdWN0LCBhY3Rpdml0eSwgYWR2ZXJ0aXNlbWVudCkgYmFzZWQgb24gdGhlIGdpdmVuIHRleHQ6IDxUX3M+IGFuZCBmb2xsb3dpbmcgdGhlIGdpdmVuIGltYWdlIHByb21wdCA8RF9pPi4gcGxlYXNlIGFsc28gaGlnaGxpZ2h0IHRoZSBrZXl3b3JkcyB3aXRoIGFzdGVyaXNrcyBhbmQga2VlcCByZW5kZXJpbmcgaWNvbnMgaW4gZWFjaCBjYXB0aW9uLg==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.1" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.2" style="font-size:80%;">Prompt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.3" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.4" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.5" style="font-size:80%;">Generate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.6" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.7" style="font-size:80%;">three</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.8" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.9" style="font-size:80%;">promotional</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.10" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.11" style="font-size:80%;">captions</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.12" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.13" style="font-size:80%;">for</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.14" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.15" style="font-size:80%;">each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.16" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.17" style="font-size:80%;">dimension</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.18" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.19" style="font-size:80%;">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.20" style="font-size:80%;">product</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.21" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.22" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.23" style="font-size:80%;">activity</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.24" style="font-size:80%;">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.25" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.26" style="font-size:80%;">advertisement</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.27" style="font-size:80%;">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.28" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.29" style="font-size:80%;">based</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.30" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.31" style="font-size:80%;">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.32" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.33" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.34" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.35" style="font-size:80%;">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.36" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.37" style="font-size:80%;">text</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.38" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.39" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.40" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.41" style="font-size:80%;">T_s</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.42" style="font-size:80%;">&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.43" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.44" style="font-size:80%;">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.45" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.46" style="font-size:80%;">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.47" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.48" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.49" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.50" style="font-size:80%;">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.51" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.52" style="font-size:80%;">image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.53" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.54" style="font-size:80%;">prompt</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.55" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.56" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.57" style="font-size:80%;">D_i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.58" style="font-size:80%;">&gt;.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.59" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.60" style="font-size:80%;">please</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.61" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.62" style="font-size:80%;">also</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.63" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.64" style="font-size:80%;">highlight</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.65" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.66" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.67" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.68" style="font-size:80%;">keywords</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.69" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.70" style="font-size:80%;">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.71" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.72" style="font-size:80%;">asterisks</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.73" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.74" style="font-size:80%;">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.75" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.76" style="font-size:80%;">keep</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.77" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.78" style="font-size:80%;">rendering</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.79" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.80" style="font-size:80%;">icons</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.81" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.82" style="font-size:80%;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.83" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.84" style="font-size:80%;">each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.85" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.86" style="font-size:80%;">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.87" style="font-size:80%;">.</span>
</div>
</div>
</div>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Image and Caption Fusion</h3>
<section class="ltx_subsubsection" id="S5.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.1. </span><span class="ltx_text ltx_font_bold" id="S5.SS4.SSS1.1.1">Text-based Fusion</span>
</h4>
<div class="ltx_para" id="S5.SS4.SSS1.p1">
<p class="ltx_p" id="S5.SS4.SSS1.p1.1">In promotional post design, combining text and images, originating from text, enhances the creative process by allowing seamless integration and customization. These fusion features encompass the following key aspects, each addressing a specific aspect of text integration to optimize promotional content creation:</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS1.p2">
<p class="ltx_p" id="S5.SS4.SSS1.p2.5"><span class="ltx_text ltx_font_bold" id="S5.SS4.SSS1.p2.5.1">(a) Text Fused with Image.</span> One key aspect involves fusing text prompts <math alttext="T_{p}" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p2.1.m1.1"><semantics id="S5.SS4.SSS1.p2.1.m1.1a"><msub id="S5.SS4.SSS1.p2.1.m1.1.1" xref="S5.SS4.SSS1.p2.1.m1.1.1.cmml"><mi id="S5.SS4.SSS1.p2.1.m1.1.1.2" xref="S5.SS4.SSS1.p2.1.m1.1.1.2.cmml">T</mi><mi id="S5.SS4.SSS1.p2.1.m1.1.1.3" xref="S5.SS4.SSS1.p2.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.1.m1.1b"><apply id="S5.SS4.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p2.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.SSS1.p2.1.m1.1.1.2.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.2">𝑇</ci><ci id="S5.SS4.SSS1.p2.1.m1.1.1.3.cmml" xref="S5.SS4.SSS1.p2.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.1.m1.1c">T_{p}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p2.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> with images, where users can generate prompts to customize and regenerate images according to their promotional requirements. Similarly, we run the image captioning model ofa_imagecaption_coco_large_en  <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib71" title="">2022</a>)</cite> to extract contextual information <math alttext="D_{i}" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p2.2.m2.1"><semantics id="S5.SS4.SSS1.p2.2.m2.1a"><msub id="S5.SS4.SSS1.p2.2.m2.1.1" xref="S5.SS4.SSS1.p2.2.m2.1.1.cmml"><mi id="S5.SS4.SSS1.p2.2.m2.1.1.2" xref="S5.SS4.SSS1.p2.2.m2.1.1.2.cmml">D</mi><mi id="S5.SS4.SSS1.p2.2.m2.1.1.3" xref="S5.SS4.SSS1.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.2.m2.1b"><apply id="S5.SS4.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS4.SSS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p2.2.m2.1.1.1.cmml" xref="S5.SS4.SSS1.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS4.SSS1.p2.2.m2.1.1.2.cmml" xref="S5.SS4.SSS1.p2.2.m2.1.1.2">𝐷</ci><ci id="S5.SS4.SSS1.p2.2.m2.1.1.3.cmml" xref="S5.SS4.SSS1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.2.m2.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p2.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from the target image <math alttext="I_{t}" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p2.3.m3.1"><semantics id="S5.SS4.SSS1.p2.3.m3.1a"><msub id="S5.SS4.SSS1.p2.3.m3.1.1" xref="S5.SS4.SSS1.p2.3.m3.1.1.cmml"><mi id="S5.SS4.SSS1.p2.3.m3.1.1.2" xref="S5.SS4.SSS1.p2.3.m3.1.1.2.cmml">I</mi><mi id="S5.SS4.SSS1.p2.3.m3.1.1.3" xref="S5.SS4.SSS1.p2.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.3.m3.1b"><apply id="S5.SS4.SSS1.p2.3.m3.1.1.cmml" xref="S5.SS4.SSS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p2.3.m3.1.1.1.cmml" xref="S5.SS4.SSS1.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS4.SSS1.p2.3.m3.1.1.2.cmml" xref="S5.SS4.SSS1.p2.3.m3.1.1.2">𝐼</ci><ci id="S5.SS4.SSS1.p2.3.m3.1.1.3.cmml" xref="S5.SS4.SSS1.p2.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.3.m3.1c">I_{t}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p2.3.m3.1d">italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>, then write a prompt including <math alttext="D_{i}" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p2.4.m4.1"><semantics id="S5.SS4.SSS1.p2.4.m4.1a"><msub id="S5.SS4.SSS1.p2.4.m4.1.1" xref="S5.SS4.SSS1.p2.4.m4.1.1.cmml"><mi id="S5.SS4.SSS1.p2.4.m4.1.1.2" xref="S5.SS4.SSS1.p2.4.m4.1.1.2.cmml">D</mi><mi id="S5.SS4.SSS1.p2.4.m4.1.1.3" xref="S5.SS4.SSS1.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.4.m4.1b"><apply id="S5.SS4.SSS1.p2.4.m4.1.1.cmml" xref="S5.SS4.SSS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p2.4.m4.1.1.1.cmml" xref="S5.SS4.SSS1.p2.4.m4.1.1">subscript</csymbol><ci id="S5.SS4.SSS1.p2.4.m4.1.1.2.cmml" xref="S5.SS4.SSS1.p2.4.m4.1.1.2">𝐷</ci><ci id="S5.SS4.SSS1.p2.4.m4.1.1.3.cmml" xref="S5.SS4.SSS1.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.4.m4.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p2.4.m4.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="T_{p}" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p2.5.m5.1"><semantics id="S5.SS4.SSS1.p2.5.m5.1a"><msub id="S5.SS4.SSS1.p2.5.m5.1.1" xref="S5.SS4.SSS1.p2.5.m5.1.1.cmml"><mi id="S5.SS4.SSS1.p2.5.m5.1.1.2" xref="S5.SS4.SSS1.p2.5.m5.1.1.2.cmml">T</mi><mi id="S5.SS4.SSS1.p2.5.m5.1.1.3" xref="S5.SS4.SSS1.p2.5.m5.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p2.5.m5.1b"><apply id="S5.SS4.SSS1.p2.5.m5.1.1.cmml" xref="S5.SS4.SSS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p2.5.m5.1.1.1.cmml" xref="S5.SS4.SSS1.p2.5.m5.1.1">subscript</csymbol><ci id="S5.SS4.SSS1.p2.5.m5.1.1.2.cmml" xref="S5.SS4.SSS1.p2.5.m5.1.1.2">𝑇</ci><ci id="S5.SS4.SSS1.p2.5.m5.1.1.3.cmml" xref="S5.SS4.SSS1.p2.5.m5.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p2.5.m5.1c">T_{p}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p2.5.m5.1d">italic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> into the DALL·E <cite class="ltx_cite ltx_citemacro_citep">(Ramesh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib56" title="">2021</a>)</cite> model to generate a new promotional image. The prompt we feed into the DALL·E is shown below. This fusion empowers users to tailor the visual aspects of their promotional posts, aligning with the intended message and target audience.</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="S5.SS4.SSS1.p2.6">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,ICAgIFByb21wdDogPERfaT4gKyBiYXNlZCBvbiB0aGUgcmVxdWlyZW1lbnQ6ICsgPFRfcD4=">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.1" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.2" style="font-size:80%;">Prompt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.3" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.4" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.5" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.6" style="font-size:80%;">D_i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.7" style="font-size:80%;">&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.8" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.9" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.10" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.11" style="font-size:80%;">based</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.12" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.13" style="font-size:80%;">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.14" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.15" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.16" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.17" style="font-size:80%;">requirement</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.18" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.19" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.20" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.21" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.22" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.23" style="font-size:80%;">T_p</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.24" style="font-size:80%;">&gt;</span>
</div>
</div>
</div>
<div class="ltx_para" id="S5.SS4.SSS1.p3">
<p class="ltx_p" id="S5.SS4.SSS1.p3.2"><span class="ltx_text ltx_font_bold" id="S5.SS4.SSS1.p3.2.1">(b) Text Fused with Caption.</span> Incorporating text seamlessly with captions is a crucial fusion feature. Users can input prompts to refine captions, making them more detailed, engaging, and aligned with the brand’s messaging. Additionally, storytelling elements can be infused, augmenting the narrative of the promotional content. This fusion is implemented by concatenating the text prompt <math alttext="T_{p}" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p3.1.m1.1"><semantics id="S5.SS4.SSS1.p3.1.m1.1a"><msub id="S5.SS4.SSS1.p3.1.m1.1.1" xref="S5.SS4.SSS1.p3.1.m1.1.1.cmml"><mi id="S5.SS4.SSS1.p3.1.m1.1.1.2" xref="S5.SS4.SSS1.p3.1.m1.1.1.2.cmml">T</mi><mi id="S5.SS4.SSS1.p3.1.m1.1.1.3" xref="S5.SS4.SSS1.p3.1.m1.1.1.3.cmml">p</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.1.m1.1b"><apply id="S5.SS4.SSS1.p3.1.m1.1.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p3.1.m1.1.1.1.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.SSS1.p3.1.m1.1.1.2.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.2">𝑇</ci><ci id="S5.SS4.SSS1.p3.1.m1.1.1.3.cmml" xref="S5.SS4.SSS1.p3.1.m1.1.1.3">𝑝</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.1.m1.1c">T_{p}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p3.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT</annotation></semantics></math> with the target caption <math alttext="C_{t}" class="ltx_Math" display="inline" id="S5.SS4.SSS1.p3.2.m2.1"><semantics id="S5.SS4.SSS1.p3.2.m2.1a"><msub id="S5.SS4.SSS1.p3.2.m2.1.1" xref="S5.SS4.SSS1.p3.2.m2.1.1.cmml"><mi id="S5.SS4.SSS1.p3.2.m2.1.1.2" xref="S5.SS4.SSS1.p3.2.m2.1.1.2.cmml">C</mi><mi id="S5.SS4.SSS1.p3.2.m2.1.1.3" xref="S5.SS4.SSS1.p3.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS1.p3.2.m2.1b"><apply id="S5.SS4.SSS1.p3.2.m2.1.1.cmml" xref="S5.SS4.SSS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS1.p3.2.m2.1.1.1.cmml" xref="S5.SS4.SSS1.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS4.SSS1.p3.2.m2.1.1.2.cmml" xref="S5.SS4.SSS1.p3.2.m2.1.1.2">𝐶</ci><ci id="S5.SS4.SSS1.p3.2.m2.1.1.3.cmml" xref="S5.SS4.SSS1.p3.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS1.p3.2.m2.1c">C_{t}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS1.p3.2.m2.1d">italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and feeding it to the GPT-4 API <cite class="ltx_cite ltx_citemacro_citep">(OpenAI, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib50" title="">2023b</a>)</cite> for the generation of the new caption under the user’s requirements. The caption is shown below:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="S5.SS4.SSS1.p3.3">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,ICAgIFByb21wdDogUmVnZW5lcmF0ZSB0aGUgZm9sbG93aW5nIHByb21vdGlvbmFsIHBvc3QgY2FwdGlvbjogPENfdD4gYmFzZWQgb24gdGhlIGdpdmVuIHRleHQgcHJvbXB0OiA8VF9wPi4gUGxlYXNlIGFsc28gaGlnaGxpZ2h0IHRoZSByZWxhdGVkIGtleXdvcmRzIHdpdGggYXN0ZXJpc2tzIGFuZCBrZWVwIHJlbmRlcmluZyBpY29ucyBpbiB0aGUgY2FwdGlvbg==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.1" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.2" style="font-size:80%;">Prompt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.3" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.4" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.5" style="font-size:80%;">Regenerate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.6" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.7" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.8" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.9" style="font-size:80%;">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.10" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.11" style="font-size:80%;">promotional</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.12" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.13" style="font-size:80%;">post</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.14" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.15" style="font-size:80%;">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.16" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.17" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.18" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.19" style="font-size:80%;">C_t</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.20" style="font-size:80%;">&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.21" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.22" style="font-size:80%;">based</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.23" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.24" style="font-size:80%;">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.25" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.26" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.27" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.28" style="font-size:80%;">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.29" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.30" style="font-size:80%;">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.31" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.32" style="font-size:80%;">prompt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.33" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.34" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.35" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.36" style="font-size:80%;">T_p</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.37" style="font-size:80%;">&gt;.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.38" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.39" style="font-size:80%;">Please</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.40" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.41" style="font-size:80%;">also</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.42" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.43" style="font-size:80%;">highlight</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.44" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.45" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.46" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.47" style="font-size:80%;">related</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.48" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.49" style="font-size:80%;">keywords</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.50" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.51" style="font-size:80%;">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.52" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.53" style="font-size:80%;">asterisks</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.54" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.55" style="font-size:80%;">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.56" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.57" style="font-size:80%;">keep</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.58" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.59" style="font-size:80%;">rendering</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.60" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.61" style="font-size:80%;">icons</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.62" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.63" style="font-size:80%;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.64" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.65" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.66" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.67" style="font-size:80%;">caption</span>
</div>
</div>
</div>
</section>
<section class="ltx_subsubsection" id="S5.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.4.2. </span><span class="ltx_text ltx_font_bold" id="S5.SS4.SSS2.1.1">Image-based Fusion</span>
</h4>
<div class="ltx_para" id="S5.SS4.SSS2.p1">
<p class="ltx_p" id="S5.SS4.SSS2.p1.1">The integration from images also forms a crucial aspect, enhancing the visual appeal and message delivery. The following elaborates various facets of image-based fusion features, focusing on customization and optimization to align with promotional objectives:</p>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p2">
<p class="ltx_p" id="S5.SS4.SSS2.p2.6"><span class="ltx_text ltx_font_bold" id="S5.SS4.SSS2.p2.6.1">(a) Image Fused with Image.</span> One fundamental aspect involves fusing a user’s personal interest image <math alttext="I_{i}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p2.1.m1.1"><semantics id="S5.SS4.SSS2.p2.1.m1.1a"><msub id="S5.SS4.SSS2.p2.1.m1.1.1" xref="S5.SS4.SSS2.p2.1.m1.1.1.cmml"><mi id="S5.SS4.SSS2.p2.1.m1.1.1.2" xref="S5.SS4.SSS2.p2.1.m1.1.1.2.cmml">I</mi><mi id="S5.SS4.SSS2.p2.1.m1.1.1.3" xref="S5.SS4.SSS2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p2.1.m1.1b"><apply id="S5.SS4.SSS2.p2.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.1.m1.1.1.1.cmml" xref="S5.SS4.SSS2.p2.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p2.1.m1.1.1.2.cmml" xref="S5.SS4.SSS2.p2.1.m1.1.1.2">𝐼</ci><ci id="S5.SS4.SSS2.p2.1.m1.1.1.3.cmml" xref="S5.SS4.SSS2.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p2.1.m1.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p2.1.m1.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> with the target image <math alttext="I_{t}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p2.2.m2.1"><semantics id="S5.SS4.SSS2.p2.2.m2.1a"><msub id="S5.SS4.SSS2.p2.2.m2.1.1" xref="S5.SS4.SSS2.p2.2.m2.1.1.cmml"><mi id="S5.SS4.SSS2.p2.2.m2.1.1.2" xref="S5.SS4.SSS2.p2.2.m2.1.1.2.cmml">I</mi><mi id="S5.SS4.SSS2.p2.2.m2.1.1.3" xref="S5.SS4.SSS2.p2.2.m2.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p2.2.m2.1b"><apply id="S5.SS4.SSS2.p2.2.m2.1.1.cmml" xref="S5.SS4.SSS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.2.m2.1.1.1.cmml" xref="S5.SS4.SSS2.p2.2.m2.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p2.2.m2.1.1.2.cmml" xref="S5.SS4.SSS2.p2.2.m2.1.1.2">𝐼</ci><ci id="S5.SS4.SSS2.p2.2.m2.1.1.3.cmml" xref="S5.SS4.SSS2.p2.2.m2.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p2.2.m2.1c">I_{t}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p2.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>. This allows for customization by uploading images of personal relevance or interest, ensuring a unique and personalized touch to the promotional content. Incorporating personal images establishes a more relatable and engaging connection with the audience. Specifically, we need to extract information <math alttext="D_{i}^{i}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p2.3.m3.1"><semantics id="S5.SS4.SSS2.p2.3.m3.1a"><msubsup id="S5.SS4.SSS2.p2.3.m3.1.1" xref="S5.SS4.SSS2.p2.3.m3.1.1.cmml"><mi id="S5.SS4.SSS2.p2.3.m3.1.1.2.2" xref="S5.SS4.SSS2.p2.3.m3.1.1.2.2.cmml">D</mi><mi id="S5.SS4.SSS2.p2.3.m3.1.1.2.3" xref="S5.SS4.SSS2.p2.3.m3.1.1.2.3.cmml">i</mi><mi id="S5.SS4.SSS2.p2.3.m3.1.1.3" xref="S5.SS4.SSS2.p2.3.m3.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p2.3.m3.1b"><apply id="S5.SS4.SSS2.p2.3.m3.1.1.cmml" xref="S5.SS4.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.3.m3.1.1.1.cmml" xref="S5.SS4.SSS2.p2.3.m3.1.1">superscript</csymbol><apply id="S5.SS4.SSS2.p2.3.m3.1.1.2.cmml" xref="S5.SS4.SSS2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.3.m3.1.1.2.1.cmml" xref="S5.SS4.SSS2.p2.3.m3.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p2.3.m3.1.1.2.2.cmml" xref="S5.SS4.SSS2.p2.3.m3.1.1.2.2">𝐷</ci><ci id="S5.SS4.SSS2.p2.3.m3.1.1.2.3.cmml" xref="S5.SS4.SSS2.p2.3.m3.1.1.2.3">𝑖</ci></apply><ci id="S5.SS4.SSS2.p2.3.m3.1.1.3.cmml" xref="S5.SS4.SSS2.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p2.3.m3.1c">D_{i}^{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p2.3.m3.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> , <math alttext="D_{i}^{t}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p2.4.m4.1"><semantics id="S5.SS4.SSS2.p2.4.m4.1a"><msubsup id="S5.SS4.SSS2.p2.4.m4.1.1" xref="S5.SS4.SSS2.p2.4.m4.1.1.cmml"><mi id="S5.SS4.SSS2.p2.4.m4.1.1.2.2" xref="S5.SS4.SSS2.p2.4.m4.1.1.2.2.cmml">D</mi><mi id="S5.SS4.SSS2.p2.4.m4.1.1.2.3" xref="S5.SS4.SSS2.p2.4.m4.1.1.2.3.cmml">i</mi><mi id="S5.SS4.SSS2.p2.4.m4.1.1.3" xref="S5.SS4.SSS2.p2.4.m4.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p2.4.m4.1b"><apply id="S5.SS4.SSS2.p2.4.m4.1.1.cmml" xref="S5.SS4.SSS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.4.m4.1.1.1.cmml" xref="S5.SS4.SSS2.p2.4.m4.1.1">superscript</csymbol><apply id="S5.SS4.SSS2.p2.4.m4.1.1.2.cmml" xref="S5.SS4.SSS2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.4.m4.1.1.2.1.cmml" xref="S5.SS4.SSS2.p2.4.m4.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p2.4.m4.1.1.2.2.cmml" xref="S5.SS4.SSS2.p2.4.m4.1.1.2.2">𝐷</ci><ci id="S5.SS4.SSS2.p2.4.m4.1.1.2.3.cmml" xref="S5.SS4.SSS2.p2.4.m4.1.1.2.3">𝑖</ci></apply><ci id="S5.SS4.SSS2.p2.4.m4.1.1.3.cmml" xref="S5.SS4.SSS2.p2.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p2.4.m4.1c">D_{i}^{t}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p2.4.m4.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> from these two images using the image captioning model ofa_imagecaption_coco_large_enframework <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib71" title="">2022</a>)</cite>. Then we concatenate <math alttext="D_{i}^{i}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p2.5.m5.1"><semantics id="S5.SS4.SSS2.p2.5.m5.1a"><msubsup id="S5.SS4.SSS2.p2.5.m5.1.1" xref="S5.SS4.SSS2.p2.5.m5.1.1.cmml"><mi id="S5.SS4.SSS2.p2.5.m5.1.1.2.2" xref="S5.SS4.SSS2.p2.5.m5.1.1.2.2.cmml">D</mi><mi id="S5.SS4.SSS2.p2.5.m5.1.1.2.3" xref="S5.SS4.SSS2.p2.5.m5.1.1.2.3.cmml">i</mi><mi id="S5.SS4.SSS2.p2.5.m5.1.1.3" xref="S5.SS4.SSS2.p2.5.m5.1.1.3.cmml">i</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p2.5.m5.1b"><apply id="S5.SS4.SSS2.p2.5.m5.1.1.cmml" xref="S5.SS4.SSS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.5.m5.1.1.1.cmml" xref="S5.SS4.SSS2.p2.5.m5.1.1">superscript</csymbol><apply id="S5.SS4.SSS2.p2.5.m5.1.1.2.cmml" xref="S5.SS4.SSS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.5.m5.1.1.2.1.cmml" xref="S5.SS4.SSS2.p2.5.m5.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p2.5.m5.1.1.2.2.cmml" xref="S5.SS4.SSS2.p2.5.m5.1.1.2.2">𝐷</ci><ci id="S5.SS4.SSS2.p2.5.m5.1.1.2.3.cmml" xref="S5.SS4.SSS2.p2.5.m5.1.1.2.3">𝑖</ci></apply><ci id="S5.SS4.SSS2.p2.5.m5.1.1.3.cmml" xref="S5.SS4.SSS2.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p2.5.m5.1c">D_{i}^{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p2.5.m5.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_i end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="D_{i}^{t}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p2.6.m6.1"><semantics id="S5.SS4.SSS2.p2.6.m6.1a"><msubsup id="S5.SS4.SSS2.p2.6.m6.1.1" xref="S5.SS4.SSS2.p2.6.m6.1.1.cmml"><mi id="S5.SS4.SSS2.p2.6.m6.1.1.2.2" xref="S5.SS4.SSS2.p2.6.m6.1.1.2.2.cmml">D</mi><mi id="S5.SS4.SSS2.p2.6.m6.1.1.2.3" xref="S5.SS4.SSS2.p2.6.m6.1.1.2.3.cmml">i</mi><mi id="S5.SS4.SSS2.p2.6.m6.1.1.3" xref="S5.SS4.SSS2.p2.6.m6.1.1.3.cmml">t</mi></msubsup><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p2.6.m6.1b"><apply id="S5.SS4.SSS2.p2.6.m6.1.1.cmml" xref="S5.SS4.SSS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.6.m6.1.1.1.cmml" xref="S5.SS4.SSS2.p2.6.m6.1.1">superscript</csymbol><apply id="S5.SS4.SSS2.p2.6.m6.1.1.2.cmml" xref="S5.SS4.SSS2.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p2.6.m6.1.1.2.1.cmml" xref="S5.SS4.SSS2.p2.6.m6.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p2.6.m6.1.1.2.2.cmml" xref="S5.SS4.SSS2.p2.6.m6.1.1.2.2">𝐷</ci><ci id="S5.SS4.SSS2.p2.6.m6.1.1.2.3.cmml" xref="S5.SS4.SSS2.p2.6.m6.1.1.2.3">𝑖</ci></apply><ci id="S5.SS4.SSS2.p2.6.m6.1.1.3.cmml" xref="S5.SS4.SSS2.p2.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p2.6.m6.1c">D_{i}^{t}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p2.6.m6.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT</annotation></semantics></math> into a prompt, with one serving as the context and the other as the main component. This final prompt is then passed to DALL·E <cite class="ltx_cite ltx_citemacro_citep">(Ramesh et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib56" title="">2021</a>)</cite> for fusion. The prompt to generate a new image is shown below:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="S5.SS4.SSS2.p2.7">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,ICAgIFByb21wdDogPERfaV50PiArIHVuZGVyIHRoZSBjb250ZXh0IG9mOiArIDxEX2leaT4=">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx6">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.1" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.2" style="font-size:80%;">Prompt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.3" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.4" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.5" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.6" style="font-size:80%;">D_i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.7" style="font-size:80%;">^</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.8" style="font-size:80%;">t</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.9" style="font-size:80%;">&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.10" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.11" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.12" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.13" style="font-size:80%;">under</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.14" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.15" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.16" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.17" style="font-size:80%;">context</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.18" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.19" style="font-size:80%;">of</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.20" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.21" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.22" style="font-size:80%;">+</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx6.23" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.24" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.25" style="font-size:80%;">D_i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.26" style="font-size:80%;">^</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx6.27" style="font-size:80%;">i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx6.28" style="font-size:80%;">&gt;</span>
</div>
</div>
</div>
<div class="ltx_para" id="S5.SS4.SSS2.p3">
<p class="ltx_p" id="S5.SS4.SSS2.p3.4"><span class="ltx_text ltx_font_bold" id="S5.SS4.SSS2.p3.4.1">(b) Image Fused with Caption.</span> Integrating images with captions is vital for conveying a comprehensive message. Users can upload personal or brand logo images to customize captions, making them more relevant to the company’s image or aligning with specific areas of interest. This fusion provides an opportunity to infuse brand identity and captivate the audience with intriguing visual-textual combinations. Similarly, we use the image captioning model ofa_imagecaption_coco_large_en  <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib71" title="">2022</a>)</cite> to extract the information <math alttext="D_{i}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p3.1.m1.1"><semantics id="S5.SS4.SSS2.p3.1.m1.1a"><msub id="S5.SS4.SSS2.p3.1.m1.1.1" xref="S5.SS4.SSS2.p3.1.m1.1.1.cmml"><mi id="S5.SS4.SSS2.p3.1.m1.1.1.2" xref="S5.SS4.SSS2.p3.1.m1.1.1.2.cmml">D</mi><mi id="S5.SS4.SSS2.p3.1.m1.1.1.3" xref="S5.SS4.SSS2.p3.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.1.m1.1b"><apply id="S5.SS4.SSS2.p3.1.m1.1.1.cmml" xref="S5.SS4.SSS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p3.1.m1.1.1.1.cmml" xref="S5.SS4.SSS2.p3.1.m1.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p3.1.m1.1.1.2.cmml" xref="S5.SS4.SSS2.p3.1.m1.1.1.2">𝐷</ci><ci id="S5.SS4.SSS2.p3.1.m1.1.1.3.cmml" xref="S5.SS4.SSS2.p3.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.1.m1.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p3.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> from the context image <math alttext="I_{i}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p3.2.m2.1"><semantics id="S5.SS4.SSS2.p3.2.m2.1a"><msub id="S5.SS4.SSS2.p3.2.m2.1.1" xref="S5.SS4.SSS2.p3.2.m2.1.1.cmml"><mi id="S5.SS4.SSS2.p3.2.m2.1.1.2" xref="S5.SS4.SSS2.p3.2.m2.1.1.2.cmml">I</mi><mi id="S5.SS4.SSS2.p3.2.m2.1.1.3" xref="S5.SS4.SSS2.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.2.m2.1b"><apply id="S5.SS4.SSS2.p3.2.m2.1.1.cmml" xref="S5.SS4.SSS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p3.2.m2.1.1.1.cmml" xref="S5.SS4.SSS2.p3.2.m2.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p3.2.m2.1.1.2.cmml" xref="S5.SS4.SSS2.p3.2.m2.1.1.2">𝐼</ci><ci id="S5.SS4.SSS2.p3.2.m2.1.1.3.cmml" xref="S5.SS4.SSS2.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.2.m2.1c">I_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p3.2.m2.1d">italic_I start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>. Then, we write the following prompt based on <math alttext="D_{i}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p3.3.m3.1"><semantics id="S5.SS4.SSS2.p3.3.m3.1a"><msub id="S5.SS4.SSS2.p3.3.m3.1.1" xref="S5.SS4.SSS2.p3.3.m3.1.1.cmml"><mi id="S5.SS4.SSS2.p3.3.m3.1.1.2" xref="S5.SS4.SSS2.p3.3.m3.1.1.2.cmml">D</mi><mi id="S5.SS4.SSS2.p3.3.m3.1.1.3" xref="S5.SS4.SSS2.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.3.m3.1b"><apply id="S5.SS4.SSS2.p3.3.m3.1.1.cmml" xref="S5.SS4.SSS2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p3.3.m3.1.1.1.cmml" xref="S5.SS4.SSS2.p3.3.m3.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p3.3.m3.1.1.2.cmml" xref="S5.SS4.SSS2.p3.3.m3.1.1.2">𝐷</ci><ci id="S5.SS4.SSS2.p3.3.m3.1.1.3.cmml" xref="S5.SS4.SSS2.p3.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.3.m3.1c">D_{i}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p3.3.m3.1d">italic_D start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and the target caption <math alttext="C_{t}" class="ltx_Math" display="inline" id="S5.SS4.SSS2.p3.4.m4.1"><semantics id="S5.SS4.SSS2.p3.4.m4.1a"><msub id="S5.SS4.SSS2.p3.4.m4.1.1" xref="S5.SS4.SSS2.p3.4.m4.1.1.cmml"><mi id="S5.SS4.SSS2.p3.4.m4.1.1.2" xref="S5.SS4.SSS2.p3.4.m4.1.1.2.cmml">C</mi><mi id="S5.SS4.SSS2.p3.4.m4.1.1.3" xref="S5.SS4.SSS2.p3.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS4.SSS2.p3.4.m4.1b"><apply id="S5.SS4.SSS2.p3.4.m4.1.1.cmml" xref="S5.SS4.SSS2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S5.SS4.SSS2.p3.4.m4.1.1.1.cmml" xref="S5.SS4.SSS2.p3.4.m4.1.1">subscript</csymbol><ci id="S5.SS4.SSS2.p3.4.m4.1.1.2.cmml" xref="S5.SS4.SSS2.p3.4.m4.1.1.2">𝐶</ci><ci id="S5.SS4.SSS2.p3.4.m4.1.1.3.cmml" xref="S5.SS4.SSS2.p3.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS4.SSS2.p3.4.m4.1c">C_{t}</annotation><annotation encoding="application/x-llamapun" id="S5.SS4.SSS2.p3.4.m4.1d">italic_C start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> to generate a new caption under the image context. The prompt to refine the caption is shown below:</p>
<div class="ltx_listing ltx_lstlisting ltx_framed ltx_framed_rectangle ltx_listing" id="S5.SS4.SSS2.p3.5">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,ICAgIFByb21wdDogUmVnZW5lcmF0ZSB0aGUgZm9sbG93aW5nIHByb21vdGlvbmFsIGNhcHRpb24gPENfdD4gYmFzZWQgb24gdGhlIGdpdmVuIGltYWdlIGNvbnRleHQ6IDxEX2k+LiBQbGVhc2UgYWxzbyBoaWdobGlnaHQgdGhlIHJlbGF0ZWQga2V5d29yZHMgd2l0aCBhc3Rlcmlza3MgYW5kIGtlZXAgcmVuZGVyaW5nIGljb25zIGluIHRoZSBjYXB0aW9uLg==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.1" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.2" style="font-size:80%;">Prompt</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.3" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.4" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.5" style="font-size:80%;">Regenerate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.6" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.7" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.8" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.9" style="font-size:80%;">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.10" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.11" style="font-size:80%;">promotional</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.12" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.13" style="font-size:80%;">caption</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.14" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.15" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.16" style="font-size:80%;">C_t</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.17" style="font-size:80%;">&gt;</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.18" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.19" style="font-size:80%;">based</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.20" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.21" style="font-size:80%;">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.22" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.23" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.24" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.25" style="font-size:80%;">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.26" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.27" style="font-size:80%;">image</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.28" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.29" style="font-size:80%;">context</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.30" style="font-size:80%;">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.31" style="font-size:80%;"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.32" style="font-size:80%;">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.33" style="font-size:80%;">D_i</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.34" style="font-size:80%;">&gt;.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.35" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.36" style="font-size:80%;">Please</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.37" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.38" style="font-size:80%;">also</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.39" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.40" style="font-size:80%;">highlight</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.41" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.42" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.43" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.44" style="font-size:80%;">related</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.45" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.46" style="font-size:80%;">keywords</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.47" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.48" style="font-size:80%;">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.49" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.50" style="font-size:80%;">asterisks</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.51" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.52" style="font-size:80%;">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.53" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.54" style="font-size:80%;">keep</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.55" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.56" style="font-size:80%;">rendering</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.57" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.58" style="font-size:80%;">icons</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.59" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.60" style="font-size:80%;">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.61" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.62" style="font-size:80%;">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.63" style="font-size:80%;"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.64" style="font-size:80%;">caption</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.65" style="font-size:80%;">.</span>
</div>
</div>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>User Evaluation</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">We conducted a within-subjects controlled experiment comparing Influencer with a Baseline including Google Search for ideation and Figma for post design (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S6.F8" title="Figure 8 ‣ 6. User Evaluation ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 8</span></a>). Since promotional post design is a complex process, we structured a within-subjects study to mitigate the potential impact of individual differences in this creative process. According to insights from our formative study, Google Search stands as the most commonly utilized platform for seeking inspiration, and Figma is also commonly used by users, making it a suitable baseline system for promotional post design.
This baseline featured a Google Search-like interface with essential functions such as image searching (utilizing the same database) and image saving capabilities. Users can then use Figma to customize the image and write the caption to create a promotional post.</p>
</div>
<figure class="ltx_figure" id="S6.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="274" id="S6.F8.g1" src="extracted/5743646/Figure/Baseline.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>The Baseline system includes two parts: (A) search engine and (B) Figma, which resembles the current workflow with industry standard tools.</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S6.F8.1">\Description</span></div>
</div>
</figure>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Participants</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.8">We recruited 12 participants (7 women and 5 men, age: <math alttext="M=27.3" class="ltx_Math" display="inline" id="S6.SS1.p1.1.m1.1"><semantics id="S6.SS1.p1.1.m1.1a"><mrow id="S6.SS1.p1.1.m1.1.1" xref="S6.SS1.p1.1.m1.1.1.cmml"><mi id="S6.SS1.p1.1.m1.1.1.2" xref="S6.SS1.p1.1.m1.1.1.2.cmml">M</mi><mo id="S6.SS1.p1.1.m1.1.1.1" xref="S6.SS1.p1.1.m1.1.1.1.cmml">=</mo><mn id="S6.SS1.p1.1.m1.1.1.3" xref="S6.SS1.p1.1.m1.1.1.3.cmml">27.3</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.1.m1.1b"><apply id="S6.SS1.p1.1.m1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1"><eq id="S6.SS1.p1.1.m1.1.1.1.cmml" xref="S6.SS1.p1.1.m1.1.1.1"></eq><ci id="S6.SS1.p1.1.m1.1.1.2.cmml" xref="S6.SS1.p1.1.m1.1.1.2">𝑀</ci><cn id="S6.SS1.p1.1.m1.1.1.3.cmml" type="float" xref="S6.SS1.p1.1.m1.1.1.3">27.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.1.m1.1c">M=27.3</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.1.m1.1d">italic_M = 27.3</annotation></semantics></math>, <math alttext="SD=3.6" class="ltx_Math" display="inline" id="S6.SS1.p1.2.m2.1"><semantics id="S6.SS1.p1.2.m2.1a"><mrow id="S6.SS1.p1.2.m2.1.1" xref="S6.SS1.p1.2.m2.1.1.cmml"><mrow id="S6.SS1.p1.2.m2.1.1.2" xref="S6.SS1.p1.2.m2.1.1.2.cmml"><mi id="S6.SS1.p1.2.m2.1.1.2.2" xref="S6.SS1.p1.2.m2.1.1.2.2.cmml">S</mi><mo id="S6.SS1.p1.2.m2.1.1.2.1" xref="S6.SS1.p1.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S6.SS1.p1.2.m2.1.1.2.3" xref="S6.SS1.p1.2.m2.1.1.2.3.cmml">D</mi></mrow><mo id="S6.SS1.p1.2.m2.1.1.1" xref="S6.SS1.p1.2.m2.1.1.1.cmml">=</mo><mn id="S6.SS1.p1.2.m2.1.1.3" xref="S6.SS1.p1.2.m2.1.1.3.cmml">3.6</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.2.m2.1b"><apply id="S6.SS1.p1.2.m2.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1"><eq id="S6.SS1.p1.2.m2.1.1.1.cmml" xref="S6.SS1.p1.2.m2.1.1.1"></eq><apply id="S6.SS1.p1.2.m2.1.1.2.cmml" xref="S6.SS1.p1.2.m2.1.1.2"><times id="S6.SS1.p1.2.m2.1.1.2.1.cmml" xref="S6.SS1.p1.2.m2.1.1.2.1"></times><ci id="S6.SS1.p1.2.m2.1.1.2.2.cmml" xref="S6.SS1.p1.2.m2.1.1.2.2">𝑆</ci><ci id="S6.SS1.p1.2.m2.1.1.2.3.cmml" xref="S6.SS1.p1.2.m2.1.1.2.3">𝐷</ci></apply><cn id="S6.SS1.p1.2.m2.1.1.3.cmml" type="float" xref="S6.SS1.p1.2.m2.1.1.3">3.6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.2.m2.1c">SD=3.6</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.2.m2.1d">italic_S italic_D = 3.6</annotation></semantics></math>) via social media and mailing lists. The majority of them (10 out of 12) need to engage with graphic design in their daily tasks, although they are not proficient designers. With a pre-study questionnaire, they were considered novice designers based on their self-reported design experiences on a 5-point Likert Scale, including expertise in promotional post design (<math alttext="MD=1" class="ltx_Math" display="inline" id="S6.SS1.p1.3.m3.1"><semantics id="S6.SS1.p1.3.m3.1a"><mrow id="S6.SS1.p1.3.m3.1.1" xref="S6.SS1.p1.3.m3.1.1.cmml"><mrow id="S6.SS1.p1.3.m3.1.1.2" xref="S6.SS1.p1.3.m3.1.1.2.cmml"><mi id="S6.SS1.p1.3.m3.1.1.2.2" xref="S6.SS1.p1.3.m3.1.1.2.2.cmml">M</mi><mo id="S6.SS1.p1.3.m3.1.1.2.1" xref="S6.SS1.p1.3.m3.1.1.2.1.cmml">⁢</mo><mi id="S6.SS1.p1.3.m3.1.1.2.3" xref="S6.SS1.p1.3.m3.1.1.2.3.cmml">D</mi></mrow><mo id="S6.SS1.p1.3.m3.1.1.1" xref="S6.SS1.p1.3.m3.1.1.1.cmml">=</mo><mn id="S6.SS1.p1.3.m3.1.1.3" xref="S6.SS1.p1.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.3.m3.1b"><apply id="S6.SS1.p1.3.m3.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1"><eq id="S6.SS1.p1.3.m3.1.1.1.cmml" xref="S6.SS1.p1.3.m3.1.1.1"></eq><apply id="S6.SS1.p1.3.m3.1.1.2.cmml" xref="S6.SS1.p1.3.m3.1.1.2"><times id="S6.SS1.p1.3.m3.1.1.2.1.cmml" xref="S6.SS1.p1.3.m3.1.1.2.1"></times><ci id="S6.SS1.p1.3.m3.1.1.2.2.cmml" xref="S6.SS1.p1.3.m3.1.1.2.2">𝑀</ci><ci id="S6.SS1.p1.3.m3.1.1.2.3.cmml" xref="S6.SS1.p1.3.m3.1.1.2.3">𝐷</ci></apply><cn id="S6.SS1.p1.3.m3.1.1.3.cmml" type="integer" xref="S6.SS1.p1.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.3.m3.1c">MD=1</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.3.m3.1d">italic_M italic_D = 1</annotation></semantics></math>, <math alttext="SD=0.42" class="ltx_Math" display="inline" id="S6.SS1.p1.4.m4.1"><semantics id="S6.SS1.p1.4.m4.1a"><mrow id="S6.SS1.p1.4.m4.1.1" xref="S6.SS1.p1.4.m4.1.1.cmml"><mrow id="S6.SS1.p1.4.m4.1.1.2" xref="S6.SS1.p1.4.m4.1.1.2.cmml"><mi id="S6.SS1.p1.4.m4.1.1.2.2" xref="S6.SS1.p1.4.m4.1.1.2.2.cmml">S</mi><mo id="S6.SS1.p1.4.m4.1.1.2.1" xref="S6.SS1.p1.4.m4.1.1.2.1.cmml">⁢</mo><mi id="S6.SS1.p1.4.m4.1.1.2.3" xref="S6.SS1.p1.4.m4.1.1.2.3.cmml">D</mi></mrow><mo id="S6.SS1.p1.4.m4.1.1.1" xref="S6.SS1.p1.4.m4.1.1.1.cmml">=</mo><mn id="S6.SS1.p1.4.m4.1.1.3" xref="S6.SS1.p1.4.m4.1.1.3.cmml">0.42</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.4.m4.1b"><apply id="S6.SS1.p1.4.m4.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1"><eq id="S6.SS1.p1.4.m4.1.1.1.cmml" xref="S6.SS1.p1.4.m4.1.1.1"></eq><apply id="S6.SS1.p1.4.m4.1.1.2.cmml" xref="S6.SS1.p1.4.m4.1.1.2"><times id="S6.SS1.p1.4.m4.1.1.2.1.cmml" xref="S6.SS1.p1.4.m4.1.1.2.1"></times><ci id="S6.SS1.p1.4.m4.1.1.2.2.cmml" xref="S6.SS1.p1.4.m4.1.1.2.2">𝑆</ci><ci id="S6.SS1.p1.4.m4.1.1.2.3.cmml" xref="S6.SS1.p1.4.m4.1.1.2.3">𝐷</ci></apply><cn id="S6.SS1.p1.4.m4.1.1.3.cmml" type="float" xref="S6.SS1.p1.4.m4.1.1.3">0.42</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.4.m4.1c">SD=0.42</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.4.m4.1d">italic_S italic_D = 0.42</annotation></semantics></math>) as well as familiarity
with color theories (<math alttext="MD=1.5" class="ltx_Math" display="inline" id="S6.SS1.p1.5.m5.1"><semantics id="S6.SS1.p1.5.m5.1a"><mrow id="S6.SS1.p1.5.m5.1.1" xref="S6.SS1.p1.5.m5.1.1.cmml"><mrow id="S6.SS1.p1.5.m5.1.1.2" xref="S6.SS1.p1.5.m5.1.1.2.cmml"><mi id="S6.SS1.p1.5.m5.1.1.2.2" xref="S6.SS1.p1.5.m5.1.1.2.2.cmml">M</mi><mo id="S6.SS1.p1.5.m5.1.1.2.1" xref="S6.SS1.p1.5.m5.1.1.2.1.cmml">⁢</mo><mi id="S6.SS1.p1.5.m5.1.1.2.3" xref="S6.SS1.p1.5.m5.1.1.2.3.cmml">D</mi></mrow><mo id="S6.SS1.p1.5.m5.1.1.1" xref="S6.SS1.p1.5.m5.1.1.1.cmml">=</mo><mn id="S6.SS1.p1.5.m5.1.1.3" xref="S6.SS1.p1.5.m5.1.1.3.cmml">1.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.5.m5.1b"><apply id="S6.SS1.p1.5.m5.1.1.cmml" xref="S6.SS1.p1.5.m5.1.1"><eq id="S6.SS1.p1.5.m5.1.1.1.cmml" xref="S6.SS1.p1.5.m5.1.1.1"></eq><apply id="S6.SS1.p1.5.m5.1.1.2.cmml" xref="S6.SS1.p1.5.m5.1.1.2"><times id="S6.SS1.p1.5.m5.1.1.2.1.cmml" xref="S6.SS1.p1.5.m5.1.1.2.1"></times><ci id="S6.SS1.p1.5.m5.1.1.2.2.cmml" xref="S6.SS1.p1.5.m5.1.1.2.2">𝑀</ci><ci id="S6.SS1.p1.5.m5.1.1.2.3.cmml" xref="S6.SS1.p1.5.m5.1.1.2.3">𝐷</ci></apply><cn id="S6.SS1.p1.5.m5.1.1.3.cmml" type="float" xref="S6.SS1.p1.5.m5.1.1.3">1.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.5.m5.1c">MD=1.5</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.5.m5.1d">italic_M italic_D = 1.5</annotation></semantics></math>, <math alttext="SD=0.78" class="ltx_Math" display="inline" id="S6.SS1.p1.6.m6.1"><semantics id="S6.SS1.p1.6.m6.1a"><mrow id="S6.SS1.p1.6.m6.1.1" xref="S6.SS1.p1.6.m6.1.1.cmml"><mrow id="S6.SS1.p1.6.m6.1.1.2" xref="S6.SS1.p1.6.m6.1.1.2.cmml"><mi id="S6.SS1.p1.6.m6.1.1.2.2" xref="S6.SS1.p1.6.m6.1.1.2.2.cmml">S</mi><mo id="S6.SS1.p1.6.m6.1.1.2.1" xref="S6.SS1.p1.6.m6.1.1.2.1.cmml">⁢</mo><mi id="S6.SS1.p1.6.m6.1.1.2.3" xref="S6.SS1.p1.6.m6.1.1.2.3.cmml">D</mi></mrow><mo id="S6.SS1.p1.6.m6.1.1.1" xref="S6.SS1.p1.6.m6.1.1.1.cmml">=</mo><mn id="S6.SS1.p1.6.m6.1.1.3" xref="S6.SS1.p1.6.m6.1.1.3.cmml">0.78</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.6.m6.1b"><apply id="S6.SS1.p1.6.m6.1.1.cmml" xref="S6.SS1.p1.6.m6.1.1"><eq id="S6.SS1.p1.6.m6.1.1.1.cmml" xref="S6.SS1.p1.6.m6.1.1.1"></eq><apply id="S6.SS1.p1.6.m6.1.1.2.cmml" xref="S6.SS1.p1.6.m6.1.1.2"><times id="S6.SS1.p1.6.m6.1.1.2.1.cmml" xref="S6.SS1.p1.6.m6.1.1.2.1"></times><ci id="S6.SS1.p1.6.m6.1.1.2.2.cmml" xref="S6.SS1.p1.6.m6.1.1.2.2">𝑆</ci><ci id="S6.SS1.p1.6.m6.1.1.2.3.cmml" xref="S6.SS1.p1.6.m6.1.1.2.3">𝐷</ci></apply><cn id="S6.SS1.p1.6.m6.1.1.3.cmml" type="float" xref="S6.SS1.p1.6.m6.1.1.3">0.78</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.6.m6.1c">SD=0.78</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.6.m6.1d">italic_S italic_D = 0.78</annotation></semantics></math>), and Figma (<math alttext="MD=1" class="ltx_Math" display="inline" id="S6.SS1.p1.7.m7.1"><semantics id="S6.SS1.p1.7.m7.1a"><mrow id="S6.SS1.p1.7.m7.1.1" xref="S6.SS1.p1.7.m7.1.1.cmml"><mrow id="S6.SS1.p1.7.m7.1.1.2" xref="S6.SS1.p1.7.m7.1.1.2.cmml"><mi id="S6.SS1.p1.7.m7.1.1.2.2" xref="S6.SS1.p1.7.m7.1.1.2.2.cmml">M</mi><mo id="S6.SS1.p1.7.m7.1.1.2.1" xref="S6.SS1.p1.7.m7.1.1.2.1.cmml">⁢</mo><mi id="S6.SS1.p1.7.m7.1.1.2.3" xref="S6.SS1.p1.7.m7.1.1.2.3.cmml">D</mi></mrow><mo id="S6.SS1.p1.7.m7.1.1.1" xref="S6.SS1.p1.7.m7.1.1.1.cmml">=</mo><mn id="S6.SS1.p1.7.m7.1.1.3" xref="S6.SS1.p1.7.m7.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.7.m7.1b"><apply id="S6.SS1.p1.7.m7.1.1.cmml" xref="S6.SS1.p1.7.m7.1.1"><eq id="S6.SS1.p1.7.m7.1.1.1.cmml" xref="S6.SS1.p1.7.m7.1.1.1"></eq><apply id="S6.SS1.p1.7.m7.1.1.2.cmml" xref="S6.SS1.p1.7.m7.1.1.2"><times id="S6.SS1.p1.7.m7.1.1.2.1.cmml" xref="S6.SS1.p1.7.m7.1.1.2.1"></times><ci id="S6.SS1.p1.7.m7.1.1.2.2.cmml" xref="S6.SS1.p1.7.m7.1.1.2.2">𝑀</ci><ci id="S6.SS1.p1.7.m7.1.1.2.3.cmml" xref="S6.SS1.p1.7.m7.1.1.2.3">𝐷</ci></apply><cn id="S6.SS1.p1.7.m7.1.1.3.cmml" type="integer" xref="S6.SS1.p1.7.m7.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.7.m7.1c">MD=1</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.7.m7.1d">italic_M italic_D = 1</annotation></semantics></math>, <math alttext="SD=0.56" class="ltx_Math" display="inline" id="S6.SS1.p1.8.m8.1"><semantics id="S6.SS1.p1.8.m8.1a"><mrow id="S6.SS1.p1.8.m8.1.1" xref="S6.SS1.p1.8.m8.1.1.cmml"><mrow id="S6.SS1.p1.8.m8.1.1.2" xref="S6.SS1.p1.8.m8.1.1.2.cmml"><mi id="S6.SS1.p1.8.m8.1.1.2.2" xref="S6.SS1.p1.8.m8.1.1.2.2.cmml">S</mi><mo id="S6.SS1.p1.8.m8.1.1.2.1" xref="S6.SS1.p1.8.m8.1.1.2.1.cmml">⁢</mo><mi id="S6.SS1.p1.8.m8.1.1.2.3" xref="S6.SS1.p1.8.m8.1.1.2.3.cmml">D</mi></mrow><mo id="S6.SS1.p1.8.m8.1.1.1" xref="S6.SS1.p1.8.m8.1.1.1.cmml">=</mo><mn id="S6.SS1.p1.8.m8.1.1.3" xref="S6.SS1.p1.8.m8.1.1.3.cmml">0.56</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.SS1.p1.8.m8.1b"><apply id="S6.SS1.p1.8.m8.1.1.cmml" xref="S6.SS1.p1.8.m8.1.1"><eq id="S6.SS1.p1.8.m8.1.1.1.cmml" xref="S6.SS1.p1.8.m8.1.1.1"></eq><apply id="S6.SS1.p1.8.m8.1.1.2.cmml" xref="S6.SS1.p1.8.m8.1.1.2"><times id="S6.SS1.p1.8.m8.1.1.2.1.cmml" xref="S6.SS1.p1.8.m8.1.1.2.1"></times><ci id="S6.SS1.p1.8.m8.1.1.2.2.cmml" xref="S6.SS1.p1.8.m8.1.1.2.2">𝑆</ci><ci id="S6.SS1.p1.8.m8.1.1.2.3.cmml" xref="S6.SS1.p1.8.m8.1.1.2.3">𝐷</ci></apply><cn id="S6.SS1.p1.8.m8.1.1.3.cmml" type="float" xref="S6.SS1.p1.8.m8.1.1.3">0.56</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p1.8.m8.1c">SD=0.56</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p1.8.m8.1d">italic_S italic_D = 0.56</annotation></semantics></math>), where 1 indicates less expertise/ familiarity. Thus, they are representatives of the typical target users for Influencer (e.g., marketers, product managers, UI designers, and small business owners) who want to easily and quickly produce a few promotional post designs for broadcasting events or products.
</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Tasks and Design</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">We employed a within-subjects design for the study.
We designed three tasks to compare Influencer with the Baseline on multiple aspects of creativity and assistance in promotional post design tasks.
All tasks require users to design a promotional post from the provided topic which is the most common case for novices to not start from scratch.
We set a time limit (60 minutes) for each task to simulate a timing design scenario.
This also allowed the study length to be reasonable in a within-subject design. </p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">Task 1</span> focuses on a product promotion scenario. Given a product topic “Fruit Juice”, users need to design relevant images to comply with the topic. They also need to generate the related captions to be compatible with the images and make the final post design harmonic overall.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p3.1.1">Task 2</span> focuses on the scenario of activity promotion. Provided with an activity theme: “Basketball Game”, participants need to design the promotional image to achieve a design matching the theme. Participants should also generate reasonable captions to express a particular design need and suitably describe the image. There should be at least two versions of the final product with different colors to accommodate different sentiments or occasions.</p>
</div>
<div class="ltx_para" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p4.1.1">Task 3</span> aims to test the brand-based promotion design scenario. Given a brand image and topic: “Ramen”, participants need to design an image compatible with this brand image on the canvas, which includes the color of the image, and the object of the image. In addition, the final design should be aesthetically pleasing.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Procedure</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">During the study, participants complete the above three tasks using both of the study systems (i.e., Influencer and Baseline), one after another. The order and combination of study data and tools were counterbalanced. For each condition, participants were first introduced to the study system (i.e., Baseline or Influencer).
During the Influencer condition, a short tutorial video demonstrating its basic functionality was shown to users. Once users were comfortable using the tool, tasks were presented sequentially with the final deliverables for each task. Post task completion, each participant completed a questionnaire related to their experience, comparing the Baseline and the Influencer. The questionnaire included the Creative Support Index (CSI) assessment <cite class="ltx_cite ltx_citemacro_citep">(Cherry and Latulipe, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib15" title="">2014</a>)</cite> regarding users’ experience in exploration, expressiveness, enjoyment, workload, etc. Besides, we conducted a short semi-structured interview with each participant to gain their qualitative feedback. The entire study lasted approximately 60 minutes for each participant, and they were remunerated with $20.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Results</h2>
<figure class="ltx_figure" id="S7.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="171" id="S7.F9.g1" src="extracted/5743646/Figure/example.png" width="598"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Examples of six drafted posts generated by three participants (P1, P4, P10) with Influencer and Baseline system in three tasks: (a) a fruit juice promotional post for a small grocery store, (b) a post for promoting basketball game activity, and (c) a ramen advertisement for a Japanese restaurant. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S7.F9.1">\Description</span></div>
</div>
</figure>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Quantitative Results</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">In the following, we report our results on task completion rate and participants’ ratings for Influencer and Baseline.</p>
</div>
<div class="ltx_para" id="S7.SS1.p2">
<p class="ltx_p" id="S7.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.p2.1.1">Completion rate.</span> With Influencer, all participants claimed they had completed all the required promotional posts within the given time for the three tasks. With the Baseline, 91.67% (11/12) of participants completed the post design in Task 1, but only 75.00% (9/12) of the post design in Task 2 and 83.3% (10/12) in Task 3 were fully completed. This indicates that Influencer has the potential to accelerate design processes when designers need to edit images or quickly generate alternative designs. Here, we present six representative drafts among 72 drafts generated with Influencer and the baseline system in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S7.F9" title="Figure 9 ‣ 7. Results ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 9</span></a>.</p>
</div>
<div class="ltx_para" id="S7.SS1.p3">
<p class="ltx_p" id="S7.SS1.p3.8"><span class="ltx_text ltx_font_bold" id="S7.SS1.p3.8.1">Design Experience.</span> We utilized the Creativity Support Index (CSI) <cite class="ltx_cite ltx_citemacro_citep">(Cherry and Latulipe, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib15" title="">2014</a>)</cite> to measure the degree of creativity support for Influencer and the Baseline in the study. Participants rated five creativity support factors with scores on a Likert scale from 0 (strongly disagree) to 10 (strongly agree). <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S7.F10" title="Figure 10 ‣ 7.1. Quantitative Results ‣ 7. Results ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 10</span></a> shows the individual CSI score for each factor, i.e., enjoyment, exploration, expressiveness, immersion, and results-worth-effort. Overall, Influencer achieved a CSI score of 76.86 (<math alttext="SD=13.52" class="ltx_Math" display="inline" id="S7.SS1.p3.1.m1.1"><semantics id="S7.SS1.p3.1.m1.1a"><mrow id="S7.SS1.p3.1.m1.1.1" xref="S7.SS1.p3.1.m1.1.1.cmml"><mrow id="S7.SS1.p3.1.m1.1.1.2" xref="S7.SS1.p3.1.m1.1.1.2.cmml"><mi id="S7.SS1.p3.1.m1.1.1.2.2" xref="S7.SS1.p3.1.m1.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p3.1.m1.1.1.2.1" xref="S7.SS1.p3.1.m1.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p3.1.m1.1.1.2.3" xref="S7.SS1.p3.1.m1.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p3.1.m1.1.1.1" xref="S7.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="S7.SS1.p3.1.m1.1.1.3" xref="S7.SS1.p3.1.m1.1.1.3.cmml">13.52</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.1.m1.1b"><apply id="S7.SS1.p3.1.m1.1.1.cmml" xref="S7.SS1.p3.1.m1.1.1"><eq id="S7.SS1.p3.1.m1.1.1.1.cmml" xref="S7.SS1.p3.1.m1.1.1.1"></eq><apply id="S7.SS1.p3.1.m1.1.1.2.cmml" xref="S7.SS1.p3.1.m1.1.1.2"><times id="S7.SS1.p3.1.m1.1.1.2.1.cmml" xref="S7.SS1.p3.1.m1.1.1.2.1"></times><ci id="S7.SS1.p3.1.m1.1.1.2.2.cmml" xref="S7.SS1.p3.1.m1.1.1.2.2">𝑆</ci><ci id="S7.SS1.p3.1.m1.1.1.2.3.cmml" xref="S7.SS1.p3.1.m1.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p3.1.m1.1.1.3.cmml" type="float" xref="S7.SS1.p3.1.m1.1.1.3">13.52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.1.m1.1c">SD=13.52</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.1.m1.1d">italic_S italic_D = 13.52</annotation></semantics></math>), which is higher than the Baseline with a score of 61.5 (<math alttext="SD=16.64" class="ltx_Math" display="inline" id="S7.SS1.p3.2.m2.1"><semantics id="S7.SS1.p3.2.m2.1a"><mrow id="S7.SS1.p3.2.m2.1.1" xref="S7.SS1.p3.2.m2.1.1.cmml"><mrow id="S7.SS1.p3.2.m2.1.1.2" xref="S7.SS1.p3.2.m2.1.1.2.cmml"><mi id="S7.SS1.p3.2.m2.1.1.2.2" xref="S7.SS1.p3.2.m2.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p3.2.m2.1.1.2.1" xref="S7.SS1.p3.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p3.2.m2.1.1.2.3" xref="S7.SS1.p3.2.m2.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p3.2.m2.1.1.1" xref="S7.SS1.p3.2.m2.1.1.1.cmml">=</mo><mn id="S7.SS1.p3.2.m2.1.1.3" xref="S7.SS1.p3.2.m2.1.1.3.cmml">16.64</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.2.m2.1b"><apply id="S7.SS1.p3.2.m2.1.1.cmml" xref="S7.SS1.p3.2.m2.1.1"><eq id="S7.SS1.p3.2.m2.1.1.1.cmml" xref="S7.SS1.p3.2.m2.1.1.1"></eq><apply id="S7.SS1.p3.2.m2.1.1.2.cmml" xref="S7.SS1.p3.2.m2.1.1.2"><times id="S7.SS1.p3.2.m2.1.1.2.1.cmml" xref="S7.SS1.p3.2.m2.1.1.2.1"></times><ci id="S7.SS1.p3.2.m2.1.1.2.2.cmml" xref="S7.SS1.p3.2.m2.1.1.2.2">𝑆</ci><ci id="S7.SS1.p3.2.m2.1.1.2.3.cmml" xref="S7.SS1.p3.2.m2.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p3.2.m2.1.1.3.cmml" type="float" xref="S7.SS1.p3.2.m2.1.1.3">16.64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.2.m2.1c">SD=16.64</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.2.m2.1d">italic_S italic_D = 16.64</annotation></semantics></math>).
The paired t-tests show that the overall CSI score of Influencer is significantly higher than that of Baseline (<math alttext="t=4.136,p=.0007" class="ltx_Math" display="inline" id="S7.SS1.p3.3.m3.2"><semantics id="S7.SS1.p3.3.m3.2a"><mrow id="S7.SS1.p3.3.m3.2.2.2" xref="S7.SS1.p3.3.m3.2.2.3.cmml"><mrow id="S7.SS1.p3.3.m3.1.1.1.1" xref="S7.SS1.p3.3.m3.1.1.1.1.cmml"><mi id="S7.SS1.p3.3.m3.1.1.1.1.2" xref="S7.SS1.p3.3.m3.1.1.1.1.2.cmml">t</mi><mo id="S7.SS1.p3.3.m3.1.1.1.1.1" xref="S7.SS1.p3.3.m3.1.1.1.1.1.cmml">=</mo><mn id="S7.SS1.p3.3.m3.1.1.1.1.3" xref="S7.SS1.p3.3.m3.1.1.1.1.3.cmml">4.136</mn></mrow><mo id="S7.SS1.p3.3.m3.2.2.2.3" xref="S7.SS1.p3.3.m3.2.2.3a.cmml">,</mo><mrow id="S7.SS1.p3.3.m3.2.2.2.2" xref="S7.SS1.p3.3.m3.2.2.2.2.cmml"><mi id="S7.SS1.p3.3.m3.2.2.2.2.2" xref="S7.SS1.p3.3.m3.2.2.2.2.2.cmml">p</mi><mo id="S7.SS1.p3.3.m3.2.2.2.2.1" xref="S7.SS1.p3.3.m3.2.2.2.2.1.cmml">=</mo><mn id="S7.SS1.p3.3.m3.2.2.2.2.3" xref="S7.SS1.p3.3.m3.2.2.2.2.3.cmml">.0007</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.3.m3.2b"><apply id="S7.SS1.p3.3.m3.2.2.3.cmml" xref="S7.SS1.p3.3.m3.2.2.2"><csymbol cd="ambiguous" id="S7.SS1.p3.3.m3.2.2.3a.cmml" xref="S7.SS1.p3.3.m3.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS1.p3.3.m3.1.1.1.1.cmml" xref="S7.SS1.p3.3.m3.1.1.1.1"><eq id="S7.SS1.p3.3.m3.1.1.1.1.1.cmml" xref="S7.SS1.p3.3.m3.1.1.1.1.1"></eq><ci id="S7.SS1.p3.3.m3.1.1.1.1.2.cmml" xref="S7.SS1.p3.3.m3.1.1.1.1.2">𝑡</ci><cn id="S7.SS1.p3.3.m3.1.1.1.1.3.cmml" type="float" xref="S7.SS1.p3.3.m3.1.1.1.1.3">4.136</cn></apply><apply id="S7.SS1.p3.3.m3.2.2.2.2.cmml" xref="S7.SS1.p3.3.m3.2.2.2.2"><eq id="S7.SS1.p3.3.m3.2.2.2.2.1.cmml" xref="S7.SS1.p3.3.m3.2.2.2.2.1"></eq><ci id="S7.SS1.p3.3.m3.2.2.2.2.2.cmml" xref="S7.SS1.p3.3.m3.2.2.2.2.2">𝑝</ci><cn id="S7.SS1.p3.3.m3.2.2.2.2.3.cmml" type="float" xref="S7.SS1.p3.3.m3.2.2.2.2.3">.0007</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.3.m3.2c">t=4.136,p=.0007</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.3.m3.2d">italic_t = 4.136 , italic_p = .0007</annotation></semantics></math>). It was found that Influencer generated statistically significant improvements in enjoyment (<math alttext="t=3.362,p=.0009" class="ltx_Math" display="inline" id="S7.SS1.p3.4.m4.2"><semantics id="S7.SS1.p3.4.m4.2a"><mrow id="S7.SS1.p3.4.m4.2.2.2" xref="S7.SS1.p3.4.m4.2.2.3.cmml"><mrow id="S7.SS1.p3.4.m4.1.1.1.1" xref="S7.SS1.p3.4.m4.1.1.1.1.cmml"><mi id="S7.SS1.p3.4.m4.1.1.1.1.2" xref="S7.SS1.p3.4.m4.1.1.1.1.2.cmml">t</mi><mo id="S7.SS1.p3.4.m4.1.1.1.1.1" xref="S7.SS1.p3.4.m4.1.1.1.1.1.cmml">=</mo><mn id="S7.SS1.p3.4.m4.1.1.1.1.3" xref="S7.SS1.p3.4.m4.1.1.1.1.3.cmml">3.362</mn></mrow><mo id="S7.SS1.p3.4.m4.2.2.2.3" xref="S7.SS1.p3.4.m4.2.2.3a.cmml">,</mo><mrow id="S7.SS1.p3.4.m4.2.2.2.2" xref="S7.SS1.p3.4.m4.2.2.2.2.cmml"><mi id="S7.SS1.p3.4.m4.2.2.2.2.2" xref="S7.SS1.p3.4.m4.2.2.2.2.2.cmml">p</mi><mo id="S7.SS1.p3.4.m4.2.2.2.2.1" xref="S7.SS1.p3.4.m4.2.2.2.2.1.cmml">=</mo><mn id="S7.SS1.p3.4.m4.2.2.2.2.3" xref="S7.SS1.p3.4.m4.2.2.2.2.3.cmml">.0009</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.4.m4.2b"><apply id="S7.SS1.p3.4.m4.2.2.3.cmml" xref="S7.SS1.p3.4.m4.2.2.2"><csymbol cd="ambiguous" id="S7.SS1.p3.4.m4.2.2.3a.cmml" xref="S7.SS1.p3.4.m4.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS1.p3.4.m4.1.1.1.1.cmml" xref="S7.SS1.p3.4.m4.1.1.1.1"><eq id="S7.SS1.p3.4.m4.1.1.1.1.1.cmml" xref="S7.SS1.p3.4.m4.1.1.1.1.1"></eq><ci id="S7.SS1.p3.4.m4.1.1.1.1.2.cmml" xref="S7.SS1.p3.4.m4.1.1.1.1.2">𝑡</ci><cn id="S7.SS1.p3.4.m4.1.1.1.1.3.cmml" type="float" xref="S7.SS1.p3.4.m4.1.1.1.1.3">3.362</cn></apply><apply id="S7.SS1.p3.4.m4.2.2.2.2.cmml" xref="S7.SS1.p3.4.m4.2.2.2.2"><eq id="S7.SS1.p3.4.m4.2.2.2.2.1.cmml" xref="S7.SS1.p3.4.m4.2.2.2.2.1"></eq><ci id="S7.SS1.p3.4.m4.2.2.2.2.2.cmml" xref="S7.SS1.p3.4.m4.2.2.2.2.2">𝑝</ci><cn id="S7.SS1.p3.4.m4.2.2.2.2.3.cmml" type="float" xref="S7.SS1.p3.4.m4.2.2.2.2.3">.0009</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.4.m4.2c">t=3.362,p=.0009</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.4.m4.2d">italic_t = 3.362 , italic_p = .0009</annotation></semantics></math>), exploration (<math alttext="t=3.445,p=.0018" class="ltx_Math" display="inline" id="S7.SS1.p3.5.m5.2"><semantics id="S7.SS1.p3.5.m5.2a"><mrow id="S7.SS1.p3.5.m5.2.2.2" xref="S7.SS1.p3.5.m5.2.2.3.cmml"><mrow id="S7.SS1.p3.5.m5.1.1.1.1" xref="S7.SS1.p3.5.m5.1.1.1.1.cmml"><mi id="S7.SS1.p3.5.m5.1.1.1.1.2" xref="S7.SS1.p3.5.m5.1.1.1.1.2.cmml">t</mi><mo id="S7.SS1.p3.5.m5.1.1.1.1.1" xref="S7.SS1.p3.5.m5.1.1.1.1.1.cmml">=</mo><mn id="S7.SS1.p3.5.m5.1.1.1.1.3" xref="S7.SS1.p3.5.m5.1.1.1.1.3.cmml">3.445</mn></mrow><mo id="S7.SS1.p3.5.m5.2.2.2.3" xref="S7.SS1.p3.5.m5.2.2.3a.cmml">,</mo><mrow id="S7.SS1.p3.5.m5.2.2.2.2" xref="S7.SS1.p3.5.m5.2.2.2.2.cmml"><mi id="S7.SS1.p3.5.m5.2.2.2.2.2" xref="S7.SS1.p3.5.m5.2.2.2.2.2.cmml">p</mi><mo id="S7.SS1.p3.5.m5.2.2.2.2.1" xref="S7.SS1.p3.5.m5.2.2.2.2.1.cmml">=</mo><mn id="S7.SS1.p3.5.m5.2.2.2.2.3" xref="S7.SS1.p3.5.m5.2.2.2.2.3.cmml">.0018</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.5.m5.2b"><apply id="S7.SS1.p3.5.m5.2.2.3.cmml" xref="S7.SS1.p3.5.m5.2.2.2"><csymbol cd="ambiguous" id="S7.SS1.p3.5.m5.2.2.3a.cmml" xref="S7.SS1.p3.5.m5.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS1.p3.5.m5.1.1.1.1.cmml" xref="S7.SS1.p3.5.m5.1.1.1.1"><eq id="S7.SS1.p3.5.m5.1.1.1.1.1.cmml" xref="S7.SS1.p3.5.m5.1.1.1.1.1"></eq><ci id="S7.SS1.p3.5.m5.1.1.1.1.2.cmml" xref="S7.SS1.p3.5.m5.1.1.1.1.2">𝑡</ci><cn id="S7.SS1.p3.5.m5.1.1.1.1.3.cmml" type="float" xref="S7.SS1.p3.5.m5.1.1.1.1.3">3.445</cn></apply><apply id="S7.SS1.p3.5.m5.2.2.2.2.cmml" xref="S7.SS1.p3.5.m5.2.2.2.2"><eq id="S7.SS1.p3.5.m5.2.2.2.2.1.cmml" xref="S7.SS1.p3.5.m5.2.2.2.2.1"></eq><ci id="S7.SS1.p3.5.m5.2.2.2.2.2.cmml" xref="S7.SS1.p3.5.m5.2.2.2.2.2">𝑝</ci><cn id="S7.SS1.p3.5.m5.2.2.2.2.3.cmml" type="float" xref="S7.SS1.p3.5.m5.2.2.2.2.3">.0018</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.5.m5.2c">t=3.445,p=.0018</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.5.m5.2d">italic_t = 3.445 , italic_p = .0018</annotation></semantics></math>), expressiveness (<math alttext="t=2.762,p=.0275" class="ltx_Math" display="inline" id="S7.SS1.p3.6.m6.2"><semantics id="S7.SS1.p3.6.m6.2a"><mrow id="S7.SS1.p3.6.m6.2.2.2" xref="S7.SS1.p3.6.m6.2.2.3.cmml"><mrow id="S7.SS1.p3.6.m6.1.1.1.1" xref="S7.SS1.p3.6.m6.1.1.1.1.cmml"><mi id="S7.SS1.p3.6.m6.1.1.1.1.2" xref="S7.SS1.p3.6.m6.1.1.1.1.2.cmml">t</mi><mo id="S7.SS1.p3.6.m6.1.1.1.1.1" xref="S7.SS1.p3.6.m6.1.1.1.1.1.cmml">=</mo><mn id="S7.SS1.p3.6.m6.1.1.1.1.3" xref="S7.SS1.p3.6.m6.1.1.1.1.3.cmml">2.762</mn></mrow><mo id="S7.SS1.p3.6.m6.2.2.2.3" xref="S7.SS1.p3.6.m6.2.2.3a.cmml">,</mo><mrow id="S7.SS1.p3.6.m6.2.2.2.2" xref="S7.SS1.p3.6.m6.2.2.2.2.cmml"><mi id="S7.SS1.p3.6.m6.2.2.2.2.2" xref="S7.SS1.p3.6.m6.2.2.2.2.2.cmml">p</mi><mo id="S7.SS1.p3.6.m6.2.2.2.2.1" xref="S7.SS1.p3.6.m6.2.2.2.2.1.cmml">=</mo><mn id="S7.SS1.p3.6.m6.2.2.2.2.3" xref="S7.SS1.p3.6.m6.2.2.2.2.3.cmml">.0275</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.6.m6.2b"><apply id="S7.SS1.p3.6.m6.2.2.3.cmml" xref="S7.SS1.p3.6.m6.2.2.2"><csymbol cd="ambiguous" id="S7.SS1.p3.6.m6.2.2.3a.cmml" xref="S7.SS1.p3.6.m6.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS1.p3.6.m6.1.1.1.1.cmml" xref="S7.SS1.p3.6.m6.1.1.1.1"><eq id="S7.SS1.p3.6.m6.1.1.1.1.1.cmml" xref="S7.SS1.p3.6.m6.1.1.1.1.1"></eq><ci id="S7.SS1.p3.6.m6.1.1.1.1.2.cmml" xref="S7.SS1.p3.6.m6.1.1.1.1.2">𝑡</ci><cn id="S7.SS1.p3.6.m6.1.1.1.1.3.cmml" type="float" xref="S7.SS1.p3.6.m6.1.1.1.1.3">2.762</cn></apply><apply id="S7.SS1.p3.6.m6.2.2.2.2.cmml" xref="S7.SS1.p3.6.m6.2.2.2.2"><eq id="S7.SS1.p3.6.m6.2.2.2.2.1.cmml" xref="S7.SS1.p3.6.m6.2.2.2.2.1"></eq><ci id="S7.SS1.p3.6.m6.2.2.2.2.2.cmml" xref="S7.SS1.p3.6.m6.2.2.2.2.2">𝑝</ci><cn id="S7.SS1.p3.6.m6.2.2.2.2.3.cmml" type="float" xref="S7.SS1.p3.6.m6.2.2.2.2.3">.0275</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.6.m6.2c">t=2.762,p=.0275</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.6.m6.2d">italic_t = 2.762 , italic_p = .0275</annotation></semantics></math>), immersion (<math alttext="t=2.614,p=.0296" class="ltx_Math" display="inline" id="S7.SS1.p3.7.m7.2"><semantics id="S7.SS1.p3.7.m7.2a"><mrow id="S7.SS1.p3.7.m7.2.2.2" xref="S7.SS1.p3.7.m7.2.2.3.cmml"><mrow id="S7.SS1.p3.7.m7.1.1.1.1" xref="S7.SS1.p3.7.m7.1.1.1.1.cmml"><mi id="S7.SS1.p3.7.m7.1.1.1.1.2" xref="S7.SS1.p3.7.m7.1.1.1.1.2.cmml">t</mi><mo id="S7.SS1.p3.7.m7.1.1.1.1.1" xref="S7.SS1.p3.7.m7.1.1.1.1.1.cmml">=</mo><mn id="S7.SS1.p3.7.m7.1.1.1.1.3" xref="S7.SS1.p3.7.m7.1.1.1.1.3.cmml">2.614</mn></mrow><mo id="S7.SS1.p3.7.m7.2.2.2.3" xref="S7.SS1.p3.7.m7.2.2.3a.cmml">,</mo><mrow id="S7.SS1.p3.7.m7.2.2.2.2" xref="S7.SS1.p3.7.m7.2.2.2.2.cmml"><mi id="S7.SS1.p3.7.m7.2.2.2.2.2" xref="S7.SS1.p3.7.m7.2.2.2.2.2.cmml">p</mi><mo id="S7.SS1.p3.7.m7.2.2.2.2.1" xref="S7.SS1.p3.7.m7.2.2.2.2.1.cmml">=</mo><mn id="S7.SS1.p3.7.m7.2.2.2.2.3" xref="S7.SS1.p3.7.m7.2.2.2.2.3.cmml">.0296</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.7.m7.2b"><apply id="S7.SS1.p3.7.m7.2.2.3.cmml" xref="S7.SS1.p3.7.m7.2.2.2"><csymbol cd="ambiguous" id="S7.SS1.p3.7.m7.2.2.3a.cmml" xref="S7.SS1.p3.7.m7.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS1.p3.7.m7.1.1.1.1.cmml" xref="S7.SS1.p3.7.m7.1.1.1.1"><eq id="S7.SS1.p3.7.m7.1.1.1.1.1.cmml" xref="S7.SS1.p3.7.m7.1.1.1.1.1"></eq><ci id="S7.SS1.p3.7.m7.1.1.1.1.2.cmml" xref="S7.SS1.p3.7.m7.1.1.1.1.2">𝑡</ci><cn id="S7.SS1.p3.7.m7.1.1.1.1.3.cmml" type="float" xref="S7.SS1.p3.7.m7.1.1.1.1.3">2.614</cn></apply><apply id="S7.SS1.p3.7.m7.2.2.2.2.cmml" xref="S7.SS1.p3.7.m7.2.2.2.2"><eq id="S7.SS1.p3.7.m7.2.2.2.2.1.cmml" xref="S7.SS1.p3.7.m7.2.2.2.2.1"></eq><ci id="S7.SS1.p3.7.m7.2.2.2.2.2.cmml" xref="S7.SS1.p3.7.m7.2.2.2.2.2">𝑝</ci><cn id="S7.SS1.p3.7.m7.2.2.2.2.3.cmml" type="float" xref="S7.SS1.p3.7.m7.2.2.2.2.3">.0296</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.7.m7.2c">t=2.614,p=.0296</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.7.m7.2d">italic_t = 2.614 , italic_p = .0296</annotation></semantics></math>), and results worth effort (<math alttext="t=3.184,p=.0266" class="ltx_Math" display="inline" id="S7.SS1.p3.8.m8.2"><semantics id="S7.SS1.p3.8.m8.2a"><mrow id="S7.SS1.p3.8.m8.2.2.2" xref="S7.SS1.p3.8.m8.2.2.3.cmml"><mrow id="S7.SS1.p3.8.m8.1.1.1.1" xref="S7.SS1.p3.8.m8.1.1.1.1.cmml"><mi id="S7.SS1.p3.8.m8.1.1.1.1.2" xref="S7.SS1.p3.8.m8.1.1.1.1.2.cmml">t</mi><mo id="S7.SS1.p3.8.m8.1.1.1.1.1" xref="S7.SS1.p3.8.m8.1.1.1.1.1.cmml">=</mo><mn id="S7.SS1.p3.8.m8.1.1.1.1.3" xref="S7.SS1.p3.8.m8.1.1.1.1.3.cmml">3.184</mn></mrow><mo id="S7.SS1.p3.8.m8.2.2.2.3" xref="S7.SS1.p3.8.m8.2.2.3a.cmml">,</mo><mrow id="S7.SS1.p3.8.m8.2.2.2.2" xref="S7.SS1.p3.8.m8.2.2.2.2.cmml"><mi id="S7.SS1.p3.8.m8.2.2.2.2.2" xref="S7.SS1.p3.8.m8.2.2.2.2.2.cmml">p</mi><mo id="S7.SS1.p3.8.m8.2.2.2.2.1" xref="S7.SS1.p3.8.m8.2.2.2.2.1.cmml">=</mo><mn id="S7.SS1.p3.8.m8.2.2.2.2.3" xref="S7.SS1.p3.8.m8.2.2.2.2.3.cmml">.0266</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p3.8.m8.2b"><apply id="S7.SS1.p3.8.m8.2.2.3.cmml" xref="S7.SS1.p3.8.m8.2.2.2"><csymbol cd="ambiguous" id="S7.SS1.p3.8.m8.2.2.3a.cmml" xref="S7.SS1.p3.8.m8.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS1.p3.8.m8.1.1.1.1.cmml" xref="S7.SS1.p3.8.m8.1.1.1.1"><eq id="S7.SS1.p3.8.m8.1.1.1.1.1.cmml" xref="S7.SS1.p3.8.m8.1.1.1.1.1"></eq><ci id="S7.SS1.p3.8.m8.1.1.1.1.2.cmml" xref="S7.SS1.p3.8.m8.1.1.1.1.2">𝑡</ci><cn id="S7.SS1.p3.8.m8.1.1.1.1.3.cmml" type="float" xref="S7.SS1.p3.8.m8.1.1.1.1.3">3.184</cn></apply><apply id="S7.SS1.p3.8.m8.2.2.2.2.cmml" xref="S7.SS1.p3.8.m8.2.2.2.2"><eq id="S7.SS1.p3.8.m8.2.2.2.2.1.cmml" xref="S7.SS1.p3.8.m8.2.2.2.2.1"></eq><ci id="S7.SS1.p3.8.m8.2.2.2.2.2.cmml" xref="S7.SS1.p3.8.m8.2.2.2.2.2">𝑝</ci><cn id="S7.SS1.p3.8.m8.2.2.2.2.3.cmml" type="float" xref="S7.SS1.p3.8.m8.2.2.2.2.3">.0266</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p3.8.m8.2c">t=3.184,p=.0266</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p3.8.m8.2d">italic_t = 3.184 , italic_p = .0266</annotation></semantics></math>). The results indicate that participants enjoyed their overall experience with Influencer. Influencer supported graphic design explorations effectively, enhanced users’ expressiveness during the creative process and increased their satisfaction with their design outcomes.</p>
</div>
<figure class="ltx_figure" id="S7.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="498" id="S7.F10.g1" src="x6.png" width="830"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Results of Creative Support Index (CSI) for Influencer and Baseline (the higher the better) on the factors of enjoyment, exploration, expressiveness, immersion, and results worth effort. </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S7.F10.1">\Description</span></div>
</div>
</figure>
<div class="ltx_para" id="S7.SS1.p4">
<p class="ltx_p" id="S7.SS1.p4.18"><span class="ltx_text ltx_font_bold" id="S7.SS1.p4.18.1">Usefulness of Functions</span>. We also evaluated different functionalities within Influencer to see which ones are comparatively more useful with a 5-point Likert scale: mind-map based Visual Representation (<math alttext="M=4.1" class="ltx_Math" display="inline" id="S7.SS1.p4.1.m1.1"><semantics id="S7.SS1.p4.1.m1.1a"><mrow id="S7.SS1.p4.1.m1.1.1" xref="S7.SS1.p4.1.m1.1.1.cmml"><mi id="S7.SS1.p4.1.m1.1.1.2" xref="S7.SS1.p4.1.m1.1.1.2.cmml">M</mi><mo id="S7.SS1.p4.1.m1.1.1.1" xref="S7.SS1.p4.1.m1.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.1.m1.1.1.3" xref="S7.SS1.p4.1.m1.1.1.3.cmml">4.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.1.m1.1b"><apply id="S7.SS1.p4.1.m1.1.1.cmml" xref="S7.SS1.p4.1.m1.1.1"><eq id="S7.SS1.p4.1.m1.1.1.1.cmml" xref="S7.SS1.p4.1.m1.1.1.1"></eq><ci id="S7.SS1.p4.1.m1.1.1.2.cmml" xref="S7.SS1.p4.1.m1.1.1.2">𝑀</ci><cn id="S7.SS1.p4.1.m1.1.1.3.cmml" type="float" xref="S7.SS1.p4.1.m1.1.1.3">4.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.1.m1.1c">M=4.1</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.1.m1.1d">italic_M = 4.1</annotation></semantics></math>, <math alttext="SD=.71" class="ltx_Math" display="inline" id="S7.SS1.p4.2.m2.1"><semantics id="S7.SS1.p4.2.m2.1a"><mrow id="S7.SS1.p4.2.m2.1.1" xref="S7.SS1.p4.2.m2.1.1.cmml"><mrow id="S7.SS1.p4.2.m2.1.1.2" xref="S7.SS1.p4.2.m2.1.1.2.cmml"><mi id="S7.SS1.p4.2.m2.1.1.2.2" xref="S7.SS1.p4.2.m2.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p4.2.m2.1.1.2.1" xref="S7.SS1.p4.2.m2.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p4.2.m2.1.1.2.3" xref="S7.SS1.p4.2.m2.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p4.2.m2.1.1.1" xref="S7.SS1.p4.2.m2.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.2.m2.1.1.3" xref="S7.SS1.p4.2.m2.1.1.3.cmml">.71</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.2.m2.1b"><apply id="S7.SS1.p4.2.m2.1.1.cmml" xref="S7.SS1.p4.2.m2.1.1"><eq id="S7.SS1.p4.2.m2.1.1.1.cmml" xref="S7.SS1.p4.2.m2.1.1.1"></eq><apply id="S7.SS1.p4.2.m2.1.1.2.cmml" xref="S7.SS1.p4.2.m2.1.1.2"><times id="S7.SS1.p4.2.m2.1.1.2.1.cmml" xref="S7.SS1.p4.2.m2.1.1.2.1"></times><ci id="S7.SS1.p4.2.m2.1.1.2.2.cmml" xref="S7.SS1.p4.2.m2.1.1.2.2">𝑆</ci><ci id="S7.SS1.p4.2.m2.1.1.2.3.cmml" xref="S7.SS1.p4.2.m2.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p4.2.m2.1.1.3.cmml" type="float" xref="S7.SS1.p4.2.m2.1.1.3">.71</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.2.m2.1c">SD=.71</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.2.m2.1d">italic_S italic_D = .71</annotation></semantics></math>), Materials Fusion (<math alttext="M=3.64" class="ltx_Math" display="inline" id="S7.SS1.p4.3.m3.1"><semantics id="S7.SS1.p4.3.m3.1a"><mrow id="S7.SS1.p4.3.m3.1.1" xref="S7.SS1.p4.3.m3.1.1.cmml"><mi id="S7.SS1.p4.3.m3.1.1.2" xref="S7.SS1.p4.3.m3.1.1.2.cmml">M</mi><mo id="S7.SS1.p4.3.m3.1.1.1" xref="S7.SS1.p4.3.m3.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.3.m3.1.1.3" xref="S7.SS1.p4.3.m3.1.1.3.cmml">3.64</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.3.m3.1b"><apply id="S7.SS1.p4.3.m3.1.1.cmml" xref="S7.SS1.p4.3.m3.1.1"><eq id="S7.SS1.p4.3.m3.1.1.1.cmml" xref="S7.SS1.p4.3.m3.1.1.1"></eq><ci id="S7.SS1.p4.3.m3.1.1.2.cmml" xref="S7.SS1.p4.3.m3.1.1.2">𝑀</ci><cn id="S7.SS1.p4.3.m3.1.1.3.cmml" type="float" xref="S7.SS1.p4.3.m3.1.1.3">3.64</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.3.m3.1c">M=3.64</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.3.m3.1d">italic_M = 3.64</annotation></semantics></math>, <math alttext="SD=1.24" class="ltx_Math" display="inline" id="S7.SS1.p4.4.m4.1"><semantics id="S7.SS1.p4.4.m4.1a"><mrow id="S7.SS1.p4.4.m4.1.1" xref="S7.SS1.p4.4.m4.1.1.cmml"><mrow id="S7.SS1.p4.4.m4.1.1.2" xref="S7.SS1.p4.4.m4.1.1.2.cmml"><mi id="S7.SS1.p4.4.m4.1.1.2.2" xref="S7.SS1.p4.4.m4.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p4.4.m4.1.1.2.1" xref="S7.SS1.p4.4.m4.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p4.4.m4.1.1.2.3" xref="S7.SS1.p4.4.m4.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p4.4.m4.1.1.1" xref="S7.SS1.p4.4.m4.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.4.m4.1.1.3" xref="S7.SS1.p4.4.m4.1.1.3.cmml">1.24</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.4.m4.1b"><apply id="S7.SS1.p4.4.m4.1.1.cmml" xref="S7.SS1.p4.4.m4.1.1"><eq id="S7.SS1.p4.4.m4.1.1.1.cmml" xref="S7.SS1.p4.4.m4.1.1.1"></eq><apply id="S7.SS1.p4.4.m4.1.1.2.cmml" xref="S7.SS1.p4.4.m4.1.1.2"><times id="S7.SS1.p4.4.m4.1.1.2.1.cmml" xref="S7.SS1.p4.4.m4.1.1.2.1"></times><ci id="S7.SS1.p4.4.m4.1.1.2.2.cmml" xref="S7.SS1.p4.4.m4.1.1.2.2">𝑆</ci><ci id="S7.SS1.p4.4.m4.1.1.2.3.cmml" xref="S7.SS1.p4.4.m4.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p4.4.m4.1.1.3.cmml" type="float" xref="S7.SS1.p4.4.m4.1.1.3">1.24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.4.m4.1c">SD=1.24</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.4.m4.1d">italic_S italic_D = 1.24</annotation></semantics></math>), Post Generation (<math alttext="M=3.9" class="ltx_Math" display="inline" id="S7.SS1.p4.5.m5.1"><semantics id="S7.SS1.p4.5.m5.1a"><mrow id="S7.SS1.p4.5.m5.1.1" xref="S7.SS1.p4.5.m5.1.1.cmml"><mi id="S7.SS1.p4.5.m5.1.1.2" xref="S7.SS1.p4.5.m5.1.1.2.cmml">M</mi><mo id="S7.SS1.p4.5.m5.1.1.1" xref="S7.SS1.p4.5.m5.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.5.m5.1.1.3" xref="S7.SS1.p4.5.m5.1.1.3.cmml">3.9</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.5.m5.1b"><apply id="S7.SS1.p4.5.m5.1.1.cmml" xref="S7.SS1.p4.5.m5.1.1"><eq id="S7.SS1.p4.5.m5.1.1.1.cmml" xref="S7.SS1.p4.5.m5.1.1.1"></eq><ci id="S7.SS1.p4.5.m5.1.1.2.cmml" xref="S7.SS1.p4.5.m5.1.1.2">𝑀</ci><cn id="S7.SS1.p4.5.m5.1.1.3.cmml" type="float" xref="S7.SS1.p4.5.m5.1.1.3">3.9</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.5.m5.1c">M=3.9</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.5.m5.1d">italic_M = 3.9</annotation></semantics></math>, <math alttext="SD=.84" class="ltx_Math" display="inline" id="S7.SS1.p4.6.m6.1"><semantics id="S7.SS1.p4.6.m6.1a"><mrow id="S7.SS1.p4.6.m6.1.1" xref="S7.SS1.p4.6.m6.1.1.cmml"><mrow id="S7.SS1.p4.6.m6.1.1.2" xref="S7.SS1.p4.6.m6.1.1.2.cmml"><mi id="S7.SS1.p4.6.m6.1.1.2.2" xref="S7.SS1.p4.6.m6.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p4.6.m6.1.1.2.1" xref="S7.SS1.p4.6.m6.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p4.6.m6.1.1.2.3" xref="S7.SS1.p4.6.m6.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p4.6.m6.1.1.1" xref="S7.SS1.p4.6.m6.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.6.m6.1.1.3" xref="S7.SS1.p4.6.m6.1.1.3.cmml">.84</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.6.m6.1b"><apply id="S7.SS1.p4.6.m6.1.1.cmml" xref="S7.SS1.p4.6.m6.1.1"><eq id="S7.SS1.p4.6.m6.1.1.1.cmml" xref="S7.SS1.p4.6.m6.1.1.1"></eq><apply id="S7.SS1.p4.6.m6.1.1.2.cmml" xref="S7.SS1.p4.6.m6.1.1.2"><times id="S7.SS1.p4.6.m6.1.1.2.1.cmml" xref="S7.SS1.p4.6.m6.1.1.2.1"></times><ci id="S7.SS1.p4.6.m6.1.1.2.2.cmml" xref="S7.SS1.p4.6.m6.1.1.2.2">𝑆</ci><ci id="S7.SS1.p4.6.m6.1.1.2.3.cmml" xref="S7.SS1.p4.6.m6.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p4.6.m6.1.1.3.cmml" type="float" xref="S7.SS1.p4.6.m6.1.1.3">.84</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.6.m6.1c">SD=.84</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.6.m6.1d">italic_S italic_D = .84</annotation></semantics></math>), Caption Recommendation (<math alttext="M=3.42" class="ltx_Math" display="inline" id="S7.SS1.p4.7.m7.1"><semantics id="S7.SS1.p4.7.m7.1a"><mrow id="S7.SS1.p4.7.m7.1.1" xref="S7.SS1.p4.7.m7.1.1.cmml"><mi id="S7.SS1.p4.7.m7.1.1.2" xref="S7.SS1.p4.7.m7.1.1.2.cmml">M</mi><mo id="S7.SS1.p4.7.m7.1.1.1" xref="S7.SS1.p4.7.m7.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.7.m7.1.1.3" xref="S7.SS1.p4.7.m7.1.1.3.cmml">3.42</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.7.m7.1b"><apply id="S7.SS1.p4.7.m7.1.1.cmml" xref="S7.SS1.p4.7.m7.1.1"><eq id="S7.SS1.p4.7.m7.1.1.1.cmml" xref="S7.SS1.p4.7.m7.1.1.1"></eq><ci id="S7.SS1.p4.7.m7.1.1.2.cmml" xref="S7.SS1.p4.7.m7.1.1.2">𝑀</ci><cn id="S7.SS1.p4.7.m7.1.1.3.cmml" type="float" xref="S7.SS1.p4.7.m7.1.1.3">3.42</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.7.m7.1c">M=3.42</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.7.m7.1d">italic_M = 3.42</annotation></semantics></math>, <math alttext="SD=.65" class="ltx_Math" display="inline" id="S7.SS1.p4.8.m8.1"><semantics id="S7.SS1.p4.8.m8.1a"><mrow id="S7.SS1.p4.8.m8.1.1" xref="S7.SS1.p4.8.m8.1.1.cmml"><mrow id="S7.SS1.p4.8.m8.1.1.2" xref="S7.SS1.p4.8.m8.1.1.2.cmml"><mi id="S7.SS1.p4.8.m8.1.1.2.2" xref="S7.SS1.p4.8.m8.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p4.8.m8.1.1.2.1" xref="S7.SS1.p4.8.m8.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p4.8.m8.1.1.2.3" xref="S7.SS1.p4.8.m8.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p4.8.m8.1.1.1" xref="S7.SS1.p4.8.m8.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.8.m8.1.1.3" xref="S7.SS1.p4.8.m8.1.1.3.cmml">.65</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.8.m8.1b"><apply id="S7.SS1.p4.8.m8.1.1.cmml" xref="S7.SS1.p4.8.m8.1.1"><eq id="S7.SS1.p4.8.m8.1.1.1.cmml" xref="S7.SS1.p4.8.m8.1.1.1"></eq><apply id="S7.SS1.p4.8.m8.1.1.2.cmml" xref="S7.SS1.p4.8.m8.1.1.2"><times id="S7.SS1.p4.8.m8.1.1.2.1.cmml" xref="S7.SS1.p4.8.m8.1.1.2.1"></times><ci id="S7.SS1.p4.8.m8.1.1.2.2.cmml" xref="S7.SS1.p4.8.m8.1.1.2.2">𝑆</ci><ci id="S7.SS1.p4.8.m8.1.1.2.3.cmml" xref="S7.SS1.p4.8.m8.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p4.8.m8.1.1.3.cmml" type="float" xref="S7.SS1.p4.8.m8.1.1.3">.65</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.8.m8.1c">SD=.65</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.8.m8.1d">italic_S italic_D = .65</annotation></semantics></math>), Semantic Recommendation for Image (<math alttext="M=3.88" class="ltx_Math" display="inline" id="S7.SS1.p4.9.m9.1"><semantics id="S7.SS1.p4.9.m9.1a"><mrow id="S7.SS1.p4.9.m9.1.1" xref="S7.SS1.p4.9.m9.1.1.cmml"><mi id="S7.SS1.p4.9.m9.1.1.2" xref="S7.SS1.p4.9.m9.1.1.2.cmml">M</mi><mo id="S7.SS1.p4.9.m9.1.1.1" xref="S7.SS1.p4.9.m9.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.9.m9.1.1.3" xref="S7.SS1.p4.9.m9.1.1.3.cmml">3.88</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.9.m9.1b"><apply id="S7.SS1.p4.9.m9.1.1.cmml" xref="S7.SS1.p4.9.m9.1.1"><eq id="S7.SS1.p4.9.m9.1.1.1.cmml" xref="S7.SS1.p4.9.m9.1.1.1"></eq><ci id="S7.SS1.p4.9.m9.1.1.2.cmml" xref="S7.SS1.p4.9.m9.1.1.2">𝑀</ci><cn id="S7.SS1.p4.9.m9.1.1.3.cmml" type="float" xref="S7.SS1.p4.9.m9.1.1.3">3.88</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.9.m9.1c">M=3.88</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.9.m9.1d">italic_M = 3.88</annotation></semantics></math>, <math alttext="SD=1.24" class="ltx_Math" display="inline" id="S7.SS1.p4.10.m10.1"><semantics id="S7.SS1.p4.10.m10.1a"><mrow id="S7.SS1.p4.10.m10.1.1" xref="S7.SS1.p4.10.m10.1.1.cmml"><mrow id="S7.SS1.p4.10.m10.1.1.2" xref="S7.SS1.p4.10.m10.1.1.2.cmml"><mi id="S7.SS1.p4.10.m10.1.1.2.2" xref="S7.SS1.p4.10.m10.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p4.10.m10.1.1.2.1" xref="S7.SS1.p4.10.m10.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p4.10.m10.1.1.2.3" xref="S7.SS1.p4.10.m10.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p4.10.m10.1.1.1" xref="S7.SS1.p4.10.m10.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.10.m10.1.1.3" xref="S7.SS1.p4.10.m10.1.1.3.cmml">1.24</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.10.m10.1b"><apply id="S7.SS1.p4.10.m10.1.1.cmml" xref="S7.SS1.p4.10.m10.1.1"><eq id="S7.SS1.p4.10.m10.1.1.1.cmml" xref="S7.SS1.p4.10.m10.1.1.1"></eq><apply id="S7.SS1.p4.10.m10.1.1.2.cmml" xref="S7.SS1.p4.10.m10.1.1.2"><times id="S7.SS1.p4.10.m10.1.1.2.1.cmml" xref="S7.SS1.p4.10.m10.1.1.2.1"></times><ci id="S7.SS1.p4.10.m10.1.1.2.2.cmml" xref="S7.SS1.p4.10.m10.1.1.2.2">𝑆</ci><ci id="S7.SS1.p4.10.m10.1.1.2.3.cmml" xref="S7.SS1.p4.10.m10.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p4.10.m10.1.1.3.cmml" type="float" xref="S7.SS1.p4.10.m10.1.1.3">1.24</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.10.m10.1c">SD=1.24</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.10.m10.1d">italic_S italic_D = 1.24</annotation></semantics></math>), Color Recommendation for Image (<math alttext="M=2.43" class="ltx_Math" display="inline" id="S7.SS1.p4.11.m11.1"><semantics id="S7.SS1.p4.11.m11.1a"><mrow id="S7.SS1.p4.11.m11.1.1" xref="S7.SS1.p4.11.m11.1.1.cmml"><mi id="S7.SS1.p4.11.m11.1.1.2" xref="S7.SS1.p4.11.m11.1.1.2.cmml">M</mi><mo id="S7.SS1.p4.11.m11.1.1.1" xref="S7.SS1.p4.11.m11.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.11.m11.1.1.3" xref="S7.SS1.p4.11.m11.1.1.3.cmml">2.43</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.11.m11.1b"><apply id="S7.SS1.p4.11.m11.1.1.cmml" xref="S7.SS1.p4.11.m11.1.1"><eq id="S7.SS1.p4.11.m11.1.1.1.cmml" xref="S7.SS1.p4.11.m11.1.1.1"></eq><ci id="S7.SS1.p4.11.m11.1.1.2.cmml" xref="S7.SS1.p4.11.m11.1.1.2">𝑀</ci><cn id="S7.SS1.p4.11.m11.1.1.3.cmml" type="float" xref="S7.SS1.p4.11.m11.1.1.3">2.43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.11.m11.1c">M=2.43</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.11.m11.1d">italic_M = 2.43</annotation></semantics></math>, <math alttext="SD=1.57" class="ltx_Math" display="inline" id="S7.SS1.p4.12.m12.1"><semantics id="S7.SS1.p4.12.m12.1a"><mrow id="S7.SS1.p4.12.m12.1.1" xref="S7.SS1.p4.12.m12.1.1.cmml"><mrow id="S7.SS1.p4.12.m12.1.1.2" xref="S7.SS1.p4.12.m12.1.1.2.cmml"><mi id="S7.SS1.p4.12.m12.1.1.2.2" xref="S7.SS1.p4.12.m12.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p4.12.m12.1.1.2.1" xref="S7.SS1.p4.12.m12.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p4.12.m12.1.1.2.3" xref="S7.SS1.p4.12.m12.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p4.12.m12.1.1.1" xref="S7.SS1.p4.12.m12.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.12.m12.1.1.3" xref="S7.SS1.p4.12.m12.1.1.3.cmml">1.57</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.12.m12.1b"><apply id="S7.SS1.p4.12.m12.1.1.cmml" xref="S7.SS1.p4.12.m12.1.1"><eq id="S7.SS1.p4.12.m12.1.1.1.cmml" xref="S7.SS1.p4.12.m12.1.1.1"></eq><apply id="S7.SS1.p4.12.m12.1.1.2.cmml" xref="S7.SS1.p4.12.m12.1.1.2"><times id="S7.SS1.p4.12.m12.1.1.2.1.cmml" xref="S7.SS1.p4.12.m12.1.1.2.1"></times><ci id="S7.SS1.p4.12.m12.1.1.2.2.cmml" xref="S7.SS1.p4.12.m12.1.1.2.2">𝑆</ci><ci id="S7.SS1.p4.12.m12.1.1.2.3.cmml" xref="S7.SS1.p4.12.m12.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p4.12.m12.1.1.3.cmml" type="float" xref="S7.SS1.p4.12.m12.1.1.3">1.57</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.12.m12.1c">SD=1.57</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.12.m12.1d">italic_S italic_D = 1.57</annotation></semantics></math>), Object Recommendation for Image (<math alttext="M=3.18" class="ltx_Math" display="inline" id="S7.SS1.p4.13.m13.1"><semantics id="S7.SS1.p4.13.m13.1a"><mrow id="S7.SS1.p4.13.m13.1.1" xref="S7.SS1.p4.13.m13.1.1.cmml"><mi id="S7.SS1.p4.13.m13.1.1.2" xref="S7.SS1.p4.13.m13.1.1.2.cmml">M</mi><mo id="S7.SS1.p4.13.m13.1.1.1" xref="S7.SS1.p4.13.m13.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.13.m13.1.1.3" xref="S7.SS1.p4.13.m13.1.1.3.cmml">3.18</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.13.m13.1b"><apply id="S7.SS1.p4.13.m13.1.1.cmml" xref="S7.SS1.p4.13.m13.1.1"><eq id="S7.SS1.p4.13.m13.1.1.1.cmml" xref="S7.SS1.p4.13.m13.1.1.1"></eq><ci id="S7.SS1.p4.13.m13.1.1.2.cmml" xref="S7.SS1.p4.13.m13.1.1.2">𝑀</ci><cn id="S7.SS1.p4.13.m13.1.1.3.cmml" type="float" xref="S7.SS1.p4.13.m13.1.1.3">3.18</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.13.m13.1c">M=3.18</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.13.m13.1d">italic_M = 3.18</annotation></semantics></math>, <math alttext="SD=1.16" class="ltx_Math" display="inline" id="S7.SS1.p4.14.m14.1"><semantics id="S7.SS1.p4.14.m14.1a"><mrow id="S7.SS1.p4.14.m14.1.1" xref="S7.SS1.p4.14.m14.1.1.cmml"><mrow id="S7.SS1.p4.14.m14.1.1.2" xref="S7.SS1.p4.14.m14.1.1.2.cmml"><mi id="S7.SS1.p4.14.m14.1.1.2.2" xref="S7.SS1.p4.14.m14.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p4.14.m14.1.1.2.1" xref="S7.SS1.p4.14.m14.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p4.14.m14.1.1.2.3" xref="S7.SS1.p4.14.m14.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p4.14.m14.1.1.1" xref="S7.SS1.p4.14.m14.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.14.m14.1.1.3" xref="S7.SS1.p4.14.m14.1.1.3.cmml">1.16</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.14.m14.1b"><apply id="S7.SS1.p4.14.m14.1.1.cmml" xref="S7.SS1.p4.14.m14.1.1"><eq id="S7.SS1.p4.14.m14.1.1.1.cmml" xref="S7.SS1.p4.14.m14.1.1.1"></eq><apply id="S7.SS1.p4.14.m14.1.1.2.cmml" xref="S7.SS1.p4.14.m14.1.1.2"><times id="S7.SS1.p4.14.m14.1.1.2.1.cmml" xref="S7.SS1.p4.14.m14.1.1.2.1"></times><ci id="S7.SS1.p4.14.m14.1.1.2.2.cmml" xref="S7.SS1.p4.14.m14.1.1.2.2">𝑆</ci><ci id="S7.SS1.p4.14.m14.1.1.2.3.cmml" xref="S7.SS1.p4.14.m14.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p4.14.m14.1.1.3.cmml" type="float" xref="S7.SS1.p4.14.m14.1.1.3">1.16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.14.m14.1c">SD=1.16</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.14.m14.1d">italic_S italic_D = 1.16</annotation></semantics></math>), Regenerate Image (<math alttext="M=2.36" class="ltx_Math" display="inline" id="S7.SS1.p4.15.m15.1"><semantics id="S7.SS1.p4.15.m15.1a"><mrow id="S7.SS1.p4.15.m15.1.1" xref="S7.SS1.p4.15.m15.1.1.cmml"><mi id="S7.SS1.p4.15.m15.1.1.2" xref="S7.SS1.p4.15.m15.1.1.2.cmml">M</mi><mo id="S7.SS1.p4.15.m15.1.1.1" xref="S7.SS1.p4.15.m15.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.15.m15.1.1.3" xref="S7.SS1.p4.15.m15.1.1.3.cmml">2.36</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.15.m15.1b"><apply id="S7.SS1.p4.15.m15.1.1.cmml" xref="S7.SS1.p4.15.m15.1.1"><eq id="S7.SS1.p4.15.m15.1.1.1.cmml" xref="S7.SS1.p4.15.m15.1.1.1"></eq><ci id="S7.SS1.p4.15.m15.1.1.2.cmml" xref="S7.SS1.p4.15.m15.1.1.2">𝑀</ci><cn id="S7.SS1.p4.15.m15.1.1.3.cmml" type="float" xref="S7.SS1.p4.15.m15.1.1.3">2.36</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.15.m15.1c">M=2.36</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.15.m15.1d">italic_M = 2.36</annotation></semantics></math>, <math alttext="SD=.89" class="ltx_Math" display="inline" id="S7.SS1.p4.16.m16.1"><semantics id="S7.SS1.p4.16.m16.1a"><mrow id="S7.SS1.p4.16.m16.1.1" xref="S7.SS1.p4.16.m16.1.1.cmml"><mrow id="S7.SS1.p4.16.m16.1.1.2" xref="S7.SS1.p4.16.m16.1.1.2.cmml"><mi id="S7.SS1.p4.16.m16.1.1.2.2" xref="S7.SS1.p4.16.m16.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p4.16.m16.1.1.2.1" xref="S7.SS1.p4.16.m16.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p4.16.m16.1.1.2.3" xref="S7.SS1.p4.16.m16.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p4.16.m16.1.1.1" xref="S7.SS1.p4.16.m16.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.16.m16.1.1.3" xref="S7.SS1.p4.16.m16.1.1.3.cmml">.89</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.16.m16.1b"><apply id="S7.SS1.p4.16.m16.1.1.cmml" xref="S7.SS1.p4.16.m16.1.1"><eq id="S7.SS1.p4.16.m16.1.1.1.cmml" xref="S7.SS1.p4.16.m16.1.1.1"></eq><apply id="S7.SS1.p4.16.m16.1.1.2.cmml" xref="S7.SS1.p4.16.m16.1.1.2"><times id="S7.SS1.p4.16.m16.1.1.2.1.cmml" xref="S7.SS1.p4.16.m16.1.1.2.1"></times><ci id="S7.SS1.p4.16.m16.1.1.2.2.cmml" xref="S7.SS1.p4.16.m16.1.1.2.2">𝑆</ci><ci id="S7.SS1.p4.16.m16.1.1.2.3.cmml" xref="S7.SS1.p4.16.m16.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p4.16.m16.1.1.3.cmml" type="float" xref="S7.SS1.p4.16.m16.1.1.3">.89</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.16.m16.1c">SD=.89</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.16.m16.1d">italic_S italic_D = .89</annotation></semantics></math>), Mask Edit Image (<math alttext="M=2.85" class="ltx_Math" display="inline" id="S7.SS1.p4.17.m17.1"><semantics id="S7.SS1.p4.17.m17.1a"><mrow id="S7.SS1.p4.17.m17.1.1" xref="S7.SS1.p4.17.m17.1.1.cmml"><mi id="S7.SS1.p4.17.m17.1.1.2" xref="S7.SS1.p4.17.m17.1.1.2.cmml">M</mi><mo id="S7.SS1.p4.17.m17.1.1.1" xref="S7.SS1.p4.17.m17.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.17.m17.1.1.3" xref="S7.SS1.p4.17.m17.1.1.3.cmml">2.85</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.17.m17.1b"><apply id="S7.SS1.p4.17.m17.1.1.cmml" xref="S7.SS1.p4.17.m17.1.1"><eq id="S7.SS1.p4.17.m17.1.1.1.cmml" xref="S7.SS1.p4.17.m17.1.1.1"></eq><ci id="S7.SS1.p4.17.m17.1.1.2.cmml" xref="S7.SS1.p4.17.m17.1.1.2">𝑀</ci><cn id="S7.SS1.p4.17.m17.1.1.3.cmml" type="float" xref="S7.SS1.p4.17.m17.1.1.3">2.85</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.17.m17.1c">M=2.85</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.17.m17.1d">italic_M = 2.85</annotation></semantics></math>, <math alttext="SD=1.26" class="ltx_Math" display="inline" id="S7.SS1.p4.18.m18.1"><semantics id="S7.SS1.p4.18.m18.1a"><mrow id="S7.SS1.p4.18.m18.1.1" xref="S7.SS1.p4.18.m18.1.1.cmml"><mrow id="S7.SS1.p4.18.m18.1.1.2" xref="S7.SS1.p4.18.m18.1.1.2.cmml"><mi id="S7.SS1.p4.18.m18.1.1.2.2" xref="S7.SS1.p4.18.m18.1.1.2.2.cmml">S</mi><mo id="S7.SS1.p4.18.m18.1.1.2.1" xref="S7.SS1.p4.18.m18.1.1.2.1.cmml">⁢</mo><mi id="S7.SS1.p4.18.m18.1.1.2.3" xref="S7.SS1.p4.18.m18.1.1.2.3.cmml">D</mi></mrow><mo id="S7.SS1.p4.18.m18.1.1.1" xref="S7.SS1.p4.18.m18.1.1.1.cmml">=</mo><mn id="S7.SS1.p4.18.m18.1.1.3" xref="S7.SS1.p4.18.m18.1.1.3.cmml">1.26</mn></mrow><annotation-xml encoding="MathML-Content" id="S7.SS1.p4.18.m18.1b"><apply id="S7.SS1.p4.18.m18.1.1.cmml" xref="S7.SS1.p4.18.m18.1.1"><eq id="S7.SS1.p4.18.m18.1.1.1.cmml" xref="S7.SS1.p4.18.m18.1.1.1"></eq><apply id="S7.SS1.p4.18.m18.1.1.2.cmml" xref="S7.SS1.p4.18.m18.1.1.2"><times id="S7.SS1.p4.18.m18.1.1.2.1.cmml" xref="S7.SS1.p4.18.m18.1.1.2.1"></times><ci id="S7.SS1.p4.18.m18.1.1.2.2.cmml" xref="S7.SS1.p4.18.m18.1.1.2.2">𝑆</ci><ci id="S7.SS1.p4.18.m18.1.1.2.3.cmml" xref="S7.SS1.p4.18.m18.1.1.2.3">𝐷</ci></apply><cn id="S7.SS1.p4.18.m18.1.1.3.cmml" type="float" xref="S7.SS1.p4.18.m18.1.1.3">1.26</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS1.p4.18.m18.1c">SD=1.26</annotation><annotation encoding="application/x-llamapun" id="S7.SS1.p4.18.m18.1d">italic_S italic_D = 1.26</annotation></semantics></math>).
As we can see, mind-map based Visual Representation, Post generation, and Dragging Interaction were recognized as the most useful functions in Influencer. However, we also find that some functions have deviations, including materials fusion, semantics/color/object exploration, and mask edit images. According to our observations, this kind of disagreement in usefulness evaluation reflects different user preferences with different post design habits and the limitation of image generation models (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S8" title="8. Discussion ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Section 8</span></a> for more discussion).</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Qualitative Results</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">In general, participants appreciated various novel functions of Influencer. We summarize their feedback from the semi-structured interviews based on the following themes.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p2.1.1">General preference.</span> Overall, 10 out of 12 participants stated that they would like to use Influencer as their promotional post design tool over the baseline system. Two other participants preferred to combine the two systems to do image searches and get image and caption recommendations but did image customization on another interface.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p3.1.1">Image/ caption recommendation</span> Participants thought the overall quality of the image-searching results met their expectations. Eight noted that the images retrieved were notably creative and useful compared to their prior experiences using a search engine (P1-2, P4, P10-12). They also mentioned the usability of three-dimensional recommendation for image and caption and stated <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.2">“I would appreciate using this function when lacking initial ideas on post design. The recommendations fit my design goal well. The three-dimensional recommendation helps me find more inspiration with just a few seed images.”</span> (P6). In addition, participants mentioned that even though there are some recommended images are not suitable for the topic, it is still useful since Influencer already saves them more time in designing a final version of the post design (P8). Further, <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.3">“Based on those recommendations, the post design process becomes easier and reduces my mental effort required for brainstorming.”</span> (P7). Meanwhile, three participants (P4, P10, P11) recognized the three recommendation directions on caption based on different contexts: <span class="ltx_text ltx_font_italic" id="S7.SS2.p3.1.4">“I can quickly change my post captions to fit different scenes, making it much easier than starting from scratch.”</span> (P9). The feedback indicates that the multi-dimensional image and caption recommendation is useful to facilitate the promotional post design with desired content.</p>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p4.1.1">Context-Aware Exploration quality</span>
All participants found that the context-aware recommendation is helpful. The utility of context-aware exploration was highlighted when exploring the images and captions based on the given materials: <span class="ltx_text ltx_font_italic" id="S7.SS2.p4.1.2">e.g.,</span> <span class="ltx_text ltx_font_italic" id="S7.SS2.p4.1.3">“it swiftly guides users to find post images and captions that align with their given brand logo or product message in terms of color, semantics, and objects””</span> (P1-3), <span class="ltx_text ltx_font_italic" id="S7.SS2.p4.1.4">“context awareness on caption exploration is nice since I can get a bunch of captions at a time matching my requirement of linguistic preferences or product relevance””</span> (P5). Moreover, the logical interaction of context-aware exploration was also recognized by 3 participants(P6, P7, P10): <span class="ltx_text ltx_font_italic" id="S7.SS2.p4.1.5">“It is very user-friendly to allow me to update the recommendation stuff by dragging the uploaded images or created text prompt to the target recommendation block.”</span> Another participant also reported: <span class="ltx_text ltx_font_italic" id="S7.SS2.p4.1.6">“It tells me why this recommendation block is updated, so I can track and know what to focus on.”</span></p>
</div>
<div class="ltx_para" id="S7.SS2.p5">
<p class="ltx_p" id="S7.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p5.1.1">Flexible fusion of various images and captions.</span> This is a unique experience reported by eight participants. They found it to be very useful when they wish to directly customize an image or caption by writing a prompt or uploading the image they possess (P1-2, P8, P10-12). By flexibly fusing the image or caption, they could <span class="ltx_text ltx_font_italic" id="S7.SS2.p5.1.2">“freely customize the material based on my preference and iteratively brainstorm”</span>(P12). P1 also mentioned <span class="ltx_text ltx_font_italic" id="S7.SS2.p5.1.3">“It is very helpful when I want to rewrite a caption incorporating the image I possess since it directly helped me integrate the information from the images into the previous captions smoothly.”</span> P4 echoed, <span class="ltx_text ltx_font_italic" id="S7.SS2.p5.1.4">“I feel it convenient to design the captions or images based on my personal preference by using different prompts or images. It helps me get multiple alternatives to compare later.”</span></p>
</div>
<div class="ltx_para" id="S7.SS2.p6">
<p class="ltx_p" id="S7.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.p6.1.1">Mind-map layout to track thinking path and record design alternatives.</span> Four participants found it very useful to trace back their thinking path (P6-8, P10). By enabling tracking design history, they can <span class="ltx_text ltx_font_italic" id="S7.SS2.p6.1.2">“check their design process, freely fuse different materials, and review multiple design alternatives.”</span> P7 also added that <span class="ltx_text ltx_font_italic" id="S7.SS2.p6.1.3">“It records my thoughts and helps me track my thinking process. Moreover, I indeed feel secure with this feature, and enables me to explore freely.”</span> Another participant also mentioned different colors for dragging features helped them clearly differentiate the exploration process and customize the process (P11). He recognized the Influencer <span class="ltx_text ltx_font_italic" id="S7.SS2.p6.1.4">“showing how the initial thoughts have been refined and making it easier for users or team members to follow the thought process and rationale behind decisions”</span> P12 added, <span class="ltx_text ltx_font_italic" id="S7.SS2.p6.1.5">“I can personally upload an image or write a prompt to customize the image or caption exploration path, which is very flexible and helps me find the related image efficiently.”</span></p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3. </span>Expert Assessment</h3>
<figure class="ltx_figure" id="S7.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="250" id="S7.F11.g1" src="x7.png" width="829"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Experts’ ratings on the quality of participant-generated promotional posts on a 5-point Likert scale (the higher the better) for five aspects: overall satisfaction (R1), clarity of content (R2), audience engagement (R3), content consistency (R4), and aesthetics (R5). </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S7.F11.1">\Description</span></div>
</div>
</figure>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">We recruited three expert designers to help us rate and analyze participant promotional post designs. We also invited them to share their views regarding Influencer afterward. The experts’ backgrounds are as follows: 1) an assistant professor, a man, specializing in graphic design with a PhD degree, with 15 years of design; 2) a visual designer in the industry, a woman, focusing on advertising design, holding a master’s degree in design theory, with eight years of design experience; 3) an art teacher, a woman, teaching art design in high school and usually create design contents for the school’s social media, with ten years of design. We denote them as EA1-3, respectively.</p>
</div>
<div class="ltx_para" id="S7.SS3.p2">
<p class="ltx_p" id="S7.SS3.p2.5"><span class="ltx_text ltx_font_bold" id="S7.SS3.p2.5.1">Post Quality Assessment</span>. While the CSI assessment in our questionnaire indicates participants’ self-experience of the created promotional post, these do not reflect how the slides are received by the audience. To assess the quality of the promotional posts created with Influencer, we invited the three experts to rate the post drafts from 12 participants. For a fair comparison, we removed the trials from the two conditions if the participants did not complete that task with either of the tools. Each expert rated the promotional posts by participants on a 5-point Likert Item and in random order. The results of their ratings on five different aspects are shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2407.14928v1#S7.F11" title="Figure 11 ‣ 7.3. Expert Assessment ‣ 7. Results ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">Figure 11</span></a>. The Wilcoxon signed-rank tests found that the outcomes of Influencer had significantly better ratings than those of the Baseline for all aspects: overall satisfaction (<math alttext="t=3.245,p=.0060" class="ltx_Math" display="inline" id="S7.SS3.p2.1.m1.2"><semantics id="S7.SS3.p2.1.m1.2a"><mrow id="S7.SS3.p2.1.m1.2.2.2" xref="S7.SS3.p2.1.m1.2.2.3.cmml"><mrow id="S7.SS3.p2.1.m1.1.1.1.1" xref="S7.SS3.p2.1.m1.1.1.1.1.cmml"><mi id="S7.SS3.p2.1.m1.1.1.1.1.2" xref="S7.SS3.p2.1.m1.1.1.1.1.2.cmml">t</mi><mo id="S7.SS3.p2.1.m1.1.1.1.1.1" xref="S7.SS3.p2.1.m1.1.1.1.1.1.cmml">=</mo><mn id="S7.SS3.p2.1.m1.1.1.1.1.3" xref="S7.SS3.p2.1.m1.1.1.1.1.3.cmml">3.245</mn></mrow><mo id="S7.SS3.p2.1.m1.2.2.2.3" xref="S7.SS3.p2.1.m1.2.2.3a.cmml">,</mo><mrow id="S7.SS3.p2.1.m1.2.2.2.2" xref="S7.SS3.p2.1.m1.2.2.2.2.cmml"><mi id="S7.SS3.p2.1.m1.2.2.2.2.2" xref="S7.SS3.p2.1.m1.2.2.2.2.2.cmml">p</mi><mo id="S7.SS3.p2.1.m1.2.2.2.2.1" xref="S7.SS3.p2.1.m1.2.2.2.2.1.cmml">=</mo><mn id="S7.SS3.p2.1.m1.2.2.2.2.3" xref="S7.SS3.p2.1.m1.2.2.2.2.3.cmml">.0060</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p2.1.m1.2b"><apply id="S7.SS3.p2.1.m1.2.2.3.cmml" xref="S7.SS3.p2.1.m1.2.2.2"><csymbol cd="ambiguous" id="S7.SS3.p2.1.m1.2.2.3a.cmml" xref="S7.SS3.p2.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS3.p2.1.m1.1.1.1.1.cmml" xref="S7.SS3.p2.1.m1.1.1.1.1"><eq id="S7.SS3.p2.1.m1.1.1.1.1.1.cmml" xref="S7.SS3.p2.1.m1.1.1.1.1.1"></eq><ci id="S7.SS3.p2.1.m1.1.1.1.1.2.cmml" xref="S7.SS3.p2.1.m1.1.1.1.1.2">𝑡</ci><cn id="S7.SS3.p2.1.m1.1.1.1.1.3.cmml" type="float" xref="S7.SS3.p2.1.m1.1.1.1.1.3">3.245</cn></apply><apply id="S7.SS3.p2.1.m1.2.2.2.2.cmml" xref="S7.SS3.p2.1.m1.2.2.2.2"><eq id="S7.SS3.p2.1.m1.2.2.2.2.1.cmml" xref="S7.SS3.p2.1.m1.2.2.2.2.1"></eq><ci id="S7.SS3.p2.1.m1.2.2.2.2.2.cmml" xref="S7.SS3.p2.1.m1.2.2.2.2.2">𝑝</ci><cn id="S7.SS3.p2.1.m1.2.2.2.2.3.cmml" type="float" xref="S7.SS3.p2.1.m1.2.2.2.2.3">.0060</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p2.1.m1.2c">t=3.245,p=.0060</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p2.1.m1.2d">italic_t = 3.245 , italic_p = .0060</annotation></semantics></math>), clarity of content (<math alttext="t=2.864,p=.0405" class="ltx_Math" display="inline" id="S7.SS3.p2.2.m2.2"><semantics id="S7.SS3.p2.2.m2.2a"><mrow id="S7.SS3.p2.2.m2.2.2.2" xref="S7.SS3.p2.2.m2.2.2.3.cmml"><mrow id="S7.SS3.p2.2.m2.1.1.1.1" xref="S7.SS3.p2.2.m2.1.1.1.1.cmml"><mi id="S7.SS3.p2.2.m2.1.1.1.1.2" xref="S7.SS3.p2.2.m2.1.1.1.1.2.cmml">t</mi><mo id="S7.SS3.p2.2.m2.1.1.1.1.1" xref="S7.SS3.p2.2.m2.1.1.1.1.1.cmml">=</mo><mn id="S7.SS3.p2.2.m2.1.1.1.1.3" xref="S7.SS3.p2.2.m2.1.1.1.1.3.cmml">2.864</mn></mrow><mo id="S7.SS3.p2.2.m2.2.2.2.3" xref="S7.SS3.p2.2.m2.2.2.3a.cmml">,</mo><mrow id="S7.SS3.p2.2.m2.2.2.2.2" xref="S7.SS3.p2.2.m2.2.2.2.2.cmml"><mi id="S7.SS3.p2.2.m2.2.2.2.2.2" xref="S7.SS3.p2.2.m2.2.2.2.2.2.cmml">p</mi><mo id="S7.SS3.p2.2.m2.2.2.2.2.1" xref="S7.SS3.p2.2.m2.2.2.2.2.1.cmml">=</mo><mn id="S7.SS3.p2.2.m2.2.2.2.2.3" xref="S7.SS3.p2.2.m2.2.2.2.2.3.cmml">.0405</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p2.2.m2.2b"><apply id="S7.SS3.p2.2.m2.2.2.3.cmml" xref="S7.SS3.p2.2.m2.2.2.2"><csymbol cd="ambiguous" id="S7.SS3.p2.2.m2.2.2.3a.cmml" xref="S7.SS3.p2.2.m2.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS3.p2.2.m2.1.1.1.1.cmml" xref="S7.SS3.p2.2.m2.1.1.1.1"><eq id="S7.SS3.p2.2.m2.1.1.1.1.1.cmml" xref="S7.SS3.p2.2.m2.1.1.1.1.1"></eq><ci id="S7.SS3.p2.2.m2.1.1.1.1.2.cmml" xref="S7.SS3.p2.2.m2.1.1.1.1.2">𝑡</ci><cn id="S7.SS3.p2.2.m2.1.1.1.1.3.cmml" type="float" xref="S7.SS3.p2.2.m2.1.1.1.1.3">2.864</cn></apply><apply id="S7.SS3.p2.2.m2.2.2.2.2.cmml" xref="S7.SS3.p2.2.m2.2.2.2.2"><eq id="S7.SS3.p2.2.m2.2.2.2.2.1.cmml" xref="S7.SS3.p2.2.m2.2.2.2.2.1"></eq><ci id="S7.SS3.p2.2.m2.2.2.2.2.2.cmml" xref="S7.SS3.p2.2.m2.2.2.2.2.2">𝑝</ci><cn id="S7.SS3.p2.2.m2.2.2.2.2.3.cmml" type="float" xref="S7.SS3.p2.2.m2.2.2.2.2.3">.0405</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p2.2.m2.2c">t=2.864,p=.0405</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p2.2.m2.2d">italic_t = 2.864 , italic_p = .0405</annotation></semantics></math>), audience engagement (<math alttext="t=2.693,p=.0398" class="ltx_Math" display="inline" id="S7.SS3.p2.3.m3.2"><semantics id="S7.SS3.p2.3.m3.2a"><mrow id="S7.SS3.p2.3.m3.2.2.2" xref="S7.SS3.p2.3.m3.2.2.3.cmml"><mrow id="S7.SS3.p2.3.m3.1.1.1.1" xref="S7.SS3.p2.3.m3.1.1.1.1.cmml"><mi id="S7.SS3.p2.3.m3.1.1.1.1.2" xref="S7.SS3.p2.3.m3.1.1.1.1.2.cmml">t</mi><mo id="S7.SS3.p2.3.m3.1.1.1.1.1" xref="S7.SS3.p2.3.m3.1.1.1.1.1.cmml">=</mo><mn id="S7.SS3.p2.3.m3.1.1.1.1.3" xref="S7.SS3.p2.3.m3.1.1.1.1.3.cmml">2.693</mn></mrow><mo id="S7.SS3.p2.3.m3.2.2.2.3" xref="S7.SS3.p2.3.m3.2.2.3a.cmml">,</mo><mrow id="S7.SS3.p2.3.m3.2.2.2.2" xref="S7.SS3.p2.3.m3.2.2.2.2.cmml"><mi id="S7.SS3.p2.3.m3.2.2.2.2.2" xref="S7.SS3.p2.3.m3.2.2.2.2.2.cmml">p</mi><mo id="S7.SS3.p2.3.m3.2.2.2.2.1" xref="S7.SS3.p2.3.m3.2.2.2.2.1.cmml">=</mo><mn id="S7.SS3.p2.3.m3.2.2.2.2.3" xref="S7.SS3.p2.3.m3.2.2.2.2.3.cmml">.0398</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p2.3.m3.2b"><apply id="S7.SS3.p2.3.m3.2.2.3.cmml" xref="S7.SS3.p2.3.m3.2.2.2"><csymbol cd="ambiguous" id="S7.SS3.p2.3.m3.2.2.3a.cmml" xref="S7.SS3.p2.3.m3.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS3.p2.3.m3.1.1.1.1.cmml" xref="S7.SS3.p2.3.m3.1.1.1.1"><eq id="S7.SS3.p2.3.m3.1.1.1.1.1.cmml" xref="S7.SS3.p2.3.m3.1.1.1.1.1"></eq><ci id="S7.SS3.p2.3.m3.1.1.1.1.2.cmml" xref="S7.SS3.p2.3.m3.1.1.1.1.2">𝑡</ci><cn id="S7.SS3.p2.3.m3.1.1.1.1.3.cmml" type="float" xref="S7.SS3.p2.3.m3.1.1.1.1.3">2.693</cn></apply><apply id="S7.SS3.p2.3.m3.2.2.2.2.cmml" xref="S7.SS3.p2.3.m3.2.2.2.2"><eq id="S7.SS3.p2.3.m3.2.2.2.2.1.cmml" xref="S7.SS3.p2.3.m3.2.2.2.2.1"></eq><ci id="S7.SS3.p2.3.m3.2.2.2.2.2.cmml" xref="S7.SS3.p2.3.m3.2.2.2.2.2">𝑝</ci><cn id="S7.SS3.p2.3.m3.2.2.2.2.3.cmml" type="float" xref="S7.SS3.p2.3.m3.2.2.2.2.3">.0398</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p2.3.m3.2c">t=2.693,p=.0398</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p2.3.m3.2d">italic_t = 2.693 , italic_p = .0398</annotation></semantics></math>), content consistency (<math alttext="t=3.843,p=.0270" class="ltx_Math" display="inline" id="S7.SS3.p2.4.m4.2"><semantics id="S7.SS3.p2.4.m4.2a"><mrow id="S7.SS3.p2.4.m4.2.2.2" xref="S7.SS3.p2.4.m4.2.2.3.cmml"><mrow id="S7.SS3.p2.4.m4.1.1.1.1" xref="S7.SS3.p2.4.m4.1.1.1.1.cmml"><mi id="S7.SS3.p2.4.m4.1.1.1.1.2" xref="S7.SS3.p2.4.m4.1.1.1.1.2.cmml">t</mi><mo id="S7.SS3.p2.4.m4.1.1.1.1.1" xref="S7.SS3.p2.4.m4.1.1.1.1.1.cmml">=</mo><mn id="S7.SS3.p2.4.m4.1.1.1.1.3" xref="S7.SS3.p2.4.m4.1.1.1.1.3.cmml">3.843</mn></mrow><mo id="S7.SS3.p2.4.m4.2.2.2.3" xref="S7.SS3.p2.4.m4.2.2.3a.cmml">,</mo><mrow id="S7.SS3.p2.4.m4.2.2.2.2" xref="S7.SS3.p2.4.m4.2.2.2.2.cmml"><mi id="S7.SS3.p2.4.m4.2.2.2.2.2" xref="S7.SS3.p2.4.m4.2.2.2.2.2.cmml">p</mi><mo id="S7.SS3.p2.4.m4.2.2.2.2.1" xref="S7.SS3.p2.4.m4.2.2.2.2.1.cmml">=</mo><mn id="S7.SS3.p2.4.m4.2.2.2.2.3" xref="S7.SS3.p2.4.m4.2.2.2.2.3.cmml">.0270</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p2.4.m4.2b"><apply id="S7.SS3.p2.4.m4.2.2.3.cmml" xref="S7.SS3.p2.4.m4.2.2.2"><csymbol cd="ambiguous" id="S7.SS3.p2.4.m4.2.2.3a.cmml" xref="S7.SS3.p2.4.m4.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS3.p2.4.m4.1.1.1.1.cmml" xref="S7.SS3.p2.4.m4.1.1.1.1"><eq id="S7.SS3.p2.4.m4.1.1.1.1.1.cmml" xref="S7.SS3.p2.4.m4.1.1.1.1.1"></eq><ci id="S7.SS3.p2.4.m4.1.1.1.1.2.cmml" xref="S7.SS3.p2.4.m4.1.1.1.1.2">𝑡</ci><cn id="S7.SS3.p2.4.m4.1.1.1.1.3.cmml" type="float" xref="S7.SS3.p2.4.m4.1.1.1.1.3">3.843</cn></apply><apply id="S7.SS3.p2.4.m4.2.2.2.2.cmml" xref="S7.SS3.p2.4.m4.2.2.2.2"><eq id="S7.SS3.p2.4.m4.2.2.2.2.1.cmml" xref="S7.SS3.p2.4.m4.2.2.2.2.1"></eq><ci id="S7.SS3.p2.4.m4.2.2.2.2.2.cmml" xref="S7.SS3.p2.4.m4.2.2.2.2.2">𝑝</ci><cn id="S7.SS3.p2.4.m4.2.2.2.2.3.cmml" type="float" xref="S7.SS3.p2.4.m4.2.2.2.2.3">.0270</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p2.4.m4.2c">t=3.843,p=.0270</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p2.4.m4.2d">italic_t = 3.843 , italic_p = .0270</annotation></semantics></math>), and aesthetics (<math alttext="t=3.325,p=.0318" class="ltx_Math" display="inline" id="S7.SS3.p2.5.m5.2"><semantics id="S7.SS3.p2.5.m5.2a"><mrow id="S7.SS3.p2.5.m5.2.2.2" xref="S7.SS3.p2.5.m5.2.2.3.cmml"><mrow id="S7.SS3.p2.5.m5.1.1.1.1" xref="S7.SS3.p2.5.m5.1.1.1.1.cmml"><mi id="S7.SS3.p2.5.m5.1.1.1.1.2" xref="S7.SS3.p2.5.m5.1.1.1.1.2.cmml">t</mi><mo id="S7.SS3.p2.5.m5.1.1.1.1.1" xref="S7.SS3.p2.5.m5.1.1.1.1.1.cmml">=</mo><mn id="S7.SS3.p2.5.m5.1.1.1.1.3" xref="S7.SS3.p2.5.m5.1.1.1.1.3.cmml">3.325</mn></mrow><mo id="S7.SS3.p2.5.m5.2.2.2.3" xref="S7.SS3.p2.5.m5.2.2.3a.cmml">,</mo><mrow id="S7.SS3.p2.5.m5.2.2.2.2" xref="S7.SS3.p2.5.m5.2.2.2.2.cmml"><mi id="S7.SS3.p2.5.m5.2.2.2.2.2" xref="S7.SS3.p2.5.m5.2.2.2.2.2.cmml">p</mi><mo id="S7.SS3.p2.5.m5.2.2.2.2.1" xref="S7.SS3.p2.5.m5.2.2.2.2.1.cmml">=</mo><mn id="S7.SS3.p2.5.m5.2.2.2.2.3" xref="S7.SS3.p2.5.m5.2.2.2.2.3.cmml">.0318</mn></mrow></mrow><annotation-xml encoding="MathML-Content" id="S7.SS3.p2.5.m5.2b"><apply id="S7.SS3.p2.5.m5.2.2.3.cmml" xref="S7.SS3.p2.5.m5.2.2.2"><csymbol cd="ambiguous" id="S7.SS3.p2.5.m5.2.2.3a.cmml" xref="S7.SS3.p2.5.m5.2.2.2.3">formulae-sequence</csymbol><apply id="S7.SS3.p2.5.m5.1.1.1.1.cmml" xref="S7.SS3.p2.5.m5.1.1.1.1"><eq id="S7.SS3.p2.5.m5.1.1.1.1.1.cmml" xref="S7.SS3.p2.5.m5.1.1.1.1.1"></eq><ci id="S7.SS3.p2.5.m5.1.1.1.1.2.cmml" xref="S7.SS3.p2.5.m5.1.1.1.1.2">𝑡</ci><cn id="S7.SS3.p2.5.m5.1.1.1.1.3.cmml" type="float" xref="S7.SS3.p2.5.m5.1.1.1.1.3">3.325</cn></apply><apply id="S7.SS3.p2.5.m5.2.2.2.2.cmml" xref="S7.SS3.p2.5.m5.2.2.2.2"><eq id="S7.SS3.p2.5.m5.2.2.2.2.1.cmml" xref="S7.SS3.p2.5.m5.2.2.2.2.1"></eq><ci id="S7.SS3.p2.5.m5.2.2.2.2.2.cmml" xref="S7.SS3.p2.5.m5.2.2.2.2.2">𝑝</ci><cn id="S7.SS3.p2.5.m5.2.2.2.2.3.cmml" type="float" xref="S7.SS3.p2.5.m5.2.2.2.2.3">.0318</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S7.SS3.p2.5.m5.2c">t=3.325,p=.0318</annotation><annotation encoding="application/x-llamapun" id="S7.SS3.p2.5.m5.2d">italic_t = 3.325 , italic_p = .0318</annotation></semantics></math>). The results indicate that Influencer could effectively facilitate users to generate better quality promotional posts, more consistent with the given topic, and more engaging, aesthetic, and clear for audiences.</p>
</div>
<div class="ltx_para" id="S7.SS3.p3">
<p class="ltx_p" id="S7.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.p3.1.1">Design of Influencer</span>. The three-dimensional recommendation framework for both images and captions was recognized as reasonable by all our experts. EA1 raised that <span class="ltx_text ltx_font_italic" id="S7.SS3.p3.1.2">“This brainstorming framework is helpful for novices to quickly learn basic design elements, save time on searching, and generate high-quality drafts, especially on post captions.”</span> EA2 reported that even this tool is <span class="ltx_text ltx_font_italic" id="S7.SS3.p3.1.3">“intriguing to interact with and useful especially when we have hands-on materials to design,”</span> and he would like to give it a try. Moreover, EA3 gave high praise for the flexible fusion of various materials and also for tracking their thinking path with the mind-map on canvas. She suggested that all materials need to be allowed to be fused or linked to freely generate new examples: <span class="ltx_text ltx_font_italic" id="S7.SS3.p3.1.4">“using mind-map on canvas is an easy way to achieve this action.”</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Discussion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">This paper focuses on promotional post design, enabling everyday users to express their creativity such as in micro-entrepreneurship. However, the design of promotional posts requires high requirements on design skills as it involves blending engaging visuals with effective messages to produce appealing content, particularly challenging for design novices. To address these challenges, we have designed and evaluated the Influencer, an AI-infused tool designed to make the process of designing promotional posts simpler for those without professional design skills.
Influencer integrates four design modules: ideation, context-aware exploration, flexible fusion, and a mind-map layout for tracking the thinking path.
As indicated by the results, using Influencer could streamline the promotional post design process, enabling users to craft engaging and effective promotional content with relative ease. These experiences have been further supported and concertized by the participants’ detailed descriptions in the qualitative findings. Beyond presenting a novel design case and contextually confirming the benefits of our tool, we discuss the extracted underlying patterns and implications for future research and design below.
</p>
</div>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1. </span>Examples and Context as Key Components in Design</h3>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p" id="S8.SS1.p1.1">Studies have been conducted to explore the idea of a recommendation framework on how users could get inspiration by single visual elements <cite class="ltx_cite ltx_citemacro_citep">(Kim and Suk, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib37" title="">2017</a>; Peterson, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib53" title="">2019</a>; Jahanian et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib32" title="">2017</a>)</cite>. Kang et al. also uses a multi-dimensional recommendation method to enrich richer image search <cite class="ltx_cite ltx_citemacro_citep">(Kang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib35" title="">2021</a>)</cite>. However, little has been done to understand and leverage the context-aware exploration when specific design requirements are present. Our work has surfaced that context-aware exploration can refine the exploration process beyond simply a linear recommendation, especially when users face complex design scenarios that require tailored solutions.
Our findings from Section <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#S7" title="7. Results ‣ Influencer: Empowering Everyday Users in Creating Promotional Posts via AI-infused Exploration and Customization"><span class="ltx_text ltx_ref_tag">7</span></a> reveal distinct views of three dimensions in the context-aware exploration:
<span class="ltx_text ltx_font_italic" id="S8.SS1.p1.1.1">“Semantic dimension is easy to use and understand.”</span> (P2 and P8). <span class="ltx_text ltx_font_italic" id="S8.SS1.p1.1.2">“Color is useful in most of cases especially when I upload a brand image and want to find related images in a similar color schema.”</span> (P10), <span class="ltx_text ltx_font_italic" id="S8.SS1.p1.1.3">“Object dimension can help me find images having similar objects which are helpful when I need to design a product-type promotional post.”</span> (P11). But P6 and P7 expressed concern about color and objects in one of the tasks they did. They mention the image recommendation on color and object dimensions sometimes is slightly inaccurate. The reason may be the inaccurate color extraction and misclassification of objects caused by the limitation of the object detection model, leading to colors and objects that do not match the users intend to use in their uploaded brand image.
To better support this approach in designing, one design expert (E2) suggested that
adding a color picker to let users choose preferred colors, and then recommend related color palettes may be helpful to improve image recommendation accuracy. For object dimension, enhancing the model and training it with more object classes on a large-scale dataset would be an addition to the context-aware exploration.
</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2. </span>Flexible Fusion and Generation with Personalization</h3>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p" id="S8.SS2.p1.1">As uncovered by our empirical data, another promising opportunity of Influencer is that it could provide an interactive way to arrange and fuse elements and ease the design process by the drag-and-drop feature on canvas.
These new experiences in promotional post design have not adequately exhibited in existing creative tools. They usually offer template-based designs, where users select from pre-defined layouts and styles, limiting the scope for personalized interaction and creativity <cite class="ltx_cite ltx_citemacro_citep">(Yin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib75" title="">2013</a>; ting Qiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib66" title="">2017</a>; Tyagi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.14928v1#bib.bib69" title="">2022</a>)</cite>. The future design could build upon this experiential interaction and make the design process more immersive. For example, in 3D design, when a user drags one 3D model close to another, the system could automatically align and merge these geometric shapes, creating a unified and complex entity. Further, as lighting elements are integrated into the scene, their effects could intelligently merge, adapting to create a cohesive and harmonized lighting atmosphere that enhances the overall visual impact of the 3D space.
Besides, based on our user studies, we learned that users may have different preferences on the design styles of post images and captions. Therefore, we may leverage data analytics and machine learning to analyze user behavior, preferences, and past design choices, offering tailored design recommendations on images and captions. This is also an interesting avenue to consider in the future development of more personalized recommendation models in design.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3. </span>Trade-off between Automation and Autonomy</h3>
<div class="ltx_para" id="S8.SS3.p1">
<p class="ltx_p" id="S8.SS3.p1.1">To facilitate the creation of promotional posts by design novices, Influencer employs a nuanced recommendation system that carefully balances guidance with creative freedom.
The variety of examples collected in the findings illustrates that a multi-dimensional recommendation framework on images and captions enriches the design process by offering a diverse array of visual and textual elements, allowing users to explore design elements using different dimensions or concepts. Further, users were able to tailor the design process to the user’s specific brand narrative and aesthetic preferences with context-aware exploration.
In our study, participants could quickly customize the images and captions in the post design with little manual effort. The results indicate that Influencer outperforms the baseline on various design tasks. However, this customization is limited by the performance of the Generative Image AI models. For example, <span class="ltx_text ltx_font_italic" id="S8.SS3.p1.1.1">“The image-based fusion feature sometimes doesn’t work as I expected. Newly generated images didn’t change the style of my dragged image.”</span> (P12).
Fine-tuning the Generative Image AI model in a post image dataset could improve the visual quality of generated post images.
Allowing users to edit and polish the images later would also be a helpful addition to the system, such as adding text to the image, adjusting image saturation, and other basic features like those in Photoshop.
In summary, this suggests that an ideal design system should balance automation and autonomy to provide a wealth of inspiration with recommendation, generation, and customization abilities.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.4. </span>Limitations and Future Work</h3>
<div class="ltx_para" id="S8.SS4.p1">
<p class="ltx_p" id="S8.SS4.p1.1">This work has several limitations which we plan to address in the future. First, even though our system can auto-layout each block, the layout is still stiff when users create a large amount of alternatives. Users can only use the mouse to manually drag each overlapped block to satisfy their layout requirement. To enable a more flexible mind-map layout, we could adopt a self-defined mind-map editor for the personalized organization. In this way, users can save the useful block based on their needs.</p>
</div>
<div class="ltx_para" id="S8.SS4.p2">
<p class="ltx_p" id="S8.SS4.p2.1">Second, Influencer recommends images based on three dimensions: semantic, object, and color. However, the current visual algorithm has limitations in accurately identifying highly related images within complex images. In addition, when users upload their own images and attempt to update the image recommendation block in our system, existing vision models are unable to correctly extract the semantic meaning of the uploaded image. To improve the accuracy and integrity of recommendation algorithms, we would like to expand our dataset and experiment with a more state-of-art vision model in the future.</p>
</div>
<div class="ltx_para" id="S8.SS4.p3">
<p class="ltx_p" id="S8.SS4.p3.1">Third, since there are many kinds of information including texts, photos, typography design, and layout that can inspire users, our three-dimensional exploration would benefit by some extension. To adapt our system to more creative scenarios (e.g., banner design, photography, etc), we can include more images in the dataset or recommend images in more dimensions (e.g., typography, mood, style). Also to help professional designers, we can incorporate Influencer with Photoshop to perform more complex tasks or leverage advanced image editing features from Photoshop to fulfill more advanced user needs.</p>
</div>
<div class="ltx_para" id="S8.SS4.p4">
<p class="ltx_p" id="S8.SS4.p4.1">Fourth, while users can modify various design elements, such as images, captions, and color schemes, the flexibility of fully customizing layouts and structures remains somewhat constrained. As part of our future endeavors, we envision enhancing the system’s capabilities so users can design completely customized layouts incorporating diverse design elements precisely based on their preferences. Users can integrate features that facilitate seamless collaboration among multiple users participating in promotional post creation. Future work may try to streamline and enrich the design process, promoting more dynamic and efficient teamwork in creating compelling promotional content by enabling real-time collaboration and feedback.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>Conclusion</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">In this paper, we have introduced Influencer, a tool designed to support novice designers such as freelance creators, marketers, and product managers with promotional post design. By seamlessly integrating captivating images and well-crafted captions, Influencer addresses the challenges faced by design novices in generating attention-grabbing content. Our system offers a mindmap-like layout for ideation and incorporates multidimensional AI-powered exploration and customization of related images and captions to revolutionize the design process. Additionally, Influencer allows flexible fusion of various design elements. Through a comprehensive evaluation, including controlled experiments and expert assessment, we have demonstrated that Influencer significantly enhances the ideation and design process. It enhances the effectiveness of promotional post design in different task scenarios by fostering engaging interactions, diverse explorations, and trackable thought processes.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Adekunle and Kajumba (2020)</span>
<span class="ltx_bibblock">
Bamidele Adekunle and
Christine Kajumba. 2020.

</span>
<span class="ltx_bibblock">The Nexus between Instagram and digital
entrepreneurship.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Journal of African Development</em>
21, 1 (2020),
14–40.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Akyon et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Fatih Cagatay Akyon,
Sinan Onur Altinuc, and Alptekin
Temizel. 2022.

</span>
<span class="ltx_bibblock">Slicing Aided Hyper Inference and Fine-Tuning for
Small Object Detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">2022 IEEE
International Conference on Image Processing (ICIP)</em>.
IEEE.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1109/icip46576.2022.9897990" title="">https://doi.org/10.1109/icip46576.2022.9897990</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Anthropic (2023)</span>
<span class="ltx_bibblock">
Anthropic.
2023.

</span>
<span class="ltx_bibblock">Model Card and Evaluations for Claude Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bae et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Suyun Sandra Bae, Oh-Hyun
Kwon, Senthil Chandrasegaran, and
Kwan-Liu Ma. 2020.

</span>
<span class="ltx_bibblock">Spinneret: Aiding creative ideation through
non-obvious concept associations. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings
of the 2020 CHI Conference on Human Factors in Computing Systems</em>.
1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Banerjee (2009)</span>
<span class="ltx_bibblock">
Subhojit Banerjee.
2009.

</span>
<span class="ltx_bibblock">Effect of product category on promotional choice:
comparative study of discounts and freebies.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Management Research News</em>
32, 2 (2009),
120–131.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Batra and Keller (2016)</span>
<span class="ltx_bibblock">
Rajeev Batra and
Kevin Lane Keller. 2016.

</span>
<span class="ltx_bibblock">Integrating marketing communications: New findings,
new lessons, and new ideas.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Journal of marketing</em> 80,
6 (2016), 122–145.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonnardel and Marmèche (2005)</span>
<span class="ltx_bibblock">
Nathalie Bonnardel and
Evelyne Marmèche. 2005.

</span>
<span class="ltx_bibblock">Towards supporting evocation processes in creative
design: A cognitive approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">International journal of human-computer
studies</em> 63, 4-5 (2005),
422–435.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown (2008)</span>
<span class="ltx_bibblock">
David C Brown.
2008.

</span>
<span class="ltx_bibblock">Guiding computational design creativity research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Studying Design Creativity, Springer</em>
(2008).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Carroll (1994)</span>
<span class="ltx_bibblock">
Noel Carroll.
1994.

</span>
<span class="ltx_bibblock">Visual metaphor.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Aspects of metaphor</em>.
Springer, 189–218.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cattle and Ma (2017)</span>
<span class="ltx_bibblock">
Andrew Cattle and
Xiaojuan Ma. 2017.

</span>
<span class="ltx_bibblock">Predicting word association strengths. In
<em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2017 conference on empirical
methods in natural language processing</em>. 1283–1288.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Joel Chan, Pao
Siangliulue, Denisa Qori McDonald, Ruixue
Liu, Reza Moradinezhad, Safa Aman,
Erin T Solovey, Krzysztof Z Gajos, and
Steven P Dow. 2017.

</span>
<span class="ltx_bibblock">Semantically far inspirations considered harmful?
accounting for cognitive states in collaborative ideation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 2017 ACM SIGCHI Conference on
Creativity and Cognition</em>. 93–105.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Li Chen, Mengyi Zhao,
Yiheng Liu, Mingxu Ding,
Yangyang Song, Shizun Wang,
Xu Wang, Hao Yang, Jing
Liu, Kang Du, and Min Zheng.
2023b.

</span>
<span class="ltx_bibblock">PhotoVerse: Tuning-Free Image Customization with
Text-to-Image Diffusion Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.05793 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Xi Chen, Lianghua Huang,
Yu Liu, Yujun Shen, Deli
Zhao, and Hengshuang Zhao.
2023a.

</span>
<span class="ltx_bibblock">AnyDoor: Zero-shot Object-level Image Customization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2307.09481 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Cherry and Latulipe (2014)</span>
<span class="ltx_bibblock">
Erin Cherry and Celine
Latulipe. 2014.

</span>
<span class="ltx_bibblock">Quantifying the creativity support of digital tools
through the creativity support index.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">ACM Transactions on Computer-Human
Interaction (TOCHI)</em> 21, 4
(2014), 1–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Choi et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
DaEun Choi, Sumin Hong,
Jeongeon Park, John Joon Young Chung,
and Juho Kim. 2023.

</span>
<span class="ltx_bibblock">CreativeConnect: Supporting Reference Recombination
for Graphic Design Ideation with Generative AI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">arXiv preprint arXiv:2312.11949</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chu et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Kar-Hai Chu, Jon-Patrick
Allem, Tess Boley Cruz, and Jennifer B
Unger. 2017.

</span>
<span class="ltx_bibblock">Vaping on Instagram: cloud chasing, hand checks and
product placement.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">Tobacco control</em> 26,
5 (2017), 575–578.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chunseong Park et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Cesc Chunseong Park,
Byeongchang Kim, and Gunhee Kim.
2017.

</span>
<span class="ltx_bibblock">Attend to You: Personalized Image Captioning With
Context Sequence Memory Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">De Deyne et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Simon De Deyne, Danielle J
Navarro, Amy Perfors, Marc Brysbaert,
and Gert Storms. 2019.

</span>
<span class="ltx_bibblock">The “Small World of Words” English word
association norms for over 12,000 cue words.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Behavior research methods</em>
51 (2019), 987–1006.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Yenan Dong, Shangshang
Zhu, and Wenjie Li. 2021.

</span>
<span class="ltx_bibblock">Promoting sustainable creativity: An empirical
study on the application of mind mapping tools in graphic design education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Sustainability</em> 13,
10 (2021), 5373.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eckert and Stacey (2000)</span>
<span class="ltx_bibblock">
Claudia Eckert and
Martin Stacey. 2000.

</span>
<span class="ltx_bibblock">Sources of inspiration: a language of design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Design studies</em> 21,
5 (2000), 523–538.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Evans (2004)</span>
<span class="ltx_bibblock">
Eric Evans.
2004.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Domain-driven design: tackling complexity
in the heart of software</em>.

</span>
<span class="ltx_bibblock">Addison-Wesley Professional.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feng and Lapata (2012)</span>
<span class="ltx_bibblock">
Yansong Feng and Mirella
Lapata. 2012.

</span>
<span class="ltx_bibblock">Automatic caption generation for news images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">IEEE transactions on pattern analysis and
machine intelligence</em> 35, 4
(2012), 797–812.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gal et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Rinon Gal, Yuval Alaluf,
Yuval Atzmon, Or Patashnik,
Amit H Bermano, Gal Chechik, and
Daniel Cohen-Or. 2022.

</span>
<span class="ltx_bibblock">An image is worth one word: Personalizing
text-to-image generation using textual inversion.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">arXiv preprint arXiv:2208.01618</em>
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gkiouzepas and Hogg (2011)</span>
<span class="ltx_bibblock">
Lampros Gkiouzepas and
Margaret K Hogg. 2011.

</span>
<span class="ltx_bibblock">Articulating a new framework for visual metaphors
in advertising.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Journal of Advertising</em>
40, 1 (2011),
103–120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gonçalves et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Milene Gonçalves,
Carlos Cardoso, and Petra
Badke-Schaub. 2014.

</span>
<span class="ltx_bibblock">What inspires designers? Preferences on
inspirational approaches during idea generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Design studies</em> 35,
1 (2014), 29–53.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gough (1976)</span>
<span class="ltx_bibblock">
Harrison G Gough.
1976.

</span>
<span class="ltx_bibblock">Studying creativity by means of word association
tests.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Journal of Applied Psychology</em>
61, 3 (1976),
348.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Handayani (2015)</span>
<span class="ltx_bibblock">
Fitri Handayani.
2015.

</span>
<span class="ltx_bibblock">Instagram as a teaching tool? Really?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of ISELT FBS Universitas Negeri
Padang</em> 4, 1 (2015),
320–327.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horphet and Srijongjai (2020)</span>
<span class="ltx_bibblock">
Nichakan Horphet and
Aranya Srijongjai. 2020.

</span>
<span class="ltx_bibblock">An Analysis of Multimodal Text Design in Social
Media Marketing: A Case Study of Starbucks Posts on Instagram.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Vacana</em> 8,
1 (2020), 119–154.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hummell (2006)</span>
<span class="ltx_bibblock">
Laura Hummell.
2006.

</span>
<span class="ltx_bibblock">Synectics for creative thinking in technology
education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Technology and Engineering Teacher</em>
66, 3 (2006),
22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaakonmäki et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Roope Jaakonmäki,
Oliver Müller, and Jan Vom Brocke.
2017.

</span>
<span class="ltx_bibblock">The impact of content, context, and creator on user
engagement in social media marketing. In
<em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the Annual Hawaii International
Conference on System Sciences</em>, Vol. 50. IEEE Computer
Society Press, 1152–1160.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jahanian et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Ali Jahanian, Shaiyan
Keshvari, SVN Vishwanathan, and Jan P
Allebach. 2017.

</span>
<span class="ltx_bibblock">Colors–Messengers of Concepts: Visual Design
Mining for Learning Color Semantics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">ACM Transactions on Computer-Human
Interaction (TOCHI)</em> 24, 1
(2017), 1–39.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jansson and Smith (1991)</span>
<span class="ltx_bibblock">
David G Jansson and
Steven M Smith. 1991.

</span>
<span class="ltx_bibblock">Design fixation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Design studies</em> 12,
1 (1991), 3–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jeong (2008)</span>
<span class="ltx_bibblock">
Se-Hoon Jeong.
2008.

</span>
<span class="ltx_bibblock">Visual metaphor in advertising: Is the persuasive
effect attributable to visual argumentation or metaphorical rhetoric?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Journal of marketing communications</em>
14, 1 (2008),
59–73.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kang et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Youwen Kang, Zhida Sun,
Sitong Wang, Zeyu Huang,
Ziming Wu, and Xiaojuan Ma.
2021.

</span>
<span class="ltx_bibblock">MetaMap: Supporting visual metaphor ideation
through multi-dimensional example-based exploration. In
<em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Proceedings of the 2021 CHI Conference on Human
Factors in Computing Systems</em>. 1–15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Muhammad Rashid Khan, M
Iqhal, and Ayesha Jahan Lodhi.
2021.

</span>
<span class="ltx_bibblock">Influencer marketing on Instagram: Effects of
promotional posts on purchasing behavior of consumers.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Journal of Political Studies</em>
28, 1 (2021),
119–132.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Suk (2017)</span>
<span class="ltx_bibblock">
EunJin Kim and
Hyeon-Jeong Suk. 2017.

</span>
<span class="ltx_bibblock">Thoughts and tools for crafting colors:
Implications from designers’ behavior. In
<em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 2017 Conference on Designing
Interactive Systems</em>. 321–331.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Mikyoung Kim, Doori Song,
and Ahnlee Jang. 2021.

</span>
<span class="ltx_bibblock">Consumer response toward native advertising on
social media: the roles of source type and content type.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">Internet Research</em> 31,
5 (2021), 1656–1676.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kumari et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nupur Kumari, Bingliang
Zhang, Richard Zhang, Eli Shechtman,
and Jun-Yan Zhu. 2023.

</span>
<span class="ltx_bibblock">Multi-Concept Customization of Text-to-Image
Diffusion. In <em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition (CVPR)</em>.
1931–1941.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurer et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Moritz Laurer, Wouter van
Atteveldt, Andreu Casas, and Kasper
Welbers. 2022.

</span>
<span class="ltx_bibblock">Less Annotating, More Classifying: Addressing the
Data Scarcity Issue of Supervised Machine Learning with Deep Transfer
Learning and BERT-NLI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">Political Analysis</em> (2022),
1–33.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Leonard and Rayport (1997)</span>
<span class="ltx_bibblock">
Dorothy Leonard and
Jeffrey F Rayport. 1997.

</span>
<span class="ltx_bibblock">Spark innovation through empathic design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Harvard business review</em>
75 (1997), 102–115.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lessa et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Patrick Wendell Barbosa Lessa,
Beatriz Gondim-Matos, and
Antonio Messias Valdevino.
2023.

</span>
<span class="ltx_bibblock">Micro-entrepreneurs in the creative industry: how
resilience overcomes the impacts of the pandemic.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Revista Brasileira de Gestão de
Negócios</em> 25 (2023),
353–372.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Ting Liu, Kit Cho,
George Aaron Broadwell, Samira Shaikh,
Tomek Strzalkowski, John Lien,
Sarah M Taylor, Laurie Feldman,
Boris Yamrom, Nick Webb, et al<span class="ltx_text" id="bib.bib43.3.1">.</span>
2014.

</span>
<span class="ltx_bibblock">Automatic Expansion of the MRC Psycholinguistic
Database Imageability Ratings.. In <em class="ltx_emph ltx_font_italic" id="bib.bib43.4.1">LREC</em>.
2800–2805.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lucero (2009)</span>
<span class="ltx_bibblock">
Andrés Lucero.
2009.

</span>
<span class="ltx_bibblock">Co-designing interactive spaces for and with
designers: supporting mood-board making.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Eindhoven, the Netherlands: Eindhoven
University of Technology</em> (2009).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma (2013)</span>
<span class="ltx_bibblock">
Xiaojuan Ma.
2013.

</span>
<span class="ltx_bibblock">Evocation: analyzing and propagating a semantic
link based on free word association.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Language resources and evaluation</em>
47, 3 (2013),
819–837.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Menon et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
RG Vishnu Menon, Valdimar
Sigurdsson, Nils Magne Larsen, Asle
Fagerstrøm, Herborg Sørensen,
Helena Gunnars Marteinsdottir, and
Gordon R Foxall. 2019.

</span>
<span class="ltx_bibblock">How to grow brand post engagement on Facebook and
Twitter for airlines? An empirical investigation of design and content
factors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Journal of Air Transport Management</em>
79 (2019), 101678.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Merten and Fischer (1999)</span>
<span class="ltx_bibblock">
Thomas Merten and Ines
Fischer. 1999.

</span>
<span class="ltx_bibblock">Creativity, personality and word association
responses: Associative behaviour in forty supposedly creative persons.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Personality and Individual Differences</em>
27, 5 (1999),
933–942.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Metcalf (1973)</span>
<span class="ltx_bibblock">
Wendell O Metcalf.
1973.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Starting and managing a small business of
your own</em>.

</span>
<span class="ltx_bibblock">Number 1. Small Business Administration.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023a)</span>
<span class="ltx_bibblock">
OpenAI. 2023a.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2303.08774" title="">https://doi.org/10.48550/arXiv.2303.08774</a>
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI (2023b)</span>
<span class="ltx_bibblock">
OpenAI. 2023b.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.08774 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Cesc Chunseong Park,
Byeongchang Kim, and Gunhee Kim.
2018.

</span>
<span class="ltx_bibblock">Towards personalized image captioning via
multimodal memory networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">IEEE transactions on pattern analysis and
machine intelligence</em> 41, 4
(2018), 999–1012.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perttula and Sipilä (2007)</span>
<span class="ltx_bibblock">
Matti Perttula and Pekka
Sipilä. 2007.

</span>
<span class="ltx_bibblock">The idea exposure paradigm in design idea
generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Journal of Engineering Design</em>
18, 1 (2007),
93–102.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Peterson (2019)</span>
<span class="ltx_bibblock">
Matthew O Peterson.
2019.

</span>
<span class="ltx_bibblock">Aspects of visual metaphor: an operational typology
of visual rhetoric for research in advertising.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">International Journal of Advertising</em>
38, 1 (2019),
67–96.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Petridis and Chilton (2019)</span>
<span class="ltx_bibblock">
Savvas Petridis and
Lydia B Chilton. 2019.

</span>
<span class="ltx_bibblock">Human errors in interpreting visual metaphor. In
<em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 2019 Conference on Creativity
and Cognition</em>. 187–197.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Phillips and McQuarrie (2004)</span>
<span class="ltx_bibblock">
Barbara J Phillips and
Edward F McQuarrie. 2004.

</span>
<span class="ltx_bibblock">Beyond visual metaphor: A new typology of visual
rhetoric in advertising.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Marketing theory</em> 4,
1-2 (2004), 113–136.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ramesh et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Aditya Ramesh, Mikhail
Pavlov, Gabriel Goh, Scott Gray,
Chelsea Voss, Alec Radford,
Mark Chen, and Ilya Sutskever.
2021.

</span>
<span class="ltx_bibblock">Zero-Shot Text-to-Image Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2102.12092 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et al<span class="ltx_text" id="bib.bib57.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jinjin Ren, Liming Lin,
and Wei Zheng. 2024.

</span>
<span class="ltx_bibblock">Product promotion copywriting from multimodal data:
New benchmark and model.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.3.1">Neurocomputing</em> 575
(2024), 127253.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Richards (2006)</span>
<span class="ltx_bibblock">
Amanda Dale Richards.
2006.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Sustainable micro-entrepreneurship to ensure
positive economic growth in the Western Cape</em>.

</span>
<span class="ltx_bibblock">Ph. D. Dissertation. Cape
Peninsula University of Technology.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schultz (2017)</span>
<span class="ltx_bibblock">
Carsten D Schultz.
2017.

</span>
<span class="ltx_bibblock">Proposing to your fans: Which brand post
characteristics drive consumer engagement activities on social media brand
pages?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">Electronic Commerce Research and
Applications</em> 26 (2017),
23–34.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xinyu Shi, Ziqi Zhou,
Jing Wen Zhang, Ali Neshati,
Anjul Kumar Tyagi, Ryan Rossi,
Shunan Guo, Fan Du, and
Jian Zhao. 2023.

</span>
<span class="ltx_bibblock">De-Stijl: Facilitating Graphics Design with
Interactive 2D Color Palette Recommendation. In
<em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">Proceedings of the 2023 CHI Conference on Human
Factors in Computing Systems</em> (¡conf-loc¿, ¡city¿Hamburg¡/city¿,
¡country¿Germany¡/country¿, ¡/conf-loc¿) <em class="ltx_emph ltx_font_italic" id="bib.bib60.4.2">(CHI ’23)</em>.
Association for Computing Machinery,
New York, NY, USA, Article 122,
19 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3581070" title="">https://doi.org/10.1145/3544548.3581070</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siangliulue et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2015a)</span>
<span class="ltx_bibblock">
Pao Siangliulue, Kenneth C
Arnold, Krzysztof Z Gajos, and Steven P
Dow. 2015a.

</span>
<span class="ltx_bibblock">Toward collaborative ideation at scale: Leveraging
ideas from others to generate more creative and diverse ideas. In
<em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the 18th ACM Conference on Computer
Supported Cooperative Work &amp; Social Computing</em>. 937–945.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siangliulue et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2015b)</span>
<span class="ltx_bibblock">
Pao Siangliulue, Joel
Chan, Krzysztof Z Gajos, and Steven P
Dow. 2015b.

</span>
<span class="ltx_bibblock">Providing timely examples improves the quantity and
quality of generated ideas. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Proceedings of the
2015 ACM SIGCHI Conference on Creativity and Cognition</em>.
83–92.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Srivatsan et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Nikita Srivatsan, Sofia
Samaniego, Omar Florez, and Taylor
Berg-Kirkpatrick. 2023.

</span>
<span class="ltx_bibblock">Text Conditional Alt-Text Generation for Twitter
Images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">arXiv preprint arXiv:2305.14779</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Staiano (2022)</span>
<span class="ltx_bibblock">
Fabio Staiano.
2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Designing and Prototyping Interfaces with
Figma: Learn essential UX/UI design principles by creating interactive
prototypes for mobile, tablet, and desktop</em>.

</span>
<span class="ltx_bibblock">Packt Publishing Ltd.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tiggemann (2022)</span>
<span class="ltx_bibblock">
Marika Tiggemann.
2022.

</span>
<span class="ltx_bibblock">Digital modification and body image on social
media: Disclaimer labels, captions, hashtags, and comments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Body Image</em> 41
(2022), 172–180.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ting Qiang et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Yu ting Qiang, Yanwei Fu,
Xiao Yu, Yanwen Guo,
Zhi-Hua Zhou, and Leonid Sigal.
2017.

</span>
<span class="ltx_bibblock">Learning to Generate Posters of Scientific Papers by
Probabilistic Graphical Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:1702.06228 [cs.CV]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut
Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux,
Timothée Lacroix, Baptiste
Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al<span class="ltx_text" id="bib.bib67.3.1">.</span>
2023.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib67.4.1">arXiv preprint arXiv:2302.13971</em>
(2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Truong et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Khai N Truong, Gillian R
Hayes, and Gregory D Abowd.
2006.

</span>
<span class="ltx_bibblock">Storyboarding: an empirical determination of best
practices and effective guidelines. In <em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">Proceedings
of the 6th conference on Designing Interactive systems</em>.
12–21.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tyagi et al<span class="ltx_text" id="bib.bib69.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Anjul Tyagi, Jian Zhao,
Pushkar Patel, Swasti Khurana, and
Klaus Mueller. 2022.

</span>
<span class="ltx_bibblock">Infographics Wizard: Flexible Infographics
Authoring and Design Exploration.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.3.1">Computer Graphics Forum</em>
41, 3 (June
2022), 121–132.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1111/cgf.14527" title="">https://doi.org/10.1111/cgf.14527</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vanolo (2008)</span>
<span class="ltx_bibblock">
Alberto Vanolo.
2008.

</span>
<span class="ltx_bibblock">The image of the creative city: Some reflections on
urban branding in Turin.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Cities</em> 25,
6 (2008), 370–382.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib71.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Peng Wang, An Yang,
Rui Men, Junyang Lin,
Shuai Bai, Zhikang Li,
Jianxin Ma, Chang Zhou,
Jingren Zhou, and Hongxia Yang.
2022.

</span>
<span class="ltx_bibblock">OFA: Unifying Architectures, Tasks, and Modalities
Through a Simple Sequence-to-Sequence Learning Framework.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.3.1">CoRR</em> abs/2202.03052
(2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sitong Wang, Savvas
Petridis, Taeahn Kwon, Xiaojuan Ma,
and Lydia B Chilton. 2023.

</span>
<span class="ltx_bibblock">PopBlends: Strategies for conceptual blending with
large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">Proceedings of the 2023
CHI Conference on Human Factors in Computing Systems</em>.
1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wilkenfeld and Ward (2001)</span>
<span class="ltx_bibblock">
Merryl J Wilkenfeld and
Thomas B Ward. 2001.

</span>
<span class="ltx_bibblock">Similarity and emergence in conceptual
combination.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Journal of Memory and Language</em>
45, 1 (2001),
21–38.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib74.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Ziming Wu, Zhida Sun,
Taewook Kim, Manuele Reani,
Caroline Jay, and Xiaojuan Ma.
2018.

</span>
<span class="ltx_bibblock">Mediating color filter exploration with color theme
semantics derived from social curation data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.3.1">Proceedings of the ACM on Human-Computer
Interaction</em> 2, CSCW
(2018), 1–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yin et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2013)</span>
<span class="ltx_bibblock">
Wenyuan Yin, Tao Mei,
and Chang Wen Chen. 2013.

</span>
<span class="ltx_bibblock">Automatic generation of social media snippets for
mobile browsing. In <em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">Proceedings of the 21st ACM
International Conference on Multimedia</em> (Barcelona, Spain)
<em class="ltx_emph ltx_font_italic" id="bib.bib75.4.2">(MM ’13)</em>. Association for
Computing Machinery, New York, NY, USA,
927–936.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2502081.2502116" title="">https://doi.org/10.1145/2502081.2502116</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yu et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Joanne Yu, Wilson
Cheong Hin Hong, and Roman Egger.
2024.

</span>
<span class="ltx_bibblock">The Art of Post Captions: Readability and User
Engagement on Social Media.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">Journal of Travel Research</em>
(2024), 00472875241228822.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Nanxuan Zhao, Nam Wook
Kim, Laura Mariah Herman, Hanspeter
Pfister, Rynson WH Lau, Jose Echevarria,
and Zoya Bylinskii. 2020.

</span>
<span class="ltx_bibblock">Iconate: Automatic compound icon generation and
ideation. In <em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">Proceedings of the 2020 CHI
Conference on Human Factors in Computing Systems</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jul 19 20:55:19 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
