<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2204.07061] Egocentric Human-Object Interaction Detection Exploiting Synthetic Data</title><meta property="og:description" content="We consider the problem of detecting Egocentric Human-Object Interactions (EHOIs) in industrial contexts. Since collecting and labeling large amounts of real images is challenging, we propose a pipeline and a tool to gâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Egocentric Human-Object Interaction Detection Exploiting Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Egocentric Human-Object Interaction Detection Exploiting Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2204.07061">

<!--Generated on Mon Mar 11 12:46:07 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="Egocentric Human-Object Interaction Detection Synthetic Data Active Object Recognition.">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span id="id1" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>FPV@IPLAB, DMI - University of Catania, Italy </span></span></span><span id="id2" class="ltx_note ltx_role_institutetext"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Next Vision s.r.l. - Spinoff of the University of Catania, Italy</span></span></span>
<h1 class="ltx_title ltx_title_document">Egocentric Human-Object Interaction Detection
<br class="ltx_break">Exploiting Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Rosario Leonardi
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Francesco Ragusa
</span><span class="ltx_author_notes">11 2 2</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<br class="ltx_break">Antonino Furnari
</span><span class="ltx_author_notes">11 2 2</span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Giovanni Maria Farinella
</span><span class="ltx_author_notes">11 2 2</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">We consider the problem of detecting Egocentric Human-Object Interactions (EHOIs) in industrial contexts. Since collecting and labeling large amounts of real images is challenging, we propose a pipeline and a tool to generate photo-realistic synthetic First Person Vision (FPV) images automatically labeled for EHOI detection in a specific industrial scenario. To tackle the problem of EHOI detection, we propose a method that detects the hands, the objects in the scene, and determines which objects are currently involved in an interaction. We compare the performance of our method with a set of state-of-the-art baselines. Results show that using a synthetic dataset improves the performance of an EHOI detection system, especially when few real data are available. To encourage research on this topic, we publicly release the proposed dataset at the following url: <a target="_blank" href="https://iplab.dmi.unict.it/EHOI_SYNTH/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://iplab.dmi.unict.it/EHOIË™SYNTH/</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Egocentric Human-Object Interaction Detection Synthetic Data Active Object Recognition.
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Understanding Human-Object Interactions (HOI) from the first-person perspective allows to build intelligent systems able to understand how humans interact with the world. The use of wearable cameras can be highly relevant to understand usersâ€™ locations of interestÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, to assist visitors in cultural sitesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, to provide assistance to people with disabilitiesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib33" title="" class="ltx_ref">33</a>]</cite>, or to improve the safety of workers in a factoryÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. Despite the rapid growth of wearable devicesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>, the task of Egocentric Human-Object Interaction (EHOI) detection is still understudied in this domain due to the limited availability of public datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. We note that in an industrial domain, in which the set of objects of interest is known a priori (e.g., the tools and instruments the user is going to interact with), the ability to detect the userâ€™s hands, find all objects and determine which objects are involved in an interaction, can inform on the userâ€™s behavior and provide useful information for other tasks such as object interaction anticipationÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>.
Extending the definition proposed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we hence consider the problem of detecting an EHOI as the one of predicting a quadruple <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">&lt;hand, contact_state, active_object, &lt;other_objects&gt;&gt;</span>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">To develop a system able to tackle this task in a specific industrial scenario, it is generally required to collect and label large amounts of data. To reduce the significant costs usually required for data collection and annotation, we investigated whether the use of synthetic images
can help to achieve good performance when models are trained on synthetic data and tested on real one. To this end, we propose a pipeline and a tool to generate a large number of synthetic EHOIs from 3D models of a real environment and objects. Unlike previous approachesÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, we generate EHOIs simulating a photo-realistic industrial environment. The proposed pipeline (Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) allows to obtain 3D models of the objects and the environment using 3D scanners. Such models are then used with the proposed data generation tool to automatically produce labeled images of EHOIs. Even though some works provide datasets to study HOI in general domainsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> and in industrial contextsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> to the best of our knowledge, this is the first attempt to define a pipeline for the generation of a large-scale photo-realistic synthetic FPV dataset to study EHOIs with rich annotations of active and non-active objects in an industrial scenario.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2204.07061/assets/images/schema_generazione_dati_big_font.jpg" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="139" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S1.F1.3.2" class="ltx_text" style="font-size:90%;">Synthetic EHOIs generation pipeline. We first use 3D scanners to obtain 3D models of the set of objects and the environment. We hence use the proposed data generation tool to create the synthetic EHOI dataset.</span></figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">To assess the suitability of the generated synthetic data to tackle the EHOI detection task, we acquired and labeled 8 real egocentric videos in an industrial laboratory, in which subjects perform test and repair operations on electrical boards (see FigureÂ <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>). To address the problem of EHOI detection, we propose a method inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> that detects and recognizes all the objects in the scene, determining which of these are involved in an interaction, as well as the hands of the camera wearer (see Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2204.07061/assets/images/output_method_comparison.jpg" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S1.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S1.F2.3.2" class="ltx_text" style="font-size:90%;">Example output of the proposed system. The figure on the left shows an example of a real image whereas a synthetic image is shown on the right. </span></figcaption>
</figure>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">To investigate the usefulness of exploiting synthetic data for EHOIs detection, we trained the proposed method using all the synthetic images together with variable amounts of real data. In addition, we compared the results of the proposed approach with different instances of the method proposed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. The results show that using synthetic data improves the performance of the EHOI method when tested on real images.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">In sum, the contributions of this paper are as follows: 1) we present a new photo-realistic synthetic FPV dataset for EHOIs detection considering an industrial scenario with rich annotations of the hands, and the active/non-active objects, including class labels and semantic segmentation masks; 2) we propose a method inspired byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> which detects and recognizes all the objects in the scene, the hands of the camera wearer, and determines which objects are currently involved in an interaction;
3) we perform several experiments to investigate the usefulness of synthetic data for the EHOI detection task when the method is tested on real data and compare the obtained results with baseline approaches based on the state-of-the-art method described inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Datasets for Human Behavior Understanding</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">In recent years, many works focused on the Human-Object Interaction detection task considering the third-person point of view. Several datasets have been proposed to explore this problem. The authors ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> proposed the V-COCO dataset, which adds 26 verbs to the 80 object classes of the popular COCO datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. HICO-DETÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> includes over 600 distinct interaction classes, while HOI-AÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> considers 10 action categories and 11 object classes. Previous works have also proposed datasets of videos to address the action recognition task. We can mention the ActivityNet datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> which focuses on 200 different action classes as well as KineticsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> which contains over 700 human action classes. With the rapid growth of wearable devices, different datasets of images and videos captured from the first-person point of view have been proposed. Among these, the work ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> provided a dataset of 48 FPV videos of people interacting with objects, including segmentation masks for 15,000 hand instances. EPIC-KitchensÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite> is a series of egocentric datasets focused on unscripted activities of human behavior in kitchens. EGTEA Gaze+Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> is a dataset of 28 hours of video of cooking activities. The dataset 100 Days of Hands (100DOH)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> is composed of both Third Person Vision (TPV) and FPV images and is suitable to study object-class agnostic HOI detection. The authors ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite> labeled images collected from different FPV datasetsÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> providing annotations for hands, objects and their relation. The MECCANO datasetÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> contains videos acquired in an industrial-like domain also annotated with bounding boxes around active objects, together with the related classes. A massive-scale egocentric dataset named Ego4D<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Ego4D Website: <a target="_blank" href="https://ego4d-data.org/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://ego4d-data.org/</a></span></span></span> has been acquired in various domains and labeled with several annotations to address different challenges. Since the annotation phase of EHOIs is expensive in terms of costs and time, the use of synthetic datasets for training purposes is desired. A few works explored the use of synthetic images generated from the first-person point of viewÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite>. The authors of such works used different strategies to customize various aspects of the scene, such as lights and backgrounds. However, these approaches tend to produce non-photorealistic images.
Differently from the aforementioned works, we generate a dataset of photo-realistic synthetic images of EHOIs in an industrial environment with rich annotations of hands, including hand side (<span id="S2.SS0.SSS0.Px1.p1.1.1" class="ltx_text ltx_font_italic">Left/Right</span>), contact state (<span id="S2.SS0.SSS0.Px1.p1.1.2" class="ltx_text ltx_font_italic">In contact with an object/No contact</span>), and all the objects in the images with bounding boxes. We also provide a class label for each object and indicate whether it is an active object as well as semantic segmentation masks.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Understanding Human-Object Interactions</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">There has been a lot of research in computer vision focusing on understanding Human-Object Interactions. The work ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> presented a multitask learning system to tackle HOI detection. The proposed system consists of an object detection branch, a human-centric branch, and an interaction branch. The authors ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> tackled the HOI detection task from both TPV and FPV predicting different information about hands (i.e., bounding box, hand side and contact state) and a box around the interacted object. PPDMÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> defines an HOI as a point triplet <span id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">&lt;human point, interaction point, object point&gt;</span> where these points represent the center of the related bounding boxes. The authors ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite> proposed a new two-stage detector called Unaryâ€“Pairwise Transformer. This approach exploits unary and pairwise representations to detect Human-Object Interactions. However, all these works mainly consider third-person view scenarios. Indeed, this task is still understudied in the FPV domain. Previous FPV works focused on the detection of hands interacting with an object without recognizing itÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite>. Other recent works focused on object-class agnostic EHOI detectionÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. The authors ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> defined Egocentric Human-Object Interaction (EHOI) detection as the task of producing <span id="S2.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">&lt;verb, objects&gt;</span> pairs. The paper investigated the problem of recognizing active objects in industrial-like settings without considering hands. The authors ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite> considered the usage of synthetic data for recognizing the performed Human-Objects Interactions.
In this paper, we tackle the EHOIs detection task in an industrial domain and investigate the usefulness of using synthetic data for training when the system needs to be tested on real data. In addition, our approach aims to detect both active and non-active objects as well as infer their classes.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset</h2>

<section id="S3.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Industrial context</h4>

<div id="S3.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px1.p1.1" class="ltx_p">We set up a laboratory to study the EHOIs detection task in a realistic industrial context. In the considered laboratory there are different objects, such as a power supply, a welding station, sockets, and a screwdriver. In addition, there is an electrical panel that allows powering on and off the sockets<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>See supplementary material for more details.</span></span></span>.
To generate synthetic data compliant to the considered real space, we acquire 3D scans of all objects and of the environment. It is worth noting that for the small objects, high-quality reconstructions are required to generate realistic EHOIs, whereas for the reconstruction of the environment, a high accuracy is not needed. Hence, to create 3D models, we used two different 3D scanners. In particular, we used an Artec Eva<span id="footnote3" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a target="_blank" href="https://www.artec3d.com/portable-3d-scanners/artec-eva-v2" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.artec3d.com/portable-3d-scanners/artec-eva-v2</a></span></span></span> structured-light 3D scanner, which has a 3D resolution of up to 0.2 mm, to scan the objects, and a MatterPort<span id="footnote4" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a target="_blank" href="https://matterport.com/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://matterport.com/</a></span></span></span> device to scan the 3D model of the environment.</p>
</div>
</section>
<section id="S3.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Synthetic Data</h4>

<div id="S3.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p1.1" class="ltx_p">We adopted the pipeline shown in Figure <a href="#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> to generate the synthetic data of EHOIs in the considered industrial context. We developed a tool in Blender which takes as input the 3D models of the objects and the environment and generates synthetic EHOIs along with different data, including 1) photo-realistic RGB images (see Figure <a href="#S3.F3" title="Figure 3 â€£ Synthetic Data â€£ 3 Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> - left), 2) depth maps, 3) semantic segmentation masks (see Figure <a href="#S3.F3" title="Figure 3 â€£ Synthetic Data â€£ 3 Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> - right), 4) objects bounding boxes and categories indicating which of them are active, 5) hands bounding boxes and attributes, such as the hand side (<span id="S3.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">Left/Right</span>) and the contact state (<span id="S3.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">In contact with an object/No contact</span>), and 6) distance between hands and objects in the 3D space. The tool allows to customize different aspects of the virtual scene, including the camera position, the lighting, and the color of the hands for automatic acquisition. Figure <a href="#S3.F3" title="Figure 3 â€£ Synthetic Data â€£ 3 Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> shows an example of synthetic EHOIs and related labels generated with our tool.</p>
</div>
<figure id="S3.F3" class="ltx_figure"><img src="/html/2204.07061/assets/images/annotations_example_w_mask.png" id="S3.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="135" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F3.2.1.1" class="ltx_text" style="font-size:90%;">Figure 3</span>: </span><span id="S3.F3.3.2" class="ltx_text" style="font-size:90%;">Example of a synthetic EHOI generated with the developed tool. On the left, the figure shows the synthetic RGB image automatically labeled (left) as well as the semantic segmentation mask generated for the same EHOI (right).</span></figcaption>
</figure>
<div id="S3.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S3.SS0.SSS0.Px2.p2.1" class="ltx_p">The generated synthetic dataset contains a total of 20,000 images, 29,034 hands (of which 14,589 involved in an interaction), 123,827 object instances (14,589 of which are active objects), and 19 object categories including portable industrial tools (e.g., screwdriver, electrical boards) and instruments (e.g., power supply, oscilloscope, electrical panels)<span id="S3.SS0.SSS0.Px2.p2.1.1" class="ltx_ERROR undefined">\@footnotemark</span>.</p>
</div>
<figure id="S3.F4" class="ltx_figure"><img src="/html/2204.07061/assets/images/schema_rete_big_font.jpg" id="S3.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="509" height="184" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S3.F4.2.1.1" class="ltx_text" style="font-size:90%;">Figure 4</span>: </span><span id="S3.F4.3.2" class="ltx_text" style="font-size:90%;">The proposed system takes an egocentric RGB image as input and outputs several predictions about the status of hands and objects involved in the interactions.</span></figcaption>
</figure>
</section>
<section id="S3.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Real Data</h4>

<div id="S3.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S3.SS0.SSS0.Px3.p1.1" class="ltx_p">The real data consists in 8 real videos acquired using a Microsoft Hololens 2 wearable device. To this aim, we asked 7 different subjects to perform test and repair operations on electrical boards in the industrial laboratory. To simplify the acquisition process, we defined different sequences of operations that subjects have to follow (e.g., turning on the oscilloscope, connecting the power cables to the electrical board, etc). To make data collection consistent and more natural, we developed a Mixed-Reality application for Hololens 2 that guides the subjects through audio and images to the next operation they have to perform. The set of operations has been randomized in order to be less scripted. The average duration of the captured videos is 28.37 minutes. In total, we acquired 3 hours and 47 minutes of video recordings at a resolution of 2272x1278 pixels and with a framerate of 30fps. An example of the captured data is shown in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> - left. We manually annotated the real videos with all the EHOIs performed by the subjects. We used the following approach to select the image frames to be annotated: 1) we considered the first frame in which the hand touches the interacted object (i.e., contact frame), and 2) we selected the first frame that appears immediately after the hand released the object (i.e., non contact frame). For each of the considered frames we annotated: 1) hand bounding boxes and attributes, such as hand side and contact state <span id="S3.SS0.SSS0.Px3.p1.1.1" class="ltx_text ltx_font_italic">(In Contact with an object/No contact)</span>, 2) active and non-active object bounding boxes and their categories, and 3) the relationships between the hands and the active objects (e.g. <span id="S3.SS0.SSS0.Px3.p1.1.2" class="ltx_text ltx_font_italic">in contact with the right hand</span>)<span id="S3.SS0.SSS0.Px3.p1.1.3" class="ltx_ERROR undefined">\@footnotemark</span>.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Proposed Approach</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.5" class="ltx_p">Similarly toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, our method extends the popular two-stage detector Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> to address the considered EHOIs detection task. However, differently thanÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, the proposed method is able to detect all the objects in the image together with the active/no active object class. FigureÂ <a href="#S3.F4" title="Figure 4 â€£ Synthetic Data â€£ 3 Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> shows the architecture of the proposed approach. The proposed method detects the hands and the objects in an egocentric RGB image and infers: 1) object categories, 2) hands side, 3) hands contact state, and 4) EHOIs as <span id="S4.p1.5.1" class="ltx_text ltx_font_italic">&lt;hand, contact_state, active_object, &lt;other_objects&gt;&gt;</span> quadruplet. Similarly toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we extend the object detector with four additional components: 1) the hand side classification module, 2) the hand state classification module, 3) the offset vector regression module, and 4) the matching algorithm. The modules composing our method are described in the following<span id="S4.p1.5.2" class="ltx_ERROR undefined">\@footnotemark</span>.

<br class="ltx_break"><span id="S4.p1.5.3" class="ltx_text ltx_font_bold">Hands and objects detection:</span> For objects and hands detection, we adopted a Faster R-CNN detectorÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> based on a ResNet-101 backboneÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> and a Feature Pyramid Network (FPN)Â <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> due to their state-of-the-art performance. The network predicts a <span id="S4.p1.5.4" class="ltx_text ltx_font_italic">(x,y,w,h,c)</span> tuple for each object/hand in the image, where the <span id="S4.p1.5.5" class="ltx_text ltx_font_italic">(x,y,w,h)</span> tuple represents the bounding box coordinates, and <span id="S4.p1.5.6" class="ltx_text ltx_font_italic">c</span> is the predicted object class. 
<br class="ltx_break"><span id="S4.p1.5.7" class="ltx_text ltx_font_bold">Hand side classification module:</span> The hand side classification module consists of a Multi-Layer Perceptron (MLP) composed of two fully connected layers. Starting from the detected hands, it takes as input a ROI-pooled feature vector of the hand crop and predicts the side of the hand <span id="S4.p1.5.8" class="ltx_text ltx_font_italic">(left/right)</span>.
<br class="ltx_break"><span id="S4.p1.5.9" class="ltx_text ltx_font_bold">Hand state classification module:</span> We consider two contact state classes: <span id="S4.p1.5.10" class="ltx_text ltx_font_italic">InÂ Contact</span> and <span id="S4.p1.5.11" class="ltx_text ltx_font_italic">No contact</span>. Other information about the contact state is embedded in the object category, which is predicted by our method, as opposed toÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> which predicts several types of contact states such as â€œin contact with a mobile objectâ€ or â€œin contact with a fixed objectâ€. The hand state classification module is composed of a MLP with two fully connected layers. We also enlarge the hand crop by 30% relative to the detected bounding box to include information of the surrounding context (e.g., nearby objects). The module takes as input the ROI-pooled feature vectors to infer the hands contact state. 
<br class="ltx_break"><span id="S4.p1.5.12" class="ltx_text ltx_font_bold">Offset vector regression module:</span> Following the approach proposed inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we predict an offset vector that links the center of each hand bounding box to the center of the corresponding active object bounding box. The offset vector is represented by a versor <span id="S4.p1.5.13" class="ltx_text ltx_font_italic">v</span> and a magnitude <span id="S4.p1.5.14" class="ltx_text ltx_font_italic">m</span>. This module is composed of a MLP with two fully connected layers. It takes as input a ROI-pooled feature vector extracted from the enlarged hand crop and infers the &lt;<math id="S4.p1.1.m1.2" class="ltx_Math" alttext="v_{x},v_{y}" display="inline"><semantics id="S4.p1.1.m1.2a"><mrow id="S4.p1.1.m1.2.2.2" xref="S4.p1.1.m1.2.2.3.cmml"><msub id="S4.p1.1.m1.1.1.1.1" xref="S4.p1.1.m1.1.1.1.1.cmml"><mi id="S4.p1.1.m1.1.1.1.1.2" xref="S4.p1.1.m1.1.1.1.1.2.cmml">v</mi><mi id="S4.p1.1.m1.1.1.1.1.3" xref="S4.p1.1.m1.1.1.1.1.3.cmml">x</mi></msub><mo id="S4.p1.1.m1.2.2.2.3" xref="S4.p1.1.m1.2.2.3.cmml">,</mo><msub id="S4.p1.1.m1.2.2.2.2" xref="S4.p1.1.m1.2.2.2.2.cmml"><mi id="S4.p1.1.m1.2.2.2.2.2" xref="S4.p1.1.m1.2.2.2.2.2.cmml">v</mi><mi id="S4.p1.1.m1.2.2.2.2.3" xref="S4.p1.1.m1.2.2.2.2.3.cmml">y</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.p1.1.m1.2b"><list id="S4.p1.1.m1.2.2.3.cmml" xref="S4.p1.1.m1.2.2.2"><apply id="S4.p1.1.m1.1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S4.p1.1.m1.1.1.1.1.1.cmml" xref="S4.p1.1.m1.1.1.1.1">subscript</csymbol><ci id="S4.p1.1.m1.1.1.1.1.2.cmml" xref="S4.p1.1.m1.1.1.1.1.2">ğ‘£</ci><ci id="S4.p1.1.m1.1.1.1.1.3.cmml" xref="S4.p1.1.m1.1.1.1.1.3">ğ‘¥</ci></apply><apply id="S4.p1.1.m1.2.2.2.2.cmml" xref="S4.p1.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="S4.p1.1.m1.2.2.2.2.1.cmml" xref="S4.p1.1.m1.2.2.2.2">subscript</csymbol><ci id="S4.p1.1.m1.2.2.2.2.2.cmml" xref="S4.p1.1.m1.2.2.2.2.2">ğ‘£</ci><ci id="S4.p1.1.m1.2.2.2.2.3.cmml" xref="S4.p1.1.m1.2.2.2.2.3">ğ‘¦</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.1.m1.2c">v_{x},v_{y}</annotation></semantics></math><span id="S4.p1.5.15" class="ltx_text ltx_font_italic">, m&gt;</span> triplet, where <math id="S4.p1.2.m2.1" class="ltx_Math" alttext="v_{x}" display="inline"><semantics id="S4.p1.2.m2.1a"><msub id="S4.p1.2.m2.1.1" xref="S4.p1.2.m2.1.1.cmml"><mi id="S4.p1.2.m2.1.1.2" xref="S4.p1.2.m2.1.1.2.cmml">v</mi><mi id="S4.p1.2.m2.1.1.3" xref="S4.p1.2.m2.1.1.3.cmml">x</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.2.m2.1b"><apply id="S4.p1.2.m2.1.1.cmml" xref="S4.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.p1.2.m2.1.1.1.cmml" xref="S4.p1.2.m2.1.1">subscript</csymbol><ci id="S4.p1.2.m2.1.1.2.cmml" xref="S4.p1.2.m2.1.1.2">ğ‘£</ci><ci id="S4.p1.2.m2.1.1.3.cmml" xref="S4.p1.2.m2.1.1.3">ğ‘¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.2.m2.1c">v_{x}</annotation></semantics></math> and <math id="S4.p1.3.m3.1" class="ltx_Math" alttext="v_{y}" display="inline"><semantics id="S4.p1.3.m3.1a"><msub id="S4.p1.3.m3.1.1" xref="S4.p1.3.m3.1.1.cmml"><mi id="S4.p1.3.m3.1.1.2" xref="S4.p1.3.m3.1.1.2.cmml">v</mi><mi id="S4.p1.3.m3.1.1.3" xref="S4.p1.3.m3.1.1.3.cmml">y</mi></msub><annotation-xml encoding="MathML-Content" id="S4.p1.3.m3.1b"><apply id="S4.p1.3.m3.1.1.cmml" xref="S4.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S4.p1.3.m3.1.1.1.cmml" xref="S4.p1.3.m3.1.1">subscript</csymbol><ci id="S4.p1.3.m3.1.1.2.cmml" xref="S4.p1.3.m3.1.1.2">ğ‘£</ci><ci id="S4.p1.3.m3.1.1.3.cmml" xref="S4.p1.3.m3.1.1.3">ğ‘¦</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.3.m3.1c">v_{y}</annotation></semantics></math> represent the components of the versor <span id="S4.p1.5.16" class="ltx_text ltx_font_italic">v</span>. 
<br class="ltx_break"><span id="S4.p1.5.17" class="ltx_text ltx_font_bold">Matching algorithm:</span> The last component of the proposed system is a matching algorithm that takes as input the outputs from the previous modules to predict the <span id="S4.p1.5.18" class="ltx_text ltx_font_italic">&lt;hand, contact_state, active_object, &lt;other_objects&gt;&gt;</span> quadruplet. The algorithm computes for each hand in contact with an object an image point (<math id="S4.p1.4.m4.1" class="ltx_Math" alttext="p_{interaction}" display="inline"><semantics id="S4.p1.4.m4.1a"><msub id="S4.p1.4.m4.1.1" xref="S4.p1.4.m4.1.1.cmml"><mi id="S4.p1.4.m4.1.1.2" xref="S4.p1.4.m4.1.1.2.cmml">p</mi><mrow id="S4.p1.4.m4.1.1.3" xref="S4.p1.4.m4.1.1.3.cmml"><mi id="S4.p1.4.m4.1.1.3.2" xref="S4.p1.4.m4.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.3" xref="S4.p1.4.m4.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1a" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.4" xref="S4.p1.4.m4.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1b" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.5" xref="S4.p1.4.m4.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1c" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.6" xref="S4.p1.4.m4.1.1.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1d" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.7" xref="S4.p1.4.m4.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1e" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.8" xref="S4.p1.4.m4.1.1.3.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1f" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.9" xref="S4.p1.4.m4.1.1.3.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1g" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.10" xref="S4.p1.4.m4.1.1.3.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1h" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.11" xref="S4.p1.4.m4.1.1.3.11.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.p1.4.m4.1.1.3.1i" xref="S4.p1.4.m4.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.4.m4.1.1.3.12" xref="S4.p1.4.m4.1.1.3.12.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.4.m4.1b"><apply id="S4.p1.4.m4.1.1.cmml" xref="S4.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.p1.4.m4.1.1.1.cmml" xref="S4.p1.4.m4.1.1">subscript</csymbol><ci id="S4.p1.4.m4.1.1.2.cmml" xref="S4.p1.4.m4.1.1.2">ğ‘</ci><apply id="S4.p1.4.m4.1.1.3.cmml" xref="S4.p1.4.m4.1.1.3"><times id="S4.p1.4.m4.1.1.3.1.cmml" xref="S4.p1.4.m4.1.1.3.1"></times><ci id="S4.p1.4.m4.1.1.3.2.cmml" xref="S4.p1.4.m4.1.1.3.2">ğ‘–</ci><ci id="S4.p1.4.m4.1.1.3.3.cmml" xref="S4.p1.4.m4.1.1.3.3">ğ‘›</ci><ci id="S4.p1.4.m4.1.1.3.4.cmml" xref="S4.p1.4.m4.1.1.3.4">ğ‘¡</ci><ci id="S4.p1.4.m4.1.1.3.5.cmml" xref="S4.p1.4.m4.1.1.3.5">ğ‘’</ci><ci id="S4.p1.4.m4.1.1.3.6.cmml" xref="S4.p1.4.m4.1.1.3.6">ğ‘Ÿ</ci><ci id="S4.p1.4.m4.1.1.3.7.cmml" xref="S4.p1.4.m4.1.1.3.7">ğ‘</ci><ci id="S4.p1.4.m4.1.1.3.8.cmml" xref="S4.p1.4.m4.1.1.3.8">ğ‘</ci><ci id="S4.p1.4.m4.1.1.3.9.cmml" xref="S4.p1.4.m4.1.1.3.9">ğ‘¡</ci><ci id="S4.p1.4.m4.1.1.3.10.cmml" xref="S4.p1.4.m4.1.1.3.10">ğ‘–</ci><ci id="S4.p1.4.m4.1.1.3.11.cmml" xref="S4.p1.4.m4.1.1.3.11">ğ‘œ</ci><ci id="S4.p1.4.m4.1.1.3.12.cmml" xref="S4.p1.4.m4.1.1.3.12">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.4.m4.1c">p_{interaction}</annotation></semantics></math>) using the coordinates of the center of the hand bounding box and the corresponding offset vector. This point represents the predicted center of the active object bounding box. The active object is selected considering the object bounding box whose center is closest to the inferred <math id="S4.p1.5.m5.1" class="ltx_Math" alttext="p_{interaction}" display="inline"><semantics id="S4.p1.5.m5.1a"><msub id="S4.p1.5.m5.1.1" xref="S4.p1.5.m5.1.1.cmml"><mi id="S4.p1.5.m5.1.1.2" xref="S4.p1.5.m5.1.1.2.cmml">p</mi><mrow id="S4.p1.5.m5.1.1.3" xref="S4.p1.5.m5.1.1.3.cmml"><mi id="S4.p1.5.m5.1.1.3.2" xref="S4.p1.5.m5.1.1.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.3" xref="S4.p1.5.m5.1.1.3.3.cmml">n</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1a" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.4" xref="S4.p1.5.m5.1.1.3.4.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1b" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.5" xref="S4.p1.5.m5.1.1.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1c" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.6" xref="S4.p1.5.m5.1.1.3.6.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1d" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.7" xref="S4.p1.5.m5.1.1.3.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1e" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.8" xref="S4.p1.5.m5.1.1.3.8.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1f" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.9" xref="S4.p1.5.m5.1.1.3.9.cmml">t</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1g" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.10" xref="S4.p1.5.m5.1.1.3.10.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1h" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.11" xref="S4.p1.5.m5.1.1.3.11.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.p1.5.m5.1.1.3.1i" xref="S4.p1.5.m5.1.1.3.1.cmml">â€‹</mo><mi id="S4.p1.5.m5.1.1.3.12" xref="S4.p1.5.m5.1.1.3.12.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.p1.5.m5.1b"><apply id="S4.p1.5.m5.1.1.cmml" xref="S4.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S4.p1.5.m5.1.1.1.cmml" xref="S4.p1.5.m5.1.1">subscript</csymbol><ci id="S4.p1.5.m5.1.1.2.cmml" xref="S4.p1.5.m5.1.1.2">ğ‘</ci><apply id="S4.p1.5.m5.1.1.3.cmml" xref="S4.p1.5.m5.1.1.3"><times id="S4.p1.5.m5.1.1.3.1.cmml" xref="S4.p1.5.m5.1.1.3.1"></times><ci id="S4.p1.5.m5.1.1.3.2.cmml" xref="S4.p1.5.m5.1.1.3.2">ğ‘–</ci><ci id="S4.p1.5.m5.1.1.3.3.cmml" xref="S4.p1.5.m5.1.1.3.3">ğ‘›</ci><ci id="S4.p1.5.m5.1.1.3.4.cmml" xref="S4.p1.5.m5.1.1.3.4">ğ‘¡</ci><ci id="S4.p1.5.m5.1.1.3.5.cmml" xref="S4.p1.5.m5.1.1.3.5">ğ‘’</ci><ci id="S4.p1.5.m5.1.1.3.6.cmml" xref="S4.p1.5.m5.1.1.3.6">ğ‘Ÿ</ci><ci id="S4.p1.5.m5.1.1.3.7.cmml" xref="S4.p1.5.m5.1.1.3.7">ğ‘</ci><ci id="S4.p1.5.m5.1.1.3.8.cmml" xref="S4.p1.5.m5.1.1.3.8">ğ‘</ci><ci id="S4.p1.5.m5.1.1.3.9.cmml" xref="S4.p1.5.m5.1.1.3.9">ğ‘¡</ci><ci id="S4.p1.5.m5.1.1.3.10.cmml" xref="S4.p1.5.m5.1.1.3.10">ğ‘–</ci><ci id="S4.p1.5.m5.1.1.3.11.cmml" xref="S4.p1.5.m5.1.1.3.11">ğ‘œ</ci><ci id="S4.p1.5.m5.1.1.3.12.cmml" xref="S4.p1.5.m5.1.1.3.12">ğ‘›</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p1.5.m5.1c">p_{interaction}</annotation></semantics></math> point and also checking if the bounding box has a nonzero intersection with the bounding box of the considered hand.</p>
</div>
<figure id="S4.T2" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T2.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:203.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.T2.fig1.1.1.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S4.T2.fig1.2.2" class="ltx_text" style="font-size:90%;">Statistics of the three splits: Train, Validation and Test.</span></figcaption>
<div id="S4.T2.fig1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:239.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(87.3pt,-56.7pt) scale(1.90070844470527,1.90070844470527) ;">
<table id="S4.T2.fig1.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.fig1.3.1.1.1" class="ltx_tr">
<th id="S4.T2.fig1.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row"><span id="S4.T2.fig1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Split</span></th>
<th id="S4.T2.fig1.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.fig1.3.1.1.1.2.1" class="ltx_text ltx_font_bold">Train</span></th>
<th id="S4.T2.fig1.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.fig1.3.1.1.1.3.1" class="ltx_text ltx_font_bold">Val</span></th>
<th id="S4.T2.fig1.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.fig1.3.1.1.1.4.1" class="ltx_text ltx_font_bold">Test</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.fig1.3.1.2.1" class="ltx_tr">
<th id="S4.T2.fig1.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t"><span id="S4.T2.fig1.3.1.2.1.1.1" class="ltx_text ltx_font_bold">#Videos</span></th>
<td id="S4.T2.fig1.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S4.T2.fig1.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">2</td>
<td id="S4.T2.fig1.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_t">4</td>
</tr>
<tr id="S4.T2.fig1.3.1.3.2" class="ltx_tr">
<th id="S4.T2.fig1.3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.fig1.3.1.3.2.1.1" class="ltx_text ltx_font_bold">#images</span></th>
<td id="S4.T2.fig1.3.1.3.2.2" class="ltx_td ltx_align_center">992</td>
<td id="S4.T2.fig1.3.1.3.2.3" class="ltx_td ltx_align_center">734</td>
<td id="S4.T2.fig1.3.1.3.2.4" class="ltx_td ltx_align_center">1,330</td>
</tr>
<tr id="S4.T2.fig1.3.1.4.3" class="ltx_tr">
<th id="S4.T2.fig1.3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.fig1.3.1.4.3.1.1" class="ltx_text ltx_font_bold">%images</span></th>
<td id="S4.T2.fig1.3.1.4.3.2" class="ltx_td ltx_align_center">32.46</td>
<td id="S4.T2.fig1.3.1.4.3.3" class="ltx_td ltx_align_center">24.01</td>
<td id="S4.T2.fig1.3.1.4.3.4" class="ltx_td ltx_align_center">43.53</td>
</tr>
<tr id="S4.T2.fig1.3.1.5.4" class="ltx_tr">
<th id="S4.T2.fig1.3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.fig1.3.1.5.4.1.1" class="ltx_text ltx_font_bold">#Hands</span></th>
<td id="S4.T2.fig1.3.1.5.4.2" class="ltx_td ltx_align_center">1,653</td>
<td id="S4.T2.fig1.3.1.5.4.3" class="ltx_td ltx_align_center">1,036</td>
<td id="S4.T2.fig1.3.1.5.4.4" class="ltx_td ltx_align_center">1,814</td>
</tr>
<tr id="S4.T2.fig1.3.1.6.5" class="ltx_tr">
<th id="S4.T2.fig1.3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T2.fig1.3.1.6.5.1.1" class="ltx_text ltx_font_bold">#Objects</span></th>
<td id="S4.T2.fig1.3.1.6.5.2" class="ltx_td ltx_align_center">6,483</td>
<td id="S4.T2.fig1.3.1.6.5.3" class="ltx_td ltx_align_center">4,337</td>
<td id="S4.T2.fig1.3.1.6.5.4" class="ltx_td ltx_align_center">6,778</td>
</tr>
<tr id="S4.T2.fig1.3.1.7.6" class="ltx_tr">
<th id="S4.T2.fig1.3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b"><span id="S4.T2.fig1.3.1.7.6.1.1" class="ltx_text ltx_font_bold">#Active Objects</span></th>
<td id="S4.T2.fig1.3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b">1,090</td>
<td id="S4.T2.fig1.3.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b">662</td>
<td id="S4.T2.fig1.3.1.7.6.4" class="ltx_td ltx_align_center ltx_border_b">1,120</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S4.T2.fig2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:203.8pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S4.T2.fig2.1.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S4.T2.fig2.2.2" class="ltx_text" style="font-size:90%;">Object detection results using different amounts of real data.</span></figcaption>
<div id="S4.T2.fig2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:384.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(97.9pt,-102.1pt) scale(2.13394083135688,2.13394083135688) ;">
<table id="S4.T2.fig2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.fig2.3.1.1.1" class="ltx_tr">
<th id="S4.T2.fig2.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T2.fig2.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Pretraining</span></th>
<th id="S4.T2.fig2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S4.T2.fig2.3.1.1.1.2.1" class="ltx_text ltx_font_bold">Real Data%</span></th>
<th id="S4.T2.fig2.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S4.T2.fig2.3.1.1.1.3.1" class="ltx_text ltx_font_bold">mAP%</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.fig2.3.1.2.1" class="ltx_tr">
<td id="S4.T2.fig2.3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">Synthetic</td>
<td id="S4.T2.fig2.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">0</td>
<td id="S4.T2.fig2.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_t">66.44</td>
</tr>
<tr id="S4.T2.fig2.3.1.3.2" class="ltx_tr">
<td id="S4.T2.fig2.3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.fig2.3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">10</td>
<td id="S4.T2.fig2.3.1.3.2.3" class="ltx_td ltx_align_center">53.27</td>
</tr>
<tr id="S4.T2.fig2.3.1.4.3" class="ltx_tr">
<td id="S4.T2.fig2.3.1.4.3.1" class="ltx_td ltx_align_center ltx_border_r">Synthetic</td>
<td id="S4.T2.fig2.3.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">10</td>
<td id="S4.T2.fig2.3.1.4.3.3" class="ltx_td ltx_align_center">72.69</td>
</tr>
<tr id="S4.T2.fig2.3.1.5.4" class="ltx_tr">
<td id="S4.T2.fig2.3.1.5.4.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.fig2.3.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">25</td>
<td id="S4.T2.fig2.3.1.5.4.3" class="ltx_td ltx_align_center">52.34</td>
</tr>
<tr id="S4.T2.fig2.3.1.6.5" class="ltx_tr">
<td id="S4.T2.fig2.3.1.6.5.1" class="ltx_td ltx_align_center ltx_border_r">Synthetic</td>
<td id="S4.T2.fig2.3.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">25</td>
<td id="S4.T2.fig2.3.1.6.5.3" class="ltx_td ltx_align_center">76.19</td>
</tr>
<tr id="S4.T2.fig2.3.1.7.6" class="ltx_tr">
<td id="S4.T2.fig2.3.1.7.6.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.fig2.3.1.7.6.2" class="ltx_td ltx_align_center ltx_border_r">50</td>
<td id="S4.T2.fig2.3.1.7.6.3" class="ltx_td ltx_align_center">71.17</td>
</tr>
<tr id="S4.T2.fig2.3.1.8.7" class="ltx_tr">
<td id="S4.T2.fig2.3.1.8.7.1" class="ltx_td ltx_align_center ltx_border_r">Synthetic</td>
<td id="S4.T2.fig2.3.1.8.7.2" class="ltx_td ltx_align_center ltx_border_r">50</td>
<td id="S4.T2.fig2.3.1.8.7.3" class="ltx_td ltx_align_center"><span id="S4.T2.fig2.3.1.8.7.3.1" class="ltx_text ltx_font_bold">77.29</span></td>
</tr>
<tr id="S4.T2.fig2.3.1.9.8" class="ltx_tr">
<td id="S4.T2.fig2.3.1.9.8.1" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S4.T2.fig2.3.1.9.8.2" class="ltx_td ltx_align_center ltx_border_r">100</td>
<td id="S4.T2.fig2.3.1.9.8.3" class="ltx_td ltx_align_center">70.84</td>
</tr>
<tr id="S4.T2.fig2.3.1.10.9" class="ltx_tr">
<td id="S4.T2.fig2.3.1.10.9.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">Synthetic</td>
<td id="S4.T2.fig2.3.1.10.9.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">100</td>
<td id="S4.T2.fig2.3.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b"><span id="S4.T2.fig2.3.1.10.9.3.1" class="ltx_text ltx_framed ltx_framed_underline">77.14</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments and Results</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We split the real dataset into training, validation, and test sets. TableÂ <a href="#S4.T2" title="Table 2 â€£ 4 Proposed Approach â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> reports statistics about these splits. We trained our models in two stages. In the first stage, the models have been trained using only synthetic data (i.e., 0% of real data). In the second stage, we finetuned the models considering different amount of the real training data, namely, 10%, 25%, 50%, and 100%.</p>
</div>
<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Object Detection Performance</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.8" class="ltx_p">We evaluated the object detection performance of our method considering 19 object categories. We used the mean Average Precision metric, with an <span id="S5.SS1.p1.8.1" class="ltx_text ltx_font_italic">Intersection over Union (IoU)</span> threshold of <math id="S5.SS1.p1.1.m1.1" class="ltx_Math" alttext="0.5" display="inline"><semantics id="S5.SS1.p1.1.m1.1a"><mn id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">0.5</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><cn type="float" id="S5.SS1.p1.1.m1.1.1.cmml" xref="S5.SS1.p1.1.m1.1.1">0.5</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">0.5</annotation></semantics></math> (<span id="S5.SS1.p1.8.2" class="ltx_text ltx_font_italic">mAP@50</span>)<span id="footnote5" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>We used the following implementation: <a target="_blank" href="https://github.com/cocodataset/cocoapi" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://github.com/cocodataset/cocoapi</a></span></span></span>. We report the results in Table <a href="#S4.T2" title="Table 2 â€£ 4 Proposed Approach â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. The <span id="S5.SS1.p1.8.3" class="ltx_text ltx_font_italic">â€œPretrainingâ€</span> column indicates whether synthetic data were used to pretrain the models. The <span id="S5.SS1.p1.8.4" class="ltx_text ltx_font_italic">â€œReal Data%â€</span> column reports the percentage of real data used to finetune the models. The table shows the best results in bold, whereas the second best results are underlined. The results show that using only synthetic data to train the model (first row of Table <a href="#S4.T2" title="Table 2 â€£ 4 Proposed Approach â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>) allows to achieve reasonable performance for this task (<span id="S5.SS1.p1.8.5" class="ltx_text ltx_font_italic">mAP</span> of <math id="S5.SS1.p1.2.m2.1" class="ltx_Math" alttext="66.44\%" display="inline"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">66.44</mn><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.p1.2.m2.1.1.2.cmml" xref="S5.SS1.p1.2.m2.1.1.2">66.44</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">66.44\%</annotation></semantics></math>). The best result (<span id="S5.SS1.p1.8.6" class="ltx_text ltx_font_italic">mAP</span> of <math id="S5.SS1.p1.3.m3.1" class="ltx_Math" alttext="77.29\%" display="inline"><semantics id="S5.SS1.p1.3.m3.1a"><mrow id="S5.SS1.p1.3.m3.1.1" xref="S5.SS1.p1.3.m3.1.1.cmml"><mn id="S5.SS1.p1.3.m3.1.1.2" xref="S5.SS1.p1.3.m3.1.1.2.cmml">77.29</mn><mo id="S5.SS1.p1.3.m3.1.1.1" xref="S5.SS1.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.3.m3.1b"><apply id="S5.SS1.p1.3.m3.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS1.p1.3.m3.1.1.1.cmml" xref="S5.SS1.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.p1.3.m3.1.1.2.cmml" xref="S5.SS1.p1.3.m3.1.1.2">77.29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.3.m3.1c">77.29\%</annotation></semantics></math>) was obtained by the model pretrained on the synthetic dataset and finetuned with <math id="S5.SS1.p1.4.m4.1" class="ltx_Math" alttext="50\%" display="inline"><semantics id="S5.SS1.p1.4.m4.1a"><mrow id="S5.SS1.p1.4.m4.1.1" xref="S5.SS1.p1.4.m4.1.1.cmml"><mn id="S5.SS1.p1.4.m4.1.1.2" xref="S5.SS1.p1.4.m4.1.1.2.cmml">50</mn><mo id="S5.SS1.p1.4.m4.1.1.1" xref="S5.SS1.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.4.m4.1b"><apply id="S5.SS1.p1.4.m4.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1"><csymbol cd="latexml" id="S5.SS1.p1.4.m4.1.1.1.cmml" xref="S5.SS1.p1.4.m4.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p1.4.m4.1.1.2.cmml" xref="S5.SS1.p1.4.m4.1.1.2">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.4.m4.1c">50\%</annotation></semantics></math> of the real dataset, while the second best result (<span id="S5.SS1.p1.8.7" class="ltx_text ltx_font_italic">mAP</span> of <math id="S5.SS1.p1.5.m5.1" class="ltx_Math" alttext="77.14\%" display="inline"><semantics id="S5.SS1.p1.5.m5.1a"><mrow id="S5.SS1.p1.5.m5.1.1" xref="S5.SS1.p1.5.m5.1.1.cmml"><mn id="S5.SS1.p1.5.m5.1.1.2" xref="S5.SS1.p1.5.m5.1.1.2.cmml">77.14</mn><mo id="S5.SS1.p1.5.m5.1.1.1" xref="S5.SS1.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.5.m5.1b"><apply id="S5.SS1.p1.5.m5.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1"><csymbol cd="latexml" id="S5.SS1.p1.5.m5.1.1.1.cmml" xref="S5.SS1.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.p1.5.m5.1.1.2.cmml" xref="S5.SS1.p1.5.m5.1.1.2">77.14</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.5.m5.1c">77.14\%</annotation></semantics></math>) comes from the model pretrained on the synthetic dataset and finetuned with <math id="S5.SS1.p1.6.m6.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S5.SS1.p1.6.m6.1a"><mrow id="S5.SS1.p1.6.m6.1.1" xref="S5.SS1.p1.6.m6.1.1.cmml"><mn id="S5.SS1.p1.6.m6.1.1.2" xref="S5.SS1.p1.6.m6.1.1.2.cmml">100</mn><mo id="S5.SS1.p1.6.m6.1.1.1" xref="S5.SS1.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.6.m6.1b"><apply id="S5.SS1.p1.6.m6.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1"><csymbol cd="latexml" id="S5.SS1.p1.6.m6.1.1.1.cmml" xref="S5.SS1.p1.6.m6.1.1.1">percent</csymbol><cn type="integer" id="S5.SS1.p1.6.m6.1.1.2.cmml" xref="S5.SS1.p1.6.m6.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.6.m6.1c">100\%</annotation></semantics></math> of the real dataset. The results also highlight how combining synthetic and real data allows to increase the performance for the object detection task. Indeed, all the models which have been pretrained using synthetic data outperformed the corresponding models trained only with real data, especially when little real data is available. Furthermore, it is worth noting that the model pretrained on the synthetic dataset and finetuned with 10% of the real dataset obtained a higher performance (<span id="S5.SS1.p1.8.8" class="ltx_text ltx_font_italic">mAP</span> of <math id="S5.SS1.p1.7.m7.1" class="ltx_Math" alttext="72.69\%" display="inline"><semantics id="S5.SS1.p1.7.m7.1a"><mrow id="S5.SS1.p1.7.m7.1.1" xref="S5.SS1.p1.7.m7.1.1.cmml"><mn id="S5.SS1.p1.7.m7.1.1.2" xref="S5.SS1.p1.7.m7.1.1.2.cmml">72.69</mn><mo id="S5.SS1.p1.7.m7.1.1.1" xref="S5.SS1.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.7.m7.1b"><apply id="S5.SS1.p1.7.m7.1.1.cmml" xref="S5.SS1.p1.7.m7.1.1"><csymbol cd="latexml" id="S5.SS1.p1.7.m7.1.1.1.cmml" xref="S5.SS1.p1.7.m7.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.p1.7.m7.1.1.2.cmml" xref="S5.SS1.p1.7.m7.1.1.2">72.69</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.7.m7.1c">72.69\%</annotation></semantics></math>) than all the models trained using only the real data (<span id="S5.SS1.p1.8.9" class="ltx_text ltx_font_italic">mAP</span> of <math id="S5.SS1.p1.8.m8.1" class="ltx_Math" alttext="70.84\%" display="inline"><semantics id="S5.SS1.p1.8.m8.1a"><mrow id="S5.SS1.p1.8.m8.1.1" xref="S5.SS1.p1.8.m8.1.1.cmml"><mn id="S5.SS1.p1.8.m8.1.1.2" xref="S5.SS1.p1.8.m8.1.1.2.cmml">70.84</mn><mo id="S5.SS1.p1.8.m8.1.1.1" xref="S5.SS1.p1.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.8.m8.1b"><apply id="S5.SS1.p1.8.m8.1.1.cmml" xref="S5.SS1.p1.8.m8.1.1"><csymbol cd="latexml" id="S5.SS1.p1.8.m8.1.1.1.cmml" xref="S5.SS1.p1.8.m8.1.1.1">percent</csymbol><cn type="float" id="S5.SS1.p1.8.m8.1.1.2.cmml" xref="S5.SS1.p1.8.m8.1.1.2">70.84</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.8.m8.1c">70.84\%</annotation></semantics></math> using 100% of real data), which supports the usefulness of synthetic data. See supplementary material for qualitative results.</p>
</div>
<figure id="S5.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T3.2.1.1" class="ltx_text" style="font-size:90%;">Table 3</span>: </span><span id="S5.T3.3.2" class="ltx_text" style="font-size:90%;">Results for the EHOI detection task.</span></figcaption>
<div id="S5.T3.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:368.6pt;height:135.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-60.8pt,22.3pt) scale(0.752074083698259,0.752074083698259) ;">
<table id="S5.T3.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T3.4.1.1.1" class="ltx_tr">
<th id="S5.T3.4.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S5.T3.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Pretraining</span></th>
<th id="S5.T3.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S5.T3.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Real Data%</span></th>
<th id="S5.T3.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T3.4.1.1.1.3.1" class="ltx_text ltx_font_bold">AP Hand</span></th>
<th id="S5.T3.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T3.4.1.1.1.4.1" class="ltx_text ltx_font_bold">mAP Obj</span></th>
<th id="S5.T3.4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T3.4.1.1.1.5.1" class="ltx_text ltx_font_bold">AP H+Side</span></th>
<th id="S5.T3.4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T3.4.1.1.1.6.1" class="ltx_text ltx_font_bold">AP H+State</span></th>
<th id="S5.T3.4.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T3.4.1.1.1.7.1" class="ltx_text ltx_font_bold">mAP H+Obj</span></th>
<th id="S5.T3.4.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T3.4.1.1.1.8.1" class="ltx_text ltx_font_bold">mAP All</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T3.4.1.2.1" class="ltx_tr">
<th id="S5.T3.4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Synthetic</th>
<th id="S5.T3.4.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">0</th>
<td id="S5.T3.4.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80.89</td>
<td id="S5.T3.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.52</td>
<td id="S5.T3.4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.65</td>
<td id="S5.T3.4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.16</td>
<td id="S5.T3.4.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.29</td>
<td id="S5.T3.4.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t">23.78</td>
</tr>
<tr id="S5.T3.4.1.3.2" class="ltx_tr">
<th id="S5.T3.4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<th id="S5.T3.4.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10</th>
<td id="S5.T3.4.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">90.48</td>
<td id="S5.T3.4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">23.26</td>
<td id="S5.T3.4.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r">79.46</td>
<td id="S5.T3.4.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.4.1.3.2.6.1" class="ltx_text ltx_framed ltx_framed_underline">50.44</span></td>
<td id="S5.T3.4.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r">21.79</td>
<td id="S5.T3.4.1.3.2.8" class="ltx_td ltx_align_center">18.59</td>
</tr>
<tr id="S5.T3.4.1.4.3" class="ltx_tr">
<th id="S5.T3.4.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S5.T3.4.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">10</th>
<td id="S5.T3.4.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">81.69</td>
<td id="S5.T3.4.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r">34.19</td>
<td id="S5.T3.4.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r">80.28</td>
<td id="S5.T3.4.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r">48.59</td>
<td id="S5.T3.4.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r">30.98</td>
<td id="S5.T3.4.1.4.3.8" class="ltx_td ltx_align_center">28.14</td>
</tr>
<tr id="S5.T3.4.1.5.4" class="ltx_tr">
<th id="S5.T3.4.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<th id="S5.T3.4.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">25</th>
<td id="S5.T3.4.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">90.46</td>
<td id="S5.T3.4.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r">18.83</td>
<td id="S5.T3.4.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r">80.28</td>
<td id="S5.T3.4.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r">49.25</td>
<td id="S5.T3.4.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r">17.50</td>
<td id="S5.T3.4.1.5.4.8" class="ltx_td ltx_align_center">15.92</td>
</tr>
<tr id="S5.T3.4.1.6.5" class="ltx_tr">
<th id="S5.T3.4.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S5.T3.4.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">25</th>
<td id="S5.T3.4.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.4.1.6.5.3.1" class="ltx_text ltx_framed ltx_framed_underline">90.61</span></td>
<td id="S5.T3.4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r">31.17</td>
<td id="S5.T3.4.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r">80.50</td>
<td id="S5.T3.4.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r">48.90</td>
<td id="S5.T3.4.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r">28.38</td>
<td id="S5.T3.4.1.6.5.8" class="ltx_td ltx_align_center">26.60</td>
</tr>
<tr id="S5.T3.4.1.7.6" class="ltx_tr">
<th id="S5.T3.4.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<th id="S5.T3.4.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">50</th>
<td id="S5.T3.4.1.7.6.3" class="ltx_td ltx_align_center ltx_border_r">90.38</td>
<td id="S5.T3.4.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r">27.08</td>
<td id="S5.T3.4.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r">79.98</td>
<td id="S5.T3.4.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r">48.95</td>
<td id="S5.T3.4.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r">25.54</td>
<td id="S5.T3.4.1.7.6.8" class="ltx_td ltx_align_center">23.27</td>
</tr>
<tr id="S5.T3.4.1.8.7" class="ltx_tr">
<th id="S5.T3.4.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S5.T3.4.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">50</th>
<td id="S5.T3.4.1.8.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.4.1.8.7.3.1" class="ltx_text ltx_framed ltx_framed_underline">90.61</span></td>
<td id="S5.T3.4.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.4.1.8.7.4.1" class="ltx_text ltx_font_bold">36.23</span></td>
<td id="S5.T3.4.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r">79.69</td>
<td id="S5.T3.4.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r">48.81</td>
<td id="S5.T3.4.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.4.1.8.7.7.1" class="ltx_text ltx_framed ltx_framed_underline">31.87</span></td>
<td id="S5.T3.4.1.8.7.8" class="ltx_td ltx_align_center"><span id="S5.T3.4.1.8.7.8.1" class="ltx_text ltx_framed ltx_framed_underline">30.50</span></td>
</tr>
<tr id="S5.T3.4.1.9.8" class="ltx_tr">
<th id="S5.T3.4.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<th id="S5.T3.4.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">100</th>
<td id="S5.T3.4.1.9.8.3" class="ltx_td ltx_align_center ltx_border_r">90.47</td>
<td id="S5.T3.4.1.9.8.4" class="ltx_td ltx_align_center ltx_border_r">26.29</td>
<td id="S5.T3.4.1.9.8.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T3.4.1.9.8.5.1" class="ltx_text ltx_framed ltx_framed_underline">89.20</span></td>
<td id="S5.T3.4.1.9.8.6" class="ltx_td ltx_align_center ltx_border_r">50.13</td>
<td id="S5.T3.4.1.9.8.7" class="ltx_td ltx_align_center ltx_border_r">25.04</td>
<td id="S5.T3.4.1.9.8.8" class="ltx_td ltx_align_center">22.70</td>
</tr>
<tr id="S5.T3.4.1.10.9" class="ltx_tr">
<th id="S5.T3.4.1.10.9.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Synthetic</th>
<th id="S5.T3.4.1.10.9.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">100</th>
<td id="S5.T3.4.1.10.9.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T3.4.1.10.9.3.1" class="ltx_text ltx_font_bold">90.67</span></td>
<td id="S5.T3.4.1.10.9.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T3.4.1.10.9.4.1" class="ltx_text ltx_framed ltx_framed_underline">35.43</span></td>
<td id="S5.T3.4.1.10.9.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T3.4.1.10.9.5.1" class="ltx_text ltx_font_bold">89.37</span></td>
<td id="S5.T3.4.1.10.9.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T3.4.1.10.9.6.1" class="ltx_text ltx_font_bold">50.58</span></td>
<td id="S5.T3.4.1.10.9.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T3.4.1.10.9.7.1" class="ltx_text ltx_font_bold">34.09</span></td>
<td id="S5.T3.4.1.10.9.8" class="ltx_td ltx_align_center ltx_border_b"><span id="S5.T3.4.1.10.9.8.1" class="ltx_text ltx_font_bold">32.61</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Egocentric Human Object Interaction Detection</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.9" class="ltx_p">We evaluated our method considering the following metrics: 1) <span id="S5.SS2.p1.9.1" class="ltx_text ltx_font_italic">AP Hand</span>: Average Precision of the hand detections; 2) <span id="S5.SS2.p1.9.2" class="ltx_text ltx_font_italic">mAP Obj</span>: mean Average Precision of the active objects; 3) <span id="S5.SS2.p1.9.3" class="ltx_text ltx_font_italic">AP H+Side</span>: Average precision of the hand detections when the correctness of the side (<span id="S5.SS2.p1.9.4" class="ltx_text ltx_font_italic">Left/Right</span>) is required; 4) <span id="S5.SS2.p1.9.5" class="ltx_text ltx_font_italic">AP H+State</span>: Average precision of the hand detections when the correctness of the contact state (<span id="S5.SS2.p1.9.6" class="ltx_text ltx_font_italic">In contact/No contact</span>) is required; 5) <span id="S5.SS2.p1.9.7" class="ltx_text ltx_font_italic">mAP H+Obj</span>: mean Average Precision of the active objects when the correctness of the associated hand is required, and 6) <span id="S5.SS2.p1.9.8" class="ltx_text ltx_font_italic">mAP All</span>: mean Average Precision of the hand detections when the correctness of the side, contact state, and associated active object are required. Note that, while most of these metrics are based onÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we modified the metrics influenced by active objects (i.e., <span id="S5.SS2.p1.9.9" class="ltx_text ltx_font_italic">mAP Obj</span>, <span id="S5.SS2.p1.9.10" class="ltx_text ltx_font_italic">mAP H+Obj</span>, and <span id="S5.SS2.p1.9.11" class="ltx_text ltx_font_italic">mAP All</span>) to include the recognition of the object categories (switching from AP to mAP). The results summarized in TableÂ <a href="#S5.T3" title="Table 3 â€£ 5.1 Object Detection Performance â€£ 5 Experiments and Results â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> highlight that using synthetic data allows to achieve the best performance. Indeed, the model pretrained with synthetic data and finetuned with 100% of the real dataset (last row) obtained the best results considering all the evaluation measures, except for the mAP Obj measure, in which it obtains the second best result of <math id="S5.SS2.p1.1.m1.1" class="ltx_Math" alttext="35.43\%" display="inline"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mn id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml">35.43</mn><mo id="S5.SS2.p1.1.m1.1.1.1" xref="S5.SS2.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2">35.43</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">35.43\%</annotation></semantics></math>. In particular, considering the mAP all measure (<math id="S5.SS2.p1.2.m2.1" class="ltx_Math" alttext="32.61\%" display="inline"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mn id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">32.61</mn><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.2.m2.1.1.2.cmml" xref="S5.SS2.p1.2.m2.1.1.2">32.61</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">32.61\%</annotation></semantics></math>), it outperforms the model trained using 100% of real data (<math id="S5.SS2.p1.3.m3.1" class="ltx_Math" alttext="22.70\%" display="inline"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mn id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">22.70</mn><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.3.m3.1.1.2.cmml" xref="S5.SS2.p1.3.m3.1.1.2">22.70</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">22.70\%</annotation></semantics></math>) by a significant margin of 9.91%. The model trained using only synthetic data (first row) outperforms all the models using only real data with respect to the evaluation measures influenced by the active objects. Indeed, the aforementioned model obtains the best results with respect to mAP Obj (<math id="S5.SS2.p1.4.m4.1" class="ltx_Math" alttext="29.52\%" display="inline"><semantics id="S5.SS2.p1.4.m4.1a"><mrow id="S5.SS2.p1.4.m4.1.1" xref="S5.SS2.p1.4.m4.1.1.cmml"><mn id="S5.SS2.p1.4.m4.1.1.2" xref="S5.SS2.p1.4.m4.1.1.2.cmml">29.52</mn><mo id="S5.SS2.p1.4.m4.1.1.1" xref="S5.SS2.p1.4.m4.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.4.m4.1b"><apply id="S5.SS2.p1.4.m4.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1"><csymbol cd="latexml" id="S5.SS2.p1.4.m4.1.1.1.cmml" xref="S5.SS2.p1.4.m4.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.4.m4.1.1.2.cmml" xref="S5.SS2.p1.4.m4.1.1.2">29.52</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.4.m4.1c">29.52\%</annotation></semantics></math>), mAP H+Obj (<math id="S5.SS2.p1.5.m5.1" class="ltx_Math" alttext="26.29\%" display="inline"><semantics id="S5.SS2.p1.5.m5.1a"><mrow id="S5.SS2.p1.5.m5.1.1" xref="S5.SS2.p1.5.m5.1.1.cmml"><mn id="S5.SS2.p1.5.m5.1.1.2" xref="S5.SS2.p1.5.m5.1.1.2.cmml">26.29</mn><mo id="S5.SS2.p1.5.m5.1.1.1" xref="S5.SS2.p1.5.m5.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.5.m5.1b"><apply id="S5.SS2.p1.5.m5.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1"><csymbol cd="latexml" id="S5.SS2.p1.5.m5.1.1.1.cmml" xref="S5.SS2.p1.5.m5.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.5.m5.1.1.2.cmml" xref="S5.SS2.p1.5.m5.1.1.2">26.29</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.5.m5.1c">26.29\%</annotation></semantics></math>), and mAP All (<math id="S5.SS2.p1.6.m6.1" class="ltx_Math" alttext="23.78\%" display="inline"><semantics id="S5.SS2.p1.6.m6.1a"><mrow id="S5.SS2.p1.6.m6.1.1" xref="S5.SS2.p1.6.m6.1.1.cmml"><mn id="S5.SS2.p1.6.m6.1.1.2" xref="S5.SS2.p1.6.m6.1.1.2.cmml">23.78</mn><mo id="S5.SS2.p1.6.m6.1.1.1" xref="S5.SS2.p1.6.m6.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.6.m6.1b"><apply id="S5.SS2.p1.6.m6.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1"><csymbol cd="latexml" id="S5.SS2.p1.6.m6.1.1.1.cmml" xref="S5.SS2.p1.6.m6.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.6.m6.1.1.2.cmml" xref="S5.SS2.p1.6.m6.1.1.2">23.78</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.6.m6.1c">23.78\%</annotation></semantics></math>). These performance scores are higher as compared to those achieved by the model trained with 50% of real data (i.e., <math id="S5.SS2.p1.7.m7.1" class="ltx_Math" alttext="27.08\%" display="inline"><semantics id="S5.SS2.p1.7.m7.1a"><mrow id="S5.SS2.p1.7.m7.1.1" xref="S5.SS2.p1.7.m7.1.1.cmml"><mn id="S5.SS2.p1.7.m7.1.1.2" xref="S5.SS2.p1.7.m7.1.1.2.cmml">27.08</mn><mo id="S5.SS2.p1.7.m7.1.1.1" xref="S5.SS2.p1.7.m7.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.7.m7.1b"><apply id="S5.SS2.p1.7.m7.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1"><csymbol cd="latexml" id="S5.SS2.p1.7.m7.1.1.1.cmml" xref="S5.SS2.p1.7.m7.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.7.m7.1.1.2.cmml" xref="S5.SS2.p1.7.m7.1.1.2">27.08</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.7.m7.1c">27.08\%</annotation></semantics></math>, <math id="S5.SS2.p1.8.m8.1" class="ltx_Math" alttext="25.54\%" display="inline"><semantics id="S5.SS2.p1.8.m8.1a"><mrow id="S5.SS2.p1.8.m8.1.1" xref="S5.SS2.p1.8.m8.1.1.cmml"><mn id="S5.SS2.p1.8.m8.1.1.2" xref="S5.SS2.p1.8.m8.1.1.2.cmml">25.54</mn><mo id="S5.SS2.p1.8.m8.1.1.1" xref="S5.SS2.p1.8.m8.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.8.m8.1b"><apply id="S5.SS2.p1.8.m8.1.1.cmml" xref="S5.SS2.p1.8.m8.1.1"><csymbol cd="latexml" id="S5.SS2.p1.8.m8.1.1.1.cmml" xref="S5.SS2.p1.8.m8.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.8.m8.1.1.2.cmml" xref="S5.SS2.p1.8.m8.1.1.2">25.54</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.8.m8.1c">25.54\%</annotation></semantics></math> and <math id="S5.SS2.p1.9.m9.1" class="ltx_Math" alttext="23.27\%" display="inline"><semantics id="S5.SS2.p1.9.m9.1a"><mrow id="S5.SS2.p1.9.m9.1.1" xref="S5.SS2.p1.9.m9.1.1.cmml"><mn id="S5.SS2.p1.9.m9.1.1.2" xref="S5.SS2.p1.9.m9.1.1.2.cmml">23.27</mn><mo id="S5.SS2.p1.9.m9.1.1.1" xref="S5.SS2.p1.9.m9.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.9.m9.1b"><apply id="S5.SS2.p1.9.m9.1.1.cmml" xref="S5.SS2.p1.9.m9.1.1"><csymbol cd="latexml" id="S5.SS2.p1.9.m9.1.1.1.cmml" xref="S5.SS2.p1.9.m9.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p1.9.m9.1.1.2.cmml" xref="S5.SS2.p1.9.m9.1.1.2">23.27</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.9.m9.1c">23.27\%</annotation></semantics></math>). Nevertheless, for the measures related to the hands (i.e., AP Hand, AP H+Side and AP H+State), the discussed method achieves limited performance, probably due to the gap between the real and synthetic domains. Please see FigureÂ <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>-left for a qualitative example of the proposed method.</p>
</div>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.3" class="ltx_p">We also compare the proposed method with different baselines based on the state-of-the-art method introduced inÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, which was pretrained on the large-scale dataset 100DOHÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> which contains over 100K labeled frames of HOIs. To be able to compare the proposed method withÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>, we extend the former to recognize the class of active objects following to two different approaches. The first approach consists in training a Resnet-18 CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> to classify image patches extracted from the active object detections. We trained the classifier with four different sets of data: 1) BS1: 19 videos, one per object class, in which only the considered object is observed. This provides a minimal training set that can be collected with a modest labeling effort; 2) BS2: images sampled from the proposed real dataset; 3) BS3: synthetic data and 4) BS4: both real and synthetic data. Note that this set requires a significant data collection and labeling effort. The second approach (BS5) uses the YOLOv5<span id="footnote6" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>YOLOv5: <a target="_blank" href="https://github.com/ultralytics/yolov5" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://github.com/ultralytics/yolov5</a></span></span></span> object detector to assign a label to the active objects predicted byÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>. In particular, to each active object, we assign the class of the object with the highest IoU among those predicted by YOLOv5, otherwise, if there are no box intersections, the proposal is discarded. TableÂ <a href="#S5.T4" title="Table 4 â€£ 5.2 Egocentric Human Object Interaction Detection â€£ 5 Experiments and Results â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> reports the obtained results. Considering the measures based on the active objects (i.e., mAP Obj, mAP H+Obj, and mAP all), our method trained with 50% of the real data (496 images) outperforms all the baselines based onÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> obtaining performances of <math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="36.23\%" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mrow id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml"><mn id="S5.SS2.p2.1.m1.1.1.2" xref="S5.SS2.p2.1.m1.1.1.2.cmml">36.23</mn><mo id="S5.SS2.p2.1.m1.1.1.1" xref="S5.SS2.p2.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><apply id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1"><csymbol cd="latexml" id="S5.SS2.p2.1.m1.1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p2.1.m1.1.1.2.cmml" xref="S5.SS2.p2.1.m1.1.1.2">36.23</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">36.23\%</annotation></semantics></math>, <math id="S5.SS2.p2.2.m2.1" class="ltx_Math" alttext="31.87\%" display="inline"><semantics id="S5.SS2.p2.2.m2.1a"><mrow id="S5.SS2.p2.2.m2.1.1" xref="S5.SS2.p2.2.m2.1.1.cmml"><mn id="S5.SS2.p2.2.m2.1.1.2" xref="S5.SS2.p2.2.m2.1.1.2.cmml">31.87</mn><mo id="S5.SS2.p2.2.m2.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.1b"><apply id="S5.SS2.p2.2.m2.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.p2.2.m2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p2.2.m2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.2">31.87</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.1c">31.87\%</annotation></semantics></math> and <math id="S5.SS2.p2.3.m3.1" class="ltx_Math" alttext="30.50\%" display="inline"><semantics id="S5.SS2.p2.3.m3.1a"><mrow id="S5.SS2.p2.3.m3.1.1" xref="S5.SS2.p2.3.m3.1.1.cmml"><mn id="S5.SS2.p2.3.m3.1.1.2" xref="S5.SS2.p2.3.m3.1.1.2.cmml">30.50</mn><mo id="S5.SS2.p2.3.m3.1.1.1" xref="S5.SS2.p2.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.3.m3.1b"><apply id="S5.SS2.p2.3.m3.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p2.3.m3.1.1.1.cmml" xref="S5.SS2.p2.3.m3.1.1.1">percent</csymbol><cn type="float" id="S5.SS2.p2.3.m3.1.1.2.cmml" xref="S5.SS2.p2.3.m3.1.1.2">30.50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.3.m3.1c">30.50\%</annotation></semantics></math> respectively. Moreover, using 100% of the real data, our approach obtains comparable performance considering the measures based on the hands (i.e., AP Hand, AP H+Side, and AP H+State). See supplementary material for qualitative comparison.</p>
</div>
<figure id="S5.T4" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S5.T4.2.1.1" class="ltx_text" style="font-size:90%;">Table 4</span>: </span><span id="S5.T4.3.2" class="ltx_text" style="font-size:90%;">Comparison between the proposed method and different baseline approaches based onÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.</span></figcaption>
<div id="S5.T4.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:390.3pt;height:109.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-92.7pt,26.1pt) scale(0.677950406545874,0.677950406545874) ;">
<table id="S5.T4.4.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T4.4.1.1.1" class="ltx_tr">
<th id="S5.T4.4.1.1.1.1" class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_r"></th>
<th id="S5.T4.4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S5.T4.4.1.1.1.2.1" class="ltx_text ltx_font_bold">Pretraining</span></th>
<th id="S5.T4.4.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S5.T4.4.1.1.1.3.1" class="ltx_text ltx_font_bold">Real Data%</span></th>
<th id="S5.T4.4.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T4.4.1.1.1.4.1" class="ltx_text ltx_font_bold">AP Hand</span></th>
<th id="S5.T4.4.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T4.4.1.1.1.5.1" class="ltx_text ltx_font_bold">mAP Obj</span></th>
<th id="S5.T4.4.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T4.4.1.1.1.6.1" class="ltx_text ltx_font_bold">AP H+Side</span></th>
<th id="S5.T4.4.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T4.4.1.1.1.7.1" class="ltx_text ltx_font_bold">AP H+State</span></th>
<th id="S5.T4.4.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S5.T4.4.1.1.1.8.1" class="ltx_text ltx_font_bold">mAP H+Obj</span></th>
<th id="S5.T4.4.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S5.T4.4.1.1.1.9.1" class="ltx_text ltx_font_bold">mAP All</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T4.4.1.2.1" class="ltx_tr">
<th id="S5.T4.4.1.2.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S5.T4.4.1.2.1.1.1" class="ltx_text ltx_font_bold">Proposed method</span></th>
<th id="S5.T4.4.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Synthetic</th>
<th id="S5.T4.4.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">0</th>
<td id="S5.T4.4.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">80.89</td>
<td id="S5.T4.4.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">29.52</td>
<td id="S5.T4.4.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.65</td>
<td id="S5.T4.4.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">36.16</td>
<td id="S5.T4.4.1.2.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">26.29</td>
<td id="S5.T4.4.1.2.1.9" class="ltx_td ltx_align_center ltx_border_t">23.78</td>
</tr>
<tr id="S5.T4.4.1.3.2" class="ltx_tr">
<th id="S5.T4.4.1.3.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.4.1.3.2.1.1" class="ltx_text ltx_font_bold">Proposed method</span></th>
<th id="S5.T4.4.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S5.T4.4.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">50</th>
<td id="S5.T4.4.1.3.2.4" class="ltx_td ltx_align_center ltx_border_r">90.61</td>
<td id="S5.T4.4.1.3.2.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.3.2.5.1" class="ltx_text ltx_font_bold">36.23</span></td>
<td id="S5.T4.4.1.3.2.6" class="ltx_td ltx_align_center ltx_border_r">79.69</td>
<td id="S5.T4.4.1.3.2.7" class="ltx_td ltx_align_center ltx_border_r">48.81</td>
<td id="S5.T4.4.1.3.2.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.3.2.8.1" class="ltx_text ltx_framed ltx_framed_underline">31.87</span></td>
<td id="S5.T4.4.1.3.2.9" class="ltx_td ltx_align_center"><span id="S5.T4.4.1.3.2.9.1" class="ltx_text ltx_framed ltx_framed_underline">30.50</span></td>
</tr>
<tr id="S5.T4.4.1.4.3" class="ltx_tr">
<th id="S5.T4.4.1.4.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.4.1.4.3.1.1" class="ltx_text ltx_font_bold">Proposed method</span></th>
<th id="S5.T4.4.1.4.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S5.T4.4.1.4.3.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">100</th>
<td id="S5.T4.4.1.4.3.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.4.3.4.1" class="ltx_text ltx_framed ltx_framed_underline">90.67</span></td>
<td id="S5.T4.4.1.4.3.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.4.3.5.1" class="ltx_text ltx_framed ltx_framed_underline">35.43</span></td>
<td id="S5.T4.4.1.4.3.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.4.3.6.1" class="ltx_text ltx_framed ltx_framed_underline">89.37</span></td>
<td id="S5.T4.4.1.4.3.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.4.3.7.1" class="ltx_text ltx_framed ltx_framed_underline">50.58</span></td>
<td id="S5.T4.4.1.4.3.8" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.4.3.8.1" class="ltx_text ltx_font_bold">34.09</span></td>
<td id="S5.T4.4.1.4.3.9" class="ltx_td ltx_align_center"><span id="S5.T4.4.1.4.3.9.1" class="ltx_text ltx_font_bold">32.61</span></td>
</tr>
<tr id="S5.T4.4.1.5.4" class="ltx_tr">
<th id="S5.T4.4.1.5.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_border_t"><span id="S5.T4.4.1.5.4.1.1" class="ltx_text ltx_font_bold">BS1</span></th>
<th id="S5.T4.4.1.5.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_border_t">-</th>
<th id="S5.T4.4.1.5.4.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_tt ltx_border_t">100</th>
<td id="S5.T4.4.1.5.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t"><span id="S5.T4.4.1.5.4.4.1" class="ltx_text ltx_font_bold">90.76</span></td>
<td id="S5.T4.4.1.5.4.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t">14.10</td>
<td id="S5.T4.4.1.5.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t"><span id="S5.T4.4.1.5.4.6.1" class="ltx_text ltx_font_bold">89.78</span></td>
<td id="S5.T4.4.1.5.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t"><span id="S5.T4.4.1.5.4.7.1" class="ltx_text ltx_font_bold">59.23</span></td>
<td id="S5.T4.4.1.5.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_tt ltx_border_t">12.42</td>
<td id="S5.T4.4.1.5.4.9" class="ltx_td ltx_align_center ltx_border_tt ltx_border_t">11.23</td>
</tr>
<tr id="S5.T4.4.1.6.5" class="ltx_tr">
<th id="S5.T4.4.1.6.5.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.4.1.6.5.1.1" class="ltx_text ltx_font_bold">BS2</span></th>
<th id="S5.T4.4.1.6.5.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">-</th>
<th id="S5.T4.4.1.6.5.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">100</th>
<td id="S5.T4.4.1.6.5.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.6.5.4.1" class="ltx_text ltx_font_bold">90.76</span></td>
<td id="S5.T4.4.1.6.5.5" class="ltx_td ltx_align_center ltx_border_r">22.17</td>
<td id="S5.T4.4.1.6.5.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.6.5.6.1" class="ltx_text ltx_font_bold">89.78</span></td>
<td id="S5.T4.4.1.6.5.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.6.5.7.1" class="ltx_text ltx_font_bold">59.23</span></td>
<td id="S5.T4.4.1.6.5.8" class="ltx_td ltx_align_center ltx_border_r">19.76</td>
<td id="S5.T4.4.1.6.5.9" class="ltx_td ltx_align_center">18.05</td>
</tr>
<tr id="S5.T4.4.1.7.6" class="ltx_tr">
<th id="S5.T4.4.1.7.6.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.4.1.7.6.1.1" class="ltx_text ltx_font_bold">BS3</span></th>
<th id="S5.T4.4.1.7.6.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S5.T4.4.1.7.6.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">0</th>
<td id="S5.T4.4.1.7.6.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.7.6.4.1" class="ltx_text ltx_font_bold">90.76</span></td>
<td id="S5.T4.4.1.7.6.5" class="ltx_td ltx_align_center ltx_border_r">09.44</td>
<td id="S5.T4.4.1.7.6.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.7.6.6.1" class="ltx_text ltx_font_bold">89.78</span></td>
<td id="S5.T4.4.1.7.6.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.7.6.7.1" class="ltx_text ltx_font_bold">59.23</span></td>
<td id="S5.T4.4.1.7.6.8" class="ltx_td ltx_align_center ltx_border_r">08.51</td>
<td id="S5.T4.4.1.7.6.9" class="ltx_td ltx_align_center">07.49</td>
</tr>
<tr id="S5.T4.4.1.8.7" class="ltx_tr">
<th id="S5.T4.4.1.8.7.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r"><span id="S5.T4.4.1.8.7.1.1" class="ltx_text ltx_font_bold">BS4</span></th>
<th id="S5.T4.4.1.8.7.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">Synthetic</th>
<th id="S5.T4.4.1.8.7.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r">100</th>
<td id="S5.T4.4.1.8.7.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.8.7.4.1" class="ltx_text ltx_font_bold">90.76</span></td>
<td id="S5.T4.4.1.8.7.5" class="ltx_td ltx_align_center ltx_border_r">26.36</td>
<td id="S5.T4.4.1.8.7.6" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.8.7.6.1" class="ltx_text ltx_font_bold">89.78</span></td>
<td id="S5.T4.4.1.8.7.7" class="ltx_td ltx_align_center ltx_border_r"><span id="S5.T4.4.1.8.7.7.1" class="ltx_text ltx_font_bold">59.23</span></td>
<td id="S5.T4.4.1.8.7.8" class="ltx_td ltx_align_center ltx_border_r">21.00</td>
<td id="S5.T4.4.1.8.7.9" class="ltx_td ltx_align_center">19.48</td>
</tr>
<tr id="S5.T4.4.1.9.8" class="ltx_tr">
<th id="S5.T4.4.1.9.8.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S5.T4.4.1.9.8.1.1" class="ltx_text ltx_font_bold">BS5</span></th>
<th id="S5.T4.4.1.9.8.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Synthetic</th>
<th id="S5.T4.4.1.9.8.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">100</th>
<td id="S5.T4.4.1.9.8.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T4.4.1.9.8.4.1" class="ltx_text ltx_font_bold">90.76</span></td>
<td id="S5.T4.4.1.9.8.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">30.26</td>
<td id="S5.T4.4.1.9.8.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T4.4.1.9.8.6.1" class="ltx_text ltx_font_bold">89.78</span></td>
<td id="S5.T4.4.1.9.8.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S5.T4.4.1.9.8.7.1" class="ltx_text ltx_font_bold">59.23</span></td>
<td id="S5.T4.4.1.9.8.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">28.77</td>
<td id="S5.T4.4.1.9.8.9" class="ltx_td ltx_align_center ltx_border_b">27.12</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">We considered the EHOI detection task in the industrial domain. Since labeling images is expensive in terms of time and costs, we explored how using synthetic data can improve the performance of EHOIs detection systems. To this end, we generated a new dataset of automatically labeled photo-realistic synthetic EHOIs in an industrial scenario and collected 8 egocentric real videos, which have been manually labeled. We proposed a method to tackle EHOI detection and compared it with different baseline approaches based on the state-of-the-art method ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite>.
Our analysis shows that exploiting synthetic data to train the proposed method greatly improves performance when tested on real data. Future work will explore how the knowledge of active/no active objects, inferred by our system, can provide useful information for other tasks such as next active object prediction.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">This research has been supported by Next Vision<span id="footnote7" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>Next Vision: <a target="_blank" href="https://www.nextvisionlab.it/" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://www.nextvisionlab.it/</a></span></span></span> s.r.l., by the project MISE - PON I&amp;C 2014-2020 - Progetto ENIGMA - Prog n. F/190050/02/X44 â€“ CUP: B61B19000520008, and by Research Program Pia.ce.ri. 2020/2022 Linea 2 - University of Catania.</p>
</div>
</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">SUPPLEMENTARY MATERIAL</h2>

</section>
<section id="S7" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Additional Details on the Dataset</h2>

<section id="S7.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Context:</h4>

<div id="S7.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px1.p1.1" class="ltx_p">Figure <a href="#S7.F5" title="Figure 5 â€£ Statistics: â€£ 7 Additional Details on the Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> shows the laboratory that we set up to study the EHOIs detection task in industrial domain.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Categories:</h4>

<div id="S7.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px2.p1.1" class="ltx_p">We considered the following 19 objects categories: <span id="S7.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">power supply, oscilloscope, welder station, electric screwdriver, screwdriver, pliers, welder probe tip, oscilloscope probe tip, low voltage board, high voltage board, register, electric screwdriver battery, working area, welder base, socket, left red button, left green button, right red button</span> and <span id="S7.SS0.SSS0.Px2.p1.1.2" class="ltx_text ltx_font_italic">right green button</span>. Figure <a href="#S7.F6" title="Figure 6 â€£ Statistics: â€£ 7 Additional Details on the Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> shows all the objects categories.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px3" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Privacy:</h4>

<div id="S7.SS0.SSS0.Px3.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px3.p1.1" class="ltx_p">During the acquisitions of the real dataset, the subjects removed all the items that could have revealed their identity (e.g., rings, watches, etc.). No other subjects appear in the captured videos.</p>
</div>
</section>
<section id="S7.SS0.SSS0.Px4" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Statistics:</h4>

<div id="S7.SS0.SSS0.Px4.p1" class="ltx_para">
<p id="S7.SS0.SSS0.Px4.p1.1" class="ltx_p">Table <a href="#S7.T6" title="Table 6 â€£ Statistics: â€£ 7 Additional Details on the Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reports the statistics related to the synthetic generated dataset, while Table <a href="#S7.T6" title="Table 6 â€£ Statistics: â€£ 7 Additional Details on the Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> reports the statistics of the real dataset. Figure <a href="#S7.F7" title="Figure 7 â€£ Statistics: â€£ 7 Additional Details on the Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> shows the distributions of all/active objects instances of the synthetic and the real datasets. 
<br class="ltx_break"></p>
</div>
<figure id="S7.F5" class="ltx_figure"><img src="/html/2204.07061/assets/images/industrial_lab_combo.jpg" id="S7.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F5.2.1.1" class="ltx_text" style="font-size:90%;">Figure 5</span>: </span><span id="S7.F5.3.2" class="ltx_text" style="font-size:90%;">Industrial-like laboratory.</span></figcaption>
</figure>
<figure id="S7.F6" class="ltx_figure"><img src="/html/2204.07061/assets/images/sup/objects_categories.jpg" id="S7.F6.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="598" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F6.2.1.1" class="ltx_text" style="font-size:90%;">Figure 6</span>: </span><span id="S7.F6.3.2" class="ltx_text" style="font-size:90%;">Objects categories.</span></figcaption>
</figure>
<figure id="S7.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/x1.png" id="S7.F7.sf1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="379" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F7.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S7.F7.sf1.3.2" class="ltx_text" style="font-size:90%;">Distributions of the all objects instances in the synthetic dataset.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/x2.png" id="S7.F7.sf2.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="379" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F7.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S7.F7.sf2.3.2" class="ltx_text" style="font-size:90%;">Distributions of the all objects instances in the real dataset.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/x3.png" id="S7.F7.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="379" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F7.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S7.F7.sf3.3.2" class="ltx_text" style="font-size:90%;">Distributions of the active objects instances in the synthetic dataset.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.F7.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/x4.png" id="S7.F7.sf4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="461" height="379" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F7.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S7.F7.sf4.3.2" class="ltx_text" style="font-size:90%;">Distributions of the active objects instances in the real dataset.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F7.2.1.1" class="ltx_text" style="font-size:90%;">Figure 7</span>: </span><span id="S7.F7.3.2" class="ltx_text" style="font-size:90%;">The distributions of all/active objects instances in the two datasets.</span></figcaption>
</figure>
<figure id="S7.T6" class="ltx_table">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.T6.fig1" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:195.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.T6.fig1.1.1.1" class="ltx_text" style="font-size:90%;">Table 5</span>: </span><span id="S7.T6.fig1.2.2" class="ltx_text" style="font-size:90%;">Statistics of the generated synthetic dataset.</span></figcaption>
<div id="S7.T6.fig1.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:428.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(134.9pt,-133.4pt) scale(2.6471582218463,2.6471582218463) ;">
<table id="S7.T6.fig1.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S7.T6.fig1.3.1.1.1" class="ltx_tr">
<th id="S7.T6.fig1.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig1.3.1.1.1.1.1" class="ltx_text ltx_font_bold">Total number of images</span></th>
<td id="S7.T6.fig1.3.1.1.1.2" class="ltx_td ltx_align_center">20,000</td>
</tr>
<tr id="S7.T6.fig1.3.1.2.2" class="ltx_tr">
<th id="S7.T6.fig1.3.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S7.T6.fig1.3.1.2.2.1.1" class="ltx_text ltx_font_bold">#hands</span></th>
<td id="S7.T6.fig1.3.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t">29,034</td>
</tr>
<tr id="S7.T6.fig1.3.1.3.3" class="ltx_tr">
<th id="S7.T6.fig1.3.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig1.3.1.3.3.1.1" class="ltx_text ltx_font_bold">#hands in contact</span></th>
<td id="S7.T6.fig1.3.1.3.3.2" class="ltx_td ltx_align_center">14,589</td>
</tr>
<tr id="S7.T6.fig1.3.1.4.4" class="ltx_tr">
<th id="S7.T6.fig1.3.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig1.3.1.4.4.1.1" class="ltx_text ltx_font_bold">#hands not in contact</span></th>
<td id="S7.T6.fig1.3.1.4.4.2" class="ltx_td ltx_align_center">14,445</td>
</tr>
<tr id="S7.T6.fig1.3.1.5.5" class="ltx_tr">
<th id="S7.T6.fig1.3.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig1.3.1.5.5.1.1" class="ltx_text ltx_font_bold">#left hands</span></th>
<td id="S7.T6.fig1.3.1.5.5.2" class="ltx_td ltx_align_center">14,473</td>
</tr>
<tr id="S7.T6.fig1.3.1.6.6" class="ltx_tr">
<th id="S7.T6.fig1.3.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig1.3.1.6.6.1.1" class="ltx_text ltx_font_bold">#right hands</span></th>
<td id="S7.T6.fig1.3.1.6.6.2" class="ltx_td ltx_align_center">14,561</td>
</tr>
<tr id="S7.T6.fig1.3.1.7.7" class="ltx_tr">
<th id="S7.T6.fig1.3.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig1.3.1.7.7.1.1" class="ltx_text ltx_font_bold">#object categories</span></th>
<td id="S7.T6.fig1.3.1.7.7.2" class="ltx_td ltx_align_center">19</td>
</tr>
<tr id="S7.T6.fig1.3.1.8.8" class="ltx_tr">
<th id="S7.T6.fig1.3.1.8.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig1.3.1.8.8.1.1" class="ltx_text ltx_font_bold">#objects</span></th>
<td id="S7.T6.fig1.3.1.8.8.2" class="ltx_td ltx_align_center">123,827</td>
</tr>
<tr id="S7.T6.fig1.3.1.9.9" class="ltx_tr">
<th id="S7.T6.fig1.3.1.9.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S7.T6.fig1.3.1.9.9.1.1" class="ltx_text ltx_font_bold">#active objects</span></th>
<td id="S7.T6.fig1.3.1.9.9.2" class="ltx_td ltx_align_center ltx_border_b">14,589</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S7.T6.fig2" class="ltx_figure ltx_figure_panel ltx_minipage ltx_align_middle" style="width:195.1pt;">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.T6.fig2.1.1.1" class="ltx_text" style="font-size:90%;">Table 6</span>: </span><span id="S7.T6.fig2.2.2" class="ltx_text" style="font-size:90%;">Statistics of the real dataset.</span></figcaption>
<div id="S7.T6.fig2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:601.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(132.4pt,-183.6pt) scale(2.56917228676758,2.56917228676758) ;">
<table id="S7.T6.fig2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S7.T6.fig2.3.1.1.1" class="ltx_tr">
<th id="S7.T6.fig2.3.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.1.1.1.1" class="ltx_text ltx_font_bold">#videos</span></th>
<th id="S7.T6.fig2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column">8</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S7.T6.fig2.3.1.2.1" class="ltx_tr">
<th id="S7.T6.fig2.3.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S7.T6.fig2.3.1.2.1.1.1" class="ltx_text ltx_font_bold">Total videos length</span></th>
<td id="S7.T6.fig2.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_t">227 min</td>
</tr>
<tr id="S7.T6.fig2.3.1.3.2" class="ltx_tr">
<th id="S7.T6.fig2.3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.3.2.1.1" class="ltx_text ltx_font_bold">Avg. video duration</span></th>
<td id="S7.T6.fig2.3.1.3.2.2" class="ltx_td ltx_align_center">28.37 min</td>
</tr>
<tr id="S7.T6.fig2.3.1.4.3" class="ltx_tr">
<th id="S7.T6.fig2.3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.4.3.1.1" class="ltx_text ltx_font_bold">#subjects</span></th>
<td id="S7.T6.fig2.3.1.4.3.2" class="ltx_td ltx_align_center">7</td>
</tr>
<tr id="S7.T6.fig2.3.1.5.4" class="ltx_tr">
<th id="S7.T6.fig2.3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.5.4.1.1" class="ltx_text ltx_font_bold">#images</span></th>
<td id="S7.T6.fig2.3.1.5.4.2" class="ltx_td ltx_align_center">3,056</td>
</tr>
<tr id="S7.T6.fig2.3.1.6.5" class="ltx_tr">
<th id="S7.T6.fig2.3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.6.5.1.1" class="ltx_text ltx_font_bold">#hands</span></th>
<td id="S7.T6.fig2.3.1.6.5.2" class="ltx_td ltx_align_center">4,503</td>
</tr>
<tr id="S7.T6.fig2.3.1.7.6" class="ltx_tr">
<th id="S7.T6.fig2.3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.7.6.1.1" class="ltx_text ltx_font_bold">#hands in contact</span></th>
<td id="S7.T6.fig2.3.1.7.6.2" class="ltx_td ltx_align_center">3,311</td>
</tr>
<tr id="S7.T6.fig2.3.1.8.7" class="ltx_tr">
<th id="S7.T6.fig2.3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.8.7.1.1" class="ltx_text ltx_font_bold">#hands not in contact</span></th>
<td id="S7.T6.fig2.3.1.8.7.2" class="ltx_td ltx_align_center">1,192</td>
</tr>
<tr id="S7.T6.fig2.3.1.9.8" class="ltx_tr">
<th id="S7.T6.fig2.3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.9.8.1.1" class="ltx_text ltx_font_bold">#left hands</span></th>
<td id="S7.T6.fig2.3.1.9.8.2" class="ltx_td ltx_align_center">2,013</td>
</tr>
<tr id="S7.T6.fig2.3.1.10.9" class="ltx_tr">
<th id="S7.T6.fig2.3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.10.9.1.1" class="ltx_text ltx_font_bold">#right hands</span></th>
<td id="S7.T6.fig2.3.1.10.9.2" class="ltx_td ltx_align_center">2,490</td>
</tr>
<tr id="S7.T6.fig2.3.1.11.10" class="ltx_tr">
<th id="S7.T6.fig2.3.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.11.10.1.1" class="ltx_text ltx_font_bold">#object categories</span></th>
<td id="S7.T6.fig2.3.1.11.10.2" class="ltx_td ltx_align_center">19</td>
</tr>
<tr id="S7.T6.fig2.3.1.12.11" class="ltx_tr">
<th id="S7.T6.fig2.3.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r"><span id="S7.T6.fig2.3.1.12.11.1.1" class="ltx_text ltx_font_bold">#objects</span></th>
<td id="S7.T6.fig2.3.1.12.11.2" class="ltx_td ltx_align_center">17,598</td>
</tr>
<tr id="S7.T6.fig2.3.1.13.12" class="ltx_tr">
<th id="S7.T6.fig2.3.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S7.T6.fig2.3.1.13.12.1.1" class="ltx_text ltx_font_bold">#active objects</span></th>
<td id="S7.T6.fig2.3.1.13.12.2" class="ltx_td ltx_align_center ltx_border_b">2,872</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</div>
</div>
</figure>
</section>
<section id="S7.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span>Additional Dataset</h3>

<div id="S7.SS1.p1" class="ltx_para">
<p id="S7.SS1.p1.1" class="ltx_p">In order to train the classifier used in the baseline <span id="S7.SS1.p1.1.1" class="ltx_text ltx_font_italic">BS1</span>, we collected and annotated an additional set of data in the same environment. To this end, we acquired 19 videos (i.e., one for each object class) at a resolution of 1920x1080 pixels and with a framerate of 30fps using a smartphone. Each video shows a single object from different points of view. From these videos, we have extracted about 27,000 frames labeled with the object category. FigureÂ <a href="#S7.F8" title="Figure 8 â€£ 7.1 Additional Dataset â€£ 7 Additional Details on the Dataset â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows some examples of the extracted frames.</p>
</div>
<figure id="S7.F8" class="ltx_figure"><img src="/html/2204.07061/assets/images/sup/oscilloscope_views.jpg" id="S7.F8.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="113" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S7.F8.2.1.1" class="ltx_text" style="font-size:90%;">Figure 8</span>: </span><span id="S7.F8.3.2" class="ltx_text" style="font-size:90%;">Examples of frames in which the object has been acquired from different points of view.</span></figcaption>
</figure>
</section>
</section>
<section id="S8" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Details of the Proposed Approach</h2>

<section id="S8.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span>Network Details</h3>

<div id="S8.SS1.p1" class="ltx_para">
<p id="S8.SS1.p1.1" class="ltx_p">We built our system by extending the implementation of Faster R-CNNÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> from the detectron2 framework<span id="footnote8" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a target="_blank" href="https://github.com/facebookresearch/detectron2" title="" class="ltx_ref ltx_url" style="color:#0000FF;">https://github.com/facebookresearch/detectron2</a></span></span></span> with four additional components: 1) the hand side classification module, 2) the hand state classification module, 3) the offset vector regression module, and 4) the matching algorithm. FigureÂ <a href="#S8.F9" title="Figure 9 â€£ 8.1 Network Details â€£ 8 Details of the Proposed Approach â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">9</span></a> shows the architectures of the modules.</p>
</div>
<figure id="S8.F9" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S8.F9.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/images/modules/hand_side_module.jpg" id="S8.F9.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="306" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F9.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S8.F9.sf1.3.2" class="ltx_text" style="font-size:90%;">Hand side classification module.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S8.F9.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/images/modules/hand_state_module.jpg" id="S8.F9.sf2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="306" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F9.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S8.F9.sf2.3.2" class="ltx_text" style="font-size:90%;">Hand state classification module.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S8.F9.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/images/modules/hand_vector.jpg" id="S8.F9.sf3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="479" height="406" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F9.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S8.F9.sf3.3.2" class="ltx_text" style="font-size:90%;">Offset vector regression module.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F9.2.1.1" class="ltx_text" style="font-size:90%;">Figure 9</span>: </span><span id="S8.F9.3.2" class="ltx_text" style="font-size:90%;">Architectures of the proposed modules.</span></figcaption>
</figure>
</section>
<section id="S8.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span>Synthetic Motion Blur</h3>

<div id="S8.SS2.p1" class="ltx_para">
<p id="S8.SS2.p1.1" class="ltx_p">Since in the real dataset several frames are blurred due to the camera motion (see FigureÂ <a href="#S8.F10" title="Figure 10 â€£ 8.2 Synthetic Motion Blur â€£ 8 Details of the Proposed Approach â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10</span></a>), we adopted a non-linear motion blur procedureÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> on the synthetic images to further reduce the gap between real and synthetic domains. 
<br class="ltx_break">During the training phase, given a synthetic image (FigureÂ <a href="#S8.F11" title="Figure 11 â€£ 8.2 Synthetic Motion Blur â€£ 8 Details of the Proposed Approach â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> - (a)) we applied a non-linear motion blur kernel (FigureÂ <a href="#S8.F11" title="Figure 11 â€£ 8.2 Synthetic Motion Blur â€£ 8 Details of the Proposed Approach â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> - (b)). As shown in the work ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite>, correcting the coordinates of the bounding boxes further increases object detection performance. To do this, we applied the same kernel to the related semantic mask obtaining the new coordinates of the objects bounding boxes (FigureÂ <a href="#S8.F11" title="Figure 11 â€£ 8.2 Synthetic Motion Blur â€£ 8 Details of the Proposed Approach â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">11</span></a> - (c)).</p>
</div>
<figure id="S8.F10" class="ltx_figure"><img src="/html/2204.07061/assets/images/examples/44_17163_motion_blur_real.jpg" id="S8.F10.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="419" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F10.2.1.1" class="ltx_text" style="font-size:90%;">Figure 10</span>: </span><span id="S8.F10.3.2" class="ltx_text" style="font-size:90%;">Example of blurred frame due to camera motion.</span></figcaption>
</figure>
<figure id="S8.F11" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S8.F11.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/images/blurred_synth/normal_w_box.jpg" id="S8.F11.sf1.g1" class="ltx_graphics ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S8.F11.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S8.F11.sf1.3.2" class="ltx_text" style="font-size:90%;">Synthetic image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S8.F11.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/images/blurred_synth/blurred_normal.jpg" id="S8.F11.sf2.g1" class="ltx_graphics ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S8.F11.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S8.F11.sf2.3.2" class="ltx_text" style="font-size:90%;">Blurred image</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S8.F11.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/images/blurred_synth/blurred_corrected.jpg" id="S8.F11.sf3.g1" class="ltx_graphics ltx_img_landscape" width="598" height="337" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S8.F11.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S8.F11.sf3.3.2" class="ltx_text" style="font-size:90%;">Boxes correction</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S8.F11.2.1.1" class="ltx_text" style="font-size:90%;">Figure 11</span>: </span><span id="S8.F11.3.2" class="ltx_text" style="font-size:90%;">Figure (a) shows the image before applying the synthetic motion blur, whereas in (b) is shown the same image after the application of motion blur, and (c) shows the correction process of the bounding boxes (indicated with purple boxes).</span></figcaption>
</figure>
</section>
</section>
<section id="S9" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span>Additional Experiments and Results</h2>

<section id="S9.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span>Training Details</h3>

<div id="S9.SS1.p1" class="ltx_para">
<p id="S9.SS1.p1.2" class="ltx_p">We used an Nvidia V100 GPU to perform the experiments. To train the model with the synthetic dataset, we initialized the learning rate to 0.001 and set a warm-up factor of 1000. We set the batch size to 4 and trained for 50,000 iterations. Instead, for training with real data, we decreased the learning rate by a factor of 10 after 12,500 and 15,000 iterations. We have set the batch size to 2 and trained for 20,000 iterations. Lastly, all the images were rescaled to <math id="S9.SS1.p1.1.m1.1" class="ltx_Math" alttext="1280" display="inline"><semantics id="S9.SS1.p1.1.m1.1a"><mn id="S9.SS1.p1.1.m1.1.1" xref="S9.SS1.p1.1.m1.1.1.cmml">1280</mn><annotation-xml encoding="MathML-Content" id="S9.SS1.p1.1.m1.1b"><cn type="integer" id="S9.SS1.p1.1.m1.1.1.cmml" xref="S9.SS1.p1.1.m1.1.1">1280</cn></annotation-xml><annotation encoding="application/x-tex" id="S9.SS1.p1.1.m1.1c">1280</annotation></semantics></math>x<math id="S9.SS1.p1.2.m2.1" class="ltx_Math" alttext="720" display="inline"><semantics id="S9.SS1.p1.2.m2.1a"><mn id="S9.SS1.p1.2.m2.1.1" xref="S9.SS1.p1.2.m2.1.1.cmml">720</mn><annotation-xml encoding="MathML-Content" id="S9.SS1.p1.2.m2.1b"><cn type="integer" id="S9.SS1.p1.2.m2.1.1.cmml" xref="S9.SS1.p1.2.m2.1.1">720</cn></annotation-xml><annotation encoding="application/x-tex" id="S9.SS1.p1.2.m2.1c">720</annotation></semantics></math> pixels for both the training and testing phases. 
<br class="ltx_break">To train the network, we used the standard Faster R-CNN losses. In addition, for the <span id="S9.SS1.p1.2.1" class="ltx_text ltx_font_italic">hand side classification</span> and the <span id="S9.SS1.p1.2.2" class="ltx_text ltx_font_italic">hand state classification</span> modules, we used the standard binary cross-entropy loss, whereas for the <span id="S9.SS1.p1.2.3" class="ltx_text ltx_font_italic">offset vector regression module</span>, we used the mean squared error loss. The final loss is the sum of all the losses:</p>
<table id="S9.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S9.E1.m1.1" class="ltx_Math" alttext="Loss=L_{faster\_rcnn}+L_{side}+L_{state}+L_{vector}" display="block"><semantics id="S9.E1.m1.1a"><mrow id="S9.E1.m1.1.1" xref="S9.E1.m1.1.1.cmml"><mrow id="S9.E1.m1.1.1.2" xref="S9.E1.m1.1.1.2.cmml"><mi id="S9.E1.m1.1.1.2.2" xref="S9.E1.m1.1.1.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.2.1" xref="S9.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.2.3" xref="S9.E1.m1.1.1.2.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.2.1a" xref="S9.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.2.4" xref="S9.E1.m1.1.1.2.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.2.1b" xref="S9.E1.m1.1.1.2.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.2.5" xref="S9.E1.m1.1.1.2.5.cmml">s</mi></mrow><mo id="S9.E1.m1.1.1.1" xref="S9.E1.m1.1.1.1.cmml">=</mo><mrow id="S9.E1.m1.1.1.3" xref="S9.E1.m1.1.1.3.cmml"><msub id="S9.E1.m1.1.1.3.2" xref="S9.E1.m1.1.1.3.2.cmml"><mi id="S9.E1.m1.1.1.3.2.2" xref="S9.E1.m1.1.1.3.2.2.cmml">L</mi><mrow id="S9.E1.m1.1.1.3.2.3" xref="S9.E1.m1.1.1.3.2.3.cmml"><mi id="S9.E1.m1.1.1.3.2.3.2" xref="S9.E1.m1.1.1.3.2.3.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.2.3.3" xref="S9.E1.m1.1.1.3.2.3.3.cmml">a</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1a" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.2.3.4" xref="S9.E1.m1.1.1.3.2.3.4.cmml">s</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1b" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.2.3.5" xref="S9.E1.m1.1.1.3.2.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1c" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.2.3.6" xref="S9.E1.m1.1.1.3.2.3.6.cmml">e</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1d" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.2.3.7" xref="S9.E1.m1.1.1.3.2.3.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1e" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi mathvariant="normal" id="S9.E1.m1.1.1.3.2.3.8" xref="S9.E1.m1.1.1.3.2.3.8.cmml">_</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1f" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.2.3.9" xref="S9.E1.m1.1.1.3.2.3.9.cmml">r</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1g" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.2.3.10" xref="S9.E1.m1.1.1.3.2.3.10.cmml">c</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1h" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.2.3.11" xref="S9.E1.m1.1.1.3.2.3.11.cmml">n</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.2.3.1i" xref="S9.E1.m1.1.1.3.2.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.2.3.12" xref="S9.E1.m1.1.1.3.2.3.12.cmml">n</mi></mrow></msub><mo id="S9.E1.m1.1.1.3.1" xref="S9.E1.m1.1.1.3.1.cmml">+</mo><msub id="S9.E1.m1.1.1.3.3" xref="S9.E1.m1.1.1.3.3.cmml"><mi id="S9.E1.m1.1.1.3.3.2" xref="S9.E1.m1.1.1.3.3.2.cmml">L</mi><mrow id="S9.E1.m1.1.1.3.3.3" xref="S9.E1.m1.1.1.3.3.3.cmml"><mi id="S9.E1.m1.1.1.3.3.3.2" xref="S9.E1.m1.1.1.3.3.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.3.3.1" xref="S9.E1.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.3.3.3" xref="S9.E1.m1.1.1.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.3.3.1a" xref="S9.E1.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.3.3.4" xref="S9.E1.m1.1.1.3.3.3.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.3.3.1b" xref="S9.E1.m1.1.1.3.3.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.3.3.5" xref="S9.E1.m1.1.1.3.3.3.5.cmml">e</mi></mrow></msub><mo id="S9.E1.m1.1.1.3.1a" xref="S9.E1.m1.1.1.3.1.cmml">+</mo><msub id="S9.E1.m1.1.1.3.4" xref="S9.E1.m1.1.1.3.4.cmml"><mi id="S9.E1.m1.1.1.3.4.2" xref="S9.E1.m1.1.1.3.4.2.cmml">L</mi><mrow id="S9.E1.m1.1.1.3.4.3" xref="S9.E1.m1.1.1.3.4.3.cmml"><mi id="S9.E1.m1.1.1.3.4.3.2" xref="S9.E1.m1.1.1.3.4.3.2.cmml">s</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.4.3.1" xref="S9.E1.m1.1.1.3.4.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.4.3.3" xref="S9.E1.m1.1.1.3.4.3.3.cmml">t</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.4.3.1a" xref="S9.E1.m1.1.1.3.4.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.4.3.4" xref="S9.E1.m1.1.1.3.4.3.4.cmml">a</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.4.3.1b" xref="S9.E1.m1.1.1.3.4.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.4.3.5" xref="S9.E1.m1.1.1.3.4.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.4.3.1c" xref="S9.E1.m1.1.1.3.4.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.4.3.6" xref="S9.E1.m1.1.1.3.4.3.6.cmml">e</mi></mrow></msub><mo id="S9.E1.m1.1.1.3.1b" xref="S9.E1.m1.1.1.3.1.cmml">+</mo><msub id="S9.E1.m1.1.1.3.5" xref="S9.E1.m1.1.1.3.5.cmml"><mi id="S9.E1.m1.1.1.3.5.2" xref="S9.E1.m1.1.1.3.5.2.cmml">L</mi><mrow id="S9.E1.m1.1.1.3.5.3" xref="S9.E1.m1.1.1.3.5.3.cmml"><mi id="S9.E1.m1.1.1.3.5.3.2" xref="S9.E1.m1.1.1.3.5.3.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.5.3.1" xref="S9.E1.m1.1.1.3.5.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.5.3.3" xref="S9.E1.m1.1.1.3.5.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.5.3.1a" xref="S9.E1.m1.1.1.3.5.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.5.3.4" xref="S9.E1.m1.1.1.3.5.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.5.3.1b" xref="S9.E1.m1.1.1.3.5.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.5.3.5" xref="S9.E1.m1.1.1.3.5.3.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.5.3.1c" xref="S9.E1.m1.1.1.3.5.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.5.3.6" xref="S9.E1.m1.1.1.3.5.3.6.cmml">o</mi><mo lspace="0em" rspace="0em" id="S9.E1.m1.1.1.3.5.3.1d" xref="S9.E1.m1.1.1.3.5.3.1.cmml">â€‹</mo><mi id="S9.E1.m1.1.1.3.5.3.7" xref="S9.E1.m1.1.1.3.5.3.7.cmml">r</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S9.E1.m1.1b"><apply id="S9.E1.m1.1.1.cmml" xref="S9.E1.m1.1.1"><eq id="S9.E1.m1.1.1.1.cmml" xref="S9.E1.m1.1.1.1"></eq><apply id="S9.E1.m1.1.1.2.cmml" xref="S9.E1.m1.1.1.2"><times id="S9.E1.m1.1.1.2.1.cmml" xref="S9.E1.m1.1.1.2.1"></times><ci id="S9.E1.m1.1.1.2.2.cmml" xref="S9.E1.m1.1.1.2.2">ğ¿</ci><ci id="S9.E1.m1.1.1.2.3.cmml" xref="S9.E1.m1.1.1.2.3">ğ‘œ</ci><ci id="S9.E1.m1.1.1.2.4.cmml" xref="S9.E1.m1.1.1.2.4">ğ‘ </ci><ci id="S9.E1.m1.1.1.2.5.cmml" xref="S9.E1.m1.1.1.2.5">ğ‘ </ci></apply><apply id="S9.E1.m1.1.1.3.cmml" xref="S9.E1.m1.1.1.3"><plus id="S9.E1.m1.1.1.3.1.cmml" xref="S9.E1.m1.1.1.3.1"></plus><apply id="S9.E1.m1.1.1.3.2.cmml" xref="S9.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S9.E1.m1.1.1.3.2.1.cmml" xref="S9.E1.m1.1.1.3.2">subscript</csymbol><ci id="S9.E1.m1.1.1.3.2.2.cmml" xref="S9.E1.m1.1.1.3.2.2">ğ¿</ci><apply id="S9.E1.m1.1.1.3.2.3.cmml" xref="S9.E1.m1.1.1.3.2.3"><times id="S9.E1.m1.1.1.3.2.3.1.cmml" xref="S9.E1.m1.1.1.3.2.3.1"></times><ci id="S9.E1.m1.1.1.3.2.3.2.cmml" xref="S9.E1.m1.1.1.3.2.3.2">ğ‘“</ci><ci id="S9.E1.m1.1.1.3.2.3.3.cmml" xref="S9.E1.m1.1.1.3.2.3.3">ğ‘</ci><ci id="S9.E1.m1.1.1.3.2.3.4.cmml" xref="S9.E1.m1.1.1.3.2.3.4">ğ‘ </ci><ci id="S9.E1.m1.1.1.3.2.3.5.cmml" xref="S9.E1.m1.1.1.3.2.3.5">ğ‘¡</ci><ci id="S9.E1.m1.1.1.3.2.3.6.cmml" xref="S9.E1.m1.1.1.3.2.3.6">ğ‘’</ci><ci id="S9.E1.m1.1.1.3.2.3.7.cmml" xref="S9.E1.m1.1.1.3.2.3.7">ğ‘Ÿ</ci><ci id="S9.E1.m1.1.1.3.2.3.8.cmml" xref="S9.E1.m1.1.1.3.2.3.8">_</ci><ci id="S9.E1.m1.1.1.3.2.3.9.cmml" xref="S9.E1.m1.1.1.3.2.3.9">ğ‘Ÿ</ci><ci id="S9.E1.m1.1.1.3.2.3.10.cmml" xref="S9.E1.m1.1.1.3.2.3.10">ğ‘</ci><ci id="S9.E1.m1.1.1.3.2.3.11.cmml" xref="S9.E1.m1.1.1.3.2.3.11">ğ‘›</ci><ci id="S9.E1.m1.1.1.3.2.3.12.cmml" xref="S9.E1.m1.1.1.3.2.3.12">ğ‘›</ci></apply></apply><apply id="S9.E1.m1.1.1.3.3.cmml" xref="S9.E1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S9.E1.m1.1.1.3.3.1.cmml" xref="S9.E1.m1.1.1.3.3">subscript</csymbol><ci id="S9.E1.m1.1.1.3.3.2.cmml" xref="S9.E1.m1.1.1.3.3.2">ğ¿</ci><apply id="S9.E1.m1.1.1.3.3.3.cmml" xref="S9.E1.m1.1.1.3.3.3"><times id="S9.E1.m1.1.1.3.3.3.1.cmml" xref="S9.E1.m1.1.1.3.3.3.1"></times><ci id="S9.E1.m1.1.1.3.3.3.2.cmml" xref="S9.E1.m1.1.1.3.3.3.2">ğ‘ </ci><ci id="S9.E1.m1.1.1.3.3.3.3.cmml" xref="S9.E1.m1.1.1.3.3.3.3">ğ‘–</ci><ci id="S9.E1.m1.1.1.3.3.3.4.cmml" xref="S9.E1.m1.1.1.3.3.3.4">ğ‘‘</ci><ci id="S9.E1.m1.1.1.3.3.3.5.cmml" xref="S9.E1.m1.1.1.3.3.3.5">ğ‘’</ci></apply></apply><apply id="S9.E1.m1.1.1.3.4.cmml" xref="S9.E1.m1.1.1.3.4"><csymbol cd="ambiguous" id="S9.E1.m1.1.1.3.4.1.cmml" xref="S9.E1.m1.1.1.3.4">subscript</csymbol><ci id="S9.E1.m1.1.1.3.4.2.cmml" xref="S9.E1.m1.1.1.3.4.2">ğ¿</ci><apply id="S9.E1.m1.1.1.3.4.3.cmml" xref="S9.E1.m1.1.1.3.4.3"><times id="S9.E1.m1.1.1.3.4.3.1.cmml" xref="S9.E1.m1.1.1.3.4.3.1"></times><ci id="S9.E1.m1.1.1.3.4.3.2.cmml" xref="S9.E1.m1.1.1.3.4.3.2">ğ‘ </ci><ci id="S9.E1.m1.1.1.3.4.3.3.cmml" xref="S9.E1.m1.1.1.3.4.3.3">ğ‘¡</ci><ci id="S9.E1.m1.1.1.3.4.3.4.cmml" xref="S9.E1.m1.1.1.3.4.3.4">ğ‘</ci><ci id="S9.E1.m1.1.1.3.4.3.5.cmml" xref="S9.E1.m1.1.1.3.4.3.5">ğ‘¡</ci><ci id="S9.E1.m1.1.1.3.4.3.6.cmml" xref="S9.E1.m1.1.1.3.4.3.6">ğ‘’</ci></apply></apply><apply id="S9.E1.m1.1.1.3.5.cmml" xref="S9.E1.m1.1.1.3.5"><csymbol cd="ambiguous" id="S9.E1.m1.1.1.3.5.1.cmml" xref="S9.E1.m1.1.1.3.5">subscript</csymbol><ci id="S9.E1.m1.1.1.3.5.2.cmml" xref="S9.E1.m1.1.1.3.5.2">ğ¿</ci><apply id="S9.E1.m1.1.1.3.5.3.cmml" xref="S9.E1.m1.1.1.3.5.3"><times id="S9.E1.m1.1.1.3.5.3.1.cmml" xref="S9.E1.m1.1.1.3.5.3.1"></times><ci id="S9.E1.m1.1.1.3.5.3.2.cmml" xref="S9.E1.m1.1.1.3.5.3.2">ğ‘£</ci><ci id="S9.E1.m1.1.1.3.5.3.3.cmml" xref="S9.E1.m1.1.1.3.5.3.3">ğ‘’</ci><ci id="S9.E1.m1.1.1.3.5.3.4.cmml" xref="S9.E1.m1.1.1.3.5.3.4">ğ‘</ci><ci id="S9.E1.m1.1.1.3.5.3.5.cmml" xref="S9.E1.m1.1.1.3.5.3.5">ğ‘¡</ci><ci id="S9.E1.m1.1.1.3.5.3.6.cmml" xref="S9.E1.m1.1.1.3.5.3.6">ğ‘œ</ci><ci id="S9.E1.m1.1.1.3.5.3.7.cmml" xref="S9.E1.m1.1.1.3.5.3.7">ğ‘Ÿ</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S9.E1.m1.1c">Loss=L_{faster\_rcnn}+L_{side}+L_{state}+L_{vector}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section id="S9.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span>Object Detection Performance</h3>

<div id="S9.SS2.p1" class="ltx_para">
<p id="S9.SS2.p1.1" class="ltx_p">FigureÂ <a href="#S9.F12" title="Figure 12 â€£ 9.2 Object Detection Performance â€£ 9 Additional Experiments and Results â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> - (a) shows a comparison of the object detection performance, considering the â€œ<span id="S9.SS2.p1.1.1" class="ltx_text ltx_font_italic">mAP</span>â€ curves, between the models
trained using synthetic data and different amount of real data (0%, 10%, 25%, 50%, 100%). TableÂ <a href="#S9.T7" title="Table 7 â€£ 9.2 Object Detection Performance â€£ 9 Additional Experiments and Results â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a> reports the AP of each class. Note that most of the best results come from the models pretrained using the synthetic data.</p>
</div>
<figure id="S9.F12" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S9.F12.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/x5.png" id="S9.F12.sf1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="285" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S9.F12.sf1.3.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S9.F12.sf1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">mAP</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S9.F12.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="" id="S9.F12.sf2.g1" class="ltx_graphics ltx_centering ltx_missing ltx_missing_image" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S9.F12.sf2.3.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S9.F12.sf2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">mAP All</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S9.F12.4.1.1" class="ltx_text" style="font-size:90%;">Figure 12</span>: </span><span id="S9.F12.5.2" class="ltx_text" style="font-size:90%;">Comparison of the results of the proposed method on the real test data in term of â€œ<span id="S9.F12.5.2.1" class="ltx_text ltx_font_italic">mAP</span>â€ (a) and â€œ<span id="S9.F12.5.2.2" class="ltx_text ltx_font_italic">mAP All</span>â€ (b). The blue curves report the results of the models pretrained using synthetic data and finetuned with different amount of real data, while the red curves report the results of the models that used only real data.</span></figcaption>
</figure>
<figure id="S9.T7" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S9.T7.2.1.1" class="ltx_text" style="font-size:90%;">Table 7</span>: </span><span id="S9.T7.3.2" class="ltx_text" style="font-size:90%;">Object detection results per class using different amounts of real data. Best results in bold.</span></figcaption>
<div id="S9.T7.4" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:382.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-16.8pt,15.6pt) scale(0.924719705092589,0.924719705092589) ;">
<table id="S9.T7.4.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S9.T7.4.1.1.1" class="ltx_tr">
<td id="S9.T7.4.1.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t" rowspan="4"><span id="S9.T7.4.1.1.1.1.1" class="ltx_text ltx_font_bold">Category</span></td>
<td id="S9.T7.4.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t" colspan="9"><span id="S9.T7.4.1.1.1.2.1" class="ltx_text ltx_font_bold">AP%</span></td>
</tr>
<tr id="S9.T7.4.1.2.2" class="ltx_tr">
<td id="S9.T7.4.1.2.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="9"><span id="S9.T7.4.1.2.2.1.1" class="ltx_text ltx_font_bold">Real Data%</span></td>
</tr>
<tr id="S9.T7.4.1.3.3" class="ltx_tr">
<td id="S9.T7.4.1.3.3.1" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" colspan="5"><span id="S9.T7.4.1.3.3.1.1" class="ltx_text ltx_font_bold">Pretraining Synthetic</span></td>
<td id="S9.T7.4.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t" colspan="4"><span id="S9.T7.4.1.3.3.2.1" class="ltx_text ltx_font_bold">No Pretraining</span></td>
</tr>
<tr id="S9.T7.4.1.4.4" class="ltx_tr">
<td id="S9.T7.4.1.4.4.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T7.4.1.4.4.1.1" class="ltx_text ltx_font_bold">0%</span></td>
<td id="S9.T7.4.1.4.4.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T7.4.1.4.4.2.1" class="ltx_text ltx_font_bold">10%</span></td>
<td id="S9.T7.4.1.4.4.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T7.4.1.4.4.3.1" class="ltx_text ltx_font_bold">25%</span></td>
<td id="S9.T7.4.1.4.4.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T7.4.1.4.4.4.1" class="ltx_text ltx_font_bold">50%</span></td>
<td id="S9.T7.4.1.4.4.5" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t"><span id="S9.T7.4.1.4.4.5.1" class="ltx_text ltx_font_bold">100%</span></td>
<td id="S9.T7.4.1.4.4.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T7.4.1.4.4.6.1" class="ltx_text ltx_font_bold">10%</span></td>
<td id="S9.T7.4.1.4.4.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T7.4.1.4.4.7.1" class="ltx_text ltx_font_bold">25%</span></td>
<td id="S9.T7.4.1.4.4.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T7.4.1.4.4.8.1" class="ltx_text ltx_font_bold">50%</span></td>
<td id="S9.T7.4.1.4.4.9" class="ltx_td ltx_align_center ltx_border_t"><span id="S9.T7.4.1.4.4.9.1" class="ltx_text ltx_font_bold">100%</span></td>
</tr>
<tr id="S9.T7.4.1.5.5" class="ltx_tr">
<td id="S9.T7.4.1.5.5.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t"><span id="S9.T7.4.1.5.5.1.1" class="ltx_text ltx_font_bold">power supply</span></td>
<td id="S9.T7.4.1.5.5.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">77.34</td>
<td id="S9.T7.4.1.5.5.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">78.48</td>
<td id="S9.T7.4.1.5.5.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">79.92</td>
<td id="S9.T7.4.1.5.5.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T7.4.1.5.5.5.1" class="ltx_text ltx_font_bold">88.51</span></td>
<td id="S9.T7.4.1.5.5.6" class="ltx_td ltx_align_center ltx_border_rr ltx_border_t">70.24</td>
<td id="S9.T7.4.1.5.5.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">74.48</td>
<td id="S9.T7.4.1.5.5.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">58.69</td>
<td id="S9.T7.4.1.5.5.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">70.32</td>
<td id="S9.T7.4.1.5.5.10" class="ltx_td ltx_align_center ltx_border_t">59.8</td>
</tr>
<tr id="S9.T7.4.1.6.6" class="ltx_tr">
<td id="S9.T7.4.1.6.6.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.6.6.1.1" class="ltx_text ltx_font_bold">oscilloscope</span></td>
<td id="S9.T7.4.1.6.6.2" class="ltx_td ltx_align_center ltx_border_r">80.64</td>
<td id="S9.T7.4.1.6.6.3" class="ltx_td ltx_align_center ltx_border_r">89.59</td>
<td id="S9.T7.4.1.6.6.4" class="ltx_td ltx_align_center ltx_border_r">89.27</td>
<td id="S9.T7.4.1.6.6.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.6.6.5.1" class="ltx_text ltx_font_bold">90.36</span></td>
<td id="S9.T7.4.1.6.6.6" class="ltx_td ltx_align_center ltx_border_rr">90.17</td>
<td id="S9.T7.4.1.6.6.7" class="ltx_td ltx_align_center ltx_border_r">80.35</td>
<td id="S9.T7.4.1.6.6.8" class="ltx_td ltx_align_center ltx_border_r">80.79</td>
<td id="S9.T7.4.1.6.6.9" class="ltx_td ltx_align_center ltx_border_r">81.03</td>
<td id="S9.T7.4.1.6.6.10" class="ltx_td ltx_align_center">89.74</td>
</tr>
<tr id="S9.T7.4.1.7.7" class="ltx_tr">
<td id="S9.T7.4.1.7.7.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.7.7.1.1" class="ltx_text ltx_font_bold">welder station</span></td>
<td id="S9.T7.4.1.7.7.2" class="ltx_td ltx_align_center ltx_border_r">80.86</td>
<td id="S9.T7.4.1.7.7.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.7.7.3.1" class="ltx_text ltx_font_bold">89.59</span></td>
<td id="S9.T7.4.1.7.7.4" class="ltx_td ltx_align_center ltx_border_r">89.25</td>
<td id="S9.T7.4.1.7.7.5" class="ltx_td ltx_align_center ltx_border_r">89.49</td>
<td id="S9.T7.4.1.7.7.6" class="ltx_td ltx_align_center ltx_border_rr">89.52</td>
<td id="S9.T7.4.1.7.7.7" class="ltx_td ltx_align_center ltx_border_r">88</td>
<td id="S9.T7.4.1.7.7.8" class="ltx_td ltx_align_center ltx_border_r">89.12</td>
<td id="S9.T7.4.1.7.7.9" class="ltx_td ltx_align_center ltx_border_r">88.51</td>
<td id="S9.T7.4.1.7.7.10" class="ltx_td ltx_align_center"><span id="S9.T7.4.1.7.7.10.1" class="ltx_text ltx_font_bold">89.59</span></td>
</tr>
<tr id="S9.T7.4.1.8.8" class="ltx_tr">
<td id="S9.T7.4.1.8.8.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.8.8.1.1" class="ltx_text ltx_font_bold">electric screwdriver</span></td>
<td id="S9.T7.4.1.8.8.2" class="ltx_td ltx_align_center ltx_border_r">75.38</td>
<td id="S9.T7.4.1.8.8.3" class="ltx_td ltx_align_center ltx_border_r">71.68</td>
<td id="S9.T7.4.1.8.8.4" class="ltx_td ltx_align_center ltx_border_r">71.32</td>
<td id="S9.T7.4.1.8.8.5" class="ltx_td ltx_align_center ltx_border_r">80.04</td>
<td id="S9.T7.4.1.8.8.6" class="ltx_td ltx_align_center ltx_border_rr">79.98</td>
<td id="S9.T7.4.1.8.8.7" class="ltx_td ltx_align_center ltx_border_r">62.2</td>
<td id="S9.T7.4.1.8.8.8" class="ltx_td ltx_align_center ltx_border_r">70.93</td>
<td id="S9.T7.4.1.8.8.9" class="ltx_td ltx_align_center ltx_border_r">71.62</td>
<td id="S9.T7.4.1.8.8.10" class="ltx_td ltx_align_center"><span id="S9.T7.4.1.8.8.10.1" class="ltx_text ltx_font_bold">80.78</span></td>
</tr>
<tr id="S9.T7.4.1.9.9" class="ltx_tr">
<td id="S9.T7.4.1.9.9.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.9.9.1.1" class="ltx_text ltx_font_bold">screwdriver</span></td>
<td id="S9.T7.4.1.9.9.2" class="ltx_td ltx_align_center ltx_border_r">42.76</td>
<td id="S9.T7.4.1.9.9.3" class="ltx_td ltx_align_center ltx_border_r">50.28</td>
<td id="S9.T7.4.1.9.9.4" class="ltx_td ltx_align_center ltx_border_r">53.68</td>
<td id="S9.T7.4.1.9.9.5" class="ltx_td ltx_align_center ltx_border_r">56.41</td>
<td id="S9.T7.4.1.9.9.6" class="ltx_td ltx_align_center ltx_border_rr">55.83</td>
<td id="S9.T7.4.1.9.9.7" class="ltx_td ltx_align_center ltx_border_r">40.96</td>
<td id="S9.T7.4.1.9.9.8" class="ltx_td ltx_align_center ltx_border_r">48.6</td>
<td id="S9.T7.4.1.9.9.9" class="ltx_td ltx_align_center ltx_border_r">53.6</td>
<td id="S9.T7.4.1.9.9.10" class="ltx_td ltx_align_center"><span id="S9.T7.4.1.9.9.10.1" class="ltx_text ltx_font_bold">57.11</span></td>
</tr>
<tr id="S9.T7.4.1.10.10" class="ltx_tr">
<td id="S9.T7.4.1.10.10.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.10.10.1.1" class="ltx_text ltx_font_bold">pliers</span></td>
<td id="S9.T7.4.1.10.10.2" class="ltx_td ltx_align_center ltx_border_r">78.78</td>
<td id="S9.T7.4.1.10.10.3" class="ltx_td ltx_align_center ltx_border_r">79.83</td>
<td id="S9.T7.4.1.10.10.4" class="ltx_td ltx_align_center ltx_border_r">79.36</td>
<td id="S9.T7.4.1.10.10.5" class="ltx_td ltx_align_center ltx_border_r">80.37</td>
<td id="S9.T7.4.1.10.10.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S9.T7.4.1.10.10.6.1" class="ltx_text ltx_font_bold">80.77</span></td>
<td id="S9.T7.4.1.10.10.7" class="ltx_td ltx_align_center ltx_border_r">77.77</td>
<td id="S9.T7.4.1.10.10.8" class="ltx_td ltx_align_center ltx_border_r">78.29</td>
<td id="S9.T7.4.1.10.10.9" class="ltx_td ltx_align_center ltx_border_r">71.1</td>
<td id="S9.T7.4.1.10.10.10" class="ltx_td ltx_align_center">80.58</td>
</tr>
<tr id="S9.T7.4.1.11.11" class="ltx_tr">
<td id="S9.T7.4.1.11.11.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.11.11.1.1" class="ltx_text ltx_font_bold">welder probe tip</span></td>
<td id="S9.T7.4.1.11.11.2" class="ltx_td ltx_align_center ltx_border_r">9.09</td>
<td id="S9.T7.4.1.11.11.3" class="ltx_td ltx_align_center ltx_border_r">32.64</td>
<td id="S9.T7.4.1.11.11.4" class="ltx_td ltx_align_center ltx_border_r">47.87</td>
<td id="S9.T7.4.1.11.11.5" class="ltx_td ltx_align_center ltx_border_r">49.5</td>
<td id="S9.T7.4.1.11.11.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S9.T7.4.1.11.11.6.1" class="ltx_text ltx_font_bold">50.37</span></td>
<td id="S9.T7.4.1.11.11.7" class="ltx_td ltx_align_center ltx_border_r">20.29</td>
<td id="S9.T7.4.1.11.11.8" class="ltx_td ltx_align_center ltx_border_r">37.6</td>
<td id="S9.T7.4.1.11.11.9" class="ltx_td ltx_align_center ltx_border_r">38.67</td>
<td id="S9.T7.4.1.11.11.10" class="ltx_td ltx_align_center">47.7</td>
</tr>
<tr id="S9.T7.4.1.12.12" class="ltx_tr">
<td id="S9.T7.4.1.12.12.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.12.12.1.1" class="ltx_text ltx_font_bold">oscilloscope probe tip</span></td>
<td id="S9.T7.4.1.12.12.2" class="ltx_td ltx_align_center ltx_border_r">1.82</td>
<td id="S9.T7.4.1.12.12.3" class="ltx_td ltx_align_center ltx_border_r">40.01</td>
<td id="S9.T7.4.1.12.12.4" class="ltx_td ltx_align_center ltx_border_r">41.41</td>
<td id="S9.T7.4.1.12.12.5" class="ltx_td ltx_align_center ltx_border_r">38.64</td>
<td id="S9.T7.4.1.12.12.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S9.T7.4.1.12.12.6.1" class="ltx_text ltx_font_bold">49.93</span></td>
<td id="S9.T7.4.1.12.12.7" class="ltx_td ltx_align_center ltx_border_r">37.85</td>
<td id="S9.T7.4.1.12.12.8" class="ltx_td ltx_align_center ltx_border_r">40.98</td>
<td id="S9.T7.4.1.12.12.9" class="ltx_td ltx_align_center ltx_border_r">42.52</td>
<td id="S9.T7.4.1.12.12.10" class="ltx_td ltx_align_center">43.28</td>
</tr>
<tr id="S9.T7.4.1.13.13" class="ltx_tr">
<td id="S9.T7.4.1.13.13.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.13.13.1.1" class="ltx_text ltx_font_bold">low voltage board</span></td>
<td id="S9.T7.4.1.13.13.2" class="ltx_td ltx_align_center ltx_border_r">62.59</td>
<td id="S9.T7.4.1.13.13.3" class="ltx_td ltx_align_center ltx_border_r">70.66</td>
<td id="S9.T7.4.1.13.13.4" class="ltx_td ltx_align_center ltx_border_r">79.45</td>
<td id="S9.T7.4.1.13.13.5" class="ltx_td ltx_align_center ltx_border_r">79.98</td>
<td id="S9.T7.4.1.13.13.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S9.T7.4.1.13.13.6.1" class="ltx_text ltx_font_bold">80.46</span></td>
<td id="S9.T7.4.1.13.13.7" class="ltx_td ltx_align_center ltx_border_r">68.15</td>
<td id="S9.T7.4.1.13.13.8" class="ltx_td ltx_align_center ltx_border_r">75.88</td>
<td id="S9.T7.4.1.13.13.9" class="ltx_td ltx_align_center ltx_border_r">79.26</td>
<td id="S9.T7.4.1.13.13.10" class="ltx_td ltx_align_center">80.13</td>
</tr>
<tr id="S9.T7.4.1.14.14" class="ltx_tr">
<td id="S9.T7.4.1.14.14.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.14.14.1.1" class="ltx_text ltx_font_bold">high voltage board</span></td>
<td id="S9.T7.4.1.14.14.2" class="ltx_td ltx_align_center ltx_border_r">41.45</td>
<td id="S9.T7.4.1.14.14.3" class="ltx_td ltx_align_center ltx_border_r">51.44</td>
<td id="S9.T7.4.1.14.14.4" class="ltx_td ltx_align_center ltx_border_r">59.08</td>
<td id="S9.T7.4.1.14.14.5" class="ltx_td ltx_align_center ltx_border_r">51.28</td>
<td id="S9.T7.4.1.14.14.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S9.T7.4.1.14.14.6.1" class="ltx_text ltx_font_bold">61.49</span></td>
<td id="S9.T7.4.1.14.14.7" class="ltx_td ltx_align_center ltx_border_r">50.59</td>
<td id="S9.T7.4.1.14.14.8" class="ltx_td ltx_align_center ltx_border_r">49.81</td>
<td id="S9.T7.4.1.14.14.9" class="ltx_td ltx_align_center ltx_border_r">54.84</td>
<td id="S9.T7.4.1.14.14.10" class="ltx_td ltx_align_center">57.93</td>
</tr>
<tr id="S9.T7.4.1.15.15" class="ltx_tr">
<td id="S9.T7.4.1.15.15.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.15.15.1.1" class="ltx_text ltx_font_bold">register</span></td>
<td id="S9.T7.4.1.15.15.2" class="ltx_td ltx_align_center ltx_border_r">58.48</td>
<td id="S9.T7.4.1.15.15.3" class="ltx_td ltx_align_center ltx_border_r">54.49</td>
<td id="S9.T7.4.1.15.15.4" class="ltx_td ltx_align_center ltx_border_r">79.05</td>
<td id="S9.T7.4.1.15.15.5" class="ltx_td ltx_align_center ltx_border_r">68.77</td>
<td id="S9.T7.4.1.15.15.6" class="ltx_td ltx_align_center ltx_border_rr">70.35</td>
<td id="S9.T7.4.1.15.15.7" class="ltx_td ltx_align_center ltx_border_r">56.88</td>
<td id="S9.T7.4.1.15.15.8" class="ltx_td ltx_align_center ltx_border_r">63.64</td>
<td id="S9.T7.4.1.15.15.9" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.15.15.9.1" class="ltx_text ltx_font_bold">79.09</span></td>
<td id="S9.T7.4.1.15.15.10" class="ltx_td ltx_align_center">75.22</td>
</tr>
<tr id="S9.T7.4.1.16.16" class="ltx_tr">
<td id="S9.T7.4.1.16.16.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.16.16.1.1" class="ltx_text ltx_font_bold">electric screwdriver battery</span></td>
<td id="S9.T7.4.1.16.16.2" class="ltx_td ltx_align_center ltx_border_r">68.87</td>
<td id="S9.T7.4.1.16.16.3" class="ltx_td ltx_align_center ltx_border_r">53.9</td>
<td id="S9.T7.4.1.16.16.4" class="ltx_td ltx_align_center ltx_border_r">58.33</td>
<td id="S9.T7.4.1.16.16.5" class="ltx_td ltx_align_center ltx_border_r">61.16</td>
<td id="S9.T7.4.1.16.16.6" class="ltx_td ltx_align_center ltx_border_rr">62.12</td>
<td id="S9.T7.4.1.16.16.7" class="ltx_td ltx_align_center ltx_border_r">18.18</td>
<td id="S9.T7.4.1.16.16.8" class="ltx_td ltx_align_center ltx_border_r">31.17</td>
<td id="S9.T7.4.1.16.16.9" class="ltx_td ltx_align_center ltx_border_r">58.6</td>
<td id="S9.T7.4.1.16.16.10" class="ltx_td ltx_align_center"><span id="S9.T7.4.1.16.16.10.1" class="ltx_text ltx_font_bold">62.81</span></td>
</tr>
<tr id="S9.T7.4.1.17.17" class="ltx_tr">
<td id="S9.T7.4.1.17.17.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.17.17.1.1" class="ltx_text ltx_font_bold">working area</span></td>
<td id="S9.T7.4.1.17.17.2" class="ltx_td ltx_align_center ltx_border_r">69.23</td>
<td id="S9.T7.4.1.17.17.3" class="ltx_td ltx_align_center ltx_border_r">79.5</td>
<td id="S9.T7.4.1.17.17.4" class="ltx_td ltx_align_center ltx_border_r">79.03</td>
<td id="S9.T7.4.1.17.17.5" class="ltx_td ltx_align_center ltx_border_r">79.67</td>
<td id="S9.T7.4.1.17.17.6" class="ltx_td ltx_align_center ltx_border_rr">79.68</td>
<td id="S9.T7.4.1.17.17.7" class="ltx_td ltx_align_center ltx_border_r">77.76</td>
<td id="S9.T7.4.1.17.17.8" class="ltx_td ltx_align_center ltx_border_r">77.54</td>
<td id="S9.T7.4.1.17.17.9" class="ltx_td ltx_align_center ltx_border_r">77.8</td>
<td id="S9.T7.4.1.17.17.10" class="ltx_td ltx_align_center">78.77</td>
</tr>
<tr id="S9.T7.4.1.18.18" class="ltx_tr">
<td id="S9.T7.4.1.18.18.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.18.18.1.1" class="ltx_text ltx_font_bold">welder base</span></td>
<td id="S9.T7.4.1.18.18.2" class="ltx_td ltx_align_center ltx_border_r">81.35</td>
<td id="S9.T7.4.1.18.18.3" class="ltx_td ltx_align_center ltx_border_r">81.77</td>
<td id="S9.T7.4.1.18.18.4" class="ltx_td ltx_align_center ltx_border_r">81.73</td>
<td id="S9.T7.4.1.18.18.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.18.18.5.1" class="ltx_text ltx_font_bold">90.79</span></td>
<td id="S9.T7.4.1.18.18.6" class="ltx_td ltx_align_center ltx_border_rr">81.82</td>
<td id="S9.T7.4.1.18.18.7" class="ltx_td ltx_align_center ltx_border_r">71.37</td>
<td id="S9.T7.4.1.18.18.8" class="ltx_td ltx_align_center ltx_border_r">81.07</td>
<td id="S9.T7.4.1.18.18.9" class="ltx_td ltx_align_center ltx_border_r">81.47</td>
<td id="S9.T7.4.1.18.18.10" class="ltx_td ltx_align_center">81.73</td>
</tr>
<tr id="S9.T7.4.1.19.19" class="ltx_tr">
<td id="S9.T7.4.1.19.19.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.19.19.1.1" class="ltx_text ltx_font_bold">socket</span></td>
<td id="S9.T7.4.1.19.19.2" class="ltx_td ltx_align_center ltx_border_r">61.45</td>
<td id="S9.T7.4.1.19.19.3" class="ltx_td ltx_align_center ltx_border_r">85.08</td>
<td id="S9.T7.4.1.19.19.4" class="ltx_td ltx_align_center ltx_border_r">88.48</td>
<td id="S9.T7.4.1.19.19.5" class="ltx_td ltx_align_center ltx_border_r">89.81</td>
<td id="S9.T7.4.1.19.19.6" class="ltx_td ltx_align_center ltx_border_rr">90.09</td>
<td id="S9.T7.4.1.19.19.7" class="ltx_td ltx_align_center ltx_border_r">86.25</td>
<td id="S9.T7.4.1.19.19.8" class="ltx_td ltx_align_center ltx_border_r">89.82</td>
<td id="S9.T7.4.1.19.19.9" class="ltx_td ltx_align_center ltx_border_r">90.19</td>
<td id="S9.T7.4.1.19.19.10" class="ltx_td ltx_align_center"><span id="S9.T7.4.1.19.19.10.1" class="ltx_text ltx_font_bold">90.43</span></td>
</tr>
<tr id="S9.T7.4.1.20.20" class="ltx_tr">
<td id="S9.T7.4.1.20.20.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.20.20.1.1" class="ltx_text ltx_font_bold">left red button</span></td>
<td id="S9.T7.4.1.20.20.2" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.20.20.2.1" class="ltx_text ltx_font_bold">90.91</span></td>
<td id="S9.T7.4.1.20.20.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.20.20.3.1" class="ltx_text ltx_font_bold">90.91</span></td>
<td id="S9.T7.4.1.20.20.4" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.20.20.4.1" class="ltx_text ltx_font_bold">90.91</span></td>
<td id="S9.T7.4.1.20.20.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.20.20.5.1" class="ltx_text ltx_font_bold">90.91</span></td>
<td id="S9.T7.4.1.20.20.6" class="ltx_td ltx_align_center ltx_border_rr"><span id="S9.T7.4.1.20.20.6.1" class="ltx_text ltx_font_bold">90.91</span></td>
<td id="S9.T7.4.1.20.20.7" class="ltx_td ltx_align_center ltx_border_r">9.09</td>
<td id="S9.T7.4.1.20.20.8" class="ltx_td ltx_align_center ltx_border_r">0</td>
<td id="S9.T7.4.1.20.20.9" class="ltx_td ltx_align_center ltx_border_r">72.73</td>
<td id="S9.T7.4.1.20.20.10" class="ltx_td ltx_align_center">54.55</td>
</tr>
<tr id="S9.T7.4.1.21.21" class="ltx_tr">
<td id="S9.T7.4.1.21.21.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.21.21.1.1" class="ltx_text ltx_font_bold">left green button</span></td>
<td id="S9.T7.4.1.21.21.2" class="ltx_td ltx_align_center ltx_border_r">88.48</td>
<td id="S9.T7.4.1.21.21.3" class="ltx_td ltx_align_center ltx_border_r">88.48</td>
<td id="S9.T7.4.1.21.21.4" class="ltx_td ltx_align_center ltx_border_r">87.88</td>
<td id="S9.T7.4.1.21.21.5" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.21.21.5.1" class="ltx_text ltx_font_bold">88.79</span></td>
<td id="S9.T7.4.1.21.21.6" class="ltx_td ltx_align_center ltx_border_rr">88.18</td>
<td id="S9.T7.4.1.21.21.7" class="ltx_td ltx_align_center ltx_border_r">13.64</td>
<td id="S9.T7.4.1.21.21.8" class="ltx_td ltx_align_center ltx_border_r">6.06</td>
<td id="S9.T7.4.1.21.21.9" class="ltx_td ltx_align_center ltx_border_r">70.55</td>
<td id="S9.T7.4.1.21.21.10" class="ltx_td ltx_align_center">62.34</td>
</tr>
<tr id="S9.T7.4.1.22.22" class="ltx_tr">
<td id="S9.T7.4.1.22.22.1" class="ltx_td ltx_align_left ltx_border_r"><span id="S9.T7.4.1.22.22.1.1" class="ltx_text ltx_font_bold">right red button</span></td>
<td id="S9.T7.4.1.22.22.2" class="ltx_td ltx_align_center ltx_border_r">95.82</td>
<td id="S9.T7.4.1.22.22.3" class="ltx_td ltx_align_center ltx_border_r"><span id="S9.T7.4.1.22.22.3.1" class="ltx_text ltx_font_bold">96.06</span></td>
<td id="S9.T7.4.1.22.22.4" class="ltx_td ltx_align_center ltx_border_r">94.55</td>
<td id="S9.T7.4.1.22.22.5" class="ltx_td ltx_align_center ltx_border_r">95.83</td>
<td id="S9.T7.4.1.22.22.6" class="ltx_td ltx_align_center ltx_border_rr">95.3</td>
<td id="S9.T7.4.1.22.22.7" class="ltx_td ltx_align_center ltx_border_r">51.13</td>
<td id="S9.T7.4.1.22.22.8" class="ltx_td ltx_align_center ltx_border_r">14.55</td>
<td id="S9.T7.4.1.22.22.9" class="ltx_td ltx_align_center ltx_border_r">86.57</td>
<td id="S9.T7.4.1.22.22.10" class="ltx_td ltx_align_center">67.74</td>
</tr>
<tr id="S9.T7.4.1.23.23" class="ltx_tr">
<td id="S9.T7.4.1.23.23.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_r"><span id="S9.T7.4.1.23.23.1.1" class="ltx_text ltx_font_bold">right green button</span></td>
<td id="S9.T7.4.1.23.23.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">96.97</td>
<td id="S9.T7.4.1.23.23.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">96.67</td>
<td id="S9.T7.4.1.23.23.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">96.97</td>
<td id="S9.T7.4.1.23.23.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">98.18</td>
<td id="S9.T7.4.1.23.23.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_rr"><span id="S9.T7.4.1.23.23.6.1" class="ltx_text ltx_font_bold">98.48</span></td>
<td id="S9.T7.4.1.23.23.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">27.27</td>
<td id="S9.T7.4.1.23.23.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">0</td>
<td id="S9.T7.4.1.23.23.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">83.8</td>
<td id="S9.T7.4.1.23.23.10" class="ltx_td ltx_align_center ltx_border_b">85.74</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="S9.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.3 </span>Egocentric Human Object Interaction Detection</h3>

<div id="S9.SS3.p1" class="ltx_para">
<p id="S9.SS3.p1.1" class="ltx_p">Figure <a href="#S9.F12" title="Figure 12 â€£ 9.2 Object Detection Performance â€£ 9 Additional Experiments and Results â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> - (b) shows the â€œ<span id="S9.SS3.p1.1.1" class="ltx_text ltx_font_italic">mAP All</span>â€ curves related to the different models trained using synthetic data and different amount of real data (0%, 10%, 25%, 50%, 100%). FigureÂ <a href="#S9.F13" title="Figure 13 â€£ 9.3 Egocentric Human Object Interaction Detection â€£ 9 Additional Experiments and Results â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> reports some qualitative examples obtained using the model pretrained on the synthetic dataset and finetuned with <math id="S9.SS3.p1.1.m1.1" class="ltx_Math" alttext="100\%" display="inline"><semantics id="S9.SS3.p1.1.m1.1a"><mrow id="S9.SS3.p1.1.m1.1.1" xref="S9.SS3.p1.1.m1.1.1.cmml"><mn id="S9.SS3.p1.1.m1.1.1.2" xref="S9.SS3.p1.1.m1.1.1.2.cmml">100</mn><mo id="S9.SS3.p1.1.m1.1.1.1" xref="S9.SS3.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S9.SS3.p1.1.m1.1b"><apply id="S9.SS3.p1.1.m1.1.1.cmml" xref="S9.SS3.p1.1.m1.1.1"><csymbol cd="latexml" id="S9.SS3.p1.1.m1.1.1.1.cmml" xref="S9.SS3.p1.1.m1.1.1.1">percent</csymbol><cn type="integer" id="S9.SS3.p1.1.m1.1.1.2.cmml" xref="S9.SS3.p1.1.m1.1.1.2">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S9.SS3.p1.1.m1.1c">100\%</annotation></semantics></math> of the real data. A qualitative comparison between the proposed method andÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> is shown in FigureÂ <a href="#S9.F14" title="Figure 14 â€£ 9.3 Egocentric Human Object Interaction Detection â€£ 9 Additional Experiments and Results â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a>.</p>
</div>
<figure id="S9.F13" class="ltx_figure"><img src="/html/2204.07061/assets/images/Qualtitative_examples_ours.jpg" id="S9.F13.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="169" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S9.F13.2.1.1" class="ltx_text" style="font-size:90%;">Figure 13</span>: </span><span id="S9.F13.3.2" class="ltx_text" style="font-size:90%;">Qualitative results of the proposed method pretrained with the synthetic dataset and finetuned with the 100% of the real data.</span></figcaption>
</figure>
<figure id="S9.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S9.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/images/proposed_system_comparison.jpg" id="S9.F14.sf1.g1" class="ltx_graphics ltx_img_square" width="598" height="673" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S9.F14.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S9.F14.sf1.3.2" class="ltx_text" style="font-size:90%;">Proposed system</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S9.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2204.07061/assets/images/hic_comparison.jpg" id="S9.F14.sf2.g1" class="ltx_graphics ltx_img_square" width="598" height="673" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S9.F14.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S9.F14.sf2.3.2" class="ltx_text" style="font-size:90%;">Method ofÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> (BS5)</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S9.F14.2.1.1" class="ltx_text" style="font-size:90%;">Figure 14</span>: </span><span id="S9.F14.3.2" class="ltx_text" style="font-size:90%;">Comparison between our method trained with synthetic data and 100% of the real dataset (a) and BS5 based onÂ <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> (b).</span></figcaption>
</figure>
</section>
<section id="S9.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.4 </span>Additional experiment</h3>

<div id="S9.SS4.p1" class="ltx_para">
<p id="S9.SS4.p1.1" class="ltx_p">Table <a href="#S9.T8" title="Table 8 â€£ 9.4 Additional experiment â€£ 9 Additional Experiments and Results â€£ Egocentric Human-Object Interaction Detection Exploiting Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">8</span></a> shows an experiment in which the proposed method was trained using only the annotations of the active objects. From the results obtained is clear that including all objects in the object detection phase helps to obtain better overall results (<span id="S9.SS4.p1.1.1" class="ltx_text ltx_font_italic">mAP All</span> of <math id="S9.SS4.p1.1.m1.1" class="ltx_Math" alttext="32.61\%" display="inline"><semantics id="S9.SS4.p1.1.m1.1a"><mrow id="S9.SS4.p1.1.m1.1.1" xref="S9.SS4.p1.1.m1.1.1.cmml"><mn id="S9.SS4.p1.1.m1.1.1.2" xref="S9.SS4.p1.1.m1.1.1.2.cmml">32.61</mn><mo id="S9.SS4.p1.1.m1.1.1.1" xref="S9.SS4.p1.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S9.SS4.p1.1.m1.1b"><apply id="S9.SS4.p1.1.m1.1.1.cmml" xref="S9.SS4.p1.1.m1.1.1"><csymbol cd="latexml" id="S9.SS4.p1.1.m1.1.1.1.cmml" xref="S9.SS4.p1.1.m1.1.1.1">percent</csymbol><cn type="float" id="S9.SS4.p1.1.m1.1.1.2.cmml" xref="S9.SS4.p1.1.m1.1.1.2">32.61</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S9.SS4.p1.1.m1.1c">32.61\%</annotation></semantics></math>).</p>
</div>
<figure id="S9.T8" class="ltx_table">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span id="S9.T8.4.1.1" class="ltx_text" style="font-size:90%;">Table 8</span>: </span><span id="S9.T8.5.2" class="ltx_text" style="font-size:90%;">Comparison between the models trained to recognize all the objects (<span id="S9.T8.5.2.1" class="ltx_text ltx_font_italic">EHOI_S+R</span>) or only the active objects (<span id="S9.T8.5.2.2" class="ltx_text ltx_font_italic">EHOI_ACTIVE_S+R</span>).</span></figcaption>
<div id="S9.T8.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:411.9pt;height:42.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-56.2pt,5.8pt) scale(0.785764327053796,0.785764327053796) ;">
<table id="S9.T8.6.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S9.T8.6.1.1.1" class="ltx_tr">
<th id="S9.T8.6.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S9.T8.6.1.1.1.1.1" class="ltx_text ltx_font_bold">Model</span></th>
<th id="S9.T8.6.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S9.T8.6.1.1.1.2.1" class="ltx_text ltx_font_bold">Pretraining</span></th>
<th id="S9.T8.6.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r"><span id="S9.T8.6.1.1.1.3.1" class="ltx_text ltx_font_bold">Real Data%</span></th>
<th id="S9.T8.6.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S9.T8.6.1.1.1.4.1" class="ltx_text ltx_font_bold">AP Hand</span></th>
<th id="S9.T8.6.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S9.T8.6.1.1.1.5.1" class="ltx_text ltx_font_bold">mAP Obj</span></th>
<th id="S9.T8.6.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S9.T8.6.1.1.1.6.1" class="ltx_text ltx_font_bold">mAR Obj</span></th>
<th id="S9.T8.6.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r"><span id="S9.T8.6.1.1.1.7.1" class="ltx_text ltx_font_bold">mAP H+Obj</span></th>
<th id="S9.T8.6.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column"><span id="S9.T8.6.1.1.1.8.1" class="ltx_text ltx_font_bold">mAP All</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S9.T8.6.1.2.1" class="ltx_tr">
<th id="S9.T8.6.1.2.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t"><span id="S9.T8.6.1.2.1.1.1" class="ltx_text ltx_font_bold">EHOI_S+R</span></th>
<th id="S9.T8.6.1.2.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Syntehtic</th>
<th id="S9.T8.6.1.2.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">100</th>
<td id="S9.T8.6.1.2.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T8.6.1.2.1.4.1" class="ltx_text ltx_font_bold">90.67</span></td>
<td id="S9.T8.6.1.2.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35.43</td>
<td id="S9.T8.6.1.2.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T8.6.1.2.1.6.1" class="ltx_text ltx_font_bold">49.03</span></td>
<td id="S9.T8.6.1.2.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S9.T8.6.1.2.1.7.1" class="ltx_text ltx_font_bold">34.09</span></td>
<td id="S9.T8.6.1.2.1.8" class="ltx_td ltx_align_center ltx_border_t"><span id="S9.T8.6.1.2.1.8.1" class="ltx_text ltx_font_bold">32.61</span></td>
</tr>
<tr id="S9.T8.6.1.3.2" class="ltx_tr">
<th id="S9.T8.6.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_r"><span id="S9.T8.6.1.3.2.1.1" class="ltx_text ltx_font_bold">EHOI_ACTIVE_S+R</span></th>
<th id="S9.T8.6.1.3.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">Syntehtic</th>
<th id="S9.T8.6.1.3.2.3" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r">100</th>
<td id="S9.T8.6.1.3.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S9.T8.6.1.3.2.4.1" class="ltx_text ltx_font_bold">90.67</span></td>
<td id="S9.T8.6.1.3.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S9.T8.6.1.3.2.5.1" class="ltx_text ltx_font_bold">37.20</span></td>
<td id="S9.T8.6.1.3.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">48.44</td>
<td id="S9.T8.6.1.3.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">29.50</td>
<td id="S9.T8.6.1.3.2.8" class="ltx_td ltx_align_center ltx_border_b">27.68</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Bambach, S., Lee, S., Crandall, D.J., Yu, C.: Lending a hand: Detecting hands
and recognizing activities in complex egocentric interactions. In:
International Conference on Computer Vision (2015)

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Betancourt, A., Morerio, P., Regazzoni, C.S., Rauterberg, M.: The evolution of
first person vision methods: A survey. IEEE Transactions on Circuits and
Systems for Video Technology (2015)

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Chao, Y.W., Liu, Y., Liu, X., Zeng, H., Deng, J.: Learning to detect
human-object interactions. In: Winter Conference on Applications of Computer
Vision (2018)

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Chen, Y., Huang, S., Yuan, T., Qi, S., Zhu, Y., Zhu, S.C.: Holistic++ scene
understanding: Single-view 3d holistic scene parsing and human pose
estimation with human-object interaction and physical commonsense. In:
International Conference on Computer Vision (2019)

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Cucchiara, R., DelÂ Bimbo, A.: Visions for augmented cultural heritage
experience. IEEE Multimedia (2014)

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Damen, D., Doughty, H., Farinella, G.M., , Furnari, A., Ma, J., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: Rescaling
egocentric vision: Collection, pipeline and challenges for epic-kitchens-100.
International Journal of Computer Vision (2021)

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Damen, D., Doughty, H., Farinella, G.M., Fidler, S., Furnari, A., Kazakos, E.,
Moltisanti, D., Munro, J., Perrett, T., Price, W., Wray, M.: Scaling
egocentric vision: The epic-kitchens dataset. In: European Conference on
Computer Vision (2018)

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Fu, Q., Liu, X., Kitani, K.M.: Sequential voting with relational box fields for
active object detection. ArXiv <span id="bib.bib8.1.1" class="ltx_text ltx_font_bold">abs/2110.11524</span> (2021)

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Furnari, A., Farinella, G., Battiato, S.: Temporal segmentation of egocentric
videos to highlight personal locations of interest. In: Lecture Notes in
Computer Science (2016)

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Furnari, A., Battiato, S., Grauman, K., Farinella, G.M.: Next-active-object
prediction from egocentric videos. Journal of Visual Communication and Image
Representation (2017)

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Furnari, A., Farinella, G.M.: Rolling-unrolling lstms for action anticipation
from first-person video. IEEE Transactions on Pattern Analysis and Machine
Intelligence (PAMI) (2021)

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Garcia-Hernando, G., Yuan, S., Baek, S., Kim, T.K.: First-person hand action
benchmark with rgb-d videos and 3d hand pose annotations. In: Conference on
Computer Vision and Pattern Recognition (2018)

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Gkioxari, G., Girshick, R., DollÃ¡r, P., He, K.: Detecting and recognizing
human-object intaractions. In: Conference on Computer Vision and Pattern
Recognition (2018)

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Gupta, S., Malik, J.: Visual semantic role labeling. ArXiv
<span id="bib.bib14.1.1" class="ltx_text ltx_font_bold">abs/1505.04474</span> (2015)

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Hasson, Y., Varol, G., Tzionas, D., Kalevatykh, I., Black, M., Laptev, I.,
Schmid, C.: Learning joint reconstruction of hands and manipulated objects.
In: Conference on Computer Vision and Pattern Recognition (2019)

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
recognition. In: Conference on Computer Vision and Pattern Recognition (2016)

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Heilbron, F.C., Escorcia, V., Ghanem, B., Niebles, J.C.: Activitynet: A
large-scale video benchmark for human activity understanding. In: Conference
on Computer Vision and Pattern Recognition (2015)

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.:
The kinetics human action video dataset. ArXiv <span id="bib.bib18.1.1" class="ltx_text ltx_font_bold">abs/1705.06950</span>
(2017)

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Li, Y., Liu, M., Rehg, J.M.: In the eye of the beholder: Gaze and actions in
first person video. IEEE transactions on pattern analysis and machine
intelligence (2021)

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Li, Y., Ye, Z., Rehg, J.M.: Delving into egocentric actions. In: Conference on
Computer Vision and Pattern Recognition (2015)

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Liao, Y., Liu, S., Wang, F., Chen, Y., Feng, J.: Ppdm: Parallel point detection
and matching for real-time human-object interaction detection. In: Conference
on Computer Vision and Pattern Recognition (2020)

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Lin, T.Y., DollÃ¡r, P., Girshick, R., He, K., Hariharan, B., Belongie, S.:
Feature pyramid networks for object detection. In: Conference on Computer
Vision and Pattern Recognition (2017)

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Lin, T.Y., Maire, M., Belongie, S.J., Hays, J., Perona, P., Ramanan, D.,
DollÃ¡r, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In:
European Conference on Computer Vision (2014)

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Lu, Y., Mayol-Cuevas, W.: The object at hand: Automated editing for mixed
reality video guidance from hand-object interactions. In: International
Symposium on Mixed and Augmented Reality (2021)

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Lu, Y., Mayol-Cuevas, W.W.: Understanding egocentric hand-object interactions
from hand pose estimation. ArXiv <span id="bib.bib25.1.1" class="ltx_text ltx_font_bold">abs/2109.14657</span> (2021)

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Mueller, F., Bernard, F., Sotnychenko, O., Mehta, D., Sridhar, S., Casas, D.,
Theobalt, C.: Ganerated hands for real-time 3d hand tracking from monocular
rgb. In: Conference on Computer Vision and Pattern Recognition (2018)

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Mueller, F., Mehta, D., Sotnychenko, O., Sridhar, S., Casas, D., Theobalt, C.:
Real-time hand tracking under occlusion from an egocentric rgb-d sensor. In:
International Conference on Computer Vision (2017)

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Ragusa, F., Furnari, A., Battiato, S., Signorello, G., Farinella, G.:
Egocentric visitors localization in cultural sites. Journal on Computing and
Cultural Heritage (2019)

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Ragusa, F., Furnari, A., Livatino, S., Farinella, G.M.: The meccano dataset:
Understanding human-object interactions from egocentric videos in an
industrial-like domain. In: Winter Conference on Applications of Computer
Vision (2021)

</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ren, S., He, K., Girshick, R., Sun, J.: Faster R-CNN: Towards real-time
object detection with region proposal networks. In: Advances in Neural
Information Processing Systems (2015)

</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Sayed, M., Brostow, G.: Improved handling of motion blur in online object
detection. In: Conference on Computer Vision and Pattern Recognition (2021)

</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Shan, D., Geng, J., Shu, M., Fouhey, D.F.: Understanding human hands in contact
at internet scale. In: Conference on Computer Vision and Pattern Recognition
(2020)

</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Wang, H., Pirk, S., Yumer, E., Kim, V.G., Sener, O., Sridhar, S., Guibas, L.J.:
Learning a generative model for multiâ€step humanâ€object interactions from
videos. Computer Graphics Forum (2019)

</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Zhang, F.Z., Campbell, D., Gould, S.: Efficient two-stage detection of
human-object interactions with a novel unary-pairwise transformer
<span id="bib.bib34.1.1" class="ltx_text ltx_font_bold">abs/2112.01838</span> (2021)

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2204.07060" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2204.07061" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2204.07061">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2204.07061" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2204.07063" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Mon Mar 11 12:46:07 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
