<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2309.03381] Active shooter detection and robust tracking utilizing supplemental synthetic data</title><meta property="og:description" content="The increasing concern surrounding gun violence in the United States has led to a focus on developing systems to improve public safety. One approach to developing such a system is to detect and track shooters, which wo…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Active shooter detection and robust tracking utilizing supplemental synthetic data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Active shooter detection and robust tracking utilizing supplemental synthetic data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2309.03381">

<!--Generated on Wed Feb 28 07:41:12 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_fleqn">
<h1 class="ltx_title ltx_title_document">Active shooter detection and robust tracking utilizing supplemental synthetic data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Joshua R. Waite
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Mechanical Engineering, Iowa State University, Ames, IA 50011, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiale Feng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Computer Science, Iowa State University, Ames, IA 50011, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Riley Tavassoli
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Mechanical Engineering, University of Tennessee, Knoxville, TN 37996, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Laura Harris
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Mechanical Engineering, University of Tennessee, Knoxville, TN 37996, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sin Yong Tan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Mechanical Engineering, Iowa State University, Ames, IA 50011, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Subhadeep Chakraborty
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Mechanical Engineering, University of Tennessee, Knoxville, TN 37996, USA
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Soumik Sarkar
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Mechanical Engineering, Iowa State University, Ames, IA 50011, USA
</span>
<span class="ltx_contact ltx_role_affiliation">soumiks@iastate.edu
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">The increasing concern surrounding gun violence in the United States has led to a focus on developing systems to improve public safety. One approach to developing such a system is to detect and track shooters, which would help prevent or mitigate the impact of violent incidents. In this paper, we proposed detecting shooters as a whole, rather than just guns, which would allow for improved tracking robustness, as obscuring the gun would no longer cause the system to lose sight of the threat. However, publicly available data on shooters is much more limited and challenging to create than a gun dataset alone. Therefore, we explore the use of domain randomization and transfer learning to improve the effectiveness of training with synthetic data obtained from Unreal Engine environments. This enables the model to be trained on a wider range of data, increasing its ability to generalize to different situations. Using these techniques with YOLOv8 and Deep OC-SORT, we implemented an initial version of a shooter tracking system capable of running on edge hardware, including both a Raspberry Pi and a Jetson Nano.</p>
</div>
<section id="Sx1" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Introduction</h2>

<div id="Sx1.p1" class="ltx_para">
<p id="Sx1.p1.1" class="ltx_p">In 2005, the Federal Bureau of Investigation (FBI) and leading criminologists defined a ‘mass shooting’ as an attack in a public place where four or more victims were killed. Using this definition, there have been at least 149 public mass shootings across the United States since 1982 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. While mass shootings are relatively rare, the impact they can have on a community, especially in the case of a school shooting, is more than enough reason to strive for improving public safety <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. However, the exact approach to improving school safety is heavily debated, often over concerns of invasion of privacy or degrading the quality of the student learning environment. On top of that, some of the common approaches, including metal detectors, armed school resource officers, and backpack searches, have shown varying effectiveness in different schools <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. A possible explanation for this is differences in implementations and resource availability. School resource officers can vary significantly from one school to another, sometimes serving a purely disciplinary role and other times serving a more supportive role <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.</p>
</div>
<div id="Sx1.p2" class="ltx_para">
<p id="Sx1.p2.1" class="ltx_p">Besides the common approaches mentioned above, there are also recent works that utilize advanced technology in detecting shooters.
One approach to shooter detection is using acoustic sensors in gunshot detection technology. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>. This type of system would simply alert law enforcement agencies when a gunshot is detected. However, this approach may have limitations in distinguishing between gunshots and other loud noises, such as fireworks or car backfires, and inaccurately capturing the exact location of a shooter moving or shooting from a distance
Without visual information, this kind of shooter detection cannot provide details such as the number of shooters or their targets. Additionally, it requires the installation of additional specialized devices, which can be a disadvantage.</p>
</div>
<div id="Sx1.p3" class="ltx_para">
<p id="Sx1.p3.1" class="ltx_p">Our approach aims to minimize invasion of privacy as it would unobtrusively leverage video from existing security cameras. Additionally, our approach would automatically detect threats instead of relying on security personnel to monitor a surveillance system. As a result, the time taken to relay information about the shooter would be reduced. This can improve the effectiveness of law enforcement responding to the scene and be used to evacuate civilians when it is safe to move from cover. While schools are often the most discussed setting for this topic, our system could also be used in any public space where there is a risk of a shooting, such as hospitals, shopping malls, and airports.</p>
</div>
<div id="Sx1.p4" class="ltx_para">
<p id="Sx1.p4.1" class="ltx_p">While there is existing work and datasets focusing on the detection of guns or other weapons <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib12" title="" class="ltx_ref">12</a>, <a href="#bib.bib13" title="" class="ltx_ref">13</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>, the tracking of shooters, however, has not been extensively explored. Detecting the entire shooter has the potential to improve tracking robustness, as their location would not be as easily lost if the vision of the gun is obstructed. The challenge with this approach is that gathering good-quality data on shooters has proven time-consuming and difficult. Most publicly available data comprises poor-quality surveillance videos, often split into short, discontinuous clips unsuitable for training. Additionally, places where such a system would reasonably be implemented would likely already have good-quality security cameras. Thus, the data used to train should be of at least a reasonable baseline quality to avoid making the already difficult task of detecting guns more challenging than it needs to be. Another common type of video source used for this task is movies; however, the camera perspectives in movies are rarely similar to those that a security camera would capture. This is important to note because the appearance of a gun can change significantly, especially in the case of handguns, where they can appear to be nothing but a rectangle, similar to a smartphone. There are also privacy concerns with conducting experiments to record videos for the dataset, both for the individuals participating and the public buildings that would be used to simulate a shooting event.</p>
</div>
<div id="Sx1.p5" class="ltx_para">
<p id="Sx1.p5.1" class="ltx_p">As a result, we have explored the use of synthetic data generated with Unreal Engine to supplement the limited availability of real data. However, a well-known limitation of model training with synthetic data (even with the semi-realistic textures) is that the model does not directly transfer well to inference on real data. To improve the efficacy of training with synthetic data, we utilize domain randomization, which is a domain adaptation technique that aims to generalize a model through training with highly variable synthetic data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Besides that, transfer learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> also allows us to use various combinations of textured images of the Unreal Engine environment, masked images with random colors, and real data by sequentially training the detection model. We also augment the textured synthetic data with camera sensor effects to further help bridge the gap between synthetic and real data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. These effects include noise, blur, chromatic aberration, exposure, and color shift, which are randomly applied with varying strengths.</p>
</div>
<div id="Sx1.p6" class="ltx_para">
<p id="Sx1.p6.1" class="ltx_p">An overview of our system can be seen in Fig. <a href="#Sx1.F1" title="Figure 1 ‣ Introduction ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. It first shows the three types of data, real, textured synthetic, and masked synthetic, that comprise our shooter dataset. Various amounts of each type of data, the specifics of which are discussed later, are used sequentially to train YOLOv8n. The best performing YOLOv8n model is used with Deep Observation-Centric (OC)-SORT to track shooters in sources such as security videos, which can then be used to enable more informed law enforcement responses.</p>
</div>
<figure id="Sx1.F1" class="ltx_figure"><img src="/html/2309.03381/assets/Figures/ASTERS_CV_Overview.png" id="Sx1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="375" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
An overview of the entire system. We train YOLOv8n using a combination of synthetic and real data. The best model is used for inference with Deep OC-SORT tracking to localize a shooter, enabling a faster and more informed response for law enforcement.</figcaption>
</figure>
<div id="Sx1.p7" class="ltx_para">
<p id="Sx1.p7.1" class="ltx_p">In terms of deployment, the use of edge devices, or small, relatively inexpensive computers, can also increase privacy by only transmitting necessary information, such as whether a threat is detected or not (binary), rather than the entire camera frames/images. On top of that, this also decreases the network bandwidth used for the system. Additionally, decentralized systems like this are generally more scalable and robust. While passing all video frames to a central server to be processed would be functionally the same, it would be more complicated to expand in the future, and the failure of the server would bring the whole system down. For example, if a building wanted to add additional cameras after the initial installation, they may be required to upgrade their entire server to handle the increased computational demand. On the other hand, since the edge devices can handle the computations in this system, they would only need additional devices to pair with the new cameras, providing a more straightforward cost estimation for expanding the system.
</p>
</div>
<div id="Sx1.p8" class="ltx_para">
<p id="Sx1.p8.1" class="ltx_p">For the task of detecting and tracking shooters in public places, we make the following contributions:
</p>
<ul id="Sx1.I1" class="ltx_itemize">
<li id="Sx1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i1.p1" class="ltx_para">
<p id="Sx1.I1.i1.p1.1" class="ltx_p">The creation of a publicly available dataset (synthetic and real) with annotations for gun and shooter classes will allow for further exploration of the detection and tracking of shooters.</p>
</div>
</li>
<li id="Sx1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i2.p1" class="ltx_para">
<p id="Sx1.I1.i2.p1.1" class="ltx_p">Development of a robust tracking system utilizing gun detection-based shooter confirmation to reduce false positives while being more likely to keep track of a threat through occlusions.</p>
</div>
</li>
<li id="Sx1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="Sx1.I1.i3.p1" class="ltx_para">
<p id="Sx1.I1.i3.p1.1" class="ltx_p">Evaluation on implementing the proposed system on edge hardware, such as a Jetson Nano, and the considerations required.</p>
</div>
</li>
</ul>
</div>
</section>
<section id="Sx2" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Results</h2>

<div id="Sx2.p1" class="ltx_para">
<p id="Sx2.p1.1" class="ltx_p">Our results are broken into four primary subsections. We first present detection performance for when You Only Look Once v8 nano (YOLOv8n) models are trained with varying combinations of real, textured synthetic, and masked synthetic data. Next, we present the tracking performance using Deep OC-SORT with Omni-Scale Network (OSNET) Re-Identification (ReID) with and without gun confirmation for shooter IDs. We also analyze the system-level performance in a more realistic context rather than just using standard metrics. Lastly, we report the system’s performance on edge devices such as a Jetson Nano and Rasberry Pi 4.
</p>
</div>
<section id="Sx2.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Detection with YOLOv8n</h3>

<figure id="Sx2.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx2.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.03381/assets/x1.png" id="Sx2.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="312" height="234" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F2.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="Sx2.F2.sf1.3.2" class="ltx_text" style="font-size:80%;">UE5</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx2.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.03381/assets/x2.png" id="Sx2.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="312" height="234" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F2.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="Sx2.F2.sf2.3.2" class="ltx_text" style="font-size:80%;">UE4 &amp; UE5</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx2.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.03381/assets/x3.png" id="Sx2.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="312" height="234" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F2.sf3.2.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="Sx2.F2.sf3.3.2" class="ltx_text" style="font-size:80%;">UE5 Augmented</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx2.F2.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.03381/assets/x4.png" id="Sx2.F2.sf4.g1" class="ltx_graphics ltx_img_landscape" width="312" height="234" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F2.sf4.2.1.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="Sx2.F2.sf4.3.2" class="ltx_text" style="font-size:80%;">UE4 &amp; UE5 Augmented</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>The detection testing results (PR curves) of different YOLOv8n models trained on different data combinations for the shooter class.</figcaption>
</figure>
<div id="Sx2.SSx1.p1" class="ltx_para">
<p id="Sx2.SSx1.p1.1" class="ltx_p">After obtaining the models by fine-tuning the pretrained YOLOv8n model on 71 different data combinations, we evaluate the performance by testing them on a dataset of 100 real images. We set the batch size to 1, the object confidence threshold for detection to 0.001, and IoU to 0.5 for the testing. The result of the shooter class detection is shown in Fig. <a href="#Sx2.F2" title="Figure 2 ‣ Detection with YOLOv8n ‣ Results ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> and the result of combine shooter and gun detection is shown in Supplementary Fig. <span class="ltx_ref ltx_ref_self">S1</span>. We can see that by combining with Unreal Engine 4 (UE4) data and with the help of augmentation, the performance is improved compared to only using Unreal Engine 5 (UE5) data for training. Precision (P), recall (R), and mean average precision (mAP) results for the shooter and gun classes can be found in Table <a href="#Sx2.T1" title="Table 1 ‣ Detection with YOLOv8n ‣ Results ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> and Supplementary Table <span class="ltx_ref ltx_ref_self">S1</span>, respectively.</p>
</div>
<figure id="Sx2.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>YOLOv8n testing results for the shooter class. Combination ID represented the order of training. The maximum amount of synthetic data was used for all sequential training scenarios, and S, M, and L amounts of real data correspond to 100, 300, and 500 images, respectively. For example, Textured_Masked_Real_M is first trained on textured synthetic images, followed by masked synthetic images, and finished with 300 real images. The column labels represent the type of data used, signifying if UE5 or both UE4 and UE5 data is used and whether the textured synthetic data is augmented with camera sensor effects.</figcaption>
<div id="Sx2.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:678.1pt;height:451pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<p id="Sx2.T1.1.1" class="ltx_p"><span id="Sx2.T1.1.1.1" class="ltx_text">

<span id="Sx2.T1.1.1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<span class="ltx_tbody">
<span id="Sx2.T1.1.1.1.1.1.1" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T1.1.1.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Combination</span></span>
<span id="Sx2.T1.1.1.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3"><span id="Sx2.T1.1.1.1.1.1.1.2.1" class="ltx_text ltx_font_bold">UE5</span></span>
<span id="Sx2.T1.1.1.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3"><span id="Sx2.T1.1.1.1.1.1.1.3.1" class="ltx_text ltx_font_bold">UE4 &amp; UE5</span></span>
<span id="Sx2.T1.1.1.1.1.1.1.4" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3"><span id="Sx2.T1.1.1.1.1.1.1.4.1" class="ltx_text ltx_font_bold">UE5 Augmented</span></span>
<span id="Sx2.T1.1.1.1.1.1.1.5" class="ltx_td ltx_align_center ltx_border_t ltx_colspan ltx_colspan_3"><span id="Sx2.T1.1.1.1.1.1.1.5.1" class="ltx_text ltx_font_bold">UE4 &amp; UE5 Augmented</span></span></span>
<span id="Sx2.T1.1.1.1.1.2.2" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.1.1" class="ltx_text ltx_font_bold">ID</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.2.1" class="ltx_text ltx_font_bold">precision</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.3.1" class="ltx_text ltx_font_bold">recall</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.4" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.4.1" class="ltx_text ltx_font_bold">mAP50</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.5" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.5.1" class="ltx_text ltx_font_bold">precision</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.6" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.6.1" class="ltx_text ltx_font_bold">recall</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.7" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.7.1" class="ltx_text ltx_font_bold">mAP50</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.8" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.8.1" class="ltx_text ltx_font_bold">precision</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.9" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.9.1" class="ltx_text ltx_font_bold">recall</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.10" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.10.1" class="ltx_text ltx_font_bold">mAP50</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.11" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.11.1" class="ltx_text ltx_font_bold">precision</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.12" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.12.1" class="ltx_text ltx_font_bold">recall</span></span>
<span id="Sx2.T1.1.1.1.1.2.2.13" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.2.2.13.1" class="ltx_text ltx_font_bold">mAP50</span></span></span>
<span id="Sx2.T1.1.1.1.1.3.3" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.3.3.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T1.1.1.1.1.3.3.1.1" class="ltx_text ltx_font_bold">Masked_L</span></span>
<span id="Sx2.T1.1.1.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_t">0.125</span>
<span id="Sx2.T1.1.1.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_t">0.13</span>
<span id="Sx2.T1.1.1.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_t">0.0843</span>
<span id="Sx2.T1.1.1.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_t">0.152</span>
<span id="Sx2.T1.1.1.1.1.3.3.6" class="ltx_td ltx_align_center ltx_border_t">0.16</span>
<span id="Sx2.T1.1.1.1.1.3.3.7" class="ltx_td ltx_align_center ltx_border_t">0.1</span>
<span id="Sx2.T1.1.1.1.1.3.3.8" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.3.3.9" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.3.3.10" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.3.3.11" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.3.3.12" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.3.3.13" class="ltx_td ltx_align_center ltx_border_t">-</span></span>
<span id="Sx2.T1.1.1.1.1.4.4" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.4.4.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.4.4.1.1" class="ltx_text ltx_font_bold">Masked_M</span></span>
<span id="Sx2.T1.1.1.1.1.4.4.2" class="ltx_td ltx_align_center">0.00749</span>
<span id="Sx2.T1.1.1.1.1.4.4.3" class="ltx_td ltx_align_center">0.01</span>
<span id="Sx2.T1.1.1.1.1.4.4.4" class="ltx_td ltx_align_center">0.000763</span>
<span id="Sx2.T1.1.1.1.1.4.4.5" class="ltx_td ltx_align_center">0.056</span>
<span id="Sx2.T1.1.1.1.1.4.4.6" class="ltx_td ltx_align_center">0.02</span>
<span id="Sx2.T1.1.1.1.1.4.4.7" class="ltx_td ltx_align_center">0.0182</span>
<span id="Sx2.T1.1.1.1.1.4.4.8" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.4.4.9" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.4.4.10" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.4.4.11" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.4.4.12" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.4.4.13" class="ltx_td ltx_align_center">-</span></span>
<span id="Sx2.T1.1.1.1.1.5.5" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.5.5.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.5.5.1.1" class="ltx_text ltx_font_bold">Masked_S</span></span>
<span id="Sx2.T1.1.1.1.1.5.5.2" class="ltx_td ltx_align_center">0.0717</span>
<span id="Sx2.T1.1.1.1.1.5.5.3" class="ltx_td ltx_align_center">0.59</span>
<span id="Sx2.T1.1.1.1.1.5.5.4" class="ltx_td ltx_align_center">0.0792</span>
<span id="Sx2.T1.1.1.1.1.5.5.5" class="ltx_td ltx_align_center">0.071</span>
<span id="Sx2.T1.1.1.1.1.5.5.6" class="ltx_td ltx_align_center">0.09</span>
<span id="Sx2.T1.1.1.1.1.5.5.7" class="ltx_td ltx_align_center">0.0387</span>
<span id="Sx2.T1.1.1.1.1.5.5.8" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.5.5.9" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.5.5.10" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.5.5.11" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.5.5.12" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.5.5.13" class="ltx_td ltx_align_center">-</span></span>
<span id="Sx2.T1.1.1.1.1.6.6" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.6.6.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T1.1.1.1.1.6.6.1.1" class="ltx_text ltx_font_bold">Textured_L</span></span>
<span id="Sx2.T1.1.1.1.1.6.6.2" class="ltx_td ltx_align_center ltx_border_t">0.00625</span>
<span id="Sx2.T1.1.1.1.1.6.6.3" class="ltx_td ltx_align_center ltx_border_t">0.84</span>
<span id="Sx2.T1.1.1.1.1.6.6.4" class="ltx_td ltx_align_center ltx_border_t">0.019</span>
<span id="Sx2.T1.1.1.1.1.6.6.5" class="ltx_td ltx_align_center ltx_border_t">0.202</span>
<span id="Sx2.T1.1.1.1.1.6.6.6" class="ltx_td ltx_align_center ltx_border_t">0.19</span>
<span id="Sx2.T1.1.1.1.1.6.6.7" class="ltx_td ltx_align_center ltx_border_t">0.0961</span>
<span id="Sx2.T1.1.1.1.1.6.6.8" class="ltx_td ltx_align_center ltx_border_t">0.411</span>
<span id="Sx2.T1.1.1.1.1.6.6.9" class="ltx_td ltx_align_center ltx_border_t">0.01</span>
<span id="Sx2.T1.1.1.1.1.6.6.10" class="ltx_td ltx_align_center ltx_border_t">0.195</span>
<span id="Sx2.T1.1.1.1.1.6.6.11" class="ltx_td ltx_align_center ltx_border_t">0.697</span>
<span id="Sx2.T1.1.1.1.1.6.6.12" class="ltx_td ltx_align_center ltx_border_t">0.07</span>
<span id="Sx2.T1.1.1.1.1.6.6.13" class="ltx_td ltx_align_center ltx_border_t">0.134</span></span>
<span id="Sx2.T1.1.1.1.1.7.7" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.7.7.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.7.7.1.1" class="ltx_text ltx_font_bold">Textured_M</span></span>
<span id="Sx2.T1.1.1.1.1.7.7.2" class="ltx_td ltx_align_center">0.0831</span>
<span id="Sx2.T1.1.1.1.1.7.7.3" class="ltx_td ltx_align_center">0.124</span>
<span id="Sx2.T1.1.1.1.1.7.7.4" class="ltx_td ltx_align_center">0.0384</span>
<span id="Sx2.T1.1.1.1.1.7.7.5" class="ltx_td ltx_align_center">0.284</span>
<span id="Sx2.T1.1.1.1.1.7.7.6" class="ltx_td ltx_align_center">0.0198</span>
<span id="Sx2.T1.1.1.1.1.7.7.7" class="ltx_td ltx_align_center">0.103</span>
<span id="Sx2.T1.1.1.1.1.7.7.8" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.7.7.8.1" class="ltx_text ltx_font_bold">0.743</span></span>
<span id="Sx2.T1.1.1.1.1.7.7.9" class="ltx_td ltx_align_center">0.0579</span>
<span id="Sx2.T1.1.1.1.1.7.7.10" class="ltx_td ltx_align_center">0.0754</span>
<span id="Sx2.T1.1.1.1.1.7.7.11" class="ltx_td ltx_align_center">0.389</span>
<span id="Sx2.T1.1.1.1.1.7.7.12" class="ltx_td ltx_align_center">0.19</span>
<span id="Sx2.T1.1.1.1.1.7.7.13" class="ltx_td ltx_align_center">0.16</span></span>
<span id="Sx2.T1.1.1.1.1.8.8" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.8.8.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.8.8.1.1" class="ltx_text ltx_font_bold">Textured_S</span></span>
<span id="Sx2.T1.1.1.1.1.8.8.2" class="ltx_td ltx_align_center">0.00836</span>
<span id="Sx2.T1.1.1.1.1.8.8.3" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.8.8.3.1" class="ltx_text ltx_font_bold">0.89</span></span>
<span id="Sx2.T1.1.1.1.1.8.8.4" class="ltx_td ltx_align_center">0.11</span>
<span id="Sx2.T1.1.1.1.1.8.8.5" class="ltx_td ltx_align_center">0.0857</span>
<span id="Sx2.T1.1.1.1.1.8.8.6" class="ltx_td ltx_align_center">0.15</span>
<span id="Sx2.T1.1.1.1.1.8.8.7" class="ltx_td ltx_align_center">0.0508</span>
<span id="Sx2.T1.1.1.1.1.8.8.8" class="ltx_td ltx_align_center">0.364</span>
<span id="Sx2.T1.1.1.1.1.8.8.9" class="ltx_td ltx_align_center">0.14</span>
<span id="Sx2.T1.1.1.1.1.8.8.10" class="ltx_td ltx_align_center">0.143</span>
<span id="Sx2.T1.1.1.1.1.8.8.11" class="ltx_td ltx_align_center">0.246</span>
<span id="Sx2.T1.1.1.1.1.8.8.12" class="ltx_td ltx_align_center">0.22</span>
<span id="Sx2.T1.1.1.1.1.8.8.13" class="ltx_td ltx_align_center">0.161</span></span>
<span id="Sx2.T1.1.1.1.1.9.9" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.9.9.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T1.1.1.1.1.9.9.1.1" class="ltx_text ltx_font_bold">Real_L</span></span>
<span id="Sx2.T1.1.1.1.1.9.9.2" class="ltx_td ltx_align_center ltx_border_t">0.732</span>
<span id="Sx2.T1.1.1.1.1.9.9.3" class="ltx_td ltx_align_center ltx_border_t">0.273</span>
<span id="Sx2.T1.1.1.1.1.9.9.4" class="ltx_td ltx_align_center ltx_border_t">0.401</span>
<span id="Sx2.T1.1.1.1.1.9.9.5" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.9.9.6" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.9.9.7" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.9.9.8" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.9.9.9" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.9.9.10" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.9.9.11" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.9.9.12" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.9.9.13" class="ltx_td ltx_align_center ltx_border_t">-</span></span>
<span id="Sx2.T1.1.1.1.1.10.10" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.10.10.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.10.10.1.1" class="ltx_text ltx_font_bold">Real_M</span></span>
<span id="Sx2.T1.1.1.1.1.10.10.2" class="ltx_td ltx_align_center">0.701</span>
<span id="Sx2.T1.1.1.1.1.10.10.3" class="ltx_td ltx_align_center">0.235</span>
<span id="Sx2.T1.1.1.1.1.10.10.4" class="ltx_td ltx_align_center">0.521</span>
<span id="Sx2.T1.1.1.1.1.10.10.5" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.10.10.6" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.10.10.7" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.10.10.8" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.10.10.9" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.10.10.10" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.10.10.11" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.10.10.12" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.10.10.13" class="ltx_td ltx_align_center">-</span></span>
<span id="Sx2.T1.1.1.1.1.11.11" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.11.11.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.11.11.1.1" class="ltx_text ltx_font_bold">Real_S</span></span>
<span id="Sx2.T1.1.1.1.1.11.11.2" class="ltx_td ltx_align_center">0.00667</span>
<span id="Sx2.T1.1.1.1.1.11.11.3" class="ltx_td ltx_align_center">1</span>
<span id="Sx2.T1.1.1.1.1.11.11.4" class="ltx_td ltx_align_center">0.483</span>
<span id="Sx2.T1.1.1.1.1.11.11.5" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.11.11.6" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.11.11.7" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.11.11.8" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.11.11.9" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.11.11.10" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.11.11.11" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.11.11.12" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.11.11.13" class="ltx_td ltx_align_center">-</span></span>
<span id="Sx2.T1.1.1.1.1.12.12" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.12.12.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T1.1.1.1.1.12.12.1.1" class="ltx_text ltx_font_bold">Masked_Real_L</span></span>
<span id="Sx2.T1.1.1.1.1.12.12.2" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.12.12.2.1" class="ltx_text ltx_font_bold">0.785</span></span>
<span id="Sx2.T1.1.1.1.1.12.12.3" class="ltx_td ltx_align_center ltx_border_t">0.28</span>
<span id="Sx2.T1.1.1.1.1.12.12.4" class="ltx_td ltx_align_center ltx_border_t">0.35</span>
<span id="Sx2.T1.1.1.1.1.12.12.5" class="ltx_td ltx_align_center ltx_border_t">0.304</span>
<span id="Sx2.T1.1.1.1.1.12.12.6" class="ltx_td ltx_align_center ltx_border_t">0.4</span>
<span id="Sx2.T1.1.1.1.1.12.12.7" class="ltx_td ltx_align_center ltx_border_t">0.326</span>
<span id="Sx2.T1.1.1.1.1.12.12.8" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.12.12.9" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.12.12.10" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.12.12.11" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.12.12.12" class="ltx_td ltx_align_center ltx_border_t">-</span>
<span id="Sx2.T1.1.1.1.1.12.12.13" class="ltx_td ltx_align_center ltx_border_t">-</span></span>
<span id="Sx2.T1.1.1.1.1.13.13" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.13.13.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.13.13.1.1" class="ltx_text ltx_font_bold">Masked_Real_M</span></span>
<span id="Sx2.T1.1.1.1.1.13.13.2" class="ltx_td ltx_align_center">0.536</span>
<span id="Sx2.T1.1.1.1.1.13.13.3" class="ltx_td ltx_align_center">0.255</span>
<span id="Sx2.T1.1.1.1.1.13.13.4" class="ltx_td ltx_align_center">0.325</span>
<span id="Sx2.T1.1.1.1.1.13.13.5" class="ltx_td ltx_align_center">0.631</span>
<span id="Sx2.T1.1.1.1.1.13.13.6" class="ltx_td ltx_align_center">0.36</span>
<span id="Sx2.T1.1.1.1.1.13.13.7" class="ltx_td ltx_align_center">0.445</span>
<span id="Sx2.T1.1.1.1.1.13.13.8" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.13.13.9" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.13.13.10" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.13.13.11" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.13.13.12" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.13.13.13" class="ltx_td ltx_align_center">-</span></span>
<span id="Sx2.T1.1.1.1.1.14.14" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.14.14.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.14.14.1.1" class="ltx_text ltx_font_bold">Masked_Real_S</span></span>
<span id="Sx2.T1.1.1.1.1.14.14.2" class="ltx_td ltx_align_center">0.477</span>
<span id="Sx2.T1.1.1.1.1.14.14.3" class="ltx_td ltx_align_center">0.484</span>
<span id="Sx2.T1.1.1.1.1.14.14.4" class="ltx_td ltx_align_center">0.43</span>
<span id="Sx2.T1.1.1.1.1.14.14.5" class="ltx_td ltx_align_center">0.579</span>
<span id="Sx2.T1.1.1.1.1.14.14.6" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.14.14.6.1" class="ltx_text ltx_font_bold">0.62</span></span>
<span id="Sx2.T1.1.1.1.1.14.14.7" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.14.14.7.1" class="ltx_text ltx_font_bold">0.579</span></span>
<span id="Sx2.T1.1.1.1.1.14.14.8" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.14.14.9" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.14.14.10" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.14.14.11" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.14.14.12" class="ltx_td ltx_align_center">-</span>
<span id="Sx2.T1.1.1.1.1.14.14.13" class="ltx_td ltx_align_center">-</span></span>
<span id="Sx2.T1.1.1.1.1.15.15" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.15.15.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T1.1.1.1.1.15.15.1.1" class="ltx_text ltx_font_bold">Textured_Real_L</span></span>
<span id="Sx2.T1.1.1.1.1.15.15.2" class="ltx_td ltx_align_center ltx_border_t">0.524</span>
<span id="Sx2.T1.1.1.1.1.15.15.3" class="ltx_td ltx_align_center ltx_border_t">0.23</span>
<span id="Sx2.T1.1.1.1.1.15.15.4" class="ltx_td ltx_align_center ltx_border_t">0.316</span>
<span id="Sx2.T1.1.1.1.1.15.15.5" class="ltx_td ltx_align_center ltx_border_t"><span id="Sx2.T1.1.1.1.1.15.15.5.1" class="ltx_text ltx_font_bold">0.713</span></span>
<span id="Sx2.T1.1.1.1.1.15.15.6" class="ltx_td ltx_align_center ltx_border_t">0.273</span>
<span id="Sx2.T1.1.1.1.1.15.15.7" class="ltx_td ltx_align_center ltx_border_t">0.391</span>
<span id="Sx2.T1.1.1.1.1.15.15.8" class="ltx_td ltx_align_center ltx_border_t">0.714</span>
<span id="Sx2.T1.1.1.1.1.15.15.9" class="ltx_td ltx_align_center ltx_border_t">0.249</span>
<span id="Sx2.T1.1.1.1.1.15.15.10" class="ltx_td ltx_align_center ltx_border_t">0.331</span>
<span id="Sx2.T1.1.1.1.1.15.15.11" class="ltx_td ltx_align_center ltx_border_t">0.687</span>
<span id="Sx2.T1.1.1.1.1.15.15.12" class="ltx_td ltx_align_center ltx_border_t">0.28</span>
<span id="Sx2.T1.1.1.1.1.15.15.13" class="ltx_td ltx_align_center ltx_border_t">0.336</span></span>
<span id="Sx2.T1.1.1.1.1.16.16" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.16.16.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.16.16.1.1" class="ltx_text ltx_font_bold">Textured_Real_M</span></span>
<span id="Sx2.T1.1.1.1.1.16.16.2" class="ltx_td ltx_align_center">0.771</span>
<span id="Sx2.T1.1.1.1.1.16.16.3" class="ltx_td ltx_align_center">0.201</span>
<span id="Sx2.T1.1.1.1.1.16.16.4" class="ltx_td ltx_align_center">0.312</span>
<span id="Sx2.T1.1.1.1.1.16.16.5" class="ltx_td ltx_align_center">0.533</span>
<span id="Sx2.T1.1.1.1.1.16.16.6" class="ltx_td ltx_align_center">0.31</span>
<span id="Sx2.T1.1.1.1.1.16.16.7" class="ltx_td ltx_align_center">0.34</span>
<span id="Sx2.T1.1.1.1.1.16.16.8" class="ltx_td ltx_align_center">0.509</span>
<span id="Sx2.T1.1.1.1.1.16.16.9" class="ltx_td ltx_align_center">0.331</span>
<span id="Sx2.T1.1.1.1.1.16.16.10" class="ltx_td ltx_align_center">0.323</span>
<span id="Sx2.T1.1.1.1.1.16.16.11" class="ltx_td ltx_align_center">0.784</span>
<span id="Sx2.T1.1.1.1.1.16.16.12" class="ltx_td ltx_align_center">0.27</span>
<span id="Sx2.T1.1.1.1.1.16.16.13" class="ltx_td ltx_align_center">0.43</span></span>
<span id="Sx2.T1.1.1.1.1.17.17" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.17.17.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.17.17.1.1" class="ltx_text ltx_font_bold">Textured_Real_S</span></span>
<span id="Sx2.T1.1.1.1.1.17.17.2" class="ltx_td ltx_align_center">0.557</span>
<span id="Sx2.T1.1.1.1.1.17.17.3" class="ltx_td ltx_align_center">0.315</span>
<span id="Sx2.T1.1.1.1.1.17.17.4" class="ltx_td ltx_align_center">0.311</span>
<span id="Sx2.T1.1.1.1.1.17.17.5" class="ltx_td ltx_align_center">0.474</span>
<span id="Sx2.T1.1.1.1.1.17.17.6" class="ltx_td ltx_align_center">0.23</span>
<span id="Sx2.T1.1.1.1.1.17.17.7" class="ltx_td ltx_align_center">0.24</span>
<span id="Sx2.T1.1.1.1.1.17.17.8" class="ltx_td ltx_align_center">0.468</span>
<span id="Sx2.T1.1.1.1.1.17.17.9" class="ltx_td ltx_align_center">0.42</span>
<span id="Sx2.T1.1.1.1.1.17.17.10" class="ltx_td ltx_align_center">0.405</span>
<span id="Sx2.T1.1.1.1.1.17.17.11" class="ltx_td ltx_align_center">0.522</span>
<span id="Sx2.T1.1.1.1.1.17.17.12" class="ltx_td ltx_align_center">0.26</span>
<span id="Sx2.T1.1.1.1.1.17.17.13" class="ltx_td ltx_align_center">0.271</span></span>
<span id="Sx2.T1.1.1.1.1.18.18" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.18.18.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T1.1.1.1.1.18.18.1.1" class="ltx_text ltx_font_bold">Masked_Textured</span></span>
<span id="Sx2.T1.1.1.1.1.18.18.2" class="ltx_td ltx_align_center ltx_border_t">0.0835</span>
<span id="Sx2.T1.1.1.1.1.18.18.3" class="ltx_td ltx_align_center ltx_border_t">0.06</span>
<span id="Sx2.T1.1.1.1.1.18.18.4" class="ltx_td ltx_align_center ltx_border_t">0.0507</span>
<span id="Sx2.T1.1.1.1.1.18.18.5" class="ltx_td ltx_align_center ltx_border_t">0.134</span>
<span id="Sx2.T1.1.1.1.1.18.18.6" class="ltx_td ltx_align_center ltx_border_t">0.06</span>
<span id="Sx2.T1.1.1.1.1.18.18.7" class="ltx_td ltx_align_center ltx_border_t">0.0944</span>
<span id="Sx2.T1.1.1.1.1.18.18.8" class="ltx_td ltx_align_center ltx_border_t">0.103</span>
<span id="Sx2.T1.1.1.1.1.18.18.9" class="ltx_td ltx_align_center ltx_border_t">0.13</span>
<span id="Sx2.T1.1.1.1.1.18.18.10" class="ltx_td ltx_align_center ltx_border_t">0.074</span>
<span id="Sx2.T1.1.1.1.1.18.18.11" class="ltx_td ltx_align_center ltx_border_t">0.23</span>
<span id="Sx2.T1.1.1.1.1.18.18.12" class="ltx_td ltx_align_center ltx_border_t">0.377</span>
<span id="Sx2.T1.1.1.1.1.18.18.13" class="ltx_td ltx_align_center ltx_border_t">0.201</span></span>
<span id="Sx2.T1.1.1.1.1.19.19" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.19.19.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.19.19.1.1" class="ltx_text ltx_font_bold">Masked_Textured_Real_L</span></span>
<span id="Sx2.T1.1.1.1.1.19.19.2" class="ltx_td ltx_align_center">0.485</span>
<span id="Sx2.T1.1.1.1.1.19.19.3" class="ltx_td ltx_align_center">0.565</span>
<span id="Sx2.T1.1.1.1.1.19.19.4" class="ltx_td ltx_align_center">0.476</span>
<span id="Sx2.T1.1.1.1.1.19.19.5" class="ltx_td ltx_align_center">0.667</span>
<span id="Sx2.T1.1.1.1.1.19.19.6" class="ltx_td ltx_align_center">0.23</span>
<span id="Sx2.T1.1.1.1.1.19.19.7" class="ltx_td ltx_align_center">0.332</span>
<span id="Sx2.T1.1.1.1.1.19.19.8" class="ltx_td ltx_align_center">0.659</span>
<span id="Sx2.T1.1.1.1.1.19.19.9" class="ltx_td ltx_align_center">0.27</span>
<span id="Sx2.T1.1.1.1.1.19.19.10" class="ltx_td ltx_align_center">0.342</span>
<span id="Sx2.T1.1.1.1.1.19.19.11" class="ltx_td ltx_align_center">0.355</span>
<span id="Sx2.T1.1.1.1.1.19.19.12" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.19.19.12.1" class="ltx_text ltx_font_bold">0.65</span></span>
<span id="Sx2.T1.1.1.1.1.19.19.13" class="ltx_td ltx_align_center">0.517</span></span>
<span id="Sx2.T1.1.1.1.1.20.20" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.20.20.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.20.20.1.1" class="ltx_text ltx_font_bold">Masked_Textured_Real_M</span></span>
<span id="Sx2.T1.1.1.1.1.20.20.2" class="ltx_td ltx_align_center">0.664</span>
<span id="Sx2.T1.1.1.1.1.20.20.3" class="ltx_td ltx_align_center">0.27</span>
<span id="Sx2.T1.1.1.1.1.20.20.4" class="ltx_td ltx_align_center">0.361</span>
<span id="Sx2.T1.1.1.1.1.20.20.5" class="ltx_td ltx_align_center">0.595</span>
<span id="Sx2.T1.1.1.1.1.20.20.6" class="ltx_td ltx_align_center">0.35</span>
<span id="Sx2.T1.1.1.1.1.20.20.7" class="ltx_td ltx_align_center">0.395</span>
<span id="Sx2.T1.1.1.1.1.20.20.8" class="ltx_td ltx_align_center">0.69</span>
<span id="Sx2.T1.1.1.1.1.20.20.9" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.20.20.9.1" class="ltx_text ltx_font_bold">0.378</span></span>
<span id="Sx2.T1.1.1.1.1.20.20.10" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.20.20.10.1" class="ltx_text ltx_font_bold">0.44</span></span>
<span id="Sx2.T1.1.1.1.1.20.20.11" class="ltx_td ltx_align_center">0.555</span>
<span id="Sx2.T1.1.1.1.1.20.20.12" class="ltx_td ltx_align_center">0.41</span>
<span id="Sx2.T1.1.1.1.1.20.20.13" class="ltx_td ltx_align_center">0.421</span></span>
<span id="Sx2.T1.1.1.1.1.21.21" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.21.21.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.21.21.1.1" class="ltx_text ltx_font_bold">Masked_Textured_Real_S</span></span>
<span id="Sx2.T1.1.1.1.1.21.21.2" class="ltx_td ltx_align_center">0.501</span>
<span id="Sx2.T1.1.1.1.1.21.21.3" class="ltx_td ltx_align_center">0.43</span>
<span id="Sx2.T1.1.1.1.1.21.21.4" class="ltx_td ltx_align_center">0.379</span>
<span id="Sx2.T1.1.1.1.1.21.21.5" class="ltx_td ltx_align_center">0.486</span>
<span id="Sx2.T1.1.1.1.1.21.21.6" class="ltx_td ltx_align_center">0.26</span>
<span id="Sx2.T1.1.1.1.1.21.21.7" class="ltx_td ltx_align_center">0.283</span>
<span id="Sx2.T1.1.1.1.1.21.21.8" class="ltx_td ltx_align_center">0.391</span>
<span id="Sx2.T1.1.1.1.1.21.21.9" class="ltx_td ltx_align_center">0.36</span>
<span id="Sx2.T1.1.1.1.1.21.21.10" class="ltx_td ltx_align_center">0.332</span>
<span id="Sx2.T1.1.1.1.1.21.21.11" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.21.21.11.1" class="ltx_text ltx_font_bold">0.803</span></span>
<span id="Sx2.T1.1.1.1.1.21.21.12" class="ltx_td ltx_align_center">0.204</span>
<span id="Sx2.T1.1.1.1.1.21.21.13" class="ltx_td ltx_align_center">0.311</span></span>
<span id="Sx2.T1.1.1.1.1.22.22" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.22.22.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_t"><span id="Sx2.T1.1.1.1.1.22.22.1.1" class="ltx_text ltx_font_bold">Textured_Masked</span></span>
<span id="Sx2.T1.1.1.1.1.22.22.2" class="ltx_td ltx_align_center ltx_border_t">0.0945</span>
<span id="Sx2.T1.1.1.1.1.22.22.3" class="ltx_td ltx_align_center ltx_border_t">0.06</span>
<span id="Sx2.T1.1.1.1.1.22.22.4" class="ltx_td ltx_align_center ltx_border_t">0.0454</span>
<span id="Sx2.T1.1.1.1.1.22.22.5" class="ltx_td ltx_align_center ltx_border_t">0.0533</span>
<span id="Sx2.T1.1.1.1.1.22.22.6" class="ltx_td ltx_align_center ltx_border_t">0.09</span>
<span id="Sx2.T1.1.1.1.1.22.22.7" class="ltx_td ltx_align_center ltx_border_t">0.034</span>
<span id="Sx2.T1.1.1.1.1.22.22.8" class="ltx_td ltx_align_center ltx_border_t">0.174</span>
<span id="Sx2.T1.1.1.1.1.22.22.9" class="ltx_td ltx_align_center ltx_border_t">0.19</span>
<span id="Sx2.T1.1.1.1.1.22.22.10" class="ltx_td ltx_align_center ltx_border_t">0.0958</span>
<span id="Sx2.T1.1.1.1.1.22.22.11" class="ltx_td ltx_align_center ltx_border_t">0.0398</span>
<span id="Sx2.T1.1.1.1.1.22.22.12" class="ltx_td ltx_align_center ltx_border_t">0.09</span>
<span id="Sx2.T1.1.1.1.1.22.22.13" class="ltx_td ltx_align_center ltx_border_t">0.0121</span></span>
<span id="Sx2.T1.1.1.1.1.23.23" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.23.23.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.23.23.1.1" class="ltx_text ltx_font_bold">Textured_Masked_Real_L</span></span>
<span id="Sx2.T1.1.1.1.1.23.23.2" class="ltx_td ltx_align_center">0.717</span>
<span id="Sx2.T1.1.1.1.1.23.23.3" class="ltx_td ltx_align_center">0.228</span>
<span id="Sx2.T1.1.1.1.1.23.23.4" class="ltx_td ltx_align_center">0.306</span>
<span id="Sx2.T1.1.1.1.1.23.23.5" class="ltx_td ltx_align_center">0.526</span>
<span id="Sx2.T1.1.1.1.1.23.23.6" class="ltx_td ltx_align_center">0.432</span>
<span id="Sx2.T1.1.1.1.1.23.23.7" class="ltx_td ltx_align_center">0.419</span>
<span id="Sx2.T1.1.1.1.1.23.23.8" class="ltx_td ltx_align_center">0.419</span>
<span id="Sx2.T1.1.1.1.1.23.23.9" class="ltx_td ltx_align_center">0.23</span>
<span id="Sx2.T1.1.1.1.1.23.23.10" class="ltx_td ltx_align_center">0.259</span>
<span id="Sx2.T1.1.1.1.1.23.23.11" class="ltx_td ltx_align_center">0.769</span>
<span id="Sx2.T1.1.1.1.1.23.23.12" class="ltx_td ltx_align_center">0.31</span>
<span id="Sx2.T1.1.1.1.1.23.23.13" class="ltx_td ltx_align_center">0.37</span></span>
<span id="Sx2.T1.1.1.1.1.24.24" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.24.24.1" class="ltx_td ltx_align_right ltx_th ltx_th_row"><span id="Sx2.T1.1.1.1.1.24.24.1.1" class="ltx_text ltx_font_bold">Textured_Masked_Real_M</span></span>
<span id="Sx2.T1.1.1.1.1.24.24.2" class="ltx_td ltx_align_center">0.565</span>
<span id="Sx2.T1.1.1.1.1.24.24.3" class="ltx_td ltx_align_center">0.689</span>
<span id="Sx2.T1.1.1.1.1.24.24.4" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.24.24.4.1" class="ltx_text ltx_font_bold">0.606</span></span>
<span id="Sx2.T1.1.1.1.1.24.24.5" class="ltx_td ltx_align_center">0.543</span>
<span id="Sx2.T1.1.1.1.1.24.24.6" class="ltx_td ltx_align_center">0.36</span>
<span id="Sx2.T1.1.1.1.1.24.24.7" class="ltx_td ltx_align_center">0.361</span>
<span id="Sx2.T1.1.1.1.1.24.24.8" class="ltx_td ltx_align_center">0.636</span>
<span id="Sx2.T1.1.1.1.1.24.24.9" class="ltx_td ltx_align_center">0.24</span>
<span id="Sx2.T1.1.1.1.1.24.24.10" class="ltx_td ltx_align_center">0.314</span>
<span id="Sx2.T1.1.1.1.1.24.24.11" class="ltx_td ltx_align_center"><span id="Sx2.T1.1.1.1.1.24.24.11.1" class="ltx_text ltx_font_bold">0.803</span></span>
<span id="Sx2.T1.1.1.1.1.24.24.12" class="ltx_td ltx_align_center">0.3</span>
<span id="Sx2.T1.1.1.1.1.24.24.13" class="ltx_td ltx_align_center">0.432</span></span>
<span id="Sx2.T1.1.1.1.1.25.25" class="ltx_tr">
<span id="Sx2.T1.1.1.1.1.25.25.1" class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_b"><span id="Sx2.T1.1.1.1.1.25.25.1.1" class="ltx_text ltx_font_bold">Textured_Masked_Real_S</span></span>
<span id="Sx2.T1.1.1.1.1.25.25.2" class="ltx_td ltx_align_center ltx_border_b">0.389</span>
<span id="Sx2.T1.1.1.1.1.25.25.3" class="ltx_td ltx_align_center ltx_border_b">0.6</span>
<span id="Sx2.T1.1.1.1.1.25.25.4" class="ltx_td ltx_align_center ltx_border_b">0.443</span>
<span id="Sx2.T1.1.1.1.1.25.25.5" class="ltx_td ltx_align_center ltx_border_b">0.468</span>
<span id="Sx2.T1.1.1.1.1.25.25.6" class="ltx_td ltx_align_center ltx_border_b">0.2</span>
<span id="Sx2.T1.1.1.1.1.25.25.7" class="ltx_td ltx_align_center ltx_border_b">0.265</span>
<span id="Sx2.T1.1.1.1.1.25.25.8" class="ltx_td ltx_align_center ltx_border_b">0.513</span>
<span id="Sx2.T1.1.1.1.1.25.25.9" class="ltx_td ltx_align_center ltx_border_b">0.27</span>
<span id="Sx2.T1.1.1.1.1.25.25.10" class="ltx_td ltx_align_center ltx_border_b">0.331</span>
<span id="Sx2.T1.1.1.1.1.25.25.11" class="ltx_td ltx_align_center ltx_border_b">0.55</span>
<span id="Sx2.T1.1.1.1.1.25.25.12" class="ltx_td ltx_align_center ltx_border_b">0.49</span>
<span id="Sx2.T1.1.1.1.1.25.25.13" class="ltx_td ltx_align_center ltx_border_b"><span id="Sx2.T1.1.1.1.1.25.25.13.1" class="ltx_text ltx_font_bold">0.522</span></span></span>
</span>
</span></span></p>
</span></div>
</figure>
</section>
<section id="Sx2.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Tracking with Deep OC-SORT and OSNET ReID</h3>

<div id="Sx2.SSx2.p1" class="ltx_para">
<p id="Sx2.SSx2.p1.1" class="ltx_p">Evaluating tracking performance is much more tedious for custom data than detection performance. Rather than just frames, entire videos must be annotated frame by frame with consistent IDs throughout. Our preliminary tracking evaluation is limited to six real videos we annotated. Rather than only include videos with at least one shooter in it, we also included a video of a busy mall to challenge the false positive rate of the models. All 71 detection models were tried with individually varying confidence thresholds for the gun and shooter classes. Additionally, we ran each configuration for both tracking a shooter alone and tracking a shooter with our gun-based confirmation. The overall best-performing data combination combines UE4 and UE5 data with augmented textured synthetic data, shown in Fig. <a href="#Sx2.F3" title="Figure 3 ‣ Tracking with Deep OC-SORT and OSNET ReID ‣ Results ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. Results for the other data types can be seen in Supplementary Fig. <span class="ltx_ref ltx_ref_self">S2</span>.</p>
</div>
<figure id="Sx2.F3" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx2.F3.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.03381/assets/x5.png" id="Sx2.F3.sf1.g1" class="ltx_graphics ltx_img_landscape" width="312" height="189" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F3.sf1.2.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="Sx2.F3.sf1.3.2" class="ltx_text" style="font-size:80%;">Shooter Only</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="Sx2.F3.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2309.03381/assets/x6.png" id="Sx2.F3.sf2.g1" class="ltx_graphics ltx_img_landscape" width="312" height="189" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="Sx2.F3.sf2.2.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="Sx2.F3.sf2.3.2" class="ltx_text" style="font-size:80%;">Shooter Confirmed with Gun</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The testing results (PR curves) for (a) tracking with only shooter detections and (b) tracking with shooter detections confirmed with a gun detection. Confidence thresholds for the gun and shooter detections were varied individually from 0.1 to 0.9. Only combined augmented models are shown here, the other data combinations are shown in Supplementary Fig. S2.</figcaption>
</figure>
<div id="Sx2.SSx2.p2" class="ltx_para">
<p id="Sx2.SSx2.p2.1" class="ltx_p">The tracking results for the <span id="Sx2.SSx2.p2.1.1" class="ltx_text ltx_font_italic">AugCTextured_CMasked_Real_S</span>, with a gun confidence threshold of 0.8 and a shooter confidence threshold of 0.6, can be seen in Table <a href="#Sx2.T2" title="Table 2 ‣ Tracking with Deep OC-SORT and OSNET ReID ‣ Results ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. This table includes many of the standard multi-object tracking metrics, such as ID F1 score (IDf1), precision (IDP), and recall (IDR), regular recall (R) and precision (P), true positives (TP), false positives (FP), false negatives (FN), ID switches (IDSW), and multi-object tracking accuracy (MOTA). Metrics with an ID prefix correspond to the performance of maintaining the correct IDs.</p>
</div>
<figure id="Sx2.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Tracking results using the YOLOv8n model, <span id="Sx2.T2.2.1" class="ltx_text ltx_font_italic">AugCTextured_CMasked_Real_S</span>, trained with 2,545 augmented textured synthetic images from UE4 and UE5, followed by 12,698 masked synthetic images from UE4 and UE5, and finished with 100 real images. A gun confidence threshold of 0.8 and a shooter confidence threshold of 0.6 was used for these results.</figcaption>
<div id="Sx2.T2.3" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:325.2pt;height:30.9pt;vertical-align:-1.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-37.3pt,3.4pt) scale(0.813544466491875,0.813544466491875) ;">
<table id="Sx2.T2.3.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="Sx2.T2.3.1.1.1" class="ltx_tr">
<th id="Sx2.T2.3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.1.1" class="ltx_text ltx_font_bold">IDF1</span></th>
<th id="Sx2.T2.3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.2.1" class="ltx_text ltx_font_bold">IDP</span></th>
<th id="Sx2.T2.3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.3.1" class="ltx_text ltx_font_bold">IDR</span></th>
<th id="Sx2.T2.3.1.1.1.4" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.4.1" class="ltx_text ltx_font_bold">R</span></th>
<th id="Sx2.T2.3.1.1.1.5" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.5.1" class="ltx_text ltx_font_bold">P</span></th>
<th id="Sx2.T2.3.1.1.1.6" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.6.1" class="ltx_text ltx_font_bold">TP</span></th>
<th id="Sx2.T2.3.1.1.1.7" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.7.1" class="ltx_text ltx_font_bold">FP</span></th>
<th id="Sx2.T2.3.1.1.1.8" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.8.1" class="ltx_text ltx_font_bold">FN</span></th>
<th id="Sx2.T2.3.1.1.1.9" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.9.1" class="ltx_text ltx_font_bold">IDSW</span></th>
<th id="Sx2.T2.3.1.1.1.10" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t"><span id="Sx2.T2.3.1.1.1.10.1" class="ltx_text ltx_font_bold">MOTA</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="Sx2.T2.3.1.2.1" class="ltx_tr">
<td id="Sx2.T2.3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t">0.302</td>
<td id="Sx2.T2.3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.443</td>
<td id="Sx2.T2.3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.229</td>
<td id="Sx2.T2.3.1.2.1.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.349</td>
<td id="Sx2.T2.3.1.2.1.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.674</td>
<td id="Sx2.T2.3.1.2.1.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">10</td>
<td id="Sx2.T2.3.1.2.1.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">432</td>
<td id="Sx2.T2.3.1.2.1.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1,673</td>
<td id="Sx2.T2.3.1.2.1.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">24</td>
<td id="Sx2.T2.3.1.2.1.10" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">0.171</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section id="Sx2.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">System-level performance</h3>

<div id="Sx2.SSx3.p1" class="ltx_para">
<p id="Sx2.SSx3.p1.1" class="ltx_p">Standard performance metrics alone do not comprehensively measure the system’s performance in real-world use. To account for this, we also consider varying windows of frames around the ground truth where a bounding box would be considered. The intuition behind this is that while a constant track may not be achievable, consistent updates can provide valuable information. The number of frames in the window varies from 1 to 60, where the videos are 30 or 50 frames per second.</p>
</div>
<figure id="Sx2.F4" class="ltx_figure"><img src="/html/2309.03381/assets/Figures/Tracking/p_r_f1_vs_time_window_both.png" id="Sx2.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="449" height="269" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>System-level performance (precision, recall, and F1 score) of the AugCTextured_CMasked_Real_S model with a varying window of frames to consider for bounding box matches and with and without gun-based shooter confirmation.</figcaption>
</figure>
</section>
<section id="Sx2.SSx4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Edge-device performance</h3>

<div id="Sx2.SSx4.p1" class="ltx_para">
<p id="Sx2.SSx4.p1.2" class="ltx_p">For this system to be useful, it must be able to run at high enough fps to capture quick movement through a sometimes relatively narrow field of view, such as running across a hallway. We measured the time required for each component’s computation on a Raspberry Pi 4 (RPi4) with 4GB of RAM and Jetson Nano. The inferencing time for YOLOv8n and Deep OC-SORT are also tabulated in Table <a href="#Sx2.T3" title="Table 3 ‣ Edge-device performance ‣ Results ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>. For both of these devices, a consumer <math id="Sx2.SSx4.p1.1.m1.1" class="ltx_Math" alttext="1920\times 1080" display="inline"><semantics id="Sx2.SSx4.p1.1.m1.1a"><mrow id="Sx2.SSx4.p1.1.m1.1.1" xref="Sx2.SSx4.p1.1.m1.1.1.cmml"><mn id="Sx2.SSx4.p1.1.m1.1.1.2" xref="Sx2.SSx4.p1.1.m1.1.1.2.cmml">1920</mn><mo lspace="0.222em" rspace="0.222em" id="Sx2.SSx4.p1.1.m1.1.1.1" xref="Sx2.SSx4.p1.1.m1.1.1.1.cmml">×</mo><mn id="Sx2.SSx4.p1.1.m1.1.1.3" xref="Sx2.SSx4.p1.1.m1.1.1.3.cmml">1080</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.1.m1.1b"><apply id="Sx2.SSx4.p1.1.m1.1.1.cmml" xref="Sx2.SSx4.p1.1.m1.1.1"><times id="Sx2.SSx4.p1.1.m1.1.1.1.cmml" xref="Sx2.SSx4.p1.1.m1.1.1.1"></times><cn type="integer" id="Sx2.SSx4.p1.1.m1.1.1.2.cmml" xref="Sx2.SSx4.p1.1.m1.1.1.2">1920</cn><cn type="integer" id="Sx2.SSx4.p1.1.m1.1.1.3.cmml" xref="Sx2.SSx4.p1.1.m1.1.1.3">1080</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.1.m1.1c">1920\times 1080</annotation></semantics></math> webcam was used as the video source, which was then padded and resized to <math id="Sx2.SSx4.p1.2.m2.1" class="ltx_Math" alttext="640\times 640" display="inline"><semantics id="Sx2.SSx4.p1.2.m2.1a"><mrow id="Sx2.SSx4.p1.2.m2.1.1" xref="Sx2.SSx4.p1.2.m2.1.1.cmml"><mn id="Sx2.SSx4.p1.2.m2.1.1.2" xref="Sx2.SSx4.p1.2.m2.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="Sx2.SSx4.p1.2.m2.1.1.1" xref="Sx2.SSx4.p1.2.m2.1.1.1.cmml">×</mo><mn id="Sx2.SSx4.p1.2.m2.1.1.3" xref="Sx2.SSx4.p1.2.m2.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx2.SSx4.p1.2.m2.1b"><apply id="Sx2.SSx4.p1.2.m2.1.1.cmml" xref="Sx2.SSx4.p1.2.m2.1.1"><times id="Sx2.SSx4.p1.2.m2.1.1.1.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.1"></times><cn type="integer" id="Sx2.SSx4.p1.2.m2.1.1.2.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.2">640</cn><cn type="integer" id="Sx2.SSx4.p1.2.m2.1.1.3.cmml" xref="Sx2.SSx4.p1.2.m2.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx2.SSx4.p1.2.m2.1c">640\times 640</annotation></semantics></math> before being processed by YOLOv8 and Deep OC-SORT.</p>
</div>
<figure id="Sx2.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>The inference speed results for running detection and tracking on the selected edge devices.</figcaption>
<div id="Sx2.T3.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:260.2pt;height:66.2pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-11.3pt,2.9pt) scale(0.920066060322295,0.920066060322295) ;">
<table id="Sx2.T3.1.1" class="ltx_tabular ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="Sx2.T3.1.1.1.1" class="ltx_tr">
<td id="Sx2.T3.1.1.1.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="Sx2.T3.1.1.1.1.1.1" class="ltx_text ltx_font_bold">Device</span></td>
<td id="Sx2.T3.1.1.1.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="3"><span id="Sx2.T3.1.1.1.1.2.1" class="ltx_text ltx_font_bold">Computation Time per Image (ms)</span></td>
<td id="Sx2.T3.1.1.1.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t" rowspan="2"><span id="Sx2.T3.1.1.1.1.3.1" class="ltx_text ltx_font_bold">Total (FPS)</span></td>
</tr>
<tr id="Sx2.T3.1.1.2.2" class="ltx_tr">
<td id="Sx2.T3.1.1.2.2.1" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx2.T3.1.1.2.2.1.1" class="ltx_text ltx_font_bold">Detection</span></td>
<td id="Sx2.T3.1.1.2.2.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx2.T3.1.1.2.2.2.1" class="ltx_text ltx_font_bold">Tracking</span></td>
<td id="Sx2.T3.1.1.2.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="Sx2.T3.1.1.2.2.3.1" class="ltx_text ltx_font_bold">Total</span></td>
</tr>
<tr id="Sx2.T3.1.1.3.3" class="ltx_tr">
<td id="Sx2.T3.1.1.3.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t"><span id="Sx2.T3.1.1.3.3.1.1" class="ltx_text ltx_font_bold">Raspberry Pi 4</span></td>
<td id="Sx2.T3.1.1.3.3.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">650</td>
<td id="Sx2.T3.1.1.3.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">250</td>
<td id="Sx2.T3.1.1.3.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">900</td>
<td id="Sx2.T3.1.1.3.3.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1.11</td>
</tr>
<tr id="Sx2.T3.1.1.4.4" class="ltx_tr">
<td id="Sx2.T3.1.1.4.4.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="Sx2.T3.1.1.4.4.1.1" class="ltx_text ltx_font_bold">Jetson Nano</span></td>
<td id="Sx2.T3.1.1.4.4.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">125</td>
<td id="Sx2.T3.1.1.4.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">55</td>
<td id="Sx2.T3.1.1.4.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">180</td>
<td id="Sx2.T3.1.1.4.4.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">5.56</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section id="Sx3" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Discussion</h2>

<div id="Sx3.p1" class="ltx_para">
<p id="Sx3.p1.1" class="ltx_p">After training 71 YOLOv8n models with the various data combinations, a few trends are noticeable. First of all, gun detection performance is inferior compared to shooter detection performance. There are a few potential causes, such as the task simply being more difficult than detecting shooters due to the smaller size of guns. It may also be due to not strict enough filtering of the synthetic data using a bounding box size threshold. Removing some of the very small instances of gun labels in the training set may allow the models to learn more effectively. Another approach would be to merge our data with an existing gun dataset. By merging the existing gun dataset in our model training, it provides greater data variability and potentially improves the detection performance and robustness of the model in detecting different types of guns.
Another trend is that training with some synthetic followed by real data consistently performs better than training with only real data. For example, by comparing the performance of <span id="Sx3.p1.1.1" class="ltx_text ltx_font_italic">Real_M</span> with <span id="Sx3.p1.1.2" class="ltx_text ltx_font_italic">Textured_Masked_Real_M</span>, we can see that the mAP increased by 16.3%.</p>
</div>
<div id="Sx3.p2" class="ltx_para">
<p id="Sx3.p2.1" class="ltx_p">It’s difficult to determine which model is best for our application based only on the detection results, so we also tested all 71 models with varying confidence thresholds with tracking on six videos, one of which is of a busy mall with no shooters. We included the video of the mall to challenge the false positive rate of the models. Tracking also allows the use of our gun-based shooter confirmation system, which is discussed in more detail later. As expected, this system is very dependent on the quality of gun detection, which, while it does improve the performance of some models, it also collapses the performance of others. The overall best data combination for tracking is augmented UE4+UE5 and real data, with <span id="Sx3.p2.1.1" class="ltx_text ltx_font_italic">AugCTextured_CMasked_Real_S</span> being the best model for both tracking with only the shooter and tracking with gun-based confirmation. Interestingly, a model trained with only 100 real samples, rather than 300 or 500, is the best performing. This may be due to the method of training where different data types are used sequentially, which could imply that the synthetic data better generalizes the models compared to training with a larger amount of the limited real data.
The tracking results for the other data combinations can also be seen in Fig. <a href="#Sx2.F3" title="Figure 3 ‣ Tracking with Deep OC-SORT and OSNET ReID ‣ Results ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>; however, these models seem particularly impacted by low gun detection performance. Regardless, some other trends are apparent. Training with UE5 and Real data performs relatively well, but adding UE4 data or augmenting UE5 data slightly decreases performance. However, adding <span id="Sx3.p2.1.2" class="ltx_text ltx_font_italic">augmented</span> UE4 data and <span id="Sx3.p2.1.3" class="ltx_text ltx_font_italic">augmented</span> UE5 improves performance. The obvious thing to try would be to add augmented UE4 data to unaugmented UE5 data. Lastly, the current system seems prone to ID switches and has difficulty maintaining a constant track. Regardless, tracking in its current form still reduces false positives, thus increasing robustness.</p>
</div>
<div id="Sx3.p3" class="ltx_para">
<p id="Sx3.p3.1" class="ltx_p">Another benefit of tracking rather than just detection is that it allows the further processing of a window of frames. This helps give a better sense of the real-world performance of the system where consistent, but not necessarily constant, updates can provide valuable information about a shooting event in real-time.</p>
</div>
<div id="Sx3.p4" class="ltx_para">
<p id="Sx3.p4.1" class="ltx_p">Higher FPS and resolution for a security system would always be ideal, but in practice, the additional cost of just the higher-quality cameras, not to mention the more powerful computing hardware that would be required, would make such a system hard to adopt. That being said, anecdotally, it seems that around 4 FPS or higher is sufficient to adequately capture the speeds at which a person can move through the view of a security camera, although more thorough testing would need to be done to verify this. Another consideration for tracking at lower FPS is approximations used to predict motion. The error associated with assuming constant velocity will increase as the time between frames increases.</p>
</div>
</section>
<section id="Sx4" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Methods</h2>

<div id="Sx4.p1" class="ltx_para">
<p id="Sx4.p1.1" class="ltx_p">Our implementation of shooter tracking consists of three primary stages: (i) synthetic data generation, (ii) training YOLOv8, and (iii) tracking with Deep OC-SORT using OSNet ReID with gun detection-based shooter confirmation.
</p>
</div>
<section id="Sx4.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Synthetic data generation</h3>

<div id="Sx4.SSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.p1.1" class="ltx_p">We generate synthetic data and perform domain randomization using Unreal Engine 4 and Unreal Engine 5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> environments. The individual aspects of synthetic data generation will be discussed further in the subsections below.</p>
</div>
<section id="Sx4.SSx1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Unreal Engine environment</h4>

<div id="Sx4.SSx1.SSSx1.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p1.1" class="ltx_p">Unreal Engine 4 was used to simulate an active shooter’s movements and those of evacuees, and the shooter was holding an assault rifle to differentiate them visually from the evacuees. The shooter did not fire their weapon during the simulation. Three sections of a hospital building were used, an open room, a hallway, and a staircase. Nodes were designated to correspond to specific locations inside the simulated environment, such as a junction point or an endpoint. Sample images from the environment can be seen in Fig. <a href="#Sx4.F5" title="Figure 5 ‣ Camera sensor effects ‣ Synthetic data generation ‣ Methods ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. All actors, shooters, and evacuees were designed to move from one node to another. Evacuees were programmed to reach the nearest exit, and the shooter was programmed to reach a target node. The movement of the actors was facilitated through a navigation mesh, and the shooter was a dynamic obstacle on the mesh, so the evacuees tried to avoid the shooter. Cameras were placed strategically in the building to observe the shooter and evacuees. With proper camera placements, we could capture various movement interactions between the actors and the camera, such as the actors moving toward the camera, away from the camera, and perpendicular to the camera.</p>
</div>
<div id="Sx4.SSx1.SSSx1.p2" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p2.1" class="ltx_p">The UnrealCV plugin <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> was used to allow a Python script to modify the Unreal Engine 4 environment. This plugin enables us to modify the simulation settings, such as the position (location and rotation) and color of objects, along with the camera position and view type. There are two view types that we used in the simulation, the <span id="Sx4.SSx1.SSSx1.p2.1.1" class="ltx_text ltx_font_italic">image</span> and <span id="Sx4.SSx1.SSSx1.p2.1.2" class="ltx_text ltx_font_italic">object mask</span> view types, which represent the default textures and solid-colored segmentation masks, respectively. These capabilities allow us to randomize scenes with a shooter, evacuees, and multiple camera locations. On top of that, the images from both textured and masked images can be saved for further processing.</p>
</div>
<div id="Sx4.SSx1.SSSx1.p3" class="ltx_para">
<p id="Sx4.SSx1.SSSx1.p3.1" class="ltx_p">We also used Unreal Engine 5 to simulate an active shooter with civilians in various higher-fidelity environments. These environments consist of a school, a supermarket, and a bank; in which we recorded data from three locations in both the school and supermarket and four locations in the bank. The shooter can hold either a handgun or a rifle to increase the variety in the data. We also vary the number of civilians to have low- and high-density scenes. The civilian models are randomly generated from a pool of assets with adjustable parameters. Rather than limit the possible character positions through creating a realistic shooting simulation, we choose to have the civilians move in essentially random paths along with randomly placing the shooter. This creates more challenging scenarios where the shooter is partially occluded. However, this data is only useful for training detection since there is no continuity between frames. Sample images from the environments can be seen in Fig. <a href="#Sx4.F5" title="Figure 5 ‣ Camera sensor effects ‣ Synthetic data generation ‣ Methods ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section id="Sx4.SSx1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Domain randomization</h4>

<div id="Sx4.SSx1.SSSx2.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p1.1" class="ltx_p">While synthetic data is an amazing tool for deep learning, some care needs to be taken when using it to train models. There are domain differences between real and synthetic data; thus, when synthetic data is used for training, it generally cannot be expected to work directly for inference on real data. Closing this gap between domains is called domain adaptation, for which many different techniques exist. Probably the most intuitive technique is to have a high-fidelity simulation to appear as realistic as possible. While we try to have fairly high-fidelity synthetic data, especially in the case of the UE5 environments, we also choose to use domain randomization due to the level of control achievable with the Unreal Engine environments. This approach allows for simple yet effective implementation where we randomly sample positions for the shooter and evacuees within bounded areas and randomly sample colors for everything in the environment.</p>
</div>
<div id="Sx4.SSx1.SSSx2.p2" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p2.1" class="ltx_p">The synthetic data generation process with Unreal Engine 4 can be seen in Fig. <a href="#Sx4.F5" title="Figure 5 ‣ Camera sensor effects ‣ Synthetic data generation ‣ Methods ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>. The environment initializes with the actual textures of the environment. The first step is to update the position of the actors, which includes the evacuees and the shooter. We achieved this by randomly sampling positions within the bounded area (shown in green). The camera positions and viewing angles are randomized within smaller valid regions (shown in red). From here, textured synthetic (TS) images are exported before randomizing the colors of everything in the environment. The colors are changed by switching to the object mask view and randomly setting the red, green, and blue (RGB) channel values to between 0 and 255 for each object using UnrealCV. The resulting scene is exported as domain-randomized masked synthetic (MS) images. Then, the selected colors for the shooter and guns are used to easily threshold the segmented images to generate tight bounding box annotations. We found this extra step to manually create precise bounding boxes necessary because bounding boxes created within Unreal Engine 4 would often have a hand or foot of the shooter partially outside. Finally, the view is switched back to default with actual textures, and the randomization loop restarts from the beginning.</p>
</div>
<div id="Sx4.SSx1.SSSx2.p3" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p3.1" class="ltx_p">This configuration allows us to easily make adjustments to the overall process. One such adjustment is that instead of randomizing the position of the actors in the UE4 TS data, we freeze and unfreeze the simulation to capture time-continuous data. Additionally, the TS output images don’t necessarily need to be saved when generating MS data. However, the MS images are still temporarily required to generate the bounding box annotations for the TS images.</p>
</div>
<div id="Sx4.SSx1.SSSx2.p4" class="ltx_para">
<p id="Sx4.SSx1.SSSx2.p4.1" class="ltx_p">The generation process with Unreal Engine 5 can be seen in Fig. <a href="#Sx4.F5" title="Figure 5 ‣ Camera sensor effects ‣ Synthetic data generation ‣ Methods ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> and consists of running a short simulation in each desired area. The cameras are positioned to emulate the view of actual security cameras. However, to increase variety in the data, we slightly perturb the position and viewing angle for each image. Textured and masked data are captured separately, and bounding box annotations are generated directly using Unreal Engine 5. Instead of varying the colors of everything in Unreal Engine, we keep constant mask colors throughout the simulation and randomize the colors in a separate Python script using simple thresholding based on the already masked images. This significantly speeds up the data generation process because Unreal Engine would need to cycle through every object in the environment rather than just the visible masks.</p>
</div>
</section>
<section id="Sx4.SSx1.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection">Camera sensor effects</h4>

<div id="Sx4.SSx1.SSSx3.p1" class="ltx_para">
<p id="Sx4.SSx1.SSSx3.p1.1" class="ltx_p">We augment the textured synthetic data using camera sensor effect modeling <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. Rather than applying all the effects on all the images uniformly, they are applied at random levels. The objective of augmenting the textured synthetic data is to help blend the differences between synthetic and real data by making it more similar to real data. We chose to augment only textured data rather than the masked data to limit the already large number of combinations. It’s possible that augmenting the masked data would further improve its ability to transfer to real data. This is done by breaking up aspects of synthetic data, such as extremely well-defined edges. Adding noise mimics the normal noise found in real images introduced by limitations in the camera’s sensor. Blurring the images helps make the edge less defined. Chromatic aberration mimics an effect along the edges of objects caused by camera lenses. Adjusting exposure helps account for varying camera quality and lighting changes throughout the day. Lastly, color shift helps account for different camera sensors, where one may be more sensitive to certain colors than another. These camera sensor effects can be seen in Fig. <a href="#Sx4.F5" title="Figure 5 ‣ Camera sensor effects ‣ Synthetic data generation ‣ Methods ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<figure id="Sx4.F5" class="ltx_figure"><img src="/html/2309.03381/assets/Figures/ASTERS_Synthetic_Data.png" id="Sx4.F5.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="326" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>
Complete synthetic data generation process. We utilize the UnrealCV plugin for UE4 to generate textured and masked synthetic data to obtain accurate bounding boxes by thresholding the masked images. Textured and masked data are generated separately in UE5 as we are able to extract accurate bounding boxes directly. Both UE4 and UE5 textured images are augmented with camera sensor effects.</figcaption>
</figure>
</section>
</section>
<section id="Sx4.SSx2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Training YOLOv8</h3>

<div id="Sx4.SSx2.p1" class="ltx_para">
<p id="Sx4.SSx2.p1.1" class="ltx_p">YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite> is one of the latest versions of the popular you only look once (YOLO) object detection framework. It has made several advancements upon previous versions that significantly improve its performance with small models, making it well-suited for edge devices.</p>
</div>
<div id="Sx4.SSx2.p2" class="ltx_para">
<p id="Sx4.SSx2.p2.1" class="ltx_p">The classes we aim to detect are shooter and gun, where the shooter class bounding boxes contain both the person and the visible portion of the gun. If the gun becomes completely obscured, we still label the person as a shooter based on prior knowledge and subtle posture hints, such as hunched shoulders. While we are primarily concerned with tracking the shooter, including gun detection as confirmation of a new shooter may help reduce false positives. We have two groups of image data: real images extracted from videos publicly available online and synthetic images created using the Unreal Engine environments. The synthetic data can be further divided into two subgroups: semi-realistic textured synthetic images (TS) and masked synthetic (MS) images. Both sets of synthetic data are created from scenes where the characters’ positions are randomized for each frame, with animations updating their appearance as if they were moving.</p>
</div>
<div id="Sx4.SSx2.p3" class="ltx_para">
<p id="Sx4.SSx2.p3.3" class="ltx_p">Our dataset includes 700 real images, split into 500 training, 100 validation, and 100 testing images. Synthetic data with semi-realistic textures consists of 1,567 images from UE5 and 978 images from UE4, and synthetic data with masked textures includes 7,415 images from UE5 and 5,283 images from UE4. All synthetic data is used only for training. We also explore the use of camera sensor effect augmentation for the TS data. All synthetic data is used only for training. The object detection ground truth contains the <math id="Sx4.SSx2.p3.1.m1.1" class="ltx_Math" alttext="x" display="inline"><semantics id="Sx4.SSx2.p3.1.m1.1a"><mi id="Sx4.SSx2.p3.1.m1.1.1" xref="Sx4.SSx2.p3.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p3.1.m1.1b"><ci id="Sx4.SSx2.p3.1.m1.1.1.cmml" xref="Sx4.SSx2.p3.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p3.1.m1.1c">x</annotation></semantics></math> and <math id="Sx4.SSx2.p3.2.m2.1" class="ltx_Math" alttext="y" display="inline"><semantics id="Sx4.SSx2.p3.2.m2.1a"><mi id="Sx4.SSx2.p3.2.m2.1.1" xref="Sx4.SSx2.p3.2.m2.1.1.cmml">y</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p3.2.m2.1b"><ci id="Sx4.SSx2.p3.2.m2.1.1.cmml" xref="Sx4.SSx2.p3.2.m2.1.1">𝑦</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p3.2.m2.1c">y</annotation></semantics></math> coordinates of the center of the bounding box localizing the shooter or gun, as well as the width and height of the bounding box. The training image size is the default value of <math id="Sx4.SSx2.p3.3.m3.1" class="ltx_Math" alttext="640\times 640" display="inline"><semantics id="Sx4.SSx2.p3.3.m3.1a"><mrow id="Sx4.SSx2.p3.3.m3.1.1" xref="Sx4.SSx2.p3.3.m3.1.1.cmml"><mn id="Sx4.SSx2.p3.3.m3.1.1.2" xref="Sx4.SSx2.p3.3.m3.1.1.2.cmml">640</mn><mo lspace="0.222em" rspace="0.222em" id="Sx4.SSx2.p3.3.m3.1.1.1" xref="Sx4.SSx2.p3.3.m3.1.1.1.cmml">×</mo><mn id="Sx4.SSx2.p3.3.m3.1.1.3" xref="Sx4.SSx2.p3.3.m3.1.1.3.cmml">640</mn></mrow><annotation-xml encoding="MathML-Content" id="Sx4.SSx2.p3.3.m3.1b"><apply id="Sx4.SSx2.p3.3.m3.1.1.cmml" xref="Sx4.SSx2.p3.3.m3.1.1"><times id="Sx4.SSx2.p3.3.m3.1.1.1.cmml" xref="Sx4.SSx2.p3.3.m3.1.1.1"></times><cn type="integer" id="Sx4.SSx2.p3.3.m3.1.1.2.cmml" xref="Sx4.SSx2.p3.3.m3.1.1.2">640</cn><cn type="integer" id="Sx4.SSx2.p3.3.m3.1.1.3.cmml" xref="Sx4.SSx2.p3.3.m3.1.1.3">640</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx2.p3.3.m3.1c">640\times 640</annotation></semantics></math>, so the images are resized and padded before training.</p>
</div>
<div id="Sx4.SSx2.p4" class="ltx_para">
<p id="Sx4.SSx2.p4.1" class="ltx_p">Although not exhaustive, we conducted a series of training experiments to find the best combination of these three image data types: real (R), textured synthetic (TS), and masked synthetic (MS). For each combination, different numbers of images are used. We train four sets of 23 different data combinations, as seen in the combination column of Table <a href="#Sx2.T1" title="Table 1 ‣ Detection with YOLOv8n ‣ Results ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The four sets include data generated with UE5, data generated with UE4 and UE5, augmented data generated with UE5, and augmented data generated with UE4 and UE5. Rather than starting training from scratch, we use the YOLOv8n weights pre-trained on the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. These weights can be used to detect 80 classes of objects found in the COCO dataset. We run training for the default 100 epochs with early stopping and patience value of 50 epochs.When multiple data types are being used, the first type of data starts with the pre-trained weights and trains for 100 epochs, then the following type of data resumes the training with the weights obtained from the previous training phase and trains for another 100 epochs.
</p>
</div>
</section>
<section id="Sx4.SSx3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Tracking with Deep OC-SORT, OSNET ReID, and shooter confirmation with gun detections</h3>

<div id="Sx4.SSx3.p1" class="ltx_para">
<p id="Sx4.SSx3.p1.3" class="ltx_p">The YOLO tracking toolbox <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> was immensely helpful when exploring the performance of different tracking algorithms on a Jetson Nano. We chose to use Deep OC-SORT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite> with OSNET <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib27" title="" class="ltx_ref">27</a>]</cite> using x0 25 MSMT17 weights for the re-identification model for the balance of speed and accuracy, even at a low framerate. As its name suggests, Deep OC-SORT builds upon OC-SORT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, which addresses some limitations of SORT <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite>. One of the limitations of SORT is that it is a purely motion-based tracker. This means that when an object is lost, the estimated location from the Kalman filter will likely deviate from the actual location as time continues. This means that even when the object is detected again, it will not be part of the same track. Observation-centric Re-Update (ORU) reduces the accumulated error by backchecking and updating the parameters of the Kalman filter when an object is detected again. This re-update is based on virtual trajectories of the untracked period. Observation-Centric Momentum (OCM) aims to consider the size of <math id="Sx4.SSx3.p1.1.m1.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="Sx4.SSx3.p1.1.m1.1a"><mi mathvariant="normal" id="Sx4.SSx3.p1.1.m1.1.1" xref="Sx4.SSx3.p1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p1.1.m1.1b"><ci id="Sx4.SSx3.p1.1.m1.1.1.cmml" xref="Sx4.SSx3.p1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p1.1.m1.1c">\Delta</annotation></semantics></math>t used to estimate the velocity. While a small <math id="Sx4.SSx3.p1.2.m2.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="Sx4.SSx3.p1.2.m2.1a"><mi mathvariant="normal" id="Sx4.SSx3.p1.2.m2.1.1" xref="Sx4.SSx3.p1.2.m2.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p1.2.m2.1b"><ci id="Sx4.SSx3.p1.2.m2.1.1.cmml" xref="Sx4.SSx3.p1.2.m2.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p1.2.m2.1c">\Delta</annotation></semantics></math>t is necessary for the linear-motion assumption, a large <math id="Sx4.SSx3.p1.3.m3.1" class="ltx_Math" alttext="\Delta" display="inline"><semantics id="Sx4.SSx3.p1.3.m3.1a"><mi mathvariant="normal" id="Sx4.SSx3.p1.3.m3.1.1" xref="Sx4.SSx3.p1.3.m3.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="Sx4.SSx3.p1.3.m3.1b"><ci id="Sx4.SSx3.p1.3.m3.1.1.cmml" xref="Sx4.SSx3.p1.3.m3.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="Sx4.SSx3.p1.3.m3.1c">\Delta</annotation></semantics></math>t also increases noise in the measurement; thus, the choice to increase it comes at a trade-off. Lastly, Observation-Centric Recovery (OCR) starts a second association attempt between the last unmatched observations and tracks. This can handle objects stopping or being occluded for a short duration.
Deep OC-SORT improves upon OC-SORT in a few ways. Firstly, they implement camera motion compensation to adjust predicted bounding boxes based on the camera’s movement. However, we disable camera motion compensation to slightly improve inference speed because the cameras in our application are stationary. Secondly, their implementation of appearance association is dynamic and linearly scales based on the confidence of the detection. Lastly, adaptive weighting is used to increase the weight of appearance features depending on the discriminativeness of the embeddings. This is used to boost track-box scores for cases where there is a high degree of similarity.</p>
</div>
<div id="Sx4.SSx3.p2" class="ltx_para">
<p id="Sx4.SSx3.p2.1" class="ltx_p">While detecting and tracking shooters as a whole allows for more robust tracking when compared to only guns, it also increases the likelihood of falsely detecting a regular person as a shooter. To address this, we require that a gun detection overlaps with a shooter detection before we begin tracking that person as a shooter. We are able to do this because our shooter class contains both the person and the gun when it is visible. After this confirmation step, we no longer require gun detection to continue tracking the shooter as long as the ReID model associates them with the same ID. If a new potential shooter ID is introduced, a gun detection will be required before that ID is labeled as a shooter. A new ID doesn’t necessarily mean a new shooter since the ReID model can be confused by cases where the appearance of a shooter changes significantly between frames. This system is implemented for the track initialization stage of Deep OC-SORT; as such, a shooter ID can only be introduced in that stage. However, once the ID exists, it can be used in the first and second association stages of Deep OC-SORT to keep track of the shooter through occlusions or other situations where the detection accuracy decreases. This process can be seen in Fig. <a href="#Sx4.F6" title="Figure 6 ‣ Tracking with Deep OC-SORT, OSNET ReID, and shooter confirmation with gun detections ‣ Methods ‣ Active shooter detection and robust tracking utilizing supplemental synthetic data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure id="Sx4.F6" class="ltx_figure"><img src="/html/2309.03381/assets/Figures/ASTERS_Gun_Confirmation.png" id="Sx4.F6.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="224" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>
The pipeline of our gun confirmation system for shooter tracking. Shooter and gun detection boxes are sent to the Deep OC-SORT association phase. The gun detections are not tracked but are instead used in track initialization to confirm a new shooter detection before assigning that track an ID. After a shooter has an ID, that track no longer requires a gun detection to continue being tracked. If a shooter detection does not match an existing ID and does not have a gun detection to confirm it, it is discarded.</figcaption>
</figure>
</section>
</section>
<section id="Sx5" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Availability of materials and data</h2>

<div id="Sx5.p1" class="ltx_para">
<p id="Sx5.p1.1" class="ltx_p">The datasets used and/or analyzed during the current study are available from the corresponding author upon reasonable request.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography" lang="en">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Follman, M., Aronsen, G. &amp;
Pan, D.

</span>
<span class="ltx_bibblock">Us mass shootings, 1982–2022: Data from mother
jones’ investigation (2012).

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Ames, B.

</span>
<span class="ltx_bibblock">Making schools safe for students.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib2.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>National Institute of Justice</em>
(2019).

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Jonson, C. L.

</span>
<span class="ltx_bibblock">Preventing school shootings: The
effectiveness of safety measures.

</span>
<span class="ltx_bibblock"><em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib3.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Victims &amp; Offenders</em>
<span id="bib.bib3.2.2" class="ltx_text ltx_font_bold">12</span>, 956–973,
DOI: <a href="10.1080/15564886.2017.1307293" title="" class="ltx_ref ltx_url">10.1080/15564886.2017.1307293</a> (2017).

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1080/15564886.2017.1307293" title="" class="ltx_ref ltx_url">https://doi.org/10.1080/15564886.2017.1307293</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Madfis, E.

</span>
<span class="ltx_bibblock">“it’s better to overreact”:
School officials’ fear and perceived risk of rampage attacks and the
criminalization of american public schools.

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib4.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Critical Criminology</em>
<span id="bib.bib4.2.2" class="ltx_text ltx_font_bold">24</span>, 39–55,
DOI: <a href="10.1007/s10612-015-9297-0" title="" class="ltx_ref ltx_url">10.1007/s10612-015-9297-0</a> (2016).

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Valenzise, G., Gerosa, L.,
Tagliasacchi, M., Antonacci, F. &amp;
Sarti, A.

</span>
<span class="ltx_bibblock">Scream and gunshot detection and localization for
audio-surveillance systems.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">2007 IEEE Conference on Advanced Video
and Signal Based Surveillance</em>, 21–26
(IEEE, 2007).

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Mares, D. &amp; Blackburn, E.

</span>
<span class="ltx_bibblock">Acoustic gunshot detection systems:
a quasi-experimental evaluation in st. louis, mo.

</span>
<span class="ltx_bibblock"><em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib6.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Journal of experimental criminology</em>
<span id="bib.bib6.2.2" class="ltx_text ltx_font_bold">17</span>, 193–215
(2021).

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Qi, D., Tan, W., Liu, Z.,
Yao, Q. &amp; Liu, J.

</span>
<span class="ltx_bibblock">A gun detection dataset and
searching for embedded device solutions.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib7.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>CoRR</em>
<span id="bib.bib7.2.2" class="ltx_text ltx_font_bold">abs/2105.01058</span> (2021).

</span>
<span class="ltx_bibblock"><a href="2105.01058" title="" class="ltx_ref ltx_url">2105.01058</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Narejo, S., Pandey, B.,
Esenarro vargas, D., Rodriguez, C. &amp;
Anjum, M. R.

</span>
<span class="ltx_bibblock">Weapon detection using yolo v3 for
smart surveillance system.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib8.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Mathematical Problems in Engineering</em>
<span id="bib.bib8.2.2" class="ltx_text ltx_font_bold">2021</span>, 9975700,
DOI: <a href="10.1155/2021/9975700" title="" class="ltx_ref ltx_url">10.1155/2021/9975700</a> (2021).

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Salido, J., Lomas, V.,
Ruiz-Santaquiteria, J. &amp; Deniz, O.

</span>
<span class="ltx_bibblock">Automatic handgun detection with
deep learning in video surveillance images.

</span>
<span class="ltx_bibblock"><em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib9.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Applied Sciences</em>
<span id="bib.bib9.2.2" class="ltx_text ltx_font_bold">11</span>, DOI: <a href="10.3390/app11136085" title="" class="ltx_ref ltx_url">10.3390/app11136085</a>
(2021).

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Ahmed, S., Bhatti, M. T.,
Khan, M. G., Lövström, B. &amp;
Shahid, M.

</span>
<span class="ltx_bibblock">Development and optimization of
deep learning models for weapon detection in surveillance videos.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib10.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Applied Sciences</em> (2022).

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Ashraf, A. H. <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Weapons detection for security and
video surveillance using cnn and yolo-v5s.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.2.1" class="ltx_emph ltx_font_italic"><span id="bib.bib11.2.1.1" class="ltx_ERROR undefined">\JournalTitle</span>CMC-Comput. Mater. Contin</em>
<span id="bib.bib11.3.2" class="ltx_text ltx_font_bold">70</span>, 2761–2775
(2022).

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Narejo, S., Pandey, B.,
Esenarro Vargas, D., Rodriguez, C. &amp;
Anjum, M. R.

</span>
<span class="ltx_bibblock">Weapon detection using yolo v3 for
smart surveillance system.

</span>
<span class="ltx_bibblock"><em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib12.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Mathematical Problems in Engineering</em>
<span id="bib.bib12.2.2" class="ltx_text ltx_font_bold">2021</span>, 1–9
(2021).

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Warsi, A. <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Gun detection system using yolov3.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.2.1" class="ltx_emph ltx_font_italic">2019 IEEE International Conference on
Smart Instrumentation, Measurement and Application (ICSIMA)</em>,
1–4, DOI: <a href="10.1109/ICSIMA47653.2019.9057329" title="" class="ltx_ref ltx_url">10.1109/ICSIMA47653.2019.9057329</a>
(2019).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Manikandan, V. &amp; Rahamathunnisa, U.

</span>
<span class="ltx_bibblock">A neural network aided attuned
scheme for gun detection in video surveillance images.

</span>
<span class="ltx_bibblock"><em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib14.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Image and Vision Computing</em>
<span id="bib.bib14.2.2" class="ltx_text ltx_font_bold">120</span>, 104406,
DOI: <a target="_blank" href="https://doi.org/10.1016/j.imavis.2022.104406" title="" class="ltx_ref ltx_url">https://doi.org/10.1016/j.imavis.2022.104406</a>
(2022).

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Salazar González, J. L., Zaccaro, C.,
Álvarez García, J. A., Soria Morillo,
L. M. &amp; Sancho Caparrini, F.

</span>
<span class="ltx_bibblock">Real-time gun detection in cctv: An
open problem.

</span>
<span class="ltx_bibblock"><em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic"><span id="bib.bib15.1.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Neural Networks</em>
<span id="bib.bib15.2.2" class="ltx_text ltx_font_bold">132</span>, 297–308,
DOI: <a target="_blank" href="https://doi.org/10.1016/j.neunet.2020.09.013" title="" class="ltx_ref ltx_url">https://doi.org/10.1016/j.neunet.2020.09.013</a>
(2020).

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Verma, G. K. &amp; Dhillon, A.

</span>
<span class="ltx_bibblock">A handheld gun detection using faster r-cnn deep
learning.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 7th International
Conference on Computer and Communication Technology</em>, ICCCT-2017,
84–88, DOI: <a href="10.1145/3154979.3154988" title="" class="ltx_ref ltx_url">10.1145/3154979.3154988</a>
(Association for Computing Machinery,
New York, NY, USA, 2017).

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Tobin, J. <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Domain randomization for transferring deep neural
networks from simulation to the real world.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.2.1" class="ltx_emph ltx_font_italic">2017 IEEE/RSJ international conference
on intelligent robots and systems (IROS)</em>, 23–30
(IEEE, 2017).

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Tremblay, J. <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Training deep networks with synthetic data: Bridging
the reality gap by domain randomization.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.2.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition (CVPR) Workshops</em>
(2018).

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Zhuang, F. <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">A comprehensive survey on transfer
learning.

</span>
<span class="ltx_bibblock"><em id="bib.bib19.2.1" class="ltx_emph ltx_font_italic"><span id="bib.bib19.2.1.1" class="ltx_ERROR undefined">\JournalTitle</span>Proceedings of the IEEE</em>
<span id="bib.bib19.3.2" class="ltx_text ltx_font_bold">109</span>, 43–76,
DOI: <a href="10.1109/JPROC.2020.3004555" title="" class="ltx_ref ltx_url">10.1109/JPROC.2020.3004555</a> (2021).

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Carlson, A., Skinner, K. A.,
Vasudevan, R. &amp; Johnson-Roberson, M.

</span>
<span class="ltx_bibblock">Modeling camera effects to improve visual learning
from synthetic data (2018).

</span>
<span class="ltx_bibblock"><a href="1803.07721" title="" class="ltx_ref ltx_url">1803.07721</a>.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Epic Games.

</span>
<span class="ltx_bibblock">Unreal engine (2019).

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Qiu, W. <em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Unrealcv: Virtual worlds for
computer vision.

</span>
<span class="ltx_bibblock"><em id="bib.bib22.2.1" class="ltx_emph ltx_font_italic"><span id="bib.bib22.2.1.1" class="ltx_ERROR undefined">\JournalTitle</span>ACM Multimedia Open Source Software
Competition</em> (2017).

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Jocher, G., Chaurasia, A. &amp;
Qiu, J.

</span>
<span class="ltx_bibblock">YOLO by Ultralytics (2023).

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Lin, T.-Y. <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">et al.</em>

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context
(2015).

</span>
<span class="ltx_bibblock"><a href="1405.0312" title="" class="ltx_ref ltx_url">1405.0312</a>.

</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Broström, M.

</span>
<span class="ltx_bibblock">Real-time multi-object tracking and segmentation
using Yolov8 with StrongSORT and OSNet,
DOI: <a target="_blank" href="https://zenodo.org/record/7629840" title="" class="ltx_ref ltx_url">https://zenodo.org/record/7629840</a>.

</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Maggiolino, G., Ahmad, A.,
Cao, J. &amp; Kitani, K.

</span>
<span class="ltx_bibblock">Deep oc-sort: Multi-pedestrian tracking by adaptive
re-identification (2023).

</span>
<span class="ltx_bibblock"><a href="2302.11813" title="" class="ltx_ref ltx_url">2302.11813</a>.

</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zhou, K., Yang, Y.,
Cavallaro, A. &amp; Xiang, T.

</span>
<span class="ltx_bibblock">Omni-scale feature learning for person
re-identification (2019).

</span>
<span class="ltx_bibblock"><a href="1905.00953" title="" class="ltx_ref ltx_url">1905.00953</a>.

</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Cao, J., Pang, J., Weng,
X., Khirodkar, R. &amp; Kitani, K.

</span>
<span class="ltx_bibblock">Observation-centric sort: Rethinking sort for robust
multi-object tracking (2023).

</span>
<span class="ltx_bibblock"><a href="2203.14360" title="" class="ltx_ref ltx_url">2203.14360</a>.

</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Bewley, A., Ge, Z., Ott,
L., Ramos, F. &amp; Upcroft, B.

</span>
<span class="ltx_bibblock">Simple online and realtime tracking.

</span>
<span class="ltx_bibblock">In <em id="bib.bib29.1.1" class="ltx_emph ltx_font_italic">2016 IEEE International Conference on
Image Processing (ICIP)</em>, DOI: <a href="10.1109/icip.2016.7533003" title="" class="ltx_ref ltx_url">10.1109/icip.2016.7533003</a>
(IEEE, 2016).

</span>
</li>
</ul>
</section>
<section id="Sx6" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>

<div id="Sx6.p1" class="ltx_para">
<p id="Sx6.p1.1" class="ltx_p">This work was supported by the National Science Foundation Award# CNS-1932033.</p>
</div>
</section>
<section id="Sx7" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Author contributions statement</h2>

<div id="Sx7.p1" class="ltx_para">
<p id="Sx7.p1.1" class="ltx_p">Conceptualization, J.R.W., S.Y.T., and S.S.;
Methodology, J.R.W.;
Software, J.R.W.;
Validation, J.R.W. and J.F.;
Investigation, J.R.W. and J.F.;
Data Curation, J.R.W., R.T. and L.H.;
Writing - Original Draft, J.R.W. and J.F.;
Writing - Review &amp; Editing, all authors;
Visualization, J.R.W. and J.F.;
Supervision, S.C. and S.S.;
Project Administration, S.C. and S.S.;
Funding Acquisition, S.C. and S.S.
</p>
</div>
</section>
<section id="Sx8" class="ltx_section" lang="en">
<h2 class="ltx_title ltx_title_section">Additional information</h2>

<section id="Sx8.SSx1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">Competing interests</h3>

<div id="Sx8.SSx1.p1" class="ltx_para">
<p id="Sx8.SSx1.p1.1" class="ltx_p">The authors declare no competing interests.</p>
</div>
<div id="Sx8.SSx1.p2" class="ltx_para">
<p id="Sx8.SSx1.p2.1" class="ltx_p">See pages - of <a href="supplementary_temp.pdf" title="" class="ltx_ref">supplementary_temp.pdf</a></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2309.03380" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2309.03381" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2309.03381">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2309.03381" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2309.03383" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 07:41:12 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
