<!DOCTYPE html><html prefix="dcterms: http://purl.org/dc/terms/" lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2402.05593] A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only</title><meta property="og:description" content="In medieval times, stuccoworkers used a red color, called sinopia, to first create a sketch of the to-be-made statue on the wall. Today, many of these statues are destroyed, but using the original drawings, deriving fr…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2402.05593">

<!--Generated on Tue Mar  5 18:13:00 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\WsShortPaper</span><span id="p1.2" class="ltx_ERROR undefined">\BibtexOrBiblatex</span><span id="p1.3" class="ltx_ERROR undefined">\electronicVersion</span><span id="p1.4" class="ltx_ERROR undefined">\PrintedOrElectronic</span>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\teaser</span><img src="/html/2402.05593/assets/images/teaser_min.jpg" id="p2.g1" class="ltx_graphics ltx_img_landscape" width="479" height="80" alt="[Uncaptioned image]">
<p id="p2.2" class="ltx_p ltx_align_center"><span id="p2.2.1" class="ltx_text ltx_caption">Historic sketches used for stucco work as found in the Princely Abbey of Corvey. Illustrations as found in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx2" title="" class="ltx_ref">CS07</a>]</cite>.</span></p>
</div>
<h1 class="ltx_title ltx_title_document">A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
<span id="id2.2.2" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:433.6pt;">
<span id="id2.2.2.2" class="ltx_p">T. Pöllabauer<sup id="id2.2.2.2.1" class="ltx_sup"><span id="id2.2.2.2.1.1" class="ltx_text ltx_font_italic">1,2</span></sup><span id="id2.2.2.2.2" class="ltx_ERROR undefined">\orcid</span>0000-0003-0075-1181
and J. Kühn<sup id="id2.2.2.2.3" class="ltx_sup"><span id="id2.2.2.2.3.1" class="ltx_text ltx_font_italic">1</span></sup><span id="id2.2.2.2.4" class="ltx_ERROR undefined">\orcid</span>0000-0003-2458-0030</span>
</span>

<br class="ltx_break">
<span id="id4.4.4" class="ltx_inline-block ltx_parbox ltx_align_middle" style="width:433.6pt;">
<span id="id3.3.3.1" class="ltx_p"><sup id="id3.3.3.1.1" class="ltx_sup">1</sup>Fraunhofer Institute for Computer Graphics Research IGD</span>
<span id="id4.4.4.2" class="ltx_p ltx_align_center"><sup id="id4.4.4.2.1" class="ltx_sup">2</sup>Interactive Graphics Research Group, TU Darmstadt</span>
</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="5.1" class="ltx_p">In medieval times, stuccoworkers used a red color, called sinopia, to first create a sketch of the to-be-made statue on the wall. Today, many of these statues are destroyed, but using the original drawings, deriving from the red color also called sinopia, we can reconstruct how the final statue might have looked. We propose a fully-automated approach to reconstruct a point cloud and show preliminary results by generating a color-image, a depth-map, as well as surface normals requiring only a single sketch, and without requiring a collection of other, similar samples. Our proposed solution allows real-time reconstruction on-site, for instance, within an exhibition, or to generate a useful starting point for an expert, trying to manually reconstruct the statue, all while using only synthetic data for training.
<span id="5.1.1" class="ltx_ERROR undefined">{CCSXML}</span>
&lt;ccs2012&gt;
&lt;concept&gt;
&lt;concept_id&gt;10010147.10010178.10010224.10010245.10010254&lt;/concept_id&gt;
&lt;concept_desc&gt;Computing methodologies Reconstruction&lt;/concept_desc&gt;
&lt;concept_significance&gt;500&lt;/concept_significance&gt;
&lt;/concept&gt;
&lt;concept&gt;
&lt;concept_id&gt;10010147.10010257.10010258.10010259&lt;/concept_id&gt;
&lt;concept_desc&gt;Computing methodologies Supervised learning&lt;/concept_desc&gt;
&lt;concept_significance&gt;500&lt;/concept_significance&gt;
&lt;/concept&gt;
&lt;concept&gt;
&lt;concept_id&gt;10010405.10010432.10010434&lt;/concept_id&gt;
&lt;concept_desc&gt;Applied computing Archaeology&lt;/concept_desc&gt;
&lt;concept_significance&gt;500&lt;/concept_significance&gt;
&lt;/concept&gt;
&lt;/ccs2012&gt;</p>
<span id="6.2" class="ltx_ERROR undefined">\ccsdesc</span>
<p id="7.3" class="ltx_p">[500]Computing methodologies Reconstruction
<span id="7.3.1" class="ltx_ERROR undefined">\ccsdesc</span>[500]Computing methodologies Supervised learning
<span id="7.3.2" class="ltx_ERROR undefined">\ccsdesc</span>[500]Applied computing Archaeology</p>
<span id="8.4" class="ltx_ERROR undefined">\printccsdesc</span>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">In 1992, during an inspection of the stonework in the westwork of the Princely Abbey of Corvey, oxide red brushstrokes were unexpectedly found <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx16" title="" class="ltx_ref">Poe02</a>]</cite>. It turned out, these brushstrokes belonged to one of six wall drawings, called sinopia, depicting four men, and two women. However, it soon became clear that these drawings were not meant to be the preliminary stage of a painting, but of stucco statues, as proved by wooden stakes driven into the wall and small residues of material containing gypsum, around these stakes. In addition, stucco fragments found in 1961 proved to be a match for the newly found sinopia. Given the wall drawings and the few remaining fragments, interest arose in reconstructing the destroyed statues. 
<br class="ltx_break">While with only six drawings, one can manually do the reconstruction, an automatable approach could be the basis for a more general solution, also applicable to other historic drawings, found at other locations. Ideally, a solution would not require a domain expert at every part of the process and works with only little to no real data for parameterization, since real samples tend to be scarce and vary drastically in their conservation state and details. 
<br class="ltx_break">We propose a data driven approach to (semi-)automatically produce a first estimate of a destroyed statue, based on a degraded-by-time line drawing. We present the outline of our proposed end-to-end pipeline, as well as first results, demonstrating the feasibility of reconstructing without real data. Our contributions are:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">An end-to-end pipeline for geometry reconstruction of historic statues and figurines based on sketches from the period.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">As a test for feasibility, given a simple line drawing of a historic sketch, we generate a depth estimation, full-color reconstruction, as well as surface normals.</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">We train on synthetic data, i.e. we do not need any other sketches, neither from the time period, nor other, thereby solving the prevailing problem in cultural heritage, of not having enough data for machine learning approaches.</p>
</div>
</li>
<li id="S1.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i4.p1" class="ltx_para">
<p id="S1.I1.i4.p1.1" class="ltx_p">By use of abstraction, our approach is applicable not only to sinopia.</p>
</div>
</li>
<li id="S1.I1.i5" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span> 
<div id="S1.I1.i5.p1" class="ltx_para">
<p id="S1.I1.i5.p1.1" class="ltx_p">Aside of initial sketch restoration, we do not incorporate domain knowledge, making the solution more general.</p>
</div>
</li>
</ul>
</div>
<figure id="S1.F1" class="ltx_figure">
<p id="S1.F1.1" class="ltx_p ltx_align_center"><span id="S1.F1.1.1" class="ltx_text"></span>
<img src="/html/2402.05593/assets/images/GCH22_Diagramm.png" id="S1.F1.1.g1" class="ltx_graphics ltx_img_landscape" width="287" height="190" alt="Refer to caption"></p>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span> Overview of our approach. We solve the problem in 3 stages: First, we collect unrelated statues depicting humans. We use this data to generate images, which we reduce to mere line drawings, making it less distinguishable from our real samples. Second, we train a highly expressive deep neural network to reconstruct high quality information from the reduced representation. Third, we apply our image translation to our handful of real-world samples before estimating the 3d shape. To make the pipeline end-to-end learn-able, the mapping function between 2d-3d information should ideally be differentiable. </figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Geometry reconstruction based on images is a highly relevant challenge when trying to preserve cultural artifacts and locales. One application is scanning using sophisticated sensors, such as laser or RGB(+D) sensors together with photogrammetry. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx6" title="" class="ltx_ref">GRS14</a>]</cite> provides a survey of these and similar methods. A major drawback of these approaches is the requirement of existing geometry. While we have some remaining fragments, these represent so little of the original statue’s volume that we cannot reconstruct the full statue relying only on these fragments, using a technique as described in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx7" title="" class="ltx_ref">GSP<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>14</a>]</cite> for example. Instead, we have to rely on a single depiction, the sketch on the wall, to reconstruct the appearance. 
<br class="ltx_break">Reconstructing 3d information from images is a long withstanding problem in the computer vision community. It is an inherently ill-posed problem since the imaging process, by design, forfeits knowledge about the depicted scene. One way of dealing with this loss of information is the use of multiple images to reconstruct the 3d information. The availability of additional view points reintroduces much of the lost information about the scene. However, the need for multiple images of a single scene greatly reduces the applicability in real world scenarios by requiring either a multi-camera setup or a static scene, photographed from various view points. This constraint led to interest in solving the reconstruction problem via a single image only. Solving the problem with only one view requires the introduction of additional information, for instance knowledge about objects depicted in the scene. 
<br class="ltx_break">Another possible solution, which proved very successful recently, is the use of machine learning and especially deep neural networks. These algorithms can be trained with a wide variety of scenes with available ground truth data and thereby acquire some general scene understanding or, in other words, a useful prior on the basic construction of scenes and their projection on a 2-dimensional image plane <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">LKL17</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx21" title="" class="ltx_ref">XYZ<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>20</a>]</cite>. These approaches often lead to impressive results given the ill posed problem, but usually require real world photographs or realistic imagery, such as renderings. Also, in the cultural heritage domain, current deep learning approaches face the problem of too little data, as noted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx3" title="" class="ltx_ref">FKP<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>20</a>]</cite>, surveying the use of machine learning in cultural heritage. 
<br class="ltx_break">There is a much smaller body of work dealing with geometry estimation based on hand-drawn sketches of various degrees of complexity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">WYZ<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>20</a>]</cite>. These approaches might use segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx20" title="" class="ltx_ref">WYZ<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>20</a>]</cite> and/or estimate depth and normals before fusing the results to different kind of 3d representations <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx12" title="" class="ltx_ref">LGK<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>17</a>]</cite>.
<br class="ltx_break">Some works, especially those relying on differentiable rendering, do not require anything, but geometric understanding of the imaging process <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx15" title="" class="ltx_ref">NJJ21</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx9" title="" class="ltx_ref">JJHZ19</a>]</cite>. In our data limited scenario, however, we favor approaches that allow to learn a prior on other, similar, available data. A often-utilized approach is the use of encoder-decoder architectures.
Encoder-decoder architectures break an input image (usually using a convolutional neural network) down into a, compared to the original image, small vector representation. This vector space is called the latent space. A second network, the decoder samples vectors from this latent space and has to reconstruct the encoded image. This allows to learn a powerful representation on available training data to be used in cases with only a few real samples, an ideal fit to our challenge.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Our proposed solution takes a sketch of a sinopia. As shown in row 1 of Fig. <span class="ltx_ref ltx_ref_self"><span class="ltx_text ltx_ref_title">A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only</span></span>, the drawing is almost unobservable, which is why we require help of a domain expert to extract the contours and fill in missing details. This is the only time we require domain knowledge. Next, we use image-to-image-translation to project this sketch in our intermediary domain of line drawings. Finally, we pass the converted image to an encoder-decoder network, pre-trained on a dataset of statues with geometry information available. Our approach is depicted in Fig. <a href="#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<table id="S3.T1.40" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S3.T1.40.41.1" class="ltx_tr">
<td id="S3.T1.40.41.1.1" class="ltx_td ltx_align_center ltx_align_top ltx_border_l ltx_border_r ltx_border_t" colspan="12">Results on Test Split</td>
</tr>
<tr id="S3.T1.40.42.2" class="ltx_tr">
<td id="S3.T1.40.42.2.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.1.1.1" class="ltx_p" style="width:5.7pt;">No</span>
</span>
</td>
<td id="S3.T1.40.42.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr ltx_border_t">
<span id="S3.T1.40.42.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.2.1.1" class="ltx_p" style="width:34.1pt;">Sketch</span>
</span>
</td>
<td id="S3.T1.40.42.2.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.3.1.1" class="ltx_p" style="width:34.1pt;">RGB</span>
</span>
</td>
<td id="S3.T1.40.42.2.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.4.1.1" class="ltx_p" style="width:34.1pt;">Depth</span>
</span>
</td>
<td id="S3.T1.40.42.2.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.5.1.1" class="ltx_p" style="width:34.1pt;">Normals</span>
</span>
</td>
<td id="S3.T1.40.42.2.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.6.1.1" class="ltx_p" style="width:34.1pt;">Mask</span>
</span>
</td>
<td id="S3.T1.40.42.2.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.7.1.1" class="ltx_p" style="width:5.7pt;">No</span>
</span>
</td>
<td id="S3.T1.40.42.2.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr ltx_border_t">
<span id="S3.T1.40.42.2.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.8.1.1" class="ltx_p" style="width:34.1pt;">Sketch</span>
</span>
</td>
<td id="S3.T1.40.42.2.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.9.1.1" class="ltx_p" style="width:34.1pt;">RGB</span>
</span>
</td>
<td id="S3.T1.40.42.2.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.10.1.1" class="ltx_p" style="width:34.1pt;">Depth</span>
</span>
</td>
<td id="S3.T1.40.42.2.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.11.1.1" class="ltx_p" style="width:34.1pt;">Normals</span>
</span>
</td>
<td id="S3.T1.40.42.2.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.40.42.2.12.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.42.2.12.1.1" class="ltx_p" style="width:34.1pt;">Mask</span>
</span>
</td>
</tr>
<tr id="S3.T1.10.10" class="ltx_tr">
<td id="S3.T1.10.10.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t">
<span id="S3.T1.10.10.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.10.10.11.1.1" class="ltx_p" style="width:5.7pt;">1</span>
</span>
</td>
<td id="S3.T1.1.1.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr ltx_border_t">
<span id="S3.T1.1.1.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.1.1.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.1.1.1.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/front_views/143_00270.png" id="S3.T1.1.1.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.2.2.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.2.2.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.2.2.2.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.2.2.2.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/rgb_recon_143_00270.png" id="S3.T1.2.2.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.3.3.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.3.3.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.3.3.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.3.3.3.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/depth_recon_143_00270.png" id="S3.T1.3.3.3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.4.4.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.4.4.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.4.4.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.4.4.4.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/normals_recon_143_00270.png" id="S3.T1.4.4.4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.5.5.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.5.5.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.5.5.5.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.5.5.5.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/mask_recon_143_00270.png" id="S3.T1.5.5.5.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.10.10.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.10.10.12.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.10.10.12.1.1" class="ltx_p" style="width:5.7pt;">2</span>
</span>
</td>
<td id="S3.T1.6.6.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr ltx_border_t">
<span id="S3.T1.6.6.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.6.6.6.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.6.6.6.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/front_views/157_00090.png" id="S3.T1.6.6.6.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.7.7.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.7.7.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.7.7.7.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.7.7.7.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/rgb_recon_157_00090.png" id="S3.T1.7.7.7.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.8.8.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.8.8.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.8.8.8.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.8.8.8.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/depth_recon_157_00090.png" id="S3.T1.8.8.8.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.9.9.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.9.9.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.9.9.9.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.9.9.9.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/normals_recon_157_00090.png" id="S3.T1.9.9.9.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.10.10.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t">
<span id="S3.T1.10.10.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.10.10.10.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.10.10.10.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/mask_recon_157_00090.png" id="S3.T1.10.10.10.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
</tr>
<tr id="S3.T1.20.20" class="ltx_tr">
<td id="S3.T1.20.20.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.20.20.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.20.20.11.1.1" class="ltx_p" style="width:5.7pt;">3</span>
</span>
</td>
<td id="S3.T1.11.11.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr">
<span id="S3.T1.11.11.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.11.11.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.11.11.1.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/front_views/118_00270.png" id="S3.T1.11.11.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.12.12.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.12.12.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.12.12.2.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.12.12.2.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/rgb_recon_118_00270.png" id="S3.T1.12.12.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.13.13.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.13.13.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.13.13.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.13.13.3.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/depth_recon_118_00270.png" id="S3.T1.13.13.3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.14.14.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.14.14.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.14.14.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.14.14.4.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/normals_recon_118_00270.png" id="S3.T1.14.14.4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.15.15.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.15.15.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.15.15.5.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.15.15.5.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/mask_recon_118_00270.png" id="S3.T1.15.15.5.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.20.20.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.20.20.12.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.20.20.12.1.1" class="ltx_p" style="width:5.7pt;">4</span>
</span>
</td>
<td id="S3.T1.16.16.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr">
<span id="S3.T1.16.16.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.16.16.6.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.16.16.6.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/front_views/162_00090.png" id="S3.T1.16.16.6.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.17.17.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.17.17.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.17.17.7.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.17.17.7.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/rgb_recon_162_00090.png" id="S3.T1.17.17.7.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.18.18.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.18.18.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.18.18.8.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.18.18.8.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/depth_recon_162_00090.png" id="S3.T1.18.18.8.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.19.19.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.19.19.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.19.19.9.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.19.19.9.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/normals_recon_162_00090.png" id="S3.T1.19.19.9.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.20.20.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.20.20.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.20.20.10.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.20.20.10.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/mask_recon_162_00090.png" id="S3.T1.20.20.10.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
</tr>
<tr id="S3.T1.30.30" class="ltx_tr">
<td id="S3.T1.30.30.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r">
<span id="S3.T1.30.30.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.30.30.11.1.1" class="ltx_p" style="width:5.7pt;">5</span>
</span>
</td>
<td id="S3.T1.21.21.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr">
<span id="S3.T1.21.21.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.21.21.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.21.21.1.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/front_views/52_00180.png" id="S3.T1.21.21.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.22.22.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.22.22.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.22.22.2.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.22.22.2.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/rgb_recon_52_00180.png" id="S3.T1.22.22.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.23.23.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.23.23.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.23.23.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.23.23.3.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/depth_recon_52_00180.png" id="S3.T1.23.23.3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.24.24.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.24.24.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.24.24.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.24.24.4.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/normals_recon_52_00180.png" id="S3.T1.24.24.4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.25.25.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.25.25.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.25.25.5.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.25.25.5.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/mask_recon_52_00180.png" id="S3.T1.25.25.5.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.30.30.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.30.30.12.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.30.30.12.1.1" class="ltx_p" style="width:5.7pt;">6</span>
</span>
</td>
<td id="S3.T1.26.26.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_rr">
<span id="S3.T1.26.26.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.26.26.6.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.26.26.6.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/front_views/73_00270.png" id="S3.T1.26.26.6.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.27.27.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.27.27.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.27.27.7.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.27.27.7.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/rgb_recon_73_00270.png" id="S3.T1.27.27.7.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.28.28.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.28.28.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.28.28.8.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.28.28.8.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/depth_recon_73_00270.png" id="S3.T1.28.28.8.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.29.29.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.29.29.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.29.29.9.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.29.29.9.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/normals_recon_73_00270.png" id="S3.T1.29.29.9.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.30.30.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_r">
<span id="S3.T1.30.30.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.30.30.10.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.30.30.10.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/mask_recon_73_00270.png" id="S3.T1.30.30.10.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
</tr>
<tr id="S3.T1.40.40" class="ltx_tr">
<td id="S3.T1.40.40.11" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r">
<span id="S3.T1.40.40.11.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.40.11.1.1" class="ltx_p" style="width:5.7pt;">7</span>
</span>
</td>
<td id="S3.T1.31.31.1" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_rr">
<span id="S3.T1.31.31.1.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.31.31.1.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.31.31.1.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/front_views/67_00105.png" id="S3.T1.31.31.1.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.32.32.2" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.32.32.2.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.32.32.2.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.32.32.2.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/rgb_recon_67_00105.png" id="S3.T1.32.32.2.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.33.33.3" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.33.33.3.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.33.33.3.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.33.33.3.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/depth_recon_67_00105.png" id="S3.T1.33.33.3.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.34.34.4" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.34.34.4.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.34.34.4.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.34.34.4.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/normals_recon_67_00105.png" id="S3.T1.34.34.4.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.35.35.5" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.35.35.5.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.35.35.5.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.35.35.5.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/mask_recon_67_00105.png" id="S3.T1.35.35.5.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.40.40.12" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.40.40.12.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.40.12.1.1" class="ltx_p" style="width:5.7pt;">8</span>
</span>
</td>
<td id="S3.T1.36.36.6" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_rr">
<span id="S3.T1.36.36.6.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.36.36.6.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.36.36.6.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/front_views/46_00106.png" id="S3.T1.36.36.6.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.37.37.7" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.37.37.7.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.37.37.7.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.37.37.7.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/rgb_recon_46_00106.png" id="S3.T1.37.37.7.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.38.38.8" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.38.38.8.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.38.38.8.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.38.38.8.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/depth_recon_46_00106.png" id="S3.T1.38.38.8.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.39.39.9" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.39.39.9.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.39.39.9.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.39.39.9.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/normals_recon_46_00106.png" id="S3.T1.39.39.9.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
<td id="S3.T1.40.40.10" class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r">
<span id="S3.T1.40.40.10.1" class="ltx_inline-block ltx_align_top">
<span id="S3.T1.40.40.10.1.1" class="ltx_p" style="width:34.1pt;"><span id="S3.T1.40.40.10.1.1.1" class="ltx_text" style="position:relative; bottom:-0.5pt;"><img src="/html/2402.05593/assets/images/test_set/mask_recon_46_00106.png" id="S3.T1.40.40.10.1.1.1.g1" class="ltx_graphics ltx_img_square" width="55" height="55" alt="[Uncaptioned image]"></span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>
Preliminary results on our test set of unseen statues. We show the automatically generated sketch, and the outputs of our solution (RGB, depth, normals, mask). (License for Models 1-4, 6-8: <a target="_blank" href="https://creativecommons.org/licenses/by-nc/4.0/" title="" class="ltx_ref ltx_href">CC BY-NC 4.0</a>, sketchfab/noe-3d.at; 5: <a target="_blank" href="https://creativecommons.org/licenses/by/4.0/" title="" class="ltx_ref ltx_href">CC BY 4.0</a>, sketchfab/HarrisonHag1) </figcaption>
</figure>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">We build on the idea of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx13" title="" class="ltx_ref">LKL17</a>]</cite>, which predicts additional views, given an input photograph, estimates a mask plus depth per view and, fusing these predictions, generates a point cloud. Most importantly, by using the projection into 2d for learning, a purely geometric operation, they supervise the learning in an efficient fashion. While we have not yet decided on the final 3d reconstruction method, we adopt the strategy of not directly estimating 3d information. Instead, we require the decoder to come up with intermediary information we intend to use for 3d reconstruction. Also, we extend the capabilities of our network by, additonally to a one-hot image mask separating the figurine from the background, and a depth map, predicting surface normals, as well as an RGB reconstruction. Introducing multiple criterions stabilizes training, while also introducing additional queues to the network without requiring additional inputs at inference time. Altogether this allows us to train completely on synthetic data, not requiring any real data from the target domain. We show outputs of in Table <a href="#S3.T1" title="Table 1 ‣ 3 Approach ‣ A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. As for the 3d reconstruction stage, we suggest the use of differentiable rendering techniques of which an evaluation for our use case is in the works.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Data Generation</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">We download a set of 110 historical statues from
<a href="www.sketchfab.com" title="" class="ltx_ref ltx_href">sketchfab</a>. We do not filter by epoch, combining ancient greek, ancient roman, and medieval statues. Next, we render 360 orthographic views per statue, rotating 1° per view, using Blender, giving us the RGB, depth, normals, and mask.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Image to Image Translation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Only using the gradients of our renderings (canny edge detection, laplacian, or mixture of gaussians) does - in our tests - not suffice to make them indistinguishable to our sinopia images. The remaining domain gap is large enough to make our estimation algorithms work well on our training data, but not on our real world images. Therefore we looked into image-to-image translation techniques. 
<br class="ltx_break">Instead of relying on a fixed function, such as the above mentioned edge filters or similar ones, such as in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx17" title="" class="ltx_ref">RDPS18</a>]</cite>, we turned to style transfer techniques popularized by works such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx4" title="" class="ltx_ref">GEB16</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx8" title="" class="ltx_ref">HB17</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx11" title="" class="ltx_ref">KLA19</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx23" title="" class="ltx_ref">ZPIE17</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx19" title="" class="ltx_ref">WLZ<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>18</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx18" title="" class="ltx_ref">RPK20</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx14" title="" class="ltx_ref">LX22</a>]</cite>. Using a learned non-linear function greatly increases the expressiveness of our intermediary sketch domain, while preventing a linear 1-to-1 mapping given the right loss function. We rely on the very recent <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx1" title="" class="ltx_ref">CDI22</a>]</cite> for our sketch generation, which explicitly tackles the encoding of 3d shape via a geometric loss function.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Encoder Decoder Network</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">To make up for our low data quality we need an expressive model that can be primed with knowledge from many examples, while being able to generalize to unseen statues. Among the possible architectures to address this challenge, (conditional) GANs and autoencoders stand out. We choose to build an encoder-decoder architecture, but add an adversarial component via an additional classification output in the decoder, together with a gradient reversal layer. This technique, as shown in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx5" title="" class="ltx_ref">GL15</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx22" title="" class="ltx_ref">ZKI19</a>]</cite>, discourages the encoder to distinguish between different inputs (e.g. different statues), regularizing the weights and increasing generalization between different figurines. Next we describe our architecture in more detail.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2402.05593/assets/images/gch-figure2_0001.jpg" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="100" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span> Preliminary qualitative results on our sinopia. Row 1 shows all outputs of our process, from left to right: First, restoration by an expert (Prof. Dr. Stiegemann), used as input to our network, followed by the mask, normals, depth and color estimate of our approach. Second row shows input-output pairs of additional sketches and their color reconstruction. </figcaption>
</figure>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Architectural details</span>.
We use an encoder network to extract a powerful representation of a single input sketch depicting the target object. Our encoder leverages a residual design similar to those used in state-of-the-art work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bibx10" title="" class="ltx_ref">KAL<sup class="ltx_sup"><span class="ltx_text ltx_font_italic">∗</span></sup>21</a>]</cite> projecting our 640x640 pixel-sized images into a vector space of 2050 dimensions. Similarly, the decoder uses a residual design to generate RGB, depth, normals, and mask images given samples from this latent space, sharing the bigger portion of parameters, before splitting into different heads, one for each output modality and one additional for the classifier, required for our gradient reversal regularization.
<br class="ltx_break"><span id="S3.SS3.p2.1.2" class="ltx_text ltx_font_bold">Training details</span>.
We use 110 statues to generate 39.600 samples, each consisting of RGB, depth, normals, and mask. We split the set into 91 statues for training (32.760 images), 10 for validation (3.600 images), and 9 for testing (3.240 images). Aside of gradient reversal, we added dropout, and varied our choice as well as scaling of our individual loss terms.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Preliminary Results</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Our network has to generalize to unseen objects in order to solve the problem as presented. Therefore we only test on unseen objects. First, we present preliminary results on our test split, converted renderings, before applying our technique on restorations of historic sinopia. First we present qualitative results (Table <a href="#S3.T1" title="Table 1 ‣ 3 Approach ‣ A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) on renderings from unseen objects. Next we evaluate our model qualitatively on six frontal views of our sinopia (Figure <a href="#S3.F2" title="Figure 2 ‣ 3.3 Encoder Decoder Network ‣ 3 Approach ‣ A Concept for Reconstructing Stucco Statues from historic Sketches using synthetic Data only" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>).
On our synthetic data we see a strong shape reconstruction for statues similar to our training set (humanoid statues from ancient Rome, ancient Greece and the Middle Ages) in all images but 5 and 7. 5, being a modern, non-human figure, strongly differs in shape and style from human statues, 7 contains a lot of unseen detail (the cross). Since details of the training set are transferred to unseen samples, one might want to focus on statues with similar appearance to the target domain. Results on real data show general shape reconstruction, but a lack of detail such as found in the clothing.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion and Future Work</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We presented a pipeline for 3d geometry reconstruction based on historic drawings. We demonstrated the (semi-)automated reconstruction of RGB, depth, normals, and object mask based only on six medieval sinopia and without requiring any additional sinopia from that or any other time period. We produce all our data via rendering, process the renderings to make them less distinguishable from our target data and finally train an encoder-decoder architecture. Future work encompasses the extension to full 3d reconstruction, based on the predicted information, as well as evaluating the approach on a set of statues consisting only of samples from the era. The second point re-introduces the need for domain knowledge, but should drastically improve the plausability of results. Also, the process as presented can be applied to non-sketch inputs, such as drawings, paintings, (historic) photographs and similar.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bibx1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CDI22]</span>
<span class="ltx_bibblock">
<span id="bib.bibx1.1.1" class="ltx_text ltx_font_smallcaps">Chan C., Durand F., Isola P.</span>:

</span>
<span class="ltx_bibblock">Learning to generate line drawings that convey geometry and
semantics.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx1.2.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em> (2022), pp. 7915–7925.

</span>
</li>
<li id="bib.bibx2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[CS07]</span>
<span class="ltx_bibblock">
<span id="bib.bibx2.1.1" class="ltx_text ltx_font_smallcaps">Claussen H., Skriver A.</span>:

</span>
<span class="ltx_bibblock"><em id="bib.bibx2.2.1" class="ltx_emph ltx_font_italic">Die Klosterkirche Corvey Band 2: Wandmalerei und Stuck aus
karolingischer Zeit</em>.

</span>
<span class="ltx_bibblock">Zabern Philipp Von Gmbh, 2007.

</span>
</li>
<li id="bib.bibx3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[FKP<sup id="bib.bibx3.4.4.1" class="ltx_sup"><span id="bib.bibx3.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>20]</span>
<span class="ltx_bibblock">
<span id="bib.bibx3.7.1" class="ltx_text ltx_font_smallcaps">Fiorucci M., Khoroshiltseva M., Pontil M., Traviglia A., Del Bue A.,
James S.</span>:

</span>
<span class="ltx_bibblock">Machine learning for cultural heritage: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bibx3.8.1" class="ltx_emph ltx_font_italic">Pattern Recognition Letters 133</em> (2020), 102–108.

</span>
<span class="ltx_bibblock">URL:
<a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0167865520300532" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0167865520300532</a>,
<a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.patrec.2020.02.017" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:https://doi.org/10.1016/j.patrec.2020.02.017</span></a>.

</span>
</li>
<li id="bib.bibx4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[GEB16]</span>
<span class="ltx_bibblock">
<span id="bib.bibx4.1.1" class="ltx_text ltx_font_smallcaps">Gatys L. A., Ecker A. S., Bethge M.</span>:

</span>
<span class="ltx_bibblock">Image style transfer using convolutional neural networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx4.2.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (CVPR)</em> (June 2016).

</span>
</li>
<li id="bib.bibx5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[GL15]</span>
<span class="ltx_bibblock">
<span id="bib.bibx5.1.1" class="ltx_text ltx_font_smallcaps">Ganin Y., Lempitsky V.</span>:

</span>
<span class="ltx_bibblock">Unsupervised domain adaptation by backpropagation.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx5.2.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em> (2015), PMLR,
pp. 1180–1189.

</span>
</li>
<li id="bib.bibx6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[GRS14]</span>
<span class="ltx_bibblock">
<span id="bib.bibx6.1.1" class="ltx_text ltx_font_smallcaps">Gomes L., Regina Pereira Bellon O., Silva L.</span>:

</span>
<span class="ltx_bibblock">3d reconstruction methods for digital preservation of cultural
heritage: A survey.

</span>
<span class="ltx_bibblock"><em id="bib.bibx6.2.1" class="ltx_emph ltx_font_italic">Pattern Recognition Letters 50</em> (2014), 3–14.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/https://doi.org/10.1016/j.patrec.2014.03.023" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:https://doi.org/10.1016/j.patrec.2014.03.023</span></a>.

</span>
</li>
<li id="bib.bibx7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[GSP<sup id="bib.bibx7.4.4.1" class="ltx_sup"><span id="bib.bibx7.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>14]</span>
<span class="ltx_bibblock">
<span id="bib.bibx7.7.1" class="ltx_text ltx_font_smallcaps">Gregor R., Sipiran I., Papaioannou G., Schreck T., Andreadis A.,
Mavridis P.</span>:

</span>
<span class="ltx_bibblock">Towards automated 3d reconstruction of defective cultural heritage
objects.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx7.8.1" class="ltx_emph ltx_font_italic">GCH</em> (2014), Citeseer, pp. 135–144.

</span>
</li>
<li id="bib.bibx8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[HB17]</span>
<span class="ltx_bibblock">
<span id="bib.bibx8.1.1" class="ltx_text ltx_font_smallcaps">Huang X., Belongie S.</span>:

</span>
<span class="ltx_bibblock">Arbitrary style transfer in real-time with adaptive instance
normalization.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx8.2.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE international conference on computer
vision</em> (2017), pp. 1501–1510.

</span>
</li>
<li id="bib.bibx9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[JJHZ19]</span>
<span class="ltx_bibblock">
<span id="bib.bibx9.1.1" class="ltx_text ltx_font_smallcaps">Jiang Y., Ji D., Han Z., Zwicker M.</span>:

</span>
<span class="ltx_bibblock">Sdfdiff: Differentiable rendering of signed distance fields for 3d
shape optimization.

</span>
<span class="ltx_bibblock"><em id="bib.bibx9.2.1" class="ltx_emph ltx_font_italic">CoRR abs/1912.07109</em> (2019).

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/1912.07109" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1912.07109</a>, <a target="_blank" href="http://arxiv.org/abs/1912.07109" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1912.07109</span></a>.

</span>
</li>
<li id="bib.bibx10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[KAL<sup id="bib.bibx10.4.4.1" class="ltx_sup"><span id="bib.bibx10.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>21]</span>
<span class="ltx_bibblock">
<span id="bib.bibx10.7.1" class="ltx_text ltx_font_smallcaps">Karras T., Aittala M., Laine S., Härkönen E., Hellsten J.,
Lehtinen J., Aila T.</span>:

</span>
<span class="ltx_bibblock">Alias-free generative adversarial networks.

</span>
<span class="ltx_bibblock"><em id="bib.bibx10.8.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems 34</em> (2021),
852–863.

</span>
</li>
<li id="bib.bibx11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[KLA19]</span>
<span class="ltx_bibblock">
<span id="bib.bibx11.1.1" class="ltx_text ltx_font_smallcaps">Karras T., Laine S., Aila T.</span>:

</span>
<span class="ltx_bibblock">A style-based generator architecture for generative adversarial
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx11.2.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF conference on computer vision
and pattern recognition</em> (2019), pp. 4401–4410.

</span>
</li>
<li id="bib.bibx12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[LGK<sup id="bib.bibx12.4.4.1" class="ltx_sup"><span id="bib.bibx12.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>17]</span>
<span class="ltx_bibblock">
<span id="bib.bibx12.7.1" class="ltx_text ltx_font_smallcaps">Lun Z., Gadelha M., Kalogerakis E., Maji S., Wang R.</span>:

</span>
<span class="ltx_bibblock">3d shape reconstruction from sketches via multi-view convolutional
networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx12.8.1" class="ltx_emph ltx_font_italic">2017 International Conference on 3D Vision (3DV)</em> (2017),
IEEE, pp. 67–77.

</span>
</li>
<li id="bib.bibx13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[LKL17]</span>
<span class="ltx_bibblock">
<span id="bib.bibx13.1.1" class="ltx_text ltx_font_smallcaps">Lin C., Kong C., Lucey S.</span>:

</span>
<span class="ltx_bibblock">Learning efficient point cloud generation for dense 3d object
reconstruction.

</span>
<span class="ltx_bibblock"><em id="bib.bibx13.2.1" class="ltx_emph ltx_font_italic">CoRR abs/1706.07036</em> (2017).

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="http://arxiv.org/abs/1706.07036" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1706.07036</a>, <a target="_blank" href="http://arxiv.org/abs/1706.07036" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:1706.07036</span></a>.

</span>
</li>
<li id="bib.bibx14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[LX22]</span>
<span class="ltx_bibblock">
<span id="bib.bibx14.1.1" class="ltx_text ltx_font_smallcaps">Li Y., Xu W.</span>:

</span>
<span class="ltx_bibblock">Using cyclegan to achieve the sketch recognition process of
sketch-based modeling.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx14.2.1" class="ltx_emph ltx_font_italic">Proceedings of the 2021 DigitalFUTURES</em> (2022), Yuan P. F.,
Chai H., Yan C., Leach N., (Eds.), Springer, pp. 26–34.

</span>
</li>
<li id="bib.bibx15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[NJJ21]</span>
<span class="ltx_bibblock">
<span id="bib.bibx15.1.1" class="ltx_text ltx_font_smallcaps">Nicolet B., Jacobson A., Jakob W.</span>:

</span>
<span class="ltx_bibblock">Large steps in inverse rendering of geometry.

</span>
<span class="ltx_bibblock"><em id="bib.bibx15.2.1" class="ltx_emph ltx_font_italic">ACM Transactions on Graphics (TOG) 40</em>, 6 (2021), 1–13.

</span>
</li>
<li id="bib.bibx16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[Poe02]</span>
<span class="ltx_bibblock">
<span id="bib.bibx16.1.1" class="ltx_text ltx_font_smallcaps">Poeschke J.</span>:

</span>
<span class="ltx_bibblock"><em id="bib.bibx16.2.1" class="ltx_emph ltx_font_italic">Sinopien und Stuck im Westwerk der karolingischen Klosterkirche
von Corvey</em>.

</span>
<span class="ltx_bibblock">Rhema, 2002.

</span>
</li>
<li id="bib.bibx17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RDPS18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx17.1.1" class="ltx_text ltx_font_smallcaps">Rambach J., Deng C., Pagani A., Stricker D.</span>:

</span>
<span class="ltx_bibblock">Learning 6dof object poses from synthetic single channel images.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx17.2.1" class="ltx_emph ltx_font_italic">2018 IEEE International Symposium on Mixed and Augmented
Reality Adjunct (ISMAR-Adjunct)</em> (2018), IEEE, pp. 164–169.

</span>
</li>
<li id="bib.bibx18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[RPK20]</span>
<span class="ltx_bibblock">
<span id="bib.bibx18.1.1" class="ltx_text ltx_font_smallcaps">Rojtberg P., Pöllabauer T., Kuijper A.</span>:

</span>
<span class="ltx_bibblock">Style-transfer gans for bridging the domain gap in synthetic pose
estimator training.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx18.2.1" class="ltx_emph ltx_font_italic">2020 IEEE International Conference on Artificial
Intelligence and Virtual Reality (AIVR)</em> (2020), IEEE, pp. 188–195.

</span>
</li>
<li id="bib.bibx19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[WLZ<sup id="bib.bibx19.4.4.1" class="ltx_sup"><span id="bib.bibx19.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>18]</span>
<span class="ltx_bibblock">
<span id="bib.bibx19.7.1" class="ltx_text ltx_font_smallcaps">Wang T.-C., Liu M.-Y., Zhu J.-Y., Tao A., Kautz J., Catanzaro B.</span>:

</span>
<span class="ltx_bibblock">High-resolution image synthesis and semantic manipulation with
conditional gans.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx19.8.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</em> (2018).

</span>
</li>
<li id="bib.bibx20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[WYZ<sup id="bib.bibx20.4.4.1" class="ltx_sup"><span id="bib.bibx20.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>20]</span>
<span class="ltx_bibblock">
<span id="bib.bibx20.7.1" class="ltx_text ltx_font_smallcaps">Wang F., Yang Y., Zhao B., Jiang J., Zhou T., Jiang D., Cai T.</span>:

</span>
<span class="ltx_bibblock">Deep 3d shape reconstruction from single-view sketch image.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx20.8.1" class="ltx_emph ltx_font_italic">2020 8th International Conference on Digital Home (ICDH)</em>
(2020), pp. 184–189.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://doi.org/10.1109/ICDH51081.2020.00039" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">doi:10.1109/ICDH51081.2020.00039</span></a>.

</span>
</li>
<li id="bib.bibx21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[XYZ<sup id="bib.bibx21.4.4.1" class="ltx_sup"><span id="bib.bibx21.4.4.1.1" class="ltx_text ltx_font_italic">∗</span></sup>20]</span>
<span class="ltx_bibblock">
<span id="bib.bibx21.7.1" class="ltx_text ltx_font_smallcaps">Xie H., Yao H., Zhang S., Zhou S., Sun W.</span>:

</span>
<span class="ltx_bibblock">Pix2vox++: Multi-scale context-aware 3d object reconstruction from
single and multiple images.

</span>
<span class="ltx_bibblock"><em id="bib.bibx21.8.1" class="ltx_emph ltx_font_italic">CoRR abs/2006.12250</em> (2020).

</span>
<span class="ltx_bibblock">URL: <a target="_blank" href="https://arxiv.org/abs/2006.12250" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2006.12250</a>, <a target="_blank" href="http://arxiv.org/abs/2006.12250" title="" class="ltx_ref ltx_href"><span class="ltx_ref ltx_nolink ltx_path ltx_font_typewriter">arXiv:2006.12250</span></a>.

</span>
</li>
<li id="bib.bibx22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[ZKI19]</span>
<span class="ltx_bibblock">
<span id="bib.bibx22.1.1" class="ltx_text ltx_font_smallcaps">Zakharov S., Kehl W., Ilic S.</span>:

</span>
<span class="ltx_bibblock">Deceptionnet: Network-driven domain randomization.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx22.2.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF International Conference on
Computer Vision (ICCV)</em> (October 2019).

</span>
</li>
<li id="bib.bibx23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[ZPIE17]</span>
<span class="ltx_bibblock">
<span id="bib.bibx23.1.1" class="ltx_text ltx_font_smallcaps">Zhu J.-Y., Park T., Isola P., Efros A. A.</span>:

</span>
<span class="ltx_bibblock">Unpaired image-to-image translation using cycle-consistent
adversarial networks.

</span>
<span class="ltx_bibblock">In <em id="bib.bibx23.2.1" class="ltx_emph ltx_font_italic">Computer Vision (ICCV), 2017 IEEE International Conference
on</em> (2017).

</span>
</li>
</ul>
</section><div class="ltx_rdf" about="" property="dcterms:creator" content="{\@shortauthor}"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="{Computer Graphics Forum"></div>
<div class="ltx_rdf" about="" property="dcterms:subject" content="{\pdf@Subject}"></div>
<div class="ltx_rdf" about="" property="dcterms:title" content="{\@shorttitle}"></div>

</article>
</div>
<div class="ar5iv-footer"><a href="/html/2402.05592" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2402.05593" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2402.05593">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2402.05593" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2402.05594" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Tue Mar  5 18:13:00 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
