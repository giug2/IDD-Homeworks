<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2403.12044] Mobile Application for Oral Disease Detection using Federated Learning</title><meta property="og:description" content="The mouth, often regarded as a window to the internal state of the body, plays an important role in reflecting oneâ€™s overall health. Poor oral hygiene has far-reaching consequences, contributing to severe conditions liâ€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Mobile Application for Oral Disease Detection using Federated Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Mobile Application for Oral Disease Detection using Federated Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2403.12044">

<!--Generated on Fri Apr  5 17:54:15 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="keywords" lang="en" content="
Progressive Web Application,  Federated Learning,  Object Detection
">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Mobile Application for Oral Disease Detection using Federated Learning
<br class="ltx_break">
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shankara Narayanan V
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id1.1.id1" class="ltx_text ltx_font_italic">Department of Computer Science and Engineering, </span>
<br class="ltx_break"><span id="id2.2.id2" class="ltx_text ltx_font_italic">Amrita School of Computing, Coimbatore,
<br class="ltx_break"></span>Amrita Vishwa Vidyapeetham, India 
<br class="ltx_break">cb.en.u4cse20656@cb.students.amrita.edu
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sneha Varsha M
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id3.1.id1" class="ltx_text ltx_font_italic">Department of Computer Science and Engineering,</span>
<br class="ltx_break"><span id="id4.2.id2" class="ltx_text ltx_font_italic">Amrita School of Computing, Coimbatore,
<br class="ltx_break"></span>Amrita Vishwa Vidyapeetham, India 
<br class="ltx_break">cb.en.u4cse20659@cb.students.amrita.edu
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Syed Ashfaq Ahmed
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id5.1.id1" class="ltx_text ltx_font_italic">Department of Computer Science and Engineering,</span>
<br class="ltx_break"><span id="id6.2.id2" class="ltx_text ltx_font_italic">Amrita School of Computing, Coimbatore,
<br class="ltx_break"></span>Amrita Vishwa Vidyapeetham, India 
<br class="ltx_break">cb.en.u4cse20665@cb.students.amrita.edu
</span></span></span>
<span class="ltx_author_before">â€ƒâ€ƒ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Guruprakash J
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span id="id7.1.id1" class="ltx_text ltx_font_italic">Department of Computer Science and Engineering,</span>
<br class="ltx_break"><span id="id8.2.id2" class="ltx_text ltx_font_italic">Amrita School of Computing, Coimbatore,
<br class="ltx_break"></span>Amrita Vishwa Vidyapeetham, India 
<br class="ltx_break">j_guruprakash@cb.amrita.edu
</span></span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id9.id1" class="ltx_p">The mouth, often regarded as a window to the internal state of the body, plays an important role in reflecting oneâ€™s overall health. Poor oral hygiene has far-reaching consequences, contributing to severe conditions like heart disease, cancer, and diabetes, while inadequate care leads to discomfort, pain, and costly treatments. Federated Learning (FL) for object detection can be utilized for this use case due to the sensitivity of the oral image data of the patients. FL ensures data privacy by storing the images used for object detection on the local device and trains the model on the edge. The updated weights are federated to a central server where all the collected weights are updated via The Federated Averaging algorithm. Finally, we have developed a mobile app named OralH which provides user-friendly solutions, allowing people to conduct self-assessments through mouth scans and providing quick oral health insights. Upon detection of the issues, the application alerts the user about potential oral health concerns or diseases and provides details about dental clinics in the userâ€™s locality. Designed as a Progressive Web Application (PWA), the platform ensures ubiquitous access, catering to users across devices for a seamless experience. The application aims to provide state-of-the-art segmentation and detection techniques, leveraging the YOLOv8 object detection model to identify oral hygiene issues and diseases. This study deals with the benefits of leveraging FL in healthcare with promising real-world results.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Progressive Web Application, Federated Learning, Object Detection

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span id="S1.1.1" class="ltx_text ltx_font_smallcaps">Introduction</span>
</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Oral health is an essential part of a personâ€™s overall health and well-being. It is taught to children from a tender age to follow acts in order to maintain oral health. Oral health involves the healthiness of gums, palate, linings of mouth and throat, tongue, lips, salivary glands, nerves, chewing muscles, and bones of upper and lower jaws<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>. Studies have indicated that negligence to oral health can result in possible associations between chronic oral infections and diabetes, stroke, heart, and lung disease, and low birth weight or premature births <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. So, it is important to monitor our Oral health and practice Oral hygiene.
Currently, Oral health is monitored predominantly through routine dental examinations where dentists visually inspect the mouth for issues. But this practice of visiting dentists regularly is not followed by a huge number of the population. This is attributed to the fact that potential patients do not have easier access to dentists and dental clinics as they are not distributed evenly <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Detecting ailments at an earlier juncture holds the potential for enhanced treatment outcomes, increased chances of recovery, or extended periods of survival<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
This creates a requirement for technology that will help people monitor their oral health without much effort and time. Thereâ€™s been a rise in the development of mobile apps and wearable devices designed to help individuals monitor their oral health. These apps might remind users to brush and floss regularly, track their oral hygiene habits, and even provide educational content. However, the development of an application that will scan peopleâ€™s mouths, detect potential threats, and suggest dentists in the neighborhood will definitely prove to be very useful. This poses a need for deep learning and computer vision techniques that include image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> and object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2403.12044/assets/Figures/Untitled_Diagram.drawio.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="299" height="197" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Federated Learning Implementation.</figcaption>
</figure>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Computer Vision is an important tool used in various industries like the agriculture industry for analyzing grain quality, the security industry for face recognition, the automobile industry for self-driving cars, and even the medical industry for diagnostic imaging <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. Integration of computer vision with deep learning models can potentially help in diagnosing medical images to know potential threats. However, it is important to tackle the issue of data privacy when using medical data to train these models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>. This leads to introducing the concept of federated learning where data remains on the clientâ€™s local device and is trained on local models. The weights obtained from training these local models are then sent to the server and aggregated. The aggregated global model is sent back to the clients for future training. This approach preserves data privacy by keeping sensitive information on the clientâ€™s side.
The inclusion of a user interface along with these techniques will help in building full-fledged applications that help users examine their oral health and hygiene <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. By integrating features like dental clinic recommendations and articles on oral health, such applications can offer assistance to people in various ways.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span id="S2.1.1" class="ltx_text ltx_font_smallcaps">Related Works</span>
</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">The model discussed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> gives a federated learning approach to a real-world image dataset created by them which consisted of images taken from street cameras and had a total of seven recognizable objects. The paper explores the YOLOv3 and Faster R-CNN models for object detection purposes and compares the performance of the two. The model uses a modified FederatedAveraging (FedAvg) algorithm done by introducing checkpoint saving and restoring on hard devices. This reduces the complication of model aggregation processing.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">The model proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> classifies breast density using varied and diverse data with the help of federated learning. The federated algorithm used in this model is Federated Averaging. The study reported an average of 6.3% better performance against a model trained on a local dataset. Additionally, the modelâ€™s generalizability was reported to have increased by an average of 45.8% when evaluated on other unknown testing datasets.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span id="S3.1.1" class="ltx_text ltx_font_smallcaps">Experiments</span>
</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">In this section, we will discuss the various experiments conducted, which include disease detection using the object detection frameworks YOLOv5 and YOLOv8, followed by federated learning and lastly constructing a mobile application wherein users will be able to click photos of their mouths and the model would detect diseases using the object detection model which would be modified and improved using federated learning.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS1.4.1.1" class="ltx_text">III-A</span> </span><span id="S3.SS1.5.2" class="ltx_text ltx_font_italic">Dataset Description</span>
</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The dental condition dataset is a collection of images curated specifically for dental research and analysis. The dataset comprises a total of 1058 images and four classes encompassing a wide range of dental conditions including Caries, Gingivitis, tooth discoloration, and ulcers. The images are sourced from multiple hospitals and dental websites. This is made to ensure the authenticity and diversity of the dental conditions.
The dataset is curated using MakeSense AI, which provides a user-friendly interface for annotating and augmenting images. The labeling of data is done in YOLO labeling format with object class references mentioned in Table <a href="#S3.T1" title="TABLE I â€£ III-A Dataset Description â€£ III Experiments â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.
Furthermore, the dataset is divided into 1465 training images and 43 testing images as displayed in Table <a href="#S3.T1" title="TABLE I â€£ III-A Dataset Description â€£ III Experiments â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure id="S3.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Class Counts for training and testing data</figcaption>
<table id="S3.T1.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S3.T1.1.1.1" class="ltx_tr">
<th id="S3.T1.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.1.1.1.1.1" class="ltx_text">Class</span></th>
<th id="S3.T1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_t" rowspan="2"><span id="S3.T1.1.1.1.2.1" class="ltx_text">Object</span></th>
<th id="S3.T1.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2">Instances</th>
</tr>
<tr id="S3.T1.1.2.2" class="ltx_tr">
<th id="S3.T1.1.2.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Training</th>
<th id="S3.T1.1.2.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Testing</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S3.T1.1.3.1" class="ltx_tr">
<th id="S3.T1.1.3.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">0</th>
<th id="S3.T1.1.3.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Caries</th>
<td id="S3.T1.1.3.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">2051</td>
<td id="S3.T1.1.3.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">35</td>
</tr>
<tr id="S3.T1.1.4.2" class="ltx_tr">
<th id="S3.T1.1.4.2.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">1</th>
<th id="S3.T1.1.4.2.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Ulcer</th>
<td id="S3.T1.1.4.2.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">364</td>
<td id="S3.T1.1.4.2.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
</tr>
<tr id="S3.T1.1.5.3" class="ltx_tr">
<th id="S3.T1.1.5.3.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t">2</th>
<th id="S3.T1.1.5.3.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t">Tooth Discoloration</th>
<td id="S3.T1.1.5.3.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">315</td>
<td id="S3.T1.1.5.3.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">18</td>
</tr>
<tr id="S3.T1.1.6.4" class="ltx_tr">
<th id="S3.T1.1.6.4.1" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t">3</th>
<th id="S3.T1.1.6.4.2" class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_r ltx_border_t">Gingivitis</th>
<td id="S3.T1.1.6.4.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">653</td>
<td id="S3.T1.1.6.4.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">106</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2403.12044/assets/Figures/hiii.drawio.png" id="S3.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="479" height="236" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Oral Disease Detection Pipeline</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS2.4.1.1" class="ltx_text">III-B</span> </span><span id="S3.SS2.5.2" class="ltx_text ltx_font_italic">Object Detection</span>
</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">The main idea behind object detection is to localize a region of interest and giving a class to this region like an image classifier. The presence of many regions of interest makes it an advanced problem of image classification. YOLO (You Only Look Once) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is a model significantly known for its object detection accuracy and speed, and is often used for multiple object detection <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. It uses an end-to-end neural network that makes predictions of class probabilities and bounding boxes at once. YOLO takes an input image and divides it into a P*P grid. Given the grid cell contains the center of an object, that particular grid cell is responsible for detecting that object. Every grid cell has the possibility to predict Q bounding boxes and can give confidence scores for the boundary boxes. A post-processing step called Non-Maximum Suppression (NMS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> is incorporated in YOLO to improve the efficiency of object detection. It is used to distinguish and eliminate incorrect and redundant bounding boxes to output a single bounding box for each image object. The YOLO modelâ€™s performance in the detection of real-world data is the primary reason for selecting this object detection framework over other existing models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.
YOLOv5 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite> and YOLOv8 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> models are used in this application to perform object detection. YOLO models which do single-shot object detection are better compared to two-shot object detection models like Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite> for this particular application. In general, single-shot object detection is suited better for real-time applications when compared to two-shot object detection models. As the concept of federated learning is also incorporated, it is important to perform object detection faster in order to avoid latency. Hence, the single-shot object detection model, YOLO, was chosen for this particular use-case due to its suitability when compared to RCNN-based models that follow two-shot object detection.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS3.4.1.1" class="ltx_text">III-C</span> </span><span id="S3.SS3.5.2" class="ltx_text ltx_font_italic">Federated Learning</span>
</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">Federated Learning is an innovative framework in the field of machine learning and artificial intelligence that proposes a mechanism to tackle problems in artificial intelligence such as data privacy and scalability. Groundbreaking work has been achieved in recent times to produce relevant and necessary advancements in this field that have profound impacts on society such as <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> which uses federated learning practices to predict the spread of the COVID-19 pandemic in India and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> which uses autonomous vehicle data to predict steering angle and discusses the impact of the communication a vehicle has with its surroundings.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p">The model we propose uses a client-server-based federated learning approach, wherein the model is trained locally on the data collected by each client. Upon completion of a round of local training, the modified model weights are forwarded to the server for aggregation. Following aggregation, the updated weights are distributed to the clients to start the next round of training. Models at each client converge after the completion of a certain number of FL rounds, where the client chooses the locally best model based on a performance metric. The best model will be chosen by comparing the performance metric of the global model sent by the server and the intermediate local models trained on the client side. The federated averaging algorithm as proposed by <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> was used for model weights upgradation. The three primary parameters of the FedAvg algorithm that affect the performance are:</p>
<ul id="S3.I1" class="ltx_itemize">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Number of Clients (C)</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">Number of tries performed on the local dataset by each client during a round (E)</p>
</div>
</li>
<li id="S3.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S3.I1.i3.p1" class="ltx_para">
<p id="S3.I1.i3.p1.1" class="ltx_p">Minibatch size used for client updates (B)</p>
</div>
</li>
</ul>
<p id="S3.SS3.p2.2" class="ltx_p">Throughout the experiment, the number of clients(C) used was 5 and the minibatch size(B) was maintained at 1. As illustrated in Fig. <a href="#S1.F1" title="Figure 1 â€£ I Introduction â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> the server communicates with the clients and uses local weights of the client to update the central model using the federated averaging algorithm.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.3" class="ltx_Math" alttext="f(w)=\sum_{k=1}^{K}\left(\frac{n_{k}}{n}\right)\cdot F_{k}(w)" display="block"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.4" xref="S3.E1.m1.3.4.cmml"><mrow id="S3.E1.m1.3.4.2" xref="S3.E1.m1.3.4.2.cmml"><mi id="S3.E1.m1.3.4.2.2" xref="S3.E1.m1.3.4.2.2.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.2.1" xref="S3.E1.m1.3.4.2.1.cmml">â€‹</mo><mrow id="S3.E1.m1.3.4.2.3.2" xref="S3.E1.m1.3.4.2.cmml"><mo stretchy="false" id="S3.E1.m1.3.4.2.3.2.1" xref="S3.E1.m1.3.4.2.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">w</mi><mo stretchy="false" id="S3.E1.m1.3.4.2.3.2.2" xref="S3.E1.m1.3.4.2.cmml">)</mo></mrow></mrow><mo rspace="0.111em" id="S3.E1.m1.3.4.1" xref="S3.E1.m1.3.4.1.cmml">=</mo><mrow id="S3.E1.m1.3.4.3" xref="S3.E1.m1.3.4.3.cmml"><munderover id="S3.E1.m1.3.4.3.1" xref="S3.E1.m1.3.4.3.1.cmml"><mo movablelimits="false" rspace="0em" id="S3.E1.m1.3.4.3.1.2.2" xref="S3.E1.m1.3.4.3.1.2.2.cmml">âˆ‘</mo><mrow id="S3.E1.m1.3.4.3.1.2.3" xref="S3.E1.m1.3.4.3.1.2.3.cmml"><mi id="S3.E1.m1.3.4.3.1.2.3.2" xref="S3.E1.m1.3.4.3.1.2.3.2.cmml">k</mi><mo id="S3.E1.m1.3.4.3.1.2.3.1" xref="S3.E1.m1.3.4.3.1.2.3.1.cmml">=</mo><mn id="S3.E1.m1.3.4.3.1.2.3.3" xref="S3.E1.m1.3.4.3.1.2.3.3.cmml">1</mn></mrow><mi id="S3.E1.m1.3.4.3.1.3" xref="S3.E1.m1.3.4.3.1.3.cmml">K</mi></munderover><mrow id="S3.E1.m1.3.4.3.2" xref="S3.E1.m1.3.4.3.2.cmml"><mrow id="S3.E1.m1.3.4.3.2.2" xref="S3.E1.m1.3.4.3.2.2.cmml"><mrow id="S3.E1.m1.3.4.3.2.2.2.2" xref="S3.E1.m1.2.2.cmml"><mo id="S3.E1.m1.3.4.3.2.2.2.2.1" xref="S3.E1.m1.2.2.cmml">(</mo><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><msub id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml"><mi id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.2.cmml">n</mi><mi id="S3.E1.m1.2.2.2.3" xref="S3.E1.m1.2.2.2.3.cmml">k</mi></msub><mi id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">n</mi></mfrac><mo rspace="0.055em" id="S3.E1.m1.3.4.3.2.2.2.2.2" xref="S3.E1.m1.2.2.cmml">)</mo></mrow><mo rspace="0.222em" id="S3.E1.m1.3.4.3.2.2.1" xref="S3.E1.m1.3.4.3.2.2.1.cmml">â‹…</mo><msub id="S3.E1.m1.3.4.3.2.2.3" xref="S3.E1.m1.3.4.3.2.2.3.cmml"><mi id="S3.E1.m1.3.4.3.2.2.3.2" xref="S3.E1.m1.3.4.3.2.2.3.2.cmml">F</mi><mi id="S3.E1.m1.3.4.3.2.2.3.3" xref="S3.E1.m1.3.4.3.2.2.3.3.cmml">k</mi></msub></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.4.3.2.1" xref="S3.E1.m1.3.4.3.2.1.cmml">â€‹</mo><mrow id="S3.E1.m1.3.4.3.2.3.2" xref="S3.E1.m1.3.4.3.2.cmml"><mo stretchy="false" id="S3.E1.m1.3.4.3.2.3.2.1" xref="S3.E1.m1.3.4.3.2.cmml">(</mo><mi id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml">w</mi><mo stretchy="false" id="S3.E1.m1.3.4.3.2.3.2.2" xref="S3.E1.m1.3.4.3.2.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.4.cmml" xref="S3.E1.m1.3.4"><eq id="S3.E1.m1.3.4.1.cmml" xref="S3.E1.m1.3.4.1"></eq><apply id="S3.E1.m1.3.4.2.cmml" xref="S3.E1.m1.3.4.2"><times id="S3.E1.m1.3.4.2.1.cmml" xref="S3.E1.m1.3.4.2.1"></times><ci id="S3.E1.m1.3.4.2.2.cmml" xref="S3.E1.m1.3.4.2.2">ğ‘“</ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ‘¤</ci></apply><apply id="S3.E1.m1.3.4.3.cmml" xref="S3.E1.m1.3.4.3"><apply id="S3.E1.m1.3.4.3.1.cmml" xref="S3.E1.m1.3.4.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.4.3.1.1.cmml" xref="S3.E1.m1.3.4.3.1">superscript</csymbol><apply id="S3.E1.m1.3.4.3.1.2.cmml" xref="S3.E1.m1.3.4.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.4.3.1.2.1.cmml" xref="S3.E1.m1.3.4.3.1">subscript</csymbol><sum id="S3.E1.m1.3.4.3.1.2.2.cmml" xref="S3.E1.m1.3.4.3.1.2.2"></sum><apply id="S3.E1.m1.3.4.3.1.2.3.cmml" xref="S3.E1.m1.3.4.3.1.2.3"><eq id="S3.E1.m1.3.4.3.1.2.3.1.cmml" xref="S3.E1.m1.3.4.3.1.2.3.1"></eq><ci id="S3.E1.m1.3.4.3.1.2.3.2.cmml" xref="S3.E1.m1.3.4.3.1.2.3.2">ğ‘˜</ci><cn type="integer" id="S3.E1.m1.3.4.3.1.2.3.3.cmml" xref="S3.E1.m1.3.4.3.1.2.3.3">1</cn></apply></apply><ci id="S3.E1.m1.3.4.3.1.3.cmml" xref="S3.E1.m1.3.4.3.1.3">ğ¾</ci></apply><apply id="S3.E1.m1.3.4.3.2.cmml" xref="S3.E1.m1.3.4.3.2"><times id="S3.E1.m1.3.4.3.2.1.cmml" xref="S3.E1.m1.3.4.3.2.1"></times><apply id="S3.E1.m1.3.4.3.2.2.cmml" xref="S3.E1.m1.3.4.3.2.2"><ci id="S3.E1.m1.3.4.3.2.2.1.cmml" xref="S3.E1.m1.3.4.3.2.2.1">â‹…</ci><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.3.4.3.2.2.2.2"><divide id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.3.4.3.2.2.2.2"></divide><apply id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2">subscript</csymbol><ci id="S3.E1.m1.2.2.2.2.cmml" xref="S3.E1.m1.2.2.2.2">ğ‘›</ci><ci id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.3">ğ‘˜</ci></apply><ci id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3">ğ‘›</ci></apply><apply id="S3.E1.m1.3.4.3.2.2.3.cmml" xref="S3.E1.m1.3.4.3.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.4.3.2.2.3.1.cmml" xref="S3.E1.m1.3.4.3.2.2.3">subscript</csymbol><ci id="S3.E1.m1.3.4.3.2.2.3.2.cmml" xref="S3.E1.m1.3.4.3.2.2.3.2">ğ¹</ci><ci id="S3.E1.m1.3.4.3.2.2.3.3.cmml" xref="S3.E1.m1.3.4.3.2.2.3.3">ğ‘˜</ci></apply></apply><ci id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3">ğ‘¤</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">f(w)=\sum_{k=1}^{K}\left(\frac{n_{k}}{n}\right)\cdot F_{k}(w)</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.3" class="ltx_p">The Eq. <a href="#S3.E1" title="In III-C Federated Learning â€£ III Experiments â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> provides the mathematical description of the federated averaging algorithm, where <math id="S3.SS3.p4.1.m1.1" class="ltx_Math" alttext="F_{k}(w)" display="inline"><semantics id="S3.SS3.p4.1.m1.1a"><mrow id="S3.SS3.p4.1.m1.1.2" xref="S3.SS3.p4.1.m1.1.2.cmml"><msub id="S3.SS3.p4.1.m1.1.2.2" xref="S3.SS3.p4.1.m1.1.2.2.cmml"><mi id="S3.SS3.p4.1.m1.1.2.2.2" xref="S3.SS3.p4.1.m1.1.2.2.2.cmml">F</mi><mi id="S3.SS3.p4.1.m1.1.2.2.3" xref="S3.SS3.p4.1.m1.1.2.2.3.cmml">k</mi></msub><mo lspace="0em" rspace="0em" id="S3.SS3.p4.1.m1.1.2.1" xref="S3.SS3.p4.1.m1.1.2.1.cmml">â€‹</mo><mrow id="S3.SS3.p4.1.m1.1.2.3.2" xref="S3.SS3.p4.1.m1.1.2.cmml"><mo stretchy="false" id="S3.SS3.p4.1.m1.1.2.3.2.1" xref="S3.SS3.p4.1.m1.1.2.cmml">(</mo><mi id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml">w</mi><mo stretchy="false" id="S3.SS3.p4.1.m1.1.2.3.2.2" xref="S3.SS3.p4.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.2"><times id="S3.SS3.p4.1.m1.1.2.1.cmml" xref="S3.SS3.p4.1.m1.1.2.1"></times><apply id="S3.SS3.p4.1.m1.1.2.2.cmml" xref="S3.SS3.p4.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.2.2.1.cmml" xref="S3.SS3.p4.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS3.p4.1.m1.1.2.2.2.cmml" xref="S3.SS3.p4.1.m1.1.2.2.2">ğ¹</ci><ci id="S3.SS3.p4.1.m1.1.2.2.3.cmml" xref="S3.SS3.p4.1.m1.1.2.2.3">ğ‘˜</ci></apply><ci id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1">ğ‘¤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">F_{k}(w)</annotation></semantics></math> refers to the loss function of each client, <math id="S3.SS3.p4.2.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS3.p4.2.m2.1a"><mi id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><ci id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">ğ‘˜</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">k</annotation></semantics></math> refers to the number of clients, and <math id="S3.SS3.p4.3.m3.1" class="ltx_Math" alttext="\frac{n_{k}}{n}" display="inline"><semantics id="S3.SS3.p4.3.m3.1a"><mfrac id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml"><msub id="S3.SS3.p4.3.m3.1.1.2" xref="S3.SS3.p4.3.m3.1.1.2.cmml"><mi id="S3.SS3.p4.3.m3.1.1.2.2" xref="S3.SS3.p4.3.m3.1.1.2.2.cmml">n</mi><mi id="S3.SS3.p4.3.m3.1.1.2.3" xref="S3.SS3.p4.3.m3.1.1.2.3.cmml">k</mi></msub><mi id="S3.SS3.p4.3.m3.1.1.3" xref="S3.SS3.p4.3.m3.1.1.3.cmml">n</mi></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"><divide id="S3.SS3.p4.3.m3.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"></divide><apply id="S3.SS3.p4.3.m3.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.1.1.2.1.cmml" xref="S3.SS3.p4.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS3.p4.3.m3.1.1.2.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2.2">ğ‘›</ci><ci id="S3.SS3.p4.3.m3.1.1.2.3.cmml" xref="S3.SS3.p4.3.m3.1.1.2.3">ğ‘˜</ci></apply><ci id="S3.SS3.p4.3.m3.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3">ğ‘›</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">\frac{n_{k}}{n}</annotation></semantics></math> refers to the weighted average based on the size of the clientsâ€™ dataset.</p>
</div>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span id="S3.SS4.4.1.1" class="ltx_text">III-D</span> </span><span id="S3.SS4.5.2" class="ltx_text ltx_font_italic">Mobile Application</span>
</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">The designed model is combined with the user interface to build an application that helps people monitor their oral health. The application is made as a Progressive Web Application(PWA) instead of a native mobile application. The reason why PWAs were chosen over native mobile applications is that developing PWAs is more cost-effective when compared to native mobile apps since they consolidate all development efforts into web apps rather than requiring the creation of separate applications for each mobile platform <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>]</cite>. The web application was built with HTML, CSS, and JavaScript for the front end, while the back end was built using PHP and MySQL. The PWA features are incorporated by adding a web manifest file and icons. To enable offline functionality and caching, service workers were implemented.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para">
<p id="S3.SS4.p2.1" class="ltx_p">The camera module is integrated using the WebRTC API <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite>, and it ensures that the flashlight is kept on when a user is using the app on a mobile platform. This ensures proper image capture without leaving out any details, enabling better analysis. It is seamlessly integrated with geolocation services <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite> to offer personalized recommendations of nearby hospitals and dental clinics in case of detection of potential problems to prioritize prompt professional care. An SQL database containing oral health articles is created and is suggested to the user based on their oral health assessment results.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para">
<p id="S3.SS4.p3.1" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 â€£ III-A Dataset Description â€£ III Experiments â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> explains the pipeline of oral disease detection using the YOLOv8 model through a federated learning approach.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span id="S4.1.1" class="ltx_text ltx_font_smallcaps">Results and Discussions</span>
</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">This section discusses the performance of the object detection models YOLOv5 and YOLOv8 when trained using local data alone and when federated learning is used. The performance metrics used in evaluating the two frameworks are:</p>
<ul id="S4.I1" class="ltx_itemize">
<li id="S4.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i1.p1" class="ltx_para">
<p id="S4.I1.i1.p1.1" class="ltx_p">Mean Average Precision</p>
</div>
</li>
<li id="S4.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span> 
<div id="S4.I1.i2.p1" class="ltx_para">
<p id="S4.I1.i2.p1.1" class="ltx_p">F1-Score</p>
</div>
</li>
</ul>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p">Given below are the mathematical representations of the evaluation metrics:</p>
<table id="S4.E2" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E2.m1.1" class="ltx_Math" alttext="mAP=\frac{1}{n}\sum_{k=1}^{n}AP_{k}" display="block"><semantics id="S4.E2.m1.1a"><mrow id="S4.E2.m1.1.1" xref="S4.E2.m1.1.1.cmml"><mrow id="S4.E2.m1.1.1.2" xref="S4.E2.m1.1.1.2.cmml"><mi id="S4.E2.m1.1.1.2.2" xref="S4.E2.m1.1.1.2.2.cmml">m</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1" xref="S4.E2.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.2.3" xref="S4.E2.m1.1.1.2.3.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.2.1a" xref="S4.E2.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E2.m1.1.1.2.4" xref="S4.E2.m1.1.1.2.4.cmml">P</mi></mrow><mo id="S4.E2.m1.1.1.1" xref="S4.E2.m1.1.1.1.cmml">=</mo><mrow id="S4.E2.m1.1.1.3" xref="S4.E2.m1.1.1.3.cmml"><mfrac id="S4.E2.m1.1.1.3.2" xref="S4.E2.m1.1.1.3.2.cmml"><mn id="S4.E2.m1.1.1.3.2.2" xref="S4.E2.m1.1.1.3.2.2.cmml">1</mn><mi id="S4.E2.m1.1.1.3.2.3" xref="S4.E2.m1.1.1.3.2.3.cmml">n</mi></mfrac><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.1" xref="S4.E2.m1.1.1.3.1.cmml">â€‹</mo><mrow id="S4.E2.m1.1.1.3.3" xref="S4.E2.m1.1.1.3.3.cmml"><munderover id="S4.E2.m1.1.1.3.3.1" xref="S4.E2.m1.1.1.3.3.1.cmml"><mo movablelimits="false" id="S4.E2.m1.1.1.3.3.1.2.2" xref="S4.E2.m1.1.1.3.3.1.2.2.cmml">âˆ‘</mo><mrow id="S4.E2.m1.1.1.3.3.1.2.3" xref="S4.E2.m1.1.1.3.3.1.2.3.cmml"><mi id="S4.E2.m1.1.1.3.3.1.2.3.2" xref="S4.E2.m1.1.1.3.3.1.2.3.2.cmml">k</mi><mo id="S4.E2.m1.1.1.3.3.1.2.3.1" xref="S4.E2.m1.1.1.3.3.1.2.3.1.cmml">=</mo><mn id="S4.E2.m1.1.1.3.3.1.2.3.3" xref="S4.E2.m1.1.1.3.3.1.2.3.3.cmml">1</mn></mrow><mi id="S4.E2.m1.1.1.3.3.1.3" xref="S4.E2.m1.1.1.3.3.1.3.cmml">n</mi></munderover><mrow id="S4.E2.m1.1.1.3.3.2" xref="S4.E2.m1.1.1.3.3.2.cmml"><mi id="S4.E2.m1.1.1.3.3.2.2" xref="S4.E2.m1.1.1.3.3.2.2.cmml">A</mi><mo lspace="0em" rspace="0em" id="S4.E2.m1.1.1.3.3.2.1" xref="S4.E2.m1.1.1.3.3.2.1.cmml">â€‹</mo><msub id="S4.E2.m1.1.1.3.3.2.3" xref="S4.E2.m1.1.1.3.3.2.3.cmml"><mi id="S4.E2.m1.1.1.3.3.2.3.2" xref="S4.E2.m1.1.1.3.3.2.3.2.cmml">P</mi><mi id="S4.E2.m1.1.1.3.3.2.3.3" xref="S4.E2.m1.1.1.3.3.2.3.3.cmml">k</mi></msub></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E2.m1.1b"><apply id="S4.E2.m1.1.1.cmml" xref="S4.E2.m1.1.1"><eq id="S4.E2.m1.1.1.1.cmml" xref="S4.E2.m1.1.1.1"></eq><apply id="S4.E2.m1.1.1.2.cmml" xref="S4.E2.m1.1.1.2"><times id="S4.E2.m1.1.1.2.1.cmml" xref="S4.E2.m1.1.1.2.1"></times><ci id="S4.E2.m1.1.1.2.2.cmml" xref="S4.E2.m1.1.1.2.2">ğ‘š</ci><ci id="S4.E2.m1.1.1.2.3.cmml" xref="S4.E2.m1.1.1.2.3">ğ´</ci><ci id="S4.E2.m1.1.1.2.4.cmml" xref="S4.E2.m1.1.1.2.4">ğ‘ƒ</ci></apply><apply id="S4.E2.m1.1.1.3.cmml" xref="S4.E2.m1.1.1.3"><times id="S4.E2.m1.1.1.3.1.cmml" xref="S4.E2.m1.1.1.3.1"></times><apply id="S4.E2.m1.1.1.3.2.cmml" xref="S4.E2.m1.1.1.3.2"><divide id="S4.E2.m1.1.1.3.2.1.cmml" xref="S4.E2.m1.1.1.3.2"></divide><cn type="integer" id="S4.E2.m1.1.1.3.2.2.cmml" xref="S4.E2.m1.1.1.3.2.2">1</cn><ci id="S4.E2.m1.1.1.3.2.3.cmml" xref="S4.E2.m1.1.1.3.2.3">ğ‘›</ci></apply><apply id="S4.E2.m1.1.1.3.3.cmml" xref="S4.E2.m1.1.1.3.3"><apply id="S4.E2.m1.1.1.3.3.1.cmml" xref="S4.E2.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.3.1.1.cmml" xref="S4.E2.m1.1.1.3.3.1">superscript</csymbol><apply id="S4.E2.m1.1.1.3.3.1.2.cmml" xref="S4.E2.m1.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.3.1.2.1.cmml" xref="S4.E2.m1.1.1.3.3.1">subscript</csymbol><sum id="S4.E2.m1.1.1.3.3.1.2.2.cmml" xref="S4.E2.m1.1.1.3.3.1.2.2"></sum><apply id="S4.E2.m1.1.1.3.3.1.2.3.cmml" xref="S4.E2.m1.1.1.3.3.1.2.3"><eq id="S4.E2.m1.1.1.3.3.1.2.3.1.cmml" xref="S4.E2.m1.1.1.3.3.1.2.3.1"></eq><ci id="S4.E2.m1.1.1.3.3.1.2.3.2.cmml" xref="S4.E2.m1.1.1.3.3.1.2.3.2">ğ‘˜</ci><cn type="integer" id="S4.E2.m1.1.1.3.3.1.2.3.3.cmml" xref="S4.E2.m1.1.1.3.3.1.2.3.3">1</cn></apply></apply><ci id="S4.E2.m1.1.1.3.3.1.3.cmml" xref="S4.E2.m1.1.1.3.3.1.3">ğ‘›</ci></apply><apply id="S4.E2.m1.1.1.3.3.2.cmml" xref="S4.E2.m1.1.1.3.3.2"><times id="S4.E2.m1.1.1.3.3.2.1.cmml" xref="S4.E2.m1.1.1.3.3.2.1"></times><ci id="S4.E2.m1.1.1.3.3.2.2.cmml" xref="S4.E2.m1.1.1.3.3.2.2">ğ´</ci><apply id="S4.E2.m1.1.1.3.3.2.3.cmml" xref="S4.E2.m1.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S4.E2.m1.1.1.3.3.2.3.1.cmml" xref="S4.E2.m1.1.1.3.3.2.3">subscript</csymbol><ci id="S4.E2.m1.1.1.3.3.2.3.2.cmml" xref="S4.E2.m1.1.1.3.3.2.3.2">ğ‘ƒ</ci><ci id="S4.E2.m1.1.1.3.3.2.3.3.cmml" xref="S4.E2.m1.1.1.3.3.2.3.3">ğ‘˜</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E2.m1.1c">mAP=\frac{1}{n}\sum_{k=1}^{n}AP_{k}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div id="S4.p3" class="ltx_para">
<table id="S4.E3" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S4.E3.m1.1" class="ltx_Math" alttext="F1\text{-}Score=2\cdot\frac{Precison\cdot Recall}{Precision+Recall}" display="block"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mrow id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2.cmml"><mi id="S4.E3.m1.1.1.2.2" xref="S4.E3.m1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.1" xref="S4.E3.m1.1.1.2.1.cmml">â€‹</mo><mn id="S4.E3.m1.1.1.2.3" xref="S4.E3.m1.1.1.2.3.cmml">1</mn><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.1a" xref="S4.E3.m1.1.1.2.1.cmml">â€‹</mo><mtext id="S4.E3.m1.1.1.2.4" xref="S4.E3.m1.1.1.2.4a.cmml">-</mtext><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.1b" xref="S4.E3.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.2.5" xref="S4.E3.m1.1.1.2.5.cmml">S</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.1c" xref="S4.E3.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.2.6" xref="S4.E3.m1.1.1.2.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.1d" xref="S4.E3.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.2.7" xref="S4.E3.m1.1.1.2.7.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.1e" xref="S4.E3.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.2.8" xref="S4.E3.m1.1.1.2.8.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.2.1f" xref="S4.E3.m1.1.1.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.2.9" xref="S4.E3.m1.1.1.2.9.cmml">e</mi></mrow><mo id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml">=</mo><mrow id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml"><mn id="S4.E3.m1.1.1.3.2" xref="S4.E3.m1.1.1.3.2.cmml">2</mn><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.1.1.3.1" xref="S4.E3.m1.1.1.3.1.cmml">â‹…</mo><mfrac id="S4.E3.m1.1.1.3.3" xref="S4.E3.m1.1.1.3.3.cmml"><mrow id="S4.E3.m1.1.1.3.3.2" xref="S4.E3.m1.1.1.3.3.2.cmml"><mrow id="S4.E3.m1.1.1.3.3.2.2" xref="S4.E3.m1.1.1.3.3.2.2.cmml"><mrow id="S4.E3.m1.1.1.3.3.2.2.2" xref="S4.E3.m1.1.1.3.3.2.2.2.cmml"><mi id="S4.E3.m1.1.1.3.3.2.2.2.2" xref="S4.E3.m1.1.1.3.3.2.2.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.2.2.1" xref="S4.E3.m1.1.1.3.3.2.2.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.2.2.3" xref="S4.E3.m1.1.1.3.3.2.2.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.2.2.1a" xref="S4.E3.m1.1.1.3.3.2.2.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.2.2.4" xref="S4.E3.m1.1.1.3.3.2.2.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.2.2.1b" xref="S4.E3.m1.1.1.3.3.2.2.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.2.2.5" xref="S4.E3.m1.1.1.3.3.2.2.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.2.2.1c" xref="S4.E3.m1.1.1.3.3.2.2.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.2.2.6" xref="S4.E3.m1.1.1.3.3.2.2.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.2.2.1d" xref="S4.E3.m1.1.1.3.3.2.2.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.2.2.7" xref="S4.E3.m1.1.1.3.3.2.2.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.2.2.1e" xref="S4.E3.m1.1.1.3.3.2.2.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.2.2.8" xref="S4.E3.m1.1.1.3.3.2.2.2.8.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.2.2.1f" xref="S4.E3.m1.1.1.3.3.2.2.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.2.2.9" xref="S4.E3.m1.1.1.3.3.2.2.2.9.cmml">n</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S4.E3.m1.1.1.3.3.2.2.1" xref="S4.E3.m1.1.1.3.3.2.2.1.cmml">â‹…</mo><mi id="S4.E3.m1.1.1.3.3.2.2.3" xref="S4.E3.m1.1.1.3.3.2.2.3.cmml">R</mi></mrow><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.1" xref="S4.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.3" xref="S4.E3.m1.1.1.3.3.2.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.1a" xref="S4.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.4" xref="S4.E3.m1.1.1.3.3.2.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.1b" xref="S4.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.5" xref="S4.E3.m1.1.1.3.3.2.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.1c" xref="S4.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.6" xref="S4.E3.m1.1.1.3.3.2.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.2.1d" xref="S4.E3.m1.1.1.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.2.7" xref="S4.E3.m1.1.1.3.3.2.7.cmml">l</mi></mrow><mrow id="S4.E3.m1.1.1.3.3.3" xref="S4.E3.m1.1.1.3.3.3.cmml"><mrow id="S4.E3.m1.1.1.3.3.3.2" xref="S4.E3.m1.1.1.3.3.3.2.cmml"><mi id="S4.E3.m1.1.1.3.3.3.2.2" xref="S4.E3.m1.1.1.3.3.3.2.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.2.1" xref="S4.E3.m1.1.1.3.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.2.3" xref="S4.E3.m1.1.1.3.3.3.2.3.cmml">r</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.2.1a" xref="S4.E3.m1.1.1.3.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.2.4" xref="S4.E3.m1.1.1.3.3.3.2.4.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.2.1b" xref="S4.E3.m1.1.1.3.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.2.5" xref="S4.E3.m1.1.1.3.3.3.2.5.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.2.1c" xref="S4.E3.m1.1.1.3.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.2.6" xref="S4.E3.m1.1.1.3.3.3.2.6.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.2.1d" xref="S4.E3.m1.1.1.3.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.2.7" xref="S4.E3.m1.1.1.3.3.3.2.7.cmml">s</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.2.1e" xref="S4.E3.m1.1.1.3.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.2.8" xref="S4.E3.m1.1.1.3.3.3.2.8.cmml">i</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.2.1f" xref="S4.E3.m1.1.1.3.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.2.9" xref="S4.E3.m1.1.1.3.3.3.2.9.cmml">o</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.2.1g" xref="S4.E3.m1.1.1.3.3.3.2.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.2.10" xref="S4.E3.m1.1.1.3.3.3.2.10.cmml">n</mi></mrow><mo id="S4.E3.m1.1.1.3.3.3.1" xref="S4.E3.m1.1.1.3.3.3.1.cmml">+</mo><mrow id="S4.E3.m1.1.1.3.3.3.3" xref="S4.E3.m1.1.1.3.3.3.3.cmml"><mi id="S4.E3.m1.1.1.3.3.3.3.2" xref="S4.E3.m1.1.1.3.3.3.3.2.cmml">R</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.3.1" xref="S4.E3.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.3.3" xref="S4.E3.m1.1.1.3.3.3.3.3.cmml">e</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.3.1a" xref="S4.E3.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.3.4" xref="S4.E3.m1.1.1.3.3.3.3.4.cmml">c</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.3.1b" xref="S4.E3.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.3.5" xref="S4.E3.m1.1.1.3.3.3.3.5.cmml">a</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.3.1c" xref="S4.E3.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.3.6" xref="S4.E3.m1.1.1.3.3.3.3.6.cmml">l</mi><mo lspace="0em" rspace="0em" id="S4.E3.m1.1.1.3.3.3.3.1d" xref="S4.E3.m1.1.1.3.3.3.3.1.cmml">â€‹</mo><mi id="S4.E3.m1.1.1.3.3.3.3.7" xref="S4.E3.m1.1.1.3.3.3.3.7.cmml">l</mi></mrow></mrow></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"></eq><apply id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2"><times id="S4.E3.m1.1.1.2.1.cmml" xref="S4.E3.m1.1.1.2.1"></times><ci id="S4.E3.m1.1.1.2.2.cmml" xref="S4.E3.m1.1.1.2.2">ğ¹</ci><cn type="integer" id="S4.E3.m1.1.1.2.3.cmml" xref="S4.E3.m1.1.1.2.3">1</cn><ci id="S4.E3.m1.1.1.2.4a.cmml" xref="S4.E3.m1.1.1.2.4"><mtext id="S4.E3.m1.1.1.2.4.cmml" xref="S4.E3.m1.1.1.2.4">-</mtext></ci><ci id="S4.E3.m1.1.1.2.5.cmml" xref="S4.E3.m1.1.1.2.5">ğ‘†</ci><ci id="S4.E3.m1.1.1.2.6.cmml" xref="S4.E3.m1.1.1.2.6">ğ‘</ci><ci id="S4.E3.m1.1.1.2.7.cmml" xref="S4.E3.m1.1.1.2.7">ğ‘œ</ci><ci id="S4.E3.m1.1.1.2.8.cmml" xref="S4.E3.m1.1.1.2.8">ğ‘Ÿ</ci><ci id="S4.E3.m1.1.1.2.9.cmml" xref="S4.E3.m1.1.1.2.9">ğ‘’</ci></apply><apply id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3"><ci id="S4.E3.m1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.3.1">â‹…</ci><cn type="integer" id="S4.E3.m1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.3.2">2</cn><apply id="S4.E3.m1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.3.3"><divide id="S4.E3.m1.1.1.3.3.1.cmml" xref="S4.E3.m1.1.1.3.3"></divide><apply id="S4.E3.m1.1.1.3.3.2.cmml" xref="S4.E3.m1.1.1.3.3.2"><times id="S4.E3.m1.1.1.3.3.2.1.cmml" xref="S4.E3.m1.1.1.3.3.2.1"></times><apply id="S4.E3.m1.1.1.3.3.2.2.cmml" xref="S4.E3.m1.1.1.3.3.2.2"><ci id="S4.E3.m1.1.1.3.3.2.2.1.cmml" xref="S4.E3.m1.1.1.3.3.2.2.1">â‹…</ci><apply id="S4.E3.m1.1.1.3.3.2.2.2.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2"><times id="S4.E3.m1.1.1.3.3.2.2.2.1.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2.1"></times><ci id="S4.E3.m1.1.1.3.3.2.2.2.2.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2.2">ğ‘ƒ</ci><ci id="S4.E3.m1.1.1.3.3.2.2.2.3.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2.3">ğ‘Ÿ</ci><ci id="S4.E3.m1.1.1.3.3.2.2.2.4.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2.4">ğ‘’</ci><ci id="S4.E3.m1.1.1.3.3.2.2.2.5.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2.5">ğ‘</ci><ci id="S4.E3.m1.1.1.3.3.2.2.2.6.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2.6">ğ‘–</ci><ci id="S4.E3.m1.1.1.3.3.2.2.2.7.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2.7">ğ‘ </ci><ci id="S4.E3.m1.1.1.3.3.2.2.2.8.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2.8">ğ‘œ</ci><ci id="S4.E3.m1.1.1.3.3.2.2.2.9.cmml" xref="S4.E3.m1.1.1.3.3.2.2.2.9">ğ‘›</ci></apply><ci id="S4.E3.m1.1.1.3.3.2.2.3.cmml" xref="S4.E3.m1.1.1.3.3.2.2.3">ğ‘…</ci></apply><ci id="S4.E3.m1.1.1.3.3.2.3.cmml" xref="S4.E3.m1.1.1.3.3.2.3">ğ‘’</ci><ci id="S4.E3.m1.1.1.3.3.2.4.cmml" xref="S4.E3.m1.1.1.3.3.2.4">ğ‘</ci><ci id="S4.E3.m1.1.1.3.3.2.5.cmml" xref="S4.E3.m1.1.1.3.3.2.5">ğ‘</ci><ci id="S4.E3.m1.1.1.3.3.2.6.cmml" xref="S4.E3.m1.1.1.3.3.2.6">ğ‘™</ci><ci id="S4.E3.m1.1.1.3.3.2.7.cmml" xref="S4.E3.m1.1.1.3.3.2.7">ğ‘™</ci></apply><apply id="S4.E3.m1.1.1.3.3.3.cmml" xref="S4.E3.m1.1.1.3.3.3"><plus id="S4.E3.m1.1.1.3.3.3.1.cmml" xref="S4.E3.m1.1.1.3.3.3.1"></plus><apply id="S4.E3.m1.1.1.3.3.3.2.cmml" xref="S4.E3.m1.1.1.3.3.3.2"><times id="S4.E3.m1.1.1.3.3.3.2.1.cmml" xref="S4.E3.m1.1.1.3.3.3.2.1"></times><ci id="S4.E3.m1.1.1.3.3.3.2.2.cmml" xref="S4.E3.m1.1.1.3.3.3.2.2">ğ‘ƒ</ci><ci id="S4.E3.m1.1.1.3.3.3.2.3.cmml" xref="S4.E3.m1.1.1.3.3.3.2.3">ğ‘Ÿ</ci><ci id="S4.E3.m1.1.1.3.3.3.2.4.cmml" xref="S4.E3.m1.1.1.3.3.3.2.4">ğ‘’</ci><ci id="S4.E3.m1.1.1.3.3.3.2.5.cmml" xref="S4.E3.m1.1.1.3.3.3.2.5">ğ‘</ci><ci id="S4.E3.m1.1.1.3.3.3.2.6.cmml" xref="S4.E3.m1.1.1.3.3.3.2.6">ğ‘–</ci><ci id="S4.E3.m1.1.1.3.3.3.2.7.cmml" xref="S4.E3.m1.1.1.3.3.3.2.7">ğ‘ </ci><ci id="S4.E3.m1.1.1.3.3.3.2.8.cmml" xref="S4.E3.m1.1.1.3.3.3.2.8">ğ‘–</ci><ci id="S4.E3.m1.1.1.3.3.3.2.9.cmml" xref="S4.E3.m1.1.1.3.3.3.2.9">ğ‘œ</ci><ci id="S4.E3.m1.1.1.3.3.3.2.10.cmml" xref="S4.E3.m1.1.1.3.3.3.2.10">ğ‘›</ci></apply><apply id="S4.E3.m1.1.1.3.3.3.3.cmml" xref="S4.E3.m1.1.1.3.3.3.3"><times id="S4.E3.m1.1.1.3.3.3.3.1.cmml" xref="S4.E3.m1.1.1.3.3.3.3.1"></times><ci id="S4.E3.m1.1.1.3.3.3.3.2.cmml" xref="S4.E3.m1.1.1.3.3.3.3.2">ğ‘…</ci><ci id="S4.E3.m1.1.1.3.3.3.3.3.cmml" xref="S4.E3.m1.1.1.3.3.3.3.3">ğ‘’</ci><ci id="S4.E3.m1.1.1.3.3.3.3.4.cmml" xref="S4.E3.m1.1.1.3.3.3.3.4">ğ‘</ci><ci id="S4.E3.m1.1.1.3.3.3.3.5.cmml" xref="S4.E3.m1.1.1.3.3.3.3.5">ğ‘</ci><ci id="S4.E3.m1.1.1.3.3.3.3.6.cmml" xref="S4.E3.m1.1.1.3.3.3.3.6">ğ‘™</ci><ci id="S4.E3.m1.1.1.3.3.3.3.7.cmml" xref="S4.E3.m1.1.1.3.3.3.3.7">ğ‘™</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">F1\text{-}Score=2\cdot\frac{Precison\cdot Recall}{Precision+Recall}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<figure id="S4.T2" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Performance Metrics of Federated Framework 
<br class="ltx_break">Number of Communication rounds required to achieve the target mAP 
<br class="ltx_break">Target mAP = 80%</figcaption>
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T2.1.1.1" class="ltx_tr">
<th id="S4.T2.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S4.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">E</th>
<th id="S4.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">Rounds</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T2.1.2.1" class="ltx_tr">
<td id="S4.T2.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">Federated - YOLOv5</td>
<td id="S4.T2.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">1</td>
<td id="S4.T2.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">168</td>
</tr>
<tr id="S4.T2.1.3.2" class="ltx_tr">
<td id="S4.T2.1.3.2.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Federated - YOLOv5</td>
<td id="S4.T2.1.3.2.2" class="ltx_td ltx_align_center ltx_border_r">5</td>
<td id="S4.T2.1.3.2.3" class="ltx_td ltx_align_center ltx_border_r">59</td>
</tr>
<tr id="S4.T2.1.4.3" class="ltx_tr">
<td id="S4.T2.1.4.3.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Federated - YOLOv5</td>
<td id="S4.T2.1.4.3.2" class="ltx_td ltx_align_center ltx_border_r">10</td>
<td id="S4.T2.1.4.3.3" class="ltx_td ltx_align_center ltx_border_r">122</td>
</tr>
<tr id="S4.T2.1.5.4" class="ltx_tr">
<td id="S4.T2.1.5.4.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Federated - YOLOv8</td>
<td id="S4.T2.1.5.4.2" class="ltx_td ltx_align_center ltx_border_r">1</td>
<td id="S4.T2.1.5.4.3" class="ltx_td ltx_align_center ltx_border_r">64</td>
</tr>
<tr id="S4.T2.1.6.5" class="ltx_tr">
<td id="S4.T2.1.6.5.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r">Federated - YOLOv8</td>
<td id="S4.T2.1.6.5.2" class="ltx_td ltx_align_center ltx_border_r">5</td>
<td id="S4.T2.1.6.5.3" class="ltx_td ltx_align_center ltx_border_r">51</td>
</tr>
<tr id="S4.T2.1.7.6" class="ltx_tr">
<td id="S4.T2.1.7.6.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">Federated - YOLOv8</td>
<td id="S4.T2.1.7.6.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">10</td>
<td id="S4.T2.1.7.6.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r">97</td>
</tr>
</tbody>
</table>
</figure>
<figure id="S4.T3" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE III: </span>Performance Metrics of Local Training</figcaption>
<table id="S4.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S4.T3.1.1.1" class="ltx_tr">
<th id="S4.T3.1.1.1.1" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t">Model</th>
<th id="S4.T3.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">F1-Score</th>
<th id="S4.T3.1.1.1.3" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t">mAP</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S4.T3.1.2.1" class="ltx_tr">
<td id="S4.T3.1.2.1.1" class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t">YOLOv5</td>
<td id="S4.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">75.7%</td>
<td id="S4.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">72.6%</td>
</tr>
<tr id="S4.T3.1.3.2" class="ltx_tr">
<td id="S4.T3.1.3.2.1" class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r">YOLOv8</td>
<td id="S4.T3.1.3.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.1.3.2.2.1" class="ltx_text ltx_font_bold">82.3%</span></td>
<td id="S4.T3.1.3.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r"><span id="S4.T3.1.3.2.3.1" class="ltx_text ltx_font_bold">78.8%</span></td>
</tr>
</tbody>
</table>
</figure>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">As evident from Tables <a href="#S4.T2" title="TABLE II â€£ IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and <a href="#S4.T3" title="TABLE III â€£ IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>, and discussed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>, initially a larger E value gave faster rates of achieving the target mAP. But as the dataset is more non-IID, different values of E produce different rates of convergence.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">The YOLOv8 model contains significant improvements over its preceding YOLOv5 model. The YOLOv8 model utilizes an anchor-free model to predict the center of an object without using an offset from the anchor box. Additionally, the YOLOv8 model shows notable enhancement in accuracy when tested on the COCO dataset <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib23" title="" class="ltx_ref">23</a>]</cite>. On comparing the two YOLO models on locally trained data, it is evident that the YOLOv8 model performs better than the YOLOv5 model as tabulated in Table <a href="#S4.T3" title="TABLE III â€£ IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2403.12044/assets/Figures/yolov5.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="420" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>mAP vs. Number of Communication Rounds for the YOLOv5 Model</figcaption>
</figure>
<div id="S4.p6" class="ltx_para">
<p id="S4.p6.1" class="ltx_p">Mean Average Precision(mAP) is an evaluation metric often used to evaluate the performance of an object detection model. It utilizes the concept of Intersection over Union(IoU) to measure the overlap of the predicted object boundary against the ground truth boundary. A threshold of 0.5 was used for IoU during the experiments. The precision obtained from this is calculated for all objects and grouped and averaged on the basis of the class to which they belong.</p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2403.12044/assets/Figures/yolov8.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_square" width="419" height="420" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>mAP vs. Number of Communication Rounds for the YOLOv8 Model</figcaption>
</figure>
<div id="S4.p7" class="ltx_para">
<p id="S4.p7.1" class="ltx_p">Calculation of the mean of the average precision values provides the final mean average precision metric. Eq. <a href="#S4.E2" title="In IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> provides the mathematical description of the metric where AP refers to the average precision of each class and n refers to the total number of classes.
The locally trained framework produced an mAP value of 72.6 after the completion of 100 epochs on the YOLOv5 model and a value of 78.8 after the completion of 100 epochs on the YOLOv8 model while the federated learning framework achieved a target mAP of 80 after a certain number of communication rounds with the server depending on the value of E and the model in use as evident from Tables <a href="#S4.T2" title="TABLE II â€£ IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">II</span></a> and <a href="#S4.T3" title="TABLE III â€£ IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> as well as Figures <a href="#S4.F3" title="Figure 3 â€£ IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> and <a href="#S4.F4" title="Figure 4 â€£ IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>. This indicates the overall better performance of the federated framework.</p>
</div>
<div id="S4.p8" class="ltx_para">
<p id="S4.p8.1" class="ltx_p">The F1-score is a machine learning performance metric that utilizes the precision and recall metrics. Popularly used for object detection in imbalanced datasets, it provides an integration of the precision and recall metrics and is hence used as an ideal measure of performance for object detection purposes. Eq. <a href="#S4.E3" title="In IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> provides the mathematical description of F1-Score.</p>
</div>
<div id="S4.p9" class="ltx_para">
<p id="S4.p9.1" class="ltx_p">As evident from Table <a href="#S4.T3" title="TABLE III â€£ IV Results and Discussions â€£ Mobile Application for Oral Disease Detection using Federated Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">III</span></a> the YOLOv8 model provides a better F1-score in comparison to the YOLOv5 model and is hence considered an overall better model for implementation of object detection for this use-case.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span id="S5.1.1" class="ltx_text ltx_font_smallcaps">Conclusions and Future Work</span>
</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In conclusion, this study was able to successfully detect the diseases in the mouth using the YOLOv5 and YOLOv8 models trained locally as well as trained iteratively via Federated Learning(FL). As evident from the results discussed, FL was able to achieve better results for both the YOLOv5 and the YOLOv8 models in comparison to its counterparts. The use of varied and diverse data from different end users provides a better training opportunity for the model and hence the expected results. Between the two YOLO models, the YOLOv8 gave better performance and was hence used for the final application. Amongst the federated models, we noticed that an increase in the value of E does not necessarily imply a faster rate of convergence to achieve the target mAP, as also noted in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Future research direction includes the incorporation of explainable AI into federated learning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib24" title="" class="ltx_ref">24</a>]</cite>. Innovative methods such as federated model inspection can be incorporated into federated learning to provide greater insights into the working and decision-making aspects across the devices to the stakeholders involved. Privacy-preserving methodologies while guaranteeing interpretability are essential.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">The federated learning framework involves the communication between the server and the clients. This raises the need for secure network and communication protocols to prevent data interception. Network security aspects such as encryption, server authentication, data integrity, and secure channels must be incorporated to protect sensitive data, especially in the case of healthcare applications. Preserving data integrity ensures trust in the system and allows the framework to collaborate with decentralized edge devices.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
R.Â M. Benjamin, â€œOral health: the silent epidemic,â€ <em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Public health
reports</em>, vol. 125, no.Â 2, pp. 158â€“159, 2010.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
U.Â S. P. H. S.Â O. ofÂ the SurgeonÂ General, N.Â I. ofÂ Dental, and C.Â R. (US),
<em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">Oral health in America: a report of the Surgeon General</em>.Â Â Â US Public Health Service, Department of Health
and Human Services, 2000.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
D.Â Kim, J.Â Choi, S.Â Ahn, and E.Â Park, â€œA smart home dental care system:
integration of deep learning, image sensors, and mobile controller,â€
<em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">Journal of Ambient Intelligence and Humanized Computing</em>, pp. 1â€“9,
2021.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
S.Â Lee, H.Â Huang, and M.Â Zelen, â€œEarly detection of disease and scheduling of
screening examinations,â€ <em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Statistical methods in medical research</em>,
vol.Â 13, no.Â 6, pp. 443â€“456, 2004.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
R.Â M. Haralick and L.Â G. Shapiro, â€œImage segmentation techniques,â€
<em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Computer vision, graphics, and image processing</em>, vol.Â 29, no.Â 1, pp.
100â€“132, 1985.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
M.Â L. Mack, I.Â Gauthier, J.Â Sadr, and T.Â J. Palmeri, â€œObject detection and
basic-level categorization: Sometimes you know it is there before you know
what it is,â€ <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Psychonomic Bulletin &amp; Review</em>, vol.Â 15, no.Â 1, pp.
28â€“35, 2008.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
P.Â L.Â J. Yuen, â€œMachine learning in dentistry,â€ Nanyang Technological
University, 2023.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
H.Â Bae, J.Â Jang, D.Â Jung, H.Â Jang, H.Â Ha, H.Â Lee, and S.Â Yoon, â€œSecurity and
privacy issues in deep learning,â€ <em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1807.11655</em>,
2018.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
J.Â Luo, X.Â Wu, Y.Â Luo, A.Â Huang, Y.Â Huang, Y.Â Liu, and Q.Â Yang, â€œReal-world
image datasets for federated learning,â€ <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">arXiv preprint
arXiv:1910.11089</em>, 2019.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
H.Â R. Roth, K.Â Chang, P.Â Singh, N.Â Neumark, W.Â Li, V.Â Gupta, S.Â Gupta, L.Â Qu,
A.Â Ihsani, B.Â C. Bizzo <em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œFederated learning for breast density
classification: A real-world implementation,â€ in <em id="bib.bib10.2.2" class="ltx_emph ltx_font_italic">Domain Adaptation and
Representation Transfer, and Distributed and Collaborative Learning: Second
MICCAI Workshop, DART 2020, and First MICCAI Workshop, DCL 2020, Held in
Conjunction with MICCAI 2020, Lima, Peru, October 4â€“8, 2020, Proceedings
2</em>.Â Â Â Springer, 2020, pp. 181â€“191.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J.Â Redmon, S.Â Divvala, R.Â Girshick, and A.Â Farhadi, â€œYou only look once:
Unified, real-time object detection,â€ in <em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE
conference on computer vision and pattern recognition</em>, 2016, pp. 779â€“788.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
B.Â N.Â B. J, U.Â G. S, and M.Â Jose, â€œMultiple object recognition from smart
document images using yolov5s,â€ in <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">2023 7th International Conference
on Computing Methodologies and Communication (ICCMC)</em>, 2023, pp. 824â€“828.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
U.Â Subbiah, D.Â K. Kumar, S.Â K. Thangavel, and L.Â Parameswaran, â€œAn extensive
study and comparison of the various approaches to object detection using deep
learning,â€ in <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">2020 International Conference on Smart Electronics and
Communication (ICOSEC)</em>, 2020, pp. 183â€“194.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
G.Â Jocher, A.Â Chaurasia, A.Â Stoken, J.Â Borovec, Y.Â Kwon, K.Â Michael, J.Â Fang,
Z.Â Yifu, C.Â Wong, D.Â Montes <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">etÂ al.</em>, â€œultralytics/yolov5: v7. 0-yolov5
sota realtime instance segmentation,â€ <em id="bib.bib14.2.2" class="ltx_emph ltx_font_italic">Zenodo</em>, 2022.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
G.Â Jocher, A.Â Chaurasia, and J.Â Qiu, â€œYolo by ultralytics,â€ <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">URL:
https://github. com/ultralytics/ultralytics</em>, 2023.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
S.Â Ren, K.Â He, R.Â Girshick, and J.Â Sun, â€œFaster r-cnn: Towards real-time
object detection with region proposal networks,â€ <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Advances in neural
information processing systems</em>, vol.Â 28, 2015.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
M.Â S. Pandianchery, V.Â Sowmya, E.Â A. Gopalakrishnan, V.Â Ravi, and K.Â P. Soman,
â€œCentralized cnnâ€“gru model by federated learning for covid-19 prediction
in india,â€ <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">IEEE Transactions on Computational Social Systems</em>, pp.
1â€“10, 2023.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
A.Â MÂ P, G.Â R, and M.Â Panda, â€œSteering angle prediction for autonomous driving
using federated learning: The impact of vehicle-to-everything
communication,â€ in <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">2021 12th International Conference on Computing
Communication and Networking Technologies (ICCCNT)</em>, 2021, pp. 1â€“7.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
B.Â McMahan, E.Â Moore, D.Â Ramage, S.Â Hampson, and B.Â A. yÂ Arcas,
â€œCommunication-efficient learning of deep networks from decentralized
data,â€ in <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">Artificial intelligence and statistics</em>.Â Â Â PMLR, 2017, pp. 1273â€“1282.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
D.Â Fortunato and J.Â Bernardino, â€œProgressive web apps: An alternative to the
native mobile apps,â€ in <em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">2018 13th Iberian Conference on Information
Systems and Technologies (CISTI)</em>.Â Â Â IEEE, 2018, pp. 1â€“6.

</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
B.Â Sredojev, D.Â Samardzija, and D.Â Posarac, â€œWebrtc technology overview and
signaling solution design and implementation,â€ in <em id="bib.bib21.1.1" class="ltx_emph ltx_font_italic">2015 38th
International Convention on Information and Communication Technology,
Electronics and Microelectronics (MIPRO)</em>, 2015, pp. 1006â€“1009.

</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
B.Â PejiÄ‡, A.Â PejiÄ‡, and Z.Â ÄŒoviÄ‡, â€œUses of w3câ€™s geolocation api,â€ in
<em id="bib.bib22.1.1" class="ltx_emph ltx_font_italic">2010 11th International Symposium on Computational Intelligence and
Informatics (CINTI)</em>, 2010, pp. 319â€“322.

</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J.Â Solawetz and Francesco, â€œWhatâ€™s new in yolov8,â€ <em id="bib.bib23.1.1" class="ltx_emph ltx_font_italic">Roboflow Blog</em>,
January 2023. [Online]. Available:
https://blog.roboflow.com/whats-new-in-yolov8/

</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J.Â L.Â C. BÃ¡rcena, M.Â Daole, P.Â Ducange, F.Â Marcelloni, A.Â Renda,
F.Â Ruffini, and A.Â Schiavo, â€œFed-xai: Federated learning of explainable
artificial intelligence models,â€ in <em id="bib.bib24.1.1" class="ltx_emph ltx_font_italic">3rd Italian Workshop on
Explainable Artificial Intelligence (XAI. it 2022)</em>, 2022.

</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2403.12043" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2403.12044" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2403.12044">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2403.12044" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2403.12045" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Apr  5 17:54:15 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
