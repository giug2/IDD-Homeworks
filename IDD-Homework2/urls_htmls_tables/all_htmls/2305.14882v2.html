<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2305.14882] Interpretable by Design Visual Question Answering</title><meta property="og:description" content="Model interpretability has long been a hard problem for the AI community especially in the multimodal setting, where vision and language need to be aligned and reasoned at the same time.
In this paper, we specifically â€¦">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Interpretable by Design Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Interpretable by Design Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2305.14882">

<!--Generated on Thu Feb 29 05:55:50 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Interpretable by Design Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xingyu Fu Â  Ben Zhou Â  Sihao Chen Â  Mark Yatskar Â  Dan Roth
<br class="ltx_break">University of Pennsylvania â€„â€„â€„
<br class="ltx_break">{xingyuf2, xyzhou, sihaoc, myatskar, danroth}@seas.upenn.edu
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Model interpretability has long been a hard problem for the AI community especially in the multimodal setting, where vision and language need to be aligned and reasoned at the same time.
In this paper, we specifically focus on the problem of Visual Question Answering (VQA).
While previous researches try to probe into the network structures of black-box multimodal models, we propose to tackle the problem from a different angle â€“
to treat interpretability as an explicit additional goal.</p>
<p id="id2.id2" class="ltx_p">Given an image and question, we argue that an interpretable VQA model should be able to tell what conclusions it can get from which part of the image, and show how each statement help to arrive at an answer. We introduce <span id="id2.id2.1" class="ltx_text ltx_font_typewriter">InterVQA</span>: Interpretable-by-design VQA, where we design an explicit intermediate dynamic reasoning structure for VQA problems and enforce symbolic reasoning that only use the structure for final answer prediction to take place.
<span id="id2.id2.2" class="ltx_text ltx_font_typewriter">InterVQA</span> produces high-quality explicit intermediate reasoning steps, while maintaining similar to the state-of-the-art (sota) end-task performance.</p>
</div>
<figure id="S0.F1" class="ltx_figure"><img src="/html/2305.14882/assets/figures/overall.png" id="S0.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="226" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Schematic illustration of comparisons between (1) chain-of-thought reasoning using black-box LLMs <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>)</cite>, (2) previous VQA methods that consider using raionales <cite class="ltx_cite ltx_citemacro_cite">Schwenk etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>, and (3) our interpretable-by-design VQA system. Each grey rectangle box represents a dynamic abduction condition, which is a natural language sentence that serves as an intermediate condition toward problem solving. See concrete examples of how conditions are generated and fulfilled in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</figcaption>
</figure>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Most existing systems of visual questions answering (VQA) <cite class="ltx_cite ltx_citemacro_cite">Goyal etÂ al. (<a href="#bib.bib6" title="" class="ltx_ref">2017</a>); Li etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite> are black box models â€“ people cannot trust their output because they have little insight into why models make the predictions they do.
It is important to have reliable models that output the correct prediction for the correct reasons.
Specifically, we want the VQA model to tell us what conclusions it can get from which part of the image, and how each statement can help lead us to the correct final answer.
Such a multi-modal interpretation requires understanding of the question while considering the image context and world knowledge.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">Some earlier attempts to rationalize VQA decisions <cite class="ltx_cite ltx_citemacro_cite">Xiong etÂ al. (<a href="#bib.bib18" title="" class="ltx_ref">2016</a>); Shih etÂ al. (<a href="#bib.bib16" title="" class="ltx_ref">2016</a>); Das etÂ al. (<a href="#bib.bib4" title="" class="ltx_ref">2017</a>)</cite> try to answer the question â€œWhere should we look at the image to answer the question?â€ through attention maps. However, it is by design unclear how focusing on certain parts of the image help answer the question.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Several more recent works <cite class="ltx_cite ltx_citemacro_cite">Dua etÂ al. (<a href="#bib.bib5" title="" class="ltx_ref">2021</a>); Schwenk etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite> suggested to provide rationales, which are paragraphs of natural language explanations for an VQA answer, as shown in Figure <a href="#S0.F1" title="Figure 1 â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(a) and (b). Most of the methods, as in (a), follow a post-hoc manner. Given an image and a question, the VQA model first generates an answer, and then generates rationales.
Such post-hoc rationales are hard to evaluate, and cannot be proved that they are the real reasons, or the only sources used in the black-box models while making the final answer prediction.
Some other methods <cite class="ltx_cite ltx_citemacro_cite">Schwenk etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>); Zhang etÂ al. (<a href="#bib.bib20" title="" class="ltx_ref">2023</a>)</cite> as in Figure <a href="#S0.F1" title="Figure 1 â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>(b) avoid the post-hoc manner by using large language models (LLM) to generate rationales first, and then use the output to prompt LLM again for a final answer. However, we cannot know what additional information if any, or which parts of the rationale, are really used by the LLM to predict a final answer. Therefore, it cannot be proved that the models are making correct decisions for correct reasons, and these methods are not that reliable by design.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Inspired by generative language modelsâ€™ <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite> capability of eliciting <em id="S1.p4.1.1" class="ltx_emph ltx_font_italic">rationale</em> for reasoning in open-domain question answering settings <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>); Creswell etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>)</cite>,
we introduce <span id="S1.p4.1.2" class="ltx_text ltx_font_typewriter">InterVQA</span>: an Interpretable by Design VQA system in this paper. To better understand the problem, we first define the structure of explanations needed in VQA, to be in forms of: (1) visual clues, which are natural language descriptions grounded to the image corresponding to â€œwhat statements can be made about the image to help answer the questionâ€, and (2) inferences based on the clues, which show how the visual clues can help arrive at the answer.
We then propose intermediate interpretable structures, which are abduction proposals consisting of dynamic conditions to solve the questions that are dependent on states and requiring additional search on the context.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Given an image and a question pair, <span id="S1.p5.1.1" class="ltx_text ltx_font_typewriter">InterVQA</span> solves the problem in three steps. As shown in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we first generate abduction proposals consisting of dynamic conditions, which are natural language sentences of situations that should be true to lead to a possible answer.
Then, we train to generate visual clues grounded to the image.
Finally, with the help of textual entailment methods, we gradually search for fulfilled dynamic conditions to compose symbolic reasoning graph and majority vote to achieve the final answer.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">We collect a 1.4k dataset for evaluation and learning purpose based on a subset of previous VQA dataset <cite class="ltx_cite ltx_citemacro_cite">Selvaraju etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>, which includes questions that require reasoning instead of simple recognition to answer, using Amazon Turk.
Our results show that we can achieve close to state-of-the-art black-box VQA performance on the collected dataset while providing high quality intermediate symbolic reasonings which humans can trust, since we ensure the model to generate the reasons how it achieves an answer step by step.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Our main contributions are: (a) We propose a new interpretable-by-design VQA framework that explicitly illustrates how it achieves at an answer step by step for the first time; (b) We collect a 1.4k dataset for the problem; (c) We achieve similar performance as a sota black-box VQA model while providing reliable natural language explanations.</p>
</div>
<figure id="S1.F2" class="ltx_figure"><img src="/html/2305.14882/assets/x1.png" id="S1.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="461" height="259" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>An illustration of our interpretable-by-design VQA system on an example visual question answering pair, with explicit steps of (1) abduction proposals on dynamic conditions (Top), (2) visual clue generation (bottom left), and (3) deductive reasoning between visual clues and dynamic conditions, symbolic reasoning graph generation, and majority voting for final answer prediction (bottom right). Note that the blue boxes are conditions directly deducible from visual clues, and blue arrows represent the deductive reasoning. Dashed arrows mean weak deductions.</figcaption>
</figure>
<figure id="S1.F3" class="ltx_figure"><img src="/html/2305.14882/assets/figures/model.png" id="S1.F3.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="538" height="93" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>The modified Blip-2 model we use to generate image clues given an image question pair.</figcaption>
</figure>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Interpretable by design VQA</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Our method consists of three steps: abduction proposals on dynamic conditions, visual clue generation, and symbolic reasoning graph validation and majority voting.
The overview of the proposed model is shown in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.13" class="ltx_p"><span id="S2.p2.13.1" class="ltx_text ltx_font_bold">Problem Formulation</span>
Given a training dataset <math id="S2.p2.1.m1.1" class="ltx_Math" alttext="D=\left\{\left(v_{i},q_{i},a_{i}\right)\right\}_{i=1}^{N}" display="inline"><semantics id="S2.p2.1.m1.1a"><mrow id="S2.p2.1.m1.1.1" xref="S2.p2.1.m1.1.1.cmml"><mi id="S2.p2.1.m1.1.1.3" xref="S2.p2.1.m1.1.1.3.cmml">D</mi><mo id="S2.p2.1.m1.1.1.2" xref="S2.p2.1.m1.1.1.2.cmml">=</mo><msubsup id="S2.p2.1.m1.1.1.1" xref="S2.p2.1.m1.1.1.1.cmml"><mrow id="S2.p2.1.m1.1.1.1.1.1.1" xref="S2.p2.1.m1.1.1.1.1.1.2.cmml"><mo id="S2.p2.1.m1.1.1.1.1.1.1.2" xref="S2.p2.1.m1.1.1.1.1.1.2.cmml">{</mo><mrow id="S2.p2.1.m1.1.1.1.1.1.1.1.3" xref="S2.p2.1.m1.1.1.1.1.1.1.1.4.cmml"><mo id="S2.p2.1.m1.1.1.1.1.1.1.1.3.4" xref="S2.p2.1.m1.1.1.1.1.1.1.1.4.cmml">(</mo><msub id="S2.p2.1.m1.1.1.1.1.1.1.1.1.1" xref="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.2" xref="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.2.cmml">v</mi><mi id="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.3" xref="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.p2.1.m1.1.1.1.1.1.1.1.3.5" xref="S2.p2.1.m1.1.1.1.1.1.1.1.4.cmml">,</mo><msub id="S2.p2.1.m1.1.1.1.1.1.1.1.2.2" xref="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.cmml"><mi id="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.2" xref="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.2.cmml">q</mi><mi id="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.3" xref="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.3.cmml">i</mi></msub><mo id="S2.p2.1.m1.1.1.1.1.1.1.1.3.6" xref="S2.p2.1.m1.1.1.1.1.1.1.1.4.cmml">,</mo><msub id="S2.p2.1.m1.1.1.1.1.1.1.1.3.3" xref="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.cmml"><mi id="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.2" xref="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.2.cmml">a</mi><mi id="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.3" xref="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.3.cmml">i</mi></msub><mo id="S2.p2.1.m1.1.1.1.1.1.1.1.3.7" xref="S2.p2.1.m1.1.1.1.1.1.1.1.4.cmml">)</mo></mrow><mo id="S2.p2.1.m1.1.1.1.1.1.1.3" xref="S2.p2.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S2.p2.1.m1.1.1.1.1.3" xref="S2.p2.1.m1.1.1.1.1.3.cmml"><mi id="S2.p2.1.m1.1.1.1.1.3.2" xref="S2.p2.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S2.p2.1.m1.1.1.1.1.3.1" xref="S2.p2.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S2.p2.1.m1.1.1.1.1.3.3" xref="S2.p2.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mi id="S2.p2.1.m1.1.1.1.3" xref="S2.p2.1.m1.1.1.1.3.cmml">N</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.1.m1.1b"><apply id="S2.p2.1.m1.1.1.cmml" xref="S2.p2.1.m1.1.1"><eq id="S2.p2.1.m1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.2"></eq><ci id="S2.p2.1.m1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.3">ğ·</ci><apply id="S2.p2.1.m1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.1">superscript</csymbol><apply id="S2.p2.1.m1.1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.1">subscript</csymbol><set id="S2.p2.1.m1.1.1.1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1"><vector id="S2.p2.1.m1.1.1.1.1.1.1.1.4.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.3"><apply id="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.2">ğ‘£</ci><ci id="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply><apply id="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.1.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.2.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.2">ğ‘</ci><ci id="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.3.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.2.2.3">ğ‘–</ci></apply><apply id="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.1.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.2.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.2">ğ‘</ci><ci id="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.3.cmml" xref="S2.p2.1.m1.1.1.1.1.1.1.1.3.3.3">ğ‘–</ci></apply></vector></set><apply id="S2.p2.1.m1.1.1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.1.1.3"><eq id="S2.p2.1.m1.1.1.1.1.3.1.cmml" xref="S2.p2.1.m1.1.1.1.1.3.1"></eq><ci id="S2.p2.1.m1.1.1.1.1.3.2.cmml" xref="S2.p2.1.m1.1.1.1.1.3.2">ğ‘–</ci><cn type="integer" id="S2.p2.1.m1.1.1.1.1.3.3.cmml" xref="S2.p2.1.m1.1.1.1.1.3.3">1</cn></apply></apply><ci id="S2.p2.1.m1.1.1.1.3.cmml" xref="S2.p2.1.m1.1.1.1.3">ğ‘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.1.m1.1c">D=\left\{\left(v_{i},q_{i},a_{i}\right)\right\}_{i=1}^{N}</annotation></semantics></math>, where <math id="S2.p2.2.m2.1" class="ltx_Math" alttext="v_{i}" display="inline"><semantics id="S2.p2.2.m2.1a"><msub id="S2.p2.2.m2.1.1" xref="S2.p2.2.m2.1.1.cmml"><mi id="S2.p2.2.m2.1.1.2" xref="S2.p2.2.m2.1.1.2.cmml">v</mi><mi id="S2.p2.2.m2.1.1.3" xref="S2.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.2.m2.1b"><apply id="S2.p2.2.m2.1.1.cmml" xref="S2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.p2.2.m2.1.1.1.cmml" xref="S2.p2.2.m2.1.1">subscript</csymbol><ci id="S2.p2.2.m2.1.1.2.cmml" xref="S2.p2.2.m2.1.1.2">ğ‘£</ci><ci id="S2.p2.2.m2.1.1.3.cmml" xref="S2.p2.2.m2.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.2.m2.1c">v_{i}</annotation></semantics></math> denotes the i-th training image and <math id="S2.p2.3.m3.1" class="ltx_Math" alttext="N" display="inline"><semantics id="S2.p2.3.m3.1a"><mi id="S2.p2.3.m3.1.1" xref="S2.p2.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S2.p2.3.m3.1b"><ci id="S2.p2.3.m3.1.1.cmml" xref="S2.p2.3.m3.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.3.m3.1c">N</annotation></semantics></math> is the total number of the training images, <math id="S2.p2.4.m4.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S2.p2.4.m4.1a"><msub id="S2.p2.4.m4.1.1" xref="S2.p2.4.m4.1.1.cmml"><mi id="S2.p2.4.m4.1.1.2" xref="S2.p2.4.m4.1.1.2.cmml">q</mi><mi id="S2.p2.4.m4.1.1.3" xref="S2.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.4.m4.1b"><apply id="S2.p2.4.m4.1.1.cmml" xref="S2.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S2.p2.4.m4.1.1.1.cmml" xref="S2.p2.4.m4.1.1">subscript</csymbol><ci id="S2.p2.4.m4.1.1.2.cmml" xref="S2.p2.4.m4.1.1.2">ğ‘</ci><ci id="S2.p2.4.m4.1.1.3.cmml" xref="S2.p2.4.m4.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.4.m4.1c">q_{i}</annotation></semantics></math> and <math id="S2.p2.5.m5.1" class="ltx_Math" alttext="a_{i}" display="inline"><semantics id="S2.p2.5.m5.1a"><msub id="S2.p2.5.m5.1.1" xref="S2.p2.5.m5.1.1.cmml"><mi id="S2.p2.5.m5.1.1.2" xref="S2.p2.5.m5.1.1.2.cmml">a</mi><mi id="S2.p2.5.m5.1.1.3" xref="S2.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.5.m5.1b"><apply id="S2.p2.5.m5.1.1.cmml" xref="S2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S2.p2.5.m5.1.1.1.cmml" xref="S2.p2.5.m5.1.1">subscript</csymbol><ci id="S2.p2.5.m5.1.1.2.cmml" xref="S2.p2.5.m5.1.1.2">ğ‘</ci><ci id="S2.p2.5.m5.1.1.3.cmml" xref="S2.p2.5.m5.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.5.m5.1c">a_{i}</annotation></semantics></math> represent the i-th question and its corresponding answer, respectively.
We first generate a set of answer choices <math id="S2.p2.6.m6.4" class="ltx_Math" alttext="\hat{A_{i}}=\{\hat{a_{i0}},\hat{a_{i1}},\hat{a_{i2}},...\}" display="inline"><semantics id="S2.p2.6.m6.4a"><mrow id="S2.p2.6.m6.4.5" xref="S2.p2.6.m6.4.5.cmml"><mover accent="true" id="S2.p2.6.m6.4.5.2" xref="S2.p2.6.m6.4.5.2.cmml"><msub id="S2.p2.6.m6.4.5.2.2" xref="S2.p2.6.m6.4.5.2.2.cmml"><mi id="S2.p2.6.m6.4.5.2.2.2" xref="S2.p2.6.m6.4.5.2.2.2.cmml">A</mi><mi id="S2.p2.6.m6.4.5.2.2.3" xref="S2.p2.6.m6.4.5.2.2.3.cmml">i</mi></msub><mo id="S2.p2.6.m6.4.5.2.1" xref="S2.p2.6.m6.4.5.2.1.cmml">^</mo></mover><mo id="S2.p2.6.m6.4.5.1" xref="S2.p2.6.m6.4.5.1.cmml">=</mo><mrow id="S2.p2.6.m6.4.5.3.2" xref="S2.p2.6.m6.4.5.3.1.cmml"><mo stretchy="false" id="S2.p2.6.m6.4.5.3.2.1" xref="S2.p2.6.m6.4.5.3.1.cmml">{</mo><mover accent="true" id="S2.p2.6.m6.1.1" xref="S2.p2.6.m6.1.1.cmml"><msub id="S2.p2.6.m6.1.1.2" xref="S2.p2.6.m6.1.1.2.cmml"><mi id="S2.p2.6.m6.1.1.2.2" xref="S2.p2.6.m6.1.1.2.2.cmml">a</mi><mrow id="S2.p2.6.m6.1.1.2.3" xref="S2.p2.6.m6.1.1.2.3.cmml"><mi id="S2.p2.6.m6.1.1.2.3.2" xref="S2.p2.6.m6.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p2.6.m6.1.1.2.3.1" xref="S2.p2.6.m6.1.1.2.3.1.cmml">â€‹</mo><mn id="S2.p2.6.m6.1.1.2.3.3" xref="S2.p2.6.m6.1.1.2.3.3.cmml">0</mn></mrow></msub><mo id="S2.p2.6.m6.1.1.1" xref="S2.p2.6.m6.1.1.1.cmml">^</mo></mover><mo id="S2.p2.6.m6.4.5.3.2.2" xref="S2.p2.6.m6.4.5.3.1.cmml">,</mo><mover accent="true" id="S2.p2.6.m6.2.2" xref="S2.p2.6.m6.2.2.cmml"><msub id="S2.p2.6.m6.2.2.2" xref="S2.p2.6.m6.2.2.2.cmml"><mi id="S2.p2.6.m6.2.2.2.2" xref="S2.p2.6.m6.2.2.2.2.cmml">a</mi><mrow id="S2.p2.6.m6.2.2.2.3" xref="S2.p2.6.m6.2.2.2.3.cmml"><mi id="S2.p2.6.m6.2.2.2.3.2" xref="S2.p2.6.m6.2.2.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p2.6.m6.2.2.2.3.1" xref="S2.p2.6.m6.2.2.2.3.1.cmml">â€‹</mo><mn id="S2.p2.6.m6.2.2.2.3.3" xref="S2.p2.6.m6.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.p2.6.m6.2.2.1" xref="S2.p2.6.m6.2.2.1.cmml">^</mo></mover><mo id="S2.p2.6.m6.4.5.3.2.3" xref="S2.p2.6.m6.4.5.3.1.cmml">,</mo><mover accent="true" id="S2.p2.6.m6.3.3" xref="S2.p2.6.m6.3.3.cmml"><msub id="S2.p2.6.m6.3.3.2" xref="S2.p2.6.m6.3.3.2.cmml"><mi id="S2.p2.6.m6.3.3.2.2" xref="S2.p2.6.m6.3.3.2.2.cmml">a</mi><mrow id="S2.p2.6.m6.3.3.2.3" xref="S2.p2.6.m6.3.3.2.3.cmml"><mi id="S2.p2.6.m6.3.3.2.3.2" xref="S2.p2.6.m6.3.3.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p2.6.m6.3.3.2.3.1" xref="S2.p2.6.m6.3.3.2.3.1.cmml">â€‹</mo><mn id="S2.p2.6.m6.3.3.2.3.3" xref="S2.p2.6.m6.3.3.2.3.3.cmml">2</mn></mrow></msub><mo id="S2.p2.6.m6.3.3.1" xref="S2.p2.6.m6.3.3.1.cmml">^</mo></mover><mo id="S2.p2.6.m6.4.5.3.2.4" xref="S2.p2.6.m6.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.p2.6.m6.4.4" xref="S2.p2.6.m6.4.4.cmml">â€¦</mi><mo stretchy="false" id="S2.p2.6.m6.4.5.3.2.5" xref="S2.p2.6.m6.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.6.m6.4b"><apply id="S2.p2.6.m6.4.5.cmml" xref="S2.p2.6.m6.4.5"><eq id="S2.p2.6.m6.4.5.1.cmml" xref="S2.p2.6.m6.4.5.1"></eq><apply id="S2.p2.6.m6.4.5.2.cmml" xref="S2.p2.6.m6.4.5.2"><ci id="S2.p2.6.m6.4.5.2.1.cmml" xref="S2.p2.6.m6.4.5.2.1">^</ci><apply id="S2.p2.6.m6.4.5.2.2.cmml" xref="S2.p2.6.m6.4.5.2.2"><csymbol cd="ambiguous" id="S2.p2.6.m6.4.5.2.2.1.cmml" xref="S2.p2.6.m6.4.5.2.2">subscript</csymbol><ci id="S2.p2.6.m6.4.5.2.2.2.cmml" xref="S2.p2.6.m6.4.5.2.2.2">ğ´</ci><ci id="S2.p2.6.m6.4.5.2.2.3.cmml" xref="S2.p2.6.m6.4.5.2.2.3">ğ‘–</ci></apply></apply><set id="S2.p2.6.m6.4.5.3.1.cmml" xref="S2.p2.6.m6.4.5.3.2"><apply id="S2.p2.6.m6.1.1.cmml" xref="S2.p2.6.m6.1.1"><ci id="S2.p2.6.m6.1.1.1.cmml" xref="S2.p2.6.m6.1.1.1">^</ci><apply id="S2.p2.6.m6.1.1.2.cmml" xref="S2.p2.6.m6.1.1.2"><csymbol cd="ambiguous" id="S2.p2.6.m6.1.1.2.1.cmml" xref="S2.p2.6.m6.1.1.2">subscript</csymbol><ci id="S2.p2.6.m6.1.1.2.2.cmml" xref="S2.p2.6.m6.1.1.2.2">ğ‘</ci><apply id="S2.p2.6.m6.1.1.2.3.cmml" xref="S2.p2.6.m6.1.1.2.3"><times id="S2.p2.6.m6.1.1.2.3.1.cmml" xref="S2.p2.6.m6.1.1.2.3.1"></times><ci id="S2.p2.6.m6.1.1.2.3.2.cmml" xref="S2.p2.6.m6.1.1.2.3.2">ğ‘–</ci><cn type="integer" id="S2.p2.6.m6.1.1.2.3.3.cmml" xref="S2.p2.6.m6.1.1.2.3.3">0</cn></apply></apply></apply><apply id="S2.p2.6.m6.2.2.cmml" xref="S2.p2.6.m6.2.2"><ci id="S2.p2.6.m6.2.2.1.cmml" xref="S2.p2.6.m6.2.2.1">^</ci><apply id="S2.p2.6.m6.2.2.2.cmml" xref="S2.p2.6.m6.2.2.2"><csymbol cd="ambiguous" id="S2.p2.6.m6.2.2.2.1.cmml" xref="S2.p2.6.m6.2.2.2">subscript</csymbol><ci id="S2.p2.6.m6.2.2.2.2.cmml" xref="S2.p2.6.m6.2.2.2.2">ğ‘</ci><apply id="S2.p2.6.m6.2.2.2.3.cmml" xref="S2.p2.6.m6.2.2.2.3"><times id="S2.p2.6.m6.2.2.2.3.1.cmml" xref="S2.p2.6.m6.2.2.2.3.1"></times><ci id="S2.p2.6.m6.2.2.2.3.2.cmml" xref="S2.p2.6.m6.2.2.2.3.2">ğ‘–</ci><cn type="integer" id="S2.p2.6.m6.2.2.2.3.3.cmml" xref="S2.p2.6.m6.2.2.2.3.3">1</cn></apply></apply></apply><apply id="S2.p2.6.m6.3.3.cmml" xref="S2.p2.6.m6.3.3"><ci id="S2.p2.6.m6.3.3.1.cmml" xref="S2.p2.6.m6.3.3.1">^</ci><apply id="S2.p2.6.m6.3.3.2.cmml" xref="S2.p2.6.m6.3.3.2"><csymbol cd="ambiguous" id="S2.p2.6.m6.3.3.2.1.cmml" xref="S2.p2.6.m6.3.3.2">subscript</csymbol><ci id="S2.p2.6.m6.3.3.2.2.cmml" xref="S2.p2.6.m6.3.3.2.2">ğ‘</ci><apply id="S2.p2.6.m6.3.3.2.3.cmml" xref="S2.p2.6.m6.3.3.2.3"><times id="S2.p2.6.m6.3.3.2.3.1.cmml" xref="S2.p2.6.m6.3.3.2.3.1"></times><ci id="S2.p2.6.m6.3.3.2.3.2.cmml" xref="S2.p2.6.m6.3.3.2.3.2">ğ‘–</ci><cn type="integer" id="S2.p2.6.m6.3.3.2.3.3.cmml" xref="S2.p2.6.m6.3.3.2.3.3">2</cn></apply></apply></apply><ci id="S2.p2.6.m6.4.4.cmml" xref="S2.p2.6.m6.4.4">â€¦</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.6.m6.4c">\hat{A_{i}}=\{\hat{a_{i0}},\hat{a_{i1}},\hat{a_{i2}},...\}</annotation></semantics></math> using a frozen PLM <math id="S2.p2.7.m7.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.p2.7.m7.1a"><mi id="S2.p2.7.m7.1.1" xref="S2.p2.7.m7.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.p2.7.m7.1b"><ci id="S2.p2.7.m7.1.1.cmml" xref="S2.p2.7.m7.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.7.m7.1c">g</annotation></semantics></math>, then search for abduction proposals composed of dynamic conditions, which are situations for possible answers to become true
for possible answer <math id="S2.p2.8.m8.1" class="ltx_Math" alttext="\hat{a_{ij}}" display="inline"><semantics id="S2.p2.8.m8.1a"><mover accent="true" id="S2.p2.8.m8.1.1" xref="S2.p2.8.m8.1.1.cmml"><msub id="S2.p2.8.m8.1.1.2" xref="S2.p2.8.m8.1.1.2.cmml"><mi id="S2.p2.8.m8.1.1.2.2" xref="S2.p2.8.m8.1.1.2.2.cmml">a</mi><mrow id="S2.p2.8.m8.1.1.2.3" xref="S2.p2.8.m8.1.1.2.3.cmml"><mi id="S2.p2.8.m8.1.1.2.3.2" xref="S2.p2.8.m8.1.1.2.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p2.8.m8.1.1.2.3.1" xref="S2.p2.8.m8.1.1.2.3.1.cmml">â€‹</mo><mi id="S2.p2.8.m8.1.1.2.3.3" xref="S2.p2.8.m8.1.1.2.3.3.cmml">j</mi></mrow></msub><mo id="S2.p2.8.m8.1.1.1" xref="S2.p2.8.m8.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S2.p2.8.m8.1b"><apply id="S2.p2.8.m8.1.1.cmml" xref="S2.p2.8.m8.1.1"><ci id="S2.p2.8.m8.1.1.1.cmml" xref="S2.p2.8.m8.1.1.1">^</ci><apply id="S2.p2.8.m8.1.1.2.cmml" xref="S2.p2.8.m8.1.1.2"><csymbol cd="ambiguous" id="S2.p2.8.m8.1.1.2.1.cmml" xref="S2.p2.8.m8.1.1.2">subscript</csymbol><ci id="S2.p2.8.m8.1.1.2.2.cmml" xref="S2.p2.8.m8.1.1.2.2">ğ‘</ci><apply id="S2.p2.8.m8.1.1.2.3.cmml" xref="S2.p2.8.m8.1.1.2.3"><times id="S2.p2.8.m8.1.1.2.3.1.cmml" xref="S2.p2.8.m8.1.1.2.3.1"></times><ci id="S2.p2.8.m8.1.1.2.3.2.cmml" xref="S2.p2.8.m8.1.1.2.3.2">ğ‘–</ci><ci id="S2.p2.8.m8.1.1.2.3.3.cmml" xref="S2.p2.8.m8.1.1.2.3.3">ğ‘—</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.8.m8.1c">\hat{a_{ij}}</annotation></semantics></math> using <math id="S2.p2.9.m9.1" class="ltx_Math" alttext="g" display="inline"><semantics id="S2.p2.9.m9.1a"><mi id="S2.p2.9.m9.1.1" xref="S2.p2.9.m9.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S2.p2.9.m9.1b"><ci id="S2.p2.9.m9.1.1.cmml" xref="S2.p2.9.m9.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.9.m9.1c">g</annotation></semantics></math>.
We train a separate model <math id="S2.p2.10.m10.1" class="ltx_Math" alttext="b" display="inline"><semantics id="S2.p2.10.m10.1a"><mi id="S2.p2.10.m10.1.1" xref="S2.p2.10.m10.1.1.cmml">b</mi><annotation-xml encoding="MathML-Content" id="S2.p2.10.m10.1b"><ci id="S2.p2.10.m10.1.1.cmml" xref="S2.p2.10.m10.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.10.m10.1c">b</annotation></semantics></math> that generates visual clues <math id="S2.p2.11.m11.4" class="ltx_Math" alttext="\hat{VC_{i}}=\{\hat{vc_{i0}},\hat{vc_{i1}},\hat{vc_{i2}},...\}" display="inline"><semantics id="S2.p2.11.m11.4a"><mrow id="S2.p2.11.m11.4.5" xref="S2.p2.11.m11.4.5.cmml"><mover accent="true" id="S2.p2.11.m11.4.5.2" xref="S2.p2.11.m11.4.5.2.cmml"><mrow id="S2.p2.11.m11.4.5.2.2" xref="S2.p2.11.m11.4.5.2.2.cmml"><mi id="S2.p2.11.m11.4.5.2.2.2" xref="S2.p2.11.m11.4.5.2.2.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m11.4.5.2.2.1" xref="S2.p2.11.m11.4.5.2.2.1.cmml">â€‹</mo><msub id="S2.p2.11.m11.4.5.2.2.3" xref="S2.p2.11.m11.4.5.2.2.3.cmml"><mi id="S2.p2.11.m11.4.5.2.2.3.2" xref="S2.p2.11.m11.4.5.2.2.3.2.cmml">C</mi><mi id="S2.p2.11.m11.4.5.2.2.3.3" xref="S2.p2.11.m11.4.5.2.2.3.3.cmml">i</mi></msub></mrow><mo id="S2.p2.11.m11.4.5.2.1" xref="S2.p2.11.m11.4.5.2.1.cmml">^</mo></mover><mo id="S2.p2.11.m11.4.5.1" xref="S2.p2.11.m11.4.5.1.cmml">=</mo><mrow id="S2.p2.11.m11.4.5.3.2" xref="S2.p2.11.m11.4.5.3.1.cmml"><mo stretchy="false" id="S2.p2.11.m11.4.5.3.2.1" xref="S2.p2.11.m11.4.5.3.1.cmml">{</mo><mover accent="true" id="S2.p2.11.m11.1.1" xref="S2.p2.11.m11.1.1.cmml"><mrow id="S2.p2.11.m11.1.1.2" xref="S2.p2.11.m11.1.1.2.cmml"><mi id="S2.p2.11.m11.1.1.2.2" xref="S2.p2.11.m11.1.1.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m11.1.1.2.1" xref="S2.p2.11.m11.1.1.2.1.cmml">â€‹</mo><msub id="S2.p2.11.m11.1.1.2.3" xref="S2.p2.11.m11.1.1.2.3.cmml"><mi id="S2.p2.11.m11.1.1.2.3.2" xref="S2.p2.11.m11.1.1.2.3.2.cmml">c</mi><mrow id="S2.p2.11.m11.1.1.2.3.3" xref="S2.p2.11.m11.1.1.2.3.3.cmml"><mi id="S2.p2.11.m11.1.1.2.3.3.2" xref="S2.p2.11.m11.1.1.2.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m11.1.1.2.3.3.1" xref="S2.p2.11.m11.1.1.2.3.3.1.cmml">â€‹</mo><mn id="S2.p2.11.m11.1.1.2.3.3.3" xref="S2.p2.11.m11.1.1.2.3.3.3.cmml">0</mn></mrow></msub></mrow><mo id="S2.p2.11.m11.1.1.1" xref="S2.p2.11.m11.1.1.1.cmml">^</mo></mover><mo id="S2.p2.11.m11.4.5.3.2.2" xref="S2.p2.11.m11.4.5.3.1.cmml">,</mo><mover accent="true" id="S2.p2.11.m11.2.2" xref="S2.p2.11.m11.2.2.cmml"><mrow id="S2.p2.11.m11.2.2.2" xref="S2.p2.11.m11.2.2.2.cmml"><mi id="S2.p2.11.m11.2.2.2.2" xref="S2.p2.11.m11.2.2.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m11.2.2.2.1" xref="S2.p2.11.m11.2.2.2.1.cmml">â€‹</mo><msub id="S2.p2.11.m11.2.2.2.3" xref="S2.p2.11.m11.2.2.2.3.cmml"><mi id="S2.p2.11.m11.2.2.2.3.2" xref="S2.p2.11.m11.2.2.2.3.2.cmml">c</mi><mrow id="S2.p2.11.m11.2.2.2.3.3" xref="S2.p2.11.m11.2.2.2.3.3.cmml"><mi id="S2.p2.11.m11.2.2.2.3.3.2" xref="S2.p2.11.m11.2.2.2.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m11.2.2.2.3.3.1" xref="S2.p2.11.m11.2.2.2.3.3.1.cmml">â€‹</mo><mn id="S2.p2.11.m11.2.2.2.3.3.3" xref="S2.p2.11.m11.2.2.2.3.3.3.cmml">1</mn></mrow></msub></mrow><mo id="S2.p2.11.m11.2.2.1" xref="S2.p2.11.m11.2.2.1.cmml">^</mo></mover><mo id="S2.p2.11.m11.4.5.3.2.3" xref="S2.p2.11.m11.4.5.3.1.cmml">,</mo><mover accent="true" id="S2.p2.11.m11.3.3" xref="S2.p2.11.m11.3.3.cmml"><mrow id="S2.p2.11.m11.3.3.2" xref="S2.p2.11.m11.3.3.2.cmml"><mi id="S2.p2.11.m11.3.3.2.2" xref="S2.p2.11.m11.3.3.2.2.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m11.3.3.2.1" xref="S2.p2.11.m11.3.3.2.1.cmml">â€‹</mo><msub id="S2.p2.11.m11.3.3.2.3" xref="S2.p2.11.m11.3.3.2.3.cmml"><mi id="S2.p2.11.m11.3.3.2.3.2" xref="S2.p2.11.m11.3.3.2.3.2.cmml">c</mi><mrow id="S2.p2.11.m11.3.3.2.3.3" xref="S2.p2.11.m11.3.3.2.3.3.cmml"><mi id="S2.p2.11.m11.3.3.2.3.3.2" xref="S2.p2.11.m11.3.3.2.3.3.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="S2.p2.11.m11.3.3.2.3.3.1" xref="S2.p2.11.m11.3.3.2.3.3.1.cmml">â€‹</mo><mn id="S2.p2.11.m11.3.3.2.3.3.3" xref="S2.p2.11.m11.3.3.2.3.3.3.cmml">2</mn></mrow></msub></mrow><mo id="S2.p2.11.m11.3.3.1" xref="S2.p2.11.m11.3.3.1.cmml">^</mo></mover><mo id="S2.p2.11.m11.4.5.3.2.4" xref="S2.p2.11.m11.4.5.3.1.cmml">,</mo><mi mathvariant="normal" id="S2.p2.11.m11.4.4" xref="S2.p2.11.m11.4.4.cmml">â€¦</mi><mo stretchy="false" id="S2.p2.11.m11.4.5.3.2.5" xref="S2.p2.11.m11.4.5.3.1.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.11.m11.4b"><apply id="S2.p2.11.m11.4.5.cmml" xref="S2.p2.11.m11.4.5"><eq id="S2.p2.11.m11.4.5.1.cmml" xref="S2.p2.11.m11.4.5.1"></eq><apply id="S2.p2.11.m11.4.5.2.cmml" xref="S2.p2.11.m11.4.5.2"><ci id="S2.p2.11.m11.4.5.2.1.cmml" xref="S2.p2.11.m11.4.5.2.1">^</ci><apply id="S2.p2.11.m11.4.5.2.2.cmml" xref="S2.p2.11.m11.4.5.2.2"><times id="S2.p2.11.m11.4.5.2.2.1.cmml" xref="S2.p2.11.m11.4.5.2.2.1"></times><ci id="S2.p2.11.m11.4.5.2.2.2.cmml" xref="S2.p2.11.m11.4.5.2.2.2">ğ‘‰</ci><apply id="S2.p2.11.m11.4.5.2.2.3.cmml" xref="S2.p2.11.m11.4.5.2.2.3"><csymbol cd="ambiguous" id="S2.p2.11.m11.4.5.2.2.3.1.cmml" xref="S2.p2.11.m11.4.5.2.2.3">subscript</csymbol><ci id="S2.p2.11.m11.4.5.2.2.3.2.cmml" xref="S2.p2.11.m11.4.5.2.2.3.2">ğ¶</ci><ci id="S2.p2.11.m11.4.5.2.2.3.3.cmml" xref="S2.p2.11.m11.4.5.2.2.3.3">ğ‘–</ci></apply></apply></apply><set id="S2.p2.11.m11.4.5.3.1.cmml" xref="S2.p2.11.m11.4.5.3.2"><apply id="S2.p2.11.m11.1.1.cmml" xref="S2.p2.11.m11.1.1"><ci id="S2.p2.11.m11.1.1.1.cmml" xref="S2.p2.11.m11.1.1.1">^</ci><apply id="S2.p2.11.m11.1.1.2.cmml" xref="S2.p2.11.m11.1.1.2"><times id="S2.p2.11.m11.1.1.2.1.cmml" xref="S2.p2.11.m11.1.1.2.1"></times><ci id="S2.p2.11.m11.1.1.2.2.cmml" xref="S2.p2.11.m11.1.1.2.2">ğ‘£</ci><apply id="S2.p2.11.m11.1.1.2.3.cmml" xref="S2.p2.11.m11.1.1.2.3"><csymbol cd="ambiguous" id="S2.p2.11.m11.1.1.2.3.1.cmml" xref="S2.p2.11.m11.1.1.2.3">subscript</csymbol><ci id="S2.p2.11.m11.1.1.2.3.2.cmml" xref="S2.p2.11.m11.1.1.2.3.2">ğ‘</ci><apply id="S2.p2.11.m11.1.1.2.3.3.cmml" xref="S2.p2.11.m11.1.1.2.3.3"><times id="S2.p2.11.m11.1.1.2.3.3.1.cmml" xref="S2.p2.11.m11.1.1.2.3.3.1"></times><ci id="S2.p2.11.m11.1.1.2.3.3.2.cmml" xref="S2.p2.11.m11.1.1.2.3.3.2">ğ‘–</ci><cn type="integer" id="S2.p2.11.m11.1.1.2.3.3.3.cmml" xref="S2.p2.11.m11.1.1.2.3.3.3">0</cn></apply></apply></apply></apply><apply id="S2.p2.11.m11.2.2.cmml" xref="S2.p2.11.m11.2.2"><ci id="S2.p2.11.m11.2.2.1.cmml" xref="S2.p2.11.m11.2.2.1">^</ci><apply id="S2.p2.11.m11.2.2.2.cmml" xref="S2.p2.11.m11.2.2.2"><times id="S2.p2.11.m11.2.2.2.1.cmml" xref="S2.p2.11.m11.2.2.2.1"></times><ci id="S2.p2.11.m11.2.2.2.2.cmml" xref="S2.p2.11.m11.2.2.2.2">ğ‘£</ci><apply id="S2.p2.11.m11.2.2.2.3.cmml" xref="S2.p2.11.m11.2.2.2.3"><csymbol cd="ambiguous" id="S2.p2.11.m11.2.2.2.3.1.cmml" xref="S2.p2.11.m11.2.2.2.3">subscript</csymbol><ci id="S2.p2.11.m11.2.2.2.3.2.cmml" xref="S2.p2.11.m11.2.2.2.3.2">ğ‘</ci><apply id="S2.p2.11.m11.2.2.2.3.3.cmml" xref="S2.p2.11.m11.2.2.2.3.3"><times id="S2.p2.11.m11.2.2.2.3.3.1.cmml" xref="S2.p2.11.m11.2.2.2.3.3.1"></times><ci id="S2.p2.11.m11.2.2.2.3.3.2.cmml" xref="S2.p2.11.m11.2.2.2.3.3.2">ğ‘–</ci><cn type="integer" id="S2.p2.11.m11.2.2.2.3.3.3.cmml" xref="S2.p2.11.m11.2.2.2.3.3.3">1</cn></apply></apply></apply></apply><apply id="S2.p2.11.m11.3.3.cmml" xref="S2.p2.11.m11.3.3"><ci id="S2.p2.11.m11.3.3.1.cmml" xref="S2.p2.11.m11.3.3.1">^</ci><apply id="S2.p2.11.m11.3.3.2.cmml" xref="S2.p2.11.m11.3.3.2"><times id="S2.p2.11.m11.3.3.2.1.cmml" xref="S2.p2.11.m11.3.3.2.1"></times><ci id="S2.p2.11.m11.3.3.2.2.cmml" xref="S2.p2.11.m11.3.3.2.2">ğ‘£</ci><apply id="S2.p2.11.m11.3.3.2.3.cmml" xref="S2.p2.11.m11.3.3.2.3"><csymbol cd="ambiguous" id="S2.p2.11.m11.3.3.2.3.1.cmml" xref="S2.p2.11.m11.3.3.2.3">subscript</csymbol><ci id="S2.p2.11.m11.3.3.2.3.2.cmml" xref="S2.p2.11.m11.3.3.2.3.2">ğ‘</ci><apply id="S2.p2.11.m11.3.3.2.3.3.cmml" xref="S2.p2.11.m11.3.3.2.3.3"><times id="S2.p2.11.m11.3.3.2.3.3.1.cmml" xref="S2.p2.11.m11.3.3.2.3.3.1"></times><ci id="S2.p2.11.m11.3.3.2.3.3.2.cmml" xref="S2.p2.11.m11.3.3.2.3.3.2">ğ‘–</ci><cn type="integer" id="S2.p2.11.m11.3.3.2.3.3.3.cmml" xref="S2.p2.11.m11.3.3.2.3.3.3">2</cn></apply></apply></apply></apply><ci id="S2.p2.11.m11.4.4.cmml" xref="S2.p2.11.m11.4.4">â€¦</ci></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.11.m11.4c">\hat{VC_{i}}=\{\hat{vc_{i0}},\hat{vc_{i1}},\hat{vc_{i2}},...\}</annotation></semantics></math> based on the i-th image that are related to question <math id="S2.p2.12.m12.1" class="ltx_Math" alttext="q_{i}" display="inline"><semantics id="S2.p2.12.m12.1a"><msub id="S2.p2.12.m12.1.1" xref="S2.p2.12.m12.1.1.cmml"><mi id="S2.p2.12.m12.1.1.2" xref="S2.p2.12.m12.1.1.2.cmml">q</mi><mi id="S2.p2.12.m12.1.1.3" xref="S2.p2.12.m12.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.p2.12.m12.1b"><apply id="S2.p2.12.m12.1.1.cmml" xref="S2.p2.12.m12.1.1"><csymbol cd="ambiguous" id="S2.p2.12.m12.1.1.1.cmml" xref="S2.p2.12.m12.1.1">subscript</csymbol><ci id="S2.p2.12.m12.1.1.2.cmml" xref="S2.p2.12.m12.1.1.2">ğ‘</ci><ci id="S2.p2.12.m12.1.1.3.cmml" xref="S2.p2.12.m12.1.1.3">ğ‘–</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.12.m12.1c">q_{i}</annotation></semantics></math>.
Finally, we deploy entailment methods to discover all the fulfilled dynamic conditions by the visual clues, and then form valid symbolic reasoning paths such as <math id="S2.p2.13.m13.2" class="ltx_Math" alttext="e_{1},e_{2}" display="inline"><semantics id="S2.p2.13.m13.2a"><mrow id="S2.p2.13.m13.2.2.2" xref="S2.p2.13.m13.2.2.3.cmml"><msub id="S2.p2.13.m13.1.1.1.1" xref="S2.p2.13.m13.1.1.1.1.cmml"><mi id="S2.p2.13.m13.1.1.1.1.2" xref="S2.p2.13.m13.1.1.1.1.2.cmml">e</mi><mn id="S2.p2.13.m13.1.1.1.1.3" xref="S2.p2.13.m13.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.p2.13.m13.2.2.2.3" xref="S2.p2.13.m13.2.2.3.cmml">,</mo><msub id="S2.p2.13.m13.2.2.2.2" xref="S2.p2.13.m13.2.2.2.2.cmml"><mi id="S2.p2.13.m13.2.2.2.2.2" xref="S2.p2.13.m13.2.2.2.2.2.cmml">e</mi><mn id="S2.p2.13.m13.2.2.2.2.3" xref="S2.p2.13.m13.2.2.2.2.3.cmml">2</mn></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.p2.13.m13.2b"><list id="S2.p2.13.m13.2.2.3.cmml" xref="S2.p2.13.m13.2.2.2"><apply id="S2.p2.13.m13.1.1.1.1.cmml" xref="S2.p2.13.m13.1.1.1.1"><csymbol cd="ambiguous" id="S2.p2.13.m13.1.1.1.1.1.cmml" xref="S2.p2.13.m13.1.1.1.1">subscript</csymbol><ci id="S2.p2.13.m13.1.1.1.1.2.cmml" xref="S2.p2.13.m13.1.1.1.1.2">ğ‘’</ci><cn type="integer" id="S2.p2.13.m13.1.1.1.1.3.cmml" xref="S2.p2.13.m13.1.1.1.1.3">1</cn></apply><apply id="S2.p2.13.m13.2.2.2.2.cmml" xref="S2.p2.13.m13.2.2.2.2"><csymbol cd="ambiguous" id="S2.p2.13.m13.2.2.2.2.1.cmml" xref="S2.p2.13.m13.2.2.2.2">subscript</csymbol><ci id="S2.p2.13.m13.2.2.2.2.2.cmml" xref="S2.p2.13.m13.2.2.2.2.2">ğ‘’</ci><cn type="integer" id="S2.p2.13.m13.2.2.2.2.3.cmml" xref="S2.p2.13.m13.2.2.2.2.3">2</cn></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.p2.13.m13.2c">e_{1},e_{2}</annotation></semantics></math> in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>. We do majority voting and select the answer with most symbolic reasoning paths as final prediction.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Abduction Proposals</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.1" class="ltx_p">We define <span id="S2.SS1.p1.1.1" class="ltx_text ltx_font_italic">Abduction Proposals</span> as the dynamic reasoning graphs composed of multiple intermediate dynamic conditions that require external search on the image to make the possible answer be true for the given question, as shown in the top of Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>.
Given an image-question pair, we first use the question to retrieve all the possible answers as well as the abduction proposals that could make these possible answers become true.
To retrieve high-quality abduction proposals, we use in-context learning with frozen LLMs, specifically GPT-3.5 and GPT-4 <cite class="ltx_cite ltx_citemacro_cite">Brown etÂ al. (<a href="#bib.bib1" title="" class="ltx_ref">2020</a>)</cite>.
Following previous works, we use 4-shot in-context examples and iteratively retrieve the abduction proposals.
The goal of this step is to provide a reliable intermediate interpretable structure for any open-ended question.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Visual Clue Generator</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We define <span id="S2.SS2.p1.1.1" class="ltx_text ltx_font_italic">Visual Clue</span> as natural language descriptions that help to answer the question while grounded to the corresponding image.
For clarification, for same question and different images, or same image and different questions, visual clues should be different.
The goal of this step is to develop a Visual-LLM to complete this step, and we modify and fine-tune the BLIP-2 model <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite> for this purpose, as shown in Figure <a href="#S1.F3" title="Figure 3 â€£ 1 Introduction â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>.
The model is trained with the standard language modeling loss to directly generate all the visual clues given image and question.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para">
<p id="S2.SS2.p2.4" class="ltx_p">There are some differences between the our visual clue generator and the original BLIP-2 model. Specifically, as illustrated in Figure <a href="#S1.F3" title="Figure 3 â€£ 1 Introduction â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, we add text inputs of the question, and a prompt of the format â€œQuestion: <span id="S2.SS2.p2.4.1" class="ltx_text ltx_font_italic">question</span> Clues:â€ to the Q-former and the frozen LLM during training for our desired output.
Also, the original BLIP-2 model was mainly trained for image captioning, and for <math id="S2.SS2.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p2.1.m1.1a"><mi id="S2.SS2.p2.1.m1.1.1" xref="S2.SS2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.1.m1.1b"><ci id="S2.SS2.p2.1.m1.1.1.cmml" xref="S2.SS2.p2.1.m1.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.1.m1.1c">n</annotation></semantics></math> captions they use <math id="S2.SS2.p2.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p2.2.m2.1a"><mi id="S2.SS2.p2.2.m2.1.1" xref="S2.SS2.p2.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.2.m2.1b"><ci id="S2.SS2.p2.2.m2.1.1.cmml" xref="S2.SS2.p2.2.m2.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.2.m2.1c">n</annotation></semantics></math> input data, with each data containing one caption. In our case, we follow <cite class="ltx_cite ltx_citemacro_cite">Klein etÂ al. (<a href="#bib.bib9" title="" class="ltx_ref">2022</a>)</cite> and use <math id="S2.SS2.p2.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p2.3.m3.1a"><mi id="S2.SS2.p2.3.m3.1.1" xref="S2.SS2.p2.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.3.m3.1b"><ci id="S2.SS2.p2.3.m3.1.1.cmml" xref="S2.SS2.p2.3.m3.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.3.m3.1c">n</annotation></semantics></math> input data, with each data containing one unique permutation of concatenated visual clues if the image has <math id="S2.SS2.p2.4.m4.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS2.p2.4.m4.1a"><mi id="S2.SS2.p2.4.m4.1.1" xref="S2.SS2.p2.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p2.4.m4.1b"><ci id="S2.SS2.p2.4.m4.1.1.cmml" xref="S2.SS2.p2.4.m4.1.1">ğ‘›</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p2.4.m4.1c">n</annotation></semantics></math> visual clues.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Implementation Details</span>
We implement our visual clue generator based on the BLIP-2 model in LAVIS library <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite>. In our experiments, we adopt two variations of BLIP-2 with the same image encoder ViT-L/14 from CLIP <cite class="ltx_cite ltx_citemacro_cite">Radford etÂ al. (<a href="#bib.bib13" title="" class="ltx_ref">2021</a>)</cite>) but different frozen LLMs, including FlanT5-XL (3B)and FlanT5-XXL (11B), following the original BLIP-2 paper.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para">
<p id="S2.SS2.p4.1" class="ltx_p">We fine-tune all models with a maximum of 15 epochs. For each model, a single optimal checkpoint is selected and used for evaluation. We employ a batch size of 12 for all models, The AdamW <cite class="ltx_cite ltx_citemacro_cite">Loshchilov and Hutter (<a href="#bib.bib11" title="" class="ltx_ref">2017</a>)</cite> optimizer is used wih a weight decay of 0.01. we use a constant learning rate of 10-7 instead of the linear warmup with cosine decay learning rate as in the original paper.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para">
<p id="S2.SS2.p5.3" class="ltx_p">Additionally, when we apply a two-stage training strategy and first fine-tune on a large weak supervision dataset, we apply a linear warmup of the learning rate during the initial 3K steps,
increasing from <math id="S2.SS2.p5.1.m1.1" class="ltx_Math" alttext="1\mathrm{e}{-8}" display="inline"><semantics id="S2.SS2.p5.1.m1.1a"><mrow id="S2.SS2.p5.1.m1.1.1" xref="S2.SS2.p5.1.m1.1.1.cmml"><mrow id="S2.SS2.p5.1.m1.1.1.2" xref="S2.SS2.p5.1.m1.1.1.2.cmml"><mn id="S2.SS2.p5.1.m1.1.1.2.2" xref="S2.SS2.p5.1.m1.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p5.1.m1.1.1.2.1" xref="S2.SS2.p5.1.m1.1.1.2.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.SS2.p5.1.m1.1.1.2.3" xref="S2.SS2.p5.1.m1.1.1.2.3.cmml">e</mi></mrow><mo id="S2.SS2.p5.1.m1.1.1.1" xref="S2.SS2.p5.1.m1.1.1.1.cmml">âˆ’</mo><mn id="S2.SS2.p5.1.m1.1.1.3" xref="S2.SS2.p5.1.m1.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.1b"><apply id="S2.SS2.p5.1.m1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1"><minus id="S2.SS2.p5.1.m1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1"></minus><apply id="S2.SS2.p5.1.m1.1.1.2.cmml" xref="S2.SS2.p5.1.m1.1.1.2"><times id="S2.SS2.p5.1.m1.1.1.2.1.cmml" xref="S2.SS2.p5.1.m1.1.1.2.1"></times><cn type="integer" id="S2.SS2.p5.1.m1.1.1.2.2.cmml" xref="S2.SS2.p5.1.m1.1.1.2.2">1</cn><ci id="S2.SS2.p5.1.m1.1.1.2.3.cmml" xref="S2.SS2.p5.1.m1.1.1.2.3">e</ci></apply><cn type="integer" id="S2.SS2.p5.1.m1.1.1.3.cmml" xref="S2.SS2.p5.1.m1.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.1c">1\mathrm{e}{-8}</annotation></semantics></math> to <math id="S2.SS2.p5.2.m2.1" class="ltx_Math" alttext="1\mathrm{e}{-6}" display="inline"><semantics id="S2.SS2.p5.2.m2.1a"><mrow id="S2.SS2.p5.2.m2.1.1" xref="S2.SS2.p5.2.m2.1.1.cmml"><mrow id="S2.SS2.p5.2.m2.1.1.2" xref="S2.SS2.p5.2.m2.1.1.2.cmml"><mn id="S2.SS2.p5.2.m2.1.1.2.2" xref="S2.SS2.p5.2.m2.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p5.2.m2.1.1.2.1" xref="S2.SS2.p5.2.m2.1.1.2.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.SS2.p5.2.m2.1.1.2.3" xref="S2.SS2.p5.2.m2.1.1.2.3.cmml">e</mi></mrow><mo id="S2.SS2.p5.2.m2.1.1.1" xref="S2.SS2.p5.2.m2.1.1.1.cmml">âˆ’</mo><mn id="S2.SS2.p5.2.m2.1.1.3" xref="S2.SS2.p5.2.m2.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m2.1b"><apply id="S2.SS2.p5.2.m2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1"><minus id="S2.SS2.p5.2.m2.1.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1.1"></minus><apply id="S2.SS2.p5.2.m2.1.1.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2"><times id="S2.SS2.p5.2.m2.1.1.2.1.cmml" xref="S2.SS2.p5.2.m2.1.1.2.1"></times><cn type="integer" id="S2.SS2.p5.2.m2.1.1.2.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2.2">1</cn><ci id="S2.SS2.p5.2.m2.1.1.2.3.cmml" xref="S2.SS2.p5.2.m2.1.1.2.3">e</ci></apply><cn type="integer" id="S2.SS2.p5.2.m2.1.1.3.cmml" xref="S2.SS2.p5.2.m2.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m2.1c">1\mathrm{e}{-6}</annotation></semantics></math>, followed by a cosine decay with a minimum learning rate of <math id="S2.SS2.p5.3.m3.1" class="ltx_Math" alttext="1\mathrm{e}{-8}" display="inline"><semantics id="S2.SS2.p5.3.m3.1a"><mrow id="S2.SS2.p5.3.m3.1.1" xref="S2.SS2.p5.3.m3.1.1.cmml"><mrow id="S2.SS2.p5.3.m3.1.1.2" xref="S2.SS2.p5.3.m3.1.1.2.cmml"><mn id="S2.SS2.p5.3.m3.1.1.2.2" xref="S2.SS2.p5.3.m3.1.1.2.2.cmml">1</mn><mo lspace="0em" rspace="0em" id="S2.SS2.p5.3.m3.1.1.2.1" xref="S2.SS2.p5.3.m3.1.1.2.1.cmml">â€‹</mo><mi mathvariant="normal" id="S2.SS2.p5.3.m3.1.1.2.3" xref="S2.SS2.p5.3.m3.1.1.2.3.cmml">e</mi></mrow><mo id="S2.SS2.p5.3.m3.1.1.1" xref="S2.SS2.p5.3.m3.1.1.1.cmml">âˆ’</mo><mn id="S2.SS2.p5.3.m3.1.1.3" xref="S2.SS2.p5.3.m3.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.3.m3.1b"><apply id="S2.SS2.p5.3.m3.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1"><minus id="S2.SS2.p5.3.m3.1.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1.1"></minus><apply id="S2.SS2.p5.3.m3.1.1.2.cmml" xref="S2.SS2.p5.3.m3.1.1.2"><times id="S2.SS2.p5.3.m3.1.1.2.1.cmml" xref="S2.SS2.p5.3.m3.1.1.2.1"></times><cn type="integer" id="S2.SS2.p5.3.m3.1.1.2.2.cmml" xref="S2.SS2.p5.3.m3.1.1.2.2">1</cn><ci id="S2.SS2.p5.3.m3.1.1.2.3.cmml" xref="S2.SS2.p5.3.m3.1.1.2.3">e</ci></apply><cn type="integer" id="S2.SS2.p5.3.m3.1.1.3.cmml" xref="S2.SS2.p5.3.m3.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.3.m3.1c">1\mathrm{e}{-8}</annotation></semantics></math>. All models are trained utilizing single NVIDIA RTX A6000 (48G) GPUs and are completed within four hours for direct training, and three days for two-stage training respectively.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Symbolic Reasoning Graph Voting</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.2" class="ltx_p">Given abduction proposals and visual clues, we first use natural language inference methods to check for all the dynamic conditions that are deductible and can be fulfilled by any combination of the visual clues.
Then, we find all the valid symbolic reasoning paths that lead to one possible answer. Finally, we do a majority voting on the reasoning paths and select the final answer.
For example in Figure <a href="#S1.F2" title="Figure 2 â€£ 1 Introduction â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, one valid symbolic path for the answer â€œyesâ€ is <math id="S2.SS3.p1.1.m1.1" class="ltx_Math" alttext="e_{1}" display="inline"><semantics id="S2.SS3.p1.1.m1.1a"><msub id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">e</mi><mn id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">ğ‘’</ci><cn type="integer" id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">e_{1}</annotation></semantics></math>, <math id="S2.SS3.p1.2.m2.1" class="ltx_Math" alttext="e_{2}" display="inline"><semantics id="S2.SS3.p1.2.m2.1a"><msub id="S2.SS3.p1.2.m2.1.1" xref="S2.SS3.p1.2.m2.1.1.cmml"><mi id="S2.SS3.p1.2.m2.1.1.2" xref="S2.SS3.p1.2.m2.1.1.2.cmml">e</mi><mn id="S2.SS3.p1.2.m2.1.1.3" xref="S2.SS3.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.2.m2.1b"><apply id="S2.SS3.p1.2.m2.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.2.m2.1.1.1.cmml" xref="S2.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS3.p1.2.m2.1.1.2.cmml" xref="S2.SS3.p1.2.m2.1.1.2">ğ‘’</ci><cn type="integer" id="S2.SS3.p1.2.m2.1.1.3.cmml" xref="S2.SS3.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.2.m2.1c">e_{2}</annotation></semantics></math>.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Collection</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">We mainly use Amazon Turk to collect 1.4K high-quality data for learning and evaluation purposes. Specifically, we focus on questions that require reasoning besides simple recognition or perception following <cite class="ltx_cite ltx_citemacro_cite">Selvaraju etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>.
Furthermore, we augment a larger set of weak supervision dataset from the whole <cite class="ltx_cite ltx_citemacro_cite">Selvaraju etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite> using the FlanT5 model <cite class="ltx_cite ltx_citemacro_cite">Chung etÂ al. (<a href="#bib.bib2" title="" class="ltx_ref">2022</a>)</cite>.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">As shown in Table <a href="#S4.T2" title="Table 2 â€£ 4.1 VQA Results â€£ 4 Experiments â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, we test our data quality using the GPT models, by checking whether the GPT model can get the correct answer, given the question, gold visual clues and gold inferences only, without the image. Consider that the val set size is only 225, we believe that the GPT performance is high enough to prove that our annotated data is high quality for the VQA task.</p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">All the experiments are evaluated for the end-to-end VQA accuracy performance. We provide ablation studies on the intermediate stages of symbolic reasonings.</p>
</div>
<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>VQA Results</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p">As shown in Table <a href="#S4.T1" title="Table 1 â€£ 4.1 VQA Results â€£ 4 Experiments â€£ Interpretable by Design Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, we can see that by adding an intermediate stage of explanations, the model (zero-shot BLIP-2 Caption + GPT-3) suffers from a huge gap from the direct BLIP-2 VQA prediction. However, by using our method, we can achieve much better performance while having a close-to-sota end task result.</p>
</div>
<figure id="S4.T1" class="ltx_table">
<table id="S4.T1.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T1.1.1" class="ltx_tr">
<td id="S4.T1.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">System</td>
<td id="S4.T1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">Dev Acc.</td>
</tr>
<tr id="S4.T1.1.2" class="ltx_tr">
<td id="S4.T1.1.2.1" class="ltx_td ltx_align_left ltx_border_t">BLIP-2</td>
<td id="S4.T1.1.2.2" class="ltx_td ltx_align_center ltx_border_t">81.1</td>
</tr>
<tr id="S4.T1.1.3" class="ltx_tr">
<td id="S4.T1.1.3.1" class="ltx_td ltx_align_left ltx_border_t">zero-shot Caption + GPT-3</td>
<td id="S4.T1.1.3.2" class="ltx_td ltx_align_center ltx_border_t">62.7</td>
</tr>
<tr id="S4.T1.1.4" class="ltx_tr">
<td id="S4.T1.1.4.1" class="ltx_td ltx_align_left">coco-finetuned Caption + GPT-3</td>
<td id="S4.T1.1.4.2" class="ltx_td ltx_align_center">68.0</td>
</tr>
<tr id="S4.T1.1.5" class="ltx_tr">
<td id="S4.T1.1.5.1" class="ltx_td ltx_align_left ltx_border_t">Ours (GPT-3)</td>
<td id="S4.T1.1.5.2" class="ltx_td ltx_align_center ltx_border_t">78.0</td>
</tr>
<tr id="S4.T1.1.6" class="ltx_tr">
<td id="S4.T1.1.6.1" class="ltx_td ltx_align_left ltx_border_bb">Ours</td>
<td id="S4.T1.1.6.2" class="ltx_td ltx_align_center ltx_border_bb">71.2</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>InterVQA dev set accuracy comparisons across end-to-end baseline models and our method including variations. Zero-shot BLIP-2 model <cite class="ltx_cite ltx_citemacro_cite">Li etÂ al. (<a href="#bib.bib10" title="" class="ltx_ref">2023</a>)</cite> is the pretrained ViT-L FlanT5XL version
from the original BLIP-2 paper.
All captions are generated using BLIP-2 model, with pretrained ViT-L FlanT5XL version and visual-question-answering fine-tuned ViT-L FlanT5XL version.
Ours (GPT-3) is a variation of our proposed pipeline, where we pass the intermediate stages to a black-box GPT-3 model.</figcaption>
</figure>
<figure id="S4.T2" class="ltx_table">
<table id="S4.T2.1" class="ltx_tabular ltx_centering ltx_align_middle">
<tr id="S4.T2.1.1" class="ltx_tr">
<td id="S4.T2.1.1.1" class="ltx_td ltx_align_left ltx_border_tt">Method</td>
<td id="S4.T2.1.1.2" class="ltx_td ltx_align_center ltx_border_tt">few-shot</td>
<td id="S4.T2.1.1.3" class="ltx_td ltx_align_center ltx_border_tt">Dev Acc.</td>
</tr>
<tr id="S4.T2.1.2" class="ltx_tr">
<td id="S4.T2.1.2.1" class="ltx_td ltx_align_left ltx_border_t">question only</td>
<td id="S4.T2.1.2.2" class="ltx_td ltx_align_center ltx_border_t">0-shot</td>
<td id="S4.T2.1.2.3" class="ltx_td ltx_align_center ltx_border_t">14.16</td>
</tr>
<tr id="S4.T2.1.3" class="ltx_tr">
<td id="S4.T2.1.3.1" class="ltx_td ltx_align_left">question only</td>
<td id="S4.T2.1.3.2" class="ltx_td ltx_align_center">4-shot</td>
<td id="S4.T2.1.3.3" class="ltx_td ltx_align_center">48.44</td>
</tr>
<tr id="S4.T2.1.4" class="ltx_tr">
<td id="S4.T2.1.4.1" class="ltx_td ltx_align_left">visual clues</td>
<td id="S4.T2.1.4.2" class="ltx_td ltx_align_center">0-shot</td>
<td id="S4.T2.1.4.3" class="ltx_td ltx_align_center">74.68</td>
</tr>
<tr id="S4.T2.1.5" class="ltx_tr">
<td id="S4.T2.1.5.1" class="ltx_td ltx_align_left">visual clues</td>
<td id="S4.T2.1.5.2" class="ltx_td ltx_align_center">4-shot</td>
<td id="S4.T2.1.5.3" class="ltx_td ltx_align_center">82.67</td>
</tr>
<tr id="S4.T2.1.6" class="ltx_tr">
<td id="S4.T2.1.6.1" class="ltx_td ltx_align_left">visual clues + inferences</td>
<td id="S4.T2.1.6.2" class="ltx_td ltx_align_center">0-shot</td>
<td id="S4.T2.1.6.3" class="ltx_td ltx_align_center">80.69</td>
</tr>
<tr id="S4.T2.1.7" class="ltx_tr">
<td id="S4.T2.1.7.1" class="ltx_td ltx_align_left ltx_border_bb">visual clues + inferences</td>
<td id="S4.T2.1.7.2" class="ltx_td ltx_align_center ltx_border_bb">4-shot</td>
<td id="S4.T2.1.7.3" class="ltx_td ltx_align_center ltx_border_bb">93.78</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>We conduct ablation studies on the collected InterVQA gold data using GPT-3 for final answer prediction.</figcaption>
</figure>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Related Works</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Reasoning with LLM</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p">With the rapid development of LLMs nowadays, especially after GPT-3.5 and GPT-4 came out, more and more papers start to study the problem of natural language reasoning with AI models <cite class="ltx_cite ltx_citemacro_cite">Wei etÂ al. (<a href="#bib.bib17" title="" class="ltx_ref">2022</a>); Creswell etÂ al. (<a href="#bib.bib3" title="" class="ltx_ref">2023</a>); Yao etÂ al. (<a href="#bib.bib19" title="" class="ltx_ref">2023</a>); Hong etÂ al. (<a href="#bib.bib7" title="" class="ltx_ref">2023</a>)</cite>. Some papers deploy a retrieve-then-predict format, which are indeed doing the same reasoning process with additional information. <cite class="ltx_cite ltx_citemacro_cite">Khattab etÂ al. (<a href="#bib.bib8" title="" class="ltx_ref">2022</a>)</cite>.
There methods have shown large improvement and huge potentials, but cannot be simply applied to the multi-modal area.</p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>VQA Rationales</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p">There have been some attempts in studying VQA rationales <cite class="ltx_cite ltx_citemacro_cite">Marino etÂ al. (<a href="#bib.bib12" title="" class="ltx_ref">2019</a>); Schwenk etÂ al. (<a href="#bib.bib14" title="" class="ltx_ref">2022</a>)</cite>.
Also, some works focus on answer consistency using sub-questions <cite class="ltx_cite ltx_citemacro_cite">Selvaraju etÂ al. (<a href="#bib.bib15" title="" class="ltx_ref">2020</a>)</cite>. However, these methods do not provide an interpretable-by-design structure and therefore the intermediate reasonings they make cannot be proved to be the only reason for the final answers.</p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p">In conclusion, we have presented <span id="S6.p1.1.1" class="ltx_text ltx_font_typewriter">InterVQA</span>, an innovative Interpretable by Design VQA system that tackles the challenge of model interpretability in the multi-modal setting of vision and language. By breaking down the reasoning process into abduction proposals, visual clues, and dynamic inferences, our approach enables a deeper understanding of how the model arrives at a final answer while maintaining competitive performance compared with state-of-the-art black-box VQA systems. To facilitate the development and evaluation of our approach, we have collected a 1.4k dataset using Amazon Turk, focusing on questions that require deeper reasoning abilities. Our results demonstrate the effectiveness of our approach in providing interpretable, step-by-step rationales that can be trusted by humans.</p>
</div>
<div id="S6.p2" class="ltx_para">
<p id="S6.p2.1" class="ltx_p">This work represents a significant advancement in the field of visual question answering, as it addresses the pressing need for more transparent, trustworthy, and understandable AI systems. Our contributions pave the way for future developments in implementing more reliable and interpretability-focused multi-modal AI models. By making AI more accessible and comprehensible to users, we ultimately enhance the potential for beneficial applications across a variety of real-world domains, such as healthcare, finance, and more.</p>
</div>
</section>
<section id="Sx1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Limitations</h2>

</section>
<section id="Sx2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">Ethics Statement</h2>

</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown etÂ al. (2020)</span>
<span class="ltx_bibblock">
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, JaredÂ D Kaplan, Prafulla
Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell,
etÂ al. 2020.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><em id="bib.bib1.1.1" class="ltx_emph ltx_font_italic">Advances in neural information processing systems</em>,
33:1877â€“1901.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chung etÂ al. (2022)</span>
<span class="ltx_bibblock">
HyungÂ Won Chung, LeÂ Hou, Shayne Longpre, Barret Zoph, YiÂ Tay, William Fedus,
Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, etÂ al. 2022.

</span>
<span class="ltx_bibblock">Scaling instruction-finetuned language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib2.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2210.11416</em>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Creswell etÂ al. (2023)</span>
<span class="ltx_bibblock">
Antonia Creswell, Murray Shanahan, and Irina Higgins. 2023.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://openreview.net/forum?id=3Pf3Wg6o-A4" title="" class="ltx_ref ltx_href">Selection-inference: Exploiting large language models for interpretable
logical reasoning</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib3.1.1" class="ltx_emph ltx_font_italic">The Eleventh International Conference on Learning
Representations</em>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Das etÂ al. (2017)</span>
<span class="ltx_bibblock">
Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. 2017.

</span>
<span class="ltx_bibblock">Human attention in visual question answering: Do humans and deep
networks look at the same regions?

</span>
<span class="ltx_bibblock"><em id="bib.bib4.1.1" class="ltx_emph ltx_font_italic">Computer Vision and Image Understanding</em>, 163:90â€“100.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dua etÂ al. (2021)</span>
<span class="ltx_bibblock">
Radhika Dua, SaiÂ Srinivas Kancheti, and VineethÂ N Balasubramanian. 2021.

</span>
<span class="ltx_bibblock">Beyond vqa: Generating multi-word answers and rationales to visual
questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib5.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 1623â€“1632.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal etÂ al. (2017)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
2017.

</span>
<span class="ltx_bibblock">Making the V in VQA matter: Elevating the role of image
understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib6.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern Recognition
(CVPR)</em>.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hong etÂ al. (2023)</span>
<span class="ltx_bibblock">
Ruixin Hong, Hongming Zhang, Hong Zhao, Dong Yu, and Changshui Zhang. 2023.

</span>
<span class="ltx_bibblock">Faithful question answering with monte-carlo planning.

</span>
<span class="ltx_bibblock"><em id="bib.bib7.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2305.02556</em>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khattab etÂ al. (2022)</span>
<span class="ltx_bibblock">
Omar Khattab, Keshav Santhanam, XiangÂ Lisa Li, David Hall, Percy Liang,
Christopher Potts, and Matei Zaharia. 2022.

</span>
<span class="ltx_bibblock">Demonstrate-search-predict: Composing retrieval and language models
for knowledge-intensive nlp.

</span>
<span class="ltx_bibblock"><em id="bib.bib8.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2212.14024</em>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klein etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ayal Klein, Eran Hirsch, Ron Eliav, Valentina Pyatkin, Avi Caciularu, and Ido
Dagan. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://aclanthology.org/2022.emnlp-main.528" title="" class="ltx_ref ltx_href">QASem
parsing: Text-to-text modeling of QA-based semantics</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib9.1.1" class="ltx_emph ltx_font_italic">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</em>, pages 7742â€“7756, Abu Dhabi, United Arab
Emirates. Association for Computational Linguistics.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image
encoders and large language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib10.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2301.12597</em>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter (2017)</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter. 2017.

</span>
<span class="ltx_bibblock">Decoupled weight decay regularization.

</span>
<span class="ltx_bibblock"><em id="bib.bib11.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:1711.05101</em>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marino etÂ al. (2019)</span>
<span class="ltx_bibblock">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. 2019.

</span>
<span class="ltx_bibblock">Ok-vqa: A visual question answering benchmark requiring external
knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib12.1.1" class="ltx_emph ltx_font_italic">Conference on Computer Vision and Pattern Recognition
(CVPR)</em>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
etÂ al. 2021.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock">In <em id="bib.bib13.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
8748â€“8763. PMLR.

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwenk etÂ al. (2022)</span>
<span class="ltx_bibblock">
Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and
Roozbeh Mottaghi. 2022.

</span>
<span class="ltx_bibblock">A-okvqa: A benchmark for visual question answering using world
knowledge.

</span>
<span class="ltx_bibblock">In <em id="bib.bib14.1.1" class="ltx_emph ltx_font_italic">Computer Visionâ€“ECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23â€“27, 2022, Proceedings, Part VIII</em>, pages 146â€“162.
Springer.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju etÂ al. (2020)</span>
<span class="ltx_bibblock">
RamprasaathÂ R Selvaraju, Purva Tendulkar, Devi Parikh, Eric Horvitz,
MarcoÂ Tulio Ribeiro, Besmira Nushi, and Ece Kamar. 2020.

</span>
<span class="ltx_bibblock">Squinting at vqa models: Introspecting vqa models with sub-questions.

</span>
<span class="ltx_bibblock">In <em id="bib.bib15.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pages 10003â€“10011.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shih etÂ al. (2016)</span>
<span class="ltx_bibblock">
KevinÂ J Shih, Saurabh Singh, and Derek Hoiem. 2016.

</span>
<span class="ltx_bibblock">Where to look: Focus regions for visual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib16.1.1" class="ltx_emph ltx_font_italic">Proceedings of the IEEE conference on computer vision and
pattern recognition</em>, pages 4613â€“4621.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei etÂ al. (2022)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia,
EdÂ Chi, QuocÂ V Le, and Denny Zhou. 2022.

</span>
<span class="ltx_bibblock"><a target="_blank" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/9d5609613524ecf4f15af0f7b31abca4-Paper-Conference.pdf" title="" class="ltx_ref ltx_href">Chain-of-thought prompting elicits reasoning in large language models</a>.

</span>
<span class="ltx_bibblock">In <em id="bib.bib17.1.1" class="ltx_emph ltx_font_italic">Advances in Neural Information Processing Systems</em>,
volumeÂ 35, pages 24824â€“24837. Curran Associates, Inc.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiong etÂ al. (2016)</span>
<span class="ltx_bibblock">
Caiming Xiong, Stephen Merity, and Richard Socher. 2016.

</span>
<span class="ltx_bibblock">Dynamic memory networks for visual and textual question answering.

</span>
<span class="ltx_bibblock">In <em id="bib.bib18.1.1" class="ltx_emph ltx_font_italic">International conference on machine learning</em>, pages
2397â€“2406. PMLR.

</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yao etÂ al. (2023)</span>
<span class="ltx_bibblock">
Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,
and Yuan Cao. 2023.

</span>
<span class="ltx_bibblock">ReAct: Synergizing reasoning and acting in language models.

</span>
<span class="ltx_bibblock">In <em id="bib.bib19.1.1" class="ltx_emph ltx_font_italic">International Conference on Learning Representations
(ICLR)</em>.

</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Zhuosheng Zhang, Aston Zhang, MuÂ Li, Hai Zhao, George Karypis, and Alex Smola.
2023.

</span>
<span class="ltx_bibblock">Multimodal chain-of-thought reasoning in language models.

</span>
<span class="ltx_bibblock"><em id="bib.bib20.1.1" class="ltx_emph ltx_font_italic">arXiv preprint arXiv:2302.00923</em>.

</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Example Appendix</h2>

<div id="A1.p1" class="ltx_para">
<p id="A1.p1.1" class="ltx_p">This is a section in the appendix.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2305.14881" class="ar5iv-nav-button ar5iv-nav-button-prev">â—„</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2305.14882" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2305.14882">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2305.14882" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2305.14883" class="ar5iv-nav-button ar5iv-nav-button-next">â–º</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 05:55:50 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "Ã—";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
