<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants</title>
<!--Generated on Sat Sep 28 18:19:34 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="large language models,  simulating research participants,  LLM agents,  qualitative research,  LLMs in qualitative research" lang="en" name="keywords"/>
<base href="/html/2409.19430v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S1" title="In ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S2" title="In ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S2.SS1" title="In 2. Related Work ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>LLMs as proxies for human behavior</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S2.SS2" title="In 2. Related Work ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>LLM use for research-related tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S2.SS3" title="In 2. Related Work ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Risks of harm posed by LLMs</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S3" title="In ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S4" title="In ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Findings</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S4.SS1" title="In 4. Findings ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Researcher perceptions of and interactions with LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S4.SS2" title="In 4. Findings ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Fundamental Limitations of LLMs as Simulated Subjects in Qualitative Research</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S4.SS3" title="In 4. Findings ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Imagining Use Cases for LLMs in Qualitative Research: Seeing Possibilities and Even More Caveats</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S5" title="In ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S5.SS1" title="In 5. Discussion ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>LLMs are incongruent with qualitative epistemologies</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S5.SS2" title="In 5. Discussion ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>LLM use undermines consent and autonomy of data subjects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S5.SS3" title="In 5. Discussion ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>What does it mean to simulate the user in HCI?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S6" title="In ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_italic" id="id1.id1">‘Simulacrum of Stories’</span>: Examining Large Language Models as Qualitative Research Participants</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shivani Kapania
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id2.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id3.2.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_state" id="id4.3.id3">PA</span><span class="ltx_text ltx_affiliation_country" id="id5.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:kapania@cmu.edu">kapania@cmu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">William Agnew
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id6.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id7.2.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_state" id="id8.3.id3">PA</span><span class="ltx_text ltx_affiliation_country" id="id9.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:wagnew@andrew.cmu.edu">wagnew@andrew.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Motahhare Eslami
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_state" id="id12.3.id3">PA</span><span class="ltx_text ltx_affiliation_country" id="id13.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:meslami@andrew.cmu.edu">meslami@andrew.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Hoda Heidari
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id14.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id15.2.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_state" id="id16.3.id3">PA</span><span class="ltx_text ltx_affiliation_country" id="id17.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:hheidari@andrew.cmu.edu">hheidari@andrew.cmu.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sarah Fox
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id18.1.id1">Carnegie Mellon University</span><span class="ltx_text ltx_affiliation_city" id="id19.2.id2">Pittsburgh</span><span class="ltx_text ltx_affiliation_state" id="id20.3.id3">PA</span><span class="ltx_text ltx_affiliation_country" id="id21.4.id4">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:sarahf@andrew.cmu.edu">sarahf@andrew.cmu.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2018)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id22.id1">The recent excitement around generative models has sparked a wave of proposals suggesting the replacement of human participation and labor in research and development–<span class="ltx_text ltx_font_italic" id="id22.id1.1">e.g.,</span> through surveys, experiments, and interviews—with synthetic research data generated by large language models (LLMs). We conducted interviews with 19 qualitative researchers to understand their perspectives on this paradigm shift. Initially skeptical, researchers were surprised to see similar narratives emerge in the LLM-generated data when using the interview probe. However, over several conversational turns, they went on to identify fundamental limitations, such as how LLMs foreclose participants’ consent and agency, produce responses lacking in palpability and contextual depth, and risk delegitimizing qualitative research methods. We argue that the use of LLMs as proxies for participants enacts the <span class="ltx_text ltx_font_italic" id="id22.id1.2">surrogate effect</span>, raising ethical and epistemological concerns that extend beyond the technical limitations of current models to the core of whether LLMs fit within qualitative ways of knowing.</p>
</div>
<div class="ltx_keywords">large language models, simulating research participants, LLM agents, qualitative research, LLMs in qualitative research
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_conference" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">conference: </span>Make sure to enter the correct
conference title from your rights confirmation emai; June 03–05,
2018; Woodstock, NY</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Empirical studies in HCI</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing User studies</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id8"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Natural language generation</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In 2023, a startup called Synthetic Users made waves with its bold claim to conduct user research “without the users,” offering an ostensibly faster, cheaper alternative by using Large Language Models (LLMs) to simulate human participants <cite class="ltx_cite ltx_citemacro_citep">(Inc, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib55" title="">2024</a>)</cite>. With the recent hype surrounding generative AI models, there has been a surge of similar proposals to replace human participation and labor in technology development with synthetic data. This includes simulating human participants in research and design <cite class="ltx_cite ltx_citemacro_citep">(Horton, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib53" title="">2023</a>)</cite>, and in data annotation used for training or evaluating machine learning models <cite class="ltx_cite ltx_citemacro_citep">(Dubois et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib36" title="">2023</a>; Gilardi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib44" title="">2023a</a>)</cite>. Multiple recent studies and commercial products (<span class="ltx_text ltx_font_italic" id="S1.p1.1.1">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citep">(Users, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib107" title="">2023</a>; K, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib59" title="">2023</a>)</cite>) have explored the use of LLMs as proxies for human behavior in opinion surveys <cite class="ltx_cite ltx_citemacro_citep">(Kim and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib64" title="">2023</a>)</cite>, crowdsourcing <cite class="ltx_cite ltx_citemacro_citep">(Kuzman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib66" title="">2023</a>)</cite>, interviews <cite class="ltx_cite ltx_citemacro_citep">(Hämäläinen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib49" title="">2023</a>)</cite>, and social computing research <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib86" title="">2022</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib85" title="">2023</a>)</cite>.
These substitution proposals are often motivated by goals, such as increasing the speed and scale of research, reducing costs, augmenting the diversity of collected data, or protecting participants from harm <cite class="ltx_cite ltx_citemacro_citep">(Agnew et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib4" title="">2024</a>)</cite>. A key assumption underpinning this body of work is that generative models, trained on vast datasets, encode a wide range of human behavior in their training data and thus should be able to closely mimic human-generated data <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib85" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">This shift raises crucial questions about what it means to use LLMs in qualitative research–a methodology focused on contextual and human-centered inquiry. Qualitative research seeks to understand people’s behavior by deeply engaging with people’s accounts of their lived experiences and the meanings they attach to them. Within an interpretivist paradigm <cite class="ltx_cite ltx_citemacro_citep">(Crabtree, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib32" title="">2024</a>; Schwandt, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib95" title="">1994</a>)</cite>, qualitative research is more than a method for collecting data; it is an active meaning-making process that helps understand how individuals construct their realities within specific social and cultural contexts <cite class="ltx_cite ltx_citemacro_citep">(Avis, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib8" title="">2005</a>)</cite>. Qualitative ways of knowing help develop accounts of these meanings through intersubjective depth, trust, and rapport between the researcher and the participants.
An important strength of qualitative research lies in its ability to uncover subjective meanings that are often excluded or silenced from public discourse and how those meanings are shaped by the asymmetrical power relations that govern many social interactions <cite class="ltx_cite ltx_citemacro_citep">(Silverman, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib100" title="">2004</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">In this paper, we draw on in-depth, semi-structured interviews with 19 qualitative researchers to understand their perspectives on the use of LLMs for simulating research participants. Our interviewees were primarily HCI and CSCW researchers focused on topics such as accessibility, gig work, social computing, and more. To scaffold these discussions, we developed a simple interview probe to understand how researchers might create LLM agents to simulate research participants and explored any concerns or limitations they foresee. We invited researchers to discuss one of their recent projects where they conducted semi-structured interviews, and recreate interview(s) from their study using our probe. We chose to focus on participants’ previous or current projects rather than introducing a new topic during the interview for two reasons: (1) this approach allowed us to explore a broad range of research topics within HCI, and (2) by discussing a familiar research area, participants could provide nuanced and more detailed reflections on LLM use. For concreteness and comparability, we focus on one specific qualitative research method: the semi-structured interview.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our findings highlight critical tensions with integrating LLMs into the HCI research praxis. Researchers were initially surprised to find that LLM responses contained narratives similar to those of their human participants. Over several conversational turns, however, they began to notice distinct differences between the LLM’s responses and their participants. Their concerns extend beyond the technical limitations of current models that can be seemingly fixed by prompting or ‘better’ data, to the heart of whether LLMs fit as proxies for human participation within interpretivist qualitative epistemologies. Researchers surfaced six fundamental limitations: (1) LLM responses lack palpability, (2) the model’s epistemic position remains ambiguous, (3) the practice heightens researcher positionality, (4) it forecloses participants’ consent and agency, (5) it facilitates erasure of communities’ perspectives, and (6) it risks delegitimizing qualitative ways of knowing.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To critically examine the use of LLMs as research participants, we draw on Atanasoski and Vora’s <cite class="ltx_cite ltx_citemacro_citep">(Atanasoski and Vora, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib7" title="">2019</a>)</cite> concept of the <span class="ltx_text ltx_font_italic" id="S1.p5.1.1">surrogate effect</span> to describe how this form of substitution further entrenches exploitation and erasure. When LLMs stand in for human participants, they displace the voices of communities with algorithmic simulations that often distort or oversimplify the groups they claim to simulate. The use of LLMs as research participants also raises ethical concerns about the underlying data used to train these models, including the autonomy of data subjects and the exploitation of data workers, which threaten the integrity of qualitative research. LLMs’ role as research surrogates mirrors broader dynamics of epistemic dispossession in digital economies, where the knowledge and labor of marginalized groups are reconstituted into proprietary systems that offer little benefit to the original data creators. LLMs, despite their ability to generate coherent text, lack the embodied, situated understanding necessary for producing knowledge that is grounded in lived experience, histories, emotions, and social and cultural contexts. In making this argument, this work offers three key contributions to the HCI community: (1) offers an empirical account of how qualitative researchers simulate participants using LLMs; (2) examines the fundamental limitations of using LLMs in qualitative research; and (3) analyzes these limitations through the lens of research ethics and science &amp; technology studies scholarship.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Below, we situate our work in a body of related research, starting with the recent turn towards using LLMs to simulate human behavior. We then draw attention to the variety of use cases proposed for LLMs in research settings, such as ideation, data analysis, and simulating research participants. Finally, we engage with the scholarship that examines the broader limitations and concerns surrounding the use of LLMs.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>LLMs as proxies for human behavior</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Recent advances in NLP have inspired the notion that LLMs are capable beyond text generation, positioning them as <em class="ltx_emph ltx_font_italic" id="S2.SS1.p1.1.1">agents</em> capable of conversational engagement, decision-making, task completion, and coordination. This interest has led to the development of AI agents based on LLMs <cite class="ltx_cite ltx_citemacro_citep">(Xi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib114" title="">2023</a>)</cite>, across various applications, including single-agent setups <cite class="ltx_cite ltx_citemacro_citep">(Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib63" title="">2024</a>; Deng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib35" title="">2024</a>; M. Bran et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib77" title="">2024</a>; Boiko et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib19" title="">2023</a>)</cite>, multi-agent systems <cite class="ltx_cite ltx_citemacro_citep">(Guo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib46" title="">2024</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib121" title="">2023</a>)</cite>, and human-agent interactions <cite class="ltx_cite ltx_citemacro_citep">(Hasan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib52" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">LLMs have been used to simulate human behavior, preferences, and judgment across various fields, including psychology <cite class="ltx_cite ltx_citemacro_citep">(Demszky et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib34" title="">2023</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib117" title="">2024</a>; Hu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib54" title="">2024</a>)</cite>, social sciences <cite class="ltx_cite ltx_citemacro_citep">(Halterman and Keith, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib48" title="">2024</a>; Liu and Shi, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib74" title="">2024</a>; Ziems et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib122" title="">2024</a>; Jiang and Ferrara, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib58" title="">2023</a>)</cite>, education <cite class="ltx_cite ltx_citemacro_citep">(Latif et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib68" title="">2023</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib120" title="">2024b</a>)</cite>, and professional training <cite class="ltx_cite ltx_citemacro_citep">(Ravi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib89" title="">2023</a>; Louie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib76" title="">2024</a>)</cite> as exploratory use cases. In psychology and social sciences, scholars are using LLM-based agents to model individual behaviors to understand complex social dynamics or predict outcomes in different scenarios <cite class="ltx_cite ltx_citemacro_citep">(Ke et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib61" title="">2024</a>)</cite>.
In professional training, LLM-based applications are used to simulate real-world situations for legal professionals <cite class="ltx_cite ltx_citemacro_citep">(Savelka et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib94" title="">2023</a>)</cite>, therapists <cite class="ltx_cite ltx_citemacro_citep">(Louie et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib76" title="">2024</a>)</cite>, and medical practitioners <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib111" title="">2024b</a>)</cite>. LLMs are also being used in mental health and counseling to provide supervision and feedback (<span class="ltx_text ltx_font_italic" id="S2.SS1.p2.1.1">e.g.,</span> <cite class="ltx_cite ltx_citemacro_citep">(Chaszczewicz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib29" title="">2024</a>)</cite>).</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">In recent work, researchers have also started exploring the use of LLMs to generate synthetic data through personality profiles, or <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.1">personas</span> <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib28" title="">2024</a>; Bao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib11" title="">2023</a>; Frisch and Giulianelli, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib40" title="">2024</a>; Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib27" title="">2023</a>)</cite>.
These studies suggest that personas could help steer LLM outputs toward more tailored and contextually relevant results. Park <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib85" title="">2023</a>)</cite> created generative agents to mimic human behavior in tasks such as planning, reaction, and reflection. As the field evolves, researchers have focused on <span class="ltx_text ltx_font_italic" id="S2.SS1.p3.1.3">believability</span> as the primary metric for evaluating LLM behavior. They assess whether these agents interact naturally and realistically, and if their behavior remains consistent with their designed character traits included in the profile information <cite class="ltx_cite ltx_citemacro_citep">(Xiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib115" title="">2023a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">Within research, many scholars are examining the use of LLMs to simulate human behavior for purposes like public-opinion surveys <cite class="ltx_cite ltx_citemacro_citep">(Tjuatja et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib106" title="">2023</a>)</cite>, text-annotation tasks <cite class="ltx_cite ltx_citemacro_citep">(Gilardi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib45" title="">2023b</a>; Thapa et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib105" title="">2023</a>)</cite>, and experiments <cite class="ltx_cite ltx_citemacro_citep">(Argyle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib6" title="">2023</a>; Aher et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib5" title="">2023</a>)</cite>. Argyle <span class="ltx_text ltx_font_italic" id="S2.SS1.p4.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Argyle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib6" title="">2023</a>)</cite> argue that LLMs can replicate human results in tasks involving subjective labeling, especially when researchers condition model responses with sociodemographic backstories achieving what they call ‘algorithmic fidelity’ in ‘silicon samples’ <cite class="ltx_cite ltx_citemacro_citep">(Argyle et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib6" title="">2023</a>)</cite>.
Most prior research has concentrated on evaluating LLMs’ capabilities through experimental methods, assessing their performance as proxies for human behavior in controlled settings and according to narrowly-scoped metrics. In contrast, our work shifts the focus toward understanding how qualitative researchers would engage with LLM ‘agents’ and how they perceive the LLM-generated data within the context of a qualitative inquiry.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>LLM use for research-related tasks</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">HCI researchers are increasingly relying on LLMs to support a variety of creative and analytical tasks <cite class="ltx_cite ltx_citemacro_citep">(Kapania et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib60" title="">2024</a>)</cite>, including brainstorming <cite class="ltx_cite ltx_citemacro_citep">(Shaer et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib97" title="">2024</a>)</cite>, generating research questions <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib75" title="">2024</a>)</cite>, design ideation <cite class="ltx_cite ltx_citemacro_citep">(Ma et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib78" title="">2023</a>; Baek et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib9" title="">2024</a>)</cite> and writing support <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib119" title="">2023</a>)</cite>. Multiple recent studies have explored the use of AI to support data analysis within qualitative research, including both deductive and inductive approaches <cite class="ltx_cite ltx_citemacro_citep">(Feuston and Brubaker, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib37" title="">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib118" title="">2024a</a>; Xiao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib116" title="">2023b</a>; Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib42" title="">2024</a>)</cite>. This line of work aims to help researchers develop and refine codebooks and perform thematic analysis by processing and categorizing textual data <cite class="ltx_cite ltx_citemacro_citep">(Lam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib67" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Researchers have also explored LLMs for their ability to produce synthetic research data. Hämäläinen <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Hämäläinen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib50" title="">2023</a>)</cite> used the GPT-3 model to generate responses to open-ended questions on the topic of video games as art. They argue that the model could generate <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.2">plausible</span> accounts of HCI experiences and that LLM-generated data can be useful in designing and assessing experiments because it is a cheap and rapid process. However, several other studies have reported mixed results on the effectiveness of LLMs for data imputation and synthetic data generation <cite class="ltx_cite ltx_citemacro_citep">(Kim and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib64" title="">2023</a>; Aher et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib5" title="">2023</a>)</cite>. Kim and Lee <cite class="ltx_cite ltx_citemacro_citep">(Kim and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib64" title="">2023</a>)</cite>, for example, fine-tuned an LLM to fill in missing data in public opinion polls and observed varying levels of accuracy depending on socioeconomic status, political affiliation, and demographic identity of the persona. Model performance also diminished in scenarios that required prediction without prior examples <cite class="ltx_cite ltx_citemacro_citep">(Kim and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib64" title="">2023</a>)</cite>. Aher <span class="ltx_text ltx_font_italic" id="S2.SS2.p2.1.3">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Aher et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib5" title="">2023</a>)</cite> document a ‘hyper-accuracy distortion,’ revealed by one of their experiments, where more advanced LLMs produce unnaturally precise responses, raising concerns about whether these models can truly mimic human behavior.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The increasing reliance on LLMs within research has sparked critical discourse about their limitations and the broader implications of using AI as proxies for human behavior. While proponents argue that LLMs can improve research efficiency, scale, and diversity (see, <span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.1">e.g.,</span> research papers such as <cite class="ltx_cite ltx_citemacro_citep">(Byun et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib25" title="">2023</a>; Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib10" title="">2022</a>; Chiang and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib30" title="">2023</a>)</cite> and products such as <cite class="ltx_cite ltx_citemacro_citep">(Inc, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib55" title="">2024</a>; International, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib56" title="">2024</a>)</cite>), there is growing scholarship that explores the ethical implications of replacing human participants with AI-generated data <cite class="ltx_cite ltx_citemacro_citep">(Agnew et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib4" title="">2024</a>)</cite>. LLMs’ effectiveness in replacing human participants is (partly) contingent on their ability to represent the perspectives of different identities. Previous research has suggested that LLMs can simulate human behavior, largely because they are trained on vast datasets that reflect diverse human experiences <cite class="ltx_cite ltx_citemacro_citep">(Park et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib85" title="">2023</a>)</cite>. However, Wang <span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib110" title="">2024a</a>)</cite> provide a critical counterpoint by comparing LLM-generated free-text responses to those of human participants. Their findings reveal that LLMs often misrepresent specific groups (<span class="ltx_text ltx_font_italic" id="S2.SS2.p3.1.3">e.g.,</span> visually impaired people) and erase within-group heterogeneity. They argue that these issues are inherent to the current LLM training frameworks and are unlikely to be resolved by simply advancing to newer model generations <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib110" title="">2024a</a>)</cite>.
While this prior work has examined the use of LLMs for generating responses (see also <cite class="ltx_cite ltx_citemacro_citep">(Hämäläinen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib50" title="">2023</a>)</cite>), interviews involve even richer accounts of embodied experiences and complex narratives. Interviews allow for contextually informed exchanges where participants are guided to reflect, clarify, and expand on their responses. We build on this body of work to investigate how researchers would use LLMs to simulate these rich and dynamic interactions within an interview context.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Risks of harm posed by LLMs</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">The limitations of language models are well-documented, particularly regarding their tendency to reproduce harmful social biases and stereotypes <cite class="ltx_cite ltx_citemacro_citep">(Bolukbasi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib20" title="">2016</a>; Caliskan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib26" title="">2017</a>)</cite>. LMs fundamentally work by detecting the statistical patterns present in natural language data <cite class="ltx_cite ltx_citemacro_citep">(Bender et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib14" title="">2021</a>)</cite>. As a result, some communities are better represented in the training data than others. The training data itself can be biased or represent discriminatory or toxic behavior <cite class="ltx_cite ltx_citemacro_citep">(Weidinger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib112" title="">2022</a>)</cite>. An examination of C4.<span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">en</span> <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib88" title="">2020</a>)</cite>, one of the large web-scale text corpora used to train language models, revealed how many documents associated with Black and Hispanic authors, and documents mentioning sexual orientations (’lesbian’, ’gay’, ’homosexual’) are more likely to be excluded from the data <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib72" title="">2020</a>)</cite>. Extensive prior research has shown how LLMs reflect biases towards gender <cite class="ltx_cite ltx_citemacro_citep">(Bordia and Bowman, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib21" title="">2019</a>)</cite>, race <cite class="ltx_cite ltx_citemacro_citep">(Nadeem et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib81" title="">2020</a>)</cite>, religion <cite class="ltx_cite ltx_citemacro_citep">(Abid et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib3" title="">2021</a>)</cite>, nationality <cite class="ltx_cite ltx_citemacro_citep">(Venkit et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib109" title="">2023</a>)</cite>, and in social settings <cite class="ltx_cite ltx_citemacro_citep">(Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib73" title="">2021</a>)</cite>.
LMs that encode discriminatory language or social stereotypes can cause different types of allocational or representational harm <cite class="ltx_cite ltx_citemacro_citep">(Blodgett et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib17" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Scholars have highlighted concerns about the ability of LMs to align their outputs with specific demographic personas or viewpoints <cite class="ltx_cite ltx_citemacro_citep">(Gupta et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib47" title="">2023</a>)</cite>. Models are well-known to implicitly default to the well-represented perspectives (<span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.1">e.g.,</span> Western, White, masculine) <cite class="ltx_cite ltx_citemacro_citep">(Santy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib93" title="">2023</a>)</cite>, or those of the US and European countries. Within the United States, too, research has shown that these models tend to generate responses that align more closely with liberal, educated, and affluent populations while poorly representing the views and experiences of certain groups that make up a significant portion of the U.S. population (<span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.2">e.g.,</span> 65+, Mormon and widowed) <cite class="ltx_cite ltx_citemacro_citep">(Santurkar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib92" title="">2023</a>)</cite>. When LLMs are prompted to consider a particular country’s perspective changes, those responses are <span class="ltx_text ltx_font_italic" id="S2.SS3.p2.1.3">more</span> similar to the opinions of the prompted population. However, those simulated opinions often reflect over-generalizations around complex cultural values. Overall, prior work has demonstrated how current LLMs struggle to align their behaviors with assigned characters and are vulnerable to perturbations of the profile information.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">Researchers examining the ethical and social risks of large-scale language models have articulated a variety of concerns that extend beyond issues of stereotypical and discriminatory outputs <cite class="ltx_cite ltx_citemacro_citep">(Shelby et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib98" title="">2022</a>)</cite>. LMs can contribute to misinformation by predicting higher likelihoods for more prominent accounts in their training data, regardless of whether those accounts are factually accurate. Another emerging risk involves the anthropomorphization of LMs, where users attribute human-like abilities to these systems, leading to overreliance and unsafe uses. Language models could generate content that harms artists by capitalizing on their creative output. These models often produce work that is ”sufficiently distinct” from the original but still appropriate the style, structure, or essence of the original work. Finally, these models also pose significant environmental hazards, as the computational power required for training and operation leads to considerable energy consumption and carbon emissions. Refer to Weidinger <span class="ltx_text ltx_font_italic" id="S2.SS3.p3.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Weidinger et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib112" title="">2022</a>)</cite> for a more detailed landscape of risks of harm from language models. Our work builds on the critiques of large language models, particularly their tendency to produce outputs that do not account for diverse epistemic positions. We argue that this failure mode is particularly problematic if LLMs are used in qualitative research; the use of LLMs can exacerbate epistemic injustices by systematically excluding certain voices from participating in the knowledge production process <cite class="ltx_cite ltx_citemacro_citep">(Fricker, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib39" title="">2007</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Method</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Between March and June 2024, we conducted semi-structured interviews with 19 researchers experienced in qualitative research to explore the potential of using LLMs as research participants. These interviews involved discussions on our participants’ perspectives towards LLMs and included a hands-on session with an interview probe, which allowed researchers to compare LLM-generated data with human-generated data and reflect on ethical considerations and broader implications. Participants were requested to bring an anonymized interview transcript from a recent interview study they had conducted and keep it open for reference during the session. Our contextual research approach enabled us to observe how qualitative researchers interacted with LLMs, adapted their interviewing strategies, and evaluated the effectiveness and limitations of LLM-generated data, including its societal implications.</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="410" id="S3.F1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Example interaction with our interview probe</figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.p2.1.1">Interview probe.</span> We created a probe to scaffold researchers’ reflections on using LLMs in qualitative research. We designed the probe as a simple functional prototype to observe existing practices and gain insights into the social contexts where the technology might be used. This probe consisted of three components. The first component, the system prompt area, allowed researchers to set the context, provide instructions, and define participant descriptions for the LLM before posing interview questions. A system prompt can help establish a ‘role’ for the LLM to follow throughout the conversation. The most common approach to using LLMs to simulate human behavior involves assigning specific roles to the model (see section <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S2.SS2" title="2.2. LLM use for research-related tasks ‣ 2. Related Work ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_tag">2.2</span></a>), and while we recognize that personas are abstract representations, we adopted this method as it reflects current practice. We provided researchers with an initial system prompt template with placeholders for research topics and persona, formulated through multiple iterations, including pilot studies and involvement from research team members with expertise in qualitative research. The system prompt template was as follows:</p>
</div>
<div class="ltx_para" id="S3.p3">
<blockquote class="ltx_quote" id="S3.p3.1">
<p class="ltx_p" id="S3.p3.1.1">I am a researcher running a semi-structured interview on [topic]. Imagine you are [participant description]. You are participating in my study. Respond to the following questions as this interview participant. Share your lived experiences and anecdotes when appropriate. Give detailed, non-generic responses. Don’t respond with bullet points.</p>
</blockquote>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1">The second component, the interview area, enabled researchers to conduct interviews with simulated participants, while the third component, the conversation history viewer, recorded all interactions between the researcher and the LLM. This feature allowed researchers to filter conversations by each system prompt, facilitating a review and analysis of the interview data if they conducted multiple interviews. The purpose of this study is not to compare different models or benchmark their capabilities. Rather than evaluating and making claims about specific LLMs, our goal is to highlight the unique characteristics, values, and tensions with the use of LLMs, in general, in qualitative research. Drawing on prior research, we used the GPT-4-turbo API due to its superior performance as of February 2024 <cite class="ltx_cite ltx_citemacro_citep">(Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib121" title="">2023</a>)</cite>. Refer to image <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S3.F1" title="Figure 1 ‣ 3. Method ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_tag">1</span></a> for an example interaction.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Procedure.</span> Each interview began with participants describing their qualitative research orientation and a recent project where they conducted interviews. This allowed us to learn about their typical workflow. We then explored their perspectives on the use of LLMs in qualitative research. Next, we introduced the interviewing tool, which served as a probe to explore their perceptions of LLM-generated data. Each hands-on session began with the researcher creating the system prompt for the LLM with a specific participant description and research topic. After initializing the system prompt, participants could start interviewing by referencing their original interview protocol and modifying their questions as needed. During the hands-on session, researchers were asked to think aloud as they spent 30-45 minutes using the LLM-based tool to generate interview data, which we retained for analysis. Participants had the discretion to decide how many turns of conversation to engage in.
In the final part of the interview, we focused on participants’ experiences with the tool, any adjustments they made to their interviewing approach, and the limitations of interviewing LLMs. We asked them to compare the LLM-generated data with the anonymized human interview transcript from their study, evaluating the depth of responses, the flow of conversation, and the overall insightfulness of the session. Lastly, researchers reflected on the ethical considerations of using LLMs and discussed potential scenarios where LLMs could support their research. Each interview lasted up to 90 minutes.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.1.1">
<span class="ltx_p" id="S3.T1.1.1.1.1.1.1" style="width:39.8pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1.1" style="font-size:80%;">Pseudonym</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S3.T1.1.1.1.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.2.1">
<span class="ltx_p" id="S3.T1.1.1.1.2.1.1" style="width:180.7pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.2.1.1.1" style="font-size:80%;">Research topic</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.1.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.1.1">
<span class="ltx_p" id="S3.T1.1.2.1.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.2.1.1.1.1.1" style="font-size:80%;">Sophia</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.2.1.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.2.1.2.1">
<span class="ltx_p" id="S3.T1.1.2.1.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.2.1.2.1.1.1" style="font-size:80%;">Exploring worker experiences on digital labor platforms.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.3.2.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.1.1">
<span class="ltx_p" id="S3.T1.1.3.2.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.3.2.1.1.1.1" style="font-size:80%;">Henri</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.3.2.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.3.2.2.1">
<span class="ltx_p" id="S3.T1.1.3.2.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.3.2.2.1.1.1" style="font-size:80%;">Investigating the use of assistive technology by older adults.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.4.3.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.1.1">
<span class="ltx_p" id="S3.T1.1.4.3.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.4.3.1.1.1.1" style="font-size:80%;">Laila</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.4.3.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.4.3.2.1">
<span class="ltx_p" id="S3.T1.1.4.3.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.4.3.2.1.1.1" style="font-size:80%;">Studying the perspectives of teachers and social media creators.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.5.4.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.1.1">
<span class="ltx_p" id="S3.T1.1.5.4.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.5.4.1.1.1.1" style="font-size:80%;">Harper</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.5.4.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.5.4.2.1">
<span class="ltx_p" id="S3.T1.1.5.4.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.5.4.2.1.1.1" style="font-size:80%;">Examining practices of developers working on AI models.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.6.5.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.1.1">
<span class="ltx_p" id="S3.T1.1.6.5.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.6.5.1.1.1.1" style="font-size:80%;">Cameron</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.6.5.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.6.5.2.1">
<span class="ltx_p" id="S3.T1.1.6.5.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.6.5.2.1.1.1" style="font-size:80%;">Understanding perceptions of algorithmic decision-making systems.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.7.6.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.1.1">
<span class="ltx_p" id="S3.T1.1.7.6.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.7.6.1.1.1.1" style="font-size:80%;">Nolan</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.7.6.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.7.6.2.1">
<span class="ltx_p" id="S3.T1.1.7.6.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.7.6.2.1.1.1" style="font-size:80%;">Investigating transportation needs for people with mobility challenges.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.8.7.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.1.1">
<span class="ltx_p" id="S3.T1.1.8.7.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.8.7.1.1.1.1" style="font-size:80%;">Esme</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.8.7.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.8.7.2.1">
<span class="ltx_p" id="S3.T1.1.8.7.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.8.7.2.1.1.1" style="font-size:80%;">Exploring creative practices and challenges faced by content creators.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.9.8.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.1.1">
<span class="ltx_p" id="S3.T1.1.9.8.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.9.8.1.1.1.1" style="font-size:80%;">Mario</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.9.8.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.9.8.2.1">
<span class="ltx_p" id="S3.T1.1.9.8.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.9.8.2.1.1.1" style="font-size:80%;">Studying how AI professionals engage with ethics.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.10.9.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.1.1">
<span class="ltx_p" id="S3.T1.1.10.9.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.10.9.1.1.1.1" style="font-size:80%;">Yue</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.10.9.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.10.9.2.1">
<span class="ltx_p" id="S3.T1.1.10.9.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.10.9.2.1.1.1" style="font-size:80%;">Investigating the use of technology by individuals with accessibility needs.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.11.10.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.1.1">
<span class="ltx_p" id="S3.T1.1.11.10.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.11.10.1.1.1.1" style="font-size:80%;">Nico</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.11.10.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.11.10.2.1">
<span class="ltx_p" id="S3.T1.1.11.10.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.11.10.2.1.1.1" style="font-size:80%;">Exploring student experiences with remote learning.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.12.11.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.1.1">
<span class="ltx_p" id="S3.T1.1.12.11.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.12.11.1.1.1.1" style="font-size:80%;">Daria</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.12.11.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.12.11.2.1">
<span class="ltx_p" id="S3.T1.1.12.11.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.12.11.2.1.1.1" style="font-size:80%;">Understanding the data needs of workers in platform economies.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.13.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.13.12.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.1.1">
<span class="ltx_p" id="S3.T1.1.13.12.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.13.12.1.1.1.1" style="font-size:80%;">Amir</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.13.12.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.13.12.2.1">
<span class="ltx_p" id="S3.T1.1.13.12.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.13.12.2.1.1.1" style="font-size:80%;">Studying user interactions with recommendation systems.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.14.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.14.13.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.14.13.1.1">
<span class="ltx_p" id="S3.T1.1.14.13.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.14.13.1.1.1.1" style="font-size:80%;">Jenna</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.14.13.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.14.13.2.1">
<span class="ltx_p" id="S3.T1.1.14.13.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.14.13.2.1.1.1" style="font-size:80%;">Investigating public perceptions and best practices for measuring ML fairness.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.15.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.15.14.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.15.14.1.1">
<span class="ltx_p" id="S3.T1.1.15.14.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.15.14.1.1.1.1" style="font-size:80%;">Nadia</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.15.14.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.15.14.2.1">
<span class="ltx_p" id="S3.T1.1.15.14.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.15.14.2.1.1.1" style="font-size:80%;">Exploring identity representation through social media.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.16.15">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.16.15.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.16.15.1.1">
<span class="ltx_p" id="S3.T1.1.16.15.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.16.15.1.1.1.1" style="font-size:80%;">Elliot</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.16.15.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.16.15.2.1">
<span class="ltx_p" id="S3.T1.1.16.15.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.16.15.2.1.1.1" style="font-size:80%;">Studying worker views on AI in the workplace.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.17.16">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.17.16.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.17.16.1.1">
<span class="ltx_p" id="S3.T1.1.17.16.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.17.16.1.1.1.1" style="font-size:80%;">Rida</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.17.16.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.17.16.2.1">
<span class="ltx_p" id="S3.T1.1.17.16.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.17.16.2.1.1.1" style="font-size:80%;">Investigating privacy concerns related to accessibility technology.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.18.17">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.18.17.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.18.17.1.1">
<span class="ltx_p" id="S3.T1.1.18.17.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.18.17.1.1.1.1" style="font-size:80%;">Nikita</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.18.17.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.18.17.2.1">
<span class="ltx_p" id="S3.T1.1.18.17.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.18.17.2.1.1.1" style="font-size:80%;">Exploring activism and technology usage in social movements.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.19.18">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.19.18.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.19.18.1.1">
<span class="ltx_p" id="S3.T1.1.19.18.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.19.18.1.1.1.1" style="font-size:80%;">Jasmine</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.1.19.18.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.19.18.2.1">
<span class="ltx_p" id="S3.T1.1.19.18.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.19.18.2.1.1.1" style="font-size:80%;">Understanding AI literacy across K-12.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.20.19">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T1.1.20.19.1" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.20.19.1.1">
<span class="ltx_p" id="S3.T1.1.20.19.1.1.1" style="width:39.8pt;"><span class="ltx_text" id="S3.T1.1.20.19.1.1.1.1" style="font-size:80%;">Alice</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T1.1.20.19.2" style="padding-top:0.8pt;padding-bottom:0.8pt;">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.20.19.2.1">
<span class="ltx_p" id="S3.T1.1.20.19.2.1.1" style="width:180.7pt;"><span class="ltx_text" id="S3.T1.1.20.19.2.1.1.1" style="font-size:80%;">Examining value alignment in entrepreneurial ventures.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">Table 1. </span>Summary of interview participants’ pseudonyms and research topics.</figcaption>
</figure>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.1"><span class="ltx_text ltx_font_bold" id="S3.p6.1.1">Participants.</span> We recruited researchers through multiple channels: advertising on social networks, such as Twitter and LinkedIn, emailing direct contacts and messaging forums internal to our institution, and flyers around our campus. We solicited participants with prior training in qualitative methods and experience with at least one research project involving semi-structured interviews.
Eleven participants had 3-5 years of experience with qualitative research, six had more than 5 years, and two had between 1-3 years of experience. Although we invited qualitative researchers from various types of institutions, the majority of our interviewees are working in academia (16), with a smaller representation from industry (2) and the non-profit sector (1).
All our participants were located in the United States. Refer to Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S3.T1" title="Table 1 ‣ 3. Method ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_tag">1</span></a> for details on the participants’ research areas. To protect anonymity and minimize the risk of identification, we have not linked participants’ research topics with their demographic information. Each interviewee received a $45 gift card for their participation.</p>
</div>
<div class="ltx_para" id="S3.p7">
<p class="ltx_p" id="S3.p7.1"><span class="ltx_text ltx_font_bold" id="S3.p7.1.1">Analysis.</span> All interviews were conducted in English, video-recorded, and later transcribed for data analysis purposes. We followed the reflexive thematic analysis approach by Braun and Clarke <cite class="ltx_cite ltx_citemacro_citep">(Braun and Clarke, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib23" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib22" title="">2012</a>)</cite>. Reflexive thematic analysis foregrounds the researcher’s role in knowledge production, with ‘themes actively created by the researcher at the intersection of data, analytic process, and subjectivity’ <cite class="ltx_cite ltx_citemacro_citep">(Braun and Clarke, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib23" title="">2019</a>)</cite>. One member of the research team read each interview transcript multiple times, starting with familiarizing with the data, open coding instantiations of perceptions of LLMs, researchers’ definition of their process and topic, risks with using LLMs in qualitative research, and incentives and ethical considerations. The entire research team met regularly to discuss diverging interpretations or ambiguities and to define themes based on our initial codes. We transcribed 1611 minutes of video recording and obtained 638 first-level codes. As we generated themes from the codes, we also identified categories with a description and examples of each category. These categories included (1) the use of LLMs across the research workflows, (2) limitations, (3) barriers in addressing limitations, (4) potential approaches towards navigating ethical concerns, and (5) incentives for using LLMs. These categories were also discussed and iteratively refined through meeting, diverging, and synthesizing into three top-level categories, presented in our Findings. Since thematic coding was part of our analytical process rather than a final product, we do not apply inter-rater reliability (IRR) measures <cite class="ltx_cite ltx_citemacro_citep">(McDonald et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib79" title="">2019</a>; Soden et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib102" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S3.p8">
<p class="ltx_p" id="S3.p8.1">Through researchers’ interactions with the probe, we generated a dataset of 179 turns of conversation, where each conversation turn includes the system prompt set by the researcher, the interview question, and the model’s response. We conducted descriptive analyses on this data, including the number of turns per participant, turns per system prompt, and personas attempted. We also
applied thematic coding to identify patterns in the model’s response conditioned on details included in the system prompt.</p>
</div>
<div class="ltx_para" id="S3.p9">
<p class="ltx_p" id="S3.p9.1"><span class="ltx_text ltx_font_bold" id="S3.p9.1.1">Research ethics.</span> Participants were informed of the purpose of the study, the question categories, and researcher affiliations during recruitment. They signed informed consent documents acknowledging their awareness of the study’s objectives and procedures. We deleted all personally identifiable information in research files to protect participant identities. We redact all identifiable details when quoting participants and use pseudonyms to refer to them. We obtained approval from the Institutional Review Board at our institution. Although we do not release the interaction dataset to maintain participant privacy, we include relevant excerpts in section <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S4" title="4. Findings ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_tag">4</span></a> to illustrate key findings.</p>
</div>
<div class="ltx_para" id="S3.p10">
<p class="ltx_p" id="S3.p10.1"><span class="ltx_text ltx_font_bold" id="S3.p10.1.1">Limitations.</span>
Our participants were primarily academics with prior training in qualitative methods, which may not represent the diversity of perspectives outside of academic settings.
The study focused on a limited range of research topics, driven by the specific interests of our participants, which may not represent the full spectrum of qualitative research areas. The recruitment methods (social networks, email, institutional forums) may have introduced a selection bias, attracting participants who might have preconceived notions about using LLMs in qualitative research.
Additionally, we did not experiment with different language models; instead, we focused solely on GPT-4, a closed-source model. This decision allowed us to maintain consistency across interviews, but it also means that our findings may not generalize to interactions with other LLMs. We also did not fine-tune the model for specific personas, nor did we provide participants with extensive training on how to craft these personas. While this was an intentional choice to observe how researchers naturally engage with LLMs, it may have limited the variety and depth of the personas that were created.
</p>
</div>
<div class="ltx_para" id="S3.p11">
<p class="ltx_p" id="S3.p11.1"><span class="ltx_text ltx_font_bold" id="S3.p11.1.1">Positionality. </span>
Our author team brings together researchers with a range of disciplinary expertise, including HCI, ML, design, ethics, and science and technology studies. Each of us has experience in qualitative research, from three years to over a decade, in methods such as participant observation, ethnographic interviewing, contextual inquiry, and participatory design. Our qualitative research approach is grounded in an interpretivist paradigm, shaped by ethnomethodologically-informed feminist sensibilities. This orientation enables us to meaningfully engage with the situated nature of meaning-making and to critically examine the power structures that influence whose knowledge is considered legitimate. In addition to our backgrounds in research, our diverse racial and ethnic backgrounds—two team members identify as Middle Eastern, two as White Americans, and one as Asian—shape the critical lens we use to reflect on the ethical, epistemological, and methodological concerns associated with using LLMs to simulate community perspectives. We recognize that our positionality and analytic orientation played a crucial role in shaping the questions we asked and our interpretation of the data. By offering transparency about our backgrounds, we hope to clarify how these factors shaped the research process and outcomes, especially as we examine the roles of humans and AI in qualitative research and address issues of power, bias, and legitimacy with this emerging practice.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Findings</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we present an analysis of how researchers interacted with LLMs, capturing their observations and the dynamics of these interactions, including how they defined personas (section <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S4.SS1" title="4.1. Researcher perceptions of and interactions with LLMs ‣ 4. Findings ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_tag">4.1</span></a>). Our findings focus on six key limitations researchers identified with using LLMs as simulated participants (section <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S4.SS2" title="4.2. Fundamental Limitations of LLMs as Simulated Subjects in Qualitative Research ‣ 4. Findings ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_tag">4.2</span></a>). While most participants advised against using LLMs as the primary source of research data, they also acknowledged certain contexts where LLMs might be applied, albeit with significant caveats (section <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S4.SS3" title="4.3. Imagining Use Cases for LLMs in Qualitative Research: Seeing Possibilities and Even More Caveats ‣ 4. Findings ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_tag">4.3</span></a>).</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Researcher perceptions of and interactions with LLMs</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Before researchers engaged with the technology probe, we explored their attitudes toward using LLMs in qualitative research. Most participants approached the interview with a mix of skepticism and a spirit of inquiry toward LLMs. Some participants noted instances where they found LLMs helpful for tasks like writing and brainstorming, particularly in identifying key aspects of an argument that needed further emphasis. Alice, Amir, and Nico<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Please note that all names mentioned in this section are fictional. We have pseudonymized the data.</span></span></span> were open to exploring potential uses for LLMs in research but predominantly viewed them as tools for studying LLM behavior rather than understanding human behavior.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Participants elaborated on the objectives of qualitative research, often drawing on metaphors of distance to illustrate their argument. Alice, for instance, expressed uncertainty about whether <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.1">“using something several degrees removed from the source would tell [them] a lot about human behavior and underlying traits of people,”</span> suggesting that such an approach might be futile in understanding people. Jenna further argued that the strength of qualitative studies lies in their capacity to capture unique, embodied experiences.
Participants also reflected on the broader contexts of their work. Harper, an industry researcher, described her primary responsibility as storytelling with qualitative data and emphasized the importance of ensuring the data is <span class="ltx_text ltx_font_italic" id="S4.SS1.p2.1.2">“convincing enough”</span> to guide team members and management toward the right decisions. While acknowledging that LLMs could generate quick yes/no responses, she remained skeptical of their ability to surface underlying assumptions, challenges, and unspoken nuances that aren’t easily captured in writing. Overall, participants expressed skepticism about using LLMs but were intrigued by the opportunity to explore the technology through the interview probe. We include a descriptive analysis of the interaction data in table <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#S4.T2" title="Table 2 ‣ 4.1. Researcher perceptions of and interactions with LLMs ‣ 4. Findings ‣ ‘Simulacrum of Stories’: Examining Large Language Models as Qualitative Research Participants"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:166.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-40.1pt,15.3pt) scale(0.843899729422614,0.843899729422614) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T2.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.1" style="font-size:90%;">Type</span><span class="ltx_text" id="S4.T2.1.1.1.1.1.2" style="font-size:90%;"></span>
</th>
<th class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.1.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.1.1.2.1">
<span class="ltx_p" id="S4.T2.1.1.1.1.2.1.1" style="width:346.9pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.2.1.1.1" style="font-size:90%;">Count</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.2.1.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.1.1" style="font-size:90%;">Turns per researcher</span><span class="ltx_text" id="S4.T2.1.1.2.1.1.2" style="font-size:90%;"></span>
</th>
<td class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.1.2.1.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.2.1.2.1">
<span class="ltx_p" id="S4.T2.1.1.2.1.2.1.1" style="width:346.9pt;"><span class="ltx_text" id="S4.T2.1.1.2.1.2.1.1.1" style="font-size:90%;">Mean </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.2.1.1.2" style="font-size:90%;">(9.4)</span><span class="ltx_text" id="S4.T2.1.1.2.1.2.1.1.3" style="font-size:90%;">, Standard deviation </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.2.1.1.4" style="font-size:90%;">(4.1)</span><span class="ltx_text" id="S4.T2.1.1.2.1.2.1.1.5" style="font-size:90%;">, Min </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.2.1.1.6" style="font-size:90%;">(4)</span><span class="ltx_text" id="S4.T2.1.1.2.1.2.1.1.7" style="font-size:90%;">, Max </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.2.1.2.1.1.8" style="font-size:90%;">(20)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.3.2.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.3.2.1.1" style="font-size:90%;">Personas per researcher</span><span class="ltx_text" id="S4.T2.1.1.3.2.1.2" style="font-size:90%;"></span>
</th>
<td class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.1.3.2.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.3.2.2.1">
<span class="ltx_p" id="S4.T2.1.1.3.2.2.1.1" style="width:346.9pt;"><span class="ltx_text" id="S4.T2.1.1.3.2.2.1.1.1" style="font-size:90%;">Mean </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.3.2.2.1.1.2" style="font-size:90%;">(2.36)</span><span class="ltx_text" id="S4.T2.1.1.3.2.2.1.1.3" style="font-size:90%;">, Standard deviation </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.3.2.2.1.1.4" style="font-size:90%;">(1.16)</span><span class="ltx_text" id="S4.T2.1.1.3.2.2.1.1.5" style="font-size:90%;">, Min </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.3.2.2.1.1.6" style="font-size:90%;">(1)</span><span class="ltx_text" id="S4.T2.1.1.3.2.2.1.1.7" style="font-size:90%;">, Max </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.3.2.2.1.1.8" style="font-size:90%;">(6)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.4.3.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.1.1" style="font-size:90%;">Specified demographics</span><span class="ltx_text" id="S4.T2.1.1.4.3.1.2" style="font-size:90%;"></span>
</th>
<td class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.1.4.3.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.4.3.2.1">
<span class="ltx_p" id="S4.T2.1.1.4.3.2.1.1" style="width:346.9pt;"><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.1" style="font-size:90%;">age </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.2" style="font-size:90%;">(19)</span><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.3" style="font-size:90%;">, gender </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.4" style="font-size:90%;">(14)</span><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.5" style="font-size:90%;">, pronouns </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.6" style="font-size:90%;">(9)</span><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.7" style="font-size:90%;">, work history </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.8" style="font-size:90%;">(22)</span><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.9" style="font-size:90%;">, location </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.10" style="font-size:90%;">(20)</span><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.11" style="font-size:90%;">, race/ethnicity </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.12" style="font-size:90%;">(14)</span><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.13" style="font-size:90%;">, disability status </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.14" style="font-size:90%;">(7)</span><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.15" style="font-size:90%;">, immigration status </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.16" style="font-size:90%;">(9)</span><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.17" style="font-size:90%;">, religion </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.18" style="font-size:90%;">(1)</span><span class="ltx_text" id="S4.T2.1.1.4.3.2.1.1.19" style="font-size:90%;">, and name </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.2.1.1.20" style="font-size:90%;">(4)</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.1.5.4.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.5.4.1.1" style="font-size:90%;">Examples of specified background</span><span class="ltx_text" id="S4.T2.1.1.5.4.1.2" style="font-size:90%;"></span>
</th>
<td class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_t" id="S4.T2.1.1.5.4.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.5.4.2.1">
<span class="ltx_p" id="S4.T2.1.1.5.4.2.1.1" style="width:346.9pt;"><span class="ltx_text" id="S4.T2.1.1.5.4.2.1.1.1" style="font-size:90%;">Yue: </span><span class="ltx_text ltx_font_italic" id="S4.T2.1.1.5.4.2.1.1.2" style="font-size:90%;">‘You are a mental therapist who just got back from the U.S and shares sign language content about mental health’</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.1.1.6.5.1" style="padding-top:2.25pt;padding-bottom:2.25pt;"></th>
<td class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" id="S4.T2.1.1.6.5.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.6.5.2.1">
<span class="ltx_p" id="S4.T2.1.1.6.5.2.1.1" style="width:346.9pt;"><span class="ltx_text" id="S4.T2.1.1.6.5.2.1.1.1" style="font-size:90%;">Henri: </span><span class="ltx_text ltx_font_italic" id="S4.T2.1.1.6.5.2.1.1.2" style="font-size:90%;">‘He lives by himself and enjoys surfing and the sea’</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.6">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.1.1.7.6.1" style="padding-top:2.25pt;padding-bottom:2.25pt;"></th>
<td class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" id="S4.T2.1.1.7.6.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.7.6.2.1">
<span class="ltx_p" id="S4.T2.1.1.7.6.2.1.1" style="width:346.9pt;"><span class="ltx_text" id="S4.T2.1.1.7.6.2.1.1.1" style="font-size:90%;">Amal: </span><span class="ltx_text ltx_font_italic" id="S4.T2.1.1.7.6.2.1.1.2" style="font-size:90%;">‘focuses most of his lending in Africa and Asia as he sees these places are in the most need of monetary resources’</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.8.7">
<th class="ltx_td ltx_th ltx_th_row" id="S4.T2.1.1.8.7.1" style="padding-top:2.25pt;padding-bottom:2.25pt;"></th>
<td class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top" id="S4.T2.1.1.8.7.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.8.7.2.1">
<span class="ltx_p" id="S4.T2.1.1.8.7.2.1.1" style="width:346.9pt;"><span class="ltx_text" id="S4.T2.1.1.8.7.2.1.1.1" style="font-size:90%;">Nadia: </span><span class="ltx_text ltx_font_italic" id="S4.T2.1.1.8.7.2.1.1.2" style="font-size:90%;">‘you use TikTok daily and you mostly like your FYP and your feed is in English and Spanish’</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S4.T2.1.1.9.8.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.1.1.9.8.1.1" style="font-size:90%;">Model response length (#words)</span><span class="ltx_text" id="S4.T2.1.1.9.8.1.2" style="font-size:90%;"></span>
</th>
<td class="ltx_td ltx_nopad_l ltx_align_justify ltx_align_top ltx_border_bb ltx_border_t" id="S4.T2.1.1.9.8.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S4.T2.1.1.9.8.2.1">
<span class="ltx_p" id="S4.T2.1.1.9.8.2.1.1" style="width:346.9pt;"><span class="ltx_text" id="S4.T2.1.1.9.8.2.1.1.1" style="font-size:90%;">Mean </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.9.8.2.1.1.2" style="font-size:90%;">(356.5)</span><span class="ltx_text" id="S4.T2.1.1.9.8.2.1.1.3" style="font-size:90%;">, Standard deviation </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.9.8.2.1.1.4" style="font-size:90%;">(119.5)</span><span class="ltx_text" id="S4.T2.1.1.9.8.2.1.1.5" style="font-size:90%;">, Min </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.9.8.2.1.1.6" style="font-size:90%;">(33)</span><span class="ltx_text" id="S4.T2.1.1.9.8.2.1.1.7" style="font-size:90%;">, Max </span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.9.8.2.1.1.8" style="font-size:90%;">(679)</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2. </span>Summary of the interaction data with n = 179 total turns and n = 44 total system prompts across 19 qualitative researchers.
”Specified demographics” show how often particular demographic details were included in the system prompts. ”Examples of specified background” highlight research topic-related information included in the system prompts.
</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">In engaging with the interview probe, researchers observed that the perspectives emerging from LLM responses often mirrored those from their interviews with human participants. Many ideas expressed in the LLM outputs aligned with their participants’ statements, and model responses often appeared plausible. For example, Henri noted that some LLM responses related to senior care homes, particularly the lack of agency over routines and activities, closely matched sentiments expressed by older adults in his study. After observing similar responses that <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.1">“map well to what [she] found,”</span> Nadia reflected on how her recruitment methods, often relying on social media, might be limited in capturing responses from individuals with a minimal online presence, much like an LLM whose training data is predominantly drawn from online sources. Although she couldn’t find any <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.2">‘atrocious factual errors’</span>, she argued that the loss of context could complicate the process of making meaning with the data.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Several researchers commented on the level of detail in the LLM responses, which many attributed to the instructions provided in the system prompt. Amir expressed being <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.1">“impressed with the level of detail,”</span> while Laila articulated how the responses were <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.2">“detailed in a way that makes actual sense, not like gibberish.”</span> However, not all researchers shared this enthusiasm. Mario and Rida expressed frustration with the excessive detail. Mario pointed out a critical distinction: <span class="ltx_text ltx_font_italic" id="S4.SS1.p4.1.3">“There’s a difference between detail and depth. Those are orthogonal, right? You can have detail without depth. LLMs will give you reams and reams of text. You’re drowning in detail. But is it depth?”</span> Since the responses were often overtly comprehensive without prompting, researchers had to adjust their interviewing approach to ask directed questions, and they did not need to build rapport at the beginning of the interview.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Cameron, too, initially found the LLM responses impressively detailed and started to consider whether this approach might be effective in her interviews. She began by including demographic details relevant to her study in the first persona: “Imagine you are an 18-year-old Latina from Southeast Texas who was just admitted to an Ivy League school in the Northeast United States. You identify as low-income and will be a first-generation college student in the Fall.” In contrast, she removed most details from her second persona to replicate a scenario where researchers might not start with detailed information about participants before conducting interviews: “Imagine you are a college applicant who has been admitted to an Ivy League university.”</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">After running these two personas with varying levels of detail, Cameron observed subtle yet significant differences in the LLM’s responses. The responses for the unmarked Ivy League student (without demographics) depicted someone who started preparation early, had substantial resources, participated in extracurriculars, and received strong support from their family. The primary challenge for this student was balancing schoolwork with deadlines–much different from the LLM responses for the first persona of the Latina student that portrayed a <span class="ltx_text ltx_font_italic" id="S4.SS1.p6.1.1">“deficit framing in relation to [the Latina student’s] community”</span> according to Cameron. She was concerned that the model reinforced stereotypical notions of an Ivy League student and the strengths of the Latina participant’s community were being overlooked. The responses for the ‘unmarked’ persona without any demographic details reflected problematic assumptions of meritocracy that many of Cameron’s participants were actively challenging in her research. Cameron pointed out how the comprehensiveness of the responses might lead one to assume that recruiting human participants is unnecessary. However, after engaging with the probe through several personas, she realized that model responses tend to rely on unfounded assumptions about the community.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">In trying out variations of the participant description, researchers highlighted tensions with making them descriptive or not; for example, adding less information in the system prompt resulted in responses that relied on assumptions about the community. Conversely, Nolan, who included extensive background about their participant, found that the model’s responses simply <span class="ltx_text ltx_font_italic" id="S4.SS1.p7.1.1">“repeated the persona back to me,”</span> which he found amusing but not particularly useful. Qualitative analysis of the interaction data revealed instances where the model directly attributed specific traits or preferences to participant identities. For example, a model response for Esme stated, ‘Being non-binary and Black, it is crucial for me to find and create spaces where people like me can see themselves reflected in the media they consume,’ while for Nico, another model response noted, ‘As a 45-year-old sophomore experiencing both in-person and remote learning, I have a somewhat mixed perspective on remote learning in computing.’ Interviewees pointed out that the model’s overt reflexivity oversimplified the complex, intersectional nature of lived experiences and can lead to essentializing identities, as research participants do not always directly articulate their experiences in relation to parts of their identity.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Fundamental Limitations of LLMs as Simulated Subjects in Qualitative Research</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Below, we present the fundamental limitations of using today’s LLMs to understand the human experience highlighted by the participants in our study. While some concerns relate to the style and semantics (such as responses lacking in palpability), others relate to positionality and considerations of consent and autonomy. It is important to note that while some of these concerns might appear addressable (to different degrees) through prompt engineering or including more diverse data, our interviewees emphasized that such interventions can undermine methodological credibility if the researcher has to <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">‘fix’</span> or predetermine responses from their research participants.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">Model responses have limited palpability.</span>
The <em class="ltx_emph ltx_font_italic" id="S4.SS2.p2.1.2">palpability</em> of qualitative data refers to the concrete nature of reported evidence, capturing the distinct people, places, events, and motivations that convey a sense of lived experience (cf. <cite class="ltx_cite ltx_citemacro_citep">(Small and Calarco, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib101" title="">2022</a>)</cite>). Among our interviewees, several expressed frustration with the low palpability of LLM responses, which Mario likened to receiving <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.3">‘spark notes for an interview.’</span> Unlike data gathered from their human participants, which conveyed contextual nuance, the outputs from the LLM tended to be more abstract and detached from reality. Researchers noted that eliciting concrete examples and anecdotes from human participants required practice and relationship-building, whereas the model often produced a neat list of concerns at an analytical level, bypassing the skillful work of conducting qualitative research. While it was possible to craft prompts that elicited such stories from an LLM, some participants who succeeded still questioned the validity of the responses. Sophia, whose research focuses on understanding how technology mediates gig work, remarked how the model’s vague reference to an ‘unsafe neighborhood’ provided insufficient context for analyzing gendered and racialized experiences of safety and equity.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Researchers observed that LLM responses also lacked the <em class="ltx_emph ltx_font_italic" id="S4.SS2.p3.1.1">spontaneity</em> and <em class="ltx_emph ltx_font_italic" id="S4.SS2.p3.1.2">dynamism</em> that often emerge in human interviews. Their participants frequently diverged into tangents, discussing seemingly unrelated topics that turned out to be insightful for their study. Daria noted that while LLM responses were highly focused, ride-sharing workers in her research often shared specific stories about incidents (such as the participant’s car breaking down on a Saturday evening) that added richness to the data. On the other hand, Rida added that her participants would rarely volunteer detailed information about their everyday lives reflexively. Instead, through the course of an interview, participants would gradually come to realize and articulate their daily routines and access needs. In contrast, LLMs described experiences in clinical, detached terms, <span class="ltx_text ltx_font_italic" id="S4.SS2.p3.1.3">“almost like reading off a WebMD page,”</span> missing the nuances that emerge organically in interviews. Esme, whose research engaged with artists, emphasized:</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<blockquote class="ltx_quote" id="S4.SS2.p4.1">
<p class="ltx_p" id="S4.SS2.p4.1.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p4.1.1.1">An interview is an intimate act, and a machine cannot simulate the disclosures that emerge in this private space. There is a looseness to things. I had a participant who shared how ‘my creative process really kicks off when I do a bunch of mushrooms or smoke a lot of weed.’ I know this is true for many people in this data set. He actually came out and said it. I don’t know if I could convince a machine to tell me that.</span></p>
</blockquote>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">The limited palpability of LLM responses was more apparent for researchers exploring sensitive or personal topics. Nadia shared how her study on immigrant experiences included discussions about class, communism, and the trauma of escaping a country–conversations filled with emotion. Researchers noted that LLM responses, by comparison, were plain and lacked the emotional depth characteristic of human interactions. Interviews frequently elicited expressions of frustration, vulnerability, and even tears: elements absent in responses from the model. Henri shared how one of his participants cried in the interview after the loss of her son and husband. Interviewees found LLM responses limited in capturing the full spectrum of human emotion, which was crucial for understanding and reporting the complexities in participants’ stories. Models designed to generate polite and harmless responses as <span class="ltx_text ltx_font_italic" id="S4.SS2.p5.1.1">“helpful assistants”</span> (cf. <cite class="ltx_cite ltx_citemacro_citep">(Bai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib10" title="">2022</a>)</cite>) might do so at the expense of palpability, which Mario emphasized as crucial for producing rich qualitative research data.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p6.1.1">Amplifies influence of researcher positionality. </span>
Researchers are afforded several levers of control when using LLMs to simulate research participants, but this form of authority can complicate the researcher’s role in the knowledge-production process. Simulating participants typically begins with creating a persona, which requires making assumptions about the characteristics of potential participants. Researchers must choose which identity traits or background information to include, decisions that directly influence the model’s responses. While traditional interview recruitment is also subject to selection bias, simulating participants with LLMs makes these choices more explicit and high-stakes. Participants expressed concerns that this process risks reinforcing the researcher’s own biases, where researchers might inadvertently refine prompts to align the data with their expectations. The option to repeatedly prompt an LLM to receive subtly different responses each time introduces a risk of <em class="ltx_emph ltx_font_italic" id="S4.SS2.p6.1.2">confirmation bias</em>. Sophia captured this concern effectively:</p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<blockquote class="ltx_quote" id="S4.SS2.p7.1">
<p class="ltx_p" id="S4.SS2.p7.1.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p7.1.1.1">You get to dictate who you’re talking to. In real interviews, I can’t just say, ‘Only give me immigrants from Ghana in their mid-thirties’. If you go into this mindset of interviewing with an LLM where you know exactly who you will talk to, then you will not learn anything because you already have expectations of what you’ll learn.</span></p>
</blockquote>
</div>
<div class="ltx_para" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.1">Researchers also emphasized that interviewing is an active process of meaning-making. Data is not waiting to be collected; rather, it is shaped and brought into existence through the researcher’s engagement with the community and their interpretation of it. Harper, who conducted research with religious communities, noted that the researcher’s presence can influence and shift the community’s practices and dynamics. Harper reflected, <span class="ltx_text ltx_font_italic" id="S4.SS2.p8.1.1">“What foreign influences are felt? What aspects are already present? How does my respect for or belief in their system, or lack thereof, affect my approach and how they perceive it?”</span> Similarly, Yue discussed the effects of their own presentation during research with d/Deaf community members. Whether Yue presented as a hearing person with limited knowledge of sign language or as someone familiar with it could significantly impact the depth of detail participants provided about their challenges. Meaning and knowledge in qualitative research emerge from the relational context between researcher and participant.</p>
</div>
<div class="ltx_para" id="S4.SS2.p9">
<p class="ltx_p" id="S4.SS2.p9.1">When examining the limitations of using LLMs for simulating research participants, it is crucial to consider the differences between emic (insider) and etic (outsider) perspectives about the researchers’ own identities<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Emic-etic perspectives exist along a spectrum and are context-specific rather than binary and clearly differentiated. A solely emic perspective is impossible to achieve with the subjectivity of human experiences <cite class="ltx_cite ltx_citemacro_citep">(Olive et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib83" title="">2014</a>)</cite>.</span></span></span>. Nadia pointed out how out-group researchers might struggle to identify stereotypes in the data if they lack direct experience with the topic or community being studied. Esme likened this issue to <span class="ltx_text ltx_font_italic" id="S4.SS2.p9.1.1">“parachute science”</span>, where simulations are based on a shallow understanding of the community. Nikita elaborated how they avoid studying communities they aren’t embedded in: <span class="ltx_text ltx_font_italic" id="S4.SS2.p9.1.2">“I’m not a person who does research from the outside. I would never go into an inner city and figure out what’s happening there unless I lived in that city. ”</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p10">
<p class="ltx_p" id="S4.SS2.p10.1">In contrast, in-group researchers could bring contextual knowledge to help them assess whether the data perpetuates harmful misrepresentations. Researchers in our study mentioned being drawn to projects related to their own background or lived experiences. However, using LLMs as research participants introduced the potential for encountering simulated experiences that reflect their community but are ultimately flawed. Nikita described this feeling as akin to the <span class="ltx_text ltx_font_italic" id="S4.SS2.p10.1.1">“uncanny valley,”</span> where a machine’s responses seem human-like but reflect uncomfortable inaccuracies. Laila, too, felt unsettled after reading responses from the system trying to relate a person sharing similar experiences as her, describing it as <span class="ltx_text ltx_font_italic" id="S4.SS2.p10.1.2">“creepy and disingenuous”</span>. This could lead to affective discomfort and harm, as researchers might experience these simulations as micro-aggressions.</p>
</div>
<div class="ltx_para" id="S4.SS2.p11">
<p class="ltx_p" id="S4.SS2.p11.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p11.1.1">Model’s epistemic position remains ambiguous.</span>
Epistemic position refers to one’s relationship to an object of knowledge, including the methods of gathering and interpreting that knowledge. Researchers struggled to determine the epistemic position reflected by the model, including whether it conveyed a singular viewpoint. Interviewees expressed how LLM responses often aggregated arguments from multiple interview participants from their study into a single consolidated response, which Daria described as a <span class="ltx_text ltx_font_italic" id="S4.SS2.p11.1.2">“simulacrum of stories that people shared”</span>. Elliot, whose research examines workers’ perspectives on algorithmic management, observed how, <span class="ltx_text ltx_font_italic" id="S4.SS2.p11.1.3">“the model combines a fictitious worker perspective with management’s perspective. A lot of what it presents as benefits of this technology are things that managers are sold on, but workers don’t necessarily experience.”</span> For Elliot, understanding the broader context–such as the wages, interpersonal conflicts between employees &amp; employers, and power structures–was crucial for interpretation and generating a thick description. The consolidation of contrasting perspectives in LLM responses not only obscures a coherent worker voice; it also undermines the representation of partial, situated knowledge.</p>
</div>
<div class="ltx_para" id="S4.SS2.p12">
<p class="ltx_p" id="S4.SS2.p12.1">LLM responses are notoriously sensitive to the language and framing of prompts (cf. <cite class="ltx_cite ltx_citemacro_citep">(Sclar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib96" title="">2023</a>)</cite>), a pattern consistently observed in our interviews. Daria noted how the LLM was <span class="ltx_text ltx_font_italic" id="S4.SS2.p12.1.1">“picking up on the valence of the system prompt and working hard to respond to it”</span> when she shifted from probing for “transparency concerns” to asking about “experiences with information available on the app.” Interviewees highlighted how subtle shifts in language (<span class="ltx_text ltx_font_italic" id="S4.SS2.p12.1.2">e.g.,</span> with structure, framing, or emotional undertones) drastically altered the model’s responses from a negative to a positive outlook. The model’s tendency for brittleness could compromise the reliability and consistency of their qualitative data.</p>
</div>
<div class="ltx_para" id="S4.SS2.p13">
<p class="ltx_p" id="S4.SS2.p13.1">Participants expressed concerns about the lack of transparency surrounding the data used to train LLMs. When the goal of research is to understand specific groups, the validity of LLM-generated responses is questionable if it is unclear whether those groups’ perspectives are adequately represented in the training data. Sophia illustrated this issue by pointing out that responses could vary significantly depending on whether the model had been trained on worker forums like ‘Uberpeople.net’ or more corporate-driven sources such as Uber.com’s own testimonial pages. Sophia brought attention to the challenges of evaluating the model’s epistemic position by emphasizing the uncertainty about the <span class="ltx_text ltx_font_italic" id="S4.SS2.p13.1.1">“balance of this data or what it’s being trained on”</span> and the ‘freshness’ of perspectives given that the model’s training data typically only extends up to a cut-off date (<span class="ltx_text ltx_font_italic" id="S4.SS2.p13.1.2">e.g.,</span> 2023 for GPT-4). Experiences are not only situated in relation to people and places but also in time. Nikita emphasized that <span class="ltx_text ltx_font_italic" id="S4.SS2.p13.1.3">“a lot of the important bits of cultural context is its particular moment in time,”</span> and questioned whether the model was averaging perspectives across time or reflecting a specific moment, and if so, which one. Without clear information about the sources and proportions of data used to train LLMs, researchers struggled to identify the potential validity issues with the responses.</p>
</div>
<div class="ltx_para" id="S4.SS2.p14">
<p class="ltx_p" id="S4.SS2.p14.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p14.1.1">Facilitates erasure of community perspectives. </span>
Erasure refers to the systematic exclusion or invisibilization of certain groups and their standpoints when systems of knowledge production privilege certain voices over others. One significant risk of using LLMs in qualitative research is the erasure of perspectives from underrepresented groups. Laila, who studied Black social media creators on YouTube, noticed that the LLM tended to caricature participants, choosing stereotypical Black-sounding names when the topic involved Black history. She highlighted a broader concern: while LLMs might capture general sentiments <span class="ltx_text ltx_font_italic" id="S4.SS2.p14.1.2">about</span> a community, they often fail to authentically represent voices <span class="ltx_text ltx_font_italic" id="S4.SS2.p14.1.3">from</span> within that community. Similarly, Esme noticed how model responses <span class="ltx_text ltx_font_italic" id="S4.SS2.p14.1.4">“included Black history tropes like representation and freedom in a way that wouldn’t resonate with a Black person doing this artwork in the Southeast, for instance.”</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p15">
<p class="ltx_p" id="S4.SS2.p15.1">Researchers attributed much of the erasure to the pipeline for building the current generation of large language models, including the training data and the alignment process. LLM responses lacked the critical authenticity that human participants provide, particularly when researchers have invested time in building trust and rapport. Researchers felt that LLMs produced sanitized, politically correct responses, missing the messiness that characterizes real human experiences. Amir shared how his research participants would often share controversial opinions, such as which funding should be prioritized (ones related to the environment) over others (serving disabled people from developed countries). Similarly, Henri, who interviewed occupational therapists working with Alzheimer’s patients, noted that while LLMs might echo <span class="ltx_text ltx_font_italic" id="S4.SS2.p15.1.1">“by-the-book”</span> responses, they fail to capture the contradictions that emerge in interview settings:</p>
</div>
<div class="ltx_para" id="S4.SS2.p16">
<blockquote class="ltx_quote" id="S4.SS2.p16.1">
<p class="ltx_p" id="S4.SS2.p16.1.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p16.1.1.1">“In the first interview, the occupational therapist will give you the ‘best practice response’. They told us they try not to separate people with dementia from people without cognitive impairments. But yeah, they have to do it during a lot of activities, which is unfortunate and not advisable, but it’s a practice that happens, and they express on the fourth day.”</span></p>
</blockquote>
</div>
<div class="ltx_para" id="S4.SS2.p17">
<p class="ltx_p" id="S4.SS2.p17.1">Researchers also highlighted the ambiguity surrounding how LLMs generate responses that are meant to reflect specific communities. When a model is assigned a persona based on a cultural, ethnic, or social identity, there is often uncertainty about whether the model is drawing from data that represents the lived experiences and perspectives of that community or simply regurgitating the surface-level characteristics associated with that community. Interviewees like Harper and Alice, who studied small, niche communities (<span class="ltx_text ltx_font_italic" id="S4.SS2.p17.1.1">e.g.,</span> investors and AI developers) with strong value systems, observed that the LLM could grasp broad-stroke dynamics (<span class="ltx_text ltx_font_italic" id="S4.SS2.p17.1.2">“contours of the community”</span>) but missed the nuanced realities of those deeply embedded in these communities. Researchers went on to problematize the idea of constructing a persona for an LLM, asking critical, rhetorical questions such as <span class="ltx_text ltx_font_italic" id="S4.SS2.p17.1.3">‘okay, well, now [the model is] a Latina, but what does that mean? What does that mean to this LLM?’</span> If the model is only speaking <span class="ltx_text ltx_font_italic" id="S4.SS2.p17.1.4">like</span> the participant description–using language, idioms, and stylistic markers associated with that identity–without grounding its responses in the actual data from that community, the results could be misleading or, worse, harmful stereotypes. These reflections highlighted a deeper discomfort with the superficiality of assigning complex social identities to a machine that does not embody the lived experiences of individuals with those identities.</p>
</div>
<div class="ltx_para" id="S4.SS2.p18">
<p class="ltx_p" id="S4.SS2.p18.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p18.1.1">Forecloses participants’ autonomy, agency, and consent. </span>
Using LLMs to simulate human behavior introduces several risks related to participants’ autonomy, consent, and agency. One way agency and participation manifest in the research process is through the expression of disagreement. Interviewees reported several instances where their human participants challenged the researchers’ premises or wording around the research topic. This critical engagement is a valuable aspect of the research process, ensuring that the data collected accurately reflects participants’ lived experiences, which might differ from the researchers’ initial assumptions. In contrast, researchers highlighted how LLMs were less likely to exhibit such resistance unless explicitly prompted. As Daria observed:</p>
</div>
<div class="ltx_para" id="S4.SS2.p19">
<blockquote class="ltx_quote" id="S4.SS2.p19.1">
<p class="ltx_p" id="S4.SS2.p19.1.1"><span class="ltx_text ltx_font_italic" id="S4.SS2.p19.1.1.1">Many participants, especially in this kind of research, have their own agendas.
Participants in studies about app issues know that we’re asking about problems, so they might tailor their responses accordingly. The LLM, similarly, seems to fit responses into expected categories without pushing back. In human interactions, however, people are more likely to disagree with the premise or question without needing explicit prompts. When I asked drivers about the data they receive from apps and how it aids their planning, I initially assumed that the apps did not provide much information. Yet, some drivers told me, ‘I get all the information I need about my work history.’</span></p>
</blockquote>
</div>
<div class="ltx_para" id="S4.SS2.p20">
<p class="ltx_p" id="S4.SS2.p20.1">Researchers noted frustration with the model’s sycophantic tendency (cf. <cite class="ltx_cite ltx_citemacro_citep">(Perez et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib87" title="">2022</a>)</cite>) to agree with their points. Esme likened this to improvisation comedy, where performers are required to agree with everything said by their partners, but people contradict her all the time in her research, if <span class="ltx_text ltx_font_italic" id="S4.SS2.p20.1.1">“they don’t like [her] wording, they don’t have that experience, or they wouldn’t agree with what a question suggests. The whole point is that they’re not supposed to be a yes man.”</span> Hugo expressed how the model would generate the perceived ‘preferred answer’ when probed about perceptions of robots in senior care homes.</p>
</div>
<div class="ltx_para" id="S4.SS2.p21">
<p class="ltx_p" id="S4.SS2.p21.1">In many cases, participants came to interviews eager to have their stories heard. Nico, who studied remote learning during the pandemic, observed that many students were dissatisfied with their college experiences and saw the interview as an opportunity to voice their frustrations. Participants actively sought to share their experiences, often introducing their own terminologies that Nico would then adopt. Similarly, Nikita noted how they <span class="ltx_text ltx_font_italic" id="S4.SS2.p21.1.1">“don’t want to talk to a computer telling [them] that trans lives matter or that trans health care is a problem. I wanna talk, I wanna be heard. I want us to connect over that.”</span> People’s desire to influence research outcomes is an important part of their expression of agency.</p>
</div>
<div class="ltx_para" id="S4.SS2.p22">
<p class="ltx_p" id="S4.SS2.p22.1">The use of LLMs raises ethical concerns about consent. LLMs might generate responses on sensitive topics that individuals might prefer not to disclose, overstepping boundaries that would otherwise be respected in an interview setting. Sophia, investigating the phenomenon of renter-ship among gig workers, noted a participant’s reluctance to discuss the topic further: <span class="ltx_text ltx_font_italic" id="S4.SS2.p22.1.1">“I asked one of my participants about this, and he was reluctant to answer. I could tell he knew more but wasn’t comfortable discussing it.”</span> The use of LLMs as simulated subjects also compromises the autonomy and consent of the data subjects whose stories are potentially extracted in this interview data. Participants expressed hesitation about using a model whose training data is obtained without explicit consent. Laila compared this to the discourse surrounding AI-generated art, where artists’ creative works are used without permission to create new images. Similarly, potentially using experiences people share online to generate interview responses without their knowledge undermines the principle of autonomy, which is a cornerstone of research ethics.</p>
</div>
<div class="ltx_para" id="S4.SS2.p23">
<p class="ltx_p" id="S4.SS2.p23.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p23.1.1">Delegitimizes qualitative ways of knowing. </span>
The use of LLMs in qualitative research poses risks not only to the methodological integrity of the field but also to the legitimacy of qualitative research within academic circles. Several researchers voiced concerns that qualitative research is already undervalued, often perceived as less rigorous compared to quantitative approaches. The introduction of LLMs could exacerbate the marginalization of this mode of knowledge production by creating the impression that deeply contextual work of qualitative research can be easily replicated by computational models and done so more quickly. Sophia, who previously faced skepticism from reviewers when using qualitative methods, articulated how the use of LLMs in qualitative contexts seems like an attempt to demonstrate that <span class="ltx_text ltx_font_italic" id="S4.SS2.p23.1.2">“quantitative work can do what qualitative work does but faster.”</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p24">
<p class="ltx_p" id="S4.SS2.p24.1">Researchers expressed fears that the availability of LLMs could encourage a <span class="ltx_text ltx_font_italic" id="S4.SS2.p24.1.1">‘cutting corners’</span> mindset in research. The use of LLMs would function as a form of ‘data extraction’ that is antithetical to the values of qualitative research. Qualitative research is often iterative and collaborative– involving ongoing dialogue and reflection– where researchers work with participants to co-create knowledge grounded in lived experience. Indeed, researchers discussed how interviews and other forms of qualitative data collection are not just about gathering information; they are an important part of establishing and maintaining ongoing relationships with participants. Daria, Esme, and Elliot, for example, described how they continue to interact with participants beyond their research project through conversations with ride-sharing drivers, following artists on social media, or collaborating with union members. When LLMs are used to generate participant responses, this collaborative process is taken over by a more transactional approach where data is extracted from the model without ongoing engagement with communities.</p>
</div>
<div class="ltx_para" id="S4.SS2.p25">
<p class="ltx_p" id="S4.SS2.p25.1">Another important concern raised by researchers was the potential damage that using LLMs could inflict on the trust between qualitative researchers and the communities they engage with. Historically, many vulnerable communities have developed deep distrust toward researchers due to exploitative practices in which academics extracted data or introduced interventions without offering ongoing support or maintenance. This legacy of distrust could be further exacerbated if researchers begin to replace participant perspectives with LLM-generated responses. Yue, who works closely with d/Deaf participants, expressed concern that such practices would erode trust in research, especially within communities that are already wary of being misrepresented. Reducing these communities’ voices to algorithmic outputs undermines the value of their lived experiences, potentially eroding the trust that researchers worked hard to build.</p>
</div>
<div class="ltx_para" id="S4.SS2.p26">
<p class="ltx_p" id="S4.SS2.p26.1">The concerns expressed by researchers reflect a broader anxiety about the potential implications of relying on LLMs in qualitative research. For Nikita, the use of LLMs evoked a <span class="ltx_text ltx_font_italic" id="S4.SS2.p26.1.1">“sense of the dystopian,”</span> where, for example, trans communities are not heard by legislators or other decision-makers, as their perspectives are distorted by technology rather than being directly represented. Cameron summarized this concern by emphasizing that such tools miss the <span class="ltx_text ltx_font_italic" id="S4.SS2.p26.1.2">“epistemological underpinnings of qualitative methods and why we do them”</span>. She argued that if the goal of qualitative research is to obtain contextualized data that is grounded in people’s lived experiences, then LLMs fundamentally fail to meet this standard. LLMs may generate text that <span class="ltx_text ltx_font_italic" id="S4.SS2.p26.1.3">appears</span> fluent and contextually relevant, but this output lacks the depth, nuance, and authenticity that come from direct engagement with people’s lived realities.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Imagining Use Cases for LLMs in Qualitative Research: Seeing Possibilities and Even More Caveats</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Most researchers voiced their discomfort with using LLMs to generate synthetic research data, with several even suggesting that they would distrust findings from studies that utilize LLMs as research participants. As a thought exercise, we explored whether there might be specific domains, contexts, or use cases where LLMs could be more effective. Below, we outline the use cases that researchers identified as <span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">more</span> appropriate than replacing direct engagement with communities. It is important to note that our participants did not reach a consensus on any of these use cases. For each potential application, participants also highlighted ways in which LLM use could be harmful or ineffective.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Interviewees suggested that using LLMs to simulate participants could serve as a valuable pedagogical tool where the stakes are lower than in a research study. This approach could help novice researchers learn how to focus on specific aspects of responses and ask effective follow-up questions. Some interviewees also highlighted flaws in this proposition, noting that prompting an LLM often differs significantly from interacting with human participants. For example, Daria had to prompt the model thrice before it provided enough contextual detail to make a useful response. Researchers emphasized that a critical interviewing skill is managing emotions—their own and the participant’s—which is difficult to replicate when learning qualitative methods with LLMs. Jasmine also pointed out the risk that junior researchers learning qualitative methods with LLMs might develop poor interviewing habits (<span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.1">e.g.,</span> not building rapport, not probing for depth, interrupting participants, or ignoring body language) that do not translate well to real research contexts and could perpetuate the notion of <span class="ltx_text ltx_font_italic" id="S4.SS3.p2.1.2">“extracting information”</span> from research subjects.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">For most researchers, LLMs could, at their best, help test their interview protocol, especially when it is challenging to recruit participants for pilot studies. In those situations, LLMs could serve as stand-ins, allowing researchers to gauge the types of responses a question might elicit or determine what kinds of questions to ask. Here, Mario cautioned that relying on LLMs for pilot interviews or refining interview guides could shift the focus of the project in unexpected ways. He pointed out how LLMs might lead researchers to become overly fixated on specific directions that might otherwise not occur if researchers were interviewing real participants.</p>
</div>
<div class="ltx_para" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1">Several researchers observed that the decision to use LLMs depends on the research topic and the community being simulated. In sensitive research contexts, such as those involving experiences of oppression or discrimination, participants noted that engaging with LLMs could carry some potential benefits and the significant risks described above. Sensitive research topics risk placing an epistemic burden on participants and re-traumatizing them by asking them to revisit difficult experiences. While some researchers suggested that LLMs could be useful in these contexts, they also expressed concerns that using LLMs for sensitive topics might exacerbate the erasure of real human experiences. Nadia, for example, was skeptical about the ability of LLMs to accurately simulate complex human experiences, such as navigating gender identity or sexuality in the workplace or the experiences of refugees and migrants. On the other hand, Nadia expressed how there are certain communities, such as hate groups, that they don’t feel safe engaging with. She argued that those interviews could be simulated with an LLM to help develop defensive strategies against hate groups online. Nikita argued that for sensitive topics, researchers should consider collaborating with community members to gain relevant expertise and learn how to have difficult conversations, rather than <span class="ltx_text ltx_font_italic" id="S4.SS3.p4.1.1">“turning to an LLM.”</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussion</h2>
<div class="ltx_para" id="S5.p1">
<blockquote class="ltx_quote" id="S5.p1.1">
<p class="ltx_p" id="S5.p1.1.1">The claim that technologies can act as surrogates recapitulates histories of disappearance, erasure, and elimination necessary to maintain the liberal subject as the agent of historical progress.
       <span class="ltx_text" id="S5.p1.1.1.1"></span> (<span class="ltx_text" id="S5.p1.1.1.2">Neda Atanasoski and Kalindi Vora <cite class="ltx_cite ltx_citemacro_citep">(Atanasoski and Vora, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib7" title="">2019</a>)</cite></span>)</p>
</blockquote>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">We argue that the use of LLMs as research subjects enacts what Atanasoski and Vora describe as the ‘surrogate effect’ <cite class="ltx_cite ltx_citemacro_citep">(Atanasoski and Vora, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib7" title="">2019</a>)</cite>. In <span class="ltx_text ltx_font_italic" id="S5.p2.1.1">Surrogate Humanity</span>, they reveal how technologies like social robots and artificial intelligence are not neutral tools but are instead shaped by capitalist logics of differential exploitation and dispossession. These technologies are designed to act as surrogates, ostensibly to emancipate humans from “historically degraded tasks” that are considered less-than-human. When LLMs are used as stand-ins for human participants in research, they <span class="ltx_text ltx_font_italic" id="S5.p2.1.2">similarly</span> function as surrogates–replacing the voices and lived experiences of actual communities with algorithmic approximations.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">It is crucial to acknowledge the invisible gendered, classed, and racialized labor underpinning these systems.
LLMs are not self-sustaining entities that emerge in a vacuum; their creation involves vast networks of a global workforce that contribute to the training and maintenance of these systems <cite class="ltx_cite ltx_citemacro_citep">(Miceli and Posada, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib80" title="">2022</a>)</cite>. These workers, who are often underpaid and overworked <cite class="ltx_cite ltx_citemacro_citep">(Sambasivan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib91" title="">2021</a>)</cite> engage in the labor of labeling data, evaluating and moderating content, and correcting model errors, all of which are essential for the functionality of LLMs. Tech companies and platforms aim to disguise this human labor as machine labor to create a veneer of machine intelligence <cite class="ltx_cite ltx_citemacro_citep">(Atanasoski and Vora, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib7" title="">2019</a>)</cite>. As Atanasoski and Vora articulate, ”labor becomes something that is intentionally obfuscated to create the effect of machine autonomy” <cite class="ltx_cite ltx_citemacro_citep">(Atanasoski and Vora, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib7" title="">2019</a>)</cite>. This intentional obfuscation creates a <span class="ltx_text ltx_font_italic" id="S5.p3.1.1">conjuration of algorithms</span> <cite class="ltx_cite ltx_citemacro_citep">(Nagy and Neff, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib82" title="">2024</a>)</cite> that enables the fantasy of magical techno-objects and masks the exploitation and uneven distribution of labor benefits in the global tech industry.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Science and Technology Studies (STS) scholars have long interrogated the ways in which technologies are not merely tools–for research or otherwise– but are imbued with social, political, and economic values. Sheila Jasanoff’s concept of <em class="ltx_emph ltx_font_italic" id="S5.p4.1.1">co-production</em> highlights how scientific knowledge and social order are produced together, each shaping and stabilizing the other <cite class="ltx_cite ltx_citemacro_citep">(Jasanoff, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib57" title="">2004</a>)</cite>. LLMs’ role as surrogates in research is also a co-production of knowledge and power. These models are encoded with the biases, assumptions, and power dynamics of model producers and the data and contexts from which they are derived. As such, their use in research does not merely replace human participants; it reshapes the nature of the knowledge produced, often in ways that reinforce existing hierarchies and exclusions.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">We advocate for the need to critically examine how LLMs, products of specific socio-technical assemblages, contribute to a reconfiguration of what counts as valid knowledge and who gets to produce it. Technological artifacts have politics, too, as they embody specific forms of power and authority <cite class="ltx_cite ltx_citemacro_citep">(Winner, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib113" title="">2017</a>)</cite>. The use of LLMs as research subjects exemplifies this notion by centralizing the role of technology in knowledge production while marginalizing the participation of human subjects, particularly those from vulnerable or historically underrepresented communities. Below, we draw on scholarship from qualitative epistemologies and research ethics &amp; care to argue for a critical reconsideration of the use of LLMs as surrogate participants, advocating for caution and, in most cases, avoiding such practices.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>LLMs are incongruent with qualitative epistemologies</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Human subjects research is an active process of constructing meaning that Ithiel de Sola Pool describes as an <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.1">“interpersonal drama with a developing plot”</span> <cite class="ltx_cite ltx_citemacro_citep">(Silverman, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib100" title="">2004</a>)</cite>. Qualitative research epistemologies, especially within HCI, emphasize the importance of researcher positionality: the idea that knowledge is co-constructed by the researcher and the participant and is deeply influenced by the specific context in which it is generated. The interview is not a neutral process of extracting information that exists <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.2">out there</span> <cite class="ltx_cite ltx_citemacro_citep">(Taylor, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib104" title="">2011</a>)</cite>; instead, research interviews are intentional, and the interviewer is implicated in the process of assembling knowledge. Research participants, too, are not <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.1.3">‘vessels of answers’</span> waiting to be tapped in to elicit truths about facts, opinions, or their lived experiences <cite class="ltx_cite ltx_citemacro_citep">(Law, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib69" title="">2004</a>)</cite>. Such an interpretivist orientation inherently values the nuances, complexities, and particularities of lived experience, often best captured through methods that allow for deep engagement, reflection, and interpretation.</p>
</div>
<div class="ltx_para" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1">Indeed, there are many sites of knowledge production, particularly in a community-based project, that exist outside the confines of a survey questionnaire or one-hour interview <cite class="ltx_cite ltx_citemacro_citep">(Van Maanen, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib108" title="">2011</a>)</cite>. This multisitedness, articulated by Law <cite class="ltx_cite ltx_citemacro_citep">(Law, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib69" title="">2004</a>)</cite>, involves a multiplicity of locations where <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.1">‘analysis and knowledge are made material.’</span> LeDantec and Fox <cite class="ltx_cite ltx_citemacro_citep">(Le Dantec and Fox, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib70" title="">2015</a>)</cite> describe this as work that comes before the work: building relationships and demonstrating commitments that shape the outcomes of the projects. We argue that interviewing with LLMs is fundamentally misaligned with the notion that <span class="ltx_text ltx_font_italic" id="S5.SS1.p2.1.2">“methods [no longer] discover and depict realities. they participate in the enactments of those realities.”</span> <cite class="ltx_cite ltx_citemacro_citep">(Law, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib69" title="">2004</a>)</cite></p>
</div>
<div class="ltx_para" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1">Finally, qualitative methods are designed to produce knowledge that is not only rich and detailed, in the form of a <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.1">thick description</span> <cite class="ltx_cite ltx_citemacro_citep">(Geertz, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib43" title="">2008</a>)</cite>, but also situated within specific cultural, social, and material contexts. Haraway’s idea of situated knowledges invites a critical perspective that helps to problematize <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.2">‘the god trick of seeing everything from nowhere’</span> <cite class="ltx_cite ltx_citemacro_citep">(Haraway, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib51" title="">2013</a>)</cite>. Haraway urges us to resist myths of infinite vision, seemingly offered by the visualization tricks and powers of modern technology. In thinking with Haraway, we contend that the prevailing logics of LLMs as proxies for human behavior offer similar <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.3">unlocatable</span> knowledge claims.</p>
</div>
<div class="ltx_para" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1">LLMs, by their very design, operate on principles that are at odds with feminist epistemological foundations. They are built to extract patterns that can be applied across contexts with the aim of achieving universality and objectivity <cite class="ltx_cite ltx_citemacro_citep">(Barocas et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib12" title="">2018</a>)</cite>. While this capacity can seemingly lead to efficiencies and broad applicability, it also means that the knowledge produced by LLMs is often decontextualized and disembodied. The models are trained on data that, despite its volume, may lack the depth and specificity that qualitative researchers seek. Moreover, LLMs do not account for the positionality of the researcher or the participants; instead, they flatten differences and obscure the peculiarities that are central to qualitative inquiry. This turn to LLMs reflects a world in which the complexity and messiness of human experience are reduced to manageable data points that can be easily processed by algorithms. In doing so, the use of LLMs in research aligns with a technocratic vision of science that prioritizes efficiency and scalability over contextual engagement with communities.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>LLM use undermines consent and autonomy of data subjects</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">HCI research has long been dedicated to supporting the autonomy and agency of research participants. We recognize the importance of ensuring that individuals have control over their participation and the information they share. In the broader context of coming to a shared understanding of research ethics, HCI and CSCW scholars have expressed concerns with the use of ‘public’ social media data for research purposes <cite class="ltx_cite ltx_citemacro_citep">(Klassen and Fiesler, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib65" title="">2022</a>; Zimmer, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib123" title="">2020</a>)</cite>. This includes practices, such as the mining of profile pictures, to identify social categories such as sexual orientation <cite class="ltx_cite ltx_citemacro_citep">(Levin, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib71" title="">2017</a>)</cite> or facial recognition from students’ photos <cite class="ltx_cite ltx_citemacro_citep">(Fussell, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib41" title="">2019</a>)</cite>. Social media users are often unaware that their data is being used by researchers, and in fact, many participants in Fiesler and Proferes’s study asserted that researchers should not be able to use tweets without explicit consent <cite class="ltx_cite ltx_citemacro_citep">(Fiesler and Proferes, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib38" title="">2018</a>)</cite>. Online content creators, including social media users, may not realize the public nature of their data and the downstream applications the data may be supporting <cite class="ltx_cite ltx_citemacro_citep">(Fiesler and Proferes, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib38" title="">2018</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">We believe similar risks apply when researchers use LLMs to create synthetic interview data. Most current LLMs are trained on public content extracted from the Internet <cite class="ltx_cite ltx_citemacro_citep">(Byrd, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib24" title="">2023</a>)</cite>. Earlier this year, Google and OpenAI signed contracts with Reddit that give these companies real-time access to Reddit posts <cite class="ltx_cite ltx_citemacro_citep">(Roth, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib90" title="">2024</a>; David, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib33" title="">2024</a>)</cite>. OpenAI maintains that ‘training AI models using publicly available internet materials is fair use, as supported by long-standing and widely accepted precedents’ <cite class="ltx_cite ltx_citemacro_citep">(ope, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib2" title="">[n. d.]</a>)</cite>. Many other big tech companies have made arguments to similar effect when faced with lawsuits. While training on publicly available and/or copyrighted text may not be unlawful (yet), generating research data using LLMs–models created through the expropriation of the work of others–presents the risk of undermining the autonomy of data subjects powering these applications.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">A productive framework for understanding the relationship between autonomy and consent in research comes from Beauchamp’s work <cite class="ltx_cite ltx_citemacro_citep">(Beauchamp, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib13" title="">2010</a>)</cite>, which challenges the traditional notion of ‘informed consent.’ Rather than focusing narrowly on the disclosure of information to research subjects, Beauchamp advocates for a more nuanced understanding of ‘voluntary consent’ that includes self-authorization. We call for extending this consideration beyond our direct interactions with research subjects to the individuals whose data is used to produce these models, particularly in terms of how their autonomy is respected–or potentially subverted. For example, researchers using LLMs to simulate participants might expose sensitive information from online communities, such as discussions on Reddit about particular worker resistance tactics that would have remained largely unknown otherwise. These communities, whose members might not have intended to share their experiences with researchers, could find their information under a microscope without their explicit consent.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>What does it mean to simulate the user in HCI?</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">One might contend that HCI’s primary goal is to understand how people interact with technology and design systems that meet their needs <cite class="ltx_cite ltx_citemacro_citep">(Olson and Kellogg, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib84" title="">2014</a>)</cite>. Any attempts to use simulations, such as user personas or LLMs, to represent communities’ interests must then be critically examined. Design Justice, as articulated by scholars like Sasha Costanza-Chock <cite class="ltx_cite ltx_citemacro_citep">(Costanza-Chock, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib31" title="">2020</a>)</cite>, emphasizes the full inclusion of people with direct lived experience throughout the design process. Chock raises the fundamental question: “For whom do we design technology?” If our systems are designed to work only for the ‘unmarked’ group (those who are White, male, heterosexual, able-bodied, and literate), since simulations tend to reflect dominant perspectives, we risk reinforcing the spiral of exclusion.
A clear example of this is disability simulation, where designers attempt to ‘step into the shoes’ of disabled individuals. Disability scholars have long critiqued this approach for producing unrealistic and reductive understandings of life with a disability <cite class="ltx_cite ltx_citemacro_citep">(Silverman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib99" title="">2015</a>; Bennett and Rosner, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib15" title="">2019</a>)</cite>. Instead, HCI should be guided by the principle of ‘nothing about us without us,’ central to many social justice movements <cite class="ltx_cite ltx_citemacro_citep">(Spiel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib103" title="">2020</a>)</cite>. This principle goes beyond token participation: it requires deep engagement with, accountability towards, and ownership by those impacted by the systems we introduce.</p>
</div>
<div class="ltx_para" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1">Our participants were acutely aware of the misalignment between these principles and the use of large language models as surrogate participants. They suggested, instead, potential scenarios where using LLMs within the research process might be more appropriate than using them to substitute participation from research communities. This includes using LLMs to trial materials, as a pedagogical device, and in sensitive contexts or unsafe environments. Researchers also went on to articulate concerns with each of these use cases, such as erasure and misrepresentation or misleading qualitative research training. Below, we outline three critical considerations (transparency, model selection, and validation) for researchers who find appeal in the use of LLMs as simulated qualitative research participants. These considerations are not meant to serve as guidance to justify the use of LLMs. On the contrary, we urge careful reflection on how the use of LLMs involves complex methodological choices that significantly impact the integrity of qualitative research.</p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">Firstly, thoughtful model selection is not merely a performance-driven decision but one that directly impacts the validity of the synthetic research data. For example, a model trained predominantly on Western-centric datasets may not accurately simulate experiences from non-Western contexts <cite class="ltx_cite ltx_citemacro_citep">(Khandelwal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib62" title="">2024</a>)</cite>.
Different models undergo different alignment processes depending on the goals and values of the developers. Unless there is strong evidence to believe that data <span class="ltx_text ltx_font_italic" id="S5.SS3.p3.1.1">from</span> a minority group (not about them) is included in the training data, researchers should exercise caution when drawing inferences from simulated interview data. Secondly, researchers must be fully transparent about any use of LLMs in their studies, in line with the ACM policy <cite class="ltx_cite ltx_citemacro_citep">(Board, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib18" title="">2023</a>)</cite>, including any pilot studies with simulated participants.
However, simply disclosing information does not automatically invite critique or enable a better understanding of the research design; instead, visibility often shifts the burden onto individuals. Transparency can create an illusion of accountability without fostering meaningful engagement or critical reflection. Transparency, then, risks becoming <em class="ltx_emph ltx_font_italic" id="S5.SS3.p3.1.2">performative</em>, especially if disclosure can only be selective.</p>
</div>
<div class="ltx_para" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1">Finally, it would be crucial to engage in rigorous validation processes, such as member checking, where findings or interpretations are returned to community members for review and feedback <cite class="ltx_cite ltx_citemacro_citep">(Birt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19430v1#bib.bib16" title="">2016</a>)</cite>. When LLMs are involved, member checking introduces additional layers of complexity. Engaging with simulated data that reflects one’s community or personal experiences demands significant time, energy, and emotional investment. Reading simulated experiences, especially those that may inaccurately represent or trivialize lived experiences, could be distressing for community members. Encouraging community members to critically engage with LLM-generated data and voice disagreements if the synthetic data does not align with their experiences could also prove challenging. Researchers must be prepared to re-evaluate the LLM’s outputs, revise initial interpretations, or even reconsider the appropriateness of using LLMs if the feedback indicates misalignment with the community’s perspectives. It is important to recognize that none of these considerations address the deeper epistemological questions with the use of LLMs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This paper focuses on examining the use of large language models as proxies for research participants. This is no longer a speculative future; research papers and various products are already exploring LLMs’ ability to replace human participants. Through interviews with qualitative researchers, we highlight several significant limitations of using LLMs as stand-ins for qualitative research participants, including the lack of palpability in LLM responses, the ambiguity surrounding the model’s epistemic position, the amplification of researcher positionality, how the use of LLMs forecloses participants’ consent and agency, the erasure of community perspectives, and the risk of undermining qualitative methods of knowing. We conclude by drawing on STS and research ethics literature to discuss how these practices bring to light critical questions about consent, agency, and the legitimacy of using LLMs in qualitative research. The ethical and epistemological tensions demand a cautious and, in many cases, critical stance toward this form of substitution in research that seeks to understand lived experiences.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">ope ([n. d.])</span>
<span class="ltx_bibblock">
[n. d.].

</span>
<span class="ltx_bibblock">OpenAI and journalism — OpenAI.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openai.com/index/openai-and-journalism/" title="">https://openai.com/index/openai-and-journalism/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 09/01/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abid et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Abubakar Abid, Maheen Farooqi, and James Zou. 2021.

</span>
<span class="ltx_bibblock">Persistent anti-muslim bias in large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</em>. 298–306.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agnew et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
William Agnew, A Stevie Bergman, Jennifer Chien, Mark Díaz, Seliem El-Sayed, Jaylen Pittman, Shakir Mohamed, and Kevin R McKee. 2024.

</span>
<span class="ltx_bibblock">The illusion of artificial inclusion. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–12.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aher et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Gati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. 2023.

</span>
<span class="ltx_bibblock">Using large language models to simulate multiple humans and replicate human subject studies. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">International Conference on Machine Learning</em>. PMLR, 337–371.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Argyle et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting, and David Wingate. 2023.

</span>
<span class="ltx_bibblock">Out of one, many: Using language models to simulate human samples.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Political Analysis</em> 31, 3 (2023), 337–351.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Atanasoski and Vora (2019)</span>
<span class="ltx_bibblock">
Neda Atanasoski and Kalindi Vora. 2019.

</span>
<span class="ltx_bibblock">Surrogate humanity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Londres e Durham: Duke University Press. DOI</em> 10 (2019), 9781478004455.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Avis (2005)</span>
<span class="ltx_bibblock">
Mark Avis. 2005.

</span>
<span class="ltx_bibblock">Is there an epistemology for qualitative research.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Qualitative research in health care</em> (2005), 3–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baek et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. 2024.

</span>
<span class="ltx_bibblock">Researchagent: Iterative research idea generation over scientific literature with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">arXiv preprint arXiv:2404.07738</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bai et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al<span class="ltx_text" id="bib.bib10.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Constitutional ai: Harmlessness from ai feedback.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.4.1">arXiv preprint arXiv:2212.08073</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bao et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jianzhu Bao, Rui Wang, Yasheng Wang, Aixin Sun, Yitong Li, Fei Mi, and Ruifeng Xu. 2023.

</span>
<span class="ltx_bibblock">A synthetic data generation framework for grounded dialogues. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>. 10866–10882.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Barocas et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Solon Barocas, Moritz Hardt, and Arvind Narayanan. 2018.

</span>
<span class="ltx_bibblock">Fairness and Machine Learning Limitations and Opportunities.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:113402716" title="">https://api.semanticscholar.org/CorpusID:113402716</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beauchamp (2010)</span>
<span class="ltx_bibblock">
Tom L Beauchamp. 2010.

</span>
<span class="ltx_bibblock">Autonomy and consent.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">The ethics of consent: Theory and practice</em> (2010), 55–78.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bender et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021.

</span>
<span class="ltx_bibblock">On the dangers of stochastic parrots: Can language models be too big?. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Proceedings of the 2021 ACM conference on fairness, accountability, and transparency</em>. 610–623.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bennett and Rosner (2019)</span>
<span class="ltx_bibblock">
Cynthia L Bennett and Daniela K Rosner. 2019.

</span>
<span class="ltx_bibblock">The promise of empathy: Design, disability, and knowing the” other”. In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Proceedings of the 2019 CHI conference on human factors in computing systems</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Birt et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Linda Birt, Suzanne Scott, Debbie Cavers, Christine Campbell, and Fiona Walter. 2016.

</span>
<span class="ltx_bibblock">Member checking: a tool to enhance trustworthiness or merely a nod to validation?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Qualitative health research</em> 26, 13 (2016), 1802–1811.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Blodgett et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Su Lin Blodgett, Solon Barocas, Hal Daum’e, and Hanna M. Wallach. 2020.

</span>
<span class="ltx_bibblock">Language (Technology) is Power: A Critical Survey of “Bias” in NLP.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.3.1">ArXiv</em> abs/2005.14050 (2020).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:218971825" title="">https://api.semanticscholar.org/CorpusID:218971825</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Board (2023)</span>
<span class="ltx_bibblock">
ACM Publications Board. 2023.

</span>
<span class="ltx_bibblock">ACM Policy on Authorship.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.acm.org/publications/policies/new-acm-policy-on-authorship" title="">https://www.acm.org/publications/policies/new-acm-policy-on-authorship</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 09/08/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Boiko et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Daniil A Boiko, Robert MacKnight, and Gabe Gomes. 2023.

</span>
<span class="ltx_bibblock">Emergent autonomous scientific research capabilities of large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">arXiv preprint arXiv:2304.05332</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bolukbasi et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Tolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016.

</span>
<span class="ltx_bibblock">Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings. In <em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">Advances in Neural Information Processing Systems</em>, D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (Eds.), Vol. 29. Curran Associates, Inc.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bordia and Bowman (2019)</span>
<span class="ltx_bibblock">
Shikha Bordia and Samuel R Bowman. 2019.

</span>
<span class="ltx_bibblock">Identifying and reducing gender bias in word-level language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:1904.03035</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braun and Clarke (2012)</span>
<span class="ltx_bibblock">
Virginia Braun and Victoria Clarke. 2012.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Thematic analysis.</em>
</span>
<span class="ltx_bibblock">American Psychological Association.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Braun and Clarke (2019)</span>
<span class="ltx_bibblock">
Virginia Braun and Victoria Clarke. 2019.

</span>
<span class="ltx_bibblock">Reflecting on reflexive thematic analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Qualitative research in sport, exercise and health</em> 11, 4 (2019), 589–597.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Byrd (2023)</span>
<span class="ltx_bibblock">
Antonio Byrd. 2023.

</span>
<span class="ltx_bibblock">Truth-Telling: Critical Inquiries on LLMs and the Corpus Texts That Train Them.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Composition studies</em> 51, 1 (2023), 135–142.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Byun et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Courtni Byun, Piper Vasicek, and Kevin Seppi. 2023.

</span>
<span class="ltx_bibblock">Dispensing with humans in human-computer interaction research. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–26.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caliskan et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan. 2017.

</span>
<span class="ltx_bibblock">Semantics derived automatically from language corpora contain human-like biases.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Science</em> 356, 6334 (2017), 183–186.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1126/science.aal4230" title="">https://doi.org/10.1126/science.aal4230</a>
arXiv:https://www.science.org/doi/pdf/10.1126/science.aal4230

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023.

</span>
<span class="ltx_bibblock">Chateval: Towards better llm-based evaluators through multi-agent debate.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">arXiv preprint arXiv:2308.07201</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xin Chan, Xiaoyang Wang, Dian Yu, Haitao Mi, and Dong Yu. 2024.

</span>
<span class="ltx_bibblock">Scaling Synthetic Data Creation with 1,000,000,000 Personas.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">arXiv preprint arXiv:2406.20094</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chaszczewicz et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Alicja Chaszczewicz, Raj Sanjay Shah, Ryan Louie, Bruce A Arnow, Robert Kraut, and Diyi Yang. 2024.

</span>
<span class="ltx_bibblock">Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">arXiv preprint arXiv:2403.15482</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang and Lee (2023)</span>
<span class="ltx_bibblock">
Cheng-Han Chiang and Hung-yi Lee. 2023.

</span>
<span class="ltx_bibblock">Can large language models be an alternative to human evaluations?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2305.01937</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Costanza-Chock (2020)</span>
<span class="ltx_bibblock">
Sasha Costanza-Chock. 2020.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Design justice: Community-led practices to build the worlds we need</em>.

</span>
<span class="ltx_bibblock">The MIT Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crabtree (2024)</span>
<span class="ltx_bibblock">
Andy Crabtree. 2024.

</span>
<span class="ltx_bibblock">H is for Human and How (Not) To Evaluate Qualitative Research in HCI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">arXiv preprint arXiv:2409.01302</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">David (2024)</span>
<span class="ltx_bibblock">
Emilia David. 2024.

</span>
<span class="ltx_bibblock">OpenAI strikes Reddit deal to train its AI on your posts - The Verge.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.theverge.com/2024/5/16/24158529/reddit-openai-chatgpt-api-access-advertising" title="">https://www.theverge.com/2024/5/16/24158529/reddit-openai-chatgpt-api-access-advertising</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 09/01/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Demszky et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dorottya Demszky, Diyi Yang, David S Yeager, Christopher J Bryan, Margarett Clapper, Susannah Chandhok, Johannes C Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, et al<span class="ltx_text" id="bib.bib34.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Using large language models in psychology.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.4.1">Nature Reviews Psychology</em> 2, 11 (2023), 688–701.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deng et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2024.

</span>
<span class="ltx_bibblock">Mind2web: Towards a generalist agent for the web.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubois et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy S Liang, and Tatsunori B Hashimoto. 2023.

</span>
<span class="ltx_bibblock">AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. In <em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Advances in Neural Information Processing Systems</em>, A. Oh, T. Neumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine (Eds.), Vol. 36. Curran Associates, Inc., 30039–30069.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://proceedings.neurips.cc/paper_files/paper/2023/file/5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf" title="">https://proceedings.neurips.cc/paper_files/paper/2023/file/5fc47800ee5b30b8777fdd30abcaaf3b-Paper-Conference.pdf</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Feuston and Brubaker (2021)</span>
<span class="ltx_bibblock">
Jessica L Feuston and Jed R Brubaker. 2021.

</span>
<span class="ltx_bibblock">Putting tools in their place: The role of time and perspective in human-AI collaboration for qualitative analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the ACM on Human-Computer Interaction</em> 5, CSCW2 (2021), 1–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fiesler and Proferes (2018)</span>
<span class="ltx_bibblock">
Casey Fiesler and Nicholas Proferes. 2018.

</span>
<span class="ltx_bibblock">“Participant” perceptions of Twitter research ethics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Social Media+ Society</em> 4, 1 (2018), 2056305118763366.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fricker (2007)</span>
<span class="ltx_bibblock">
Miranda Fricker. 2007.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Epistemic injustice: Power and the ethics of knowing</em>.

</span>
<span class="ltx_bibblock">Oxford University Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frisch and Giulianelli (2024)</span>
<span class="ltx_bibblock">
Ivar Frisch and Mario Giulianelli. 2024.

</span>
<span class="ltx_bibblock">LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">arXiv preprint arXiv:2402.02896</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fussell (2019)</span>
<span class="ltx_bibblock">
Sidney Fussell. 2019.

</span>
<span class="ltx_bibblock">You no longer own your face.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">The Atlantic</em> (2019).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jie Gao, Yuchen Guo, Gionnieve Lim, Tianqin Zhang, Zheng Zhang, Toby Jia-Jun Li, and Simon Tangi Perrault. 2024.

</span>
<span class="ltx_bibblock">CollabCoder: a lower-barrier, rigorous workflow for inductive collaborative qualitative analysis with large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–29.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Geertz (2008)</span>
<span class="ltx_bibblock">
Clifford Geertz. 2008.

</span>
<span class="ltx_bibblock">Thick description: Toward an interpretive theory of culture.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">The cultural geography reader</em>. Routledge, 41–51.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023a.

</span>
<span class="ltx_bibblock">ChatGPT outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the National Academy of Sciences</em> 120, 30 (July 2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1073/pnas.2305016120" title="">https://doi.org/10.1073/pnas.2305016120</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gilardi et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. 2023b.

</span>
<span class="ltx_bibblock">ChatGPT outperforms crowd workers for text-annotation tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of the National Academy of Sciences</em> 120, 30 (2023), e2305016120.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Guo et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
T Guo, X Chen, Y Wang, R Chang, S Pei, NV Chawla, O Wiest, and X Zhang. 2024.

</span>
<span class="ltx_bibblock">Large Language Model based Multi-Agents: A Survey of Progress and Challenges.. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">33rd International Joint Conference on Artificial Intelligence (IJCAI 2024)</em>. IJCAI; Cornell arxiv.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gupta et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shashank Gupta, Vaishnavi Shrivastava, A. Deshpande, A. Kalyan, Peter Clark, Ashish Sabharwal, and Tushar Khot. 2023.

</span>
<span class="ltx_bibblock">Bias Runs Deep: Implicit Reasoning Biases in Persona-Assigned LLMs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">ArXiv</em> abs/2311.04892 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:265050702" title="">https://api.semanticscholar.org/CorpusID:265050702</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Halterman and Keith (2024)</span>
<span class="ltx_bibblock">
Andrew Halterman and Katherine A Keith. 2024.

</span>
<span class="ltx_bibblock">Codebook LLMs: Adapting Political Science Codebooks for LLM Use and Adapting LLMs to Follow Codebooks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">arXiv preprint arXiv:2407.10747</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hämäläinen et al<span class="ltx_text" id="bib.bib49.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023.

</span>
<span class="ltx_bibblock">Evaluating Large Language Models in Generating Synthetic HCI Research Data: a Case Study. In <em class="ltx_emph ltx_font_italic" id="bib.bib49.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> (¡conf-loc¿, ¡city¿Hamburg¡/city¿, ¡country¿Germany¡/country¿, ¡/conf-loc¿) <em class="ltx_emph ltx_font_italic" id="bib.bib49.4.2">(CHI ’23)</em>. Association for Computing Machinery, New York, NY, USA, Article 433, 19 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3544548.3580688" title="">https://doi.org/10.1145/3544548.3580688</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hämäläinen et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Perttu Hämäläinen, Mikke Tavast, and Anton Kunnari. 2023.

</span>
<span class="ltx_bibblock">Evaluating large language models in generating synthetic hci research data: a case study. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em>. 1–19.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Haraway (2013)</span>
<span class="ltx_bibblock">
Donna Haraway. 2013.

</span>
<span class="ltx_bibblock">Situated knowledges: The science question in feminism and the privilege of partial perspective 1.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Women, science, and technology</em>. Routledge, 455–472.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasan et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Masum Hasan, Cengiz Ozel, Sammy Potter, and Ehsan Hoque. 2023.

</span>
<span class="ltx_bibblock">SAPIEN: affective virtual agents powered by large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib52.3.1">2023 11th International Conference on Affective Computing and Intelligent Interaction Workshops and Demos (ACIIW)</em>. IEEE, 1–3.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horton (2023)</span>
<span class="ltx_bibblock">
John J. Horton. 2023.

</span>
<span class="ltx_bibblock">Large Language Models as Simulated Economic Agents: What Can We Learn from Homo Silicus?

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2301.07543 [econ.GN]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jinpeng Hu, Tengteng Dong, Hui Ma, Peng Zou, Xiao Sun, and Meng Wang. 2024.

</span>
<span class="ltx_bibblock">PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">arXiv preprint arXiv:2407.05721</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Inc (2024)</span>
<span class="ltx_bibblock">
Synthetic Users Inc. 2024.

</span>
<span class="ltx_bibblock">Synthetic Users: user research without the headaches.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://web.archive.org/web/20240822224158/https://www.syntheticusers.com/" title="">https://web.archive.org/web/20240822224158/https://www.syntheticusers.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 08/30/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">International (2024)</span>
<span class="ltx_bibblock">
OpinioAI International. 2024.

</span>
<span class="ltx_bibblock">AI Powered Research — OpinioAI.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.opinio.ai/" title="">https://www.opinio.ai/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 08/30/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jasanoff (2004)</span>
<span class="ltx_bibblock">
Sheila Jasanoff. 2004.

</span>
<span class="ltx_bibblock">States of Knowledge: The Co-production of Science and the Social Order.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:142900983" title="">https://api.semanticscholar.org/CorpusID:142900983</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang and Ferrara (2023)</span>
<span class="ltx_bibblock">
Julie Jiang and Emilio Ferrara. 2023.

</span>
<span class="ltx_bibblock">Social-LLM: Modeling User Behavior at Scale using Language Models and Social Network Data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">arXiv preprint arXiv:2401.00893</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">K (2023)</span>
<span class="ltx_bibblock">
Nikola K. 2023.

</span>
<span class="ltx_bibblock">AI Powered Research — OpinioAI.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://web.archive.org/web/20230914170909/https://opinio.ai/" title="">https://web.archive.org/web/20230914170909/https://opinio.ai/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 03/01/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kapania et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Shivani Kapania, Ruiyi Wang, Toby Jia-Jun Li, Tianshi Li, and Hong Shen. 2024.

</span>
<span class="ltx_bibblock">” I’m categorizing LLM as a productivity tool”: Examining ethics of LLM use in HCI research practices.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.3.1">arXiv preprint arXiv:2403.19876</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ke et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Luoma Ke, Song Tong, Peng Chen, and Kaiping Peng. 2024.

</span>
<span class="ltx_bibblock">Exploring the frontiers of llms in psychological applications: A comprehensive review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">arXiv preprint arXiv:2401.01519</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khandelwal et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Khyati Khandelwal, Manuel Tonneau, Andrew M Bean, Hannah Rose Kirk, and Scott A Hale. 2024.

</span>
<span class="ltx_bibblock">Indian-BhED: A Dataset for Measuring India-Centric Biases in Large Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Proceedings of the 2024 International Conference on Information Technology for Social Good</em>. 231–239.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Geunwoo Kim, Pierre Baldi, and Stephen McAleer. 2024.

</span>
<span class="ltx_bibblock">Language models can solve computer tasks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">Advances in Neural Information Processing Systems</em> 36 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim and Lee (2023)</span>
<span class="ltx_bibblock">
Junsol Kim and Byungkyu Lee. 2023.

</span>
<span class="ltx_bibblock">AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2305.09620 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Klassen and Fiesler (2022)</span>
<span class="ltx_bibblock">
Shamika Klassen and Casey Fiesler. 2022.

</span>
<span class="ltx_bibblock">“This isn’t your data, friend”: Black Twitter as a case study on research ethics for public data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Social Media+ Society</em> 8, 4 (2022), 20563051221144317.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kuzman et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Taja Kuzman, Igor Mozetič, and Nikola Ljubešić. 2023.

</span>
<span class="ltx_bibblock">ChatGPT: Beginning of an End of Manual Linguistic Data Annotation? Use Case of Automatic Genre Identification.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2303.03953 [cs.CL]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lam et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Michelle S Lam, Janice Teoh, James A Landay, Jeffrey Heer, and Michael S Bernstein. 2024.

</span>
<span class="ltx_bibblock">Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM. In <em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–28.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Latif et al<span class="ltx_text" id="bib.bib68.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Ehsan Latif, Luyang Fang, Ping Ma, and Xiaoming Zhai. 2023.

</span>
<span class="ltx_bibblock">Knowledge distillation of llm for education.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib68.3.1">arXiv preprint arXiv:2312.15842</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Law (2004)</span>
<span class="ltx_bibblock">
J. Law. 2004.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">After Method: Mess in Social Science Research</em>.

</span>
<span class="ltx_bibblock">Routledge.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://books.google.com/books?id=E20X7N0nBfQC" title="">https://books.google.com/books?id=E20X7N0nBfQC</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Le Dantec and Fox (2015)</span>
<span class="ltx_bibblock">
Christopher A. Le Dantec and Sarah Fox. 2015.

</span>
<span class="ltx_bibblock">Strangers at the Gate: Gaining Access, Building Rapport, and Co-Constructing Community-Based Research. In <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &amp; Social Computing</em> (Vancouver, BC, Canada) <em class="ltx_emph ltx_font_italic" id="bib.bib70.2.2">(CSCW ’15)</em>. Association for Computing Machinery, New York, NY, USA, 1348–1358.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/2675133.2675147" title="">https://doi.org/10.1145/2675133.2675147</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Levin (2017)</span>
<span class="ltx_bibblock">
Sam Levin. 2017.

</span>
<span class="ltx_bibblock">New AI can guess whether you’re gay or straight from a photograph.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">The Guardian</em> 8 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib72.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tao Li, Daniel Khashabi, Tushar Khot, Ashish Sabharwal, and Vivek Srikumar. 2020.

</span>
<span class="ltx_bibblock">UNQOVERing Stereotyping Biases via Underspecified Questions. In <em class="ltx_emph ltx_font_italic" id="bib.bib72.3.1">Findings of the Association for Computational Linguistics: EMNLP 2020</em>, Trevor Cohn, Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, Online, 3475–3489.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.18653/v1/2020.findings-emnlp.311" title="">https://doi.org/10.18653/v1/2020.findings-emnlp.311</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liang et al<span class="ltx_text" id="bib.bib73.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Paul Pu Liang, Chiyu Wu, Louis-Philippe Morency, and Ruslan Salakhutdinov. 2021.

</span>
<span class="ltx_bibblock">Towards understanding and mitigating social biases in language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib73.3.1">International Conference on Machine Learning</em>. PMLR, 6565–6576.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu and Shi (2024)</span>
<span class="ltx_bibblock">
Menglin Liu and Ge Shi. 2024.

</span>
<span class="ltx_bibblock">PoliPrompt: A High-Performance Cost-Effective LLM-Based Text Classification Framework for Political Science.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">arXiv preprint arXiv:2409.01466</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib75.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yiren Liu, Si Chen, Haocong Cheng, Mengxia Yu, Xiao Ran, Andrew Mo, Yiliu Tang, and Yun Huang. 2024.

</span>
<span class="ltx_bibblock">How ai processing delays foster creativity: Exploring research question co-creation with an llm-based agent. In <em class="ltx_emph ltx_font_italic" id="bib.bib75.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Louie et al<span class="ltx_text" id="bib.bib76.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Ryan Louie, Ananjan Nandi, William Fang, Cheng Chang, Emma Brunskill, and Diyi Yang. 2024.

</span>
<span class="ltx_bibblock">Roleplay-doh: Enabling Domain-Experts to Create LLM-simulated Patients via Eliciting and Adhering to Principles.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib76.3.1">arXiv preprint arXiv:2407.00870</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">M. Bran et al<span class="ltx_text" id="bib.bib77.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Andres M. Bran, Sam Cox, Oliver Schilter, Carlo Baldassari, Andrew D White, and Philippe Schwaller. 2024.

</span>
<span class="ltx_bibblock">Augmenting large language models with chemistry tools.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib77.3.1">Nature Machine Intelligence</em> (2024), 1–11.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al<span class="ltx_text" id="bib.bib78.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Kevin Ma, Daniele Grandi, Christopher McComb, and Kosa Goucher-Lambert. 2023.

</span>
<span class="ltx_bibblock">Conceptual design generation using large language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib78.3.1">International Design Engineering Technical Conferences and Computers and Information in Engineering Conference</em>, Vol. 87349. American Society of Mechanical Engineers, V006T06A021.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McDonald et al<span class="ltx_text" id="bib.bib79.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Nora McDonald, Sarita Schoenebeck, and Andrea Forte. 2019.

</span>
<span class="ltx_bibblock">Reliability and inter-rater reliability in qualitative research: Norms and guidelines for CSCW and HCI practice.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib79.3.1">Proceedings of the ACM on human-computer interaction</em> 3, CSCW (2019), 1–23.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Miceli and Posada (2022)</span>
<span class="ltx_bibblock">
Milagros Miceli and Julian Posada. 2022.

</span>
<span class="ltx_bibblock">The Data-Production Dispositif.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Proceedings of the ACM on Human-Computer Interaction</em> 6 (2022), 1 – 37.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:249017734" title="">https://api.semanticscholar.org/CorpusID:249017734</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nadeem et al<span class="ltx_text" id="bib.bib81.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Moin Nadeem, Anna Bethke, and Siva Reddy. 2020.

</span>
<span class="ltx_bibblock">StereoSet: Measuring stereotypical bias in pretrained language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib81.3.1">arXiv preprint arXiv:2004.09456</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nagy and Neff (2024)</span>
<span class="ltx_bibblock">
Peter Nagy and Gina Neff. 2024.

</span>
<span class="ltx_bibblock">Conjuring algorithms: Understanding the tech industry as stage magicians.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">New Media &amp; Society</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:272284979" title="">https://api.semanticscholar.org/CorpusID:272284979</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olive et al<span class="ltx_text" id="bib.bib83.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
James L Olive et al<span class="ltx_text" id="bib.bib83.3.1">.</span> 2014.

</span>
<span class="ltx_bibblock">Reflecting on the tensions between emic and etic perspectives in life history research: Lessons learned. In <em class="ltx_emph ltx_font_italic" id="bib.bib83.4.1">Forum qualitative sozialforschung/forum: qualitative social research</em>, Vol. 15.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Olson and Kellogg (2014)</span>
<span class="ltx_bibblock">
Judith S Olson and Wendy A Kellogg. 2014.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">Ways of Knowing in HCI</em>. Vol. 2.

</span>
<span class="ltx_bibblock">Springer.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span class="ltx_text" id="bib.bib85.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Joon Sung Park, Joseph O’Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023.

</span>
<span class="ltx_bibblock">Generative agents: Interactive simulacra of human behavior. In <em class="ltx_emph ltx_font_italic" id="bib.bib85.3.1">Proceedings of the 36th annual acm symposium on User Interface Software and Technology</em>. 1–22.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Park et al<span class="ltx_text" id="bib.bib86.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Joon Sung Park, Lindsay Popowski, Carrie Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2022.

</span>
<span class="ltx_bibblock">Social Simulacra: Creating Populated Prototypes for Social Computing Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib86.3.1">Proceedings of the 35th Annual ACM Symposium on User Interface Software and Technology</em> (Bend, OR, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib86.4.2">(UIST ’22)</em>. Association for Computing Machinery, New York, NY, USA, Article 74, 18 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3526113.3545616" title="">https://doi.org/10.1145/3526113.3545616</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Perez et al<span class="ltx_text" id="bib.bib87.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ethan Perez, Sam Ringer, Kamilė Lukošiūtė, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson, Sandipan Kundu, Saurav Kadavath, et al<span class="ltx_text" id="bib.bib87.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Discovering language model behaviors with model-written evaluations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib87.4.1">arXiv preprint arXiv:2212.09251</em> (2022).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al<span class="ltx_text" id="bib.bib88.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020.

</span>
<span class="ltx_bibblock">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib88.3.1">Journal of Machine Learning Research</em> 21, 140 (2020), 1–67.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="http://jmlr.org/papers/v21/20-074.html" title="">http://jmlr.org/papers/v21/20-074.html</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ravi et al<span class="ltx_text" id="bib.bib89.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Akshay Ravi, Aaron Neinstein, and Sara G Murray. 2023.

</span>
<span class="ltx_bibblock">Large language models and medical education: Preparing for a rapid transformation in how trainees will learn to be doctors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib89.3.1">ATS scholar</em> 4, 3 (2023), 282–292.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Roth (2024)</span>
<span class="ltx_bibblock">
Emma Roth. 2024.

</span>
<span class="ltx_bibblock">Google cut a deal with Reddit for AI training data - The Verge.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.theverge.com/2024/2/22/24080165/google-reddit-ai-training-data" title="">https://www.theverge.com/2024/2/22/24080165/google-reddit-ai-training-data</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 09/01/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sambasivan et al<span class="ltx_text" id="bib.bib91.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen K. Paritosh, and Lora Aroyo. 2021.

</span>
<span class="ltx_bibblock">“Everyone wants to do the model work, not the data work”: Data Cascades in High-Stakes AI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib91.3.1">Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems</em> (2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:231829607" title="">https://api.semanticscholar.org/CorpusID:231829607</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santurkar et al<span class="ltx_text" id="bib.bib92.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Shibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee, Percy Liang, and Tatsunori Hashimoto. 2023.

</span>
<span class="ltx_bibblock">Whose opinions do language models reflect?. In <em class="ltx_emph ltx_font_italic" id="bib.bib92.3.1">International Conference on Machine Learning</em>. PMLR, 29971–30004.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Santy et al<span class="ltx_text" id="bib.bib93.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. 2023.

</span>
<span class="ltx_bibblock">NLPositionality: Characterizing design biases of datasets and models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib93.3.1">arXiv preprint arXiv:2306.01943</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Savelka et al<span class="ltx_text" id="bib.bib94.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jaromir Savelka, Kevin D Ashley, Morgan A Gray, Hannes Westermann, and Huihui Xu. 2023.

</span>
<span class="ltx_bibblock">Can gpt-4 support analysis of textual data in tasks requiring highly specialized domain expertise?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib94.3.1">arXiv preprint arXiv:2306.13906</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schwandt (1994)</span>
<span class="ltx_bibblock">
TA Schwandt. 1994.

</span>
<span class="ltx_bibblock">Constructivist, interpretivist approaches to human inquiry.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Handbook of qualitative research/Sage</em> (1994).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sclar et al<span class="ltx_text" id="bib.bib96.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. 2023.

</span>
<span class="ltx_bibblock">Quantifying Language Models’ Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about prompt formatting.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib96.3.1">arXiv preprint arXiv:2310.11324</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shaer et al<span class="ltx_text" id="bib.bib97.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Orit Shaer, Angelora Cooper, Osnat Mokryn, Andrew L Kun, and Hagit Ben Shoshan. 2024.

</span>
<span class="ltx_bibblock">AI-Augmented Brainwriting: Investigating the use of LLMs in group ideation. In <em class="ltx_emph ltx_font_italic" id="bib.bib97.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–17.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shelby et al<span class="ltx_text" id="bib.bib98.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Renee Marie Shelby, Shalaleh Rismani, Kathryn Henne, AJung Moon, Negar Rostamzadeh, Paul Nicholas, N’Mah Yilla-Akbari, Jess Gallegos, Andrew Smart, Emilio Garcia, and Gurleen Virk. 2022.

</span>
<span class="ltx_bibblock">Sociotechnical Harms of Algorithmic Systems: Scoping a Taxonomy for Harm Reduction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib98.3.1">Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society</em> (2022).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:256697294" title="">https://api.semanticscholar.org/CorpusID:256697294</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silverman et al<span class="ltx_text" id="bib.bib99.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Arielle M Silverman, Jason D Gwinn, and Leaf Van Boven. 2015.

</span>
<span class="ltx_bibblock">Stumbling in their shoes: Disability simulations reduce judged capabilities of disabled people.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib99.3.1">Social Psychological and Personality Science</em> 6, 4 (2015), 464–471.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Silverman (2004)</span>
<span class="ltx_bibblock">
David Silverman. 2004.

</span>
<span class="ltx_bibblock">Qualitative research : theory, method and practice.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:142594195" title="">https://api.semanticscholar.org/CorpusID:142594195</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Small and Calarco (2022)</span>
<span class="ltx_bibblock">
Mario Luis Small and Jessica McCrory Calarco. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">Qualitative literacy: A guide to evaluating ethnographic and interview research</em>.

</span>
<span class="ltx_bibblock">Univ of California Press.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Soden et al<span class="ltx_text" id="bib.bib102.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Robert Soden, Austin Toombs, and Michaelanne Thomas. 2024.

</span>
<span class="ltx_bibblock">Evaluating interpretive research in HCI.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib102.3.1">Interactions</em> 31, 1 (2024), 38–42.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Spiel et al<span class="ltx_text" id="bib.bib103.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Katta Spiel, Kathrin Gerling, Cynthia L Bennett, Emeline Brulé, Rua M Williams, Jennifer Rode, and Jennifer Mankoff. 2020.

</span>
<span class="ltx_bibblock">Nothing about us without us: Investigating the role of critical disability studies in HCI. In <em class="ltx_emph ltx_font_italic" id="bib.bib103.3.1">Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</em>. 1–8.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taylor (2011)</span>
<span class="ltx_bibblock">
Alex S. Taylor. 2011.

</span>
<span class="ltx_bibblock">Out there. In <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> (Vancouver, BC, Canada) <em class="ltx_emph ltx_font_italic" id="bib.bib104.2.2">(CHI ’11)</em>. Association for Computing Machinery, New York, NY, USA, 685–694.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1978942.1979042" title="">https://doi.org/10.1145/1978942.1979042</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thapa et al<span class="ltx_text" id="bib.bib105.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Surendrabikram Thapa, Usman Naseem, and Mehwish Nasim. 2023.

</span>
<span class="ltx_bibblock">From humans to machines: can chatgpt-like llms effectively replace human annotators in nlp tasks. In <em class="ltx_emph ltx_font_italic" id="bib.bib105.3.1">Workshop Proceedings of the 17th International AAAI Conference on Web and Social Media</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tjuatja et al<span class="ltx_text" id="bib.bib106.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, and Graham Neubig. 2023.

</span>
<span class="ltx_bibblock">Do LLMs exhibit human-like response biases? A case study in survey design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib106.3.1">arXiv preprint arXiv:2311.04076</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Users (2023)</span>
<span class="ltx_bibblock">
Synthetic Users. 2023.

</span>
<span class="ltx_bibblock">Synthetic Users: user research without the headaches.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.syntheticusers.com/" title="">https://www.syntheticusers.com/</a>.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">(Accessed on 03/01/2024).

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Maanen (2011)</span>
<span class="ltx_bibblock">
J. Van Maanen. 2011.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Tales of the Field: On Writing Ethnography, Second Edition</em>.

</span>
<span class="ltx_bibblock">University of Chicago Press.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://books.google.com/books?id=V9hi269OD9cC" title="">https://books.google.com/books?id=V9hi269OD9cC</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Venkit et al<span class="ltx_text" id="bib.bib109.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Pranav Narayanan Venkit, Sanjana Gautam, Ruchi Panchanadikar, Ting-Hao’Kenneth’ Huang, and Shomir Wilson. 2023.

</span>
<span class="ltx_bibblock">Nationality bias in text generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib109.3.1">arXiv preprint arXiv:2302.02463</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib110.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Angelina Wang, Jamie Morgenstern, and John P Dickerson. 2024a.

</span>
<span class="ltx_bibblock">Large language models cannot replace human participants because they cannot portray identity groups.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib110.3.1">arXiv preprint arXiv:2402.01908</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib111.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Haochun Wang, Sendong Zhao, Zewen Qiang, Nuwa Xi, Bing Qin, and Ting Liu. 2024b.

</span>
<span class="ltx_bibblock">Beyond Direct Diagnosis: LLM-based Multi-Specialist Agent Consultation for Automatic Diagnosis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib111.3.1">arXiv preprint arXiv:2401.16107</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Weidinger et al<span class="ltx_text" id="bib.bib112.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Griffin, Po-Sen Huang, John Mellor, Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al<span class="ltx_text" id="bib.bib112.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Taxonomy of risks posed by language models. In <em class="ltx_emph ltx_font_italic" id="bib.bib112.4.1">Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency</em>. 214–229.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Winner (2017)</span>
<span class="ltx_bibblock">
Langdon Winner. 2017.

</span>
<span class="ltx_bibblock">Do Artifacts Have Politics?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">Emerging Technologies: Ethics, Law and Governance</em> (2017).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:15660866" title="">https://api.semanticscholar.org/CorpusID:15660866</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xi et al<span class="ltx_text" id="bib.bib114.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al<span class="ltx_text" id="bib.bib114.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">The rise and potential of large language model based agents: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib114.4.1">arXiv preprint arXiv:2309.07864</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al<span class="ltx_text" id="bib.bib115.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Yang Xiao, Yi Cheng, Jinlan Fu, Jiashuo Wang, Wenjie Li, and Pengfei Liu. 2023a.

</span>
<span class="ltx_bibblock">How far are we from believable AI agents? A framework for evaluating the believability of human behavior simulation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib115.3.1">arXiv preprint arXiv:2312.17115</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xiao et al<span class="ltx_text" id="bib.bib116.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Ziang Xiao, Xingdi Yuan, Q Vera Liao, Rania Abdelghani, and Pierre-Yves Oudeyer. 2023b.

</span>
<span class="ltx_bibblock">Supporting qualitative analysis with large language models: Combining codebook with GPT-3 for deductive coding. In <em class="ltx_emph ltx_font_italic" id="bib.bib116.3.1">Companion proceedings of the 28th international conference on intelligent user interfaces</em>. 75–78.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib117.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Qisen Yang, Zekun Wang, Honghui Chen, Shenzhi Wang, Yifan Pu, Xin Gao, Wenhao Huang, Shiji Song, and Gao Huang. 2024.

</span>
<span class="ltx_bibblock">Llm agents for psychology: A study on gamified assessments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib117.3.1">arXiv preprint arXiv:2402.12326</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib118.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
He Zhang, Chuhao Wu, Jingyi Xie, Fiona Rubino, Sydney Graver, ChanMin Kim, John M Carroll, and Jie Cai. 2024a.

</span>
<span class="ltx_bibblock">When Qualitative Research Meets Large Language Model: Exploring the Potential of QualiGPT as a Tool for Qualitative Coding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib118.3.1">arXiv preprint arXiv:2407.14925</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib119.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zheng Zhang, Jie Gao, Ranjodh Singh Dhaliwal, and Toby Jia-Jun Li. 2023.

</span>
<span class="ltx_bibblock">Visar: A human-ai argumentative writing assistant with visual programming and rapid draft prototyping. In <em class="ltx_emph ltx_font_italic" id="bib.bib119.3.1">Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology</em>. 1–30.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib120.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Zheyuan Zhang, Daniel Zhang-Li, Jifan Yu, Linlu Gong, Jinchang Zhou, Zhiyuan Liu, Lei Hou, and Juanzi Li. 2024b.

</span>
<span class="ltx_bibblock">Simulating classroom education with llm-empowered agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib120.3.1">arXiv preprint arXiv:2406.19226</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib121.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, et al<span class="ltx_text" id="bib.bib121.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Sotopia: Interactive evaluation for social intelligence in language agents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib121.4.1">arXiv preprint arXiv:2310.11667</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ziems et al<span class="ltx_text" id="bib.bib122.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2024.

</span>
<span class="ltx_bibblock">Can large language models transform computational social science?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib122.3.1">Computational Linguistics</em> 50, 1 (2024), 237–291.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zimmer (2020)</span>
<span class="ltx_bibblock">
Michael Zimmer. 2020.

</span>
<span class="ltx_bibblock">“But the data is already public”: on the ethics of research in Facebook.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">The ethics of information technologies</em>. Routledge, 229–241.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 28 18:19:34 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
