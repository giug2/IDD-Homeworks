<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO</title>
<!--Generated on Wed Oct  2 18:17:41 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.16205v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S1" title="In Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S2" title="In Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Materials</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S2.SS1" title="In 2 Materials ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Gleason dataset</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S2.SS2" title="In 2 Materials ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>SICAPv2 dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S3" title="In Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S3.SS1" title="In 3 Methods ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Mamba</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S3.SS2" title="In 3 Methods ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>H-vmunet method (Architecture)</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S3.SS3" title="In 3 Methods ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>SAM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S3.SS4" title="In 3 Methods ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>YOLO</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S4" title="In Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S5" title="In Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S6" title="In Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Ali Badiezadeh<sup class="ltx_sup" id="id11.11.id1"><span class="ltx_text ltx_font_italic" id="id11.11.id1.1">a</span></sup>, Amin Malekmohammadi<sup class="ltx_sup" id="id12.12.id2"><span class="ltx_text ltx_font_italic" id="id12.12.id2.1">b</span></sup>, Seyed Mostafa Mirhassani<sup class="ltx_sup" id="id13.13.id3"><span class="ltx_text ltx_font_italic" id="id13.13.id3.1">c</span></sup>, Parisa Gifani<sup class="ltx_sup" id="id14.14.id4"><span class="ltx_text ltx_font_italic" id="id14.14.id4.1">d</span></sup>, Majid Vafaeezadeh<sup class="ltx_sup" id="id15.15.id5"><span class="ltx_text ltx_font_italic" id="id15.15.id5.1">e,</span></sup>
<br class="ltx_break"/>
<br class="ltx_break"/><sup class="ltx_sup" id="id16.16.id6"><span class="ltx_text ltx_font_italic" id="id16.16.id6.1">a</span></sup> School of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran
<br class="ltx_break"/><sup class="ltx_sup" id="id17.17.id7"><span class="ltx_text ltx_font_italic" id="id17.17.id7.1">b</span></sup> School of Computer Engineering, Iran University of Science and Technology, Tehran, Iran
<br class="ltx_break"/><sup class="ltx_sup" id="id18.18.id8"><span class="ltx_text ltx_font_italic" id="id18.18.id8.1">c</span></sup> School of Electrical Engineering, Shahrood Branch, Islamic Azad University, Shahrood, Iran
<br class="ltx_break"/><sup class="ltx_sup" id="id19.19.id9"><span class="ltx_text ltx_font_italic" id="id19.19.id9.1">d</span></sup> Medical Sciences and Technologies Department, Science and Research Branch, Islamic Azad University, Tehran, Iran
<br class="ltx_break"/><sup class="ltx_sup" id="id20.20.id10"><span class="ltx_text ltx_font_italic" id="id20.20.id10.1">e</span></sup> Biomedical Engineering Department, School of Electrical Engineering, Iran University of Science and Technology, Tehran, Iran
<br class="ltx_break"/>
</span><span class="ltx_author_notes">corresponding author</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id21.id1">Accurate segmentation of prostate cancer histopathology images is crucial for diagnosis and treatment planning. This study presents a comparative analysis of three deep learning-based methods, Mamba, SAM, and YOLO, for segmenting prostate cancer histopathology images. We evaluated the performance of these models on two comprehensive datasets, Gleason 2019 and SICAPv2, using Dice score, precision, and recall metrics. Our results show that the High-order Vision Mamba UNet (H-vmunet) model outperforms the other two models, achieving the highest scores across all metrics on both datasets. The H-vmunet model’s advanced architecture, which integrates high-order visual state spaces and 2D-selective-scan operations, enables efficient and sensitive lesion detection across different scales. Our study demonstrates the potential of the H-vmunet model for clinical applications and highlights the importance of robust validation and comparison of deep learning-based methods for medical image analysis. The findings of this study contribute to the development of accurate and reliable computer-aided diagnosis systems for prostate cancer. The code is available at http://github.com/alibdz/prostate-segmentation.</p>
</div>
<div class="ltx_para ltx_noindent" id="p1">
<p class="ltx_p" id="p1.7"><em class="ltx_emph ltx_font_bold ltx_font_italic" id="p1.7.1">K</em><span class="ltx_text ltx_font_bold" id="p1.7.2">eywords</span> Prostate Cancer  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.1.m1.1"><semantics id="p1.1.m1.1a"><mo id="p1.1.m1.1.1" xref="p1.1.m1.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.1.m1.1b"><ci id="p1.1.m1.1.1.cmml" xref="p1.1.m1.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.1.m1.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.1.m1.1d">⋅</annotation></semantics></math>
Deep Learning  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.2.m2.1"><semantics id="p1.2.m2.1a"><mo id="p1.2.m2.1.1" xref="p1.2.m2.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.2.m2.1b"><ci id="p1.2.m2.1.1.cmml" xref="p1.2.m2.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.2.m2.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.2.m2.1d">⋅</annotation></semantics></math>
Histopathology Images  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.3.m3.1"><semantics id="p1.3.m3.1a"><mo id="p1.3.m3.1.1" xref="p1.3.m3.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.3.m3.1b"><ci id="p1.3.m3.1.1.cmml" xref="p1.3.m3.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.3.m3.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.3.m3.1d">⋅</annotation></semantics></math>
Segmentation  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.4.m4.1"><semantics id="p1.4.m4.1a"><mo id="p1.4.m4.1.1" xref="p1.4.m4.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.4.m4.1b"><ci id="p1.4.m4.1.1.cmml" xref="p1.4.m4.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.4.m4.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.4.m4.1d">⋅</annotation></semantics></math>
Mamba  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.5.m5.1"><semantics id="p1.5.m5.1a"><mo id="p1.5.m5.1.1" xref="p1.5.m5.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.5.m5.1b"><ci id="p1.5.m5.1.1.cmml" xref="p1.5.m5.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.5.m5.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.5.m5.1d">⋅</annotation></semantics></math>
SAM  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.6.m6.1"><semantics id="p1.6.m6.1a"><mo id="p1.6.m6.1.1" xref="p1.6.m6.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.6.m6.1b"><ci id="p1.6.m6.1.1.cmml" xref="p1.6.m6.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.6.m6.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.6.m6.1d">⋅</annotation></semantics></math>
YOLO  <math alttext="\cdot" class="ltx_Math" display="inline" id="p1.7.m7.1"><semantics id="p1.7.m7.1a"><mo id="p1.7.m7.1.1" xref="p1.7.m7.1.1.cmml">⋅</mo><annotation-xml encoding="MathML-Content" id="p1.7.m7.1b"><ci id="p1.7.m7.1.1.cmml" xref="p1.7.m7.1.1">⋅</ci></annotation-xml><annotation encoding="application/x-tex" id="p1.7.m7.1c">\cdot</annotation><annotation encoding="application/x-llamapun" id="p1.7.m7.1d">⋅</annotation></semantics></math>
H-VMUnet</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Prostate cancer, a widespread health concern impacting countless individuals globally, underscores the importance of accurate diagnostic methods to effectively gauge tumor aggressiveness and inform treatment plans. The Gleason scoring (grading) method, which has traditionally been, remains central to assessing the severity of prostate cancer. Nonetheless, this approach is fundamentally subjective and time-intensive, underscoring the necessity for cutting-edge techniques to boost diagnostic precision and efficiency. Recent strides in artificial intelligence (AI) and machine learning technologies present exciting opportunities for streamlining and enhancing the evaluation of histopathological images. Particularly, the utilization of image segmentation techniques within histopathology has become a critical focus of research, striving to distinguish between normal and malignant tissues with unmatched accuracy. 
<br class="ltx_break"/>Segmenting prostate’s histopathology images poses unique challenges that require innovative solutions. One of the primary challenges is the variability and complexity of the tissue structures, which can lead to inconsistencies in segmentation outcomes. This variability is compounded by the fact that prostate’s disease manifests differently across patients, necessitating a highly adaptable segmentation approach. Another significant challenge is the scale and detail of the images. High-resolution histopathology images often contain intricate details that are difficult to capture and analyze accurately. This complexity demands sophisticated algorithms capable of handling the vast amount of data while maintaining precision. 
<br class="ltx_break"/>Moreover, traditional annotation processes involving such images are not only labor-intensive but also susceptible to human error, highlighting the need for automated solutions that can reliably annotate and segment the images. The lack of standardized protocols for Gleason grading further complicates the development of universally applicable segmentation algorithms, as there is a wide range of interpretations and practices among pathologists. 
<br class="ltx_break"/>Lastly, the integration of AI and machine learning, notably those based on neural networks and deep learning models into clinical workflows presents its own set of challenges, including the need for extensive training data, the interpretability of model outputs, and the regulatory approval for clinical use.
<br class="ltx_break"/>Addressing these challenges requires a robust approach, combining advanced deep learning with rigorous validation against clinical standards. By overcoming these hurdles, we can enhance the accuracy and efficiency of prostate’s disease diagnosis, ultimately improving patient outcomes.
In recent years, the deep learning methods of medical image analysis has witnessed significant advancements, particularly in the realm of histopathology image segmentation, grading, and classification. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib1" title="">1</a>]</cite> a two-stage deep learning system was developed for automated Gleason grading, achieving high accuracy with slide-level annotations only. Another study proposed an adaptive scoring mechanism that chooses among various annotations per image, optimizing Gleason grading performance and outperforming the STAPLE algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib2" title="">2</a>]</cite>. 
<br class="ltx_break"/>Deep learning networks have also been used to classify prostate cancer images, with AlexNet showing substantial agreement with pathologists and perfect agreement with the majority vote, while GoogleNet had moderate agreement <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib3" title="">3</a>]</cite>. The Unsupervised Confidence Approximation (UCA) method was proposed to manage training with noisy data and make confident predictions concurrently, showing strong performance gains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib4" title="">4</a>]</cite>. 
<br class="ltx_break"/>In addition, a novel domain adaptation approach was proposed to address class imbalance in the source domain and involve pseudo-labeled target data points in training, achieving competitive performance on public datasets for classifying prostate cancer images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib5" title="">5</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib6" title="">6</a>]</cite> a CNN model was used to segment TMA images into Gleason grade regions, achieving a high mean Dice score.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib7" title="">7</a>]</cite> a review was conducted where investigated 21 digital pathology image segmentation challenges from 2010 to 2022, highlighting the importance of addressing inter-expert disagreement and the evaluation process chosen, as well as the quality control of various challenge elements to ensure accurate and reliable results. In another study, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib8" title="">8</a>]</cite>, a contrastive learning method was used to pretrain a network on a large, diverse dataset of histopathology images, outperforming ImageNet pretraining on classification and regression tasks, and performing comparably for segmentation on smaller networks. The diversity of the pretraining dataset proved to be more important than the number of images. Furthermore, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib9" title="">9</a>]</cite> proposed the Latent Doctor Model (LDM), which is a stochastic classification framework utilizing a CVAE and DeepSet encoder, which can better model the full label distribution and capture annotator variability compared to baselines. The predictive uncertainty of the LDM correlates with inter-observer variability. 
<br class="ltx_break"/>One notable approach is the probabilistic deep learning model proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib10" title="">10</a>]</cite>, referred to as "Pionono". This method models inter- and intra-observer variability using probability distributions in latent space, and demonstrates superior performance over previous state-of-the-art approaches by a large margin. Additionally, Pionono provides probabilistic predictions that allow for uncertainty assessment and simulation of specific expert opinions. Pionono demonstrates improved predictive performance and estimated predictive uncertainty in prostate and breast cancer segmentation tasks, outperforming other models. Its ability to accurately model variability is highlighted as a significant advantage, especially in medical image analysis where subjective interpretations play a critical role. However, the model faces limitations in capturing detailed variations in annotator segmentations, prompting suggestions for increased model complexity to enhance its effectiveness. 
<br class="ltx_break"/>Another approach that has shown promising results is the use of the STAPLE algorithm to synthesize different expert annotations, as described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib11" title="">11</a>]</cite>. This method was validated on the Vancouver Prostate Centre dataset and achieved good results in distinguishing between high-risk and low-risk cancer, as well as in distinguishing benign from malignant. Additionally, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib12" title="">12</a>]</cite>, a UNET model with four different CNN architectures as the encoder was used. After preprocessing the data and using data augmentation, the UNET model with ResNet50 encoder achieved superior outcomes on both data sets. 
<br class="ltx_break"/>In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib13" title="">13</a>]</cite> the impact of consensus methods and core-level scoring rules on the measured agreement between experts and the consensus in the analysis of the Gleason2019 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib14" title="">14</a>]</cite> was investigated. The study found that the choice of these methods affected the measured agreement, which could influence the ranking of algorithms evaluated against the consensus. The authors used various consensus methods and scoring rules, and also employed a leave-one-out approach and MDS visualization to gain more insight into the relationships between experts and the consensus. Additionally, an advanced learning framework named MoMA has been introduced <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib15" title="">15</a>]</cite>. It employs techniques such as knowledge distillation, momentum contrastive learning, and multi-head attention to facilitate the transfer of knowledge from a pre-trained teacher model to a student model that is trained with reduced dataset. The results show that MoMA is accurate and robust in transferring knowledge across varied applications, surpassing comparable techniques, and offering insights into effective learning strategy for a range of computational pathology contexts.
<br class="ltx_break"/>Over the years for segmentation and classification of medical and non-medical image modalities, a range of methods have been developed, primarily utilizing Convolutional Neural Networks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib16" title="">16</a>]</cite> and Vision Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib24" title="">24</a>]</cite>. CNNs are renowned for their effectiveness in local feature extraction, essential for tasks requiring texture and fine detail analysis. However, CNNs struggle with capturing long-range dependencies due to their inherently local receptive field, even with larger convolution kernels. Additionally, processing high-resolution images with CNNs can be computationally expensive, demanding significant hardware resources.
<br class="ltx_break"/>In contrast, Vision Transformers (ViTs) are better in grasping global dependencies across an image, thanks to their self-attention mechanism. ViTs are scalable and can process larger images and more complex patterns effectively. Nonetheless, they require large amounts of memory, posing limitations in resource-constrained environments. The self-attention mechanism also adds to the model’s complexity, making ViTs harder to train and fine-tune.
<br class="ltx_break"/>State-space models (SSMs) have emerged as a promising alternative to both CNNs and ViTs, particularly with the introduction of the 2D-selective-scan (SS2D) method. SSMs efficiently capture long-range dependencies while maintaining linear complexity and facilitating parallel training, significantly reducing the time and computational resources required. However, SSMs’ focus on global receptive fields can introduce redundant information, not always relevant to the target features.
<br class="ltx_break"/>Recent advancements in state-space models, such as the Mamba algorithm, have incorporated time-varying parameters to enhance performance with fewer parameters than traditional Transformers. This led to the development of Vision Mamba (Vim), demonstrating superior hardware-aware design, saving substantial GPU memory while maintaining high performance in high-resolution vision tasks. Despite these advancements, existing models face challenges like excessive memory usage and the integration of redundant information. This highlights the need for models that balance the extraction of local and global features while minimizing computational costs and redundancy.
<br class="ltx_break"/>U-Mamba, a network for biomedical image segmentation that combines convolutional layers and State Space Sequence Models (SSMs) to handle long-range dependencies more effectively than traditional CNNs and Transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib25" title="">25</a>]</cite>.
<br class="ltx_break"/>Swin-UMamba model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib26" title="">26</a>]</cite> is to enhance medical image segmentation by integrating multi-scale information and effectively modeling long-range global dependencies. It achieves this by combining the strengths of Mamba-based models in long sequence modeling with the advantages of ImageNet-based pretraining.
<br class="ltx_break"/>Building on advancements and addressing existing gaps, the High-order Vision Mamba UNet (H-vmunet) was developed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib27" title="">27</a>]</cite>, incorporating the strengths of Spectral-Spatial Models (SSMs), notably the 2D-selective-scan (SS2D) method, and extending it to high-order interactions to reduce redundant information while enhancing the global receptive field. The introduction of the Local-SS2D module significantly improves local feature extraction capabilities, making the model more adept at handling fine details essential in medical images. Furthermore, the H-vmunet model incorporates the High-order Visual State Space (H-VSS) module, enhancing feature extraction through multiple interaction orders. By integrating Channel and Spatial Attention modules in the skip connections, it further improves multi-level and multi-scale information fusion, accelerating convergence and increasing sensitivity to lesions of varying scales. Addressing the limitations of previous methods and introducing innovative modules, the H-vmunet aims to set a new standard in medical image segmentation, offering a more efficient, accurate, and scalable solution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib27" title="">27</a>]</cite>.
<br class="ltx_break"/>Among the deep learning methods, Mamba, SAM, and YOLO stand out for their potential in addressing the challenges of image segmentation in medical diagnostics. Mamba, a deep learning framework, is celebrated for its ability to handle large-scale image segmentation tasks efficiently. SAM, another cutting-edge method, specializes in semantic segmentation, providing detailed insights into the structural components of histopathology images. Meanwhile, YOLO is stands out for its ability to perform object detection accurately in real time, making it highly adaptable for the fast and accurate segmentation of complex structures in images.
<br class="ltx_break"/>This article delves into the comparative analysis of these three methods—Mamba, SAM, and YOLO—in the context of segmenting prostate cancer histopathology images. By exploring their strengths, limitations, and applicability in real-world clinical settings, we aim to shed light on the future directions of AI-assisted diagnostics in the field of oncology. The article is divided in such a way that at the beginning, we will have the method section which explores the Mamba, Yolo, and SAM frameworks within the context of image segmentation, starting with an explanation of their architectural designs. Following this we will explain the dataset used in this article. The Results section evaluates these frameworks’ performance on various segmentation tasks, highlighting their strengths and limitations. The Discussion segment compares these results, analyzing each framework’s unique attributes. Finally, the Conclusion summarizes our findings, emphasizing the significance of our observations for the future of image segmentation technologies.
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Materials</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Our study leverages two comprehensive datasets, namely the Gleason 2019 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib14" title="">14</a>]</cite> and the SICAPv2 dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib28" title="">28</a>]</cite>, to explore the nuances of prostate cancer tissue imagery. Each dataset offers unique insights, yet they come with their own set of limitations that necessitate careful consideration during analysis.
<br class="ltx_break"/>As part of the author’s study, it became clear that there exists five primary repositories for prostate cancer tissue imagery, with the Cancer Genome Atlas initiative holding the largest collection of approximately 720 biopsy slides <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib29" title="">29</a>]</cite>. However, the lack of Gleason grade annotations at both slide and biopsy levels limits these datasets’ applicability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib28" title="">28</a>]</cite>. In contrast, the database by Arvaniti et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib30" title="">30</a>]</cite> offers detailed pixel-level annotations for Gleason patterns across 886 smaller slide segments, though these do not fully capture the diversity of patterns in localized prostate cancer and benign conditions, hindering their utility for slide-level Gleason assessments. The PANDA challenge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib31" title="">31</a>]</cite> introduced a substantial dataset, albeit with biopsy-level Gleason score annotations, not aligning with the research’s goals. Additionally, two datasets, the Gleason19 challenge and SICAPv2, are thoroughly discussed as follows:</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Gleason dataset</h3>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The 2019 Gleason challenge dataset, comprising 244 tissue microarray (TMA) images and their corresponding pixel-level annotations prepared by six pathologists, poses significant challenges in analyzing and making automated Gleason grading. One major challenge is the inter-observer variability in the annotations, which can affect the performance of automated grading models. This variability is evident in the fact that the annotations contain contradictory segmentations, making it difficult to select a single ground truth.
<br class="ltx_break"/>The limited number of Gleason images and their original resolution (5120 x 5120 pixels) make subsampling necessary. In particular, we extracted 512 x 512 patches with 50% overlapping from each image and discarded the patches that lacked tissue texture. It is worth noting that for cross-validation, we followed a patient-based splitting strategy so that the patches belonging to a patient are confined to one fold. 
<br class="ltx_break"/>Researchers have tackled the high inter-observer variability in prostate cancer grading, particularly in the Gleason 2019 Challenge dataset, by introducing innovative methods such as a dynamic scoring mechanism to balance annotations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib2" title="">2</a>]</cite> and using Cohen’s kappa coefficient to measure agreement among pathologists (mean inter-pathologist agreement of 0.42 ± 0.23) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib1" title="">1</a>]</cite>. Other strategies include incorporating an "inter-expert prior" for medical image datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib4" title="">4</a>]</cite>, encoding annotations into separate channels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib6" title="">6</a>]</cite>, modeling the full label distribution of experts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib9" title="">9</a>]</cite>, and synthesizing expert annotations with the STAPLE algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib11" title="">11</a>]</cite>. In our research, we employed max voting to aggregate expert annotations.
<br class="ltx_break"/>In addition, researchers have proposed various strategies to address imbalanced datasets. One approach involves using a weighted cross-entropy loss function during model training, adjusting class weights to balance class representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib3" title="">3</a>]</cite>. Another strategy combines data augmentation and resampling techniques, applying transformations like rotation and flipping to enhance dataset diversity and replicating images of underrepresented classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib2" title="">2</a>]</cite>. Some studies introduce novel algorithms, such as the focal loss function, which prioritizes minority class samples during training, and co-training approaches that leverage pseudo-labeled data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib6" title="">6</a>]</cite>. Evaluation metrics like the macro F1 score are also utilized to ensure fairness across all classes, alongside techniques like random patch sampling from whole slide images to manage large datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib8" title="">8</a>]</cite>. Lastly, significant improvements have been made by increasing dataset size through extensive data augmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib11" title="">11</a>]</cite>. To counteract the class imbalance present in our dataset, where each class has a differing number of samples, we utilized data augmentation methods to achieve a balanced class representation. Specifically, the dataset underwent rotations up to 10 degrees and height shifts of up to 10%, effectively increasing the number of instances for underrepresented classes. This augmentation strategy was crucial for preventing bias towards classes with larger sample sizes, ensuring a fair and balanced learning environment for the model. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S2.F1" title="Figure 1 ‣ 2.1 Gleason dataset ‣ 2 Materials ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates various examples of Gleason2019 annotations, showcasing different grades of severity.
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S2.F1.g1" src="extracted/5896840/fig/1.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Examples of Gleason 2019 annotations. Left G3, middle G4, and right G5.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>SICAPv2 dataset</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">In the SICAPv2 dataset, pixel-level annotations are only provided for Gleason grades. To address this limitation, we implemented the technique described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib32" title="">32</a>]</cite> to segregate these tissues from the surrounding background and create distinct masks for benign areas. Specifically, an image is first stain-normalized to reduce variability due to tissue preparation. The normalized image is then converted into a Tissue-Graph (TG), where nodes represent tissue regions as superpixels and edges represent interactions between these regions. The TG is created in three steps: constructing superpixels using the Simple Linear Iterative Clustering (SLIC) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib33" title="">33</a>]</cite> algorithm, extracting features from these superpixels, and defining the graph topology. Superpixels are merged hierarchically based on color similarity to form nodes, reducing complexity and enabling large image scaling. Each node is characterized by morphological and spatial features derived from image patches and superpixel centroids. The features are extracted with MobileNetV2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib34" title="">34</a>]</cite> pre-trained on ImageNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib35" title="">35</a>]</cite>. The TG’s topology is defined by a Region Adjacency Graph (RAG)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib36" title="">36</a>]</cite> that maps the spatial connectivity of superpixels. The result of this procedure on a sample image is summarized in figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S2.F2" title="Figure 2 ‣ 2.2 SICAPv2 dataset ‣ 2 Materials ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="157" id="S2.F2.g1" src="extracted/5896840/fig/2.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Segmentation map estimation on a sample image from SICAPv2 dataset. Left: original image, middle: provided segmentation map, right: processed segmentation map with Gleason grade (G4) annotation.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Mamba</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Mamba emerges as a standout candidate, offering unique advantages that set it apart from other contemporary approaches. Mamba, a relatively new entrant in the domain of deep learning for image segmentation, has garnered attention for its ability to deliver superior performance in biomedical image segmentation tasks. Its core strength lies in combining the strengths of Convolutional Neural Networks (CNNs) and State Space Sequence Models (SSMs), effectively addressing long-range dependencies often encountered in medical imaging data. This combination enables Mamba to achieve impressive results in terms of Dice Similarity Coefficient (DSC) score across various medical imaging tasks, including the segmentation of 3D organ structures in CT and MRI scans, as well as precise delineation of cellular structures in microscopy images. 
<br class="ltx_break"/>One of the key advantages of Mamba is its versatility and adaptability. It exhibits a self-configuring capability, allowing it to seamlessly adapt to various datasets. This feature enhances its scalability and flexibility, making Mamba a versatile tool in medical diagnostics and research. Furthermore, the integration of Mamba blocks within a U-Net architecture demonstrates the potential of State Space Models in biomedical image segmentation, setting a new benchmark in medical image segmentation.
<br class="ltx_break"/>Moreover, Mamba’s robustness across varying input image sizes and its efficient processing of large-scale image datasets are notable advantages. These characteristics are crucial for practical applications where image resolutions can vary significantly, making Mamba computationally efficient and adaptable to real-world scenarios. Additionally, the leveraging of pretrained models in Mamba-based networks, such as Swin-UMamba and its variants, offers several benefits including enhanced segmentation accuracy, stable convergence, and mitigation of overfitting issues, data efficiency, and reduced computational resource consumption.
<br class="ltx_break"/>However, despite its numerous advantages, Mamba also faces certain challenges. The limited research and community support compared to more established methods like Transformers could pose potential hurdles in tackling complex tasks. Customization challenges arise due to the limited availability of pre-trained models, and adapting to Mamba may require additional learning efforts, especially for those familiar with alternative methodologies.
<br class="ltx_break"/>In conclusion, after careful consideration of the current landscape of deep learning methods for prostate cancer histopathology image segmentation, Mamba has been selected as the primary segmentation method for our study. Its superior performance, adaptability, and efficiency in handling large-scale datasets, coupled with the potential of integrating with existing architectures and the benefits of using pretrained models, make Mamba a compelling choice. As the field continues to evolve, Mamba stands poised to play a pivotal role in advancing the diagnosis and treatment of prostate cancer through accurate and efficient image segmentation. H-vmunet stands as one of the notable methodologies under the Mamba, we delve into with greater depth in subsequent section.
<br class="ltx_break"/></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>H-vmunet method (Architecture)</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">The High-order Vision Mamba UNet (H-vmunet) is a model designed for medical image segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib27" title="">27</a>]</cite>. It features a 6-layer U-shaped architecture which consists of an encoder, a decoder, and skip connections. These layers have channel sizes of 8, 16, 32, 64, 128, and 256. Layers 1 and 2 utilize standard convolution operations, while layers 3 to 6 integrate High-order Visual State Space (H-VSS) modules, each incorporating a standard convolution with a kernel size of 3. The H-VSS modules in these layers correspond to orders 2 through 5, enhancing feature extraction capabilities. The skip connections employ Channel Attention Bridge (CAB) and Spatial Attention Bridge (SAB) modules for multi-level and multi-scale information fusion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib37" title="">37</a>]</cite>, improving the model’s convergence speed and sensitivity to various lesion scales (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S3.F3" title="Figure 3 ‣ 3.2 H-vmunet method (Architecture) ‣ 3 Methods ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_tag">3</span></a>).
<br class="ltx_break"/>The 2D-selective-scan technique is fundamental to the H-vmunet architecture which is proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib38" title="">38</a>]</cite>. It involves a scan expansion process, an S6 block, and a scan merge operation, executed in four different scan order directions. Feature extraction is conducted along each direction using the S6 block <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib39" title="">39</a>]</cite>, with the resulting sequences merged to restore the initial image size.
<br class="ltx_break"/>The High-order Visual State Space (H-VSS) module replaces the self-attention layer in traditional Transformer models with the High-order 2D-selective-scan (H-SS2D). This module includes LayerNorm (LN) layers, Multi-layer Perceptron (MLP), and H-SS2D operations, facilitating spatial interactions of arbitrary order. The H-SS2D is the core component for spatial feature extraction. The following equations represent the H-VSS expression:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="y=\text{HS}[\text{LN}(x)]+x" class="ltx_Math" display="block" id="S3.E1.m1.2"><semantics id="S3.E1.m1.2a"><mrow id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mi id="S3.E1.m1.2.2.3" xref="S3.E1.m1.2.2.3.cmml">y</mi><mo id="S3.E1.m1.2.2.2" xref="S3.E1.m1.2.2.2.cmml">=</mo><mrow id="S3.E1.m1.2.2.1" xref="S3.E1.m1.2.2.1.cmml"><mrow id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml"><mtext id="S3.E1.m1.2.2.1.1.3" xref="S3.E1.m1.2.2.1.1.3a.cmml">HS</mtext><mo id="S3.E1.m1.2.2.1.1.2" xref="S3.E1.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.2.cmml"><mo id="S3.E1.m1.2.2.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml"><mtext id="S3.E1.m1.2.2.1.1.1.1.1.2" xref="S3.E1.m1.2.2.1.1.1.1.1.2a.cmml">LN</mtext><mo id="S3.E1.m1.2.2.1.1.1.1.1.1" xref="S3.E1.m1.2.2.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E1.m1.2.2.1.1.1.1.1.3.2" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml"><mo id="S3.E1.m1.2.2.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml">(</mo><mi id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml">x</mi><mo id="S3.E1.m1.2.2.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.2.2.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E1.m1.2.2.1.2" xref="S3.E1.m1.2.2.1.2.cmml">+</mo><mi id="S3.E1.m1.2.2.1.3" xref="S3.E1.m1.2.2.1.3.cmml">x</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.2b"><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><eq id="S3.E1.m1.2.2.2.cmml" xref="S3.E1.m1.2.2.2"></eq><ci id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2.3">𝑦</ci><apply id="S3.E1.m1.2.2.1.cmml" xref="S3.E1.m1.2.2.1"><plus id="S3.E1.m1.2.2.1.2.cmml" xref="S3.E1.m1.2.2.1.2"></plus><apply id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1"><times id="S3.E1.m1.2.2.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.2"></times><ci id="S3.E1.m1.2.2.1.1.3a.cmml" xref="S3.E1.m1.2.2.1.1.3"><mtext id="S3.E1.m1.2.2.1.1.3.cmml" xref="S3.E1.m1.2.2.1.1.3">HS</mtext></ci><apply id="S3.E1.m1.2.2.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.2.2.1.1.1.2.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.2.2.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1"><times id="S3.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.1"></times><ci id="S3.E1.m1.2.2.1.1.1.1.1.2a.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2"><mtext id="S3.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E1.m1.2.2.1.1.1.1.1.2">LN</mtext></ci><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">𝑥</ci></apply></apply></apply><ci id="S3.E1.m1.2.2.1.3.cmml" xref="S3.E1.m1.2.2.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.2c">y=\text{HS}[\text{LN}(x)]+x</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.2d">italic_y = HS [ LN ( italic_x ) ] + italic_x</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Out}=\text{MLP}[\text{LN}(y)]+y" class="ltx_Math" display="block" id="S3.E2.m1.2"><semantics id="S3.E2.m1.2a"><mrow id="S3.E2.m1.2.2" xref="S3.E2.m1.2.2.cmml"><mtext id="S3.E2.m1.2.2.3" xref="S3.E2.m1.2.2.3a.cmml">Out</mtext><mo id="S3.E2.m1.2.2.2" xref="S3.E2.m1.2.2.2.cmml">=</mo><mrow id="S3.E2.m1.2.2.1" xref="S3.E2.m1.2.2.1.cmml"><mrow id="S3.E2.m1.2.2.1.1" xref="S3.E2.m1.2.2.1.1.cmml"><mtext id="S3.E2.m1.2.2.1.1.3" xref="S3.E2.m1.2.2.1.1.3a.cmml">MLP</mtext><mo id="S3.E2.m1.2.2.1.1.2" xref="S3.E2.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.2.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.2.1.cmml">[</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml"><mtext id="S3.E2.m1.2.2.1.1.1.1.1.2" xref="S3.E2.m1.2.2.1.1.1.1.1.2a.cmml">LN</mtext><mo id="S3.E2.m1.2.2.1.1.1.1.1.1" xref="S3.E2.m1.2.2.1.1.1.1.1.1.cmml">⁢</mo><mrow id="S3.E2.m1.2.2.1.1.1.1.1.3.2" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml"><mo id="S3.E2.m1.2.2.1.1.1.1.1.3.2.1" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml">(</mo><mi id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml">y</mi><mo id="S3.E2.m1.2.2.1.1.1.1.1.3.2.2" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.2.2.1.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S3.E2.m1.2.2.1.2" xref="S3.E2.m1.2.2.1.2.cmml">+</mo><mi id="S3.E2.m1.2.2.1.3" xref="S3.E2.m1.2.2.1.3.cmml">y</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.2b"><apply id="S3.E2.m1.2.2.cmml" xref="S3.E2.m1.2.2"><eq id="S3.E2.m1.2.2.2.cmml" xref="S3.E2.m1.2.2.2"></eq><ci id="S3.E2.m1.2.2.3a.cmml" xref="S3.E2.m1.2.2.3"><mtext id="S3.E2.m1.2.2.3.cmml" xref="S3.E2.m1.2.2.3">Out</mtext></ci><apply id="S3.E2.m1.2.2.1.cmml" xref="S3.E2.m1.2.2.1"><plus id="S3.E2.m1.2.2.1.2.cmml" xref="S3.E2.m1.2.2.1.2"></plus><apply id="S3.E2.m1.2.2.1.1.cmml" xref="S3.E2.m1.2.2.1.1"><times id="S3.E2.m1.2.2.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.2"></times><ci id="S3.E2.m1.2.2.1.1.3a.cmml" xref="S3.E2.m1.2.2.1.1.3"><mtext id="S3.E2.m1.2.2.1.1.3.cmml" xref="S3.E2.m1.2.2.1.1.3">MLP</mtext></ci><apply id="S3.E2.m1.2.2.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="S3.E2.m1.2.2.1.1.1.2.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E2.m1.2.2.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1"><times id="S3.E2.m1.2.2.1.1.1.1.1.1.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.1"></times><ci id="S3.E2.m1.2.2.1.1.1.1.1.2a.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2"><mtext id="S3.E2.m1.2.2.1.1.1.1.1.2.cmml" xref="S3.E2.m1.2.2.1.1.1.1.1.2">LN</mtext></ci><ci id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1">𝑦</ci></apply></apply></apply><ci id="S3.E2.m1.2.2.1.3.cmml" xref="S3.E2.m1.2.2.1.3">𝑦</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.2c">\text{Out}=\text{MLP}[\text{LN}(y)]+y</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.2d">Out = MLP [ LN ( italic_y ) ] + italic_y</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1">Where LN is the LayerNorm layer, HS is the H-SS2D operation, and MLP is the multilayer perceptron <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="314" id="S3.F3.g1" src="extracted/5896840/fig/3.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>(Top) The H-vmunet model architecture overview. (Bottom) Multi-level and multi-scale information fusion module (SAB, CAB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib27" title="">27</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">The 1-order 2D-selective-scan operation, referred to as H1-SS2D, is a fundamental component of the H-vmunet architecture. It begins by transforming the input feature map in four different directions: horizontal, vertical, and two diagonal directions. Each direction undergoes an independent 2D-selective-scan, which extracts relevant features from the input. To enhance feature extraction, the Local-SS2D module combines local convolutional operations with the global 2D-selective-scan. This dual approach allows the model to effectively balance local detail and global context, leading to a comprehensive feature representation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib27" title="">27</a>]</cite>. The higher-order 2D-selective-scan generalizes this approach to any order (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S3.F4" title="Figure 4 ‣ 3.2 H-vmunet method (Architecture) ‣ 3 Methods ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_tag">4</span></a>). For higher-order scans, the process recursively applies the previous order scan. This hierarchical scanning captures increasingly complex spatial dependencies, building on the earlier scans. Each higher-order scan incorporates the findings of the previous scans, allowing the model to extract features at multiple scales and hierarchies. This recursive, layered approach enables the model to capture both fine-grained details and broad contextual information effectively <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib27" title="">27</a>]</cite>.
<br class="ltx_break"/></p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="325" id="S3.F4.g1" src="extracted/5896840/fig/4.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>(Left) Architecture of the H-VSS module. (Middle) Overview of n-order 2D-selective-scan module. (Right) Architecture of Local-SS2D module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib27" title="">27</a>]</cite></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>SAM</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">SAM sets itself apart from conventional segmentation frameworks by introducing a novel promptable segmentation task, underpinned by a flexible model architecture capable of processing prompts and a vast, diverse array of training data. A data engine is employed to create a cyclical process where the model aids in data collection, which in turn is used to improve the model’s performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib40" title="">40</a>]</cite>. 
<br class="ltx_break"/>SAM’s architecture is meticulously designed to address the complexities of image segmentation tasks, featuring three interconnected components: an image encoder, a prompt encoder, and a mask decoder (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S3.F5" title="Figure 5 ‣ 3.3 SAM ‣ 3 Methods ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_tag">5</span></a>). The image encoder, built on the ViT backbone, was pretrained using the masked autoencoder (MAE) technique <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib41" title="">41</a>]</cite>. It processes the input image, generating a rich embedding that captures the spatial and semantic features of the scene. This component plays a crucial role in SAM’s ability to understand the context and details of the image, facilitating accurate segmentation. The Prompt Encoder is designed to interpret human-provided prompts, whether they are textual descriptions, points, or bounding boxes indicating the areas of interest. This capability enables SAM to perform zero-shot segmentation, adapting to new tasks without explicit retraining, thereby enhancing its versatility and applicability. The Mask Decoder, a lightweight yet effective component, takes the embeddings produced by the encoders and generates segmentation masks. This process involves predicting pixel-wise labels that delineate the objects of interest within the image, achieving precise localization and segmentation. 
<br class="ltx_break"/>To effectively utilize SAM for specific tasks, fine-tuning is essential. This involves adjusting the model’s hyperparameters, such as learning rates and batch sizes, to optimize its performance on the target dataset. Careful selection and tuning of these parameters are critical to avoid overfitting and ensure that SAM adapts well to new data, maintaining its high level of accuracy and efficiency. Moreover, SAM’s architecture benefits from a vast dataset, comprising over 1 billion masks derived from 11 million images. This extensive training data equips SAM with robust generalization capabilities, enabling it to excel in a wide range of segmentation tasks with minimal adjustments. 
<br class="ltx_break"/>SAM offers both automatic and manual modes during testing. In the automatic mode, users simply input an image, and SAM automatically generates all masks. In the manual mode, users supply additional inputs such as boxes, points, masks, and text to provide SAM with more detailed information about the objects to be segmented <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib40" title="">40</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S3.F5.g1" src="extracted/5896840/fig/5.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>The pipeline of SAM in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib40" title="">40</a>]</cite></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">Figure 5: The pipeline of SAM in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib40" title="">40</a>]</cite></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>YOLO</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">YOLO (You Only Look Once) is a revolutionary object detection algorithm that has significantly impacted the field of computer vision by offering real-time object detection capabilities. Initially introduced by Joseph Redmon, YOLO has evolved through several iterations, with YOLOv8 representing the latest advancement in this series. A review study highlighted several key advantages of YOLO (You Only Look Once) in medical image analysis:</p>
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">Real-Time Performance: YOLO is noted for its real-time object detection capabilities, which are essential for urgent medical interventions and clinical decisions. This real-time functionality facilitates swift image processing, contributing to timely disease identification, therapeutic strategy formulation, and disease evolution tracking.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">Versatility and Generalizability: YOLO’s flexibility permits it to identify and locate a wide range of anatomical elements, abnormalities, growths, and other pertinent medical items across various medical imaging modalities. Its adaptability ensures reliable performance even in new contexts or unforeseen scenarios, a significant advantage in the ever-evolving medical landscape.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">High Accuracy: The paper mentions that YOLO outperforms other existing methods in various medical object detection tasks, including the identification of lesions, categorization of skin lesions, detection of retinal anomalies, cardiac abnormalities and brain tumor delineation This precision stems from its capacity to grasp generalized object representations, exceeding benchmarks set by models such as the Deformable Parts Model (DPM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib42" title="">42</a>]</cite> and Faster RCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib43" title="">43</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S3.I1.i4.p1">
<p class="ltx_p" id="S3.I1.i4.p1.1">Efficiency: While maintaining high precision, YOLO ensures operational efficiency, vital for real-world healthcare implementations. Its design supports quicker object detection compared to conventional two-phase methodologies, enhancing its suitability for immediate use cases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib44" title="">44</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para ltx_noindent" id="S3.I1.i5.p1">
<p class="ltx_p" id="S3.I1.i5.p1.1">Adaptability to Different Medical Domains: YOLO demonstrates efficacy across a spectrum of medical disciplines, encompassing medical imaging, surgical processes, and personal protective equipment detection. This versatility emphasizes its extensive role in healthcare solutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib45" title="">45</a>]</cite>.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">These attributes position YOLO as a formidable asset for medical image analysis, empowering medical practitioners to diagnose and manage a variety of health issues with proficiency and speed. YOLOv8, developed by the Ultralytics team, builds upon the foundational principles of YOLO but incorporates major improvements in speed, accuracy, and efficiency, making it a state-of-the-art system for object detection tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib47" title="">47</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib48" title="">48</a>]</cite>. YOLOv8, in particular, offers several key features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib49" title="">49</a>]</cite> that are advantageous for medical image analysis:</p>
<ul class="ltx_itemize" id="S3.I2">
<li class="ltx_item" id="S3.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i1.p1">
<p class="ltx_p" id="S3.I2.i1.p1.1">Multi-Scaled Detection: YOLOv8 excels in detecting objects of various sizes within an image, which is essential for medical imaging where objects can vary greatly in scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#bib.bib50" title="">50</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i2.p1">
<p class="ltx_p" id="S3.I2.i2.p1.1">Improved Accuracy: Despite its speed, YOLOv8 maintains high accuracy in object detection, making it reliable for medical applications where precision is paramount.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i3.p1">
<p class="ltx_p" id="S3.I2.i3.p1.1">Improved Accuracy: Despite its speed, YOLOv8 maintains high accuracy in object detection, making it reliable for medical applications where precision is paramount.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i4.p1">
<p class="ltx_p" id="S3.I2.i4.p1.1">Flexible Framework: YOLOv8’s unified architecture seamlessly integrates object detection, instance segmentation, and image classification capabilities, enabling a comprehensive approach to medical image analysis. This versatility allows for the localization, delineation, and categorization of anatomical structures, pathological regions, and other clinically relevant features within medical imagery.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i5.p1">
<p class="ltx_p" id="S3.I2.i5.p1.1">Pre-Trained Models: Offering pre-trained models for different tasks, YOLOv8 can be easily adapted for specific medical imaging tasks, saving time and resources.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i6.p1">
<p class="ltx_p" id="S3.I2.i6.p1.1">Advanced Customization Options: YOLOv8 provides advanced customization options, enabling tailored solutions for unique medical imaging challenges.</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S3.I2.i7.p1">
<p class="ltx_p" id="S3.I2.i7.p1.1">Anchor-Free Architecture: Unlike previous versions that relied on predefined anchor boxes, YOLOv8 employs an anchor-free architecture. This innovation allows for direct prediction of object centers without relying on fixed anchor shapes, enhancing flexibility and accuracy in object detection</p>
</div>
</li>
<li class="ltx_item" id="S3.I2.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S3.I2.i8.p1">
<p class="ltx_p" id="S3.I2.i8.p1.1">Advanced Backbone Network: YOLOv8 introduces a new backbone network, contributing to its improved performance. This advanced architecture supports more efficient feature extraction, further boosting the model’s overall detection capabilities.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1">These features make YOLOv8 ones of excellent choices for our study, as they address the critical needs of medical image analysis, including the ability to quickly and accurately detect and classify objects within medical images, thereby facilitating more informed diagnostic decisions and treatment plans. In this study we used YOLOv8m (medium) model architecture, offering a compelling trade-off between inference speed and accuracy for applications.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">We use various segmentation metrics to quantitatively assess different methods. The Dice coefficient remains unaffected by changes in pattern size because it assesses the proportion of overlap between two sets rather than their absolute sizes. Consequently, it delivers consistent similarity scores as long as the overlap proportion is unchanged, regardless of the size of the segmented regions. Additionally, since segmentation can be viewed as pixel-level classification, we also utilize precision and recall, defined as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}}" class="ltx_Math" display="block" id="S4.E3.m1.1"><semantics id="S4.E3.m1.1a"><mrow id="S4.E3.m1.1.1" xref="S4.E3.m1.1.1.cmml"><mtext id="S4.E3.m1.1.1.2" xref="S4.E3.m1.1.1.2a.cmml">Precision</mtext><mo id="S4.E3.m1.1.1.1" xref="S4.E3.m1.1.1.1.cmml">=</mo><mfrac id="S4.E3.m1.1.1.3" xref="S4.E3.m1.1.1.3.cmml"><mtext id="S4.E3.m1.1.1.3.2" xref="S4.E3.m1.1.1.3.2a.cmml">TP</mtext><mrow id="S4.E3.m1.1.1.3.3" xref="S4.E3.m1.1.1.3.3.cmml"><mtext id="S4.E3.m1.1.1.3.3.2" xref="S4.E3.m1.1.1.3.3.2a.cmml">TP</mtext><mo id="S4.E3.m1.1.1.3.3.1" xref="S4.E3.m1.1.1.3.3.1.cmml">+</mo><mtext id="S4.E3.m1.1.1.3.3.3" xref="S4.E3.m1.1.1.3.3.3a.cmml">FP</mtext></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E3.m1.1b"><apply id="S4.E3.m1.1.1.cmml" xref="S4.E3.m1.1.1"><eq id="S4.E3.m1.1.1.1.cmml" xref="S4.E3.m1.1.1.1"></eq><ci id="S4.E3.m1.1.1.2a.cmml" xref="S4.E3.m1.1.1.2"><mtext id="S4.E3.m1.1.1.2.cmml" xref="S4.E3.m1.1.1.2">Precision</mtext></ci><apply id="S4.E3.m1.1.1.3.cmml" xref="S4.E3.m1.1.1.3"><divide id="S4.E3.m1.1.1.3.1.cmml" xref="S4.E3.m1.1.1.3"></divide><ci id="S4.E3.m1.1.1.3.2a.cmml" xref="S4.E3.m1.1.1.3.2"><mtext id="S4.E3.m1.1.1.3.2.cmml" xref="S4.E3.m1.1.1.3.2">TP</mtext></ci><apply id="S4.E3.m1.1.1.3.3.cmml" xref="S4.E3.m1.1.1.3.3"><plus id="S4.E3.m1.1.1.3.3.1.cmml" xref="S4.E3.m1.1.1.3.3.1"></plus><ci id="S4.E3.m1.1.1.3.3.2a.cmml" xref="S4.E3.m1.1.1.3.3.2"><mtext id="S4.E3.m1.1.1.3.3.2.cmml" xref="S4.E3.m1.1.1.3.3.2">TP</mtext></ci><ci id="S4.E3.m1.1.1.3.3.3a.cmml" xref="S4.E3.m1.1.1.3.3.3"><mtext id="S4.E3.m1.1.1.3.3.3.cmml" xref="S4.E3.m1.1.1.3.3.3">FP</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3.m1.1c">\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}}</annotation><annotation encoding="application/x-llamapun" id="S4.E3.m1.1d">Precision = divide start_ARG TP end_ARG start_ARG TP + FP end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<table class="ltx_equation ltx_eqn_table" id="S4.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}" class="ltx_Math" display="block" id="S4.E4.m1.1"><semantics id="S4.E4.m1.1a"><mrow id="S4.E4.m1.1.1" xref="S4.E4.m1.1.1.cmml"><mtext id="S4.E4.m1.1.1.2" xref="S4.E4.m1.1.1.2a.cmml">Recall</mtext><mo id="S4.E4.m1.1.1.1" xref="S4.E4.m1.1.1.1.cmml">=</mo><mfrac id="S4.E4.m1.1.1.3" xref="S4.E4.m1.1.1.3.cmml"><mtext id="S4.E4.m1.1.1.3.2" xref="S4.E4.m1.1.1.3.2a.cmml">TP</mtext><mrow id="S4.E4.m1.1.1.3.3" xref="S4.E4.m1.1.1.3.3.cmml"><mtext id="S4.E4.m1.1.1.3.3.2" xref="S4.E4.m1.1.1.3.3.2a.cmml">TP</mtext><mo id="S4.E4.m1.1.1.3.3.1" xref="S4.E4.m1.1.1.3.3.1.cmml">+</mo><mtext id="S4.E4.m1.1.1.3.3.3" xref="S4.E4.m1.1.1.3.3.3a.cmml">FN</mtext></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E4.m1.1b"><apply id="S4.E4.m1.1.1.cmml" xref="S4.E4.m1.1.1"><eq id="S4.E4.m1.1.1.1.cmml" xref="S4.E4.m1.1.1.1"></eq><ci id="S4.E4.m1.1.1.2a.cmml" xref="S4.E4.m1.1.1.2"><mtext id="S4.E4.m1.1.1.2.cmml" xref="S4.E4.m1.1.1.2">Recall</mtext></ci><apply id="S4.E4.m1.1.1.3.cmml" xref="S4.E4.m1.1.1.3"><divide id="S4.E4.m1.1.1.3.1.cmml" xref="S4.E4.m1.1.1.3"></divide><ci id="S4.E4.m1.1.1.3.2a.cmml" xref="S4.E4.m1.1.1.3.2"><mtext id="S4.E4.m1.1.1.3.2.cmml" xref="S4.E4.m1.1.1.3.2">TP</mtext></ci><apply id="S4.E4.m1.1.1.3.3.cmml" xref="S4.E4.m1.1.1.3.3"><plus id="S4.E4.m1.1.1.3.3.1.cmml" xref="S4.E4.m1.1.1.3.3.1"></plus><ci id="S4.E4.m1.1.1.3.3.2a.cmml" xref="S4.E4.m1.1.1.3.3.2"><mtext id="S4.E4.m1.1.1.3.3.2.cmml" xref="S4.E4.m1.1.1.3.3.2">TP</mtext></ci><ci id="S4.E4.m1.1.1.3.3.3a.cmml" xref="S4.E4.m1.1.1.3.3.3"><mtext id="S4.E4.m1.1.1.3.3.3.cmml" xref="S4.E4.m1.1.1.3.3.3">FN</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E4.m1.1c">\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}}</annotation><annotation encoding="application/x-llamapun" id="S4.E4.m1.1d">Recall = divide start_ARG TP end_ARG start_ARG TP + FN end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<table class="ltx_equation ltx_eqn_table" id="S4.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{F-Score}=\frac{2\times(\text{Precision}\times\text{Recall})}{\text{%
Precision}+\text{Recall}}" class="ltx_Math" display="block" id="S4.E5.m1.1"><semantics id="S4.E5.m1.1a"><mrow id="S4.E5.m1.1.2" xref="S4.E5.m1.1.2.cmml"><mtext id="S4.E5.m1.1.2.2" xref="S4.E5.m1.1.2.2a.cmml">F-Score</mtext><mo id="S4.E5.m1.1.2.1" xref="S4.E5.m1.1.2.1.cmml">=</mo><mfrac id="S4.E5.m1.1.1" xref="S4.E5.m1.1.1.cmml"><mrow id="S4.E5.m1.1.1.1" xref="S4.E5.m1.1.1.1.cmml"><mn id="S4.E5.m1.1.1.1.3" xref="S4.E5.m1.1.1.1.3.cmml">2</mn><mo id="S4.E5.m1.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S4.E5.m1.1.1.1.2.cmml">×</mo><mrow id="S4.E5.m1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.cmml"><mo id="S4.E5.m1.1.1.1.1.1.2" stretchy="false" xref="S4.E5.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.E5.m1.1.1.1.1.1.1" xref="S4.E5.m1.1.1.1.1.1.1.cmml"><mtext id="S4.E5.m1.1.1.1.1.1.1.2" xref="S4.E5.m1.1.1.1.1.1.1.2a.cmml">Precision</mtext><mo id="S4.E5.m1.1.1.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.E5.m1.1.1.1.1.1.1.1.cmml">×</mo><mtext id="S4.E5.m1.1.1.1.1.1.1.3" xref="S4.E5.m1.1.1.1.1.1.1.3a.cmml">Recall</mtext></mrow><mo id="S4.E5.m1.1.1.1.1.1.3" stretchy="false" xref="S4.E5.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mrow id="S4.E5.m1.1.1.3" xref="S4.E5.m1.1.1.3.cmml"><mtext id="S4.E5.m1.1.1.3.2" xref="S4.E5.m1.1.1.3.2a.cmml">Precision</mtext><mo id="S4.E5.m1.1.1.3.1" xref="S4.E5.m1.1.1.3.1.cmml">+</mo><mtext id="S4.E5.m1.1.1.3.3" xref="S4.E5.m1.1.1.3.3a.cmml">Recall</mtext></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E5.m1.1b"><apply id="S4.E5.m1.1.2.cmml" xref="S4.E5.m1.1.2"><eq id="S4.E5.m1.1.2.1.cmml" xref="S4.E5.m1.1.2.1"></eq><ci id="S4.E5.m1.1.2.2a.cmml" xref="S4.E5.m1.1.2.2"><mtext id="S4.E5.m1.1.2.2.cmml" xref="S4.E5.m1.1.2.2">F-Score</mtext></ci><apply id="S4.E5.m1.1.1.cmml" xref="S4.E5.m1.1.1"><divide id="S4.E5.m1.1.1.2.cmml" xref="S4.E5.m1.1.1"></divide><apply id="S4.E5.m1.1.1.1.cmml" xref="S4.E5.m1.1.1.1"><times id="S4.E5.m1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.2"></times><cn id="S4.E5.m1.1.1.1.3.cmml" type="integer" xref="S4.E5.m1.1.1.1.3">2</cn><apply id="S4.E5.m1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1"><times id="S4.E5.m1.1.1.1.1.1.1.1.cmml" xref="S4.E5.m1.1.1.1.1.1.1.1"></times><ci id="S4.E5.m1.1.1.1.1.1.1.2a.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2"><mtext id="S4.E5.m1.1.1.1.1.1.1.2.cmml" xref="S4.E5.m1.1.1.1.1.1.1.2">Precision</mtext></ci><ci id="S4.E5.m1.1.1.1.1.1.1.3a.cmml" xref="S4.E5.m1.1.1.1.1.1.1.3"><mtext id="S4.E5.m1.1.1.1.1.1.1.3.cmml" xref="S4.E5.m1.1.1.1.1.1.1.3">Recall</mtext></ci></apply></apply><apply id="S4.E5.m1.1.1.3.cmml" xref="S4.E5.m1.1.1.3"><plus id="S4.E5.m1.1.1.3.1.cmml" xref="S4.E5.m1.1.1.3.1"></plus><ci id="S4.E5.m1.1.1.3.2a.cmml" xref="S4.E5.m1.1.1.3.2"><mtext id="S4.E5.m1.1.1.3.2.cmml" xref="S4.E5.m1.1.1.3.2">Precision</mtext></ci><ci id="S4.E5.m1.1.1.3.3a.cmml" xref="S4.E5.m1.1.1.3.3"><mtext id="S4.E5.m1.1.1.3.3.cmml" xref="S4.E5.m1.1.1.3.3">Recall</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E5.m1.1c">\text{F-Score}=\frac{2\times(\text{Precision}\times\text{Recall})}{\text{%
Precision}+\text{Recall}}</annotation><annotation encoding="application/x-llamapun" id="S4.E5.m1.1d">F-Score = divide start_ARG 2 × ( Precision × Recall ) end_ARG start_ARG Precision + Recall end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.p5">
<table class="ltx_equation ltx_eqn_table" id="S4.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Accuracy}=\frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text%
{FN}}" class="ltx_Math" display="block" id="S4.E6.m1.1"><semantics id="S4.E6.m1.1a"><mrow id="S4.E6.m1.1.1" xref="S4.E6.m1.1.1.cmml"><mtext id="S4.E6.m1.1.1.2" xref="S4.E6.m1.1.1.2a.cmml">Accuracy</mtext><mo id="S4.E6.m1.1.1.1" xref="S4.E6.m1.1.1.1.cmml">=</mo><mfrac id="S4.E6.m1.1.1.3" xref="S4.E6.m1.1.1.3.cmml"><mrow id="S4.E6.m1.1.1.3.2" xref="S4.E6.m1.1.1.3.2.cmml"><mtext id="S4.E6.m1.1.1.3.2.2" xref="S4.E6.m1.1.1.3.2.2a.cmml">TP</mtext><mo id="S4.E6.m1.1.1.3.2.1" xref="S4.E6.m1.1.1.3.2.1.cmml">+</mo><mtext id="S4.E6.m1.1.1.3.2.3" xref="S4.E6.m1.1.1.3.2.3a.cmml">TN</mtext></mrow><mrow id="S4.E6.m1.1.1.3.3" xref="S4.E6.m1.1.1.3.3.cmml"><mtext id="S4.E6.m1.1.1.3.3.2" xref="S4.E6.m1.1.1.3.3.2a.cmml">TP</mtext><mo id="S4.E6.m1.1.1.3.3.1" xref="S4.E6.m1.1.1.3.3.1.cmml">+</mo><mtext id="S4.E6.m1.1.1.3.3.3" xref="S4.E6.m1.1.1.3.3.3a.cmml">TN</mtext><mo id="S4.E6.m1.1.1.3.3.1a" xref="S4.E6.m1.1.1.3.3.1.cmml">+</mo><mtext id="S4.E6.m1.1.1.3.3.4" xref="S4.E6.m1.1.1.3.3.4a.cmml">FP</mtext><mo id="S4.E6.m1.1.1.3.3.1b" xref="S4.E6.m1.1.1.3.3.1.cmml">+</mo><mtext id="S4.E6.m1.1.1.3.3.5" xref="S4.E6.m1.1.1.3.3.5a.cmml">FN</mtext></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.E6.m1.1b"><apply id="S4.E6.m1.1.1.cmml" xref="S4.E6.m1.1.1"><eq id="S4.E6.m1.1.1.1.cmml" xref="S4.E6.m1.1.1.1"></eq><ci id="S4.E6.m1.1.1.2a.cmml" xref="S4.E6.m1.1.1.2"><mtext id="S4.E6.m1.1.1.2.cmml" xref="S4.E6.m1.1.1.2">Accuracy</mtext></ci><apply id="S4.E6.m1.1.1.3.cmml" xref="S4.E6.m1.1.1.3"><divide id="S4.E6.m1.1.1.3.1.cmml" xref="S4.E6.m1.1.1.3"></divide><apply id="S4.E6.m1.1.1.3.2.cmml" xref="S4.E6.m1.1.1.3.2"><plus id="S4.E6.m1.1.1.3.2.1.cmml" xref="S4.E6.m1.1.1.3.2.1"></plus><ci id="S4.E6.m1.1.1.3.2.2a.cmml" xref="S4.E6.m1.1.1.3.2.2"><mtext id="S4.E6.m1.1.1.3.2.2.cmml" xref="S4.E6.m1.1.1.3.2.2">TP</mtext></ci><ci id="S4.E6.m1.1.1.3.2.3a.cmml" xref="S4.E6.m1.1.1.3.2.3"><mtext id="S4.E6.m1.1.1.3.2.3.cmml" xref="S4.E6.m1.1.1.3.2.3">TN</mtext></ci></apply><apply id="S4.E6.m1.1.1.3.3.cmml" xref="S4.E6.m1.1.1.3.3"><plus id="S4.E6.m1.1.1.3.3.1.cmml" xref="S4.E6.m1.1.1.3.3.1"></plus><ci id="S4.E6.m1.1.1.3.3.2a.cmml" xref="S4.E6.m1.1.1.3.3.2"><mtext id="S4.E6.m1.1.1.3.3.2.cmml" xref="S4.E6.m1.1.1.3.3.2">TP</mtext></ci><ci id="S4.E6.m1.1.1.3.3.3a.cmml" xref="S4.E6.m1.1.1.3.3.3"><mtext id="S4.E6.m1.1.1.3.3.3.cmml" xref="S4.E6.m1.1.1.3.3.3">TN</mtext></ci><ci id="S4.E6.m1.1.1.3.3.4a.cmml" xref="S4.E6.m1.1.1.3.3.4"><mtext id="S4.E6.m1.1.1.3.3.4.cmml" xref="S4.E6.m1.1.1.3.3.4">FP</mtext></ci><ci id="S4.E6.m1.1.1.3.3.5a.cmml" xref="S4.E6.m1.1.1.3.3.5"><mtext id="S4.E6.m1.1.1.3.3.5.cmml" xref="S4.E6.m1.1.1.3.3.5">FN</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E6.m1.1c">\text{Accuracy}=\frac{\text{TP}+\text{TN}}{\text{TP}+\text{TN}+\text{FP}+\text%
{FN}}</annotation><annotation encoding="application/x-llamapun" id="S4.E6.m1.1d">Accuracy = divide start_ARG TP + TN end_ARG start_ARG TP + TN + FP + FN end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.p6">
<p class="ltx_p" id="S4.p6.1">With TP, FP, TN, and FN indiciating true positives, false positives, true negatives, and false negatives, respectively.
These metrics highlight the challenge in differentiating between four classes in Gleason2019 and SICAPv2 dataset. It is worth mentioning that due to severe class imbalance in these datasets, we opt for a weighted average over class metrics to provide a more comprehensive measurement of overall performance. This approach ensures that the performance metrics reflect the proportion of each class in the dataset, rather than being skewed by the performance on less frequent Gleason patterns.
We compared the YoloS, SAM, and H-vmunet models on the Prostate2019 and SICAP-V2 datasets using Dice score, precision, and recall metrics, as presented in Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S4.T1" title="Table 1 ‣ 4 Results ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_tag">1</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.16205v2#S4.T2" title="Table 2 ‣ 4 Results ‣ Segmentation Strategies in Deep Learning for Prostate Cancer Diagnosis: A Comparative Study of Mamba, SAM, and YOLO"><span class="ltx_text ltx_ref_tag">2</span></a>. On the Prostate2019 dataset, H-vmunet outperformed the other models, achieving the a dice score of 0.92 (highest score) across all metrics, which highlights its effectiveness in segmenting Gleason images. This superior performance can be attributed to the advanced architecture of H-vmunet, which leverages vision mamba to better capture and differentiate complex tissue structures. Similarly, on the SICAP-V2 dataset, H-vmunet again achieved the best results (dice score of 0.68) compared to SAM and YoloS, demonstrating its robustness and generalization capability across different types of pathological images. The consistent high performance of H-vmunet suggests that its sophisticated model design, which includes enhanced feature extraction and segmentation strategies, is particularly well-suited for medical image analysis, making it more effective in handling the variability and complexity inherent in these datasets.
The H-vmunet showcases notable performance metrics, with Precision and Recall values both standing at 0.68 for the SICAP-V2 and 0.92. This uniformity in scores indicates a balanced approach in identifying true positives and minimizing false negatives, suggesting a well-rounded performance of the H-vmunet method in the evaluated dataset. Such consistency across these key metrics underscores the method’s reliability and effectiveness in classification tasks, offering promising insights for further application and refinement in related domains.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Comparison results on the prostate 2019 dataset </figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T1.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.2">DSC (F1)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.3">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.1.1.1.4">Recall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T1.1.2.1.1">YoloS(avg)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.2">0.83 ± 0.015</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.3">0.84 ± 0.014</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.1.2.1.4">0.83 ± 0.016</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T1.1.3.2.1">SAM (avg)</th>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.2">0.86 ± 0.120</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.3">0.87 ± 0.100</td>
<td class="ltx_td ltx_align_center" id="S4.T1.1.3.2.4">0.85 ± 0.110</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T1.1.4.3.1">H-vmunet (avg)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.2">0.92 ± 0.062</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.3">0.92 ± 0.061</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.1.4.3.4">0.92 ± 0.064</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparison results on the SICAP-V2 dataset </figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1">Method</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.2">DSC (F1)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3">Precision</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4">Recall</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1">YoloS(avg)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.2">0.65 ± 0.025</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3">0.64 ± 0.027</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4">0.65 ± 0.024</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.3.2.1">SAM (avg)</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.2">0.66 ± 0.12</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3">0.67 ± 0.049</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4">0.66 ± 0.027</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.4.3.1">H-vmunet (avg)</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.3.2">0.68 ± 0.034</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.3.3">0.68 ± 0.047</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.4.3.4">0.68 ± 0.031</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The comparative analysis presented in this study underscores the significant role of deep learning models in advancing prostate cancer diagnosis through histopathology image segmentation. The comparative analysis of Mamba, SAM, and YOLO models for segmenting prostate cancer histopathology images reveals significant insights into the capabilities and limitations of each method. The High-order Vision Mamba UNet (H-vmunet) model emerged as the most effective across both the Gleason 2019 and SICAPv2 datasets, showcasing its robustness and adaptability in handling complex medical imaging tasks. This superior performance underscores the importance of innovative architectural designs and advanced feature extraction techniques in achieving accurate segmentation outcomes. 
<br class="ltx_break"/>H-vmunet’s standout performance can be attributed to several key factors. Firstly, its integration of High-order Visual State Space (H-VSS) modules and 2D-selective-scan operations enables efficient and sensitive lesion detection across varying scales. This capability is crucial for medical image analysis, where the ability to discern subtle differences between normal and malignant tissues is paramount, especially when it comes to accurately identifying and delineating malignant tissues within the intricate and varied structures found in prostate histopathology images. Secondly, The model’s architecture not only facilitates multi-level and multi-scale information fusion, thereby enhancing its sensitivity to lesions of varying sizes and improving convergence speed, but also balances local detail extraction with global context understanding through the Local-SS2D module and Channel and Spatial Attention modules in skip connections, further accelerating convergence and increasing sensitivity to lesions across different scales. The architectural innovations significantly contribute to H-vmunet’s superior segmentation accuracy compared to SAM and YOLO models, collectively enhancing the model’s Dice score, precision, and recall metrics observed in our study.
<br class="ltx_break"/>Despite the impressive performance of H-vmunet, it is important to acknowledge potential challenges and limitations. One such challenge is the computational complexity associated with high-order visual state spaces and 2D-selective-scan operations. While these techniques enhance segmentation accuracy, they may also increase processing time and resource requirements, potentially limiting real-time applicability in clinical settings. Additionally, the effectiveness of H-vmunet, like any deep learning model, heavily relies on the quality and diversity of training data. Insufficient training data or biases within the dataset could impact the model’s generalization capabilities, affecting its performance on unseen images.
<br class="ltx_break"/>SAM and YOLO, while demonstrating promising results in various segmentation tasks, did not outperform H-vmunet in our study. SAM’s strength lies in its flexibility and adaptability, particularly its ability to perform zero-shot segmentation, which is crucial in various applications. However, its reliance on human-provided prompts may limit its autonomy in clinical settings where immediate and accurate segmentation is required without manual intervention. Despite its innovative promptable segmentation task and flexible architecture capable of processing prompts, SAM did not match the segmentation accuracy of H-vmunet on both datasets. This discrepancy could be due to SAM’s dependence on human-provided prompts and its architectural design focusing on generalizability across various image types, potentially not fully exploiting the specific characteristics of prostate cancer histopathology images.
<br class="ltx_break"/>YOLO, renowned for its real-time object detection capabilities, provides substantial benefits in terms of speed and efficiency. However, its performance in segmenting prostate cancer histopathology images did not reach the level of H-vmunet, likely because of the intricate and variable nature of medical images necessitating advanced feature extraction techniques that go beyond simple object detection. Furthermore, even though YOLOv8 showed enhancements in speed, accuracy, and efficiency, it still lagged behind H-vmunet in histopathology image segmentation. This indicates that while YOLO shines in object detection tasks, the detailed requirements of histopathology image segmentation—demanding precise delineation of tissue structures—are better served by the specialized architectures and methodologies utilized by H-vmunet.
<br class="ltx_break"/>The findings of this study hold significant implications for clinical practice, highlighting the critical importance of accurate segmentation of prostate cancer histopathology images for diagnosis, treatment planning, and ultimately, patient outcomes. The demonstrated effectiveness of H-vmunet suggests its potential as a highly valuable tool within clinical workflows, offering enhanced precision in identifying malignant tissues. However, the successful integration of such models into clinical settings necessitates overcoming computational challenges and ensuring rigorous validation against established clinical standards. Additionally, the continuous evolution and adaptation of models like H-vmunet are imperative to maintain alignment with advancements in medical imaging technologies and the ever-changing landscape of diagnostic needs. While the model’s robust performance across various datasets indicates its broad applicability in clinical environments, making it a versatile asset for oncologists and pathologists, it is crucial to acknowledge the challenges involved in integrating AI-assisted diagnostic tools into clinical workflows. These include the necessity for extensive training data, the interpretability of model outputs, and obtaining regulatory approval for clinical use, alongside the commitment to ongoing research and validation against clinical standards to guarantee the reliability and safety of these tools.
<br class="ltx_break"/>Future research directions for H-vmunet should focus on enhancing its computational efficiency to support real-time applications without sacrificing accuracy. Exploring strategies to boost model generalization, such as incorporating more diverse training datasets or employing data augmentation techniques, could further enhance performance on unseen images. Additionally, integrating H-vmunet with other AI-driven diagnostic tools could offer comprehensive solutions for prostate cancer diagnosis, potentially transforming clinical decision-making processes. Looking ahead, future research should concentrate on refining and extending the capabilities of the H-vmunet model. This might involve exploring architectural enhancements, such as incorporating attention mechanisms or other advanced techniques, to improve segmentation accuracy. Investigating the model’s performance on other types of medical images and diseases could broaden its applicability and impact on healthcare.
<br class="ltx_break"/>This comparative study emphasizes the potential of advanced deep learning models, notably the High-order Vision Mamba UNet (H-vmunet), in significantly enhancing the accuracy and efficiency of prostate cancer diagnosis through improved histopathology image segmentation. As artificial intelligence continues to advance, ongoing research and development efforts are pivotal in unlocking its full potential to drive progress in patient care and outcomes.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This study presents a comprehensive comparative analysis of three deep learning-based methods, Mamba, SAM, and YOLO, for segmenting prostate cancer histopathology images, utilizing the Gleason 2019 and SICAPv2 datasets. The findings reveal that the High-order Vision Mamba UNet (H-vmunet) model surpasses the other models in terms of segmentation accuracy, achieving the highest scores across all evaluated metrics. This superiority is attributed to H-vmunet’s innovative architecture, which integrates high-order visual state spaces and 2D-selective-scan operations, enabling efficient and sensitive lesion detection across different scales.
<br class="ltx_break"/>The results underscore the potential of H-vmunet for clinical applications, particularly in enhancing the accuracy and efficiency of prostate cancer diagnosis through histopathology image segmentation. They highlight the importance of robust validation and comparison of deep learning-based methods for medical image analysis, contributing to the development of accurate and reliable computer-aided diagnosis systems for prostate cancer.
<br class="ltx_break"/>While Mamba, SAM, and YOLO demonstrate promising aspects, such as Mamba’s adaptability and SAM’s flexibility in processing prompts, their performance in segmenting prostate cancer histopathology images does not match that of H-vmunet. This suggests that for the specific task of prostate cancer histopathology image segmentation, specialized architectures and methodologies, like those employed by H-vmunet, are more effective.
<br class="ltx_break"/>The integration of such advanced models into clinical workflows necessitates addressing computational challenges and ensuring rigorous validation against established clinical standards. Future research should focus on enhancing H-vmunet’s computational efficiency, boosting model generalization, and investigating its application in other medical imaging tasks. This study contributes to the broader discussion on the future of AI-assisted diagnostics in oncology, emphasizing the critical role of advanced deep learning models in improving patient outcomes through precise and efficient image segmentation.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Mircea-Sebastian Şerbănescu, Carmen-Nicoleta Oancea, Costin Teodor Streba, Iancu Emil Pleşea, Daniel Pirici, Liliana Streba, and Răzvan Mihail Pleşea.

</span>
<span class="ltx_bibblock">Agreement of two pre-trained deep-learning neural networks built with transfer learning with six pathologists on 6000 patches of prostate cancer from gleason2019 challenge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Romanian Journal of Morphology and Embryology</span>, 61(2):513, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Pooya Esmaeil Akhoondi and Mahdieh Soleymani Baghshah.

</span>
<span class="ltx_bibblock">Semantic segmentation with multiple contradictory annotations using a dynamic score function.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">IEEE Access</span>, 11:64544–64558, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Yechan Mun, Inyoung Paik, Su-Jin Shin, Tae-Yeong Kwak, and Hyeyoon Chang.

</span>
<span class="ltx_bibblock">Yet another automated gleason grading system (yaaggs) by weakly supervised deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">npj Digital Medicine</span>, 4(1):99, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Navid Rabbani and Adrien Bartoli.

</span>
<span class="ltx_bibblock">Unsupervised confidence approximation: Trustworthy learning from noisy labelled data.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 4609–4617, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
S Maryam Hosseini, Abubakr Shafique, Morteza Babaie, and Hamid R Tizhoosh.

</span>
<span class="ltx_bibblock">Class-imbalanced unsupervised and semi-supervised domain adaptation for histopathology images.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">2023 45th Annual International Conference of the IEEE Engineering in Medicine &amp; Biology Society (EMBC)</span>, pages 1–7. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Yi-hong Zhang, Jing Zhang, Yang Song, Chaomin Shen, and Guang Yang.

</span>
<span class="ltx_bibblock">Gleason score prediction using deep learning in tissue microarray image.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">arXiv preprint arXiv:2005.04886</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Adrien Foucart, Olivier Debeir, and Christine Decaestecker.

</span>
<span class="ltx_bibblock">Shortcomings and areas for improvement in digital pathology image segmentation challenges.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">Computerized Medical Imaging and Graphics</span>, 103:102155, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ozan Ciga, Tony Xu, and Anne Louise Martel.

</span>
<span class="ltx_bibblock">Self supervised contrastive learning for digital histopathology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">Machine Learning with Applications</span>, 7:100198, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Jasper Linmans, Emiel Hoogeboom, Jeroen van der Laak, and Geert Litjens.

</span>
<span class="ltx_bibblock">The latent doctor model for modeling inter-observer variability.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">IEEE journal of biomedical and health informatics</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Arne Schmidt, Pablo Morales-Alvarez, and Rafael Molina.

</span>
<span class="ltx_bibblock">Probabilistic modeling of inter-and intra-observer variability in medical image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the IEEE/CVF International Conference on Computer Vision</span>, pages 21097–21106, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Yali Qiu, Yujin Hu, Peiyao Kong, Hai Xie, Xiaoliu Zhang, Jiuwen Cao, Tianfu Wang, and Baiying Lei.

</span>
<span class="ltx_bibblock">Automatic prostate gleason grading using pyramid semantic parsing network in digital histopathology.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">Frontiers in oncology</span>, 12:772403, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Muhammad Mohsin, Arslan Shaukat, Usman Akram, and Muhammad Kaab Zarrar.

</span>
<span class="ltx_bibblock">Automatic prostate cancer grading using deep architectures.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">2021 IEEE/ACS 18th International Conference on Computer Systems and Applications (AICCSA)</span>, pages 1–8. IEEE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Adrien Foucart, Olivier Debeir, and Christine Decaestecker.

</span>
<span class="ltx_bibblock">Processing multi-expert annotations in digital pathology: a study of the gleason 2019 challenge.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">17th International Symposium on Medical Information Processing and Analysis</span>, volume 12088, pages 287–294. SPIE, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Guy Nir, Soheil Hor, Davood Karimi, Ladan Fazli, Brian F Skinnider, Peyman Tavassoli, Dmitry Turbin, Carlos F Villamil, Gang Wang, R Storey Wilson, et al.

</span>
<span class="ltx_bibblock">Automatic grading of prostate cancer in digitized histopathology images: Learning from multiple experts.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Medical image analysis</span>, 50:167–180, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Trinh Thi Le Vuong and Jin Tae Kwak.

</span>
<span class="ltx_bibblock">Moma: Momentum contrastive learning with multi-head attention-based knowledge distillation for histopathology image analysis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2308.16561</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Olaf Ronneberger, Philipp Fischer, and Thomas Brox.

</span>
<span class="ltx_bibblock">U-net: Convolutional networks for biomedical image segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">Medical image computing and computer-assisted intervention–MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18</span>, pages 234–241. Springer, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2010.11929</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Majid Vafaeezadeh, Hamid Behnam, Ali Hosseinsabet, and Parisa Gifani.

</span>
<span class="ltx_bibblock">Carpnet: Transformer for mitral valve disease classification in echocardiographic videos.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">International Journal of Imaging Systems and Technology</span>, 33(5):1505–1514, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Parisa Gifani, Ahmad Shalbaf, and Majid Vafaeezadeh.

</span>
<span class="ltx_bibblock">Automated detection of covid-19 using ensemble of transfer learning with deep convolutional neural network based on ct scans.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">International journal of computer assisted radiology and surgery</span>, 16:115–123, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Majid Vafaeezadeh, Hamid Behnam, Ali Hosseinsabet, and Parisa Gifani.

</span>
<span class="ltx_bibblock">A deep learning approach for the automatic recognition of prosthetic mitral valve in echocardiographic images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">Computers in Biology and Medicine</span>, 133:104388, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Majid Vafaeezadeh, Hamid Behnam, and Parisa Gifani.

</span>
<span class="ltx_bibblock">Ultrasound image analysis with vision transformers.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">Diagnostics</span>, 14(5):542, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Parisa Gifani, Majid Vafaeezadeh, Mahdi Ghorbani, Ghazal Mehri-Kakavand, Mohamad Pursamimi, Ahmad Shalbaf, and Amirhossein Abbaskhani Davanloo.

</span>
<span class="ltx_bibblock">Automatic diagnosis of stage of covid-19 patients using an ensemble of transfer learning with convolutional neural networks based on computed tomography images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Journal of Medical Signals &amp; Sensors</span>, 13(2):101–109, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ahmad Shalbaf, Parisa Gifani, Ghazal Mehri-Kakavand, Mohamad Pursamimi, Mahdi Ghorbani, Amirhossein Abbaskhani Davanloo, and Majid Vafaeezadeh.

</span>
<span class="ltx_bibblock">Automatic diagnosis of severity of covid-19 patients using an ensemble of transfer learning models with convolutional neural networks in ct images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">Polish Journal of Medical Physics and Engineering</span>, 28(3):117–126, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Majid Vafaeezadeh, Hamid Behnam, Ali Hosseinsabet, and Parisa Gifani.

</span>
<span class="ltx_bibblock">Automatic morphological classification of mitral valve diseases in echocardiographic images based on explainable deep learning methods.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">International Journal of Computer Assisted Radiology and Surgery</span>, 17(2):413–425, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Jun Ma, Feifei Li, and Bo Wang.

</span>
<span class="ltx_bibblock">U-mamba: Enhancing long-range dependency for biomedical image segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2401.04722</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jiarun Liu, Hao Yang, Hong-Yu Zhou, Yan Xi, Lequan Yu, Yizhou Yu, Yong Liang, Guangming Shi, Shaoting Zhang, Hairong Zheng, et al.

</span>
<span class="ltx_bibblock">Swin-umamba: Mamba-based unet with imagenet-based pretraining.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2402.03302</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Renkai Wu, Yinghao Liu, Pengchen Liang, and Qing Chang.

</span>
<span class="ltx_bibblock">H-vmunet: High-order vision mamba unet for medical image segmentation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2403.13642</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Julio Silva-Rodríguez, Adrián Colomer, María A Sales, Rafael Molina, and Valery Naranjo.

</span>
<span class="ltx_bibblock">Going deeper through the gleason scoring scale: An automatic end-to-end system for histology prostate grading and cribriform pattern detection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Computer methods and programs in biomedicine</span>, 195:105637, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
John N Weinstein, Eric A Collisson, Gordon B Mills, Kenna R Shaw, Brad A Ozenberger, Kyle Ellrott, Ilya Shmulevich, Chris Sander, and Joshua M Stuart.

</span>
<span class="ltx_bibblock">The cancer genome atlas pan-cancer analysis project.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Nature genetics</span>, 45(10):1113–1120, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Eirini Arvaniti, Kim S Fricker, Michael Moret, Niels Rupp, Thomas Hermanns, Christian Fankhauser, Norbert Wey, Peter J Wild, Jan H Rueschoff, and Manfred Claassen.

</span>
<span class="ltx_bibblock">Automated gleason grading of prostate cancer tissue microarrays via deep learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Scientific reports</span>, 8(1):12054, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Wouter Bulten, Kimmo Kartasalo, Po-Hsuan Cameron Chen, Peter Ström, Hans Pinckaers, Kunal Nagpal, Yuannan Cai, David F Steiner, Hester Van Boven, Robert Vink, et al.

</span>
<span class="ltx_bibblock">Artificial intelligence for diagnosis and gleason grading of prostate cancer: the panda challenge.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Nature medicine</span>, 28(1):154–163, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Valentin Anklin, Pushpak Pati, Guillaume Jaume, Behzad Bozorgtabar, Antonio Foncubierta-Rodriguez, Jean-Philippe Thiran, Mathilde Sibony, Maria Gabrani, and Orcun Goksel.

</span>
<span class="ltx_bibblock">Learning whole-slide segmentation from inexact and incomplete labels using tissue graphs.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">Medical Image Computing and Computer Assisted Intervention–MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021, Proceedings, Part II 24</span>, pages 636–646. Springer, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Radhakrishna Achanta, Appu Shaji, Kevin Smith, Aurelien Lucchi, Pascal Fua, and Sabine Süsstrunk.

</span>
<span class="ltx_bibblock">Slic superpixels compared to state-of-the-art superpixel methods.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">IEEE transactions on pattern analysis and machine intelligence</span>, 34(11):2274–2282, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-Chieh Chen.

</span>
<span class="ltx_bibblock">Mobilenetv2: Inverted residuals and linear bottlenecks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</span>, pages 4510–4520, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.

</span>
<span class="ltx_bibblock">Imagenet: A large-scale hierarchical image database.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">2009 IEEE conference on computer vision and pattern recognition</span>, pages 248–255. Ieee, 2009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Fedde K Potjer.

</span>
<span class="ltx_bibblock">Region adjacency graphs and connected morphological operators.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">Mathematical Morphology and Its Applications to Image and Signal Processing</span>, pages 111–118, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Jiacheng Ruan, Suncheng Xiang, Mingye Xie, Ting Liu, and Yuzhuo Fu.

</span>
<span class="ltx_bibblock">Malunet: A multi-attention and light-weight unet for skin lesion segmentation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">2022 IEEE International Conference on Bioinformatics and Biomedicine (BIBM)</span>, pages 1150–1156. IEEE, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang.

</span>
<span class="ltx_bibblock">Vision mamba: Efficient visual representation learning with bidirectional state space model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">arXiv preprint arXiv:2401.09417</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Albert Gu and Tri Dao.

</span>
<span class="ltx_bibblock">Mamba: Linear-time sequence modeling with selective state spaces.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2312.00752</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Yuhao Huang, Xin Yang, Lian Liu, Han Zhou, Ao Chang, Xinrui Zhou, Rusi Chen, Junxuan Yu, Jiongquan Chen, Chaoyu Chen, et al.

</span>
<span class="ltx_bibblock">Segment anything model for medical images?

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Medical Image Analysis</span>, 92:103061, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.

</span>
<span class="ltx_bibblock">Masked autoencoders are scalable vision learners.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span>, pages 16000–16009, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
M Srikar and K Malathi.

</span>
<span class="ltx_bibblock">An improved moving object detection in a wide area environment using image classification and recognition by comparing you only look once (yolo) algorithm over deformable part models (dpm) algorithm.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">Journal of Pharmaceutical Negative Results</span>, pages 1701–1707, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Khaoula Drid, Mebarka Allaoui, and Mohammed Lamine Kherfi.

</span>
<span class="ltx_bibblock">Object detector combination for increasing accuracy and detecting more overlapping objects.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">International Conference on Image and Signal Processing</span>, pages 290–296. Springer, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Hrishitva Patel.

</span>
<span class="ltx_bibblock">A comprehensive study on object detection techniques in unconstrained environments.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2304.05295</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
Rizwan Qureshi, MOHAMMED GAMAL RAGAB, SAID JADID ABDULKADER, ALAWI ALQUSHAIB, EBRAHIM HAMID SUMIEA, Hitham Alhussian, et al.

</span>
<span class="ltx_bibblock">A comprehensive systematic review of yolo for medical object detection (2018 to 2023).

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">Authorea Preprints</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Mario Rašić, Mario Tropčić, Pjetra Karlović, Dragana Gabrić, Marko Subašić, and Predrag Knežević.

</span>
<span class="ltx_bibblock">Detection and segmentation of radiolucent lesions in the lower jaw on panoramic radiographs using deep neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Medicina</span>, 59(12):2138, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Neng Yu and Li Li.

</span>
<span class="ltx_bibblock">Pharmacy infusion bottle detection based on improved yolov8.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">2023 2nd International Conference on Artificial Intelligence, Human-Computer Interaction and Robotics (AIHCIR)</span>, pages 237–242. IEEE, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Yongjie Liu, Yang Li, Mingfeng Jiang, Shuchao Wang, Shitai Ye, Simon Walsh, and Guang Yang.

</span>
<span class="ltx_bibblock">Socr-yolo: Small objects detection algorithm in medical images.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">International Journal of Imaging Systems and Technology</span>, 34(4):e23130, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Mupparaju Sohan, Thotakura Sai Ram, Rami Reddy, and Ch Venkata.

</span>
<span class="ltx_bibblock">A review on yolov8 and its advancements.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">International Conference on Data Intelligence and Cognitive Informatics</span>, pages 529–545. Springer, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Javaria Farooq, Muhammad Muaz, Khurram Khan Jadoon, Nayyer Aafaq, and Muhammad Khizer Ali Khan.

</span>
<span class="ltx_bibblock">An improved yolov8 for foreign object debris detection with optimized architecture for small objects.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">Multimedia Tools and Applications</span>, 83(21):60921–60947, 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Wed Oct  2 18:17:41 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
