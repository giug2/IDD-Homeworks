<article class="ltx_document ltx_authors_1line">
 <h1 class="ltx_title ltx_title_document">
  LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark
 </h1>
 <div class="ltx_authors">
  <span class="ltx_creator ltx_role_author">
   <span class="ltx_personname">
    Zhenfei Yin
    <sup class="ltx_sup" id="id1.1.id1">
     1, 3
    </sup>
    ,
Jiong Wang
    <sup class="ltx_sup" id="id2.2.id2">
     1, 4
    </sup>
    <span class="ltx_note ltx_role_footnotemark" id="footnotex1">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_note_type">
        footnotemark:
       </span>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
      </span>
     </span>
    </span>
    ,
Jianjian Cao
    <sup class="ltx_sup" id="id3.3.id3">
     1, 4
    </sup>
    <span class="ltx_note ltx_role_footnotemark" id="footnotex2">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_note_type">
        footnotemark:
       </span>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
      </span>
     </span>
    </span>
    ,
Zhelun Shi
    <sup class="ltx_sup" id="id4.4.id4">
     1, 2
    </sup>
    <span class="ltx_note ltx_role_footnotemark" id="footnotex3">
     <sup class="ltx_note_mark">
      1
     </sup>
     <span class="ltx_note_outer">
      <span class="ltx_note_content">
       <sup class="ltx_note_mark">
        1
       </sup>
       <span class="ltx_note_type">
        footnotemark:
       </span>
       <span class="ltx_tag ltx_tag_note">
        1
       </span>
      </span>
     </span>
    </span>
    ,
Dingning Liu
    <sup class="ltx_sup" id="id5.5.id5">
     1, 5
    </sup>
    ,
Mukai Li
    <sup class="ltx_sup" id="id6.6.id6">
     1
    </sup>
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_bold" id="id7.7.id7">
     Xiaoshui Huang
     <sup class="ltx_sup" id="id7.7.id7.1">
      1
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id8.8.id8">
     Zhiyong Wang
     <sup class="ltx_sup" id="id8.8.id8.1">
      3
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id9.9.id9">
     Lu Sheng
     <sup class="ltx_sup" id="id9.9.id9.1">
      2
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id10.10.id10">
     Lei Bai
     <sup class="ltx_sup" id="id10.10.id10.1">
      1
     </sup>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id11.11.id11">
     Jing Shao
     <sup class="ltx_sup" id="id11.11.id11.1">
      1
     </sup>
     <span class="ltx_note ltx_role_footnotemark" id="footnotex4">
      <sup class="ltx_note_mark">
       2
      </sup>
      <span class="ltx_note_outer">
       <span class="ltx_note_content">
        <sup class="ltx_note_mark">
         2
        </sup>
        <span class="ltx_note_type">
         footnotemark:
        </span>
        <span class="ltx_tag ltx_tag_note">
         <span class="ltx_text ltx_font_medium" id="footnotex4.1.1.1">
          2
         </span>
        </span>
       </span>
      </span>
     </span>
    </span>
    ,
    <span class="ltx_text ltx_font_bold" id="id12.12.id12">
     Wanli Ouyang
     <sup class="ltx_sup" id="id12.12.id12.1">
      1
     </sup>
    </span>
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id13.13.id13">
     1
    </sup>
    Shanghai Artificial Intelligence Laboratory
    <br class="ltx_break"/>
    <sup class="ltx_sup" id="id14.14.id14">
     2
    </sup>
    Beihang University
    <sup class="ltx_sup" id="id15.15.id15">
     3
    </sup>
    The University of Sydney
    <sup class="ltx_sup" id="id16.16.id16">
     4
    </sup>
    Fudan University
    <sup class="ltx_sup" id="id17.17.id17">
     5
    </sup>
    Dalian University of Technology
    <br class="ltx_break"/>
    <span class="ltx_text ltx_font_typewriter" id="id18.18.id18" style="font-size:90%;">
     {yinzhenfei,bailei,shaojing}@pjlab.org.cn
     <span class="ltx_text ltx_font_serif" id="id18.18.id18.1">
     </span>
    </span>
   </span>
   <span class="ltx_author_notes">
    Equal Contribution
    <span class="ltx_text ltx_font_bold" id="id19.19.id1">
     Corresponding Authors: Jing Shao (shaojing@pjlab.org.cn) and Lei Bai (bailei@pjlab.org.cn)
    </span>
   </span>
  </span>
 </div>
 <div class="ltx_abstract">
  <h6 class="ltx_title ltx_title_abstract">
   Abstract
  </h6>
  <p class="ltx_p" id="id20.id1">
   Large language models have emerged as a promising approach towards achieving general-purpose AI agents.
The thriving open-source LLM community has greatly accelerated the development of agents that support human-machine dialogue interaction through natural language processing. However, human interaction with the world extends beyond only text as a modality, and other modalities such as vision are also crucial.
Recent works on multi-modal large language models, such as GPT-4V and Bard, have demonstrated their effectiveness in handling visual modalities. However, the transparency of these works is limited and insufficient to support academic research.
To the best of our knowledge, we present one of the very first open-source endeavors in the field, LAMM, encompassing a Language-Assisted Multi-Modal instruction tuning dataset, framework, and benchmark. Our aim is to establish LAMM as a growing ecosystem for training and evaluating MLLMs, with a specific focus on facilitating AI agents capable of bridging the gap between ideas and execution, thereby enabling seamless human-AI interaction.
Our main contribution is three-fold:
1) We present a comprehensive dataset and benchmark, which cover a wide range of vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark.
2) We outline the detailed methodology of constructing multi-modal instruction tuning datasets and benchmarks for MLLMs, enabling rapid scaling and extension of MLLM research to diverse domains, tasks, and modalities.
3) We provide a primary but potential MLLM training framework optimized for modality extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research.
Our baseline model is trained within 24 A100 GPU hours, framework supports training with V100 and RTX3090 is available thanks to the open-source society.
Codes and data are now available at
   <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openlamm.github.io/" target="_blank" title="">
    https://openlamm.github.io/
   </a>
   .
  </p>
 </div>
 <section class="ltx_section" id="S1">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    1
   </span>
   Introduction
  </h2>
  <div class="ltx_para" id="S1.p1">
   <p class="ltx_p" id="S1.p1.1">
    Humans interact with the real world through multi-modal information, such as vision and language, since each modality possesses unique capabilities to describe the world, thereby providing us with richer information to construct our world model.
Developing AI agents capable of processing such multi-modal information, learning and memorizing world knowledge from it, and comprehending open-world instructions from humans to take actions and complete complex tasks has long been a core aspiration in artificial intelligence.
   </p>
  </div>
  <div class="ltx_para" id="S1.p2">
   <p class="ltx_p" id="S1.p2.1">
    Large Language Models (LLM) have made remarkable progress toward achieving that aspiration. ChatGPT and GPT-4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    model can directly comprehend user intents and generalize to unknown real-world tasks
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib2" title="">
      2
     </a>
     ]
    </cite>
    . LLM has become a universal task interface for general purposes. Almost all natural language understanding and generation tasks can be transformed into instruction inputs, enabling a single LLM to perform zero-shot generalization on various downstream applications
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib3" title="">
      3
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib5" title="">
      5
     </a>
     ]
    </cite>
    .
Within the realm of open-source models, the LLaMA series
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib7" title="">
      7
     </a>
     ]
    </cite>
    stands out for its performance and transparency. Building upon the LLaMA ecosystem, models like Alpaca
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ]
    </cite>
    and Vicuna
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ]
    </cite>
    employ different strategies, such as utilizing various machine-generated high-quality instruction-following samples, to enhance the performance of LLMs, showcasing impressive results.
Notably, these efforts are all text-only. While Multi-model Large Language Models (MLLM) like GPT-4V
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    and Bard
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    demonstrate remarkable capabilities in processing visual inputs, unfortunately, they are not currently available for use within the open-source academic community.
   </p>
  </div>
  <div class="ltx_para" id="S1.p3">
   <p class="ltx_p" id="S1.p3.1">
    Hence, we present LAMM, encompassing the Language-Assisted Multi-Modal instruction tuning dataset, framework, and benchmark. As one of the very first open-source endeavors in MLLMs, our aim is to establish LAMM as a thriving ecosystem for training and evaluating MLLMs, and further empower us to cultivate multi-modal AI agents capable of bridging the gap between ideas and execution, facilitating seamless interaction between humans and AI machines.
In this work, LLMs serve as the universal task interface, with inputs from vision tokens provided by pre-trained multi-modal encoders and language instructions. The powerful modeling capability of LLMs, combined with a unified optimization objective, can help align the model to various modalities.
This design sets LAMM apart from visual foundation models
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib11" title="">
      11
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib12" title="">
      12
     </a>
     ]
    </cite>
    , where each model is finely tuned for a specific task, or from multi-modal visual language foundation models that can only be used as pre-trained models for visual tasks or possess limited zero-shot capabilities
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib13" title="">
      13
     </a>
     ]
    </cite>
    , or from multi-task foundation models struggle in tag-of-war problems
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib14" title="">
      14
     </a>
     ]
    </cite>
    .
   </p>
  </div>
  <div class="ltx_para" id="S1.p4">
   <p class="ltx_p" id="S1.p4.1">
    Thoroughly, we present a novel instruction tuning dataset, which extends the research of MLLMs to both image and point cloud. Our dataset emphasizes fine-grained information and factual knowledge. Additionally, we introduce the very first attempt of a benchmark for MLLMs that offers a comprehensive evaluation of existing open-source models on various computer vision tasks, with two new evaluation strategies designed explicitly for multi-modal language models. We conduct over 200 experiments to provide extensive results and valuable observations on the capabilities and limitations of MLLMs. Also, we establish an extensible framework to facilitate the extension of multi-modal language models to additional modalities. Our baseline model surpasses existing multi-modal language models in downstream tasks related to images, demonstrating the effectiveness of our framework and dataset.
Above all, we have open-sourced our complete codebase for training and evaluating MLLMs, instruction tuning dataset covering both image and point cloud. various baseline models trained with our dataset and framework utilizing different settings to promote the development of an open research community for MLLMs.
   </p>
  </div>
  <div class="ltx_para" id="S1.p5">
   <p class="ltx_p" id="S1.p5.1">
    <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">
     Dataset
    </span>
    We include an image instruction tuning dataset containing 186,098 image-language instruction-response pairs and a point cloud instruction tuning dataset with 10,262 point cloud-language instruction-response pairs.
Motivated by LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ]
    </cite>
    and GPT-4V
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib10" title="">
      10
     </a>
     ]
    </cite>
    , we collect images and point clouds from publicly available datasets and use the GPT-API through self-instruction
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    methods to generate instructions and responses based on the original labels from these datasets. The resulting dataset has three appealing properties: 1) To emphasize fine-grained and dense information, we add more visual information, such as visual relationships and fine-grained categories as input for the GPT-API. 2) We observe on our benchmark that existing MLLMs may struggle to understand vision task instructions. To address this, we designed a method to convert vision task annotations into instruction-response pairs, which enhances MLLMs’ understanding and generalization of vision task instructions. 3) Considering the vulnerability of LLMs to the hallucination on factual knowledge, our dataset also includes data pairs for commonsense knowledge question answering by incorporating a hierarchical knowledge graph label system from the Bamboo
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib17" title="">
      17
     </a>
     ]
    </cite>
    dataset and the corresponding Wikipedia description.
   </p>
  </div>
  <div class="ltx_para" id="S1.p6">
   <p class="ltx_p" id="S1.p6.1">
    <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">
     Benchmark
    </span>
    We evaluate 9 common image tasks, using a total of 11 datasets with over 62,439 samples, and 3 common point cloud tasks, by utilizing 3 datasets with over 12,788 data samples, while existing works only provide quantitative results on fine-tuning and evaluating specific datasets such as ScienceQA, and most works only conduct demonstration or user studies. 1) We are the very first attempt to establish a benchmark for MLLMs. We conducted a comprehensive benchmark to quantify the zero-shot and fine-tuning performance of existing multi-modal language models on various computer vision tasks and compare them against state-of-the-art methods of these tasks, including classification, object detection, pose estimation, visual question answering, facial classification, optical character recognition, object counting. 2) We also attempted two novel evaluation strategies designed explicitly for MLLMs. Specifically, as for language performance on text generation, we established a scoring logic based on the GPT-API. And for tasks involving interactions between localization points and query images, such as object detection and pose estimation, we proposed an object-locating evaluation method.
   </p>
  </div>
  <div class="ltx_para" id="S1.p7">
   <p class="ltx_p" id="S1.p7.1">
    <span class="ltx_text ltx_font_bold" id="S1.p7.1.1">
     Framework
    </span>
    To validate the effectiveness of our dataset, we propose a primary but potential MLLM training framework. To avoid modality conflicts caused by introducing multiple modalities, we differentiate the encoder, projector, and LLM finetuning blocks for different modalities in the framework design. Meanwhile, by adding encoders and decoders for other modalities, our framework can flexibly extend to cover more modalities and tasks, such as video understanding, image synthesis, and so on. We provide the results of our baseline models trained using this framework on our benchmark, and present various observations to accelerate future research.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S2">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    2
   </span>
   Related Work
  </h2>
  <div class="ltx_para" id="S2.p1">
   <p class="ltx_p" id="S2.p1.1">
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.1">
     Multimodal Large Language Model.
    </span>
    With the rapid development of Large Language Models (LLM) such as ChatGPT, GPT-4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib1" title="">
      1
     </a>
     ]
    </cite>
    , many studies manage to explore incorporating other modalities based on LLM and they can be categorized into two perspectives.
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.2">
     1) System Design Perspective:
    </span>
    Visual ChatGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib18" title="">
      18
     </a>
     ]
    </cite>
    and MMREACT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib19" title="">
      19
     </a>
     ]
    </cite>
    invoke various vision foundation models by processing user query to investigate the visual roles of ChatGPT with the help of Visual Foundation Models.
ViperGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib20" title="">
      20
     </a>
     ]
    </cite>
    instructs LLM to parse visual queries into interpretable steps expressed by Python code.
HuggingGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib21" title="">
      21
     </a>
     ]
    </cite>
    extends its framework to more modalities by integrating more expert models on Huggingface.
    <span class="ltx_text ltx_font_bold" id="S2.p1.1.3">
     2) End-to-End Trainable Model Perspective:
    </span>
    The other methodology is to connect models for different modalities into an end-to-end trainable model, also known as multimodal large language model.
Flamingo
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib22" title="">
      22
     </a>
     ]
    </cite>
    proposes a unified architecture for language and vision modeling, while BLIP-2
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib23" title="">
      23
     </a>
     ]
    </cite>
    introduces a Querying Transformer to connect information from image to text modality.
Kosmos
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib4" title="">
      4
     </a>
     ]
    </cite>
    and PaLM-E
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib24" title="">
      24
     </a>
     ]
    </cite>
    build an end-to-end trainable framework on web-scale multi-modal corpora.
With the open-sourced LLaMA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib6" title="">
      6
     </a>
     ]
    </cite>
    , Mini-GPT4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib25" title="">
      25
     </a>
     ]
    </cite>
    optimizes a trainable projection matrix only, which connects pre-trained BLIP-2 style vision encoder and large language model, while LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ]
    </cite>
    and mPLUG-OwL
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib26" title="">
      26
     </a>
     ]
    </cite>
    also finetune LLM.
Besides feeding visual info to LLM as input only, LLaMA-Adapter
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib27" title="">
      27
     </a>
     ]
    </cite>
    , Multi-modal GPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib28" title="">
      28
     </a>
     ]
    </cite>
    and Otter
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib29" title="">
      29
     </a>
     ]
    </cite>
    also integrate multi modal information with intermediate features in LLM.
   </p>
  </div>
  <div class="ltx_para" id="S2.p2">
   <p class="ltx_p" id="S2.p2.1">
    <span class="ltx_text ltx_font_bold" id="S2.p2.1.1">
     Instruction Tuning.
    </span>
    Instruction tuning
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib30" title="">
      30
     </a>
     ]
    </cite>
    is a method proposed to improve the ability of large language models to follow instructions and enhance downstream task performance. Instruction-tuned models like InstructGPT
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib31" title="">
      31
     </a>
     ]
    </cite>
    , OPT-IML
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib32" title="">
      32
     </a>
     ]
    </cite>
    , Alpaca
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ]
    </cite>
    , have shown promising improvement compared to their based model. The existing instruction tuning datasets are primarily derived from collections of academic datasets like FLAN
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib30" title="">
      30
     </a>
     ]
    </cite>
    , chatbot data collected from ChatGPT usage such as ShareGPT, or constructed using self-instruction
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    methods like Alpaca.
Apart from pure text instruction tuning datasets, Multi-Instruct
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib33" title="">
      33
     </a>
     ]
    </cite>
    covers 47 multi-modal tasks.
Mini-GPT4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib25" title="">
      25
     </a>
     ]
    </cite>
    constructs instruction following dataset by composing image-text datasets and handwritten instruction templates.
Moreover, LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ]
    </cite>
    feeds captions and bounding boxes as the context of COCO images to GPT-4 and therefore get 150K instruction data.
Otter
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib29" title="">
      29
     </a>
     ]
    </cite>
    builds such instruction tuning datasets from multi-modal MMC4 dataset
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib34" title="">
      34
     </a>
     ]
    </cite>
    and incorporates in-contextual examples into instruction tuning by grouping similar instructions together.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S3">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    3
   </span>
   Dataset
  </h2>
  <figure class="ltx_figure" id="S3.F1">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="438" id="S3.F1.g1" src="/html/2306.06687/assets/x1.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 1:
    </span>
    Overview of our dataset, demonstrating the process of constructing our Instruction Tuning dataset using the GPT-API.
By designing different system messages, in-context learning pairs, and queries, we have created the dataset that covers almost all high-level vision tasks for both 2D and 3D vision. The dataset includes four distinct groups: n-round Daily Dialogue, n-round Factual Knowledge Dialogue, 1-round Detailed Description, and 1-round Visual Dialogue. It is worth noting that for the introduction of vision tasks, we only used the GPT-API to generate instruction-response templates and did not directly generate dialogue data. Finally, some examples of the dataset are presented below, including 2D and 3D scenes and their corresponding instruction-response pairs.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S3.p1">
   <p class="ltx_p" id="S3.p1.1">
    We introduce a comprehensive multi-modal instruction tuning dataset, which involves images and point clouds from publicly available datasets for diverse vision tasks, as well as high-quality instructions and responses based on the GPT-API and self-instruction methods
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    .
To be specific, our dataset contains 186K language-image instruction-response pairs, and 10K lanuage-3D instruction-response pairs.
Figure
    <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3 Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    provides an overview of its construction process. We provide detailed information on how to construct the multi-modal instruction tuning dataset to guide the academic community, facilitating the replication and further development of our work. We showcase additional demonstrations of sample data and provide a complete prompting method in the Appendix.
   </p>
  </div>
  <div class="ltx_para" id="S3.p2">
   <p class="ltx_p" id="S3.p2.1">
    We design four kinds of multi-modal instruction-response pairs: 1)
    <em class="ltx_emph ltx_font_italic" id="S3.p2.1.1">
     C1: n-round daily dialogue
    </em>
    focuses on multi-modal daily conversations.
2)
    <em class="ltx_emph ltx_font_italic" id="S3.p2.1.2">
     C2: n-round factual knowledge dialogue
    </em>
    aims at dialogues requiring factual knowledge reasoning.
3)
    <em class="ltx_emph ltx_font_italic" id="S3.p2.1.3">
     C3: 1-round detailed description
    </em>
    aims to elaborate images and 3D scenes in texts.
4)
    <em class="ltx_emph ltx_font_italic" id="S3.p2.1.4">
     C4: 1-round visual task dialogue
    </em>
    transfers vision tasks into instruction-response pairs, aiming at enhancing generalization ability towards visual tasks.
   </p>
  </div>
  <div class="ltx_para" id="S3.p3">
   <p class="ltx_p" id="S3.p3.1">
    We include diverse 2D and 3D vision tasks into the dataset, such as captioning, scene graph recognition and VQA that are directly compatible with natural languages, as well as classification, detection, counting and OCR that output labels, bounding boxes, digits and a list of words instead.
Note that the point-cloud instruction tuning dataset does not include data in the
    <em class="ltx_emph ltx_font_italic" id="S3.p3.1.1">
     C2: n-round factual knowledge dialogue
    </em>
    category. This is due to the current lack of publicly available 3D datasets with a well-defined labeling system containing factual knowledge.
In our dataset, the instruction-response pairs are gathered from 8 image datasets and 4 point cloud datasets, which are referred in Figure
    <a class="ltx_ref" href="#S3.F1" title="Figure 1 ‣ 3 Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      1
     </span>
    </a>
    .
   </p>
  </div>
  <div class="ltx_para" id="S3.p4">
   <p class="ltx_p" id="S3.p4.1">
    The first three types of instruction-response pairs are generated by inputting several special designed prompts to the GPT-API, namely
    <em class="ltx_emph ltx_font_italic" id="S3.p4.1.1">
     system messages
    </em>
    ,
    <em class="ltx_emph ltx_font_italic" id="S3.p4.1.2">
     in-context learning pairs
    </em>
    and
    <em class="ltx_emph ltx_font_italic" id="S3.p4.1.3">
     queries
    </em>
    : (1)
    <em class="ltx_emph ltx_font_italic" id="S3.p4.1.4">
     System messages
    </em>
    are to inform the GPT-API about the task definitions and requirements.
(2) Several
    <em class="ltx_emph ltx_font_italic" id="S3.p4.1.5">
     in-context learning pairs
    </em>
    are manually annotated to ensure that the rest instruction-response pairs can be generated by a similar fashion.
(3)
    <em class="ltx_emph ltx_font_italic" id="S3.p4.1.6">
     Queries
    </em>
    include comprehensive annotations of captions, bounding boxes of objects, relations between objects, factual knowledges from the Bamboo’s label system and their Wikipedia descriptions.
   </p>
  </div>
  <div class="ltx_para" id="S3.p5">
   <p class="ltx_p" id="S3.p5.1">
    The last type of instruction-response pairs also apply the system messages and in-context learning pairs, but use GPT-API to generate a pool of templates of instruction-response pairs instead. In this way, ground-truth annotations of many vision tasks, such as object/keypoint detection, OCR, counting and
    <em class="ltx_emph ltx_font_italic" id="S3.p5.1.1">
     etc
    </em>
    .
    <span class="ltx_text" id="S3.p5.1.2">
    </span>
    , can be inserted into these templates, and thus are easier to be converted into reliable language responses, rather than aforementioned query-based conversion.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S4">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    4
   </span>
   Benchmark
  </h2>
  <div class="ltx_para" id="S4.p1">
   <p class="ltx_p" id="S4.p1.1">
    Different from LLaVA
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ]
    </cite>
    , MiniGPT4
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib25" title="">
      25
     </a>
     ]
    </cite>
    and mPLUG-owl
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib26" title="">
      26
     </a>
     ]
    </cite>
    that only provide demos and user studies to qualitatively evaluate the performances of their MLLMs, we propose the first benchmark of MLLMs, which instead evaluates the quantitative performance of MLLMs on various 2D and 3D vision tasks.
It includes an inference pipeline and a set of evaluation metrics.
To be specific, the benchmark on 2D vision tasks evaluates 9 common image tasks, using a total of 11 datasets with over 62,439 samples. The benchmark on 3D vision tasks evaluates 3 common point cloud tasks, by utilizing 3 datasets with over 12,788 data samples.
   </p>
  </div>
  <div class="ltx_para" id="S4.p2">
   <p class="ltx_p" id="S4.p2.1">
    <span class="ltx_text ltx_font_bold" id="S4.p2.1.1">
     Inference Pipeline.
    </span>
    It ensures that the MLLMs can produce reasonable responses that can be fairly evaluated, which includes the way of processing input instructions and the extracting output entities.
We construct the
    <em class="ltx_emph ltx_font_italic" id="S4.p2.1.2">
     Inference Instruction
    </em>
    to help the model better understand the task it is performing and the output structure that is required, aim to improve the stability and reliability of the benchmarking process. Inference Instruction includes Task Definition, Output Structure and the usually employed Query Questions, as shown in Figure
    <a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4 Benchmark ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    .
Inspired by chain-of-thought prompting methods
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib35" title="">
      35
     </a>
     ]
    </cite>
    , we also prompt the MLLM to perform complex reasoning followed by the final answer, so as to obtain a more reliable answer.
Then, we employ the Natural Language Toolkit (NLTK) and regular expression matching to extract entities from the output text. These entities act as the results.
   </p>
  </div>
  <figure class="ltx_figure" id="S4.F2">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="293" id="S4.F2.g1" src="/html/2306.06687/assets/x2.png" width="461"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 2:
    </span>
    An overview of our Benchmark. It includes both 2D and 3D pipelines, covering multiple computer vision tasks. For each task, we provide the task definition, output structure, and a set of questions as instructions to the MLLM model. Then the entity extraction is applied on the output to extract the key answer. The LAMM Evaluation is used to evaluate the model’s performance, which includes traditional metrics, binary-location metric and the GPT Metric.
   </figcaption>
  </figure>
  <div class="ltx_para" id="S4.p3">
   <p class="ltx_p" id="S4.p3.1">
    <span class="ltx_text ltx_font_bold" id="S4.p3.1.1">
     Evaluation Metrics.
    </span>
    The set of evaluation metrics includes Traditional Metrics, Binary Locating Metric, and GPT Metric. The Traditional Metrics are task-specific metrics from the listed 2D and 3D vision tasks, which are the most rigorous to evaluate how MLLMs handle vision tasks.
In the Binary Locating Metric, the model needs to output an approximated location of a recognized object through the instruction “output the position of the object”, whose result is considered true if it is within the object’s groundtruth bounding box. It is a straightforward metric to compare the localization ability of an MLLM model.
To evaluate the understanding and question-answering abilitis of MLLM models, we utilize the GPT metric to evaluate the answers’ relevance and accuracy to the groundtruth.
To be specific, we prompt GPT to assign scores to the outputs generated by each model through the instruction described in Figure
    <a class="ltx_ref" href="#S4.F2" title="Figure 2 ‣ 4 Benchmark ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      2
     </span>
    </a>
    . The scoring criteria were based on accuracy, relevance, fluency, logical coherence, and information richness.
   </p>
  </div>
  <div class="ltx_para" id="S4.p4">
   <p class="ltx_p" id="S4.p4.1">
    <span class="ltx_text ltx_font_bold" id="S4.p4.1.1">
     Evaluation Settings.
    </span>
    All 2D and 3D vision tasks can be evaluated in a zero-shot manner, where the testing data have no intersection with MLLM’s training data. Moreover, we also evaluate the finetuning ability of MLLMs on the test dataset about several mainstream tasks, such as detection, classification and VQA in 2D tasks, as well as detection, grounding and VQA in 3D tasks.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S5">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    5
   </span>
   Experiments and Results
  </h2>
  <section class="ltx_subsection" id="S5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.1
    </span>
    Framework
   </h3>
   <figure class="ltx_figure" id="S5.F3">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="99" id="S5.F3.g1" src="/html/2306.06687/assets/x3.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 3:
     </span>
     Framework of multi-modality language model. Each modality is encoded by corresponding pre-trained encoder and decoded by LLM. LLM is shared among modalities and trainable projection layers and LoRA parameters are modality-specific.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S5.SS1.p1">
    <p class="ltx_p" id="S5.SS1.p1.1">
     The overall framework of our baseline MLLM is depicted in Figure
     <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ 5.1 Framework ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     . Each modality, image or point cloud, is processed by corresponding encoder, whose features are then projected to the same feature space as the text embeddings by a trainable projection layer.
Instructions are directly tokenized by SentencePiece tokenizer
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib36" title="">
       36
      </a>
      ]
     </cite>
     , then the vision and text tokens are concatenated to feed into the LLM model.
To finetune LLM efficiently, we add LoRA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib37" title="">
       37
      </a>
      ]
     </cite>
     parameters to all projection layers in the self-attention layers. LoRA parameters for different vision modalities are not shared.
Multi-modal tokens are decoded by a shared LLM model and the corresponding LoRA parameters.
As shown in Figure
     <a class="ltx_ref" href="#S5.F3" title="Figure 3 ‣ 5.1 Framework ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     ,
only feature projectors and LoRA parameters are optimized during training.
We use Vicuna-13B
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib9" title="">
       9
      </a>
      ]
     </cite>
     ,
as our LLM.
Rank of LoRA modules are set to 32.
We train all parameters including projection layers and LoRA modules in a one-stage end-to-end fashion with 4 A100 GPUs.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS1.p2">
    <p class="ltx_p" id="S5.SS1.p2.1">
     Input images are resized to be 224
     <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.p2.1.m1.1">
      <semantics id="S5.SS1.p2.1.m1.1a">
       <mo id="S5.SS1.p2.1.m1.1.1" xref="S5.SS1.p2.1.m1.1.1.cmml">
        ×
       </mo>
       <annotation-xml encoding="MathML-Content" id="S5.SS1.p2.1.m1.1b">
        <times id="S5.SS1.p2.1.m1.1.1.cmml" xref="S5.SS1.p2.1.m1.1.1">
        </times>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="S5.SS1.p2.1.m1.1c">
        \times
       </annotation>
      </semantics>
     </math>
     224 and split into 256 patches.
We use CLIP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib38" title="">
       38
      </a>
      ]
     </cite>
     pre-trained ViT-L/14 and use image patch features output from transformer layers as image representations.
We follow the design of FrozenCLIP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib39" title="">
       39
      </a>
      ]
     </cite>
     to encode point clouds, in which point cloud is tokenized to be 256 tokens by PointNet++
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib40" title="">
       40
      </a>
      ]
     </cite>
     and further encoded by CLIP pretrained ViT-L/14.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.2
    </span>
    Results on Traditional Metrics
   </h3>
   <figure class="ltx_table" id="S5.T1">
    <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 1:
     </span>
     Comparison of Multi-modal Large Language Models on 2D vision tasks.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T1.9">
     <tr class="ltx_tr" id="S5.T1.9.10">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S5.T1.9.10.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_rule" style="width:100%;height:2.0pt;background:black;display:inline-block;">
       </span>
       <span class="ltx_text" id="S5.T1.9.10.1.1" style="font-size:90%;">
        Task
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S5.T1.9.10.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.10.2.1" style="font-size:90%;">
        Dataset
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S5.T1.9.10.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.10.3.1" style="font-size:90%;">
        Metric
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.10.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.10.4.1" style="font-size:90%;">
        LLaVA
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.9.10.4.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib15" title="">
         15
        </a>
        <span class="ltx_text" id="S5.T1.9.10.4.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.10.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.10.5.1" style="font-size:90%;">
        MiniGPT4
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.9.10.5.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib25" title="">
         25
        </a>
        <span class="ltx_text" id="S5.T1.9.10.5.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.10.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.10.6.1" style="font-size:90%;">
        mPLUG-owl
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.9.10.6.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib26" title="">
         26
        </a>
        <span class="ltx_text" id="S5.T1.9.10.6.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.10.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.10.7.1" style="font-size:90%;">
        LAMM
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.1.1">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S5.T1.1.1.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_rule" style="width:100%;height:2.0pt;background:black;display:inline-block;">
       </span>
       <span class="ltx_text" id="S5.T1.1.1.2.1" style="font-size:90%;">
        Classification
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S5.T1.1.1.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.1.1.3.1" style="font-size:90%;">
        CIFAR10
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.1.1.3.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib41" title="">
         41
        </a>
        <span class="ltx_text" id="S5.T1.1.1.3.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S5.T1.1.1.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.1.1.1.1" style="font-size:90%;">
        Acc
       </span>
       <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.1.1.1.m1.1">
        <semantics id="S5.T1.1.1.1.m1.1a">
         <mo id="S5.T1.1.1.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T1.1.1.1.m1.1.1.cmml">
          ↑
         </mo>
         <annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.m1.1b">
          <ci id="S5.T1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.m1.1.1">
           ↑
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S5.T1.1.1.1.m1.1c">
          \uparrow
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.1.1.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.1.1.4.1" style="font-size:90%;">
        60.83
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.1.1.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.1.1.5.1" style="font-size:90%;">
        46.22
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.1.1.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.1.1.6.1" style="font-size:90%;">
        42.5
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.1.1.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.1.1.7.1" style="font-size:90%;">
        37.9
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.2.2">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.2.2.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.2.2.2.1" style="font-size:90%;">
        Detection
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.2.2.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.2.2.3.1" style="font-size:90%;">
        VOC2012
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.2.2.3.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib42" title="">
         42
        </a>
        <span class="ltx_text" id="S5.T1.2.2.3.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.2.2.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.2.2.1.1" style="font-size:90%;">
        mAP
       </span>
       <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.2.2.1.m1.1">
        <semantics id="S5.T1.2.2.1.m1.1a">
         <mo id="S5.T1.2.2.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T1.2.2.1.m1.1.1.cmml">
          ↑
         </mo>
         <annotation-xml encoding="MathML-Content" id="S5.T1.2.2.1.m1.1b">
          <ci id="S5.T1.2.2.1.m1.1.1.cmml" xref="S5.T1.2.2.1.m1.1.1">
           ↑
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S5.T1.2.2.1.m1.1c">
          \uparrow
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.2.2.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.2.2.4.1" style="font-size:90%;">
        1.42
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.2.2.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.2.2.5.1" style="font-size:90%;">
        0.92
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.2.2.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.2.2.6.1" style="font-size:90%;">
        0.158
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.2.2.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.2.2.7.1" style="font-size:90%;">
        7.20
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.3.3">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.3.3.2" rowspan="2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.3.3.2.1" style="font-size:90%;">
        VQA
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.3.3.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.3.3.3.1" style="font-size:90%;">
        SQAimage
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.3.3.3.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib43" title="">
         43
        </a>
        <span class="ltx_text" id="S5.T1.3.3.3.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.3.3.1" rowspan="2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.3.3.1.1" style="font-size:90%;">
        Acc
        <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.3.3.1.1.m1.1">
         <semantics id="S5.T1.3.3.1.1.m1.1a">
          <mo id="S5.T1.3.3.1.1.m1.1.1" stretchy="false" xref="S5.T1.3.3.1.1.m1.1.1.cmml">
           ↑
          </mo>
          <annotation-xml encoding="MathML-Content" id="S5.T1.3.3.1.1.m1.1b">
           <ci id="S5.T1.3.3.1.1.m1.1.1.cmml" xref="S5.T1.3.3.1.1.m1.1.1">
            ↑
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S5.T1.3.3.1.1.m1.1c">
           \uparrow
          </annotation>
         </semantics>
        </math>
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.3.3.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.3.3.4.1" style="font-size:90%;">
        40.5
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.3.3.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.3.3.5.1" style="font-size:90%;">
        43.43
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.3.3.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.3.3.6.1" style="font-size:90%;">
        36.39
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.3.3.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.3.3.7.1" style="font-size:90%;">
        49.88
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.9.11">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S5.T1.9.11.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.11.1.1" style="font-size:90%;">
        AI2D
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.9.11.1.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib44" title="">
         44
        </a>
        <span class="ltx_text" id="S5.T1.9.11.1.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.11.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.11.2.1" style="font-size:90%;">
        18.13
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.11.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.11.3.1" style="font-size:90%;">
        Failed
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.11.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.11.4.1" style="font-size:90%;">
        19.31
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.11.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.9.11.5.1" style="font-size:90%;">
        20.92
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.4.4">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.4.4.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.4.4.2.1" style="font-size:90%;">
        Image Caption
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.4.4.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.4.4.3.1" style="font-size:90%;">
        flickr30k
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.4.4.3.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib45" title="">
         45
        </a>
        <span class="ltx_text" id="S5.T1.4.4.3.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.4.4.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.4.4.1.1" style="font-size:90%;">
        BLEU4
       </span>
       <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.4.4.1.m1.1">
        <semantics id="S5.T1.4.4.1.m1.1a">
         <mo id="S5.T1.4.4.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T1.4.4.1.m1.1.1.cmml">
          ↑
         </mo>
         <annotation-xml encoding="MathML-Content" id="S5.T1.4.4.1.m1.1b">
          <ci id="S5.T1.4.4.1.m1.1.1.cmml" xref="S5.T1.4.4.1.m1.1.1">
           ↑
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S5.T1.4.4.1.m1.1c">
          \uparrow
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.4.4.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.4.4.4.1" style="font-size:90%;">
        6.65
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.4.4.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.4.4.5.1" style="font-size:90%;">
        5.1
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.4.4.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.4.4.6.1" style="font-size:90%;">
        2.74
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.4.4.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.4.4.7.1" style="font-size:90%;">
        2.56
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.5.5">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.5.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.5.5.2.1" style="font-size:90%;">
        F-g classification
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.5.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.5.5.3.1" style="font-size:90%;">
        UCMerced
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.5.5.3.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib46" title="">
         46
        </a>
        <span class="ltx_text" id="S5.T1.5.5.3.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.5.5.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.5.5.1.1" style="font-size:90%;">
        Acc
       </span>
       <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.5.5.1.m1.1">
        <semantics id="S5.T1.5.5.1.m1.1a">
         <mo id="S5.T1.5.5.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T1.5.5.1.m1.1.1.cmml">
          ↑
         </mo>
         <annotation-xml encoding="MathML-Content" id="S5.T1.5.5.1.m1.1b">
          <ci id="S5.T1.5.5.1.m1.1.1.cmml" xref="S5.T1.5.5.1.m1.1.1">
           ↑
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S5.T1.5.5.1.m1.1c">
          \uparrow
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.5.5.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.5.5.4.1" style="font-size:90%;">
        47
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.5.5.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.5.5.5.1" style="font-size:90%;">
        33.6
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.5.5.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.5.5.6.1" style="font-size:90%;">
        32.5
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.5.5.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.5.5.7.1" style="font-size:90%;">
        18.23
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.6.6">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.6.6.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.6.6.2.1" style="font-size:90%;">
        Counting
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.6.6.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.6.6.3.1" style="font-size:90%;">
        FSC147
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.6.6.3.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib47" title="">
         47
        </a>
        <span class="ltx_text" id="S5.T1.6.6.3.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.6.6.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.6.6.1.1" style="font-size:90%;">
        MAE
       </span>
       <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T1.6.6.1.m1.1">
        <semantics id="S5.T1.6.6.1.m1.1a">
         <mo id="S5.T1.6.6.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T1.6.6.1.m1.1.1.cmml">
          ↓
         </mo>
         <annotation-xml encoding="MathML-Content" id="S5.T1.6.6.1.m1.1b">
          <ci id="S5.T1.6.6.1.m1.1.1.cmml" xref="S5.T1.6.6.1.m1.1.1">
           ↓
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S5.T1.6.6.1.m1.1c">
          \downarrow
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.6.6.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.6.6.4.1" style="font-size:90%;">
        56.2
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.6.6.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.6.6.5.1" style="font-size:90%;">
        Failed
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.6.6.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.6.6.6.1" style="font-size:90%;">
        60.67
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.6.6.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.6.6.7.1" style="font-size:90%;">
        46.88
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.7.7">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.7.7.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.7.7.2.1" style="font-size:90%;">
        OCR
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.7.7.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.7.7.3.1" style="font-size:90%;">
        SVT
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.7.7.3.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib48" title="">
         48
        </a>
        <span class="ltx_text" id="S5.T1.7.7.3.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.7.7.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.7.7.1.1" style="font-size:90%;">
        Word Acc
       </span>
       <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.7.7.1.m1.1">
        <semantics id="S5.T1.7.7.1.m1.1a">
         <mo id="S5.T1.7.7.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T1.7.7.1.m1.1.1.cmml">
          ↑
         </mo>
         <annotation-xml encoding="MathML-Content" id="S5.T1.7.7.1.m1.1b">
          <ci id="S5.T1.7.7.1.m1.1.1.cmml" xref="S5.T1.7.7.1.m1.1.1">
           ↑
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S5.T1.7.7.1.m1.1c">
          \uparrow
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.7.7.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.7.7.4.1" style="font-size:90%;">
        37.78
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.7.7.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.7.7.5.1" style="font-size:90%;">
        16.97
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.7.7.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.7.7.6.1" style="font-size:90%;">
        30.39
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.7.7.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.7.7.7.1" style="font-size:90%;">
        29.14
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.8.8">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.8.8.2" rowspan="2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.8.8.2.1" style="font-size:90%;">
        Facial Classification
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.8.8.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.8.8.3.1" style="font-size:90%;">
        CelebA(Smile)
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.8.8.3.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib49" title="">
         49
        </a>
        <span class="ltx_text" id="S5.T1.8.8.3.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.8.8.1" rowspan="2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.8.8.1.1" style="font-size:90%;">
        Acc
        <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.8.8.1.1.m1.1">
         <semantics id="S5.T1.8.8.1.1.m1.1a">
          <mo id="S5.T1.8.8.1.1.m1.1.1" stretchy="false" xref="S5.T1.8.8.1.1.m1.1.1.cmml">
           ↑
          </mo>
          <annotation-xml encoding="MathML-Content" id="S5.T1.8.8.1.1.m1.1b">
           <ci id="S5.T1.8.8.1.1.m1.1.1.cmml" xref="S5.T1.8.8.1.1.m1.1.1">
            ↑
           </ci>
          </annotation-xml>
          <annotation encoding="application/x-tex" id="S5.T1.8.8.1.1.m1.1c">
           \uparrow
          </annotation>
         </semantics>
        </math>
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.8.8.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.8.8.4.1" style="font-size:90%;">
        Failed
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.8.8.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.8.8.5.1" style="font-size:90%;">
        66.36
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.8.8.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.8.8.6.1" style="font-size:90%;">
        Failed
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.8.8.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.8.8.7.1" style="font-size:90%;">
        57.50
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.9.12">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S5.T1.9.12.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.12.1.1" style="font-size:90%;">
        CelebA(Hair)
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.9.12.1.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib49" title="">
         49
        </a>
        <span class="ltx_text" id="S5.T1.9.12.1.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.12.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T1.9.12.2.1" style="font-size:90%;">
        46.42
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.12.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.12.3.1" style="font-size:90%;">
        43.47
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.12.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.12.4.1" style="font-size:90%;">
        40.93
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center" id="S5.T1.9.12.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.12.5.1" style="font-size:90%;">
        56.96
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.9.9">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.9.9.2" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.9.2.1" style="font-size:90%;">
        Keypoints Detection
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.9.9.3" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.9.3.1" style="font-size:90%;">
        LSP
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T1.9.9.3.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib50" title="">
         50
        </a>
        <span class="ltx_text" id="S5.T1.9.9.3.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r ltx_border_t" id="S5.T1.9.9.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.9.1.1" style="font-size:90%;">
        PCK
       </span>
       <math alttext="\uparrow" class="ltx_Math" display="inline" id="S5.T1.9.9.1.m1.1">
        <semantics id="S5.T1.9.9.1.m1.1a">
         <mo id="S5.T1.9.9.1.m1.1.1" mathsize="90%" stretchy="false" xref="S5.T1.9.9.1.m1.1.1.cmml">
          ↑
         </mo>
         <annotation-xml encoding="MathML-Content" id="S5.T1.9.9.1.m1.1b">
          <ci id="S5.T1.9.9.1.m1.1.1.cmml" xref="S5.T1.9.9.1.m1.1.1">
           ↑
          </ci>
         </annotation-xml>
         <annotation encoding="application/x-tex" id="S5.T1.9.9.1.m1.1c">
          \uparrow
         </annotation>
        </semantics>
       </math>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.9.9.4" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.9.4.1" style="font-size:90%;">
        Failed
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.9.9.5" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.9.5.1" style="font-size:90%;">
        Failed
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.9.9.6" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.9.6.1" style="font-size:90%;">
        Failed
       </span>
      </td>
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_t" id="S5.T1.9.9.7" style="padding:0.9pt 0.0pt;">
       <span class="ltx_text" id="S5.T1.9.9.7.1" style="font-size:90%;">
        Failed
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T1.9.13">
      <td class="ltx_td ltx_nopad_l ltx_nopad_r ltx_align_center ltx_border_r" id="S5.T1.9.13.1" style="padding:0.9pt 0.0pt;">
       <span class="ltx_rule" style="width:100%;height:2.0pt;background:black;display:inline-block;">
       </span>
      </td>
      <td class="ltx_td" id="S5.T1.9.13.2" style="padding:0.9pt 0.0pt;">
      </td>
      <td class="ltx_td" id="S5.T1.9.13.3" style="padding:0.9pt 0.0pt;">
      </td>
      <td class="ltx_td" id="S5.T1.9.13.4" style="padding:0.9pt 0.0pt;">
      </td>
      <td class="ltx_td" id="S5.T1.9.13.5" style="padding:0.9pt 0.0pt;">
      </td>
      <td class="ltx_td" id="S5.T1.9.13.6" style="padding:0.9pt 0.0pt;">
      </td>
      <td class="ltx_td" id="S5.T1.9.13.7" style="padding:0.9pt 0.0pt;">
      </td>
     </tr>
    </table>
   </figure>
   <div class="ltx_para" id="S5.SS2.p1">
    <p class="ltx_p" id="S5.SS2.p1.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">
      Zero-shot Setting on 2D Vision Tasks.
     </span>
     Table
     <a class="ltx_ref" href="#S5.T1" title="Table 1 ‣ 5.2 Results on Traditional Metrics ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     shows the results of MLLM on 2D vision tasks by the Traditional Metrics. All the MLLM models were tested in a zero-shot setting.
Although MLLM models demonstrated certain abilities of recognizing open-vocabulary classes, understanding images, and answering questions, they performed poorly on tasks involving object localization, including object detection, counting and keypoints detection.
     <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.2">
      Localization-aware Tasks:
     </em>
     In detection tasks, our baseline model demonstrated stronger localization ability, but there is still a significant gap between the predicted and the ground-truth bounding boxes, indicating MLLMs’ weakness to output certain digits representing points and reasoning spatial information.
In counting tasks, the MLLM models showed a significant gap between the predicted and ground truth number of objects. MiniGPT4 failed in this task as it is unable to provide a specific number for most of the data.
As for the keypoints detection task, we asked the MLLM models to predict the position of each human keypoint in turn. However, all the predicted positions were not in an acceptable range. The MLLMs show a significant gap in this task, indicating that they have difficulty in accurately predicting the locations of the keypoints.
     <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.3">
      VQA Tasks:
     </em>
     Our baseline model demonstrated certain advantages in image understanding and multiple-choice question answering compared to other models.
Note that the LLaVA model we compared to was evaluated in the zero-shot setting.
Additionally, we removed the random choice process from the LLaVA evaluation to obtain a more straightforward evaluation.
     <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.4">
      Captioning Tasks:
     </em>
     All MLLM models performed poorly on image captioning.
We argue that BLEU4 is not an appropriate metric since longer captions may lead to lower scores, and MLLMs tend to output detailed description.
     <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.5">
      Classification Tasks:
     </em>
     In fine-grained classification tasks and face classification tasks, all MLLMs performed poorly. Specifically, on the CelebA (Smile) dataset, the LLaVA model outputs "yes" to all the queries, while the mPLUG model randomly gives predictions. However, regarding the CelebA (Hair) dataset, the MLLMs can recognize hair color since the ability to infer visual knowledge for color recognition is relatively straightforward.
These results suggest that the MLLM models may have difficulty in tasks that require fine-grained distinctions.
     <em class="ltx_emph ltx_font_italic" id="S5.SS2.p1.1.6">
      OCR Tasks:
     </em>
     As for OCR tasks, LLaVA can recognize and extract text from images. However, our baseline model performed poorly on this task. We provide more analysis of the results and identify several potential reasons for the performance gap in the Appendix.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS2.p2">
    <p class="ltx_p" id="S5.SS2.p2.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">
      Fine-tuning Setting on Image Tasks.
     </span>
     We also fine-tuned our baseline model on several vision datasets, including CIFAR10, VOC2012, and SQAimage. The results are shown in Table
     <a class="ltx_ref" href="#S5.T2" title="Table 2 ‣ 5.2 Results on Traditional Metrics ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       2
      </span>
     </a>
     .
The fine-tuned baseline achieved an accuracy of 91% on CIFAR10. It also achieved an mAP of 13% on VOC2012, in comparison with 4.8% in the zero-shot setting.
These results indicate that our baseline models can receive the ability of localizing objects after being fine-tuned on detection data.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS2.p3">
    <p class="ltx_p" id="S5.SS2.p3.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS2.p3.1.1">
      Zero-shot Setting on Point Cloud Tasks.
     </span>
     Table
     <a class="ltx_ref" href="#S5.T3" title="Table 3 ‣ 5.2 Results on Traditional Metrics ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       3
      </span>
     </a>
     shows the result of our baseline model on 3D scene understanding tasks, under the zero-shot and fine-tuning settings, respectively.
The results after finetuning are significantly better than the zero-shot setting, in all test tasks. Our baseline model finetuned on ScanQA multiple choice data almost achieves 100% accuracy, which may have an overfitting issue due to the narrow training/test gap and small scale of 3D dataset.
    </p>
   </div>
   <figure class="ltx_table" id="S5.T2">
    <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 2:
     </span>
     Results of our baseline model on selected 2D vision tasks. Both zero-shot test result and finetuned results reported. Metrics for classification and VQA is
     <span class="ltx_text ltx_font_bold" id="S5.T2.7.1">
      accuracy
     </span>
     , and that for object detection is
     <span class="ltx_text ltx_font_bold" id="S5.T2.8.2">
      mAP@0.5
     </span>
     .
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T2.9">
     <tr class="ltx_tr" id="S5.T2.9.1">
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.9.1.1">
       <span class="ltx_text" id="S5.T2.9.1.1.1" style="font-size:90%;">
        Task
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.9.1.2">
       <span class="ltx_text" id="S5.T2.9.1.2.1" style="font-size:90%;">
        Dataset
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T2.9.1.3">
       <span class="ltx_text" id="S5.T2.9.1.3.1" style="font-size:90%;">
        LAMM (Zero-Shot)
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T2.9.1.4">
       <span class="ltx_text" id="S5.T2.9.1.4.1" style="font-size:90%;">
        LAMM (Finetune)
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T2.9.2">
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.9.2.1">
       <span class="ltx_text" id="S5.T2.9.2.1.1" style="font-size:90%;">
        Classification
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.9.2.2">
       <span class="ltx_text" id="S5.T2.9.2.2.1" style="font-size:90%;">
        CIFAR10
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T2.9.2.2.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib41" title="">
         41
        </a>
        <span class="ltx_text" id="S5.T2.9.2.2.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T2.9.2.3">
       <span class="ltx_text" id="S5.T2.9.2.3.1" style="font-size:90%;">
        37.9
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.9.2.4">
       <span class="ltx_text" id="S5.T2.9.2.4.1" style="font-size:90%;">
        91.2
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T2.9.3">
      <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.9.3.1">
       <span class="ltx_text" id="S5.T2.9.3.1.1" style="font-size:90%;">
        Object Detection
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.9.3.2">
       <span class="ltx_text" id="S5.T2.9.3.2.1" style="font-size:90%;">
        VOC2012
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T2.9.3.2.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib42" title="">
         42
        </a>
        <span class="ltx_text" id="S5.T2.9.3.2.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T2.9.3.3">
       <span class="ltx_text" id="S5.T2.9.3.3.1" style="font-size:90%;">
        7.20
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T2.9.3.4">
       <span class="ltx_text" id="S5.T2.9.3.4.1" style="font-size:90%;">
        13.48
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T2.9.4">
      <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.9.4.1">
       <span class="ltx_text" id="S5.T2.9.4.1.1" style="font-size:90%;">
        VQA
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.9.4.2">
       <span class="ltx_text" id="S5.T2.9.4.2.1" style="font-size:90%;">
        SQAimage
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T2.9.4.2.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib43" title="">
         43
        </a>
        <span class="ltx_text" id="S5.T2.9.4.2.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T2.9.4.3">
       <span class="ltx_text" id="S5.T2.9.4.3.1" style="font-size:90%;">
        49.88
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.9.4.4">
       <span class="ltx_text" id="S5.T2.9.4.4.1" style="font-size:90%;">
        74.27
       </span>
      </td>
     </tr>
    </table>
   </figure>
   <figure class="ltx_table" id="S5.T3">
    <figcaption class="ltx_caption ltx_centering" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 3:
     </span>
     Results of 3D tasks. Metrics for 3D object detection and visual grounding is
     <span class="ltx_text ltx_font_bold" id="S5.T3.7.1">
      mAP@0.5
     </span>
     , and that for 3D VQA is
     <span class="ltx_text ltx_font_bold" id="S5.T3.8.2">
      accuracy
     </span>
     of multiple choice problem.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T3.9">
     <tr class="ltx_tr" id="S5.T3.9.1">
      <td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T3.9.1.1">
       <span class="ltx_text" id="S5.T3.9.1.1.1" style="font-size:90%;">
        Task
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S5.T3.9.1.2">
       <span class="ltx_text" id="S5.T3.9.1.2.1" style="font-size:90%;">
        Dataset
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T3.9.1.3">
       <span class="ltx_text" id="S5.T3.9.1.3.1" style="font-size:90%;">
        LAMM (Zero-Shot)
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T3.9.1.4">
       <span class="ltx_text" id="S5.T3.9.1.4.1" style="font-size:90%;">
        LAMM (Finetune)
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T3.9.2">
      <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.9.2.1">
       <span class="ltx_text" id="S5.T3.9.2.1.1" style="font-size:90%;">
        3D Object Detection
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.9.2.2">
       <span class="ltx_text" id="S5.T3.9.2.2.1" style="font-size:90%;">
        ScanNet
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T3.9.2.2.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib51" title="">
         51
        </a>
        <span class="ltx_text" id="S5.T3.9.2.2.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.9.2.3">
       <span class="ltx_text" id="S5.T3.9.2.3.1" style="font-size:90%;">
        9.3
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.9.2.4">
       <span class="ltx_text" id="S5.T3.9.2.4.1" style="font-size:90%;">
        11.89
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T3.9.3">
      <td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.9.3.1">
       <span class="ltx_text" id="S5.T3.9.3.1.1" style="font-size:90%;">
        Visual Grounding
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.9.3.2">
       <span class="ltx_text" id="S5.T3.9.3.2.1" style="font-size:90%;">
        ScanRefer
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T3.9.3.2.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib52" title="">
         52
        </a>
        <span class="ltx_text" id="S5.T3.9.3.2.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.9.3.3">
       <span class="ltx_text" id="S5.T3.9.3.3.1" style="font-size:90%;">
        Failed
       </span>
      </td>
      <td class="ltx_td ltx_align_center" id="S5.T3.9.3.4">
       <span class="ltx_text" id="S5.T3.9.3.4.1" style="font-size:90%;">
        3.38
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T3.9.4">
      <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T3.9.4.1">
       <span class="ltx_text" id="S5.T3.9.4.1.1" style="font-size:90%;">
        3D VQA
       </span>
      </td>
      <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T3.9.4.2">
       <span class="ltx_text" id="S5.T3.9.4.2.1" style="font-size:90%;">
        ScanQA
       </span>
       <cite class="ltx_cite ltx_citemacro_cite">
        <span class="ltx_text" id="S5.T3.9.4.2.2.1" style="font-size:90%;">
         [
        </span>
        <a class="ltx_ref" href="#bib.bib53" title="">
         53
        </a>
        <span class="ltx_text" id="S5.T3.9.4.2.3.2" style="font-size:90%;">
         ]
        </span>
       </cite>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T3.9.4.3">
       <span class="ltx_text" id="S5.T3.9.4.3.1" style="font-size:90%;">
        26.54
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T3.9.4.4">
       <span class="ltx_text" id="S5.T3.9.4.4.1" style="font-size:90%;">
        99.89
       </span>
      </td>
     </tr>
    </table>
   </figure>
   <figure class="ltx_table" id="S5.T4">
    <figcaption class="ltx_caption" style="font-size:90%;">
     <span class="ltx_tag ltx_tag_table">
      Table 4:
     </span>
     Comparison of results of Binary Locating Metric and GPT Metric of existing MLLMs. The Binary-Locating Metric is the accuracy of the predicted position, and the GPT Metric is the score from GPT response.
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="S5.T4.3">
     <tr class="ltx_tr" id="S5.T4.3.1">
      <td class="ltx_td ltx_border_r ltx_border_tt" id="S5.T4.3.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T4.3.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.1.2.1" style="font-size:90%;">
        LLaVA
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T4.3.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.1.3.1" style="font-size:90%;">
        MiniGPT4
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S5.T4.3.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.1.4.1" style="font-size:90%;">
        mPLUG-owl
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_tt" id="S5.T4.3.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.1.5.1" style="font-size:90%;">
        LAMM
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.3.2">
      <td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.2.1.1" style="font-size:90%;">
        Binary-Loc Metric
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.2.2.1" style="font-size:90%;">
        14.73
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.2.3.1" style="font-size:90%;">
        13.12
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T4.3.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.2.4.1" style="font-size:90%;">
        4.42
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.3.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.2.5.1" style="font-size:90%;">
        31.2
       </span>
      </td>
     </tr>
     <tr class="ltx_tr" id="S5.T4.3.3">
      <td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S5.T4.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.3.1.1" style="font-size:90%;">
        GPT Metric
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S5.T4.3.3.2.1" style="font-size:90%;">
        50.16
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.3.3.1" style="font-size:90%;">
        7.28
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S5.T4.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.3.4.1" style="font-size:90%;">
        41.88
       </span>
      </td>
      <td class="ltx_td ltx_align_center ltx_border_bb" id="S5.T4.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">
       <span class="ltx_text" id="S5.T4.3.3.5.1" style="font-size:90%;">
        48.44
       </span>
      </td>
     </tr>
    </table>
   </figure>
  </section>
  <section class="ltx_subsection" id="S5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.3
    </span>
    Results of Binary Locating Metric and GPT Metric
   </h3>
   <div class="ltx_para" id="S5.SS3.p1">
    <p class="ltx_p" id="S5.SS3.p1.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS3.p1.1.1">
      Binary Locating Metric.
     </span>
     Table
     <a class="ltx_ref" href="#S5.T4" title="Table 4 ‣ 5.2 Results on Traditional Metrics ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     shows the zero-shot results of the MLLMs on the proposed Binary Locating Metric and GPT Metric.
The Binary Locating Metric covers the data from VOC2012, FSC147, and LSP.
Since the our baseline model has been trained on a small amount of data with detection instructions, it significantly improves in localizing accuracy.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS3.p2">
    <p class="ltx_p" id="S5.SS3.p2.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">
      GPT Metric.
     </span>
     We calculated GPT scores
using a variety of tasks, including VQA, classification, captioning, as well as a small number of detection and counting tasks.
As shown in Table
     <a class="ltx_ref" href="#S5.T4" title="Table 4 ‣ 5.2 Results on Traditional Metrics ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , LLaVA surpasses other models in performance, while LAMM, although slightly lower than LLaVA, still outperforms Minigpt4 and mPLUG-owl by a wide margin.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="S5.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     5.4
    </span>
    Observation and Analysis
   </h3>
   <figure class="ltx_figure" id="S5.F4">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="322" id="S5.F4.g1" src="/html/2306.06687/assets/x4.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 4:
     </span>
     Observation and analysis on various tasks. (a) Visualization results on VOC2012. (b) Visualization results on CIFAR10. The right subfigure is from
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib54" title="">
       54
      </a>
      ]
     </cite>
     . (c) Results on Flickr30k.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S5.SS4.p1">
    <p class="ltx_p" id="S5.SS4.p1.1">
     We conducted dozens of experiments and observations on the MLLM model across various tasks to summarize its current capabilities and limitations.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p2">
    <p class="ltx_p" id="S5.SS4.p2.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p2.1.1">
      Better Performance in Counting Tasks with Small Number of Objects.
     </span>
     As shown in the Table
     <a class="ltx_ref" href="#S5.T1" title="Table 1 ‣ 5.2 Results on Traditional Metrics ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     , recent MLLMs perform poorly on counting tasks. In the FSC147 dataset, there are data samples with dozens or even hundreds of objects, and the MLLMs would reply with “I cannot accurately count the number” for such data samples.
Therefore, we conducted tests on the subset of the FSC147 dataset with less than 10 objects to evaluate the performance of the models on simple data, as shown in Figure
     <a class="ltx_ref" href="#S5.F5" title="Figure 5 ‣ 5.4 Observation and Analysis ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     (b).
The results show that the MLLMs are able to roughly estimate the number of specified objects in the image, but it is still unable to provide an exact numerical value.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p3">
    <p class="ltx_p" id="S5.SS4.p3.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p3.1.1">
      GPT Metric is More Appropriate Than BLEU.
     </span>
     Figure
     <a class="ltx_ref" href="#S5.F4" title="Figure 4 ‣ 5.4 Observation and Analysis ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     illustrates the comparison between the generated captions by LLaVA and LAMM on a sample data from the Flickr30k dataset. It is evident that LAMM model produces more detailed image descriptions. However, a notable drawback is the low correlation between its generated sentences and the ground truth sentences, which consequently results in the low BLEU scores indicated in Table
     <a class="ltx_ref" href="#S5.T1" title="Table 1 ‣ 5.2 Results on Traditional Metrics ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     .
Thus, we tried to adopt the GPT Metric to assess the relevance and accuracy of the model’s output captions to the ground truth captions.
GPT gives a higher score to LAMM model, compared to LLaVA, suggesting that our model is more able to generate high-quality, image-relevant text outputs. This observation also raises the possibility that using GPT-based metrics for evaluating captioning tasks instead of BLEU might offer a more effective evaluation criterion.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p4">
    <p class="ltx_p" id="S5.SS4.p4.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p4.1.1">
      Capable of Object Localization but Struggles with Precise Bounding Box Prediction.
     </span>
     We visualize the results of LLaVA on VOC2012 dataset.
Figure
     <a class="ltx_ref" href="#S5.F4" title="Figure 4 ‣ 5.4 Observation and Analysis ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     (a) shows that the LAMM model was able to roughly point out the bird in the image, but was unable to accurately locate the entire object.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p5">
    <p class="ltx_p" id="S5.SS4.p5.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p5.1.1">
      LAMM Model Exhibits Fine-Grained Classification Ability on CIFAR10.
     </span>
     As shown in Figure
     <a class="ltx_ref" href="#S5.F4" title="Figure 4 ‣ 5.4 Observation and Analysis ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     , when presented with a 32x32 pixel image of a car, the model’s prediction was a more granular category: “Fiat 500L 2012”, which accurately identifies the car’s brand and model.
The left sub figure in Figure
     <a class="ltx_ref" href="#S5.F4" title="Figure 4 ‣ 5.4 Observation and Analysis ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       4
      </span>
     </a>
     (b) shows the image of Fiat 500L 2012 on Autoevolution
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib54" title="">
       54
      </a>
      ]
     </cite>
     , revealing that it has very similar features to the input image from CIFAR10.
These results demonstrate that the MLLM trained with our dataset has the ability to perform more fine-grained classification, and is capable of recognizing subtle differences in images and assigning them to more specific categories.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p6">
    <p class="ltx_p" id="S5.SS4.p6.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p6.1.1">
      Instruction and Reasoning Enhance Performance on SQAimage Data
     </span>
     Following LLaVA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ]
     </cite>
     , we conducted experiments on the SQAimage dataset using different inference approaches, including prompts with or without reasoning or instruction. The prompts with reasoning make the MLLM output the reasoning process before presenting the final results. The prompts with instruction give MLLM the task definition and output structure to the question to help the model better understand the task.
The results in Figure
     <a class="ltx_ref" href="#S5.F5" title="Figure 5 ‣ 5.4 Observation and Analysis ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     (a) shows that the instruction and reasoning both improve the MLLM’s VQA ability.
These results highlight the importance of incorporating task-specific information and reasoning process into MLLMs.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p7">
    <p class="ltx_p" id="S5.SS4.p7.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p7.1.1">
      Difficulty in Comprehending Visual Information for Domain Shifted Data.
     </span>
     We conducted an analysis on several datasets that exhibit significant deviations from the training dataset, including UCMerced, CelebA, and LSP.
The UCMerced dataset consists of top-down views of scenes, CelebA is a facial dataset that can describe the expressions and hair colors, and the LSP dataset involves 14 key points of the human body, they are significantly different from the COCO dataset during the training phase.
These results suggest that the performance of the MLLM model may degrade significantly on datasets that exhibit significant deviations from the training dataset.
    </p>
   </div>
   <figure class="ltx_figure" id="S5.F5">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="143" id="S5.F5.g1" src="/html/2306.06687/assets/x5.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 5:
     </span>
     (a) Zero-shot Accuracy of LLaVA with different inputs on SQAimage. R. indicates reasoning and inst. indicates instruction. (b) Counting Performance on FSC147 of MLLMs. (c) Zero-shot accuracy of LAMM model trained on various data combinations on SQAimage. (d) Zero-shot accuracy of LAMM model trained additional instruction data in our dataset.
    </figcaption>
   </figure>
   <div class="ltx_para" id="S5.SS4.p8">
    <p class="ltx_p" id="S5.SS4.p8.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p8.1.1">
      Difficulty in Reading Text on SVT data.
     </span>
     We analyzed the performance of our baseline model on the SVT dataset and observed unsatisfactory results in Table
     <a class="ltx_ref" href="#S5.T1" title="Table 1 ‣ 5.2 Results on Traditional Metrics ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       1
      </span>
     </a>
     . A possible explanation is that we used the TextVQA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib55" title="">
       55
      </a>
      ]
     </cite>
     dataset to generate visual task dialogue, which is more geared towards conversational text rather than OCR-related vision tasks. This mismatch in dataset characteristics may have resulted in suboptimal generalization of our model to the SVT dataset. To address this issue, we intend to conduct further investigations and incorporate more appropriate OCR data during the training process to improve our model’s performance on OCR-related vision tasks.
    </p>
   </div>
   <div class="ltx_para" id="S5.SS4.p9">
    <p class="ltx_p" id="S5.SS4.p9.1">
     <span class="ltx_text ltx_font_bold" id="S5.SS4.p9.1.1">
      Data volume validation on SQAimage data.
     </span>
     As shown in Figure
     <a class="ltx_ref" href="#S5.F5" title="Figure 5 ‣ 5.4 Observation and Analysis ‣ 5 Experiments and Results ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     (c) (d), our four types of image instruction tuning datasets outperform LLaVA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ]
     </cite>
     on all subsets, resulting in a 7% overall performance improvement for the complete dataset.
Furthermore, we investigated the impact of sampling
     <em class="ltx_emph ltx_font_italic" id="S5.SS4.p9.1.2">
      Daily Dialogue
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="S5.SS4.p9.1.3">
      Detailed Description
     </em>
     data at different proportions. Notably, even with the small size of 10k examples, our dataset achieved comparable results to LLaVA-Dataset. As the dataset size increased, the overall performance of our model continuously improved, indicating that our dataset is scalable and can be further optimized by adding more data.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_section" id="S6">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    6
   </span>
   Limitations
  </h2>
  <div class="ltx_para" id="S6.p1">
   <p class="ltx_p" id="S6.p1.1">
    In this part, we discuss limitation and social impact of this work from perspectives of dataset, benchmark and framework.
   </p>
  </div>
  <div class="ltx_para" id="S6.p2">
   <p class="ltx_p" id="S6.p2.1">
    <span class="ltx_text ltx_font_bold" id="S6.p2.1.1">
     Dataset
    </span>
    In our study, we utilized GPT-API, a state-of-the-art language model, to generate the multi-modal instruction data. To achieve the desired format, which includes multi-round dialogue and one-round detailed descriptions, we provided system messages and example dialogues as guidance for the data generation process using GPT-API. The use of GPT-API for generating text-based conversations has been widely adopted in Natural Language Processing, and previous work in multi-modal data
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib8" title="">
      8
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib15" title="">
      15
     </a>
     ,
     <a class="ltx_ref" href="#bib.bib16" title="">
      16
     </a>
     ]
    </cite>
    has demonstrated promising results in various tasks.
   </p>
  </div>
  <div class="ltx_para" id="S6.p3">
   <p class="ltx_p" id="S6.p3.1">
    However, it is important to acknowledge the limitations inherent to the underlying GPT model, which are not altered by the use of GPT-API.
GPT-API lacks direct access to visual information and relies solely on textual context such as captions and attributes, which restricts its understanding of images and may result in missing detailed information.
While GPT-API excels at generating coherent and contextually relevant responses, it can occasionally produce responses that appear plausible but are factually incorrect or lack proper context. It may also struggle with understanding complex or ambiguous queries. Moreover, the generated data used for training may inadvertently reflect inherent biases and other truthworthy issues of GPT-API.
To address ethical concerns regarding data generated with GPT-API, we performed manual sampling to examine the data, ensuring that the generated data aligns with societal values, privacy, security, toxicity, and fairness requirements and expectations. In Appendix, we provide an evaluation of the data quality and showcase additional data samples. We also transparently provide the complete prompts used to invoke GPT-API, ensuring transparency throughout our work.
   </p>
  </div>
  <div class="ltx_para" id="S6.p4">
   <p class="ltx_p" id="S6.p4.1">
    <span class="ltx_text ltx_font_bold" id="S6.p4.1.1">
     Benchmark
    </span>
    LAMM evaluates MLLMs on formatted computer vision tasks and datasets. Due to the diversity of language models’ outputs, metrics may fluctuate across experiments. Additionally, LAMM currently adopts metrics such as GPT-eval and binary localization as an initial attempt to evaluate MLLMs’ performance. Further research is needed to enhance the stability of benchmark results and design more appropriate metrics, which can be a promising direction for future investigations.
   </p>
  </div>
  <div class="ltx_para" id="S6.p5">
   <p class="ltx_p" id="S6.p5.1">
    <span class="ltx_text ltx_font_bold" id="S6.p5.1.1">
     Framework
    </span>
    Our work establishes a simple MLLM framework to build up a baseline model for our dataset and benchmark. However, there is potential for further development and careful design of MLLMs for future work to enhance their capabilities and performance.
   </p>
  </div>
 </section>
 <section class="ltx_section" id="S7">
  <h2 class="ltx_title ltx_title_section">
   <span class="ltx_tag ltx_tag_section">
    7
   </span>
   Conclusion
  </h2>
  <div class="ltx_para" id="S7.p1">
   <p class="ltx_p" id="S7.p1.1">
    In conclusion, our work presents LAMM, an open-source endeavor in the field of multi-modal large language models. We introduce the image and point-cloud instruction tuning dataset and benchmark, aiming to establish LAMM as a thriving ecosystem for training and evaluating MLLMs. We also provide an extensible framework to facilitate the extension of MLLMs to additional modalities.
Our research showcases the effectiveness of MLLMs in handling visual modalities, including images and point clouds, and highlights their potential for generalization via instruction tuning. By making our codebase, baseline model, instruction tuning dataset, and evaluation benchmark publicly available, we aim to foster an open research community for MLLMs. We believe that our work will contribute to the advancement of MLLMs and the development of general-purpose multi-model agents.
   </p>
  </div>
  <div class="ltx_pagination ltx_role_newpage">
  </div>
 </section>
 <section class="ltx_section" id="Sx1">
  <h2 class="ltx_title ltx_title_section">
   Acknowledgement
  </h2>
  <div class="ltx_para" id="Sx1.p1">
   <p class="ltx_p" id="Sx1.p1.1">
    This work is done during Zhenfei Yin, Jiong Wang, Jianjian Cao, Zhelun Shi and Dingning Liu’s internship at Shanghai Artificial Intelligence Laboratory. This work is supported in part by the National Key R&amp;D Program of China (NO. 2022ZD0160100), and National Natural Science Foundation of China (62132001).
   </p>
  </div>
 </section>
 <section class="ltx_bibliography" id="bib">
  <h2 class="ltx_title ltx_title_bibliography">
   References
  </h2>
  <ul class="ltx_biblist">
   <li class="ltx_bibitem" id="bib.bib1">
    <span class="ltx_tag ltx_tag_bibitem">
     [1]
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     Gpt-4 technical report.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">
      ArXiv
     </span>
     , abs/2303.08774, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib2">
    <span class="ltx_tag ltx_tag_bibitem">
     [2]
    </span>
    <span class="ltx_bibblock">
     Hao Fu, Yao; Peng and Tushar Khot.
    </span>
    <span class="ltx_bibblock">
     How does gpt obtain its ability? tracing emergent abilities of
language models to their sources.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">
      Yao Fu’s Notion
     </span>
     , Dec 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib3">
    <span class="ltx_tag ltx_tag_bibitem">
     [3]
    </span>
    <span class="ltx_bibblock">
     Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming
Ma, and Furu Wei.
    </span>
    <span class="ltx_bibblock">
     Language models are general-purpose interfaces.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">
      arXiv preprint arXiv:2206.06336
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib4">
    <span class="ltx_tag ltx_tag_bibitem">
     [4]
    </span>
    <span class="ltx_bibblock">
     Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,
Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, et al.
    </span>
    <span class="ltx_bibblock">
     Language is not all you need: Aligning perception with language
models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">
      arXiv preprint arXiv:2302.14045
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib5">
    <span class="ltx_tag ltx_tag_bibitem">
     [5]
    </span>
    <span class="ltx_bibblock">
     Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu,
Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,
et al.
    </span>
    <span class="ltx_bibblock">
     Holistic evaluation of language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">
      arXiv preprint arXiv:2211.09110
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib6">
    <span class="ltx_tag ltx_tag_bibitem">
     [6]
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne
Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric
Hambro, Faisal Azhar, et al.
    </span>
    <span class="ltx_bibblock">
     Llama: Open and efficient foundation language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib6.1.1">
      arXiv preprint arXiv:2302.13971
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib7">
    <span class="ltx_tag ltx_tag_bibitem">
     [7]
    </span>
    <span class="ltx_bibblock">
     Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine
Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale,
et al.
    </span>
    <span class="ltx_bibblock">
     Llama 2: Open foundation and fine-tuned chat models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">
      arXiv preprint arXiv:2307.09288
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib8">
    <span class="ltx_tag ltx_tag_bibitem">
     [8]
    </span>
    <span class="ltx_bibblock">
     Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos
Guestrin, Percy Liang, and Tatsunori B. Hashimoto.
    </span>
    <span class="ltx_bibblock">
     Stanford alpaca: An instruction-following llama model.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/stanford_alpaca" target="_blank" title="">
      https://github.com/tatsu-lab/stanford_alpaca
     </a>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib9">
    <span class="ltx_tag ltx_tag_bibitem">
     [9]
    </span>
    <span class="ltx_bibblock">
     Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin
Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and
Eric P. Xing.
    </span>
    <span class="ltx_bibblock">
     Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt
quality, March 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib10">
    <span class="ltx_tag ltx_tag_bibitem">
     [10]
    </span>
    <span class="ltx_bibblock">
     OpenAI.
    </span>
    <span class="ltx_bibblock">
     Gpt-4v(ision) system card.
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cdn.openai.com/papers/GPTV_System_Card.pdf%22" target="_blank" title="">
      https://cdn.openai.com/papers/GPTV_System_Card.pdf"
     </a>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib11">
    <span class="ltx_tag ltx_tag_bibitem">
     [11]
    </span>
    <span class="ltx_bibblock">
     Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, et al.
    </span>
    <span class="ltx_bibblock">
     An image is worth 16x16 words: Transformers for image recognition at
scale.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">
      arXiv preprint arXiv:2010.11929
     </span>
     , 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib12">
    <span class="ltx_tag ltx_tag_bibitem">
     [12]
    </span>
    <span class="ltx_bibblock">
     Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney
von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma
Brunskill, et al.
    </span>
    <span class="ltx_bibblock">
     On the opportunities and risks of foundation models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">
      arXiv preprint arXiv:2108.07258
     </span>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib13">
    <span class="ltx_tag ltx_tag_bibitem">
     [13]
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.
    </span>
    <span class="ltx_bibblock">
     Learning transferable visual models from natural language
supervision.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">
      International conference on machine learning
     </span>
     , pages
8748–8763. PMLR, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib14">
    <span class="ltx_tag ltx_tag_bibitem">
     [14]
    </span>
    <span class="ltx_bibblock">
     Raia Hadsell, Dushyant Rao, Andrei A Rusu, and Razvan Pascanu.
    </span>
    <span class="ltx_bibblock">
     Embracing change: Continual learning in deep neural networks.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">
      Trends in cognitive sciences
     </span>
     , 24(12):1028–1040, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib15">
    <span class="ltx_tag ltx_tag_bibitem">
     [15]
    </span>
    <span class="ltx_bibblock">
     Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.
    </span>
    <span class="ltx_bibblock">
     Visual instruction tuning, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib16">
    <span class="ltx_tag ltx_tag_bibitem">
     [16]
    </span>
    <span class="ltx_bibblock">
     Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A Smith, Daniel
Khashabi, and Hannaneh Hajishirzi.
    </span>
    <span class="ltx_bibblock">
     Self-instruct: Aligning language model with self generated
instructions.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">
      arXiv preprint arXiv:2212.10560
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib17">
    <span class="ltx_tag ltx_tag_bibitem">
     [17]
    </span>
    <span class="ltx_bibblock">
     Yuanhan Zhang, Qinghong Sun, Yichun Zhou, Zexin He, Zhenfei Yin, Kun Wang,
Lu Sheng, Yu Qiao, Jing Shao, and Ziwei Liu.
    </span>
    <span class="ltx_bibblock">
     Bamboo: Building mega-scale vision dataset continually with
human-machine synergy, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib18">
    <span class="ltx_tag ltx_tag_bibitem">
     [18]
    </span>
    <span class="ltx_bibblock">
     Chenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan
Duan.
    </span>
    <span class="ltx_bibblock">
     Visual chatgpt: Talking, drawing and editing with visual foundation
models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">
      arXiv preprint arXiv:2303.04671
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib19">
    <span class="ltx_tag ltx_tag_bibitem">
     [19]
    </span>
    <span class="ltx_bibblock">
     Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal
Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, and Lijuan Wang.
    </span>
    <span class="ltx_bibblock">
     Mm-react: Prompting chatgpt for multimodal reasoning and action.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">
      arXiv preprint arXiv:2303.11381
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib20">
    <span class="ltx_tag ltx_tag_bibitem">
     [20]
    </span>
    <span class="ltx_bibblock">
     Dídac Surís, Sachit Menon, and Carl Vondrick.
    </span>
    <span class="ltx_bibblock">
     Vipergpt: Visual inference via python execution for reasoning.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">
      arXiv preprint arXiv:2303.08128
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib21">
    <span class="ltx_tag ltx_tag_bibitem">
     [21]
    </span>
    <span class="ltx_bibblock">
     Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting
Zhuang.
    </span>
    <span class="ltx_bibblock">
     Hugginggpt: Solving ai tasks with chatgpt and its friends in
huggingface.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">
      arXiv preprint arXiv:2303.17580
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib22">
    <span class="ltx_tag ltx_tag_bibitem">
     [22]
    </span>
    <span class="ltx_bibblock">
     Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr,
Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds,
et al.
    </span>
    <span class="ltx_bibblock">
     Flamingo: a visual language model for few-shot learning.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">
      Advances in Neural Information Processing Systems
     </span>
     ,
35:23716–23736, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib23">
    <span class="ltx_tag ltx_tag_bibitem">
     [23]
    </span>
    <span class="ltx_bibblock">
     Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
    </span>
    <span class="ltx_bibblock">
     Blip-2: Bootstrapping language-image pre-training with frozen image
encoders and large language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">
      arXiv preprint arXiv:2301.12597
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib24">
    <span class="ltx_tag ltx_tag_bibitem">
     [24]
    </span>
    <span class="ltx_bibblock">
     Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery,
Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al.
    </span>
    <span class="ltx_bibblock">
     Palm-e: An embodied multimodal language model.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">
      arXiv preprint arXiv:2303.03378
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib25">
    <span class="ltx_tag ltx_tag_bibitem">
     [25]
    </span>
    <span class="ltx_bibblock">
     Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.
    </span>
    <span class="ltx_bibblock">
     Minigpt-4: Enhancing vision-language understanding with advanced
large language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">
      arXiv preprint arXiv:2304.10592
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib26">
    <span class="ltx_tag ltx_tag_bibitem">
     [26]
    </span>
    <span class="ltx_bibblock">
     Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang
Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, Chaoya Jiang, Chenliang Li, Yuanhong
Xu, Hehong Chen, Junfeng Tian, Qian Qi, Ji Zhang, and Fei Huang.
    </span>
    <span class="ltx_bibblock">
     mplug-owl: Modularization empowers large language models with
multimodality, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib27">
    <span class="ltx_tag ltx_tag_bibitem">
     [27]
    </span>
    <span class="ltx_bibblock">
     Renrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu,
Hongsheng Li, Peng Gao, and Yu Qiao.
    </span>
    <span class="ltx_bibblock">
     Llama-adapter: Efficient fine-tuning of language models with
zero-init attention.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">
      arXiv preprint arXiv:2303.16199
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib28">
    <span class="ltx_tag ltx_tag_bibitem">
     [28]
    </span>
    <span class="ltx_bibblock">
     Tao Gong, Chengqi Lyu, Shilong Zhang, Yudong Wang, Miao Zheng, Qian Zhao,
Kuikun Liu, Wenwei Zhang, Ping Luo, and Kai Chen.
    </span>
    <span class="ltx_bibblock">
     Multimodal-gpt: A vision and language model for dialogue with humans.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">
      arXiv preprint arXiv:2305.04790
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib29">
    <span class="ltx_tag ltx_tag_bibitem">
     [29]
    </span>
    <span class="ltx_bibblock">
     Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu.
    </span>
    <span class="ltx_bibblock">
     Otter: A multi-modal model with in-context instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">
      arXiv preprint arXiv:2305.03726
     </span>
     , 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib30">
    <span class="ltx_tag ltx_tag_bibitem">
     [30]
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian
Lester, Nan Du, Andrew M Dai, and Quoc V Le.
    </span>
    <span class="ltx_bibblock">
     Finetuned language models are zero-shot learners.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">
      arXiv preprint arXiv:2109.01652
     </span>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib31">
    <span class="ltx_tag ltx_tag_bibitem">
     [31]
    </span>
    <span class="ltx_bibblock">
     Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela
Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
    </span>
    <span class="ltx_bibblock">
     Training language models to follow instructions with human feedback.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">
      Advances in Neural Information Processing Systems
     </span>
     ,
35:27730–27744, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib32">
    <span class="ltx_tag ltx_tag_bibitem">
     [32]
    </span>
    <span class="ltx_bibblock">
     Srinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel
Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, Xian
Li, Brian O’Horo, Gabriel Pereyra, Jeff Wang, Christopher Dewan, Asli
Celikyilmaz, Luke Zettlemoyer, and Ves Stoyanov.
    </span>
    <span class="ltx_bibblock">
     Opt-iml: Scaling language model instruction meta learning through the
lens of generalization, 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib33">
    <span class="ltx_tag ltx_tag_bibitem">
     [33]
    </span>
    <span class="ltx_bibblock">
     Zhiyang Xu, Ying Shen, and Lifu Huang.
    </span>
    <span class="ltx_bibblock">
     Multiinstruct: Improving multi-modal zero-shot learning via
instruction tuning.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">
      arXiv preprint arXiv:2212.10773
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib34">
    <span class="ltx_tag ltx_tag_bibitem">
     [34]
    </span>
    <span class="ltx_bibblock">
     Anas Awadalla, Irena Gao, Joshua Gardner, Jack Hessel, Yusuf Hanafy, Wanrong
Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Jenia Jitsev, Simon
Kornblith, Pang Wei Koh, Gabriel Ilharco, Mitchell Wortsman, and Ludwig
Schmidt.
    </span>
    <span class="ltx_bibblock">
     Openflamingo, March 2023.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib35">
    <span class="ltx_tag ltx_tag_bibitem">
     [35]
    </span>
    <span class="ltx_bibblock">
     Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V
Le, Denny Zhou, et al.
    </span>
    <span class="ltx_bibblock">
     Chain-of-thought prompting elicits reasoning in large language
models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">
      Advances in Neural Information Processing Systems
     </span>
     ,
35:24824–24837, 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib36">
    <span class="ltx_tag ltx_tag_bibitem">
     [36]
    </span>
    <span class="ltx_bibblock">
     Taku Kudo and John Richardson.
    </span>
    <span class="ltx_bibblock">
     SentencePiece: A simple and language independent subword
tokenizer and detokenizer for neural text processing.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">
      Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations
     </span>
     , pages 66–71, Brussels,
Belgium, November 2018. Association for Computational Linguistics.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib37">
    <span class="ltx_tag ltx_tag_bibitem">
     [37]
    </span>
    <span class="ltx_bibblock">
     Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean
Wang, Lu Wang, and Weizhu Chen.
    </span>
    <span class="ltx_bibblock">
     Lora: Low-rank adaptation of large language models.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">
      arXiv preprint arXiv:2106.09685
     </span>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib38">
    <span class="ltx_tag ltx_tag_bibitem">
     [38]
    </span>
    <span class="ltx_bibblock">
     Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
et al.
    </span>
    <span class="ltx_bibblock">
     Learning transferable visual models from natural language
supervision.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">
      International conference on machine learning
     </span>
     , pages
8748–8763. PMLR, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib39">
    <span class="ltx_tag ltx_tag_bibitem">
     [39]
    </span>
    <span class="ltx_bibblock">
     Xiaoshui Huang, Sheng Li, Wentao Qu, Tong He, Yifan Zuo, and Wanli Ouyang.
    </span>
    <span class="ltx_bibblock">
     Frozen clip model is efficient point cloud backbone.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">
      arXiv preprint arXiv:2212.04098
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib40">
    <span class="ltx_tag ltx_tag_bibitem">
     [40]
    </span>
    <span class="ltx_bibblock">
     Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas.
    </span>
    <span class="ltx_bibblock">
     Pointnet++: Deep hierarchical feature learning on point sets in a
metric space.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">
      arXiv preprint arXiv:1706.02413
     </span>
     , 2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib41">
    <span class="ltx_tag ltx_tag_bibitem">
     [41]
    </span>
    <span class="ltx_bibblock">
     A. Krizhevsky and G. Hinton.
    </span>
    <span class="ltx_bibblock">
     Learning multiple layers of features from tiny images.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">
      Handbook of Systemic Autoimmune Diseases
     </span>
     , 1(4), 2009.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib42">
    <span class="ltx_tag ltx_tag_bibitem">
     [42]
    </span>
    <span class="ltx_bibblock">
     M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zisserman.
    </span>
    <span class="ltx_bibblock">
     The PASCAL Visual Object Classes Challenge 2012 (VOC2012)
Results.
    </span>
    <span class="ltx_bibblock">
     http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib43">
    <span class="ltx_tag ltx_tag_bibitem">
     [43]
    </span>
    <span class="ltx_bibblock">
     Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu,
Oyvind Tafjord, Peter Clark, and Ashwin Kalyan.
    </span>
    <span class="ltx_bibblock">
     Learn to explain: Multimodal reasoning via thought chains for science
question answering.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">
      The 36th Conference on Neural Information Processing Systems
(NeurIPS)
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib44">
    <span class="ltx_tag ltx_tag_bibitem">
     [44]
    </span>
    <span class="ltx_bibblock">
     Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Min Joon Seo, Hannaneh
Hajishirzi, and Ali Farhadi.
    </span>
    <span class="ltx_bibblock">
     A diagram is worth A dozen images.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">
      CoRR
     </span>
     , abs/1603.07396, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib45">
    <span class="ltx_tag ltx_tag_bibitem">
     [45]
    </span>
    <span class="ltx_bibblock">
     Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier.
    </span>
    <span class="ltx_bibblock">
     From image descriptions to visual denotations: New similarity metrics
for semantic inference over event descriptions.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib45.1.1">
      Transactions of the Association for Computational Linguistics
     </span>
     ,
2:67–78, 2014.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib46">
    <span class="ltx_tag ltx_tag_bibitem">
     [46]
    </span>
    <span class="ltx_bibblock">
     Yi Yang and Shawn Newsam.
    </span>
    <span class="ltx_bibblock">
     Bag-of-visual-words and spatial extensions for land-use
classification.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">
      Proceedings of the 18th SIGSPATIAL international conference
on advances in geographic information systems
     </span>
     , pages 270–279, 2010.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib47">
    <span class="ltx_tag ltx_tag_bibitem">
     [47]
    </span>
    <span class="ltx_bibblock">
     Viresh Ranjan, Udbhav Sharma, Thu Nguyen, and Minh Hoai.
    </span>
    <span class="ltx_bibblock">
     Learning to count everything.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib47.1.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition (CVPR)
     </span>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib48">
    <span class="ltx_tag ltx_tag_bibitem">
     [48]
    </span>
    <span class="ltx_bibblock">
     Kai Wang, Boris Babenko, and Serge Belongie.
    </span>
    <span class="ltx_bibblock">
     End-to-end scene text recognition.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib48.1.1">
      2011 International conference on computer vision
     </span>
     , pages
1457–1464. IEEE, 2011.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib49">
    <span class="ltx_tag ltx_tag_bibitem">
     [49]
    </span>
    <span class="ltx_bibblock">
     Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang.
    </span>
    <span class="ltx_bibblock">
     Large-scale celebfaces attributes (celeba) dataset.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib49.1.1">
      Retrieved August
     </span>
     , 15(2018):11, 2018.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib50">
    <span class="ltx_tag ltx_tag_bibitem">
     [50]
    </span>
    <span class="ltx_bibblock">
     Sam Johnson and Mark Everingham.
    </span>
    <span class="ltx_bibblock">
     Clustered pose and nonlinear appearance models for human pose
estimation.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib50.1.1">
      British Machine Vision Conference
     </span>
     , 2010.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib51">
    <span class="ltx_tag ltx_tag_bibitem">
     [51]
    </span>
    <span class="ltx_bibblock">
     Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser,
and Matthias Nießner.
    </span>
    <span class="ltx_bibblock">
     Scannet: Richly-annotated 3d reconstructions of indoor scenes.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">
      Proc. Computer Vision and Pattern Recognition (CVPR), IEEE
     </span>
     ,
2017.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib52">
    <span class="ltx_tag ltx_tag_bibitem">
     [52]
    </span>
    <span class="ltx_bibblock">
     Dave Zhenyu Chen, Angel X Chang, and Matthias Nießner.
    </span>
    <span class="ltx_bibblock">
     Scanrefer: 3d object localization in rgb-d scans using natural
language.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">
      Computer Vision–ECCV 2020: 16th European Conference,
Glasgow, UK, August 23–28, 2020, Proceedings, Part XX
     </span>
     , pages 202–221.
Springer, 2020.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib53">
    <span class="ltx_tag ltx_tag_bibitem">
     [53]
    </span>
    <span class="ltx_bibblock">
     Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoki Kawanabe.
    </span>
    <span class="ltx_bibblock">
     Scanqa: 3d question answering for spatial scene understanding.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)
     </span>
     , 2022.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib54">
    <span class="ltx_tag ltx_tag_bibitem">
     [54]
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.autoevolution.com/cars/fiat-500l-2012.html#aeng_fiat-fiat-500l-2012-09l-105-hp-twinair" target="_blank" title="">
      https://www.autoevolution.com/cars/fiat-500l-2012.html#aeng_fiat-fiat-500l-2012-09l-105-hp-twinair
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib55">
    <span class="ltx_tag ltx_tag_bibitem">
     [55]
    </span>
    <span class="ltx_bibblock">
     Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv
Batra, Devi Parikh, and Marcus Rohrbach.
    </span>
    <span class="ltx_bibblock">
     Towards vqa models that can read.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR)
     </span>
     , June 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib56">
    <span class="ltx_tag ltx_tag_bibitem">
     [56]
    </span>
    <span class="ltx_bibblock">
     Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva
Ramanan, Piotr Dollár, and C. Lawrence Zitnick.
    </span>
    <span class="ltx_bibblock">
     Microsoft coco: Common objects in context, Jan 2014.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib57">
    <span class="ltx_tag ltx_tag_bibitem">
     [57]
    </span>
    <span class="ltx_bibblock">
     Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua
Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma,
Michael S. Bernstein, and Li Fei-Fei.
    </span>
    <span class="ltx_bibblock">
     Visual genome: Connecting language and vision using crowdsourced
dense image annotations.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib57.1.1">
      CoRR
     </span>
     , abs/1602.07332, 2016.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib58">
    <span class="ltx_tag ltx_tag_bibitem">
     [58]
    </span>
    <span class="ltx_bibblock">
     Xu Yan, Zhihao Yuan, Yuhao Du, Yinghong Liao, Yao Guo, Zhen Li, and Shuguang
Cui.
    </span>
    <span class="ltx_bibblock">
     Clevr3d: Compositional language and elementary visual reasoning for
question answering in 3d real-world scenes.
    </span>
    <span class="ltx_bibblock">
     <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">
      arXiv preprint arXiv:2112.11691
     </span>
     , 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib59">
    <span class="ltx_tag ltx_tag_bibitem">
     [59]
    </span>
    <span class="ltx_bibblock">
     Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias
Niessner.
    </span>
    <span class="ltx_bibblock">
     Rio: 3d object instance re-localization in changing indoor
environments.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib59.1.1">
      Proceedings IEEE International Conference on Computer Vision
(ICCV)
     </span>
     , 2019.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib60">
    <span class="ltx_tag ltx_tag_bibitem">
     [60]
    </span>
    <span class="ltx_bibblock">
     Shun-Cheng Wu, Johanna Wald, Keisuke Tateno, Nassir Navab, and Federico
Tombari.
    </span>
    <span class="ltx_bibblock">
     Scenegraphfusion: Incremental 3d scene graph prediction from rgb-d
sequences.
    </span>
    <span class="ltx_bibblock">
     In
     <span class="ltx_text ltx_font_italic" id="bib.bib60.1.1">
      Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition
     </span>
     , pages 7515–7525, 2021.
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib61">
    <span class="ltx_tag ltx_tag_bibitem">
     [61]
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Toad" target="_blank" title="">
      https://en.wikipedia.org/wiki/Toad
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib62">
    <span class="ltx_tag ltx_tag_bibitem">
     [62]
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://robbreport.com/motors/cars" target="_blank" title="">
      https://robbreport.com/motors/cars
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib63">
    <span class="ltx_tag ltx_tag_bibitem">
     [63]
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Dassault_Mirage_2000" target="_blank" title="">
      https://en.wikipedia.org/wiki/Dassault_Mirage_2000
     </a>
     .
    </span>
   </li>
   <li class="ltx_bibitem" id="bib.bib64">
    <span class="ltx_tag ltx_tag_bibitem">
     [64]
    </span>
    <span class="ltx_bibblock">
     <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://en.wikipedia.org/wiki/Police_car" target="_blank" title="">
      https://en.wikipedia.org/wiki/Police_car
     </a>
     .
    </span>
   </li>
  </ul>
 </section>
 <div class="ltx_pagination ltx_role_newpage">
 </div>
 <section class="ltx_appendix" id="Ax1">
  <h2 class="ltx_title ltx_title_appendix">
   Appendix
  </h2>
 </section>
 <section class="ltx_appendix" id="A1">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix A
   </span>
   Overview
  </h2>
  <div class="ltx_para" id="A1.p1">
   <p class="ltx_p" id="A1.p1.1">
    Dataset and code in LAMM has been open sourced at
    <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/OpenLAMM/LAMM" target="_blank" title="">
     https://github.com/OpenLAMM/LAMM
    </a>
    .
In this Appendix, we present construction pipeline and more examples of our dataset in Sec.
    <a class="ltx_ref" href="#A2" title="Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      B
     </span>
    </a>
    .
Then, Sec.
    <a class="ltx_ref" href="#A3" title="Appendix C Benchmark ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      C
     </span>
    </a>
    shows details of benchmark and related evaluation metrics.
Sec.
    <a class="ltx_ref" href="#A4" title="Appendix D Implementation Details ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      D
     </span>
    </a>
    presents implementation details of our framework. Training a model based on our framework takes about 20 A100 GPU hours.
Finally, more examples and results are visualized in Sec.
    <a class="ltx_ref" href="#A5" title="Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      E
     </span>
    </a>
    .
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A2">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix B
   </span>
   Dataset
  </h2>
  <div class="ltx_para" id="A2.p1">
   <p class="ltx_p" id="A2.p1.1">
    The paper introduces a novel method for constructing instruction tuning data, which represents an innovative departure from traditional techniques that rely solely on daily dialogue and detailed description. Instead, our dataset leverages additional factual knowledge extracted from Wikipedia to improve the quality and diversity of the training data. In addition, we also explore the use of traditional vision task data, covering common tasks in both 2D and 3D fields, which is converted into instruction tuning data for training purposes.
By combining our new data construction method with traditional vision task data, we aim to improve the accuracy and effectiveness of instruction-tuned models in various vision-related applications. Specifically, we delve into the design of 2D and 3D portion of our dataset in Section
    <a class="ltx_ref" href="#A2.SS1" title="B.1 Image Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      B.1
     </span>
    </a>
    and
    <a class="ltx_ref" href="#A2.SS2" title="B.2 Point Cloud Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      B.2
     </span>
    </a>
    , respectively. We also outlined the manual approach for checking the quality of the generated data in Section
    <a class="ltx_ref" href="#A2.SS3" title="B.3 Quality Check ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      B.3
     </span>
    </a>
    . Finally, we provide a comprehensive explanation of the license and social impact information of our dataset in Section
    <a class="ltx_ref" href="#A2.SS4" title="B.4 Social Impact ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      B.4
     </span>
    </a>
    .
   </p>
  </div>
  <section class="ltx_subsection" id="A2.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.1
    </span>
    Image Instruction Tuning Dataset
   </h3>
   <div class="ltx_para" id="A2.SS1.p1">
    <p class="ltx_p" id="A2.SS1.p1.1">
     <em class="ltx_emph ltx_font_bold ltx_font_italic" id="A2.SS1.p1.1.1">
      C1: n-round Daily Dialogue
     </em>
     <span class="ltx_text ltx_font_bold" id="A2.SS1.p1.1.2">
      &amp;
      <em class="ltx_emph ltx_font_italic" id="A2.SS1.p1.1.2.1">
       C3: 1-round Detailed Description
      </em>
      .
     </span>
     The first step of our approach involves incorporating more visual information, such as visual relationships and fine-grained categories as input to GPT-API, providing dense visual context to the generated responses. To construct the
     <em class="ltx_emph ltx_font_italic" id="A2.SS1.p1.1.3">
      C1: n-round Daily Dialogue
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="A2.SS1.p1.1.4">
      C3: 1-round Detailed Description
     </em>
     data, we use the COCO images
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib56" title="">
       56
      </a>
      ]
     </cite>
     , similar to the LLaVA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ]
     </cite>
     approach. However, we further extract object attributes and relationships from the Visual Genome dataset
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib57" title="">
       57
      </a>
      ]
     </cite>
     to emphasize fine-grained and dense information in the generated responses. Specifically, Our approach leverages image scene graph information to provide a structured representation of the objects and their relationships within the image. By doing so, we generated multi-modal dialogue data that enables us to capture the relationships between objects in the image and generate more accurate and natural language instructions.
Figures
     <a class="ltx_ref" href="#A2.F6" title="Figure 6 ‣ B.1 Image Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       6
      </span>
     </a>
     and
     <a class="ltx_ref" href="#A2.F7" title="Figure 7 ‣ B.1 Image Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       7
      </span>
     </a>
     display the messages utilized to generate daily dialogue and detailed description data in the GPT-API. Additionally, Figure
     <a class="ltx_ref" href="#A2.F8" title="Figure 8 ‣ B.1 Image Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       8
      </span>
     </a>
     provides detailed examples of the generated results for both types of data.
    </p>
   </div>
   <figure class="ltx_figure" id="A2.F6">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="247" id="A2.F6.g1" src="/html/2306.06687/assets/x6.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 6:
     </span>
     Messages used to construct
     <em class="ltx_emph ltx_font_italic" id="A2.F6.2.1">
      n-round Daily Dialogue
     </em>
     data for image instruction tuning.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A2.F7">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="218" id="A2.F7.g1" src="/html/2306.06687/assets/x7.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 7:
     </span>
     Messages used to construct
     <em class="ltx_emph ltx_font_italic" id="A2.F7.2.1">
      1-round Detailed Description
     </em>
     data for image instruction tuning.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A2.F8">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="261" id="A2.F8.g1" src="/html/2306.06687/assets/x8.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 8:
     </span>
     Example of generated
     <em class="ltx_emph ltx_font_italic" id="A2.F8.3.1">
      n-round daily dialogue
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="A2.F8.4.2">
      1-round detailed description
     </em>
     data.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A2.F9">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="268" id="A2.F9.g1" src="/html/2306.06687/assets/x9.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 9:
     </span>
     Messages used to construct
     <em class="ltx_emph ltx_font_italic" id="A2.F9.2.1">
      n-round Factual Knowledge Dialogue
     </em>
     data for image instruction tuning.
    </figcaption>
   </figure>
   <div class="ltx_para" id="A2.SS1.p2">
    <p class="ltx_p" id="A2.SS1.p2.1">
     <em class="ltx_emph ltx_font_bold ltx_font_italic" id="A2.SS1.p2.1.1">
      C2: n-round Factual Knowledge Dialogue
     </em>
     <span class="ltx_text ltx_font_bold" id="A2.SS1.p2.1.2">
      .
     </span>
     In the second step of our approach, we expand the dataset by incorporating 42K classes of knowledge graph facts from Wikipedia using the Bamboo dataset. This addition enables MLLMs to generate question-answering data related to factual knowledge, which is a valuable addition to the dataset. To generate
     <em class="ltx_emph ltx_font_italic" id="A2.SS1.p2.1.3">
      C2: n-round Factual Knowledge Dialogue
     </em>
     data, we utilize the Bamboo dataset and Wikipedia to obtain relevant information, and then use GPT-API to generate a dialogue based on the given content. Specifically, we extract the QID labels and their corresponding Wikipedia descriptions from the Bamboo dataset to generate instruction tuning data. This approach allows us to incorporate common sense knowledge into the dataset, thereby enhancing the ability of MLLMs to generate responses that draw upon a broader range of factual knowledge.
The messages used to generate factual knowledge data in the GPT-API are presented in Figure
     <a class="ltx_ref" href="#A2.F9" title="Figure 9 ‣ B.1 Image Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       9
      </span>
     </a>
     , while Figure
     <a class="ltx_ref" href="#A2.F10" title="Figure 10 ‣ B.1 Image Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       10
      </span>
     </a>
     showcases detailed examples of the factual knowledge data generated by these messages.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p3">
    <p class="ltx_p" id="A2.SS1.p3.1">
     <em class="ltx_emph ltx_font_bold ltx_font_italic" id="A2.SS1.p3.1.1">
      C4: 1-round Visual Task Dialogue
     </em>
     <span class="ltx_text ltx_font_bold" id="A2.SS1.p3.1.2">
      .
     </span>
     In addition to the three types of data discussed earlier, we also incorporate established computer vision tasks, such as image classification, object detection, keypoint detection, OCR, and object counting, into our dataset. This enables MLLMs to handle traditional computer vision tasks and generate responses that incorporate both language and visual information.
The typical computer vision dataset consists of a set of images or videos, along with their corresponding labels or annotations that represent the desired output of the computer vision task, such as the class of objects present in the image or the location of an object. However, these discrete results are not suitable for large language model dialogues, as they do not allow for natural language interactions.
To address this issue, our proposed approach involves converting computer vision tasks, such as image classification, into natural language dialogues to enable large language models to perform these tasks through dialogue interactions. In detail, we first use GPT-API to generate a template pool of questions and answers for each task. Then, we randomly select a pair from the question template pool and answer template pool to combine with a piece of data from the computer vision dataset, creating the
     <em class="ltx_emph ltx_font_italic" id="A2.SS1.p3.1.3">
      C4: 1-round Visual Task Dialogue
     </em>
     data.
Figure
     <a class="ltx_ref" href="#A5.F27" title="Figure 27 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       27
      </span>
     </a>
     -
     <a class="ltx_ref" href="#A5.F30" title="Figure 30 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       30
      </span>
     </a>
     provide some examples of the dialogues generated using our proposed approach for converting computer vision tasks into natural language dialogues. This approach allows us to leverage the rich visual information in traditional computer vision datasets and incorporate it into the instruction tuning process, thereby enhancing the ability of MLLMs to understand and respond to natural language instructions related to these tasks.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS1.p4">
    <p class="ltx_p" id="A2.SS1.p4.1">
     In summary, the construction of 2D part in our dataset provides a comprehensive and diverse samples of real-world scenarios, incorporating fine-grained and dense information from object relationships and factual knowledge sources. The dataset contains 186K unique language-image instruction-following samples, including 49K in daily dialogues, 49K in detailed descriptions, 42K in factual Knowledge dialogues, and 46K in visual task dialogues. Our experiments showed that the use of GPT-API consistently provides higher-quality instruction-following data, such as spatial reasoning. These features make our dataset a valuable resource for researchers and practitioners working in the computer vision and natural language processing fields.
    </p>
   </div>
   <figure class="ltx_figure" id="A2.F10">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="238" id="A2.F10.g1" src="/html/2306.06687/assets/x10.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 10:
     </span>
     The example for constructing
     <em class="ltx_emph ltx_font_italic" id="A2.F10.2.1">
      n-round Factual Knowledge Dialogue
     </em>
     data. The description is from Wikipedia page.
    </figcaption>
   </figure>
  </section>
  <section class="ltx_subsection" id="A2.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.2
    </span>
    Point Cloud Instruction Tuning Dataset
   </h3>
   <div class="ltx_para" id="A2.SS2.p1">
    <p class="ltx_p" id="A2.SS2.p1.1">
     The construction pipeline of point cloud instruction tuning data is similar to that of image instruction tuning data.
However, due to the limited availability of 3D data, point cloud instruction tuning dataset only consists of three major components: n-round plain conversation and 1-round detalied description data from GPT-API and 1-round visual dialogue data converted from datasets for existing 3D vision tasks.
    </p>
   </div>
   <figure class="ltx_figure" id="A2.F11">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="182" id="A2.F11.g1" src="/html/2306.06687/assets/x11.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 11:
     </span>
     Message to transfer visual question answering annotations from CLEVR3D
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib58" title="">
       58
      </a>
      ]
     </cite>
     to declarative sentences for 3D data.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A2.F12">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="221" id="A2.F12.g1" src="/html/2306.06687/assets/x12.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 12:
     </span>
     Message to generate
     <em class="ltx_emph ltx_font_italic" id="A2.F12.2.1">
      n-round Daily Conversation Dialogue
     </em>
     data in 3D portion of our dataset.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A2.F13">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="216" id="A2.F13.g1" src="/html/2306.06687/assets/x13.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 13:
     </span>
     Message to generate
     <em class="ltx_emph ltx_font_italic" id="A2.F13.2.1">
      1-round Detailed Description
     </em>
     data in 3D portion of our dataset.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A2.F14">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="258" id="A2.F14.g1" src="/html/2306.06687/assets/x14.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 14:
     </span>
     Example of GPT-generated n-round daily dialogue and 1-round detailed description data in 3D portion of our dataset.
    </figcaption>
   </figure>
   <div class="ltx_para" id="A2.SS2.p2">
    <p class="ltx_p" id="A2.SS2.p2.1">
     <em class="ltx_emph ltx_font_bold ltx_font_italic" id="A2.SS2.p2.1.1">
      C1: n-round Daily Dialogue
     </em>
     <span class="ltx_text ltx_font_bold" id="A2.SS2.p2.1.2">
      &amp;
      <em class="ltx_emph ltx_font_italic" id="A2.SS2.p2.1.2.1">
       C3: 1-round Detailed Description
      </em>
      .
     </span>
     To construct the
     <em class="ltx_emph ltx_font_italic" id="A2.SS2.p2.1.3">
      C1: n-round Daily Dialogue
     </em>
     and
     <em class="ltx_emph ltx_font_italic" id="A2.SS2.p2.1.4">
      C3: 1-round Detailed Description
     </em>
     data, we choose point clouds from 3RScan
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib59" title="">
       59
      </a>
      ]
     </cite>
     as data source and use its original 3D bounding box annotations.
Since there is no caption annotation for 3RScan, we input visual question answering (VQA) annotations from CLEVR3D
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib58" title="">
       58
      </a>
      ]
     </cite>
     to GPT-API and ask it to convert the Q&amp;A data into declarative sentences, which serves as point cloud captions in further steps.
Figure
     <a class="ltx_ref" href="#A2.F11" title="Figure 11 ‣ B.2 Point Cloud Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       11
      </span>
     </a>
     shows the corresponding prompts.
Object attributes and relationships are extracted from scene graph annotation in 3DSSG
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib60" title="">
       60
      </a>
      ]
     </cite>
     .
Figure
     <a class="ltx_ref" href="#A2.F12" title="Figure 12 ‣ B.2 Point Cloud Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       12
      </span>
     </a>
     and
     <a class="ltx_ref" href="#A2.F13" title="Figure 13 ‣ B.2 Point Cloud Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       13
      </span>
     </a>
     show the prompts to let GPT-API generate daily dialogue and detailed description data for point clouds.
Since full annotation of a scene point cloud may easily exceed input token limits of GPT-API, we randomly selected 10 captions and keep bounding box and relationships of corresponding objects as input contexts.
For GPT-generated data, We limit the number of turns in each dialogue data to no more than 10, and any data exceeding this limit will be split into different samples. Figure
     <a class="ltx_ref" href="#A2.F14" title="Figure 14 ‣ B.2 Point Cloud Instruction Tuning Dataset ‣ Appendix B Dataset ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       14
      </span>
     </a>
     shows an example of GPT-generated data.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS2.p3">
    <p class="ltx_p" id="A2.SS2.p3.1">
     <em class="ltx_emph ltx_font_bold ltx_font_italic" id="A2.SS2.p3.1.1">
      C4: 1-round Visual Task Dialogue
     </em>
     <span class="ltx_text ltx_font_bold" id="A2.SS2.p3.1.2">
      .
     </span>
     On the other hand, we also leverage annotations for existing 3D vision tasks, such as point cloud classification, 3D object detection, and CLEVR3D for 3D VQA.
Similar to 2D datasets, we designed 15 templates for instruction and response by sending definitions of the corresponding tasks to GPT-API.
Then instruction data are formulated by replacing keywords with corresponding annotations.
Templates of 3 tasks involved are presented in Figure
     <a class="ltx_ref" href="#A5.F31" title="Figure 31 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       31
      </span>
     </a>
     ,
     <a class="ltx_ref" href="#A5.F32" title="Figure 32 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       32
      </span>
     </a>
     and
     <a class="ltx_ref" href="#A5.F33" title="Figure 33 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       33
      </span>
     </a>
     , respectively.
    </p>
   </div>
   <div class="ltx_para" id="A2.SS2.p4">
    <p class="ltx_p" id="A2.SS2.p4.1">
     Finally, 3D portion of our dataset contains 10K samples in total, and the number of ShapeNet, 3RScan detection, CLEVR3D, and GPT-generated dialogue are 2K, 1.3K, 2K, and 4.9K, respectively.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A2.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.3
    </span>
    Quality Check
   </h3>
   <div class="ltx_para" id="A2.SS3.p1">
    <p class="ltx_p" id="A2.SS3.p1.1">
     In order to ensure the quality of the generated instruction tuning data, we implemented several measures. Firstly, we generate a small amount of data as a cold start and conduct manual check on the generated data. This involved carefully assessing the quality and making necessary adjustments to the message information provided as input to GPT-API. The iterative process aimed to eliminate ethical concerns and establish a strong correlation between the generated data and the corresponding inputs. We repeated this process until the desired level of quality was achieved. Once satisfied, we proceeded to generate a large volume of data. Furthermore, to verify the quality of the generated dataset, we randomly select a subset of 10% data for manual checks. This step allowed us to evaluate the generated data against our specific requirements and quality standards. During this evaluation, any formatting issues or incorrect answers generated by GPT-4 were filtered out to ensure the usability and reliability of the data. By combining manual checks during the iterative generation process and subsequent random manual checks on the final dataset, we strive to ensure that the generated data meets our rigorous quality standards and aligns with the specific needs of our dataset.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A2.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     B.4
    </span>
    Social Impact
   </h3>
   <div class="ltx_para" id="A2.SS4.p1">
    <p class="ltx_p" id="A2.SS4.p1.1">
     Our dataset is a compilation of publicly available datasets that have been licensed under the Creative Commons license (CC-BY). We have taken great care to follow all necessary legal protocols to use this data in our research, and believe that transparency in data licensing is crucial for ensuring proper attribution and appropriate use of the data. Besides, the dataset includes images sourced from publicly available datasets and language data generated using the GPT-API. While we have taken steps to ensure appropriate content, we acknowledge that problematic content may exist. If you encounter any such content, please notify us immediately, and we will make necessary modifications to maintain a high-quality dataset that is free of inappropriate content. To protect the privacy of individuals and vehicles captured in the images, we plan to obfuscate sensitive information, such as faces and license plates, before publishing the dataset. We are committed to maintaining a dataset that is both high-quality and ethically responsible and pledge to uphold principles of privacy and transparency in our work.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A3">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix C
   </span>
   Benchmark
  </h2>
  <section class="ltx_subsection" id="A3.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.1
    </span>
    Benchmark on image tasks
   </h3>
   <figure class="ltx_table" id="A3.T5">
    <figcaption class="ltx_caption">
     <span class="ltx_tag ltx_tag_table">
      Table 5:
     </span>
     CV tasks in Our Benchmark
    </figcaption>
    <table class="ltx_tabular ltx_centering ltx_align_middle" id="A3.T5.1">
     <tr class="ltx_tr" id="A3.T5.1.1">
      <td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T5.1.1.1">
       Task
      </td>
      <td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T5.1.1.2">
       Output
      </td>
      <td class="ltx_td ltx_align_left ltx_border_tt" id="A3.T5.1.1.3">
       Metrics
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.2">
      <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T5.1.2.1">
       Classification
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T5.1.2.2">
       label name
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T5.1.2.3">
       Acc
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.3">
      <td class="ltx_td ltx_align_left" id="A3.T5.1.3.1">
       Detection
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.3.2">
       list of object label and bbox
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.3.3">
       mAP50
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.4">
      <td class="ltx_td ltx_align_left" id="A3.T5.1.4.1">
       VQA
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.4.2">
       option and answer
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.4.3">
       Acc
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.5">
      <td class="ltx_td ltx_align_left" id="A3.T5.1.5.1">
       Image Caption
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.5.2">
       captions
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.5.3">
       BLEU4
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.6">
      <td class="ltx_td ltx_align_left" id="A3.T5.1.6.1">
       Fine-grained classification
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.6.2">
       fine-grained label name
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.6.3">
       Acc
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.7">
      <td class="ltx_td ltx_align_left" id="A3.T5.1.7.1">
       Object counting
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.7.2">
       number
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.7.3">
       MAE
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.8">
      <td class="ltx_td ltx_align_left" id="A3.T5.1.8.1">
       OCR
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.8.2">
       list of words
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.8.3">
       word Acc
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.9">
      <td class="ltx_td ltx_align_left" id="A3.T5.1.9.1">
       Facial classification
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.9.2">
       answer
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.9.3">
       Acc
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.10">
      <td class="ltx_td ltx_align_left" id="A3.T5.1.10.1">
       Keypoints detection
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.10.2">
       keypoints
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.10.3">
       PCK
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.11">
      <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T5.1.11.1">
       3D Detection
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T5.1.11.2">
       list of object label and bbox
      </td>
      <td class="ltx_td ltx_align_left ltx_border_t" id="A3.T5.1.11.3">
       mAP50
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.12">
      <td class="ltx_td ltx_align_left" id="A3.T5.1.12.1">
       3D VQA
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.12.2">
       option and answer
      </td>
      <td class="ltx_td ltx_align_left" id="A3.T5.1.12.3">
       Acc
      </td>
     </tr>
     <tr class="ltx_tr" id="A3.T5.1.13">
      <td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T5.1.13.1">
       3D Visual Grounding
      </td>
      <td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T5.1.13.2">
       bbox
      </td>
      <td class="ltx_td ltx_align_left ltx_border_bb" id="A3.T5.1.13.3">
       mAP50
      </td>
     </tr>
    </table>
   </figure>
   <div class="ltx_para" id="A3.SS1.p1">
    <p class="ltx_p" id="A3.SS1.p1.1">
     We selected a set of nine commonly used CV tasks to evaluate the performance of MLLM models in our benchmark on image tasks. Our task selection criteria were based on widely studied tasks in the CV field that can showcase the MLLM model’s abilities in visual interpretation, localization, and question-answering. Table
     <a class="ltx_ref" href="#A3.T5" title="Table 5 ‣ C.1 Benchmark on image tasks ‣ Appendix C Benchmark ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       5
      </span>
     </a>
     provides a summary of the tasks and the corresponding common evaluation metrics, which are based on the output that the MLLM models are required to generate for each task. We utilized a prompt-based approach to instruct the MLLM models to understand the task definition and generate the desired output. The ability of the models to understand and interpret the given instruction was also evaluated as part of the assessment criteria. As the models’ outputs are text, we use different text-processing techniques for each task to extract entities as the final answers for evaluation. For each task, We selected datasets that are distinct from the training datasets, as our benchmark evaluation is conducted in an out-of-distribution zero-shot setting.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p2">
    <p class="ltx_p" id="A3.SS1.p2.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS1.p2.1.1">
      Classification
     </span>
     This task involves predicting the most likely category label for an image. For MLLM models, the task involves performing open-vocabulary classification. We selected CIFAR-10
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib41" title="">
       41
      </a>
      ]
     </cite>
     as the test dataset for the evaluation of classification. CIFAR10 contains 10000 test images across 10 common categories. We utilize NLTK to extract noun entities from the models’ output text, and expand them to a synonym set for accuracy evaluation calculation.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p3">
    <p class="ltx_p" id="A3.SS1.p3.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS1.p3.1.1">
      Object Detection
     </span>
     We selected the VOC 2012
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib42" title="">
       42
      </a>
      ]
     </cite>
     datasets to evaluate the model’s ability to detect objects in images while considering both its visual interpretation and localization capabilities. To evaluate the accuracy of object category predictions, we employ a similar approach to classification tasks. We also use regular expression matching to extract the models’ output bounding boxes for mAP50 calculation.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p4">
    <p class="ltx_p" id="A3.SS1.p4.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS1.p4.1.1">
      Visual Question Answering
     </span>
     We selected the ScienceQA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib43" title="">
       43
      </a>
      ]
     </cite>
     and AI2D
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib44" title="">
       44
      </a>
      ]
     </cite>
     datasets to evaluate the MLLM model’s ability to answer questions about images. The ScienceQA and AI2D datasets include over 2017 and 5793 multiple-choice questions with images, respectively. We extract the image-containing data from the ScienceQA dataset to create the SQAimage dataset. We then tested MLLM models on the SQAimage dataset to evaluate their multimodal understanding skills. As both ScienceQA and AI2D datasets are presented in a multiple-choice format, we evaluated the model’s performance using the accuracy metric. Following LLaVA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib15" title="">
       15
      </a>
      ]
     </cite>
     , we prompt the MLLM to output the complex reasoning procession, followed by the final option answer.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p5">
    <p class="ltx_p" id="A3.SS1.p5.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS1.p5.1.1">
      Image Caption
     </span>
     The image caption task involves generating a textual description of an image. We selected the Flickr30k
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib45" title="">
       45
      </a>
      ]
     </cite>
     dataset to evaluate the MLLM model’s ability to understand images and generate descriptive captions. Flickr30k contains a variety of objects and scenes with diverse captions, providing a challenging task for the MLLM model. To evaluate the quality of the models’ text outputs, we split the generated text into sentences and calculate the BLEU-4 score for each. The highest score is selected as the final result.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p6">
    <p class="ltx_p" id="A3.SS1.p6.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS1.p6.1.1">
      Fine-grained classification
     </span>
     Similar to the classification task, the fine-grained classification task requires the model to make predictions across a large number of fine-grained categories. We selected UCMerced Land Use dataset
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib46" title="">
       46
      </a>
      ]
     </cite>
     as the test set. UCMerced Land Use contains 21 classes of land-use categories, including airports, forests, and residential areas. Similar to classification, we report Accuracy.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p7">
    <p class="ltx_p" id="A3.SS1.p7.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS1.p7.1.1">
      Object counting
     </span>
     We selected the FSC147 dataset for object counting evaluation. FSC147
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib47" title="">
       47
      </a>
      ]
     </cite>
     is a dataset of 1190 images containing various objects, including animals, vehicles, and household items. The images in this dataset are challenging and contain occlusions and overlapping objects, making it a suitable choice to test the model’s object recognition and localization capabilities. We utilize regular expression matching to extract the numeric entity and evaluate the model’s performance using the mean absolute error (MAE) metric.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p8">
    <p class="ltx_p" id="A3.SS1.p8.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS1.p8.1.1">
      Optical Character Recognition
     </span>
     The OCR (Optical Character Recognition) task involves recognizing and transcribing text from images. To evaluate the MLLM model’s ability to recognize text from images, we selected SVT dataset
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib48" title="">
       48
      </a>
      ]
     </cite>
     . We extract the entities enclosed in quotation marks from the generated text as the predicted word list. Word Accuracy is adopted as the evaluation metric.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p9">
    <p class="ltx_p" id="A3.SS1.p9.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS1.p9.1.1">
      Facial Classification
     </span>
     Due to the difficulty of performing face recognition tasks using MLLM, we evaluated the model’s performance on facial attribute classification tasks. We selected the CelebA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib49" title="">
       49
      </a>
      ]
     </cite>
     dataset, which contains 19962 images for testing with annotations for 40 facial attributes, including hair color and facial expression. Specifically, we evaluated the model’s ability to predict whether a person in an image is smiling, named CelebA(Smile) dataset, and the color of their hair, named CelebA(Hair) dataset. We aimed to evaluate the MLLM model’s ability to understand facial images. Classification accuracy is used as the evaluation metric.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS1.p10">
    <p class="ltx_p" id="A3.SS1.p10.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS1.p10.1.1">
      Keypoints Detection
     </span>
     To evaluate the models’ ability to perform fine-grained point localization, we utilized the LSP
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib50" title="">
       50
      </a>
      ]
     </cite>
     dataset for keypoint detection. To simplify the task difficulty for MLLM models, we employed a grounding approach, where we sequentially asked the model to predict the position of each human body keypoints in the image. The evaluation metric used for this task was PCK (Percentage of Correct Keypoints).
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A3.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.2
    </span>
    Inference Details
   </h3>
   <section class="ltx_subsubsection" id="A3.SS2.SSS1">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      C.2.1
     </span>
     System messages for image tasks
    </h4>
    <div class="ltx_para" id="A3.SS2.SSS1.p1">
     <p class="ltx_p" id="A3.SS2.SSS1.p1.1">
      Figure
      <a class="ltx_ref" href="#A5.F17" title="Figure 17 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
       <span class="ltx_text ltx_ref_tag">
        17
       </span>
      </a>
      shows the system messages defined for each image task. The system messages, which include the task definition and the output structure, is a part of the instruction that prompt the MLLM models to generated responses. This is designed to enable the model to better understand the task it is performing, focus on the critical aspects, and output the appropriate structure. Note that some tasks do not require a defined output structure. In such cases, the model can output any text as a response.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="A3.SS2.SSS2">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      C.2.2
     </span>
     Instructions for VQA
    </h4>
    <figure class="ltx_figure" id="A3.F15">
     <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="51" id="A3.F15.g1" src="/html/2306.06687/assets/x15.png" width="230"/>
     <figcaption class="ltx_caption ltx_centering">
      <span class="ltx_tag ltx_tag_figure">
       Figure 15:
      </span>
      Template instructions for VQA inference. "Response a" is the generated reasoning process, which is the output of the first inference. "Response b" is the output answer, which is the ouput following the prompt "### ANSWER".
     </figcaption>
    </figure>
    <div class="ltx_para" id="A3.SS2.SSS2.p1">
     <p class="ltx_p" id="A3.SS2.SSS2.p1.1">
      Different from other common image tasks, besides the system messages designed in
      <a class="ltx_ref" href="#A3.SS2.SSS1" title="C.2.1 System messages for image tasks ‣ C.2 Inference Details ‣ Appendix C Benchmark ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
       <span class="ltx_text ltx_ref_tag">
        C.2.1
       </span>
      </a>
      , we prompt MLLM to generate the reasoning process additionally, as figure
      <a class="ltx_ref" href="#A3.F15" title="Figure 15 ‣ C.2.2 Instructions for VQA ‣ C.2 Inference Details ‣ Appendix C Benchmark ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
       <span class="ltx_text ltx_ref_tag">
        15
       </span>
      </a>
      shows. To prompt the model to output its reasoning process, we first use conventional instruction texts to generate "Response a". We then combine the first instructions , the "Response a", and the prompt "### ANSWER" to make the model generate the option as the final answer.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="A3.SS2.SSS3">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      C.2.3
     </span>
     Metrics
    </h4>
    <div class="ltx_para" id="A3.SS2.SSS3.p1">
     <p class="ltx_p" id="A3.SS2.SSS3.p1.1">
      Our benchmark includes two evaluation settings. The first is a zero-shot setting, where we selected downstream tasks that have no intersection with the MLLM’s training data. We provide the zero-shot results of the current MLLM models on these datasets. The second setting involves fine-tuning on mainstream task datasets, covering tasks such as detection, classification, and VQA.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="A3.SS2.SSS4">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      C.2.4
     </span>
     Binary Locating Metric
    </h4>
    <div class="ltx_para" id="A3.SS2.SSS4.p1">
     <p class="ltx_p" id="A3.SS2.SSS4.p1.1">
      The ability to accurately localize objects in an image is a crucial component of MLLM models’ visual understanding skills. In addition to using conventional detection tasks to calculate mAP, we attempted a more direct method for evaluating the models’ localization ability, namely Binary Locating Metric. Distinct from object detection, which requires the model to output a bounding box, we instructed the model with "output the position of the object" instead of "output the bounding box of the object" to output the approximate position. During the evaluation phase, the model’s predicted keypoint was considered correct as long as it was within the object’s bounding box. Object locating is evaluated on all datasets involving object localization, including object detection, object counting, and keypoints detection. Compared to the traditional detection evaluation methods, the object locating evaluation method provides a more reasonable and direct approach for evaluating the localization ability of MLLM.
     </p>
    </div>
   </section>
   <section class="ltx_subsubsection" id="A3.SS2.SSS5">
    <h4 class="ltx_title ltx_title_subsubsection">
     <span class="ltx_tag ltx_tag_subsubsection">
      C.2.5
     </span>
     GPT Metric
    </h4>
    <div class="ltx_para" id="A3.SS2.SSS5.p1">
     <p class="ltx_p" id="A3.SS2.SSS5.p1.1">
      To evaluate the overall understanding and question-answering abilities of MLLM models, we utilized the GPT Metric. Unlike LLaVA
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib15" title="">
        15
       </a>
       ]
      </cite>
      and Vicuna
      <cite class="ltx_cite ltx_citemacro_cite">
       [
       <a class="ltx_ref" href="#bib.bib9" title="">
        9
       </a>
       ]
      </cite>
      , we ranked the answers of multiple models using GPT. Similar to the pipeline approach, we give GPT an instruction, informing it of the task definition, the question, and the answer provided by each model. We then ranked each model’s response based on its relevance and accuracy with the answer. Each model received a score based on its ranking, and the average score obtained on all test data served as a metric for measuring the model’s overall ability.
Our GPT evaluation datasets cover various visual tasks, including captioning and VQA tasks involving image description and answering, as well as a small number of detection and counting tasks related to object localization.
     </p>
    </div>
   </section>
  </section>
  <section class="ltx_subsection" id="A3.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     C.3
    </span>
    Benchmark on point cloud tasks
   </h3>
   <div class="ltx_para" id="A3.SS3.p1">
    <p class="ltx_p" id="A3.SS3.p1.1">
     For benchmark on point cloud tasks, we focus on three tasks of scene perception, including 3D object detection, visual grounding, and 3D visual question answering. Figure
     <a class="ltx_ref" href="#A5.F18" title="Figure 18 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       18
      </span>
     </a>
     presents system messages for point cloud tasks.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p2">
    <p class="ltx_p" id="A3.SS3.p2.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS3.p2.1.1">
      3D Object Detection
     </span>
     .
As it’s widely used in 3D object detection, we select ScanNetv2
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib51" title="">
       51
      </a>
      ]
     </cite>
     as the dataset to evaluate MLLM’s ability to locate objects in a point cloud and identify semantics, whose validation set contains 312 scenes.
In this task, MLLM is expected to list all objects along with bounding boxes, and we extract bounding boxes from the response text by entity extraction.
Boxes whose IoU with ground truth is larger than 50% count for positive predictions and we use mean Average Precision (mAP) to evaluate performance.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p3">
    <p class="ltx_p" id="A3.SS3.p3.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS3.p3.1.1">
      Visual Grounding
     </span>
     . This task aims to locate the object described by a given caption and output the corresponding bounding box.
We test on ScanRefer
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib52" title="">
       52
      </a>
      ]
     </cite>
     in this task, which provides human-labeled captions towards each object in ScanNet and its test set contains 9508 samples.
Similar with object detection, mean average precision (mAP) is reported to evaluate MLLM’s capacity.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p4">
    <p class="ltx_p" id="A3.SS3.p4.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS3.p4.1.1">
      3D Visual Question Answering
     </span>
     . ScanQA
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib53" title="">
       53
      </a>
      ]
     </cite>
     is proposed for 3D visual question answering before, and models are required to answer the given questions based on the point cloud. It has been formatted as an attribute classification task in previous work
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib53" title="">
       53
      </a>
      ]
     </cite>
     .
However, MLLM’s output cannot be constrained with several classes consistently and is usually long text to explain details, so the original metrics in ScanQA, Exact Matching
     <math alttext="\&amp;" class="ltx_Math" display="inline" id="A3.SS3.p4.1.m1.1">
      <semantics id="A3.SS3.p4.1.m1.1a">
       <mo id="A3.SS3.p4.1.m1.1.1" xref="A3.SS3.p4.1.m1.1.1.cmml">
        &amp;
       </mo>
       <annotation-xml encoding="MathML-Content" id="A3.SS3.p4.1.m1.1b">
        <and id="A3.SS3.p4.1.m1.1.1.cmml" xref="A3.SS3.p4.1.m1.1.1">
        </and>
       </annotation-xml>
       <annotation encoding="application/x-tex" id="A3.SS3.p4.1.m1.1c">
        \&amp;
       </annotation>
      </semantics>
     </math>
     BLEU, cannot be used for test, as long text is different from the style of given ground truth and the BLEU score inevitably decreases for long-text results.
Following ScienceQA in 2D VQA task, we transfer this task to be a multiple-choice problem.
First, we feed the original question-answer pairs to GPT-API and ask for 5 confusing options. Then MLLM is expected to choose the correct option or output the correct content.
Thus, a metric of accuracy is used to evaluate model performance.
    </p>
   </div>
   <div class="ltx_para" id="A3.SS3.p5">
    <p class="ltx_p" id="A3.SS3.p5.1">
     <span class="ltx_text ltx_font_bold" id="A3.SS3.p5.1.1">
      Evaluation Settings
     </span>
     Similar to evaluation for 2D tasks, our 3D benchmark includes two settings for evaluation.
The first one is a zero-shot setting.
MLLM is trained on instruction data from 3D portion of our dataset, whose point clouds come from 3RScan or ShapeNet and has no overlap with ones in downstream tasks.
Furthermore, we finetune the models trained on our 3D datasets by training a set of downstream tasks and reporting metrics on the corresponding test set.
    </p>
   </div>
  </section>
 </section>
 <section class="ltx_appendix" id="A4">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix D
   </span>
   Implementation Details
  </h2>
  <div class="ltx_para" id="A4.p1">
   <p class="ltx_p" id="A4.p1.1">
    In our experiments, 2D and 3D models are trained independently, and only the feature projection layer and LoRA parameters are optimized during training while LLM can be shared among tasks.
   </p>
  </div>
  <div class="ltx_para" id="A4.p2">
   <p class="ltx_p" id="A4.p2.1">
    For all experiments, trainable parameters are optimized by Adam optimizer with a learning rate initialized to be 5e-4, and scheduled using a linear decay scheduler. We
For 2D experiments, models are trained for 2 epochs.
For 3D experiments, we increase the number of iterations to 10,000 in case of too few samples.
We use 4 A100-80GB to conduct experiments. Each GPU process 2 samples every iteration and the effective batch size are set to 64 by gradient accumulation.
For reference, 2D experiments at most last for about 8 hours for 186K samples, while 3D experiments require about 3 hours.
   </p>
  </div>
  <figure class="ltx_figure" id="A4.F16">
   <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="38" id="A4.F16.g1" src="/html/2306.06687/assets/x16.png" width="230"/>
   <figcaption class="ltx_caption ltx_centering">
    <span class="ltx_tag ltx_tag_figure">
     Figure 16:
    </span>
    Template for multi-modal data pairs.
    <span class="ltx_text ltx_font_bold" id="A4.F16.6.1">
     Bold words
    </span>
    stand for corresponding text data and italic words indicate fixed templates.
    <math alttext="&lt;vision&gt;" class="ltx_Math" display="inline" id="A4.F16.3.m1.1">
     <semantics id="A4.F16.3.m1.1b">
      <mrow id="A4.F16.3.m1.1.1.1" xref="A4.F16.3.m1.1.1.2.cmml">
       <mo fence="true" id="A4.F16.3.m1.1.1.1.2" rspace="0em" xref="A4.F16.3.m1.1.1.2.1.cmml">
        &lt;
       </mo>
       <mrow id="A4.F16.3.m1.1.1.1.1" xref="A4.F16.3.m1.1.1.1.1.cmml">
        <mi id="A4.F16.3.m1.1.1.1.1.2" xref="A4.F16.3.m1.1.1.1.1.2.cmml">
         v
        </mi>
        <mo id="A4.F16.3.m1.1.1.1.1.1" lspace="0em" rspace="0em" xref="A4.F16.3.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.F16.3.m1.1.1.1.1.3" xref="A4.F16.3.m1.1.1.1.1.3.cmml">
         i
        </mi>
        <mo id="A4.F16.3.m1.1.1.1.1.1b" lspace="0em" rspace="0em" xref="A4.F16.3.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.F16.3.m1.1.1.1.1.4" xref="A4.F16.3.m1.1.1.1.1.4.cmml">
         s
        </mi>
        <mo id="A4.F16.3.m1.1.1.1.1.1c" lspace="0em" rspace="0em" xref="A4.F16.3.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.F16.3.m1.1.1.1.1.5" xref="A4.F16.3.m1.1.1.1.1.5.cmml">
         i
        </mi>
        <mo id="A4.F16.3.m1.1.1.1.1.1d" lspace="0em" rspace="0em" xref="A4.F16.3.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.F16.3.m1.1.1.1.1.6" xref="A4.F16.3.m1.1.1.1.1.6.cmml">
         o
        </mi>
        <mo id="A4.F16.3.m1.1.1.1.1.1e" lspace="0em" rspace="0em" xref="A4.F16.3.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.F16.3.m1.1.1.1.1.7" xref="A4.F16.3.m1.1.1.1.1.7.cmml">
         n
        </mi>
       </mrow>
       <mo fence="true" id="A4.F16.3.m1.1.1.1.3" lspace="0em" xref="A4.F16.3.m1.1.1.2.1.cmml">
        &gt;
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A4.F16.3.m1.1c">
       <apply id="A4.F16.3.m1.1.1.2.cmml" xref="A4.F16.3.m1.1.1.1">
        <csymbol cd="latexml" id="A4.F16.3.m1.1.1.2.1.cmml" xref="A4.F16.3.m1.1.1.1.2">
         expectation
        </csymbol>
        <apply id="A4.F16.3.m1.1.1.1.1.cmml" xref="A4.F16.3.m1.1.1.1.1">
         <times id="A4.F16.3.m1.1.1.1.1.1.cmml" xref="A4.F16.3.m1.1.1.1.1.1">
         </times>
         <ci id="A4.F16.3.m1.1.1.1.1.2.cmml" xref="A4.F16.3.m1.1.1.1.1.2">
          𝑣
         </ci>
         <ci id="A4.F16.3.m1.1.1.1.1.3.cmml" xref="A4.F16.3.m1.1.1.1.1.3">
          𝑖
         </ci>
         <ci id="A4.F16.3.m1.1.1.1.1.4.cmml" xref="A4.F16.3.m1.1.1.1.1.4">
          𝑠
         </ci>
         <ci id="A4.F16.3.m1.1.1.1.1.5.cmml" xref="A4.F16.3.m1.1.1.1.1.5">
          𝑖
         </ci>
         <ci id="A4.F16.3.m1.1.1.1.1.6.cmml" xref="A4.F16.3.m1.1.1.1.1.6">
          𝑜
         </ci>
         <ci id="A4.F16.3.m1.1.1.1.1.7.cmml" xref="A4.F16.3.m1.1.1.1.1.7">
          𝑛
         </ci>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A4.F16.3.m1.1d">
       &lt;vision&gt;
      </annotation>
     </semantics>
    </math>
    &amp;
    <math alttext="&lt;/vision&gt;" class="ltx_math_unparsed" display="inline" id="A4.F16.4.m2.2">
     <semantics id="A4.F16.4.m2.2b">
      <mrow id="A4.F16.4.m2.2c">
       <mo id="A4.F16.4.m2.1.1" rspace="0em">
        &lt;
       </mo>
       <mo id="A4.F16.4.m2.2.2" lspace="0em">
        /
       </mo>
       <mi id="A4.F16.4.m2.2.3">
        v
       </mi>
       <mi id="A4.F16.4.m2.2.4">
        i
       </mi>
       <mi id="A4.F16.4.m2.2.5">
        s
       </mi>
       <mi id="A4.F16.4.m2.2.6">
        i
       </mi>
       <mi id="A4.F16.4.m2.2.7">
        o
       </mi>
       <mi id="A4.F16.4.m2.2.8">
        n
       </mi>
       <mo id="A4.F16.4.m2.2.9">
        &gt;
       </mo>
      </mrow>
      <annotation encoding="application/x-tex" id="A4.F16.4.m2.2d">
       &lt;/vision&gt;
      </annotation>
     </semantics>
    </math>
    stand for start &amp; end token for vision contents.
   </figcaption>
  </figure>
  <div class="ltx_para" id="A4.p3">
   <p class="ltx_p" id="A4.p3.9">
    Following Vicuna
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib9" title="">
      9
     </a>
     ]
    </cite>
    , we format multi-modal training data as Figure
    <a class="ltx_ref" href="#A4.F16" title="Figure 16 ‣ Appendix D Implementation Details ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
     <span class="ltx_text ltx_ref_tag">
      16
     </span>
    </a>
    .
    <math alttext="[SystemMesssage]" class="ltx_Math" display="inline" id="A4.p3.1.m1.1">
     <semantics id="A4.p3.1.m1.1a">
      <mrow id="A4.p3.1.m1.1.1.1" xref="A4.p3.1.m1.1.1.2.cmml">
       <mo id="A4.p3.1.m1.1.1.1.2" stretchy="false" xref="A4.p3.1.m1.1.1.2.1.cmml">
        [
       </mo>
       <mrow id="A4.p3.1.m1.1.1.1.1" xref="A4.p3.1.m1.1.1.1.1.cmml">
        <mi id="A4.p3.1.m1.1.1.1.1.2" xref="A4.p3.1.m1.1.1.1.1.2.cmml">
         S
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.3" xref="A4.p3.1.m1.1.1.1.1.3.cmml">
         y
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1a" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.4" xref="A4.p3.1.m1.1.1.1.1.4.cmml">
         s
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1b" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.5" xref="A4.p3.1.m1.1.1.1.1.5.cmml">
         t
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1c" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.6" xref="A4.p3.1.m1.1.1.1.1.6.cmml">
         e
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1d" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.7" xref="A4.p3.1.m1.1.1.1.1.7.cmml">
         m
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1e" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.8" xref="A4.p3.1.m1.1.1.1.1.8.cmml">
         M
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1f" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.9" xref="A4.p3.1.m1.1.1.1.1.9.cmml">
         e
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1g" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.10" xref="A4.p3.1.m1.1.1.1.1.10.cmml">
         s
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1h" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.11" xref="A4.p3.1.m1.1.1.1.1.11.cmml">
         s
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1i" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.12" xref="A4.p3.1.m1.1.1.1.1.12.cmml">
         s
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1j" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.13" xref="A4.p3.1.m1.1.1.1.1.13.cmml">
         a
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1k" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.14" xref="A4.p3.1.m1.1.1.1.1.14.cmml">
         g
        </mi>
        <mo id="A4.p3.1.m1.1.1.1.1.1l" lspace="0em" rspace="0em" xref="A4.p3.1.m1.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.1.m1.1.1.1.1.15" xref="A4.p3.1.m1.1.1.1.1.15.cmml">
         e
        </mi>
       </mrow>
       <mo id="A4.p3.1.m1.1.1.1.3" stretchy="false" xref="A4.p3.1.m1.1.1.2.1.cmml">
        ]
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A4.p3.1.m1.1b">
       <apply id="A4.p3.1.m1.1.1.2.cmml" xref="A4.p3.1.m1.1.1.1">
        <csymbol cd="latexml" id="A4.p3.1.m1.1.1.2.1.cmml" xref="A4.p3.1.m1.1.1.1.2">
         delimited-[]
        </csymbol>
        <apply id="A4.p3.1.m1.1.1.1.1.cmml" xref="A4.p3.1.m1.1.1.1.1">
         <times id="A4.p3.1.m1.1.1.1.1.1.cmml" xref="A4.p3.1.m1.1.1.1.1.1">
         </times>
         <ci id="A4.p3.1.m1.1.1.1.1.2.cmml" xref="A4.p3.1.m1.1.1.1.1.2">
          𝑆
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.3.cmml" xref="A4.p3.1.m1.1.1.1.1.3">
          𝑦
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.4.cmml" xref="A4.p3.1.m1.1.1.1.1.4">
          𝑠
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.5.cmml" xref="A4.p3.1.m1.1.1.1.1.5">
          𝑡
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.6.cmml" xref="A4.p3.1.m1.1.1.1.1.6">
          𝑒
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.7.cmml" xref="A4.p3.1.m1.1.1.1.1.7">
          𝑚
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.8.cmml" xref="A4.p3.1.m1.1.1.1.1.8">
          𝑀
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.9.cmml" xref="A4.p3.1.m1.1.1.1.1.9">
          𝑒
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.10.cmml" xref="A4.p3.1.m1.1.1.1.1.10">
          𝑠
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.11.cmml" xref="A4.p3.1.m1.1.1.1.1.11">
          𝑠
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.12.cmml" xref="A4.p3.1.m1.1.1.1.1.12">
          𝑠
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.13.cmml" xref="A4.p3.1.m1.1.1.1.1.13">
          𝑎
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.14.cmml" xref="A4.p3.1.m1.1.1.1.1.14">
          𝑔
         </ci>
         <ci id="A4.p3.1.m1.1.1.1.1.15.cmml" xref="A4.p3.1.m1.1.1.1.1.15">
          𝑒
         </ci>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A4.p3.1.m1.1c">
       [SystemMesssage]
      </annotation>
     </semantics>
    </math>
    specifies the corresponding task of sample,
    <math alttext="[Query]" class="ltx_Math" display="inline" id="A4.p3.2.m2.1">
     <semantics id="A4.p3.2.m2.1a">
      <mrow id="A4.p3.2.m2.1.1.1" xref="A4.p3.2.m2.1.1.2.cmml">
       <mo id="A4.p3.2.m2.1.1.1.2" stretchy="false" xref="A4.p3.2.m2.1.1.2.1.cmml">
        [
       </mo>
       <mrow id="A4.p3.2.m2.1.1.1.1" xref="A4.p3.2.m2.1.1.1.1.cmml">
        <mi id="A4.p3.2.m2.1.1.1.1.2" xref="A4.p3.2.m2.1.1.1.1.2.cmml">
         Q
        </mi>
        <mo id="A4.p3.2.m2.1.1.1.1.1" lspace="0em" rspace="0em" xref="A4.p3.2.m2.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.2.m2.1.1.1.1.3" xref="A4.p3.2.m2.1.1.1.1.3.cmml">
         u
        </mi>
        <mo id="A4.p3.2.m2.1.1.1.1.1a" lspace="0em" rspace="0em" xref="A4.p3.2.m2.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.2.m2.1.1.1.1.4" xref="A4.p3.2.m2.1.1.1.1.4.cmml">
         e
        </mi>
        <mo id="A4.p3.2.m2.1.1.1.1.1b" lspace="0em" rspace="0em" xref="A4.p3.2.m2.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.2.m2.1.1.1.1.5" xref="A4.p3.2.m2.1.1.1.1.5.cmml">
         r
        </mi>
        <mo id="A4.p3.2.m2.1.1.1.1.1c" lspace="0em" rspace="0em" xref="A4.p3.2.m2.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.2.m2.1.1.1.1.6" xref="A4.p3.2.m2.1.1.1.1.6.cmml">
         y
        </mi>
       </mrow>
       <mo id="A4.p3.2.m2.1.1.1.3" stretchy="false" xref="A4.p3.2.m2.1.1.2.1.cmml">
        ]
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A4.p3.2.m2.1b">
       <apply id="A4.p3.2.m2.1.1.2.cmml" xref="A4.p3.2.m2.1.1.1">
        <csymbol cd="latexml" id="A4.p3.2.m2.1.1.2.1.cmml" xref="A4.p3.2.m2.1.1.1.2">
         delimited-[]
        </csymbol>
        <apply id="A4.p3.2.m2.1.1.1.1.cmml" xref="A4.p3.2.m2.1.1.1.1">
         <times id="A4.p3.2.m2.1.1.1.1.1.cmml" xref="A4.p3.2.m2.1.1.1.1.1">
         </times>
         <ci id="A4.p3.2.m2.1.1.1.1.2.cmml" xref="A4.p3.2.m2.1.1.1.1.2">
          𝑄
         </ci>
         <ci id="A4.p3.2.m2.1.1.1.1.3.cmml" xref="A4.p3.2.m2.1.1.1.1.3">
          𝑢
         </ci>
         <ci id="A4.p3.2.m2.1.1.1.1.4.cmml" xref="A4.p3.2.m2.1.1.1.1.4">
          𝑒
         </ci>
         <ci id="A4.p3.2.m2.1.1.1.1.5.cmml" xref="A4.p3.2.m2.1.1.1.1.5">
          𝑟
         </ci>
         <ci id="A4.p3.2.m2.1.1.1.1.6.cmml" xref="A4.p3.2.m2.1.1.1.1.6">
          𝑦
         </ci>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A4.p3.2.m2.1c">
       [Query]
      </annotation>
     </semantics>
    </math>
    refers to position of texts from human and
    <math alttext="[Response]" class="ltx_Math" display="inline" id="A4.p3.3.m3.1">
     <semantics id="A4.p3.3.m3.1a">
      <mrow id="A4.p3.3.m3.1.1.1" xref="A4.p3.3.m3.1.1.2.cmml">
       <mo id="A4.p3.3.m3.1.1.1.2" stretchy="false" xref="A4.p3.3.m3.1.1.2.1.cmml">
        [
       </mo>
       <mrow id="A4.p3.3.m3.1.1.1.1" xref="A4.p3.3.m3.1.1.1.1.cmml">
        <mi id="A4.p3.3.m3.1.1.1.1.2" xref="A4.p3.3.m3.1.1.1.1.2.cmml">
         R
        </mi>
        <mo id="A4.p3.3.m3.1.1.1.1.1" lspace="0em" rspace="0em" xref="A4.p3.3.m3.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.3.m3.1.1.1.1.3" xref="A4.p3.3.m3.1.1.1.1.3.cmml">
         e
        </mi>
        <mo id="A4.p3.3.m3.1.1.1.1.1a" lspace="0em" rspace="0em" xref="A4.p3.3.m3.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.3.m3.1.1.1.1.4" xref="A4.p3.3.m3.1.1.1.1.4.cmml">
         s
        </mi>
        <mo id="A4.p3.3.m3.1.1.1.1.1b" lspace="0em" rspace="0em" xref="A4.p3.3.m3.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.3.m3.1.1.1.1.5" xref="A4.p3.3.m3.1.1.1.1.5.cmml">
         p
        </mi>
        <mo id="A4.p3.3.m3.1.1.1.1.1c" lspace="0em" rspace="0em" xref="A4.p3.3.m3.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.3.m3.1.1.1.1.6" xref="A4.p3.3.m3.1.1.1.1.6.cmml">
         o
        </mi>
        <mo id="A4.p3.3.m3.1.1.1.1.1d" lspace="0em" rspace="0em" xref="A4.p3.3.m3.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.3.m3.1.1.1.1.7" xref="A4.p3.3.m3.1.1.1.1.7.cmml">
         n
        </mi>
        <mo id="A4.p3.3.m3.1.1.1.1.1e" lspace="0em" rspace="0em" xref="A4.p3.3.m3.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.3.m3.1.1.1.1.8" xref="A4.p3.3.m3.1.1.1.1.8.cmml">
         s
        </mi>
        <mo id="A4.p3.3.m3.1.1.1.1.1f" lspace="0em" rspace="0em" xref="A4.p3.3.m3.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.3.m3.1.1.1.1.9" xref="A4.p3.3.m3.1.1.1.1.9.cmml">
         e
        </mi>
       </mrow>
       <mo id="A4.p3.3.m3.1.1.1.3" stretchy="false" xref="A4.p3.3.m3.1.1.2.1.cmml">
        ]
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A4.p3.3.m3.1b">
       <apply id="A4.p3.3.m3.1.1.2.cmml" xref="A4.p3.3.m3.1.1.1">
        <csymbol cd="latexml" id="A4.p3.3.m3.1.1.2.1.cmml" xref="A4.p3.3.m3.1.1.1.2">
         delimited-[]
        </csymbol>
        <apply id="A4.p3.3.m3.1.1.1.1.cmml" xref="A4.p3.3.m3.1.1.1.1">
         <times id="A4.p3.3.m3.1.1.1.1.1.cmml" xref="A4.p3.3.m3.1.1.1.1.1">
         </times>
         <ci id="A4.p3.3.m3.1.1.1.1.2.cmml" xref="A4.p3.3.m3.1.1.1.1.2">
          𝑅
         </ci>
         <ci id="A4.p3.3.m3.1.1.1.1.3.cmml" xref="A4.p3.3.m3.1.1.1.1.3">
          𝑒
         </ci>
         <ci id="A4.p3.3.m3.1.1.1.1.4.cmml" xref="A4.p3.3.m3.1.1.1.1.4">
          𝑠
         </ci>
         <ci id="A4.p3.3.m3.1.1.1.1.5.cmml" xref="A4.p3.3.m3.1.1.1.1.5">
          𝑝
         </ci>
         <ci id="A4.p3.3.m3.1.1.1.1.6.cmml" xref="A4.p3.3.m3.1.1.1.1.6">
          𝑜
         </ci>
         <ci id="A4.p3.3.m3.1.1.1.1.7.cmml" xref="A4.p3.3.m3.1.1.1.1.7">
          𝑛
         </ci>
         <ci id="A4.p3.3.m3.1.1.1.1.8.cmml" xref="A4.p3.3.m3.1.1.1.1.8">
          𝑠
         </ci>
         <ci id="A4.p3.3.m3.1.1.1.1.9.cmml" xref="A4.p3.3.m3.1.1.1.1.9">
          𝑒
         </ci>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A4.p3.3.m3.1c">
       [Response]
      </annotation>
     </semantics>
    </math>
    refers to contents expected for LLM.
The special tokens
    <math alttext="&lt;vision&gt;" class="ltx_Math" display="inline" id="A4.p3.4.m4.1">
     <semantics id="A4.p3.4.m4.1a">
      <mrow id="A4.p3.4.m4.1.1.1" xref="A4.p3.4.m4.1.1.2.cmml">
       <mo fence="true" id="A4.p3.4.m4.1.1.1.2" rspace="0em" xref="A4.p3.4.m4.1.1.2.1.cmml">
        &lt;
       </mo>
       <mrow id="A4.p3.4.m4.1.1.1.1" xref="A4.p3.4.m4.1.1.1.1.cmml">
        <mi id="A4.p3.4.m4.1.1.1.1.2" xref="A4.p3.4.m4.1.1.1.1.2.cmml">
         v
        </mi>
        <mo id="A4.p3.4.m4.1.1.1.1.1" lspace="0em" rspace="0em" xref="A4.p3.4.m4.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.4.m4.1.1.1.1.3" xref="A4.p3.4.m4.1.1.1.1.3.cmml">
         i
        </mi>
        <mo id="A4.p3.4.m4.1.1.1.1.1a" lspace="0em" rspace="0em" xref="A4.p3.4.m4.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.4.m4.1.1.1.1.4" xref="A4.p3.4.m4.1.1.1.1.4.cmml">
         s
        </mi>
        <mo id="A4.p3.4.m4.1.1.1.1.1b" lspace="0em" rspace="0em" xref="A4.p3.4.m4.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.4.m4.1.1.1.1.5" xref="A4.p3.4.m4.1.1.1.1.5.cmml">
         i
        </mi>
        <mo id="A4.p3.4.m4.1.1.1.1.1c" lspace="0em" rspace="0em" xref="A4.p3.4.m4.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.4.m4.1.1.1.1.6" xref="A4.p3.4.m4.1.1.1.1.6.cmml">
         o
        </mi>
        <mo id="A4.p3.4.m4.1.1.1.1.1d" lspace="0em" rspace="0em" xref="A4.p3.4.m4.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.4.m4.1.1.1.1.7" xref="A4.p3.4.m4.1.1.1.1.7.cmml">
         n
        </mi>
       </mrow>
       <mo fence="true" id="A4.p3.4.m4.1.1.1.3" lspace="0em" xref="A4.p3.4.m4.1.1.2.1.cmml">
        &gt;
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A4.p3.4.m4.1b">
       <apply id="A4.p3.4.m4.1.1.2.cmml" xref="A4.p3.4.m4.1.1.1">
        <csymbol cd="latexml" id="A4.p3.4.m4.1.1.2.1.cmml" xref="A4.p3.4.m4.1.1.1.2">
         expectation
        </csymbol>
        <apply id="A4.p3.4.m4.1.1.1.1.cmml" xref="A4.p3.4.m4.1.1.1.1">
         <times id="A4.p3.4.m4.1.1.1.1.1.cmml" xref="A4.p3.4.m4.1.1.1.1.1">
         </times>
         <ci id="A4.p3.4.m4.1.1.1.1.2.cmml" xref="A4.p3.4.m4.1.1.1.1.2">
          𝑣
         </ci>
         <ci id="A4.p3.4.m4.1.1.1.1.3.cmml" xref="A4.p3.4.m4.1.1.1.1.3">
          𝑖
         </ci>
         <ci id="A4.p3.4.m4.1.1.1.1.4.cmml" xref="A4.p3.4.m4.1.1.1.1.4">
          𝑠
         </ci>
         <ci id="A4.p3.4.m4.1.1.1.1.5.cmml" xref="A4.p3.4.m4.1.1.1.1.5">
          𝑖
         </ci>
         <ci id="A4.p3.4.m4.1.1.1.1.6.cmml" xref="A4.p3.4.m4.1.1.1.1.6">
          𝑜
         </ci>
         <ci id="A4.p3.4.m4.1.1.1.1.7.cmml" xref="A4.p3.4.m4.1.1.1.1.7">
          𝑛
         </ci>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A4.p3.4.m4.1c">
       &lt;vision&gt;
      </annotation>
     </semantics>
    </math>
    &amp;
    <math alttext="&lt;/vision&gt;" class="ltx_math_unparsed" display="inline" id="A4.p3.5.m5.2">
     <semantics id="A4.p3.5.m5.2a">
      <mrow id="A4.p3.5.m5.2b">
       <mo id="A4.p3.5.m5.1.1" rspace="0em">
        &lt;
       </mo>
       <mo id="A4.p3.5.m5.2.2" lspace="0em">
        /
       </mo>
       <mi id="A4.p3.5.m5.2.3">
        v
       </mi>
       <mi id="A4.p3.5.m5.2.4">
        i
       </mi>
       <mi id="A4.p3.5.m5.2.5">
        s
       </mi>
       <mi id="A4.p3.5.m5.2.6">
        i
       </mi>
       <mi id="A4.p3.5.m5.2.7">
        o
       </mi>
       <mi id="A4.p3.5.m5.2.8">
        n
       </mi>
       <mo id="A4.p3.5.m5.2.9">
        &gt;
       </mo>
      </mrow>
      <annotation encoding="application/x-tex" id="A4.p3.5.m5.2c">
       &lt;/vision&gt;
      </annotation>
     </semantics>
    </math>
    represents start and end positions for vision content. We use
    <math alttext="&lt;Img&gt;&lt;/Img&gt;" class="ltx_math_unparsed" display="inline" id="A4.p3.6.m6.1">
     <semantics id="A4.p3.6.m6.1a">
      <mrow id="A4.p3.6.m6.1b">
       <mo id="A4.p3.6.m6.1.1">
        &lt;
       </mo>
       <mi id="A4.p3.6.m6.1.2">
        I
       </mi>
       <mi id="A4.p3.6.m6.1.3">
        m
       </mi>
       <mi id="A4.p3.6.m6.1.4">
        g
       </mi>
       <mo id="A4.p3.6.m6.1.5" rspace="0em">
        &gt;
       </mo>
       <mo id="A4.p3.6.m6.1.6" lspace="0em" rspace="0em">
        &lt;
       </mo>
       <mo id="A4.p3.6.m6.1.7" lspace="0em">
        /
       </mo>
       <mi id="A4.p3.6.m6.1.8">
        I
       </mi>
       <mi id="A4.p3.6.m6.1.9">
        m
       </mi>
       <mi id="A4.p3.6.m6.1.10">
        g
       </mi>
       <mo id="A4.p3.6.m6.1.11">
        &gt;
       </mo>
      </mrow>
      <annotation encoding="application/x-tex" id="A4.p3.6.m6.1c">
       &lt;Img&gt;&lt;/Img&gt;
      </annotation>
     </semantics>
    </math>
    and
    <math alttext="&lt;Pcl&gt;&lt;/Pcl&gt;" class="ltx_math_unparsed" display="inline" id="A4.p3.7.m7.1">
     <semantics id="A4.p3.7.m7.1a">
      <mrow id="A4.p3.7.m7.1b">
       <mo id="A4.p3.7.m7.1.1">
        &lt;
       </mo>
       <mi id="A4.p3.7.m7.1.2">
        P
       </mi>
       <mi id="A4.p3.7.m7.1.3">
        c
       </mi>
       <mi id="A4.p3.7.m7.1.4">
        l
       </mi>
       <mo id="A4.p3.7.m7.1.5" rspace="0em">
        &gt;
       </mo>
       <mo id="A4.p3.7.m7.1.6" lspace="0em" rspace="0em">
        &lt;
       </mo>
       <mo id="A4.p3.7.m7.1.7" lspace="0em">
        /
       </mo>
       <mi id="A4.p3.7.m7.1.8">
        P
       </mi>
       <mi id="A4.p3.7.m7.1.9">
        c
       </mi>
       <mi id="A4.p3.7.m7.1.10">
        l
       </mi>
       <mo id="A4.p3.7.m7.1.11">
        &gt;
       </mo>
      </mrow>
      <annotation encoding="application/x-tex" id="A4.p3.7.m7.1c">
       &lt;Pcl&gt;&lt;/Pcl&gt;
      </annotation>
     </semantics>
    </math>
    in 2D and 3D datasets, respectively.
The training objective used is next token prediction loss, and only text tokens of
    <math alttext="[Response]" class="ltx_Math" display="inline" id="A4.p3.8.m8.1">
     <semantics id="A4.p3.8.m8.1a">
      <mrow id="A4.p3.8.m8.1.1.1" xref="A4.p3.8.m8.1.1.2.cmml">
       <mo id="A4.p3.8.m8.1.1.1.2" stretchy="false" xref="A4.p3.8.m8.1.1.2.1.cmml">
        [
       </mo>
       <mrow id="A4.p3.8.m8.1.1.1.1" xref="A4.p3.8.m8.1.1.1.1.cmml">
        <mi id="A4.p3.8.m8.1.1.1.1.2" xref="A4.p3.8.m8.1.1.1.1.2.cmml">
         R
        </mi>
        <mo id="A4.p3.8.m8.1.1.1.1.1" lspace="0em" rspace="0em" xref="A4.p3.8.m8.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.8.m8.1.1.1.1.3" xref="A4.p3.8.m8.1.1.1.1.3.cmml">
         e
        </mi>
        <mo id="A4.p3.8.m8.1.1.1.1.1a" lspace="0em" rspace="0em" xref="A4.p3.8.m8.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.8.m8.1.1.1.1.4" xref="A4.p3.8.m8.1.1.1.1.4.cmml">
         s
        </mi>
        <mo id="A4.p3.8.m8.1.1.1.1.1b" lspace="0em" rspace="0em" xref="A4.p3.8.m8.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.8.m8.1.1.1.1.5" xref="A4.p3.8.m8.1.1.1.1.5.cmml">
         p
        </mi>
        <mo id="A4.p3.8.m8.1.1.1.1.1c" lspace="0em" rspace="0em" xref="A4.p3.8.m8.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.8.m8.1.1.1.1.6" xref="A4.p3.8.m8.1.1.1.1.6.cmml">
         o
        </mi>
        <mo id="A4.p3.8.m8.1.1.1.1.1d" lspace="0em" rspace="0em" xref="A4.p3.8.m8.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.8.m8.1.1.1.1.7" xref="A4.p3.8.m8.1.1.1.1.7.cmml">
         n
        </mi>
        <mo id="A4.p3.8.m8.1.1.1.1.1e" lspace="0em" rspace="0em" xref="A4.p3.8.m8.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.8.m8.1.1.1.1.8" xref="A4.p3.8.m8.1.1.1.1.8.cmml">
         s
        </mi>
        <mo id="A4.p3.8.m8.1.1.1.1.1f" lspace="0em" rspace="0em" xref="A4.p3.8.m8.1.1.1.1.1.cmml">
         ​
        </mo>
        <mi id="A4.p3.8.m8.1.1.1.1.9" xref="A4.p3.8.m8.1.1.1.1.9.cmml">
         e
        </mi>
       </mrow>
       <mo id="A4.p3.8.m8.1.1.1.3" stretchy="false" xref="A4.p3.8.m8.1.1.2.1.cmml">
        ]
       </mo>
      </mrow>
      <annotation-xml encoding="MathML-Content" id="A4.p3.8.m8.1b">
       <apply id="A4.p3.8.m8.1.1.2.cmml" xref="A4.p3.8.m8.1.1.1">
        <csymbol cd="latexml" id="A4.p3.8.m8.1.1.2.1.cmml" xref="A4.p3.8.m8.1.1.1.2">
         delimited-[]
        </csymbol>
        <apply id="A4.p3.8.m8.1.1.1.1.cmml" xref="A4.p3.8.m8.1.1.1.1">
         <times id="A4.p3.8.m8.1.1.1.1.1.cmml" xref="A4.p3.8.m8.1.1.1.1.1">
         </times>
         <ci id="A4.p3.8.m8.1.1.1.1.2.cmml" xref="A4.p3.8.m8.1.1.1.1.2">
          𝑅
         </ci>
         <ci id="A4.p3.8.m8.1.1.1.1.3.cmml" xref="A4.p3.8.m8.1.1.1.1.3">
          𝑒
         </ci>
         <ci id="A4.p3.8.m8.1.1.1.1.4.cmml" xref="A4.p3.8.m8.1.1.1.1.4">
          𝑠
         </ci>
         <ci id="A4.p3.8.m8.1.1.1.1.5.cmml" xref="A4.p3.8.m8.1.1.1.1.5">
          𝑝
         </ci>
         <ci id="A4.p3.8.m8.1.1.1.1.6.cmml" xref="A4.p3.8.m8.1.1.1.1.6">
          𝑜
         </ci>
         <ci id="A4.p3.8.m8.1.1.1.1.7.cmml" xref="A4.p3.8.m8.1.1.1.1.7">
          𝑛
         </ci>
         <ci id="A4.p3.8.m8.1.1.1.1.8.cmml" xref="A4.p3.8.m8.1.1.1.1.8">
          𝑠
         </ci>
         <ci id="A4.p3.8.m8.1.1.1.1.9.cmml" xref="A4.p3.8.m8.1.1.1.1.9">
          𝑒
         </ci>
        </apply>
       </apply>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A4.p3.8.m8.1c">
       [Response]
      </annotation>
     </semantics>
    </math>
    count for loss computation.
As we use CLIP
    <cite class="ltx_cite ltx_citemacro_cite">
     [
     <a class="ltx_ref" href="#bib.bib38" title="">
      38
     </a>
     ]
    </cite>
    pre-trained ViT-Large-14 as visual encoder, the number of vision tokens are 256 and length of text tokens after vision tokens are limited to
    <math alttext="400" class="ltx_Math" display="inline" id="A4.p3.9.m9.1">
     <semantics id="A4.p3.9.m9.1a">
      <mn id="A4.p3.9.m9.1.1" xref="A4.p3.9.m9.1.1.cmml">
       400
      </mn>
      <annotation-xml encoding="MathML-Content" id="A4.p3.9.m9.1b">
       <cn id="A4.p3.9.m9.1.1.cmml" type="integer" xref="A4.p3.9.m9.1.1">
        400
       </cn>
      </annotation-xml>
      <annotation encoding="application/x-tex" id="A4.p3.9.m9.1c">
       400
      </annotation>
     </semantics>
    </math>
    in training.
   </p>
  </div>
 </section>
 <section class="ltx_appendix" id="A5">
  <h2 class="ltx_title ltx_title_appendix">
   <span class="ltx_tag ltx_tag_appendix">
    Appendix E
   </span>
   Demonstrations
  </h2>
  <section class="ltx_subsection" id="A5.SS1">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     E.1
    </span>
    Results on CIFAR10
   </h3>
   <div class="ltx_para" id="A5.SS1.p1">
    <p class="ltx_p" id="A5.SS1.p1.1">
     Figure
     <a class="ltx_ref" href="#A5.F19" title="Figure 19 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       19
      </span>
     </a>
     presents some examples responses from model trained by our dataset on CIFAR10, where the model’s answers were judged as incorrect in the evaluation, but in fact, our model provided a more granular classification result. The left column shows the test images from CIFAR10, and the right column displays the images of the objects that the model classified, including toad
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib61" title="">
       61
      </a>
      ]
     </cite>
     , Land Rover Series II
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib62" title="">
       62
      </a>
      ]
     </cite>
     , Mirage 2000D fighter aircraft
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib63" title="">
       63
      </a>
      ]
     </cite>
     and police car
     <cite class="ltx_cite ltx_citemacro_cite">
      [
      <a class="ltx_ref" href="#bib.bib64" title="">
       64
      </a>
      ]
     </cite>
     . It is evident that the fine-grained objects classified by our model have very similar features to the input images, demonstrating its ability to perform fine-grained classification.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A5.SS2">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     E.2
    </span>
    More detailed information on image caption
   </h3>
   <div class="ltx_para" id="A5.SS2.p1">
    <p class="ltx_p" id="A5.SS2.p1.1">
     Our model performed poorly on the Flickr30k dataset in terms of BLEU scores. This is because model’s responses include additional details that are not captured by the ground truth captions. Figure
     <a class="ltx_ref" href="#A5.F20" title="Figure 20 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       20
      </span>
     </a>
     illustrates this phenomenon, where the highlighted text in red represents the matching ground truth captions, while the text in orange is not matched but is still relevant to the image content. It is evident that our model is capable of providing more detailed descriptions of the image, which is not captured by the traditional BLEU metric.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A5.SS3">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     E.3
    </span>
    Comparison with LLaVA on detection and counting tasks
   </h3>
   <div class="ltx_para" id="A5.SS3.p1">
    <p class="ltx_p" id="A5.SS3.p1.1">
     We compared the performance of model trained by our dataset with that by LLaVA on both object detection and counting tasks. Figure
     <a class="ltx_ref" href="#A5.F21" title="Figure 21 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       21
      </span>
     </a>
     illustrates the comparison results on detection, where the leftmost images represent the ground truth bounding box, and the rightmost images show the visualizations of the responses after entity extraction.
    </p>
   </div>
   <div class="ltx_para" id="A5.SS3.p2">
    <p class="ltx_p" id="A5.SS3.p2.1">
     Although LLaVA was able to identify the approximate location of the object, it was unable to provide precise bounding box coordinates. On the other hand, our model demonstrated superior detection capabilities after fine-tuning on detection-related data and was able to provide more accurate bounding box coordinates. Additionally, our model also exhibited better counting performance, as shown in Figure
     <a class="ltx_ref" href="#A5.F22" title="Figure 22 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       22
      </span>
     </a>
     . It is worth noting that counting is essentially a task that tests the model’s localization ability.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A5.SS4">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     E.4
    </span>
    Results of binary-loc metric and GPT metric
   </h3>
   <div class="ltx_para" id="A5.SS4.p1">
    <p class="ltx_p" id="A5.SS4.p1.1">
     We present the results of our model and LLaVA on the binary locating metric in Figure
     <a class="ltx_ref" href="#A5.F24" title="Figure 24 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       24
      </span>
     </a>
     (a), where our model demonstrates more precise localization abilities. The green points in the image are the visualization of the predicted key points. In the second row of the figure, our model outputs a bounding box, which we break down into two position coordinates (top-left and bottom-right) during entity extraction.
    </p>
   </div>
   <div class="ltx_para" id="A5.SS4.p2">
    <p class="ltx_p" id="A5.SS4.p2.1">
     In Figure
     <a class="ltx_ref" href="#A5.F24" title="Figure 24 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       24
      </span>
     </a>
     (b), we show the evaluation results of the two models’ image captioning responses using the GPT metric. The GPT metric considers our model’s responses to be more specific and accurate compared to LLaVA, resulting in a higher ranking.
These results further demonstrate the effectiveness of the model trained on our dataset in accurately detecting, locating, and describing objects in images.
    </p>
   </div>
  </section>
  <section class="ltx_subsection" id="A5.SS5">
   <h3 class="ltx_title ltx_title_subsection">
    <span class="ltx_tag ltx_tag_subsection">
     E.5
    </span>
    More demonstration examples
   </h3>
   <div class="ltx_para" id="A5.SS5.p1">
    <p class="ltx_p" id="A5.SS5.p1.1">
     Figure
     <a class="ltx_ref" href="#A5.F23" title="Figure 23 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       23
      </span>
     </a>
     shows the results of our model on VQA task and Figure
     <a class="ltx_ref" href="#A5.F25" title="Figure 25 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       25
      </span>
     </a>
     shows its example results on 3DVQA task. Figure
     <a class="ltx_ref" href="#A5.F26" title="Figure 26 ‣ E.5 More demonstration examples ‣ Appendix E Demonstrations ‣ LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark">
      <span class="ltx_text ltx_ref_tag">
       26
      </span>
     </a>
     shows the results on in-the-wild images.
    </p>
   </div>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
   <div class="ltx_pagination ltx_role_newpage">
   </div>
   <figure class="ltx_figure" id="A5.F17">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="710" id="A5.F17.g1" src="/html/2306.06687/assets/x17.png" width="369"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 17:
     </span>
     System messages for benchmark on image tasks
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F18">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="295" id="A5.F18.g1" src="/html/2306.06687/assets/x18.png" width="369"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 18:
     </span>
     System messages for benchmark on point cloud tasks
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F19">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="308" id="A5.F19.g1" src="/html/2306.06687/assets/x19.png" width="369"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 19:
     </span>
     Results of model trained by our dataset on CIFAR10. (a) The images from CIFAR10 test set. (b) The instruction, response from our model and the ground truth. (c) The reference images.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F20">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="467" id="A5.F20.g1" src="/html/2306.06687/assets/x20.png" width="415"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 20:
     </span>
     Our model’s Response on fickr30k dataset. The highlighted text in red represents the matching ground truth captions in BLEU evaluation. The text in orange is not matched but is still relevant to the image content.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F21">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="227" id="A5.F21.g1" src="/html/2306.06687/assets/x21.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 21:
     </span>
     Comparison of models trained on our dataset and LLaVA on VOC2012.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F22">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="110" id="A5.F22.g1" src="/html/2306.06687/assets/x22.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 22:
     </span>
     Comparison of models trained on our dataset and LLaVA on FSC147.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F23">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="265" id="A5.F23.g1" src="/html/2306.06687/assets/x23.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 23:
     </span>
     (a) Example results of models trained on our dataset on SQAimage. (b) Example results of our model on AI2D.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F24">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="493" id="A5.F24.g1" src="/html/2306.06687/assets/x24.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 24:
     </span>
     Comparison of models trained on our dataset and LLaVA on binary-loc metric and GPT metric. (a) The comparison on binary-loc metric. (b) The results of GPT metric.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F25">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="351" id="A5.F25.g1" src="/html/2306.06687/assets/x25.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 25:
     </span>
     Example results of our model on ScanQA.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F26">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="209" id="A5.F26.g1" src="/html/2306.06687/assets/x26.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 26:
     </span>
     Example results of our model on in-the-wild images.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F27">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="473" id="A5.F27.g1" src="/html/2306.06687/assets/x27.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 27:
     </span>
     Question template pool and Answer template pool for classification task.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F28">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="447" id="A5.F28.g1" src="/html/2306.06687/assets/x28.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 28:
     </span>
     Question template pool and Answer template pool for detection task in 2D vision.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F29">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="375" id="A5.F29.g1" src="/html/2306.06687/assets/x29.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 29:
     </span>
     Question template pool and Answer template pool for keypoint detection task in 2D vision.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F30">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="389" id="A5.F30.g1" src="/html/2306.06687/assets/x30.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 30:
     </span>
     Question template pool and Answer template pool for counting task in 2D vision.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F31">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="342" id="A5.F31.g1" src="/html/2306.06687/assets/x31.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 31:
     </span>
     Question template pool and Answer template pool for object classification in 3D vision.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F32">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="351" id="A5.F32.g1" src="/html/2306.06687/assets/x32.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 32:
     </span>
     Question template pool and Answer template pool for object detection in 3D vision.
    </figcaption>
   </figure>
   <figure class="ltx_figure" id="A5.F33">
    <img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="368" id="A5.F33.g1" src="/html/2306.06687/assets/x33.png" width="461"/>
    <figcaption class="ltx_caption ltx_centering">
     <span class="ltx_tag ltx_tag_figure">
      Figure 33:
     </span>
     Question template pool and Answer template pool for visual question answering in 3D vision.
    </figcaption>
   </figure>
  </section>
 </section>
</article>
