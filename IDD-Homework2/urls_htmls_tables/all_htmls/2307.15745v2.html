<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2307.15745] Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering</title><meta property="og:description" content="Visual question answering (VQA) has the potential to make the Internet more accessible in an interactive way, allowing people who cannot see images to ask questions about them. However, multiple studies have shown that‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2307.15745">

<!--Generated on Wed Feb 28 16:00:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nandita Naik
<br class="ltx_break">Stanford University
<br class="ltx_break"><span id="id1.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">nanditan@cs.stanford.edu</span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Christopher Potts
<br class="ltx_break">Stanford University
<br class="ltx_break"><span id="id2.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">cgpotts@stanford.edu</span>
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Elisa Kreiss
<br class="ltx_break">Stanford University
<br class="ltx_break"><span id="id3.1.id1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">ekreiss@stanford.edu</span>
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id4.id1" class="ltx_p">Visual question answering (VQA) has the potential to make the Internet more accessible in an interactive way, allowing people who cannot see images to ask questions about them. However, multiple studies have shown that people who are blind or have low-vision prefer image explanations that incorporate the context in which an image appears, yet current VQA datasets focus on images in isolation. We argue that VQA models will not fully succeed at meeting people‚Äôs needs unless they take context into account. To further motivate and analyze the distinction between different contexts, we introduce Context-VQA<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>More details about the dataset and code are available at <a target="_blank" href="https://github.com/nnaik39/context-vqa" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/nnaik39/context-vqa</a>.</span></span></span>, a VQA dataset that pairs images with contexts, specifically types of websites (e.g., a shopping website). We find that the types of questions vary systematically across contexts. For example, images presented in a travel context garner 2 times more ‚ÄúWhere?‚Äù¬†questions, and images on social media and news garner 2.8 and 1.8 times more ‚ÄúWho?‚Äù¬†questions than the average. We also find that context effects are especially important when participants can‚Äôt see the image. These results demonstrate that context affects the types of questions asked and that VQA models should be context-sensitive to better meet people‚Äôs needs, especially in accessibility settings.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<figure id="S1.F1" class="ltx_figure"><img src="/html/2307.15745/assets/x1.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="369" height="247" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>We propose the dataset Context-VQA, where images are paired with various contexts. This example shows how both questions and answers vary across different contexts. On a social media website, the details about people within the image are more relevant, while in the context of a shopping website, people will want to know more specific details about the suitcase.</figcaption>
</figure>
<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Images are omnipresent on the Web, appearing in various contexts‚Äîfor example, news websites, shopping websites, or social media <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>. This poses an issue for nonvisual accessibility: How will people who cannot see the image (e.g., due to image loading issues or a visual impairment) understand the content that is presented? In this case, a textual interpretation of the image is needed.
While much recent work has focused on leveraging AI to generate image descriptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite>, being able to inquire about specific details gives the user agency over obtaining information that is specifically suited to their needs (which is, for example, leveraged in interactive accessibility studies <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>).
Visual Question Answering (VQA) models are a promising tool for providing image-based information on the fly. However, most VQA efforts are focused on evaluating machine ‚Äúunderstanding,‚Äù an abstract task which seeks to test the spatial or object reasoning of machine learning models.
The resulting systems aren‚Äôt easily extendable to an accessibility application <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. The VizWiz dataset is an exception, where image‚Äìquestion pairs are obtained from the actual use case of blind and low-vision people asking questions about information in the visual world they‚Äôre taking pictures of¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">In contrast to traditional machine understanding VQA datasets, we propose Context-VQA, a dataset which recenters the focus on the use case of image accessibility and the changing information needs for images occurring in various sources online. Context-VQA complements VizWiz but is distinct from it, since the images are part of the general public discourse and not directly conditioned on a question.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Current VQA tasks assume a one-size-fits-all approach, where each image is associated with a fixed set of questions.
However, recent work suggests that different pieces of information become relevant depending on the particular context in which people encountered the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>. Consider the question and answer provided in Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. It is difficult to infer the context from the image itself, yet the meaning of the image changes depending on the context. Our Context-VQA dataset presents images in distinct contexts to elicit more naturalistic question distributions.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">The purpose of the Context-VQA dataset is to align previous VQA efforts with an explicit accessibility goal, and to situate questions and answers within diverse Web contexts
to more closely reflect user needs.
Each entry consists of a question, image, answer, and context, where the question and answer are both conditioned upon the context. We find that context shapes which questions become informative and that this contextual sensitivity is even more pronounced when participants are prompted to ask questions based on a description of the image and not the ground-truth image itself. This suggests that specifically in cases of image inaccessibility, the context plays a major role for VQA tasks.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p">Our contributions are:</p>
<ol id="S1.I1" class="ltx_enumerate">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">A context-sensitive VQA dataset, with questions and answers that were written for images situated in specific Web contexts. To account for variation in question-writing strategies, half of the participants were shown the image and half were shown an AI-generated description of the image as a basis for their questions. The dataset therefore allows not only an investigation of context-sensitivity, but also of task formulation. While the image-visible version of the study might
allow for more grounding,
the description-only version imitates the user experience more closely.</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">An assessment of how Web context shapes which pieces of information in an image are of interest. We find that the distribution of question types varies across contexts in an intuitive manner. These findings underscore that the context affects the meaning of an image, and motivate context-aware, purposeful VQA models.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>

<section id="S2.SS0.SSS0.Px1" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Context in Image Descriptions</h4>

<div id="S2.SS0.SSS0.Px1.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px1.p1.1" class="ltx_p">Making image content accessible through textual descriptions and explanations requires selecting pieces of visual information that appear most relevant.
This has been a focus of investigation specifically for the usefulness of image descriptions.
Traditionally, generating image descriptions takes a one-size-fits-all approach, where a single description is used across many different contexts. However, there is a growing consensus that people‚Äôs information needs for the same image changes depending on the context where the image appears <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib17" title="" class="ltx_ref">17</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite>. For example, while people prefer details about the color and attributes of the clothing on shopping websites, information about the relationships between people depicted in the image become more relevant in the context of social media. Furthermore, users also want image descriptions to clarify why the image appears on a site <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>. In an interactive image accessibility study between a BLV and a sighted user, this context dependence was also observable from the specific questions the BLV participant asked about the images <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, suggesting that context needs to play a role not just for image descriptions but also VQA systems.</p>
</div>
</section>
<section id="S2.SS0.SSS0.Px2" class="ltx_paragraph">
<h4 class="ltx_title ltx_title_paragraph">Visual Question Answering Datasets</h4>

<div id="S2.SS0.SSS0.Px2.p1" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p1.1" class="ltx_p">VQA tasks
probe vision‚Äìlanguage models for their alignment between the two domains.
VQA-v2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> emphasizes open-ended, free-form questions. Visual7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> requires the model to ground its answer in specific regions of the image. CLEVR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> and GQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> test spatial reasoning abilities. OK-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> requires outside knowledge to answer questions. These prior datasets are focused on evaluating machine ‚Äúunderstanding,‚Äù an abstract task which tests spatial and logical reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>. In contrast, Context-VQA seeks to prioritize the type of questions that are most relevant to <span id="S2.SS0.SSS0.Px2.p1.1.1" class="ltx_text ltx_font_italic">people</span> based on contextually situated images.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p2" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p2.1" class="ltx_p">The Visual Dialog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> dataset established the task of ‚Äúvisual chatbots,‚Äù models that can hold a conversation about an image and answer follow-up questions. Similarly, the questions in Context-VQA were generated from giving study participants image descriptions and asking them to write follow-up questions. Where Context-VQA diverges from Visual Dialog, however, is the incorporation of context into the dataset, which moves away from a one-size-fits-all approach
and towards reflecting the variety of questions asked for contextually situated images.</p>
</div>
<div id="S2.SS0.SSS0.Px2.p3" class="ltx_para">
<p id="S2.SS0.SSS0.Px2.p3.1" class="ltx_p">VizWiz <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>, similarly to Context-VQA, is also designed to reflect image accessibility needs but for a distinct goal.
All the images and the questions in VizWiz come from blind people who had a question about something in their environment, took a picture, and uploaded it to the VizWiz app along with their question, where crowdsourced human workers then provided an answer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. For instance, someone might take an image of the error message on their rebooting computer screen or a clothing item of which they want to know the color.
In VizWiz, the images are taken to explicitly answer a single question. However, the images that need to be made accessible online are often intended to meet multiple information needs in varied contexts.
Thus,
Context-VQA addresses a separate image accessibility purpose than VizWiz and complements that effort.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Construction</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">The goal for the Context-VQA dataset is to allow for a direct comparison of how questions for images change based on
image context.
To do so, we collected naturalistic images from six different types of websites, which were selected based on prior work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>.
In a norming study, participants were presented with an image and, for each of the six contexts, rated how likely it was that the image appeared in the given context. We used this study to select plausible image-context pairs that were further annotated with questions (Experiment 1) and answers (Experiment 2). All human-subject experiments were run on the crowdsourcing platform Prolific
under an IRB protocol.</p>
</div>
<figure id="S3.F2" class="ltx_figure"><img src="/html/2307.15745/assets/x2.png" id="S3.F2.1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="701" height="410" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Distribution of question types across contexts for the description-only study (top) and the image+description study (bottom), where error bars show variance.
The distinction between contexts is more pronounced in the description-only study, while in the image+description study, the question type proportions are more similar across contexts. Particularly in the description-only study, Social Media and News have more <em id="S3.F2.5.1" class="ltx_emph ltx_font_italic">who</em> questions, Travel has more <em id="S3.F2.6.2" class="ltx_emph ltx_font_italic">where</em> questions, and Shopping has more <em id="S3.F2.7.3" class="ltx_emph ltx_font_italic">what</em> questions.</figcaption>
</figure>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Materials</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">The contexts needed to be specific enough to be informative, yet broad enough to account for a range of images. Drawing inspiration from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite>, we selected six contexts: Shopping, Travel, Social Media, Health, News, and Science Magazines. These contexts also overlapped (e.g., an image of clothes could appear in Social Media and Shopping).</p>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">The images were sampled by hand, prioritizing high-quality digital images from diverse websites. Images were sourced in the Shopping category from clothes and dorm shopping websites; images in the Travel category from travel blogs and hotel websites; images in the Health category from health and fitness blogs; images on Social Media from Twitter, Pinterest, and Instagram; images in the Science category from Popular Mechanics and Popular Science; and images in the News category from the New York Times and San Francisco Chronicle. We selected seven images from each source context, resulting in 42 images. Based on the image-context norming study results, we selected 35 images for further annotation, which were rated to plausibly appear in up to 3 distinct contexts.</p>
</div>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Experiment 1: Question Generation</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">Our first study aimed to generate questions conditioned on the image and context.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para">
<p id="S3.SS2.p2.1" class="ltx_p"><span id="S3.SS2.p2.1.1" class="ltx_text ltx_font_bold">Task</span>
There were two versions of the question generation task: description-only and image+description.
In the description-only task, participants were given a context and an image description generated by BLIP-2 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite>. In the image+description version, participants were additionally shown the image itself. As an annotation quality check, we first asked them to write a justification for why the image might appear in the given context‚Äîlow-effort or incomprehensible justifications resulted in data exclusion. They were then asked to suggest two follow-up questions that a person who only had access to the image description would ask. Prior work has noted that having the questioner not see the image could help remove visual priming bias‚Äìfor example, questioners will often write, ‚ÄúIs there a dog in this image?‚Äù only if there is a dog in the image <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. The description-only condition also mimics the real-life condition, where people wouldn‚Äôt see the image directly and ask follow-up questions based on a short description.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para">
<p id="S3.SS2.p3.1" class="ltx_p">We ran six trials. Each participant saw six different, randomly selected image‚Äìcontext pairs, and we ensured that no participant saw the same image or context repeated twice.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p"><span id="S3.SS2.p4.1.1" class="ltx_text ltx_font_bold">Participants</span>
We recruited 55 participants over Prolific. They took an average of 8.5 minutes to complete the trials and were paid $13.52 per hour on average.</p>
</div>
<div id="S3.SS2.p5" class="ltx_para">
<p id="S3.SS2.p5.1" class="ltx_p"><span id="S3.SS2.p5.1.1" class="ltx_text ltx_font_bold">Post-processing</span> We collected 1,320 questions across both studies. We excluded participants who expressed in the post-questionnaire that they might have done the study incorrectly, or whose written justifications for why the image appeared in the context was less than 30 characters, resulting in 1,096 questions. We also manually filtered questions that were inappropriate or not questions, leaving 1,032 questions. For each of the 65 unique image-context pairs, we selected two questions at random from each study for further answer annotation.</p>
</div>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Experiment 2: Answer Generation</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">For the questions generated in Experiment 1, we collected answer annotations.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Task</span>
Each participant saw six unique image‚Äìcontext pairs, associated with a question, which were randomly sampled from our dataset of questions. They were asked to justify the occurrence of the image in the context and answer the question based on the image. They also had an option to indicate if the question was unanswerable.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p"><span id="S3.SS3.p3.1.1" class="ltx_text ltx_font_bold">Participants</span>
We recruited 100 participants over Prolific. Participants took an average of 8.5 minutes to complete and were paid an average of $13.57 per hour.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Post-processing</span>
We collected 1,260 answers for 202 questions. We excluded participants who expressed in the post-questionnaire that they might have done the study incorrectly, or whose written justifications for why the image appeared in the context was less than 30 characters. After exclusions, the total number of answers collected was 568, averaging 3 question-answers per image-context pair, 8 of which were rated unanswerable by at least 3 people.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Dataset Analysis</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">Based on prior research, we hypothesized that the context images are presented in shapes the questions participants will ask. To test this hypothesis, we investigated whether participants chose different question types dependent on the context. For analysis, we selected the question types of <em id="S4.p1.1.1" class="ltx_emph ltx_font_italic">what</em>, <em id="S4.p1.1.2" class="ltx_emph ltx_font_italic">who</em>, <em id="S4.p1.1.3" class="ltx_emph ltx_font_italic">where</em>, <em id="S4.p1.1.4" class="ltx_emph ltx_font_italic">why</em>, <em id="S4.p1.1.5" class="ltx_emph ltx_font_italic">is</em>, <em id="S4.p1.1.6" class="ltx_emph ltx_font_italic">how</em>, and <em id="S4.p1.1.7" class="ltx_emph ltx_font_italic">when</em>, where <em id="S4.p1.1.8" class="ltx_emph ltx_font_italic">is</em> questions represent all questions requiring only binary yes/no responses (i.e., <em id="S4.p1.1.9" class="ltx_emph ltx_font_italic">are</em>, <em id="S4.p1.1.10" class="ltx_emph ltx_font_italic">does</em>, <em id="S4.p1.1.11" class="ltx_emph ltx_font_italic">was</em>, and <em id="S4.p1.1.12" class="ltx_emph ltx_font_italic">is</em>). We prompted GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> to label all questions with their question type, and manually verified the labels. <em id="S4.p1.1.13" class="ltx_emph ltx_font_italic">When</em> questions only occurred once, so we omitted them in the analysis.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.4" class="ltx_p">Figure <a href="#S3.F2" title="Figure 2 ‚Ä£ 3 Dataset Construction ‚Ä£ Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the question type distribution across contexts for all questions collected from the description-only study (top) and the image+description study (bottom).
For both study conditions, <em id="S4.p2.4.1" class="ltx_emph ltx_font_italic">what</em> and binary response questions (<em id="S4.p2.4.2" class="ltx_emph ltx_font_italic">is</em>) occur most frequently. We observe clear effects of context, especially in the description-only study. For instance, questions asked for images that appear in the shopping context are more likely to be <em id="S4.p2.4.3" class="ltx_emph ltx_font_italic">what</em> questions (<math id="S4.p2.1.m1.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S4.p2.1.m1.1a"><mrow id="S4.p2.1.m1.1.1" xref="S4.p2.1.m1.1.1.cmml"><mi id="S4.p2.1.m1.1.1.2" xref="S4.p2.1.m1.1.1.2.cmml">p</mi><mo id="S4.p2.1.m1.1.1.1" xref="S4.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="S4.p2.1.m1.1.1.3" xref="S4.p2.1.m1.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.1.m1.1b"><apply id="S4.p2.1.m1.1.1.cmml" xref="S4.p2.1.m1.1.1"><lt id="S4.p2.1.m1.1.1.1.cmml" xref="S4.p2.1.m1.1.1.1"></lt><ci id="S4.p2.1.m1.1.1.2.cmml" xref="S4.p2.1.m1.1.1.2">ùëù</ci><cn type="float" id="S4.p2.1.m1.1.1.3.cmml" xref="S4.p2.1.m1.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.1.m1.1c">p&lt;0.001</annotation></semantics></math>) and less likely to be <em id="S4.p2.4.4" class="ltx_emph ltx_font_italic">where</em> questions (<math id="S4.p2.2.m2.1" class="ltx_Math" alttext="p&lt;0.01" display="inline"><semantics id="S4.p2.2.m2.1a"><mrow id="S4.p2.2.m2.1.1" xref="S4.p2.2.m2.1.1.cmml"><mi id="S4.p2.2.m2.1.1.2" xref="S4.p2.2.m2.1.1.2.cmml">p</mi><mo id="S4.p2.2.m2.1.1.1" xref="S4.p2.2.m2.1.1.1.cmml">&lt;</mo><mn id="S4.p2.2.m2.1.1.3" xref="S4.p2.2.m2.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.2.m2.1b"><apply id="S4.p2.2.m2.1.1.cmml" xref="S4.p2.2.m2.1.1"><lt id="S4.p2.2.m2.1.1.1.cmml" xref="S4.p2.2.m2.1.1.1"></lt><ci id="S4.p2.2.m2.1.1.2.cmml" xref="S4.p2.2.m2.1.1.2">ùëù</ci><cn type="float" id="S4.p2.2.m2.1.1.3.cmml" xref="S4.p2.2.m2.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.2.m2.1c">p&lt;0.01</annotation></semantics></math>) compared to other contexts.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>All statistical tests are generalized linear model analyses.</span></span></span> Questions for images appearing on social media, however, are the least likely to be <em id="S4.p2.4.5" class="ltx_emph ltx_font_italic">what</em> questions (<math id="S4.p2.3.m3.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S4.p2.3.m3.1a"><mrow id="S4.p2.3.m3.1.1" xref="S4.p2.3.m3.1.1.cmml"><mi id="S4.p2.3.m3.1.1.2" xref="S4.p2.3.m3.1.1.2.cmml">p</mi><mo id="S4.p2.3.m3.1.1.1" xref="S4.p2.3.m3.1.1.1.cmml">&lt;</mo><mn id="S4.p2.3.m3.1.1.3" xref="S4.p2.3.m3.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.3.m3.1b"><apply id="S4.p2.3.m3.1.1.cmml" xref="S4.p2.3.m3.1.1"><lt id="S4.p2.3.m3.1.1.1.cmml" xref="S4.p2.3.m3.1.1.1"></lt><ci id="S4.p2.3.m3.1.1.2.cmml" xref="S4.p2.3.m3.1.1.2">ùëù</ci><cn type="float" id="S4.p2.3.m3.1.1.3.cmml" xref="S4.p2.3.m3.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.3.m3.1c">p&lt;0.001</annotation></semantics></math>) but are the most likely to be <em id="S4.p2.4.6" class="ltx_emph ltx_font_italic">who</em> questions (<math id="S4.p2.4.m4.1" class="ltx_Math" alttext="p&lt;0.001" display="inline"><semantics id="S4.p2.4.m4.1a"><mrow id="S4.p2.4.m4.1.1" xref="S4.p2.4.m4.1.1.cmml"><mi id="S4.p2.4.m4.1.1.2" xref="S4.p2.4.m4.1.1.2.cmml">p</mi><mo id="S4.p2.4.m4.1.1.1" xref="S4.p2.4.m4.1.1.1.cmml">&lt;</mo><mn id="S4.p2.4.m4.1.1.3" xref="S4.p2.4.m4.1.1.3.cmml">0.001</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.p2.4.m4.1b"><apply id="S4.p2.4.m4.1.1.cmml" xref="S4.p2.4.m4.1.1"><lt id="S4.p2.4.m4.1.1.1.cmml" xref="S4.p2.4.m4.1.1.1"></lt><ci id="S4.p2.4.m4.1.1.2.cmml" xref="S4.p2.4.m4.1.1.2">ùëù</ci><cn type="float" id="S4.p2.4.m4.1.1.3.cmml" xref="S4.p2.4.m4.1.1.3">0.001</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.p2.4.m4.1c">p&lt;0.001</annotation></semantics></math>).
These results can help provide insight into where a model must be especially robust, and they highlight the importance of systems that can adjust to contextually changing user needs.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p">While some context effects are reflected in the image+description study (e.g., in the <em id="S4.p3.1.1" class="ltx_emph ltx_font_italic">where</em> and <em id="S4.p3.1.2" class="ltx_emph ltx_font_italic">how</em> distributions), they are overall less pronounced than in the description-only study.
We hypothesize this is because the questioners have less information in the description-only study to inform their questions. These results suggest that context-sensitivity in VQA needs to play an especially important role when the image itself isn‚Äôt visible to the end user, and is therefore particularly relevant for the image accessibility setting. Image+description VQA study setups might provide misleading results, since the presence of the image dampens the context effect.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Prior work suggests that when questioners don‚Äôt see the image, they ask longer and more open-ended questions, which necessitates more detailed answers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>.
The average response length for our answers, across question types and study conditions, ended up being 11.03 words. This shows that the answers in our dataset are meaningful and not restricted to binary responses. This is an uncommonly long average response length for VQA datasets‚Äìfor a point of comparison, Visual Dialog <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite> reported an average answer length of 2.9 words, which was still higher than VQA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> (1.1 words), and Visual 7W <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib19" title="" class="ltx_ref">19</a>]</cite> (2.0 words). We hypothesize the main contributors to answer length are expressions of uncertainty or additional details.</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">We present the Context-VQA dataset and use it to show how context changes the distribution of question types for VQA models. Unlike previous VQA datasets, Context-VQA is context-sensitive and reflects the distribution of question types that people will ask in different contexts. We also show initial evidence that the questions vary across contexts, especially when the image can‚Äôt be seen.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra,
C.¬†Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">VQA: Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE International Conference on Computer Vision</span><span id="bib.bib1.4.2" class="ltx_text" style="font-size:90%;">,
abs/1505.00468, 2015.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Yang¬†Trista Cao, Kyle Seelman, Kyungjun Lee, and Hal¬†III Daum√©.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">What‚Äôs Different between Visual Question Answering for Machine
‚ÄúUnderstanding‚Äù Versus for Accessibility?
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib2.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The 2nd Conference of the Asia-Pacific Chapter of the
Association for Computational Linguistics and the 12th International Joint
Conference on Natural Language Processing</span><span id="bib.bib2.5.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos√©
M.¬†F. Moura, Devi Parikh, and Dhruv Batra.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Visual Dialog, 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Pierre Dognin, Igor Melnyk, Youssef Mroueh, Inkit Padhi, Mattia Rigotti, Jarret
Ross, Yair Schiff, Richard¬†A Young, and Brian Belgodere.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">Image Captioning as an Assistive Technology: Lessons Learned from
VizWiz 2020 Challenge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Artificial Intelligence Research</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 73:437‚Äì459, 2022.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Making the V in VQA Matter: Elevating the Role of Image
Understanding in Visual Question Answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conference on Computer Vision and Pattern Recognition</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Darren Guinness, Edward Cutrell, and Meredith¬†Ringel Morris.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">Caption Crawler: Enabling Reusable Alternative Text Descriptions
using Reverse Image Search.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib6.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2018 CHI Conference on Human Factors in
Computing Systems</span><span id="bib.bib6.5.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Danna Gurari, Qing Li, Abigale¬†J Stangl, Anhong Guo, Chi Lin, Kristen Grauman,
Jiebo Luo, and Jeffrey¬†P Bigham.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">VizWiz Grand Challenge: Answering Visual Questions from Blind
People.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib7.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib7.5.3" class="ltx_text" style="font-size:90%;">, pages 3608‚Äì3617, 2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Drew¬†A Hudson and Christopher¬†D Manning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">GQA: A new dataset for real-world visual reasoning and compositional
question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib8.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib8.5.3" class="ltx_text" style="font-size:90%;">, pages 6700‚Äì6709, 2019.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Justin Johnson, Bharath Hariharan, Laurens van¬†der Maaten, Li Fei-Fei,
C¬†Lawrence Zitnick, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">CLEVR: A Diagnostic Dataset for Compositional Language and
Elementary Visual Reasoning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib9.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">2017 IEEE Conference on Computer Vision and Pattern
Recognition (CVPR)</span><span id="bib.bib9.5.3" class="ltx_text" style="font-size:90%;">, pages 1988‚Äì1997. IEEE, 2017.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Elisa Kreiss, Cynthia Bennett, Shayan Hooshmand, Eric Zelikman, Meredith¬†Ringel
Morris, and Christopher Potts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Context matters for image descriptions for accessibility: Challenges
for referenceless evaluation metrics.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib10.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</span><span id="bib.bib10.5.3" class="ltx_text" style="font-size:90%;">, pages 4685‚Äì4697, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Elisa Kreiss, Fei Fang, Noah Goodman, and Christopher Potts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Concadia: Towards image-based text generation with a purpose.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib11.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 2022 Conference on Empirical Methods in
Natural Language Processing</span><span id="bib.bib11.5.3" class="ltx_text" style="font-size:90%;">, pages 4667‚Äì4684, 2022.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Blip-2: Bootstrapping language-image pre-training with frozen image
encoders and large language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.12597</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">OK-VQA: A Visual Question Answering Benchmark Requiring External
Knowledge.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib13.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib13.5.3" class="ltx_text" style="font-size:90%;">, pages 3195‚Äì3204, 2019.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Annika Muehlbradt and Shaun¬†K Kane.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">What‚Äôs in an ALT Tag? Exploring Caption Content Priorities through
Collaborative Captioning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">ACM Transactions on Accessible Computing (TACCESS)</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">,
15(1):1‚Äì32, 2022.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
OpenAI.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">GPT-4 Technical Report, 2023.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Helen Petrie, Chandra Harrison, and Sundeep Dev.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Describing images on the web: A survey of current practice and
prospects for the future.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib16.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of Human Computer Interaction International</span><span id="bib.bib16.5.3" class="ltx_text" style="font-size:90%;">,
2005.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Abigale Stangl, Meredith¬†Ringel Morris, and Danna Gurari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">Person, Shoes, Tree. Is the Person Naked? What People with Vision
Impairments Want in Image Descriptions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib17.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Conferences on Human-Computer Interaction</span><span id="bib.bib17.5.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
Abigale Stangl, Nitin Verma, Kenneth Fleischmann, Meredith¬†Ringel Morris, and
Danna Gurari.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Going Beyond One-Size-Fits-All Image Descriptions to Satisfy the
Information Wants of People Who are Blind or Have Low Vision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib18.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">23rd International ACM SIGACCESS Conference on Computers and
Accessibility (ASSETS ‚Äô21)</span><span id="bib.bib18.5.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Yuke Zhu, Oliver Groth, Michael Bernstein, and Li Fei-Fei.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Visual7w: Grounded question answering in images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and
pattern recognition</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 4995‚Äì5004, 2016.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2307.15744" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2307.15745" class="ar5iv-text-button ar5iv-severity-warning">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2307.15745">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2307.15745" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2307.15746" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Feb 28 16:00:30 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
