<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu</title>
<!--Generated on Sat Sep 14 20:07:51 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.09520v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S1" title="In Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S2" title="In Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Works</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S2.SS1" title="In 2 Related Works ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Visual Segment Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S2.SS2" title="In 2 Related Works ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Skin Lesion Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S3" title="In Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S3.SS1" title="In 3 Methods ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Preliminary and Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S3.SS2" title="In 3 Methods ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Visual Concepts Identification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S3.SS3" title="In 3 Methods ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Cross-Attentive Fusion Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S3.SS4" title="In 3 Methods ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Interpretable Skin Diagnosis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4" title="In Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.SS1" title="In 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.SS2" title="In 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Implementation Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.SS3" title="In 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Comparison Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.SS4" title="In 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Interpretable Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.SS5" title="In 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S5" title="In Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1">(wacv)                Package wacv Warning: Package ‘hyperref’ is not loaded, but highly recommended for camera-ready version</p>
</div>
<h1 class="ltx_title ltx_title_document">Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery
<br class="ltx_break"/>with SAM Empowerment<span class="ltx_note ltx_role_thanks" id="id2.2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span><math alttext="{\dagger}" class="ltx_Math" display="inline" id="id1.1.m1.1"><semantics id="id1.1.m1.1c"><mo id="id1.1.m1.1.1" xref="id1.1.m1.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="id1.1.m1.1d"><ci id="id1.1.m1.1.1.cmml" xref="id1.1.m1.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="id1.1.m1.1e">{\dagger}</annotation><annotation encoding="application/x-llamapun" id="id1.1.m1.1f">†</annotation></semantics></math> indicates equal contribution, <math alttext="{\ddagger}" class="ltx_Math" display="inline" id="id2.2.m2.1"><semantics id="id2.2.m2.1c"><mo id="id2.2.m2.1.1" xref="id2.2.m2.1.1.cmml">‡</mo><annotation-xml encoding="MathML-Content" id="id2.2.m2.1d"><ci id="id2.2.m2.1.1.cmml" xref="id2.2.m2.1.1">‡</ci></annotation-xml><annotation encoding="application/x-tex" id="id2.2.m2.1e">{\ddagger}</annotation><annotation encoding="application/x-llamapun" id="id2.2.m2.1f">‡</annotation></semantics></math> indicates the corresponding author zding1@tulane.edu</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xin Hu<sup class="ltx_sup" id="id10.8.id1"><span class="ltx_text ltx_font_italic" id="id10.8.id1.1">1,†</span></sup>, Janet Wang<sup class="ltx_sup" id="id11.9.id2"><span class="ltx_text ltx_font_italic" id="id11.9.id2.1">1,†</span></sup>, Jihun Hamm<sup class="ltx_sup" id="id12.10.id3">1</sup>, Rie R Yotsu<sup class="ltx_sup" id="id13.11.id4">2</sup>, Zhengming Ding<sup class="ltx_sup" id="id14.12.id5"><span class="ltx_text ltx_font_italic" id="id14.12.id5.1">1,‡</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id15.13.id6">1</sup>Department of Computer Science, Tulane University
<br class="ltx_break"/><sup class="ltx_sup" id="id16.14.id7">2</sup>Department of Tropical Medicine, Tulane University
<br class="ltx_break"/>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id17.id1">Current AI-assisted skin image diagnosis has achieved dermatologist-level performance in classifying skin cancer, driven by rapid advancements in deep learning architectures. However, unlike traditional vision tasks, skin images in general present unique challenges due to the limited availability of well-annotated datasets, complex variations in conditions, and the necessity for detailed interpretations to ensure patient safety. Previous segmentation methods have sought to reduce image noise and enhance diagnostic performance, but these techniques require fine-grained, pixel-level ground truth masks for training. In contrast, with the rise of foundation models, the Segment Anything Model (SAM) has been introduced to facilitate promptable segmentation, enabling the automation of the segmentation process with simple yet effective prompts. Efforts applying SAM predominantly focus on dermatoscopy images, which present more easily identifiable lesion boundaries than clinical photos taken with smartphones. This limitation constrains the practicality of these approaches to real-world applications. To overcome the challenges posed by noisy clinical photos acquired via non-standardized protocols and to improve diagnostic accessibility, we propose a novel Cross-Attentive Fusion framework for interpretable skin lesion diagnosis. Our method leverages SAM to generate visual concepts for skin diseases using prompts, integrating local visual concepts with global image features to enhance model performance. Extensive evaluation on two skin disease datasets demonstrates our proposed method’s effectiveness on lesion diagnosis and interpretability.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="799" id="S1.F1.g1" src="x1.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Generated masks on two skin samples with prompt “lesion” and “leg”. The first row shows a “Buruli ulcer” image, in which the lesion part is clear to be detected as the visual concept, while in the second row - “Mycetoma”, the lesion boundary is ambiguous to be recognized.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Current AI-assisted diagnostic systems demonstrate expert-level capability in classifying skin cancers, which are often identified visually <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib26" title="">26</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib4" title="">4</a>]</cite>. Given that early and accurate diagnosis is important in improving treatment outcomes, these systems can significantly contribute to teledermatology as diagnostic and decision-support tools <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib10" title="">10</a>]</cite>. By utilizing photos captured from portable devices like smartphones, these methods promote diagnostic accessibility in rural areas <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib6" title="">6</a>]</cite>. However, such systems are susceptible to under-diagnosis due to non-standardized acquisition environments and protocols. Therefore, the model’s ability to accurately recognize the region of interest (ROI) within noisy backgrounds is crucial for precise diagnosis.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While image segmentation techniques such as MaskRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib16" title="">16</a>]</cite>, DeepLab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib5" title="">5</a>]</cite>, and Panoptic Segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib22" title="">22</a>]</cite> can be employed to localize ROIs and enhance diagnostic accuracy, they rely on fine-grained bounding box or pixel-wise semantic annotations. To acquire such annotations from manual labeling is labor-intensive and impractical, especially when dealing with noisy images. The Segment Anything Model (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib23" title="">23</a>]</cite> can serve as a potential solution, capable of automating the generation of two-level masks: one for the region of the body and another for the lesion. These masks also enable the explainable investigation of different features in the diagnostic results.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Two works have explored SAM to locate skin lesions and automate the segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib18" title="">18</a>]</cite>. Both studies primarily focus on the well-curated HAM10000 Dataset, which includes 10,015 verified dermoscopy images and groundtruth segmentation masks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib32" title="">32</a>]</cite>. In this dataset, examples are abundant, lesion boundaries are clear, and background noise is minimized. In contrast, clinical photos collected in real-world applications present greater difficulty. The contrast between lesions and backgrounds in these photos is often less distinct due to factors such as (1) a wide variance in lesion presentation, including differences in shape, texture, color, and location, and (2) inconsistent image quality caused by varying lighting, distance, angles, environments, or image resolutions. These variations make it difficult for the model to accurately identify the ROI from the background, limiting the practicality of existing methods. Moreover, clinical photos of skin conditions are often limited in size. Therefore, the potential application of SAM in small datasets of clinical photos remains under-explored. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">1</span></a> shows results on example images using SAM.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this work, we propose an innovative framework to locate the region of interest and diagnose skin diseases in the clinical photos. Empowered by SAM, our framework generates two-level masks to enhance the accuracy of AI diagnoses and provide robust support for remote medical personnel, enhancing diagnostic reliability and transparency. To sum up, we highlight our contributions as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We leverage the capabilities of a foundational segmentation AI model to generate two-level masks over the body and lesions with prompts, which delineates the regions of interest and reveals potential visual concepts essential for skin diagnosis.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We introduce a novel Cross-Attentive Fusion framework that integrates knowledge from global image features and local visual concepts to enhance the model’s diagnostic accuracy. This framework also facilitates interpretable analysis of visual concepts crucial for accurate diagnosis.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We comprehensively evaluate our method on two distinct skin image datasets of challenging clinical photos. Through detailed interpretation and analysis, our method demonstrates robustness and effectiveness in real-world scenarios, elevating confidence in its diagnostic capabilities.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Works</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Visual Segment Generation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Image segmentation techniques such as MaskRCNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib16" title="">16</a>]</cite>, DeepLab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib5" title="">5</a>]</cite>, and Panoptic Segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib22" title="">22</a>]</cite> are effective for localizing regions of interest and enhancing model accuracy, but they typically require fine-grained bounding box or pixel-wise semantic annotations. With a growing focus on scalability, pre-trained foundational models have gained prominence in machine learning, serving as robust starting points for various downstream tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib30" title="">30</a>]</cite>. Responding to this trend, the Segment Anything Model (SAM) has emerged, pushing image segmentation into the realm of foundational models.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="337" id="S2.F2.g1" src="x2.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.6.3.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.4.2" style="font-size:90%;">Overall framework of our proposed model, where we use Grounding-DINO and SAM to extract visual concepts, bounding boxes, and segmentation masks. The local encoder <math alttext="\mathcal{F}_{l}(\cdot)" class="ltx_Math" display="inline" id="S2.F2.3.1.m1.1"><semantics id="S2.F2.3.1.m1.1b"><mrow id="S2.F2.3.1.m1.1.2" xref="S2.F2.3.1.m1.1.2.cmml"><msub id="S2.F2.3.1.m1.1.2.2" xref="S2.F2.3.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.F2.3.1.m1.1.2.2.2" xref="S2.F2.3.1.m1.1.2.2.2.cmml">ℱ</mi><mi id="S2.F2.3.1.m1.1.2.2.3" xref="S2.F2.3.1.m1.1.2.2.3.cmml">l</mi></msub><mo id="S2.F2.3.1.m1.1.2.1" xref="S2.F2.3.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.F2.3.1.m1.1.2.3.2" xref="S2.F2.3.1.m1.1.2.cmml"><mo id="S2.F2.3.1.m1.1.2.3.2.1" stretchy="false" xref="S2.F2.3.1.m1.1.2.cmml">(</mo><mo id="S2.F2.3.1.m1.1.1" lspace="0em" rspace="0em" xref="S2.F2.3.1.m1.1.1.cmml">⋅</mo><mo id="S2.F2.3.1.m1.1.2.3.2.2" stretchy="false" xref="S2.F2.3.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.3.1.m1.1c"><apply id="S2.F2.3.1.m1.1.2.cmml" xref="S2.F2.3.1.m1.1.2"><times id="S2.F2.3.1.m1.1.2.1.cmml" xref="S2.F2.3.1.m1.1.2.1"></times><apply id="S2.F2.3.1.m1.1.2.2.cmml" xref="S2.F2.3.1.m1.1.2.2"><csymbol cd="ambiguous" id="S2.F2.3.1.m1.1.2.2.1.cmml" xref="S2.F2.3.1.m1.1.2.2">subscript</csymbol><ci id="S2.F2.3.1.m1.1.2.2.2.cmml" xref="S2.F2.3.1.m1.1.2.2.2">ℱ</ci><ci id="S2.F2.3.1.m1.1.2.2.3.cmml" xref="S2.F2.3.1.m1.1.2.2.3">𝑙</ci></apply><ci id="S2.F2.3.1.m1.1.1.cmml" xref="S2.F2.3.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.3.1.m1.1d">\mathcal{F}_{l}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.F2.3.1.m1.1e">caligraphic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> converts visual concepts to local tokens and sets them as “query” prompts to trigger the salient area of the encoded global image with cross attentive module. The classifier <math alttext="\mathcal{F}_{c}(\cdot)" class="ltx_Math" display="inline" id="S2.F2.4.2.m2.1"><semantics id="S2.F2.4.2.m2.1b"><mrow id="S2.F2.4.2.m2.1.2" xref="S2.F2.4.2.m2.1.2.cmml"><msub id="S2.F2.4.2.m2.1.2.2" xref="S2.F2.4.2.m2.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.F2.4.2.m2.1.2.2.2" xref="S2.F2.4.2.m2.1.2.2.2.cmml">ℱ</mi><mi id="S2.F2.4.2.m2.1.2.2.3" xref="S2.F2.4.2.m2.1.2.2.3.cmml">c</mi></msub><mo id="S2.F2.4.2.m2.1.2.1" xref="S2.F2.4.2.m2.1.2.1.cmml">⁢</mo><mrow id="S2.F2.4.2.m2.1.2.3.2" xref="S2.F2.4.2.m2.1.2.cmml"><mo id="S2.F2.4.2.m2.1.2.3.2.1" stretchy="false" xref="S2.F2.4.2.m2.1.2.cmml">(</mo><mo id="S2.F2.4.2.m2.1.1" lspace="0em" rspace="0em" xref="S2.F2.4.2.m2.1.1.cmml">⋅</mo><mo id="S2.F2.4.2.m2.1.2.3.2.2" stretchy="false" xref="S2.F2.4.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.F2.4.2.m2.1c"><apply id="S2.F2.4.2.m2.1.2.cmml" xref="S2.F2.4.2.m2.1.2"><times id="S2.F2.4.2.m2.1.2.1.cmml" xref="S2.F2.4.2.m2.1.2.1"></times><apply id="S2.F2.4.2.m2.1.2.2.cmml" xref="S2.F2.4.2.m2.1.2.2"><csymbol cd="ambiguous" id="S2.F2.4.2.m2.1.2.2.1.cmml" xref="S2.F2.4.2.m2.1.2.2">subscript</csymbol><ci id="S2.F2.4.2.m2.1.2.2.2.cmml" xref="S2.F2.4.2.m2.1.2.2.2">ℱ</ci><ci id="S2.F2.4.2.m2.1.2.2.3.cmml" xref="S2.F2.4.2.m2.1.2.2.3">𝑐</ci></apply><ci id="S2.F2.4.2.m2.1.1.cmml" xref="S2.F2.4.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.4.2.m2.1d">\mathcal{F}_{c}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S2.F2.4.2.m2.1e">caligraphic_F start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> transfers the latent features to CAM for classification and interpretation for the decision-making process.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">SAM is a promptable segmentation model pre-trained on SA-1B, a vast dataset containing over 1 billion masks derived from 11 million licensed and privacy-preserving images. This extensive training ensures strong generalization across diverse data distributions. SAM supports flexible prompts such as single points, sets of points, bounding boxes, or text. Its architecture is elegantly simple yet effective: a powerful image encoder computes an image embedding, a prompt encoder embeds prompts, and these inputs are fused in a lightweight mask decoder to predict segmentation masks. Specifically, SAM’s image encoder utilizes a Masked Autoencoder (MAE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib15" title="">15</a>]</cite> pre-trained on a Vision Transformer (ViT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib8" title="">8</a>]</cite>. This adaptation allows it to handle high-resolution inputs while capturing fine-grained details and long-range dependencies. The prompt encoder converts prompts into fixed-length embeddings that capture semantic meanings. These embeddings are combined with image encoder outputs to generate a set of feature maps used by the mask decoder to produce segmentation masks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Skin Lesion Analysis</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Skin lesion images are typically captured in two forms: dermatoscopy images and clinical photos. Dermoscopy images are close-up views of pigmented skin lesions captured by using professional microscopy, a dermatoscope. These images focus on the lesion, typically excluding any background, resulting in a uniform and consistent visual presentation. Clinical photos, on the other hand, are often taken with portable devices like smartphones and contain more noisy backgrounds. Extensive research has explored the use of deep Convolutional Neural Networks (CNNs) and Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib12" title="">12</a>]</cite> for skin lesion segmentation to standardize lesion variations and enhance diagnostic accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib21" title="">21</a>]</cite>. CNN-based methods for skin lesion segmentation typically employ supervised learning with large labeled datasets to extract spatial features and semantic maps from images. In contrast, GAN-based approaches address data scarcity through unsupervised learning but still require fine-grained groundtruth masks, often unavailable in real-world applications.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">With the emergence of large foundation models, there has been an effort to utilize the Segment Anything Model (SAM) for dermatoscopy images, using simple prompts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib18" title="">18</a>]</cite>. Specifically, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib19" title="">19</a>]</cite> fine-tuned SAM on the HAM10000 Dataset with fine-grained groundtruth masks, achieving state-of-the-art segmentation performance with the guidance of prompts. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib18" title="">18</a>]</cite> utilized SAM to segment cancerous areas in HAM10000 images before feeding the segmented images to a pre-trained Vision Transformer (ViT) model for classification. Their results demonstrated ViT’s superiority over traditional architectures. Despite these advancements, adapting SAM to clinical data captured by portable devices poses challenges due to limited dataset sizes for fine-tuning and the complexity of lesion manifestations for off-the-shelf SAM applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib38" title="">38</a>]</cite>. Therefore, the potential of SAM in low-data scenarios and with clinical photos remains to be fully explored.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Preliminary and Motivation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.8">Our objective is to develop an accurate and interpretable diagnostic model for complex image data. Given a batch of skin disease images, <math alttext="\mathbf{X}=\left\{\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3},\ldots,\mathbf{%
x}_{i},\ldots,x_{N}\right\}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.7"><semantics id="S3.SS1.p1.1.m1.7a"><mrow id="S3.SS1.p1.1.m1.7.7" xref="S3.SS1.p1.1.m1.7.7.cmml"><mi id="S3.SS1.p1.1.m1.7.7.7" xref="S3.SS1.p1.1.m1.7.7.7.cmml">𝐗</mi><mo id="S3.SS1.p1.1.m1.7.7.6" xref="S3.SS1.p1.1.m1.7.7.6.cmml">=</mo><mrow id="S3.SS1.p1.1.m1.7.7.5.5" xref="S3.SS1.p1.1.m1.7.7.5.6.cmml"><mo id="S3.SS1.p1.1.m1.7.7.5.5.6" xref="S3.SS1.p1.1.m1.7.7.5.6.cmml">{</mo><msub id="S3.SS1.p1.1.m1.3.3.1.1.1" xref="S3.SS1.p1.1.m1.3.3.1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.3.3.1.1.1.2" xref="S3.SS1.p1.1.m1.3.3.1.1.1.2.cmml">𝐱</mi><mn id="S3.SS1.p1.1.m1.3.3.1.1.1.3" xref="S3.SS1.p1.1.m1.3.3.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.1.m1.7.7.5.5.7" xref="S3.SS1.p1.1.m1.7.7.5.6.cmml">,</mo><msub id="S3.SS1.p1.1.m1.4.4.2.2.2" xref="S3.SS1.p1.1.m1.4.4.2.2.2.cmml"><mi id="S3.SS1.p1.1.m1.4.4.2.2.2.2" xref="S3.SS1.p1.1.m1.4.4.2.2.2.2.cmml">𝐱</mi><mn id="S3.SS1.p1.1.m1.4.4.2.2.2.3" xref="S3.SS1.p1.1.m1.4.4.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS1.p1.1.m1.7.7.5.5.8" xref="S3.SS1.p1.1.m1.7.7.5.6.cmml">,</mo><msub id="S3.SS1.p1.1.m1.5.5.3.3.3" xref="S3.SS1.p1.1.m1.5.5.3.3.3.cmml"><mi id="S3.SS1.p1.1.m1.5.5.3.3.3.2" xref="S3.SS1.p1.1.m1.5.5.3.3.3.2.cmml">𝐱</mi><mn id="S3.SS1.p1.1.m1.5.5.3.3.3.3" xref="S3.SS1.p1.1.m1.5.5.3.3.3.3.cmml">3</mn></msub><mo id="S3.SS1.p1.1.m1.7.7.5.5.9" xref="S3.SS1.p1.1.m1.7.7.5.6.cmml">,</mo><mi id="S3.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p1.1.m1.1.1.cmml">…</mi><mo id="S3.SS1.p1.1.m1.7.7.5.5.10" xref="S3.SS1.p1.1.m1.7.7.5.6.cmml">,</mo><msub id="S3.SS1.p1.1.m1.6.6.4.4.4" xref="S3.SS1.p1.1.m1.6.6.4.4.4.cmml"><mi id="S3.SS1.p1.1.m1.6.6.4.4.4.2" xref="S3.SS1.p1.1.m1.6.6.4.4.4.2.cmml">𝐱</mi><mi id="S3.SS1.p1.1.m1.6.6.4.4.4.3" xref="S3.SS1.p1.1.m1.6.6.4.4.4.3.cmml">i</mi></msub><mo id="S3.SS1.p1.1.m1.7.7.5.5.11" xref="S3.SS1.p1.1.m1.7.7.5.6.cmml">,</mo><mi id="S3.SS1.p1.1.m1.2.2" mathvariant="normal" xref="S3.SS1.p1.1.m1.2.2.cmml">…</mi><mo id="S3.SS1.p1.1.m1.7.7.5.5.12" xref="S3.SS1.p1.1.m1.7.7.5.6.cmml">,</mo><msub id="S3.SS1.p1.1.m1.7.7.5.5.5" xref="S3.SS1.p1.1.m1.7.7.5.5.5.cmml"><mi id="S3.SS1.p1.1.m1.7.7.5.5.5.2" xref="S3.SS1.p1.1.m1.7.7.5.5.5.2.cmml">x</mi><mi id="S3.SS1.p1.1.m1.7.7.5.5.5.3" xref="S3.SS1.p1.1.m1.7.7.5.5.5.3.cmml">N</mi></msub><mo id="S3.SS1.p1.1.m1.7.7.5.5.13" xref="S3.SS1.p1.1.m1.7.7.5.6.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.7b"><apply id="S3.SS1.p1.1.m1.7.7.cmml" xref="S3.SS1.p1.1.m1.7.7"><eq id="S3.SS1.p1.1.m1.7.7.6.cmml" xref="S3.SS1.p1.1.m1.7.7.6"></eq><ci id="S3.SS1.p1.1.m1.7.7.7.cmml" xref="S3.SS1.p1.1.m1.7.7.7">𝐗</ci><set id="S3.SS1.p1.1.m1.7.7.5.6.cmml" xref="S3.SS1.p1.1.m1.7.7.5.5"><apply id="S3.SS1.p1.1.m1.3.3.1.1.1.cmml" xref="S3.SS1.p1.1.m1.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.3.3.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.3.3.1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.3.3.1.1.1.2">𝐱</ci><cn id="S3.SS1.p1.1.m1.3.3.1.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.3.3.1.1.1.3">1</cn></apply><apply id="S3.SS1.p1.1.m1.4.4.2.2.2.cmml" xref="S3.SS1.p1.1.m1.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.4.4.2.2.2.1.cmml" xref="S3.SS1.p1.1.m1.4.4.2.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.4.4.2.2.2.2.cmml" xref="S3.SS1.p1.1.m1.4.4.2.2.2.2">𝐱</ci><cn id="S3.SS1.p1.1.m1.4.4.2.2.2.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.4.4.2.2.2.3">2</cn></apply><apply id="S3.SS1.p1.1.m1.5.5.3.3.3.cmml" xref="S3.SS1.p1.1.m1.5.5.3.3.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.5.5.3.3.3.1.cmml" xref="S3.SS1.p1.1.m1.5.5.3.3.3">subscript</csymbol><ci id="S3.SS1.p1.1.m1.5.5.3.3.3.2.cmml" xref="S3.SS1.p1.1.m1.5.5.3.3.3.2">𝐱</ci><cn id="S3.SS1.p1.1.m1.5.5.3.3.3.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.5.5.3.3.3.3">3</cn></apply><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">…</ci><apply id="S3.SS1.p1.1.m1.6.6.4.4.4.cmml" xref="S3.SS1.p1.1.m1.6.6.4.4.4"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.6.6.4.4.4.1.cmml" xref="S3.SS1.p1.1.m1.6.6.4.4.4">subscript</csymbol><ci id="S3.SS1.p1.1.m1.6.6.4.4.4.2.cmml" xref="S3.SS1.p1.1.m1.6.6.4.4.4.2">𝐱</ci><ci id="S3.SS1.p1.1.m1.6.6.4.4.4.3.cmml" xref="S3.SS1.p1.1.m1.6.6.4.4.4.3">𝑖</ci></apply><ci id="S3.SS1.p1.1.m1.2.2.cmml" xref="S3.SS1.p1.1.m1.2.2">…</ci><apply id="S3.SS1.p1.1.m1.7.7.5.5.5.cmml" xref="S3.SS1.p1.1.m1.7.7.5.5.5"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.7.7.5.5.5.1.cmml" xref="S3.SS1.p1.1.m1.7.7.5.5.5">subscript</csymbol><ci id="S3.SS1.p1.1.m1.7.7.5.5.5.2.cmml" xref="S3.SS1.p1.1.m1.7.7.5.5.5.2">𝑥</ci><ci id="S3.SS1.p1.1.m1.7.7.5.5.5.3.cmml" xref="S3.SS1.p1.1.m1.7.7.5.5.5.3">𝑁</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.7c">\mathbf{X}=\left\{\mathbf{x}_{1},\mathbf{x}_{2},\mathbf{x}_{3},\ldots,\mathbf{%
x}_{i},\ldots,x_{N}\right\}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.7d">bold_X = { bold_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , bold_x start_POSTSUBSCRIPT 3 end_POSTSUBSCRIPT , … , bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT }</annotation></semantics></math>, where <math alttext="N" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><mi id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><ci id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_N</annotation></semantics></math> is the total number of images, the model aims to predict the most likely skin disease for each image, represented as <math alttext="\{\mathbf{y}_{i}\}_{i=1}^{N}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msubsup id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mrow id="S3.SS1.p1.3.m3.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml"><mo id="S3.SS1.p1.3.m3.1.1.1.1.1.2" stretchy="false" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml">{</mo><msub id="S3.SS1.p1.3.m3.1.1.1.1.1.1" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.1.2" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml">𝐲</mi><mi id="S3.SS1.p1.3.m3.1.1.1.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS1.p1.3.m3.1.1.1.1.1.3" stretchy="false" xref="S3.SS1.p1.3.m3.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS1.p1.3.m3.1.1.1.3" xref="S3.SS1.p1.3.m3.1.1.1.3.cmml"><mi id="S3.SS1.p1.3.m3.1.1.1.3.2" xref="S3.SS1.p1.3.m3.1.1.1.3.2.cmml">i</mi><mo id="S3.SS1.p1.3.m3.1.1.1.3.1" xref="S3.SS1.p1.3.m3.1.1.1.3.1.cmml">=</mo><mn id="S3.SS1.p1.3.m3.1.1.1.3.3" xref="S3.SS1.p1.3.m3.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">N</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1">superscript</csymbol><apply id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1">subscript</csymbol><set id="S3.SS1.p1.3.m3.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1"><apply id="S3.SS1.p1.3.m3.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.1.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.2">𝐲</ci><ci id="S3.SS1.p1.3.m3.1.1.1.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.SS1.p1.3.m3.1.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.1.3"><eq id="S3.SS1.p1.3.m3.1.1.1.3.1.cmml" xref="S3.SS1.p1.3.m3.1.1.1.3.1"></eq><ci id="S3.SS1.p1.3.m3.1.1.1.3.2.cmml" xref="S3.SS1.p1.3.m3.1.1.1.3.2">𝑖</ci><cn id="S3.SS1.p1.3.m3.1.1.1.3.3.cmml" type="integer" xref="S3.SS1.p1.3.m3.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">\{\mathbf{y}_{i}\}_{i=1}^{N}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">{ bold_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N end_POSTSUPERSCRIPT</annotation></semantics></math>. Here, <math alttext="\mathbf{y}_{i}\in\mathbb{R}^{C}" class="ltx_Math" display="inline" id="S3.SS1.p1.4.m4.1"><semantics id="S3.SS1.p1.4.m4.1a"><mrow id="S3.SS1.p1.4.m4.1.1" xref="S3.SS1.p1.4.m4.1.1.cmml"><msub id="S3.SS1.p1.4.m4.1.1.2" xref="S3.SS1.p1.4.m4.1.1.2.cmml"><mi id="S3.SS1.p1.4.m4.1.1.2.2" xref="S3.SS1.p1.4.m4.1.1.2.2.cmml">𝐲</mi><mi id="S3.SS1.p1.4.m4.1.1.2.3" xref="S3.SS1.p1.4.m4.1.1.2.3.cmml">i</mi></msub><mo id="S3.SS1.p1.4.m4.1.1.1" xref="S3.SS1.p1.4.m4.1.1.1.cmml">∈</mo><msup id="S3.SS1.p1.4.m4.1.1.3" xref="S3.SS1.p1.4.m4.1.1.3.cmml"><mi id="S3.SS1.p1.4.m4.1.1.3.2" xref="S3.SS1.p1.4.m4.1.1.3.2.cmml">ℝ</mi><mi id="S3.SS1.p1.4.m4.1.1.3.3" xref="S3.SS1.p1.4.m4.1.1.3.3.cmml">C</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.4.m4.1b"><apply id="S3.SS1.p1.4.m4.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1"><in id="S3.SS1.p1.4.m4.1.1.1.cmml" xref="S3.SS1.p1.4.m4.1.1.1"></in><apply id="S3.SS1.p1.4.m4.1.1.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.2.1.cmml" xref="S3.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.2.2.cmml" xref="S3.SS1.p1.4.m4.1.1.2.2">𝐲</ci><ci id="S3.SS1.p1.4.m4.1.1.2.3.cmml" xref="S3.SS1.p1.4.m4.1.1.2.3">𝑖</ci></apply><apply id="S3.SS1.p1.4.m4.1.1.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.4.m4.1.1.3.1.cmml" xref="S3.SS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="S3.SS1.p1.4.m4.1.1.3.2.cmml" xref="S3.SS1.p1.4.m4.1.1.3.2">ℝ</ci><ci id="S3.SS1.p1.4.m4.1.1.3.3.cmml" xref="S3.SS1.p1.4.m4.1.1.3.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.4.m4.1c">\mathbf{y}_{i}\in\mathbb{R}^{C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.4.m4.1d">bold_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>, with <math alttext="C" class="ltx_Math" display="inline" id="S3.SS1.p1.5.m5.1"><semantics id="S3.SS1.p1.5.m5.1a"><mi id="S3.SS1.p1.5.m5.1.1" xref="S3.SS1.p1.5.m5.1.1.cmml">C</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.5.m5.1b"><ci id="S3.SS1.p1.5.m5.1.1.cmml" xref="S3.SS1.p1.5.m5.1.1">𝐶</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.5.m5.1c">C</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.5.m5.1d">italic_C</annotation></semantics></math> representing the total number of condition categories, and <math alttext="\mathbf{y}_{i,c}" class="ltx_Math" display="inline" id="S3.SS1.p1.6.m6.2"><semantics id="S3.SS1.p1.6.m6.2a"><msub id="S3.SS1.p1.6.m6.2.3" xref="S3.SS1.p1.6.m6.2.3.cmml"><mi id="S3.SS1.p1.6.m6.2.3.2" xref="S3.SS1.p1.6.m6.2.3.2.cmml">𝐲</mi><mrow id="S3.SS1.p1.6.m6.2.2.2.4" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml"><mi id="S3.SS1.p1.6.m6.1.1.1.1" xref="S3.SS1.p1.6.m6.1.1.1.1.cmml">i</mi><mo id="S3.SS1.p1.6.m6.2.2.2.4.1" xref="S3.SS1.p1.6.m6.2.2.2.3.cmml">,</mo><mi id="S3.SS1.p1.6.m6.2.2.2.2" xref="S3.SS1.p1.6.m6.2.2.2.2.cmml">c</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.6.m6.2b"><apply id="S3.SS1.p1.6.m6.2.3.cmml" xref="S3.SS1.p1.6.m6.2.3"><csymbol cd="ambiguous" id="S3.SS1.p1.6.m6.2.3.1.cmml" xref="S3.SS1.p1.6.m6.2.3">subscript</csymbol><ci id="S3.SS1.p1.6.m6.2.3.2.cmml" xref="S3.SS1.p1.6.m6.2.3.2">𝐲</ci><list id="S3.SS1.p1.6.m6.2.2.2.3.cmml" xref="S3.SS1.p1.6.m6.2.2.2.4"><ci id="S3.SS1.p1.6.m6.1.1.1.1.cmml" xref="S3.SS1.p1.6.m6.1.1.1.1">𝑖</ci><ci id="S3.SS1.p1.6.m6.2.2.2.2.cmml" xref="S3.SS1.p1.6.m6.2.2.2.2">𝑐</ci></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.6.m6.2c">\mathbf{y}_{i,c}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.6.m6.2d">bold_y start_POSTSUBSCRIPT italic_i , italic_c end_POSTSUBSCRIPT</annotation></semantics></math> the presence of skin disease category <math alttext="c" class="ltx_Math" display="inline" id="S3.SS1.p1.7.m7.1"><semantics id="S3.SS1.p1.7.m7.1a"><mi id="S3.SS1.p1.7.m7.1.1" xref="S3.SS1.p1.7.m7.1.1.cmml">c</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.7.m7.1b"><ci id="S3.SS1.p1.7.m7.1.1.cmml" xref="S3.SS1.p1.7.m7.1.1">𝑐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.7.m7.1c">c</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.7.m7.1d">italic_c</annotation></semantics></math> in <math alttext="\mathbf{x}_{i}" class="ltx_Math" display="inline" id="S3.SS1.p1.8.m8.1"><semantics id="S3.SS1.p1.8.m8.1a"><msub id="S3.SS1.p1.8.m8.1.1" xref="S3.SS1.p1.8.m8.1.1.cmml"><mi id="S3.SS1.p1.8.m8.1.1.2" xref="S3.SS1.p1.8.m8.1.1.2.cmml">𝐱</mi><mi id="S3.SS1.p1.8.m8.1.1.3" xref="S3.SS1.p1.8.m8.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.8.m8.1b"><apply id="S3.SS1.p1.8.m8.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.8.m8.1.1.1.cmml" xref="S3.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS1.p1.8.m8.1.1.2.cmml" xref="S3.SS1.p1.8.m8.1.1.2">𝐱</ci><ci id="S3.SS1.p1.8.m8.1.1.3.cmml" xref="S3.SS1.p1.8.m8.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.8.m8.1c">\mathbf{x}_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.8.m8.1d">bold_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Considering the skin images are usually captured from smartphones and tablets in the field, they lack standardization and often contain confounding background noise. The ability to easily discern the region of interest (ROI) or lesion in a poor-quality or noisy image is crucial to accurate diagnosis. In this study, we will explore the Segment Anything Model (SAM) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib23" title="">23</a>]</cite> to detect visual concepts with specific prompts, then develop a cross-attentive fusion model by leveraging the global image information and local visual cues to improve the skin disease diagnosis performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Visual Concepts Identification</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Due to the often complicated background noise, end-to-end deep learning is not sufficient to fully extract informative features from raw images with disease labels. Though SAM can capture local visual concepts of lesions, the original SAM needs bounding boxes or coarse masks as input and might produce features containing background noise. Thus, we modify variants of SAM in public repository <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib2" title="">2</a>]</cite> which utilizes Grounding DINO<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib25" title="">25</a>]</cite>, to obtain visual concepts and provide related bounding boxes.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S2.F2" title="Figure 2 ‣ 2.1 Visual Segment Generation ‣ 2 Related Works ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">2</span></a>, we use keywords describing human body parts and “lesion” as text prompts and input them with the original image to the SAM variant to generate bounding boxes, visual concepts, and segmentation masks. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">1</span></a> lists some detected visual concepts. It is noted that the text prompt “lesion” might correctly localize the pathological area for certain skin diseases whose lesion boundaries are clear, but it is challenging for other complicated conditions, such as “Mycetoma”. Still, these captured visual concepts can offer extra information and help us identify the relevant segments of the body or the lesion.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">We define local visual concepts as <math alttext="\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots\}\in\mathbf{V}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.m1.3"><semantics id="S3.SS2.p3.1.m1.3a"><mrow id="S3.SS2.p3.1.m1.3.3" xref="S3.SS2.p3.1.m1.3.3.cmml"><mrow id="S3.SS2.p3.1.m1.3.3.2.2" xref="S3.SS2.p3.1.m1.3.3.2.3.cmml"><mo id="S3.SS2.p3.1.m1.3.3.2.2.3" stretchy="false" xref="S3.SS2.p3.1.m1.3.3.2.3.cmml">{</mo><msub id="S3.SS2.p3.1.m1.2.2.1.1.1" xref="S3.SS2.p3.1.m1.2.2.1.1.1.cmml"><mi id="S3.SS2.p3.1.m1.2.2.1.1.1.2" xref="S3.SS2.p3.1.m1.2.2.1.1.1.2.cmml">𝐯</mi><mn id="S3.SS2.p3.1.m1.2.2.1.1.1.3" xref="S3.SS2.p3.1.m1.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p3.1.m1.3.3.2.2.4" xref="S3.SS2.p3.1.m1.3.3.2.3.cmml">,</mo><msub id="S3.SS2.p3.1.m1.3.3.2.2.2" xref="S3.SS2.p3.1.m1.3.3.2.2.2.cmml"><mi id="S3.SS2.p3.1.m1.3.3.2.2.2.2" xref="S3.SS2.p3.1.m1.3.3.2.2.2.2.cmml">𝐯</mi><mn id="S3.SS2.p3.1.m1.3.3.2.2.2.3" xref="S3.SS2.p3.1.m1.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS2.p3.1.m1.3.3.2.2.5" xref="S3.SS2.p3.1.m1.3.3.2.3.cmml">,</mo><mi id="S3.SS2.p3.1.m1.1.1" mathvariant="normal" xref="S3.SS2.p3.1.m1.1.1.cmml">…</mi><mo id="S3.SS2.p3.1.m1.3.3.2.2.6" stretchy="false" xref="S3.SS2.p3.1.m1.3.3.2.3.cmml">}</mo></mrow><mo id="S3.SS2.p3.1.m1.3.3.3" xref="S3.SS2.p3.1.m1.3.3.3.cmml">∈</mo><mi id="S3.SS2.p3.1.m1.3.3.4" xref="S3.SS2.p3.1.m1.3.3.4.cmml">𝐕</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.3b"><apply id="S3.SS2.p3.1.m1.3.3.cmml" xref="S3.SS2.p3.1.m1.3.3"><in id="S3.SS2.p3.1.m1.3.3.3.cmml" xref="S3.SS2.p3.1.m1.3.3.3"></in><set id="S3.SS2.p3.1.m1.3.3.2.3.cmml" xref="S3.SS2.p3.1.m1.3.3.2.2"><apply id="S3.SS2.p3.1.m1.2.2.1.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.2.2.1.1.1.1.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.m1.2.2.1.1.1.2.cmml" xref="S3.SS2.p3.1.m1.2.2.1.1.1.2">𝐯</ci><cn id="S3.SS2.p3.1.m1.2.2.1.1.1.3.cmml" type="integer" xref="S3.SS2.p3.1.m1.2.2.1.1.1.3">1</cn></apply><apply id="S3.SS2.p3.1.m1.3.3.2.2.2.cmml" xref="S3.SS2.p3.1.m1.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p3.1.m1.3.3.2.2.2.1.cmml" xref="S3.SS2.p3.1.m1.3.3.2.2.2">subscript</csymbol><ci id="S3.SS2.p3.1.m1.3.3.2.2.2.2.cmml" xref="S3.SS2.p3.1.m1.3.3.2.2.2.2">𝐯</ci><cn id="S3.SS2.p3.1.m1.3.3.2.2.2.3.cmml" type="integer" xref="S3.SS2.p3.1.m1.3.3.2.2.2.3">2</cn></apply><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">…</ci></set><ci id="S3.SS2.p3.1.m1.3.3.4.cmml" xref="S3.SS2.p3.1.m1.3.3.4">𝐕</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.3c">\{\mathbf{v}_{1},\mathbf{v}_{2},\ldots\}\in\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.m1.3d">{ bold_v start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , bold_v start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … } ∈ bold_V</annotation></semantics></math>. By focusing on these localized visual elements, our model can diagnose skin diseases more accurately and effectively. Additionally, given the visual concepts identified by SAM, we can enhance the interpretability of our model by pinpointing the specific local patterns that lead to a prediction. The framework provides valuable insights into the decision-making process, thereby increasing its transparency and trustworthiness as a diagnostic system. This dual focus on accuracy and interpretability is crucial for developing a reliable diagnostic tool that can assist healthcare professionals in making informed decisions.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Cross-Attentive Fusion Model</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Due to the unreliability of certain visual concepts from SAM (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">1</span></a>), we concurrently leverage both global image features and local visual concepts. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S2.F2" title="Figure 2 ‣ 2.1 Visual Segment Generation ‣ 2 Related Works ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">2</span></a>, our proposed dual-branch framework is designed to harness the complementary strengths of these two sources of information. This approach allows for a more robust and comprehensive understanding of the diseases by integrating detailed local visual concepts with overarching global image features, thereby improving the overall accuracy and reliability of the analysis.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.2">Firstly, we introduce a global feature encoder, denoted as <math alttext="\mathcal{F}_{g}(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><mrow id="S3.SS3.p2.1.m1.1.2" xref="S3.SS3.p2.1.m1.1.2.cmml"><msub id="S3.SS3.p2.1.m1.1.2.2" xref="S3.SS3.p2.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.1.m1.1.2.2.2" xref="S3.SS3.p2.1.m1.1.2.2.2.cmml">ℱ</mi><mi id="S3.SS3.p2.1.m1.1.2.2.3" xref="S3.SS3.p2.1.m1.1.2.2.3.cmml">g</mi></msub><mo id="S3.SS3.p2.1.m1.1.2.1" xref="S3.SS3.p2.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p2.1.m1.1.2.3.2" xref="S3.SS3.p2.1.m1.1.2.cmml"><mo id="S3.SS3.p2.1.m1.1.2.3.2.1" stretchy="false" xref="S3.SS3.p2.1.m1.1.2.cmml">(</mo><mo id="S3.SS3.p2.1.m1.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p2.1.m1.1.1.cmml">⋅</mo><mo id="S3.SS3.p2.1.m1.1.2.3.2.2" stretchy="false" xref="S3.SS3.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.2"><times id="S3.SS3.p2.1.m1.1.2.1.cmml" xref="S3.SS3.p2.1.m1.1.2.1"></times><apply id="S3.SS3.p2.1.m1.1.2.2.cmml" xref="S3.SS3.p2.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.2.2.1.cmml" xref="S3.SS3.p2.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.2.2.2.cmml" xref="S3.SS3.p2.1.m1.1.2.2.2">ℱ</ci><ci id="S3.SS3.p2.1.m1.1.2.2.3.cmml" xref="S3.SS3.p2.1.m1.1.2.2.3">𝑔</ci></apply><ci id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\mathcal{F}_{g}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">caligraphic_F start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math>, to extract the global features <math alttext="\mathbf{Z}_{g}\in\mathbb{R}^{N\times 1\times D}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><mrow id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><msub id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml"><mi id="S3.SS3.p2.2.m2.1.1.2.2" xref="S3.SS3.p2.2.m2.1.1.2.2.cmml">𝐙</mi><mi id="S3.SS3.p2.2.m2.1.1.2.3" xref="S3.SS3.p2.2.m2.1.1.2.3.cmml">g</mi></msub><mo id="S3.SS3.p2.2.m2.1.1.1" xref="S3.SS3.p2.2.m2.1.1.1.cmml">∈</mo><msup id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.2" xref="S3.SS3.p2.2.m2.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p2.2.m2.1.1.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.cmml"><mi id="S3.SS3.p2.2.m2.1.1.3.3.2" xref="S3.SS3.p2.2.m2.1.1.3.3.2.cmml">N</mi><mo id="S3.SS3.p2.2.m2.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.2.m2.1.1.3.3.1.cmml">×</mo><mn id="S3.SS3.p2.2.m2.1.1.3.3.3" xref="S3.SS3.p2.2.m2.1.1.3.3.3.cmml">1</mn><mo id="S3.SS3.p2.2.m2.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p2.2.m2.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p2.2.m2.1.1.3.3.4" xref="S3.SS3.p2.2.m2.1.1.3.3.4.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><in id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1.1"></in><apply id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.2.1.cmml" xref="S3.SS3.p2.2.m2.1.1.2">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2.2">𝐙</ci><ci id="S3.SS3.p2.2.m2.1.1.2.3.cmml" xref="S3.SS3.p2.2.m2.1.1.2.3">𝑔</ci></apply><apply id="S3.SS3.p2.2.m2.1.1.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3">superscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.2">ℝ</ci><apply id="S3.SS3.p2.2.m2.1.1.3.3.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3"><times id="S3.SS3.p2.2.m2.1.1.3.3.1.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.1"></times><ci id="S3.SS3.p2.2.m2.1.1.3.3.2.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.2">𝑁</ci><cn id="S3.SS3.p2.2.m2.1.1.3.3.3.cmml" type="integer" xref="S3.SS3.p2.2.m2.1.1.3.3.3">1</cn><ci id="S3.SS3.p2.2.m2.1.1.3.3.4.cmml" xref="S3.SS3.p2.2.m2.1.1.3.3.4">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathbf{Z}_{g}\in\mathbb{R}^{N\times 1\times D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">bold_Z start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × 1 × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> for the entire image. This is based on the premise that the overall image encapsulates most of the essential information. However, it is crucial to pinpoint the specific location of the skin disease’s origin. Moreover, global features often encompass background noise, which can degrade the prediction performance. Thus, while global features provide a broad context, their inherent noise necessitates complementary local analysis for precise disease localization and classification.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.4">To address this issue, we propose exploring local visual concepts that potentially preserve more focused information about skin disease, thereby enhancing model decision-making. As illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S2.F2" title="Figure 2 ‣ 2.1 Visual Segment Generation ‣ 2 Related Works ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">2</span></a>, we input the local visual concepts <math alttext="\mathbf{V}" class="ltx_Math" display="inline" id="S3.SS3.p3.1.m1.1"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.1.m1.1d">bold_V</annotation></semantics></math> into the local encoder <math alttext="\mathcal{F}_{l}(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p3.2.m2.1"><semantics id="S3.SS3.p3.2.m2.1a"><mrow id="S3.SS3.p3.2.m2.1.2" xref="S3.SS3.p3.2.m2.1.2.cmml"><msub id="S3.SS3.p3.2.m2.1.2.2" xref="S3.SS3.p3.2.m2.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p3.2.m2.1.2.2.2" xref="S3.SS3.p3.2.m2.1.2.2.2.cmml">ℱ</mi><mi id="S3.SS3.p3.2.m2.1.2.2.3" xref="S3.SS3.p3.2.m2.1.2.2.3.cmml">l</mi></msub><mo id="S3.SS3.p3.2.m2.1.2.1" xref="S3.SS3.p3.2.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p3.2.m2.1.2.3.2" xref="S3.SS3.p3.2.m2.1.2.cmml"><mo id="S3.SS3.p3.2.m2.1.2.3.2.1" stretchy="false" xref="S3.SS3.p3.2.m2.1.2.cmml">(</mo><mo id="S3.SS3.p3.2.m2.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p3.2.m2.1.1.cmml">⋅</mo><mo id="S3.SS3.p3.2.m2.1.2.3.2.2" stretchy="false" xref="S3.SS3.p3.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.2.m2.1b"><apply id="S3.SS3.p3.2.m2.1.2.cmml" xref="S3.SS3.p3.2.m2.1.2"><times id="S3.SS3.p3.2.m2.1.2.1.cmml" xref="S3.SS3.p3.2.m2.1.2.1"></times><apply id="S3.SS3.p3.2.m2.1.2.2.cmml" xref="S3.SS3.p3.2.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p3.2.m2.1.2.2.1.cmml" xref="S3.SS3.p3.2.m2.1.2.2">subscript</csymbol><ci id="S3.SS3.p3.2.m2.1.2.2.2.cmml" xref="S3.SS3.p3.2.m2.1.2.2.2">ℱ</ci><ci id="S3.SS3.p3.2.m2.1.2.2.3.cmml" xref="S3.SS3.p3.2.m2.1.2.2.3">𝑙</ci></apply><ci id="S3.SS3.p3.2.m2.1.1.cmml" xref="S3.SS3.p3.2.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.2.m2.1c">\mathcal{F}_{l}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.2.m2.1d">caligraphic_F start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> to extract local features <math alttext="\mathbf{Z}_{l}\in\mathbb{R}^{N\times n\times D}" class="ltx_Math" display="inline" id="S3.SS3.p3.3.m3.1"><semantics id="S3.SS3.p3.3.m3.1a"><mrow id="S3.SS3.p3.3.m3.1.1" xref="S3.SS3.p3.3.m3.1.1.cmml"><msub id="S3.SS3.p3.3.m3.1.1.2" xref="S3.SS3.p3.3.m3.1.1.2.cmml"><mi id="S3.SS3.p3.3.m3.1.1.2.2" xref="S3.SS3.p3.3.m3.1.1.2.2.cmml">𝐙</mi><mi id="S3.SS3.p3.3.m3.1.1.2.3" xref="S3.SS3.p3.3.m3.1.1.2.3.cmml">l</mi></msub><mo id="S3.SS3.p3.3.m3.1.1.1" xref="S3.SS3.p3.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS3.p3.3.m3.1.1.3" xref="S3.SS3.p3.3.m3.1.1.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.3.2" xref="S3.SS3.p3.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p3.3.m3.1.1.3.3" xref="S3.SS3.p3.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.p3.3.m3.1.1.3.3.2" xref="S3.SS3.p3.3.m3.1.1.3.3.2.cmml">N</mi><mo id="S3.SS3.p3.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p3.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p3.3.m3.1.1.3.3.3" xref="S3.SS3.p3.3.m3.1.1.3.3.3.cmml">n</mi><mo id="S3.SS3.p3.3.m3.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p3.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p3.3.m3.1.1.3.3.4" xref="S3.SS3.p3.3.m3.1.1.3.3.4.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.3.m3.1b"><apply id="S3.SS3.p3.3.m3.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1"><in id="S3.SS3.p3.3.m3.1.1.1.cmml" xref="S3.SS3.p3.3.m3.1.1.1"></in><apply id="S3.SS3.p3.3.m3.1.1.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.2.1.cmml" xref="S3.SS3.p3.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.2.2.cmml" xref="S3.SS3.p3.3.m3.1.1.2.2">𝐙</ci><ci id="S3.SS3.p3.3.m3.1.1.2.3.cmml" xref="S3.SS3.p3.3.m3.1.1.2.3">𝑙</ci></apply><apply id="S3.SS3.p3.3.m3.1.1.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p3.3.m3.1.1.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS3.p3.3.m3.1.1.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS3.p3.3.m3.1.1.3.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3"><times id="S3.SS3.p3.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.1"></times><ci id="S3.SS3.p3.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.2">𝑁</ci><ci id="S3.SS3.p3.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.3">𝑛</ci><ci id="S3.SS3.p3.3.m3.1.1.3.3.4.cmml" xref="S3.SS3.p3.3.m3.1.1.3.3.4">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.3.m3.1c">\mathbf{Z}_{l}\in\mathbb{R}^{N\times n\times D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.3.m3.1d">bold_Z start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_n × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="n" class="ltx_Math" display="inline" id="S3.SS3.p3.4.m4.1"><semantics id="S3.SS3.p3.4.m4.1a"><mi id="S3.SS3.p3.4.m4.1.1" xref="S3.SS3.p3.4.m4.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.4.m4.1b"><ci id="S3.SS3.p3.4.m4.1.1.cmml" xref="S3.SS3.p3.4.m4.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.4.m4.1c">n</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p3.4.m4.1d">italic_n</annotation></semantics></math> represents the total number of visual concepts. This method effectively filters out background noise and irrelevant objects, ensuring the model focuses on the most severely affected and informative pixels.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.4">To efficiently leverage the complementary information between local concepts and global images, we propose a cross-attentive module where local concepts serve as “query” prompts to highlight the most salient areas within the global features. Specifically, the attention map <math alttext="\mathbf{M}\in\mathbb{R}^{N\times n\times 1}" class="ltx_Math" display="inline" id="S3.SS3.p4.1.m1.1"><semantics id="S3.SS3.p4.1.m1.1a"><mrow id="S3.SS3.p4.1.m1.1.1" xref="S3.SS3.p4.1.m1.1.1.cmml"><mi id="S3.SS3.p4.1.m1.1.1.2" xref="S3.SS3.p4.1.m1.1.1.2.cmml">𝐌</mi><mo id="S3.SS3.p4.1.m1.1.1.1" xref="S3.SS3.p4.1.m1.1.1.1.cmml">∈</mo><msup id="S3.SS3.p4.1.m1.1.1.3" xref="S3.SS3.p4.1.m1.1.1.3.cmml"><mi id="S3.SS3.p4.1.m1.1.1.3.2" xref="S3.SS3.p4.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p4.1.m1.1.1.3.3" xref="S3.SS3.p4.1.m1.1.1.3.3.cmml"><mi id="S3.SS3.p4.1.m1.1.1.3.3.2" xref="S3.SS3.p4.1.m1.1.1.3.3.2.cmml">N</mi><mo id="S3.SS3.p4.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p4.1.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p4.1.m1.1.1.3.3.3" xref="S3.SS3.p4.1.m1.1.1.3.3.3.cmml">n</mi><mo id="S3.SS3.p4.1.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p4.1.m1.1.1.3.3.1.cmml">×</mo><mn id="S3.SS3.p4.1.m1.1.1.3.3.4" xref="S3.SS3.p4.1.m1.1.1.3.3.4.cmml">1</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.1.m1.1b"><apply id="S3.SS3.p4.1.m1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1"><in id="S3.SS3.p4.1.m1.1.1.1.cmml" xref="S3.SS3.p4.1.m1.1.1.1"></in><ci id="S3.SS3.p4.1.m1.1.1.2.cmml" xref="S3.SS3.p4.1.m1.1.1.2">𝐌</ci><apply id="S3.SS3.p4.1.m1.1.1.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p4.1.m1.1.1.3.1.cmml" xref="S3.SS3.p4.1.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p4.1.m1.1.1.3.2.cmml" xref="S3.SS3.p4.1.m1.1.1.3.2">ℝ</ci><apply id="S3.SS3.p4.1.m1.1.1.3.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3.3"><times id="S3.SS3.p4.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p4.1.m1.1.1.3.3.1"></times><ci id="S3.SS3.p4.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p4.1.m1.1.1.3.3.2">𝑁</ci><ci id="S3.SS3.p4.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.p4.1.m1.1.1.3.3.3">𝑛</ci><cn id="S3.SS3.p4.1.m1.1.1.3.3.4.cmml" type="integer" xref="S3.SS3.p4.1.m1.1.1.3.3.4">1</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.1.m1.1c">\mathbf{M}\in\mathbb{R}^{N\times n\times 1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.1.m1.1d">bold_M ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_n × 1 end_POSTSUPERSCRIPT</annotation></semantics></math> acts as a soft mask between the local concept features <math alttext="\mathbf{Z}_{l}" class="ltx_Math" display="inline" id="S3.SS3.p4.2.m2.1"><semantics id="S3.SS3.p4.2.m2.1a"><msub id="S3.SS3.p4.2.m2.1.1" xref="S3.SS3.p4.2.m2.1.1.cmml"><mi id="S3.SS3.p4.2.m2.1.1.2" xref="S3.SS3.p4.2.m2.1.1.2.cmml">𝐙</mi><mi id="S3.SS3.p4.2.m2.1.1.3" xref="S3.SS3.p4.2.m2.1.1.3.cmml">l</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.2.m2.1b"><apply id="S3.SS3.p4.2.m2.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.2.m2.1.1.1.cmml" xref="S3.SS3.p4.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p4.2.m2.1.1.2.cmml" xref="S3.SS3.p4.2.m2.1.1.2">𝐙</ci><ci id="S3.SS3.p4.2.m2.1.1.3.cmml" xref="S3.SS3.p4.2.m2.1.1.3">𝑙</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.2.m2.1c">\mathbf{Z}_{l}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.2.m2.1d">bold_Z start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT</annotation></semantics></math> and the global features <math alttext="\mathbf{Z}_{g}" class="ltx_Math" display="inline" id="S3.SS3.p4.3.m3.1"><semantics id="S3.SS3.p4.3.m3.1a"><msub id="S3.SS3.p4.3.m3.1.1" xref="S3.SS3.p4.3.m3.1.1.cmml"><mi id="S3.SS3.p4.3.m3.1.1.2" xref="S3.SS3.p4.3.m3.1.1.2.cmml">𝐙</mi><mi id="S3.SS3.p4.3.m3.1.1.3" xref="S3.SS3.p4.3.m3.1.1.3.cmml">g</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.3.m3.1b"><apply id="S3.SS3.p4.3.m3.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS3.p4.3.m3.1.1.1.cmml" xref="S3.SS3.p4.3.m3.1.1">subscript</csymbol><ci id="S3.SS3.p4.3.m3.1.1.2.cmml" xref="S3.SS3.p4.3.m3.1.1.2">𝐙</ci><ci id="S3.SS3.p4.3.m3.1.1.3.cmml" xref="S3.SS3.p4.3.m3.1.1.3">𝑔</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.3.m3.1c">\mathbf{Z}_{g}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.3.m3.1d">bold_Z start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT</annotation></semantics></math>. This attention map is normalized using a global <math alttext="\mathsf{softmax}" class="ltx_Math" display="inline" id="S3.SS3.p4.4.m4.1"><semantics id="S3.SS3.p4.4.m4.1a"><mi id="S3.SS3.p4.4.m4.1.1" xref="S3.SS3.p4.4.m4.1.1.cmml">𝗌𝗈𝖿𝗍𝗆𝖺𝗑</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p4.4.m4.1b"><ci id="S3.SS3.p4.4.m4.1.1.cmml" xref="S3.SS3.p4.4.m4.1.1">𝗌𝗈𝖿𝗍𝗆𝖺𝗑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p4.4.m4.1c">\mathsf{softmax}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p4.4.m4.1d">sansserif_softmax</annotation></semantics></math> function to capture the most relevant segments between the local and global features. This component further strengthens the model’s ability to focus on the critical areas, thereby enhancing the overall precision and effectiveness of the diagnosis system.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.10">Following the Transformer design, we define three variables—query, key, and value—as <math alttext="\mathbf{Q}=\mathbf{Z}_{l}\mathbf{W}_{q}" class="ltx_Math" display="inline" id="S3.SS3.p5.1.m1.1"><semantics id="S3.SS3.p5.1.m1.1a"><mrow id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml"><mi id="S3.SS3.p5.1.m1.1.1.2" xref="S3.SS3.p5.1.m1.1.1.2.cmml">𝐐</mi><mo id="S3.SS3.p5.1.m1.1.1.1" xref="S3.SS3.p5.1.m1.1.1.1.cmml">=</mo><mrow id="S3.SS3.p5.1.m1.1.1.3" xref="S3.SS3.p5.1.m1.1.1.3.cmml"><msub id="S3.SS3.p5.1.m1.1.1.3.2" xref="S3.SS3.p5.1.m1.1.1.3.2.cmml"><mi id="S3.SS3.p5.1.m1.1.1.3.2.2" xref="S3.SS3.p5.1.m1.1.1.3.2.2.cmml">𝐙</mi><mi id="S3.SS3.p5.1.m1.1.1.3.2.3" xref="S3.SS3.p5.1.m1.1.1.3.2.3.cmml">l</mi></msub><mo id="S3.SS3.p5.1.m1.1.1.3.1" xref="S3.SS3.p5.1.m1.1.1.3.1.cmml">⁢</mo><msub id="S3.SS3.p5.1.m1.1.1.3.3" xref="S3.SS3.p5.1.m1.1.1.3.3.cmml"><mi id="S3.SS3.p5.1.m1.1.1.3.3.2" xref="S3.SS3.p5.1.m1.1.1.3.3.2.cmml">𝐖</mi><mi id="S3.SS3.p5.1.m1.1.1.3.3.3" xref="S3.SS3.p5.1.m1.1.1.3.3.3.cmml">q</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><apply id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1"><eq id="S3.SS3.p5.1.m1.1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1.1"></eq><ci id="S3.SS3.p5.1.m1.1.1.2.cmml" xref="S3.SS3.p5.1.m1.1.1.2">𝐐</ci><apply id="S3.SS3.p5.1.m1.1.1.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3"><times id="S3.SS3.p5.1.m1.1.1.3.1.cmml" xref="S3.SS3.p5.1.m1.1.1.3.1"></times><apply id="S3.SS3.p5.1.m1.1.1.3.2.cmml" xref="S3.SS3.p5.1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.1.1.3.2.1.cmml" xref="S3.SS3.p5.1.m1.1.1.3.2">subscript</csymbol><ci id="S3.SS3.p5.1.m1.1.1.3.2.2.cmml" xref="S3.SS3.p5.1.m1.1.1.3.2.2">𝐙</ci><ci id="S3.SS3.p5.1.m1.1.1.3.2.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3.2.3">𝑙</ci></apply><apply id="S3.SS3.p5.1.m1.1.1.3.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p5.1.m1.1.1.3.3.1.cmml" xref="S3.SS3.p5.1.m1.1.1.3.3">subscript</csymbol><ci id="S3.SS3.p5.1.m1.1.1.3.3.2.cmml" xref="S3.SS3.p5.1.m1.1.1.3.3.2">𝐖</ci><ci id="S3.SS3.p5.1.m1.1.1.3.3.3.cmml" xref="S3.SS3.p5.1.m1.1.1.3.3.3">𝑞</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\mathbf{Q}=\mathbf{Z}_{l}\mathbf{W}_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.1.m1.1d">bold_Q = bold_Z start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathbf{K}=\mathbf{Z}_{g}\mathbf{W}_{k}" class="ltx_Math" display="inline" id="S3.SS3.p5.2.m2.1"><semantics id="S3.SS3.p5.2.m2.1a"><mrow id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml"><mi id="S3.SS3.p5.2.m2.1.1.2" xref="S3.SS3.p5.2.m2.1.1.2.cmml">𝐊</mi><mo id="S3.SS3.p5.2.m2.1.1.1" xref="S3.SS3.p5.2.m2.1.1.1.cmml">=</mo><mrow id="S3.SS3.p5.2.m2.1.1.3" xref="S3.SS3.p5.2.m2.1.1.3.cmml"><msub id="S3.SS3.p5.2.m2.1.1.3.2" xref="S3.SS3.p5.2.m2.1.1.3.2.cmml"><mi id="S3.SS3.p5.2.m2.1.1.3.2.2" xref="S3.SS3.p5.2.m2.1.1.3.2.2.cmml">𝐙</mi><mi id="S3.SS3.p5.2.m2.1.1.3.2.3" xref="S3.SS3.p5.2.m2.1.1.3.2.3.cmml">g</mi></msub><mo id="S3.SS3.p5.2.m2.1.1.3.1" xref="S3.SS3.p5.2.m2.1.1.3.1.cmml">⁢</mo><msub id="S3.SS3.p5.2.m2.1.1.3.3" xref="S3.SS3.p5.2.m2.1.1.3.3.cmml"><mi id="S3.SS3.p5.2.m2.1.1.3.3.2" xref="S3.SS3.p5.2.m2.1.1.3.3.2.cmml">𝐖</mi><mi id="S3.SS3.p5.2.m2.1.1.3.3.3" xref="S3.SS3.p5.2.m2.1.1.3.3.3.cmml">k</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><apply id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1"><eq id="S3.SS3.p5.2.m2.1.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1.1"></eq><ci id="S3.SS3.p5.2.m2.1.1.2.cmml" xref="S3.SS3.p5.2.m2.1.1.2">𝐊</ci><apply id="S3.SS3.p5.2.m2.1.1.3.cmml" xref="S3.SS3.p5.2.m2.1.1.3"><times id="S3.SS3.p5.2.m2.1.1.3.1.cmml" xref="S3.SS3.p5.2.m2.1.1.3.1"></times><apply id="S3.SS3.p5.2.m2.1.1.3.2.cmml" xref="S3.SS3.p5.2.m2.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p5.2.m2.1.1.3.2.1.cmml" xref="S3.SS3.p5.2.m2.1.1.3.2">subscript</csymbol><ci id="S3.SS3.p5.2.m2.1.1.3.2.2.cmml" xref="S3.SS3.p5.2.m2.1.1.3.2.2">𝐙</ci><ci id="S3.SS3.p5.2.m2.1.1.3.2.3.cmml" xref="S3.SS3.p5.2.m2.1.1.3.2.3">𝑔</ci></apply><apply id="S3.SS3.p5.2.m2.1.1.3.3.cmml" xref="S3.SS3.p5.2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p5.2.m2.1.1.3.3.1.cmml" xref="S3.SS3.p5.2.m2.1.1.3.3">subscript</csymbol><ci id="S3.SS3.p5.2.m2.1.1.3.3.2.cmml" xref="S3.SS3.p5.2.m2.1.1.3.3.2">𝐖</ci><ci id="S3.SS3.p5.2.m2.1.1.3.3.3.cmml" xref="S3.SS3.p5.2.m2.1.1.3.3.3">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">\mathbf{K}=\mathbf{Z}_{g}\mathbf{W}_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.2.m2.1d">bold_K = bold_Z start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\mathbf{V}=\mathbf{Z}_{g}\mathbf{W}_{v}" class="ltx_Math" display="inline" id="S3.SS3.p5.3.m3.1"><semantics id="S3.SS3.p5.3.m3.1a"><mrow id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml"><mi id="S3.SS3.p5.3.m3.1.1.2" xref="S3.SS3.p5.3.m3.1.1.2.cmml">𝐕</mi><mo id="S3.SS3.p5.3.m3.1.1.1" xref="S3.SS3.p5.3.m3.1.1.1.cmml">=</mo><mrow id="S3.SS3.p5.3.m3.1.1.3" xref="S3.SS3.p5.3.m3.1.1.3.cmml"><msub id="S3.SS3.p5.3.m3.1.1.3.2" xref="S3.SS3.p5.3.m3.1.1.3.2.cmml"><mi id="S3.SS3.p5.3.m3.1.1.3.2.2" xref="S3.SS3.p5.3.m3.1.1.3.2.2.cmml">𝐙</mi><mi id="S3.SS3.p5.3.m3.1.1.3.2.3" xref="S3.SS3.p5.3.m3.1.1.3.2.3.cmml">g</mi></msub><mo id="S3.SS3.p5.3.m3.1.1.3.1" xref="S3.SS3.p5.3.m3.1.1.3.1.cmml">⁢</mo><msub id="S3.SS3.p5.3.m3.1.1.3.3" xref="S3.SS3.p5.3.m3.1.1.3.3.cmml"><mi id="S3.SS3.p5.3.m3.1.1.3.3.2" xref="S3.SS3.p5.3.m3.1.1.3.3.2.cmml">𝐖</mi><mi id="S3.SS3.p5.3.m3.1.1.3.3.3" xref="S3.SS3.p5.3.m3.1.1.3.3.3.cmml">v</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><apply id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1"><eq id="S3.SS3.p5.3.m3.1.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1.1"></eq><ci id="S3.SS3.p5.3.m3.1.1.2.cmml" xref="S3.SS3.p5.3.m3.1.1.2">𝐕</ci><apply id="S3.SS3.p5.3.m3.1.1.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3"><times id="S3.SS3.p5.3.m3.1.1.3.1.cmml" xref="S3.SS3.p5.3.m3.1.1.3.1"></times><apply id="S3.SS3.p5.3.m3.1.1.3.2.cmml" xref="S3.SS3.p5.3.m3.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.3.2.1.cmml" xref="S3.SS3.p5.3.m3.1.1.3.2">subscript</csymbol><ci id="S3.SS3.p5.3.m3.1.1.3.2.2.cmml" xref="S3.SS3.p5.3.m3.1.1.3.2.2">𝐙</ci><ci id="S3.SS3.p5.3.m3.1.1.3.2.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3.2.3">𝑔</ci></apply><apply id="S3.SS3.p5.3.m3.1.1.3.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3.3"><csymbol cd="ambiguous" id="S3.SS3.p5.3.m3.1.1.3.3.1.cmml" xref="S3.SS3.p5.3.m3.1.1.3.3">subscript</csymbol><ci id="S3.SS3.p5.3.m3.1.1.3.3.2.cmml" xref="S3.SS3.p5.3.m3.1.1.3.3.2">𝐖</ci><ci id="S3.SS3.p5.3.m3.1.1.3.3.3.cmml" xref="S3.SS3.p5.3.m3.1.1.3.3.3">𝑣</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">\mathbf{V}=\mathbf{Z}_{g}\mathbf{W}_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.3.m3.1d">bold_V = bold_Z start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT bold_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="\mathbf{W}_{q}" class="ltx_Math" display="inline" id="S3.SS3.p5.4.m4.1"><semantics id="S3.SS3.p5.4.m4.1a"><msub id="S3.SS3.p5.4.m4.1.1" xref="S3.SS3.p5.4.m4.1.1.cmml"><mi id="S3.SS3.p5.4.m4.1.1.2" xref="S3.SS3.p5.4.m4.1.1.2.cmml">𝐖</mi><mi id="S3.SS3.p5.4.m4.1.1.3" xref="S3.SS3.p5.4.m4.1.1.3.cmml">q</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.1b"><apply id="S3.SS3.p5.4.m4.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.4.m4.1.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1">subscript</csymbol><ci id="S3.SS3.p5.4.m4.1.1.2.cmml" xref="S3.SS3.p5.4.m4.1.1.2">𝐖</ci><ci id="S3.SS3.p5.4.m4.1.1.3.cmml" xref="S3.SS3.p5.4.m4.1.1.3">𝑞</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.1c">\mathbf{W}_{q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.4.m4.1d">bold_W start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\mathbf{W}_{k}" class="ltx_Math" display="inline" id="S3.SS3.p5.5.m5.1"><semantics id="S3.SS3.p5.5.m5.1a"><msub id="S3.SS3.p5.5.m5.1.1" xref="S3.SS3.p5.5.m5.1.1.cmml"><mi id="S3.SS3.p5.5.m5.1.1.2" xref="S3.SS3.p5.5.m5.1.1.2.cmml">𝐖</mi><mi id="S3.SS3.p5.5.m5.1.1.3" xref="S3.SS3.p5.5.m5.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.5.m5.1b"><apply id="S3.SS3.p5.5.m5.1.1.cmml" xref="S3.SS3.p5.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.5.m5.1.1.1.cmml" xref="S3.SS3.p5.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p5.5.m5.1.1.2.cmml" xref="S3.SS3.p5.5.m5.1.1.2">𝐖</ci><ci id="S3.SS3.p5.5.m5.1.1.3.cmml" xref="S3.SS3.p5.5.m5.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.5.m5.1c">\mathbf{W}_{k}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.5.m5.1d">bold_W start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>, and <math alttext="\mathbf{W}_{v}" class="ltx_Math" display="inline" id="S3.SS3.p5.6.m6.1"><semantics id="S3.SS3.p5.6.m6.1a"><msub id="S3.SS3.p5.6.m6.1.1" xref="S3.SS3.p5.6.m6.1.1.cmml"><mi id="S3.SS3.p5.6.m6.1.1.2" xref="S3.SS3.p5.6.m6.1.1.2.cmml">𝐖</mi><mi id="S3.SS3.p5.6.m6.1.1.3" xref="S3.SS3.p5.6.m6.1.1.3.cmml">v</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.6.m6.1b"><apply id="S3.SS3.p5.6.m6.1.1.cmml" xref="S3.SS3.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.6.m6.1.1.1.cmml" xref="S3.SS3.p5.6.m6.1.1">subscript</csymbol><ci id="S3.SS3.p5.6.m6.1.1.2.cmml" xref="S3.SS3.p5.6.m6.1.1.2">𝐖</ci><ci id="S3.SS3.p5.6.m6.1.1.3.cmml" xref="S3.SS3.p5.6.m6.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.6.m6.1c">\mathbf{W}_{v}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.6.m6.1d">bold_W start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> are the linear projection matrices in <math alttext="\mathbb{R}^{D\times D}" class="ltx_Math" display="inline" id="S3.SS3.p5.7.m7.1"><semantics id="S3.SS3.p5.7.m7.1a"><msup id="S3.SS3.p5.7.m7.1.1" xref="S3.SS3.p5.7.m7.1.1.cmml"><mi id="S3.SS3.p5.7.m7.1.1.2" xref="S3.SS3.p5.7.m7.1.1.2.cmml">ℝ</mi><mrow id="S3.SS3.p5.7.m7.1.1.3" xref="S3.SS3.p5.7.m7.1.1.3.cmml"><mi id="S3.SS3.p5.7.m7.1.1.3.2" xref="S3.SS3.p5.7.m7.1.1.3.2.cmml">D</mi><mo id="S3.SS3.p5.7.m7.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p5.7.m7.1.1.3.1.cmml">×</mo><mi id="S3.SS3.p5.7.m7.1.1.3.3" xref="S3.SS3.p5.7.m7.1.1.3.3.cmml">D</mi></mrow></msup><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.7.m7.1b"><apply id="S3.SS3.p5.7.m7.1.1.cmml" xref="S3.SS3.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS3.p5.7.m7.1.1.1.cmml" xref="S3.SS3.p5.7.m7.1.1">superscript</csymbol><ci id="S3.SS3.p5.7.m7.1.1.2.cmml" xref="S3.SS3.p5.7.m7.1.1.2">ℝ</ci><apply id="S3.SS3.p5.7.m7.1.1.3.cmml" xref="S3.SS3.p5.7.m7.1.1.3"><times id="S3.SS3.p5.7.m7.1.1.3.1.cmml" xref="S3.SS3.p5.7.m7.1.1.3.1"></times><ci id="S3.SS3.p5.7.m7.1.1.3.2.cmml" xref="S3.SS3.p5.7.m7.1.1.3.2">𝐷</ci><ci id="S3.SS3.p5.7.m7.1.1.3.3.cmml" xref="S3.SS3.p5.7.m7.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.7.m7.1c">\mathbb{R}^{D\times D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.7.m7.1d">blackboard_R start_POSTSUPERSCRIPT italic_D × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> used to generate the query <math alttext="\mathbf{Q}" class="ltx_Math" display="inline" id="S3.SS3.p5.8.m8.1"><semantics id="S3.SS3.p5.8.m8.1a"><mi id="S3.SS3.p5.8.m8.1.1" xref="S3.SS3.p5.8.m8.1.1.cmml">𝐐</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.8.m8.1b"><ci id="S3.SS3.p5.8.m8.1.1.cmml" xref="S3.SS3.p5.8.m8.1.1">𝐐</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.8.m8.1c">\mathbf{Q}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.8.m8.1d">bold_Q</annotation></semantics></math>, key <math alttext="\mathbf{K}" class="ltx_Math" display="inline" id="S3.SS3.p5.9.m9.1"><semantics id="S3.SS3.p5.9.m9.1a"><mi id="S3.SS3.p5.9.m9.1.1" xref="S3.SS3.p5.9.m9.1.1.cmml">𝐊</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.9.m9.1b"><ci id="S3.SS3.p5.9.m9.1.1.cmml" xref="S3.SS3.p5.9.m9.1.1">𝐊</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.9.m9.1c">\mathbf{K}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.9.m9.1d">bold_K</annotation></semantics></math>, and value <math alttext="\mathbf{V}" class="ltx_Math" display="inline" id="S3.SS3.p5.10.m10.1"><semantics id="S3.SS3.p5.10.m10.1a"><mi id="S3.SS3.p5.10.m10.1.1" xref="S3.SS3.p5.10.m10.1.1.cmml">𝐕</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.10.m10.1b"><ci id="S3.SS3.p5.10.m10.1.1.cmml" xref="S3.SS3.p5.10.m10.1.1">𝐕</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.10.m10.1c">\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.10.m10.1d">bold_V</annotation></semantics></math>, respectively. These maps are then processed by each Transformer module to generate refined latent features, as described as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\left\{\begin{aligned} &amp;\mathbf{I}=\mathbf{Z}_{g}+\mathcal{F}_{\mathrm{Drop}}(%
\mathbf{M}\mathbf{V}\mathbf{W}_{proj}),\\
&amp;\mathbf{O}=\mathbf{I}+\mathcal{F}_{\mathrm{MLP}}(\mathcal{F}_{\mathrm{LN}}(%
\mathbf{I})),\end{aligned}\right." class="ltx_math_unparsed" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3b"><mo id="S3.E1.m1.3.4">{</mo><mtable columnspacing="0pt" displaystyle="true" id="S3.E1.m1.3.3" rowspacing="0pt"><mtr id="S3.E1.m1.3.3a"><mtd id="S3.E1.m1.3.3b"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.3.3c"><mrow id="S3.E1.m1.1.1.1.1.1.1"><mrow id="S3.E1.m1.1.1.1.1.1.1.1"><mi id="S3.E1.m1.1.1.1.1.1.1.1.3">𝐈</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.2">=</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.3"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.2">𝐙</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.3.3">g</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.2">+</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1"><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.2">ℱ</mi><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.3.3">Drop</mi></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.2">⁢</mo><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1"><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false">(</mo><msub id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝐌𝐕𝐖</mi><mrow id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3"><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">p</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">r</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1a">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.4">o</mi><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.1b">⁢</mo><mi id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.1.3.5">j</mi></mrow></msub><mo id="S3.E1.m1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.1.1.1.1.1.1.2">,</mo></mrow></mtd></mtr><mtr id="S3.E1.m1.3.3d"><mtd id="S3.E1.m1.3.3e"></mtd><mtd class="ltx_align_left" columnalign="left" id="S3.E1.m1.3.3f"><mrow id="S3.E1.m1.3.3.3.2.2.2"><mrow id="S3.E1.m1.3.3.3.2.2.2.1"><mi id="S3.E1.m1.3.3.3.2.2.2.1.3">𝐎</mi><mo id="S3.E1.m1.3.3.3.2.2.2.1.2">=</mo><mrow id="S3.E1.m1.3.3.3.2.2.2.1.1"><mi id="S3.E1.m1.3.3.3.2.2.2.1.1.3">𝐈</mi><mo id="S3.E1.m1.3.3.3.2.2.2.1.1.2">+</mo><mrow id="S3.E1.m1.3.3.3.2.2.2.1.1.1"><msub id="S3.E1.m1.3.3.3.2.2.2.1.1.1.3"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.3.2.2.2.1.1.1.3.2">ℱ</mi><mi id="S3.E1.m1.3.3.3.2.2.2.1.1.1.3.3">MLP</mi></msub><mo id="S3.E1.m1.3.3.3.2.2.2.1.1.1.2">⁢</mo><mrow id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1"><mo id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.2" stretchy="false">(</mo><mrow id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.1"><msub id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.1.2"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.1.2.2">ℱ</mi><mi id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.1.2.3">LN</mi></msub><mo id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.1.1">⁢</mo><mrow id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.1.3.2"><mo id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.1.3.2.1" stretchy="false">(</mo><mi id="S3.E1.m1.2.2.2.1.1.1">𝐈</mi><mo id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.1.3.2.2" stretchy="false">)</mo></mrow></mrow><mo id="S3.E1.m1.3.3.3.2.2.2.1.1.1.1.1.3" stretchy="false">)</mo></mrow></mrow></mrow></mrow><mo id="S3.E1.m1.3.3.3.2.2.2.2">,</mo></mrow></mtd></mtr></mtable></mrow><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\left\{\begin{aligned} &amp;\mathbf{I}=\mathbf{Z}_{g}+\mathcal{F}_{\mathrm{Drop}}(%
\mathbf{M}\mathbf{V}\mathbf{W}_{proj}),\\
&amp;\mathbf{O}=\mathbf{I}+\mathcal{F}_{\mathrm{MLP}}(\mathcal{F}_{\mathrm{LN}}(%
\mathbf{I})),\end{aligned}\right.</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">{ start_ROW start_CELL end_CELL start_CELL bold_I = bold_Z start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT + caligraphic_F start_POSTSUBSCRIPT roman_Drop end_POSTSUBSCRIPT ( bold_MVW start_POSTSUBSCRIPT italic_p italic_r italic_o italic_j end_POSTSUBSCRIPT ) , end_CELL end_ROW start_ROW start_CELL end_CELL start_CELL bold_O = bold_I + caligraphic_F start_POSTSUBSCRIPT roman_MLP end_POSTSUBSCRIPT ( caligraphic_F start_POSTSUBSCRIPT roman_LN end_POSTSUBSCRIPT ( bold_I ) ) , end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS3.p5.17">where <math alttext="\mathbf{W}_{proj}\in\mathbb{R}^{D\times D}" class="ltx_Math" display="inline" id="S3.SS3.p5.11.m1.1"><semantics id="S3.SS3.p5.11.m1.1a"><mrow id="S3.SS3.p5.11.m1.1.1" xref="S3.SS3.p5.11.m1.1.1.cmml"><msub id="S3.SS3.p5.11.m1.1.1.2" xref="S3.SS3.p5.11.m1.1.1.2.cmml"><mi id="S3.SS3.p5.11.m1.1.1.2.2" xref="S3.SS3.p5.11.m1.1.1.2.2.cmml">𝐖</mi><mrow id="S3.SS3.p5.11.m1.1.1.2.3" xref="S3.SS3.p5.11.m1.1.1.2.3.cmml"><mi id="S3.SS3.p5.11.m1.1.1.2.3.2" xref="S3.SS3.p5.11.m1.1.1.2.3.2.cmml">p</mi><mo id="S3.SS3.p5.11.m1.1.1.2.3.1" xref="S3.SS3.p5.11.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS3.p5.11.m1.1.1.2.3.3" xref="S3.SS3.p5.11.m1.1.1.2.3.3.cmml">r</mi><mo id="S3.SS3.p5.11.m1.1.1.2.3.1a" xref="S3.SS3.p5.11.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS3.p5.11.m1.1.1.2.3.4" xref="S3.SS3.p5.11.m1.1.1.2.3.4.cmml">o</mi><mo id="S3.SS3.p5.11.m1.1.1.2.3.1b" xref="S3.SS3.p5.11.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS3.p5.11.m1.1.1.2.3.5" xref="S3.SS3.p5.11.m1.1.1.2.3.5.cmml">j</mi></mrow></msub><mo id="S3.SS3.p5.11.m1.1.1.1" xref="S3.SS3.p5.11.m1.1.1.1.cmml">∈</mo><msup id="S3.SS3.p5.11.m1.1.1.3" xref="S3.SS3.p5.11.m1.1.1.3.cmml"><mi id="S3.SS3.p5.11.m1.1.1.3.2" xref="S3.SS3.p5.11.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p5.11.m1.1.1.3.3" xref="S3.SS3.p5.11.m1.1.1.3.3.cmml"><mi id="S3.SS3.p5.11.m1.1.1.3.3.2" xref="S3.SS3.p5.11.m1.1.1.3.3.2.cmml">D</mi><mo id="S3.SS3.p5.11.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p5.11.m1.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p5.11.m1.1.1.3.3.3" xref="S3.SS3.p5.11.m1.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.11.m1.1b"><apply id="S3.SS3.p5.11.m1.1.1.cmml" xref="S3.SS3.p5.11.m1.1.1"><in id="S3.SS3.p5.11.m1.1.1.1.cmml" xref="S3.SS3.p5.11.m1.1.1.1"></in><apply id="S3.SS3.p5.11.m1.1.1.2.cmml" xref="S3.SS3.p5.11.m1.1.1.2"><csymbol cd="ambiguous" id="S3.SS3.p5.11.m1.1.1.2.1.cmml" xref="S3.SS3.p5.11.m1.1.1.2">subscript</csymbol><ci id="S3.SS3.p5.11.m1.1.1.2.2.cmml" xref="S3.SS3.p5.11.m1.1.1.2.2">𝐖</ci><apply id="S3.SS3.p5.11.m1.1.1.2.3.cmml" xref="S3.SS3.p5.11.m1.1.1.2.3"><times id="S3.SS3.p5.11.m1.1.1.2.3.1.cmml" xref="S3.SS3.p5.11.m1.1.1.2.3.1"></times><ci id="S3.SS3.p5.11.m1.1.1.2.3.2.cmml" xref="S3.SS3.p5.11.m1.1.1.2.3.2">𝑝</ci><ci id="S3.SS3.p5.11.m1.1.1.2.3.3.cmml" xref="S3.SS3.p5.11.m1.1.1.2.3.3">𝑟</ci><ci id="S3.SS3.p5.11.m1.1.1.2.3.4.cmml" xref="S3.SS3.p5.11.m1.1.1.2.3.4">𝑜</ci><ci id="S3.SS3.p5.11.m1.1.1.2.3.5.cmml" xref="S3.SS3.p5.11.m1.1.1.2.3.5">𝑗</ci></apply></apply><apply id="S3.SS3.p5.11.m1.1.1.3.cmml" xref="S3.SS3.p5.11.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p5.11.m1.1.1.3.1.cmml" xref="S3.SS3.p5.11.m1.1.1.3">superscript</csymbol><ci id="S3.SS3.p5.11.m1.1.1.3.2.cmml" xref="S3.SS3.p5.11.m1.1.1.3.2">ℝ</ci><apply id="S3.SS3.p5.11.m1.1.1.3.3.cmml" xref="S3.SS3.p5.11.m1.1.1.3.3"><times id="S3.SS3.p5.11.m1.1.1.3.3.1.cmml" xref="S3.SS3.p5.11.m1.1.1.3.3.1"></times><ci id="S3.SS3.p5.11.m1.1.1.3.3.2.cmml" xref="S3.SS3.p5.11.m1.1.1.3.3.2">𝐷</ci><ci id="S3.SS3.p5.11.m1.1.1.3.3.3.cmml" xref="S3.SS3.p5.11.m1.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.11.m1.1c">\mathbf{W}_{proj}\in\mathbb{R}^{D\times D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.11.m1.1d">bold_W start_POSTSUBSCRIPT italic_p italic_r italic_o italic_j end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> are learnable projection matrices, <math alttext="\mathcal{F}_{\mathrm{LN}}(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p5.12.m2.1"><semantics id="S3.SS3.p5.12.m2.1a"><mrow id="S3.SS3.p5.12.m2.1.2" xref="S3.SS3.p5.12.m2.1.2.cmml"><msub id="S3.SS3.p5.12.m2.1.2.2" xref="S3.SS3.p5.12.m2.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.12.m2.1.2.2.2" xref="S3.SS3.p5.12.m2.1.2.2.2.cmml">ℱ</mi><mi id="S3.SS3.p5.12.m2.1.2.2.3" xref="S3.SS3.p5.12.m2.1.2.2.3.cmml">LN</mi></msub><mo id="S3.SS3.p5.12.m2.1.2.1" xref="S3.SS3.p5.12.m2.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p5.12.m2.1.2.3.2" xref="S3.SS3.p5.12.m2.1.2.cmml"><mo id="S3.SS3.p5.12.m2.1.2.3.2.1" stretchy="false" xref="S3.SS3.p5.12.m2.1.2.cmml">(</mo><mo id="S3.SS3.p5.12.m2.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p5.12.m2.1.1.cmml">⋅</mo><mo id="S3.SS3.p5.12.m2.1.2.3.2.2" stretchy="false" xref="S3.SS3.p5.12.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.12.m2.1b"><apply id="S3.SS3.p5.12.m2.1.2.cmml" xref="S3.SS3.p5.12.m2.1.2"><times id="S3.SS3.p5.12.m2.1.2.1.cmml" xref="S3.SS3.p5.12.m2.1.2.1"></times><apply id="S3.SS3.p5.12.m2.1.2.2.cmml" xref="S3.SS3.p5.12.m2.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.12.m2.1.2.2.1.cmml" xref="S3.SS3.p5.12.m2.1.2.2">subscript</csymbol><ci id="S3.SS3.p5.12.m2.1.2.2.2.cmml" xref="S3.SS3.p5.12.m2.1.2.2.2">ℱ</ci><ci id="S3.SS3.p5.12.m2.1.2.2.3.cmml" xref="S3.SS3.p5.12.m2.1.2.2.3">LN</ci></apply><ci id="S3.SS3.p5.12.m2.1.1.cmml" xref="S3.SS3.p5.12.m2.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.12.m2.1c">\mathcal{F}_{\mathrm{LN}}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.12.m2.1d">caligraphic_F start_POSTSUBSCRIPT roman_LN end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> is the layer normalization function, <math alttext="\mathcal{F}_{\mathrm{Drop}}(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p5.13.m3.1"><semantics id="S3.SS3.p5.13.m3.1a"><mrow id="S3.SS3.p5.13.m3.1.2" xref="S3.SS3.p5.13.m3.1.2.cmml"><msub id="S3.SS3.p5.13.m3.1.2.2" xref="S3.SS3.p5.13.m3.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.13.m3.1.2.2.2" xref="S3.SS3.p5.13.m3.1.2.2.2.cmml">ℱ</mi><mi id="S3.SS3.p5.13.m3.1.2.2.3" xref="S3.SS3.p5.13.m3.1.2.2.3.cmml">Drop</mi></msub><mo id="S3.SS3.p5.13.m3.1.2.1" xref="S3.SS3.p5.13.m3.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p5.13.m3.1.2.3.2" xref="S3.SS3.p5.13.m3.1.2.cmml"><mo id="S3.SS3.p5.13.m3.1.2.3.2.1" stretchy="false" xref="S3.SS3.p5.13.m3.1.2.cmml">(</mo><mo id="S3.SS3.p5.13.m3.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p5.13.m3.1.1.cmml">⋅</mo><mo id="S3.SS3.p5.13.m3.1.2.3.2.2" stretchy="false" xref="S3.SS3.p5.13.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.13.m3.1b"><apply id="S3.SS3.p5.13.m3.1.2.cmml" xref="S3.SS3.p5.13.m3.1.2"><times id="S3.SS3.p5.13.m3.1.2.1.cmml" xref="S3.SS3.p5.13.m3.1.2.1"></times><apply id="S3.SS3.p5.13.m3.1.2.2.cmml" xref="S3.SS3.p5.13.m3.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.13.m3.1.2.2.1.cmml" xref="S3.SS3.p5.13.m3.1.2.2">subscript</csymbol><ci id="S3.SS3.p5.13.m3.1.2.2.2.cmml" xref="S3.SS3.p5.13.m3.1.2.2.2">ℱ</ci><ci id="S3.SS3.p5.13.m3.1.2.2.3.cmml" xref="S3.SS3.p5.13.m3.1.2.2.3">Drop</ci></apply><ci id="S3.SS3.p5.13.m3.1.1.cmml" xref="S3.SS3.p5.13.m3.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.13.m3.1c">\mathcal{F}_{\mathrm{Drop}}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.13.m3.1d">caligraphic_F start_POSTSUBSCRIPT roman_Drop end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> is the dropout module, and <math alttext="\mathcal{F}_{\mathrm{MLP}}(\cdot)" class="ltx_Math" display="inline" id="S3.SS3.p5.14.m4.1"><semantics id="S3.SS3.p5.14.m4.1a"><mrow id="S3.SS3.p5.14.m4.1.2" xref="S3.SS3.p5.14.m4.1.2.cmml"><msub id="S3.SS3.p5.14.m4.1.2.2" xref="S3.SS3.p5.14.m4.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p5.14.m4.1.2.2.2" xref="S3.SS3.p5.14.m4.1.2.2.2.cmml">ℱ</mi><mi id="S3.SS3.p5.14.m4.1.2.2.3" xref="S3.SS3.p5.14.m4.1.2.2.3.cmml">MLP</mi></msub><mo id="S3.SS3.p5.14.m4.1.2.1" xref="S3.SS3.p5.14.m4.1.2.1.cmml">⁢</mo><mrow id="S3.SS3.p5.14.m4.1.2.3.2" xref="S3.SS3.p5.14.m4.1.2.cmml"><mo id="S3.SS3.p5.14.m4.1.2.3.2.1" stretchy="false" xref="S3.SS3.p5.14.m4.1.2.cmml">(</mo><mo id="S3.SS3.p5.14.m4.1.1" lspace="0em" rspace="0em" xref="S3.SS3.p5.14.m4.1.1.cmml">⋅</mo><mo id="S3.SS3.p5.14.m4.1.2.3.2.2" stretchy="false" xref="S3.SS3.p5.14.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.14.m4.1b"><apply id="S3.SS3.p5.14.m4.1.2.cmml" xref="S3.SS3.p5.14.m4.1.2"><times id="S3.SS3.p5.14.m4.1.2.1.cmml" xref="S3.SS3.p5.14.m4.1.2.1"></times><apply id="S3.SS3.p5.14.m4.1.2.2.cmml" xref="S3.SS3.p5.14.m4.1.2.2"><csymbol cd="ambiguous" id="S3.SS3.p5.14.m4.1.2.2.1.cmml" xref="S3.SS3.p5.14.m4.1.2.2">subscript</csymbol><ci id="S3.SS3.p5.14.m4.1.2.2.2.cmml" xref="S3.SS3.p5.14.m4.1.2.2.2">ℱ</ci><ci id="S3.SS3.p5.14.m4.1.2.2.3.cmml" xref="S3.SS3.p5.14.m4.1.2.2.3">MLP</ci></apply><ci id="S3.SS3.p5.14.m4.1.1.cmml" xref="S3.SS3.p5.14.m4.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.14.m4.1c">\mathcal{F}_{\mathrm{MLP}}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.14.m4.1d">caligraphic_F start_POSTSUBSCRIPT roman_MLP end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> is the multi-layer perceptron (MLP) module. The intermediate latent feature is denoted as <math alttext="\mathbf{I}" class="ltx_Math" display="inline" id="S3.SS3.p5.15.m5.1"><semantics id="S3.SS3.p5.15.m5.1a"><mi id="S3.SS3.p5.15.m5.1.1" xref="S3.SS3.p5.15.m5.1.1.cmml">𝐈</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.15.m5.1b"><ci id="S3.SS3.p5.15.m5.1.1.cmml" xref="S3.SS3.p5.15.m5.1.1">𝐈</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.15.m5.1c">\mathbf{I}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.15.m5.1d">bold_I</annotation></semantics></math>. The output <math alttext="\mathbf{O}\in\mathbb{R}^{N\times n\times D}" class="ltx_Math" display="inline" id="S3.SS3.p5.16.m6.1"><semantics id="S3.SS3.p5.16.m6.1a"><mrow id="S3.SS3.p5.16.m6.1.1" xref="S3.SS3.p5.16.m6.1.1.cmml"><mi id="S3.SS3.p5.16.m6.1.1.2" xref="S3.SS3.p5.16.m6.1.1.2.cmml">𝐎</mi><mo id="S3.SS3.p5.16.m6.1.1.1" xref="S3.SS3.p5.16.m6.1.1.1.cmml">∈</mo><msup id="S3.SS3.p5.16.m6.1.1.3" xref="S3.SS3.p5.16.m6.1.1.3.cmml"><mi id="S3.SS3.p5.16.m6.1.1.3.2" xref="S3.SS3.p5.16.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS3.p5.16.m6.1.1.3.3" xref="S3.SS3.p5.16.m6.1.1.3.3.cmml"><mi id="S3.SS3.p5.16.m6.1.1.3.3.2" xref="S3.SS3.p5.16.m6.1.1.3.3.2.cmml">N</mi><mo id="S3.SS3.p5.16.m6.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p5.16.m6.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p5.16.m6.1.1.3.3.3" xref="S3.SS3.p5.16.m6.1.1.3.3.3.cmml">n</mi><mo id="S3.SS3.p5.16.m6.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS3.p5.16.m6.1.1.3.3.1.cmml">×</mo><mi id="S3.SS3.p5.16.m6.1.1.3.3.4" xref="S3.SS3.p5.16.m6.1.1.3.3.4.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.16.m6.1b"><apply id="S3.SS3.p5.16.m6.1.1.cmml" xref="S3.SS3.p5.16.m6.1.1"><in id="S3.SS3.p5.16.m6.1.1.1.cmml" xref="S3.SS3.p5.16.m6.1.1.1"></in><ci id="S3.SS3.p5.16.m6.1.1.2.cmml" xref="S3.SS3.p5.16.m6.1.1.2">𝐎</ci><apply id="S3.SS3.p5.16.m6.1.1.3.cmml" xref="S3.SS3.p5.16.m6.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p5.16.m6.1.1.3.1.cmml" xref="S3.SS3.p5.16.m6.1.1.3">superscript</csymbol><ci id="S3.SS3.p5.16.m6.1.1.3.2.cmml" xref="S3.SS3.p5.16.m6.1.1.3.2">ℝ</ci><apply id="S3.SS3.p5.16.m6.1.1.3.3.cmml" xref="S3.SS3.p5.16.m6.1.1.3.3"><times id="S3.SS3.p5.16.m6.1.1.3.3.1.cmml" xref="S3.SS3.p5.16.m6.1.1.3.3.1"></times><ci id="S3.SS3.p5.16.m6.1.1.3.3.2.cmml" xref="S3.SS3.p5.16.m6.1.1.3.3.2">𝑁</ci><ci id="S3.SS3.p5.16.m6.1.1.3.3.3.cmml" xref="S3.SS3.p5.16.m6.1.1.3.3.3">𝑛</ci><ci id="S3.SS3.p5.16.m6.1.1.3.3.4.cmml" xref="S3.SS3.p5.16.m6.1.1.3.3.4">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.16.m6.1c">\mathbf{O}\in\mathbb{R}^{N\times n\times D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.16.m6.1d">bold_O ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_n × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>, which merges information from both local and global features, is more representative of the input image. To build the cross-attentive modules, we define <math alttext="\mathbf{M}=\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{n}}" class="ltx_Math" display="inline" id="S3.SS3.p5.17.m7.1"><semantics id="S3.SS3.p5.17.m7.1a"><mrow id="S3.SS3.p5.17.m7.1.1" xref="S3.SS3.p5.17.m7.1.1.cmml"><mi id="S3.SS3.p5.17.m7.1.1.2" xref="S3.SS3.p5.17.m7.1.1.2.cmml">𝐌</mi><mo id="S3.SS3.p5.17.m7.1.1.1" xref="S3.SS3.p5.17.m7.1.1.1.cmml">=</mo><mfrac id="S3.SS3.p5.17.m7.1.1.3" xref="S3.SS3.p5.17.m7.1.1.3.cmml"><msup id="S3.SS3.p5.17.m7.1.1.3.2" xref="S3.SS3.p5.17.m7.1.1.3.2.cmml"><mi id="S3.SS3.p5.17.m7.1.1.3.2.2" xref="S3.SS3.p5.17.m7.1.1.3.2.2.cmml">𝐐𝐊</mi><mo id="S3.SS3.p5.17.m7.1.1.3.2.3" xref="S3.SS3.p5.17.m7.1.1.3.2.3.cmml">⊤</mo></msup><msqrt id="S3.SS3.p5.17.m7.1.1.3.3" xref="S3.SS3.p5.17.m7.1.1.3.3.cmml"><mi id="S3.SS3.p5.17.m7.1.1.3.3.2" xref="S3.SS3.p5.17.m7.1.1.3.3.2.cmml">n</mi></msqrt></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.17.m7.1b"><apply id="S3.SS3.p5.17.m7.1.1.cmml" xref="S3.SS3.p5.17.m7.1.1"><eq id="S3.SS3.p5.17.m7.1.1.1.cmml" xref="S3.SS3.p5.17.m7.1.1.1"></eq><ci id="S3.SS3.p5.17.m7.1.1.2.cmml" xref="S3.SS3.p5.17.m7.1.1.2">𝐌</ci><apply id="S3.SS3.p5.17.m7.1.1.3.cmml" xref="S3.SS3.p5.17.m7.1.1.3"><divide id="S3.SS3.p5.17.m7.1.1.3.1.cmml" xref="S3.SS3.p5.17.m7.1.1.3"></divide><apply id="S3.SS3.p5.17.m7.1.1.3.2.cmml" xref="S3.SS3.p5.17.m7.1.1.3.2"><csymbol cd="ambiguous" id="S3.SS3.p5.17.m7.1.1.3.2.1.cmml" xref="S3.SS3.p5.17.m7.1.1.3.2">superscript</csymbol><ci id="S3.SS3.p5.17.m7.1.1.3.2.2.cmml" xref="S3.SS3.p5.17.m7.1.1.3.2.2">𝐐𝐊</ci><csymbol cd="latexml" id="S3.SS3.p5.17.m7.1.1.3.2.3.cmml" xref="S3.SS3.p5.17.m7.1.1.3.2.3">top</csymbol></apply><apply id="S3.SS3.p5.17.m7.1.1.3.3.cmml" xref="S3.SS3.p5.17.m7.1.1.3.3"><root id="S3.SS3.p5.17.m7.1.1.3.3a.cmml" xref="S3.SS3.p5.17.m7.1.1.3.3"></root><ci id="S3.SS3.p5.17.m7.1.1.3.3.2.cmml" xref="S3.SS3.p5.17.m7.1.1.3.3.2">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.17.m7.1c">\mathbf{M}=\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{n}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p5.17.m7.1d">bold_M = divide start_ARG bold_QK start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_n end_ARG end_ARG</annotation></semantics></math>. Additionally, multi-head attention is introduced to expand the capacity of the attention modules, allowing for a richer and more nuanced representation of the relevant features. This enhanced structure ensures a more effective integration and utilization of both local and global information.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S3.T1.3.2" style="font-size:90%;">Class distribution for the experimental dataset. “AC Dermatitis” is short for Allergic Contact Dermatitis.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.4" style="width:372.6pt;height:116.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-15.0pt,4.7pt) scale(0.925626086369863,0.925626086369863) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T1.4.1">
<tr class="ltx_tr" id="S3.T1.4.1.1">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.4.1.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span>
<span class="ltx_text ltx_font_bold" id="S3.T1.4.1.1.1.1">NTD</span>
</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Buruli ulcer</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">Yaws</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">Scabies</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.1.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">Leprosy</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.1.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">Mycetoma</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.1.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">Total</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.1.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.4.1.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Patient #</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">300</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.2.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">223</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.2.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">160</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.2.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">57</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.2.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">18</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.2.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">758</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.1.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.4.1.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Image #</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">787</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.3.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">375</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.3.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">391</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.3.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">135</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.3.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">43</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.3.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">1,731</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.1.4">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.4.1.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.4.1.4.1.1">SCIN</span></td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">Eczema</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.4.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">AC Dermatitis</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.4.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">Urticaria</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.4.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">Insect Bite</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.4.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">Psoriasis</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.4.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">Total</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.1.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.4.1.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Patient #</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">355</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.5.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">214</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.5.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">163</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.5.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">128</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.5.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">61</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.4.1.5.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">921</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.1.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.4.1.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">Image #</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.6.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">801</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.6.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">467</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.6.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">338</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.6.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">290</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.6.6" style="padding-top:0.5pt;padding-bottom:0.5pt;">126</td>
<td class="ltx_td ltx_align_left" id="S3.T1.4.1.6.7" style="padding-top:0.5pt;padding-bottom:0.5pt;">2,022</td>
</tr>
<tr class="ltx_tr" id="S3.T1.4.1.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.4.1.7.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td" id="S3.T1.4.1.7.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="S3.T1.4.1.7.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="S3.T1.4.1.7.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="S3.T1.4.1.7.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="S3.T1.4.1.7.6" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="S3.T1.4.1.7.7" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Interpretable Skin Diagnosis</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Having obtained the refined latent features <math alttext="\mathbf{O}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">𝐎</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">𝐎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\mathbf{O}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">bold_O</annotation></semantics></math>, it is essential to identify which parts contribute the most to future observations in the medical field. Typically, most datasets lack such detailed labels because annotating them is labor-intensive and usually requires professional expertise to ensure label quality in medical research. Inspired by the weakly supervised setting described by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib20" title="">20</a>]</cite>, we introduce the Class Activation Map (CAM) and develop multi-instance learning (MIL) loss for the final prediction, instead of relying on traditional classification loss.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.6">In our framework, we utilize a classifier <math alttext="\mathcal{F}_{c}(\cdot)" class="ltx_Math" display="inline" id="S3.SS4.p2.1.m1.1"><semantics id="S3.SS4.p2.1.m1.1a"><mrow id="S3.SS4.p2.1.m1.1.2" xref="S3.SS4.p2.1.m1.1.2.cmml"><msub id="S3.SS4.p2.1.m1.1.2.2" xref="S3.SS4.p2.1.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.1.m1.1.2.2.2" xref="S3.SS4.p2.1.m1.1.2.2.2.cmml">ℱ</mi><mi id="S3.SS4.p2.1.m1.1.2.2.3" xref="S3.SS4.p2.1.m1.1.2.2.3.cmml">c</mi></msub><mo id="S3.SS4.p2.1.m1.1.2.1" xref="S3.SS4.p2.1.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS4.p2.1.m1.1.2.3.2" xref="S3.SS4.p2.1.m1.1.2.cmml"><mo id="S3.SS4.p2.1.m1.1.2.3.2.1" stretchy="false" xref="S3.SS4.p2.1.m1.1.2.cmml">(</mo><mo id="S3.SS4.p2.1.m1.1.1" lspace="0em" rspace="0em" xref="S3.SS4.p2.1.m1.1.1.cmml">⋅</mo><mo id="S3.SS4.p2.1.m1.1.2.3.2.2" stretchy="false" xref="S3.SS4.p2.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><apply id="S3.SS4.p2.1.m1.1.2.cmml" xref="S3.SS4.p2.1.m1.1.2"><times id="S3.SS4.p2.1.m1.1.2.1.cmml" xref="S3.SS4.p2.1.m1.1.2.1"></times><apply id="S3.SS4.p2.1.m1.1.2.2.cmml" xref="S3.SS4.p2.1.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.1.m1.1.2.2.1.cmml" xref="S3.SS4.p2.1.m1.1.2.2">subscript</csymbol><ci id="S3.SS4.p2.1.m1.1.2.2.2.cmml" xref="S3.SS4.p2.1.m1.1.2.2.2">ℱ</ci><ci id="S3.SS4.p2.1.m1.1.2.2.3.cmml" xref="S3.SS4.p2.1.m1.1.2.2.3">𝑐</ci></apply><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\mathcal{F}_{c}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.1.m1.1d">caligraphic_F start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math>, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S2.F2" title="Figure 2 ‣ 2.1 Visual Segment Generation ‣ 2 Related Works ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">2</span></a>, to transform the latent features <math alttext="\mathbf{O}" class="ltx_Math" display="inline" id="S3.SS4.p2.2.m2.1"><semantics id="S3.SS4.p2.2.m2.1a"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">𝐎</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">𝐎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\mathbf{O}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.2.m2.1d">bold_O</annotation></semantics></math> into <math alttext="\mathbf{O}_{cam}\in\mathbb{R}^{N\times n\times C}" class="ltx_Math" display="inline" id="S3.SS4.p2.3.m3.1"><semantics id="S3.SS4.p2.3.m3.1a"><mrow id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml"><msub id="S3.SS4.p2.3.m3.1.1.2" xref="S3.SS4.p2.3.m3.1.1.2.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2.2" xref="S3.SS4.p2.3.m3.1.1.2.2.cmml">𝐎</mi><mrow id="S3.SS4.p2.3.m3.1.1.2.3" xref="S3.SS4.p2.3.m3.1.1.2.3.cmml"><mi id="S3.SS4.p2.3.m3.1.1.2.3.2" xref="S3.SS4.p2.3.m3.1.1.2.3.2.cmml">c</mi><mo id="S3.SS4.p2.3.m3.1.1.2.3.1" xref="S3.SS4.p2.3.m3.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS4.p2.3.m3.1.1.2.3.3" xref="S3.SS4.p2.3.m3.1.1.2.3.3.cmml">a</mi><mo id="S3.SS4.p2.3.m3.1.1.2.3.1a" xref="S3.SS4.p2.3.m3.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS4.p2.3.m3.1.1.2.3.4" xref="S3.SS4.p2.3.m3.1.1.2.3.4.cmml">m</mi></mrow></msub><mo id="S3.SS4.p2.3.m3.1.1.1" xref="S3.SS4.p2.3.m3.1.1.1.cmml">∈</mo><msup id="S3.SS4.p2.3.m3.1.1.3" xref="S3.SS4.p2.3.m3.1.1.3.cmml"><mi id="S3.SS4.p2.3.m3.1.1.3.2" xref="S3.SS4.p2.3.m3.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p2.3.m3.1.1.3.3" xref="S3.SS4.p2.3.m3.1.1.3.3.cmml"><mi id="S3.SS4.p2.3.m3.1.1.3.3.2" xref="S3.SS4.p2.3.m3.1.1.3.3.2.cmml">N</mi><mo id="S3.SS4.p2.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p2.3.m3.1.1.3.3.3" xref="S3.SS4.p2.3.m3.1.1.3.3.3.cmml">n</mi><mo id="S3.SS4.p2.3.m3.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p2.3.m3.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p2.3.m3.1.1.3.3.4" xref="S3.SS4.p2.3.m3.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><apply id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1"><in id="S3.SS4.p2.3.m3.1.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1.1"></in><apply id="S3.SS4.p2.3.m3.1.1.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.2.1.cmml" xref="S3.SS4.p2.3.m3.1.1.2">subscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.2.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2.2">𝐎</ci><apply id="S3.SS4.p2.3.m3.1.1.2.3.cmml" xref="S3.SS4.p2.3.m3.1.1.2.3"><times id="S3.SS4.p2.3.m3.1.1.2.3.1.cmml" xref="S3.SS4.p2.3.m3.1.1.2.3.1"></times><ci id="S3.SS4.p2.3.m3.1.1.2.3.2.cmml" xref="S3.SS4.p2.3.m3.1.1.2.3.2">𝑐</ci><ci id="S3.SS4.p2.3.m3.1.1.2.3.3.cmml" xref="S3.SS4.p2.3.m3.1.1.2.3.3">𝑎</ci><ci id="S3.SS4.p2.3.m3.1.1.2.3.4.cmml" xref="S3.SS4.p2.3.m3.1.1.2.3.4">𝑚</ci></apply></apply><apply id="S3.SS4.p2.3.m3.1.1.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p2.3.m3.1.1.3.1.cmml" xref="S3.SS4.p2.3.m3.1.1.3">superscript</csymbol><ci id="S3.SS4.p2.3.m3.1.1.3.2.cmml" xref="S3.SS4.p2.3.m3.1.1.3.2">ℝ</ci><apply id="S3.SS4.p2.3.m3.1.1.3.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3"><times id="S3.SS4.p2.3.m3.1.1.3.3.1.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3.1"></times><ci id="S3.SS4.p2.3.m3.1.1.3.3.2.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3.2">𝑁</ci><ci id="S3.SS4.p2.3.m3.1.1.3.3.3.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3.3">𝑛</ci><ci id="S3.SS4.p2.3.m3.1.1.3.3.4.cmml" xref="S3.SS4.p2.3.m3.1.1.3.3.4">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">\mathbf{O}_{cam}\in\mathbb{R}^{N\times n\times C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.3.m3.1d">bold_O start_POSTSUBSCRIPT italic_c italic_a italic_m end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_n × italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>. Subsequently, a top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS4.p2.4.m4.1"><semantics id="S3.SS4.p2.4.m4.1a"><mi id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><ci id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.4.m4.1d">italic_k</annotation></semantics></math> strategy is employed to identify the concepts that contribute the most, denoted <math alttext="\mathbf{\hat{O}}_{cam}\in\mathbb{R}^{N\times k\times C}" class="ltx_Math" display="inline" id="S3.SS4.p2.5.m5.1"><semantics id="S3.SS4.p2.5.m5.1a"><mrow id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml"><msub id="S3.SS4.p2.5.m5.1.1.2" xref="S3.SS4.p2.5.m5.1.1.2.cmml"><mover accent="true" id="S3.SS4.p2.5.m5.1.1.2.2" xref="S3.SS4.p2.5.m5.1.1.2.2.cmml"><mi id="S3.SS4.p2.5.m5.1.1.2.2.2" xref="S3.SS4.p2.5.m5.1.1.2.2.2.cmml">𝐎</mi><mo id="S3.SS4.p2.5.m5.1.1.2.2.1" xref="S3.SS4.p2.5.m5.1.1.2.2.1.cmml">^</mo></mover><mrow id="S3.SS4.p2.5.m5.1.1.2.3" xref="S3.SS4.p2.5.m5.1.1.2.3.cmml"><mi id="S3.SS4.p2.5.m5.1.1.2.3.2" xref="S3.SS4.p2.5.m5.1.1.2.3.2.cmml">c</mi><mo id="S3.SS4.p2.5.m5.1.1.2.3.1" xref="S3.SS4.p2.5.m5.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS4.p2.5.m5.1.1.2.3.3" xref="S3.SS4.p2.5.m5.1.1.2.3.3.cmml">a</mi><mo id="S3.SS4.p2.5.m5.1.1.2.3.1a" xref="S3.SS4.p2.5.m5.1.1.2.3.1.cmml">⁢</mo><mi id="S3.SS4.p2.5.m5.1.1.2.3.4" xref="S3.SS4.p2.5.m5.1.1.2.3.4.cmml">m</mi></mrow></msub><mo id="S3.SS4.p2.5.m5.1.1.1" xref="S3.SS4.p2.5.m5.1.1.1.cmml">∈</mo><msup id="S3.SS4.p2.5.m5.1.1.3" xref="S3.SS4.p2.5.m5.1.1.3.cmml"><mi id="S3.SS4.p2.5.m5.1.1.3.2" xref="S3.SS4.p2.5.m5.1.1.3.2.cmml">ℝ</mi><mrow id="S3.SS4.p2.5.m5.1.1.3.3" xref="S3.SS4.p2.5.m5.1.1.3.3.cmml"><mi id="S3.SS4.p2.5.m5.1.1.3.3.2" xref="S3.SS4.p2.5.m5.1.1.3.3.2.cmml">N</mi><mo id="S3.SS4.p2.5.m5.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p2.5.m5.1.1.3.3.3" xref="S3.SS4.p2.5.m5.1.1.3.3.3.cmml">k</mi><mo id="S3.SS4.p2.5.m5.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="S3.SS4.p2.5.m5.1.1.3.3.1.cmml">×</mo><mi id="S3.SS4.p2.5.m5.1.1.3.3.4" xref="S3.SS4.p2.5.m5.1.1.3.3.4.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b"><apply id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1"><in id="S3.SS4.p2.5.m5.1.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1.1"></in><apply id="S3.SS4.p2.5.m5.1.1.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.2.1.cmml" xref="S3.SS4.p2.5.m5.1.1.2">subscript</csymbol><apply id="S3.SS4.p2.5.m5.1.1.2.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2.2"><ci id="S3.SS4.p2.5.m5.1.1.2.2.1.cmml" xref="S3.SS4.p2.5.m5.1.1.2.2.1">^</ci><ci id="S3.SS4.p2.5.m5.1.1.2.2.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2.2.2">𝐎</ci></apply><apply id="S3.SS4.p2.5.m5.1.1.2.3.cmml" xref="S3.SS4.p2.5.m5.1.1.2.3"><times id="S3.SS4.p2.5.m5.1.1.2.3.1.cmml" xref="S3.SS4.p2.5.m5.1.1.2.3.1"></times><ci id="S3.SS4.p2.5.m5.1.1.2.3.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2.3.2">𝑐</ci><ci id="S3.SS4.p2.5.m5.1.1.2.3.3.cmml" xref="S3.SS4.p2.5.m5.1.1.2.3.3">𝑎</ci><ci id="S3.SS4.p2.5.m5.1.1.2.3.4.cmml" xref="S3.SS4.p2.5.m5.1.1.2.3.4">𝑚</ci></apply></apply><apply id="S3.SS4.p2.5.m5.1.1.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3"><csymbol cd="ambiguous" id="S3.SS4.p2.5.m5.1.1.3.1.cmml" xref="S3.SS4.p2.5.m5.1.1.3">superscript</csymbol><ci id="S3.SS4.p2.5.m5.1.1.3.2.cmml" xref="S3.SS4.p2.5.m5.1.1.3.2">ℝ</ci><apply id="S3.SS4.p2.5.m5.1.1.3.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3"><times id="S3.SS4.p2.5.m5.1.1.3.3.1.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.1"></times><ci id="S3.SS4.p2.5.m5.1.1.3.3.2.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.2">𝑁</ci><ci id="S3.SS4.p2.5.m5.1.1.3.3.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.3">𝑘</ci><ci id="S3.SS4.p2.5.m5.1.1.3.3.4.cmml" xref="S3.SS4.p2.5.m5.1.1.3.3.4">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">\mathbf{\hat{O}}_{cam}\in\mathbb{R}^{N\times k\times C}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.5.m5.1d">over^ start_ARG bold_O end_ARG start_POSTSUBSCRIPT italic_c italic_a italic_m end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_k × italic_C end_POSTSUPERSCRIPT</annotation></semantics></math>. The final prediction is based on the average value of these top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS4.p2.6.m6.1"><semantics id="S3.SS4.p2.6.m6.1a"><mi id="S3.SS4.p2.6.m6.1.1" xref="S3.SS4.p2.6.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.6.m6.1b"><ci id="S3.SS4.p2.6.m6.1.1.cmml" xref="S3.SS4.p2.6.m6.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.6.m6.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.6.m6.1d">italic_k</annotation></semantics></math> concepts, as given by:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{O}_{\mathrm{pred}}=\mathcal{F}_{\mathrm{AVG}}(\mathbf{\hat{O}}_{%
\mathrm{cam}})," class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><mrow id="S3.E2.m1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.3.2.cmml">𝐎</mi><mi id="S3.E2.m1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.3.3.cmml">pred</mi></msub><mo id="S3.E2.m1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.2.cmml">=</mo><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.3.2.cmml">ℱ</mi><mi id="S3.E2.m1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.3.3.cmml">AVG</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml">𝐎</mi><mo id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">cam</mi></msub><mo id="S3.E2.m1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><eq id="S3.E2.m1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.2"></eq><apply id="S3.E2.m1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.3.2">𝐎</ci><ci id="S3.E2.m1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.3.3">pred</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><times id="S3.E2.m1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.2"></times><apply id="S3.E2.m1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.3.2">ℱ</ci><ci id="S3.E2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.3.3">AVG</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2"><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.1">^</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.2">𝐎</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">cam</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathbf{O}_{\mathrm{pred}}=\mathcal{F}_{\mathrm{AVG}}(\mathbf{\hat{O}}_{%
\mathrm{cam}}),</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">bold_O start_POSTSUBSCRIPT roman_pred end_POSTSUBSCRIPT = caligraphic_F start_POSTSUBSCRIPT roman_AVG end_POSTSUBSCRIPT ( over^ start_ARG bold_O end_ARG start_POSTSUBSCRIPT roman_cam end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p2.8">where <math alttext="\mathcal{F}_{\mathrm{AVG}}(\cdot)" class="ltx_Math" display="inline" id="S3.SS4.p2.7.m1.1"><semantics id="S3.SS4.p2.7.m1.1a"><mrow id="S3.SS4.p2.7.m1.1.2" xref="S3.SS4.p2.7.m1.1.2.cmml"><msub id="S3.SS4.p2.7.m1.1.2.2" xref="S3.SS4.p2.7.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.7.m1.1.2.2.2" xref="S3.SS4.p2.7.m1.1.2.2.2.cmml">ℱ</mi><mi id="S3.SS4.p2.7.m1.1.2.2.3" xref="S3.SS4.p2.7.m1.1.2.2.3.cmml">AVG</mi></msub><mo id="S3.SS4.p2.7.m1.1.2.1" xref="S3.SS4.p2.7.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS4.p2.7.m1.1.2.3.2" xref="S3.SS4.p2.7.m1.1.2.cmml"><mo id="S3.SS4.p2.7.m1.1.2.3.2.1" stretchy="false" xref="S3.SS4.p2.7.m1.1.2.cmml">(</mo><mo id="S3.SS4.p2.7.m1.1.1" lspace="0em" rspace="0em" xref="S3.SS4.p2.7.m1.1.1.cmml">⋅</mo><mo id="S3.SS4.p2.7.m1.1.2.3.2.2" stretchy="false" xref="S3.SS4.p2.7.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.7.m1.1b"><apply id="S3.SS4.p2.7.m1.1.2.cmml" xref="S3.SS4.p2.7.m1.1.2"><times id="S3.SS4.p2.7.m1.1.2.1.cmml" xref="S3.SS4.p2.7.m1.1.2.1"></times><apply id="S3.SS4.p2.7.m1.1.2.2.cmml" xref="S3.SS4.p2.7.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.7.m1.1.2.2.1.cmml" xref="S3.SS4.p2.7.m1.1.2.2">subscript</csymbol><ci id="S3.SS4.p2.7.m1.1.2.2.2.cmml" xref="S3.SS4.p2.7.m1.1.2.2.2">ℱ</ci><ci id="S3.SS4.p2.7.m1.1.2.2.3.cmml" xref="S3.SS4.p2.7.m1.1.2.2.3">AVG</ci></apply><ci id="S3.SS4.p2.7.m1.1.1.cmml" xref="S3.SS4.p2.7.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.7.m1.1c">\mathcal{F}_{\mathrm{AVG}}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.7.m1.1d">caligraphic_F start_POSTSUBSCRIPT roman_AVG end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> represents the average pooling function along the dimension <math alttext="k" class="ltx_Math" display="inline" id="S3.SS4.p2.8.m2.1"><semantics id="S3.SS4.p2.8.m2.1a"><mi id="S3.SS4.p2.8.m2.1.1" xref="S3.SS4.p2.8.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.8.m2.1b"><ci id="S3.SS4.p2.8.m2.1.1.cmml" xref="S3.SS4.p2.8.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.8.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.8.m2.1d">italic_k</annotation></semantics></math>. The final loss is formulated as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\mathcal{L}_{\mathrm{MIL}}(\mathbf{O}_{\mathrm{pred}},\mathbf{y}_{%
i})," class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.4" xref="S3.E3.m1.1.1.1.1.4.cmml">ℒ</mi><mo id="S3.E3.m1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.3.cmml">=</mo><mrow id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml"><msub id="S3.E3.m1.1.1.1.1.2.4" xref="S3.E3.m1.1.1.1.1.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E3.m1.1.1.1.1.2.4.2" xref="S3.E3.m1.1.1.1.1.2.4.2.cmml">ℒ</mi><mi id="S3.E3.m1.1.1.1.1.2.4.3" xref="S3.E3.m1.1.1.1.1.2.4.3.cmml">MIL</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S3.E3.m1.1.1.1.1.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml"><mo id="S3.E3.m1.1.1.1.1.2.2.2.3" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">(</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml">𝐎</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml">pred</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.2.2.4" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">,</mo><msub id="S3.E3.m1.1.1.1.1.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.cmml"><mi id="S3.E3.m1.1.1.1.1.2.2.2.2.2" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml">𝐲</mi><mi id="S3.E3.m1.1.1.1.1.2.2.2.2.3" xref="S3.E3.m1.1.1.1.1.2.2.2.2.3.cmml">i</mi></msub><mo id="S3.E3.m1.1.1.1.1.2.2.2.5" stretchy="false" xref="S3.E3.m1.1.1.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="S3.E3.m1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><eq id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3"></eq><ci id="S3.E3.m1.1.1.1.1.4.cmml" xref="S3.E3.m1.1.1.1.1.4">ℒ</ci><apply id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"><times id="S3.E3.m1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.3"></times><apply id="S3.E3.m1.1.1.1.1.2.4.cmml" xref="S3.E3.m1.1.1.1.1.2.4"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.4.1.cmml" xref="S3.E3.m1.1.1.1.1.2.4">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.4.2.cmml" xref="S3.E3.m1.1.1.1.1.2.4.2">ℒ</ci><ci id="S3.E3.m1.1.1.1.1.2.4.3.cmml" xref="S3.E3.m1.1.1.1.1.2.4.3">MIL</ci></apply><interval closure="open" id="S3.E3.m1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2">𝐎</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">pred</ci></apply><apply id="S3.E3.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.2.2.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.2">𝐲</ci><ci id="S3.E3.m1.1.1.1.1.2.2.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.2.2.2.2.3">𝑖</ci></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\mathcal{L}=\mathcal{L}_{\mathrm{MIL}}(\mathbf{O}_{\mathrm{pred}},\mathbf{y}_{%
i}),</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">caligraphic_L = caligraphic_L start_POSTSUBSCRIPT roman_MIL end_POSTSUBSCRIPT ( bold_O start_POSTSUBSCRIPT roman_pred end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS4.p2.9">where <math alttext="\mathcal{L}_{\mathrm{MIL}}(\cdot)" class="ltx_Math" display="inline" id="S3.SS4.p2.9.m1.1"><semantics id="S3.SS4.p2.9.m1.1a"><mrow id="S3.SS4.p2.9.m1.1.2" xref="S3.SS4.p2.9.m1.1.2.cmml"><msub id="S3.SS4.p2.9.m1.1.2.2" xref="S3.SS4.p2.9.m1.1.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS4.p2.9.m1.1.2.2.2" xref="S3.SS4.p2.9.m1.1.2.2.2.cmml">ℒ</mi><mi id="S3.SS4.p2.9.m1.1.2.2.3" xref="S3.SS4.p2.9.m1.1.2.2.3.cmml">MIL</mi></msub><mo id="S3.SS4.p2.9.m1.1.2.1" xref="S3.SS4.p2.9.m1.1.2.1.cmml">⁢</mo><mrow id="S3.SS4.p2.9.m1.1.2.3.2" xref="S3.SS4.p2.9.m1.1.2.cmml"><mo id="S3.SS4.p2.9.m1.1.2.3.2.1" stretchy="false" xref="S3.SS4.p2.9.m1.1.2.cmml">(</mo><mo id="S3.SS4.p2.9.m1.1.1" lspace="0em" rspace="0em" xref="S3.SS4.p2.9.m1.1.1.cmml">⋅</mo><mo id="S3.SS4.p2.9.m1.1.2.3.2.2" stretchy="false" xref="S3.SS4.p2.9.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.9.m1.1b"><apply id="S3.SS4.p2.9.m1.1.2.cmml" xref="S3.SS4.p2.9.m1.1.2"><times id="S3.SS4.p2.9.m1.1.2.1.cmml" xref="S3.SS4.p2.9.m1.1.2.1"></times><apply id="S3.SS4.p2.9.m1.1.2.2.cmml" xref="S3.SS4.p2.9.m1.1.2.2"><csymbol cd="ambiguous" id="S3.SS4.p2.9.m1.1.2.2.1.cmml" xref="S3.SS4.p2.9.m1.1.2.2">subscript</csymbol><ci id="S3.SS4.p2.9.m1.1.2.2.2.cmml" xref="S3.SS4.p2.9.m1.1.2.2.2">ℒ</ci><ci id="S3.SS4.p2.9.m1.1.2.2.3.cmml" xref="S3.SS4.p2.9.m1.1.2.2.3">MIL</ci></apply><ci id="S3.SS4.p2.9.m1.1.1.cmml" xref="S3.SS4.p2.9.m1.1.1">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.9.m1.1c">\mathcal{L}_{\mathrm{MIL}}(\cdot)</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p2.9.m1.1d">caligraphic_L start_POSTSUBSCRIPT roman_MIL end_POSTSUBSCRIPT ( ⋅ )</annotation></semantics></math> denotes the multi-instance learning loss function. This approach allows us to effectively utilize the weakly supervised signals to identify and focus on the most critical parts of the input image, thereby enhancing the accuracy and reliability of the final medical diagnosis.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.3">Equation (<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S3.E2" title="Equation 2 ‣ 3.4 Interpretable Skin Diagnosis ‣ 3 Methods ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">2</span></a>) illustrates that the final decision is based on the average value of the top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS4.p3.1.m1.1"><semantics id="S3.SS4.p3.1.m1.1a"><mi id="S3.SS4.p3.1.m1.1.1" xref="S3.SS4.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.1.m1.1b"><ci id="S3.SS4.p3.1.m1.1.1.cmml" xref="S3.SS4.p3.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.1.m1.1d">italic_k</annotation></semantics></math> visual concepts, indicating that these <math alttext="k" class="ltx_Math" display="inline" id="S3.SS4.p3.2.m2.1"><semantics id="S3.SS4.p3.2.m2.1a"><mi id="S3.SS4.p3.2.m2.1.1" xref="S3.SS4.p3.2.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.2.m2.1b"><ci id="S3.SS4.p3.2.m2.1.1.cmml" xref="S3.SS4.p3.2.m2.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.2.m2.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.2.m2.1d">italic_k</annotation></semantics></math> concepts provide the most significant contributions. However, in the context of a skin disease diagnosis system, it is common to search for the final interpretation with one most important local visual concept. As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.F6" title="Figure 6 ‣ 4.3 Comparison Results ‣ 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">6</span></a>, our framework first determines the most probable skin disease from top-<math alttext="k" class="ltx_Math" display="inline" id="S3.SS4.p3.3.m3.1"><semantics id="S3.SS4.p3.3.m3.1a"><mi id="S3.SS4.p3.3.m3.1.1" xref="S3.SS4.p3.3.m3.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p3.3.m3.1b"><ci id="S3.SS4.p3.3.m3.1.1.cmml" xref="S3.SS4.p3.3.m3.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p3.3.m3.1c">k</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p3.3.m3.1d">italic_k</annotation></semantics></math> concepts. Subsequently, the single concept that contributes the most is used as the basis for interpreting our model’s decision. This ensures that the diagnosis is not only accurate but also interpretable, allowing medical professionals to understand which specific feature the model deems most indicative of the disease.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.2.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S3.T2.3.2" style="font-size:90%;">Comparison of different methods on the MIND-the-SKIN dataset.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.4" style="width:464.7pt;height:594pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(77.4pt,-99.0pt) scale(1.5,1.5) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.4.1">
<tr class="ltx_tr" id="S3.T2.4.1.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.1.1" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span><span class="ltx_text" id="S3.T2.4.1.1.1.1" style="font-size:70%;">
</span><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.1.2" style="font-size:70%;">Split Ratio</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.1.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.2.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.1.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.3.1" style="font-size:70%;">Precision</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.1.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.4.1" style="font-size:70%;">Recall</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.1.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.5.1" style="font-size:70%;">F1</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.1.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.1.6.1" style="font-size:70%;">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.2.1" rowspan="4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.2.1.1" style="font-size:70%;">train/total = 0.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.2.2" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.2.2.1" style="font-size:70%;">ResNet</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.2.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib17" title="">17</a><span class="ltx_text" id="S3.T2.4.1.2.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.2.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.2.3.1" style="font-size:70%;">0.381</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.2.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.2.4.1" style="font-size:70%;">0.324</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.2.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.2.5.1" style="font-size:70%;">0.350</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.2.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.2.6.1" style="font-size:70%;">0.535</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.3.1" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.3.1.1" style="font-size:70%;">ViT</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib8" title="">8</a><span class="ltx_text" id="S3.T2.4.1.3.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.3.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.3.2.1" style="font-size:70%;">0.393</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.3.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.3.3.1" style="font-size:70%;">0.364</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.3.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.3.4.1" style="font-size:70%;">0.378</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.3.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.3.5.1" style="font-size:70%;">0.578</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.4.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.4.1.1" style="font-size:70%;">Baseline(global-only)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.4.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.4.2.1" style="font-size:70%;">0.313</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.4.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.4.3.1" style="font-size:70%;">0.330</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.4.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.4.4.1" style="font-size:70%;">0.321</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.4.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.4.5.1" style="font-size:70%;">0.539</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.5.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.5.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.5.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.5.2.1" style="font-size:70%;">0.375</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.5.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.5.3.1" style="font-size:70%;">0.381</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.5.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.5.4.1" style="font-size:70%;">0.378</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.5.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.5.5.1" style="font-size:70%;">0.579</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.6.1" rowspan="4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.6.1.1" style="font-size:70%;">train/total = 0.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.6.2" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.6.2.1" style="font-size:70%;">ResNet</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.6.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib17" title="">17</a><span class="ltx_text" id="S3.T2.4.1.6.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.6.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.6.3.1" style="font-size:70%;">0.575</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.6.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.6.4.1" style="font-size:70%;">0.538</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.6.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.6.5.1" style="font-size:70%;">0.556</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.6.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.6.6.1" style="font-size:70%;">0.721</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.7.1" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.7.1.1" style="font-size:70%;">ViT</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.7.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib8" title="">8</a><span class="ltx_text" id="S3.T2.4.1.7.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.7.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.7.2.1" style="font-size:70%;">0.603</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.7.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.7.3.1" style="font-size:70%;">0.536</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.7.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.7.4.1" style="font-size:70%;">0.567</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.7.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.7.5.1" style="font-size:70%;">0.746</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.8.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.8.1.1" style="font-size:70%;">Baseline (global-only)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.8.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.8.2.1" style="font-size:70%;">0.606</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.8.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.8.3.1" style="font-size:70%;">0.495</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.8.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.8.4.1" style="font-size:70%;">0.545</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.8.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.8.5.1" style="font-size:70%;">0.743</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.9.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.9.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.9.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.9.2.1" style="font-size:70%;">0.643</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.9.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.9.3.1" style="font-size:70%;">0.509</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.9.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.9.4.1" style="font-size:70%;">0.568</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.9.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.9.5.1" style="font-size:70%;">0.750</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.10.1" rowspan="4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.10.1.1" style="font-size:70%;">train/total = 0.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.10.2" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.10.2.1" style="font-size:70%;">ResNet</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.10.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib17" title="">17</a><span class="ltx_text" id="S3.T2.4.1.10.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.10.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.10.3.1" style="font-size:70%;">0.623</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.10.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.10.4.1" style="font-size:70%;">0.547</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.10.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.10.5.1" style="font-size:70%;">0.582</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.10.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.10.6.1" style="font-size:70%;">0.777</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.11.1" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.11.1.1" style="font-size:70%;">ViT</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.11.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib8" title="">8</a><span class="ltx_text" id="S3.T2.4.1.11.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.11.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.11.2.1" style="font-size:70%;">0.554</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.11.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.11.3.1" style="font-size:70%;">0.567</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.11.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.11.4.1" style="font-size:70%;">0.560</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.11.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.11.5.1" style="font-size:70%;">0.760</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.12.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.12.1.1" style="font-size:70%;">Baseline (global-only)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.12.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.12.2.1" style="font-size:70%;">0.550</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.12.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.12.3.1" style="font-size:70%;">0.565</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.12.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.12.4.1" style="font-size:70%;">0.557</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.12.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.12.5.1" style="font-size:70%;">0.764</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.13.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.13.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.13.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.13.2.1" style="font-size:70%;">0.651</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.13.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.13.3.1" style="font-size:70%;">0.631</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.13.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.13.4.1" style="font-size:70%;">0.641</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.13.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.13.5.1" style="font-size:70%;">0.809</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.14.1" rowspan="4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.14.1.1" style="font-size:70%;">train/total = 0.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.14.2" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.14.2.1" style="font-size:70%;">ResNet</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.14.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib17" title="">17</a><span class="ltx_text" id="S3.T2.4.1.14.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.14.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.14.3.1" style="font-size:70%;">0.740</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.14.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.14.4.1" style="font-size:70%;">0.588</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.14.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.14.5.1" style="font-size:70%;">0.655</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.14.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.14.6.1" style="font-size:70%;">0.798</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.15">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.15.1" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.15.1.1" style="font-size:70%;">ViT</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.15.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib8" title="">8</a><span class="ltx_text" id="S3.T2.4.1.15.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.15.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.15.2.1" style="font-size:70%;">0.756</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.15.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.15.3.1" style="font-size:70%;">0.663</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.15.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.15.4.1" style="font-size:70%;">0.706</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.15.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.15.5.1" style="font-size:70%;">0.789</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.16.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.16.1.1" style="font-size:70%;">Baseline (global-only)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.16.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.16.2.1" style="font-size:70%;">0.738</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.16.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.16.3.1" style="font-size:70%;">0.671</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.16.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.16.4.1" style="font-size:70%;">0.702</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.16.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.16.5.1" style="font-size:70%;">0.816</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.17.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.17.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.17.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.17.2.1" style="font-size:70%;">0.805</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.17.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.17.3.1" style="font-size:70%;">0.768</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.17.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.17.4.1" style="font-size:70%;">0.786</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.17.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.17.5.1" style="font-size:70%;">0.847</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.18">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.18.1" rowspan="4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.18.1.1" style="font-size:70%;">train/total = 0.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.4.1.18.2" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.18.2.1" style="font-size:70%;">ResNet</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.18.2.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib17" title="">17</a><span class="ltx_text" id="S3.T2.4.1.18.2.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.18.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.18.3.1" style="font-size:70%;">0.739</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.18.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.18.4.1" style="font-size:70%;">0.732</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.18.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.18.5.1" style="font-size:70%;">0.735</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.4.1.18.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.18.6.1" style="font-size:70%;">0.829</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.19.1" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_text" id="S3.T2.4.1.19.1.1" style="font-size:70%;">ViT</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.T2.4.1.19.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib8" title="">8</a><span class="ltx_text" id="S3.T2.4.1.19.1.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.19.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.19.2.1" style="font-size:70%;">0.824</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.19.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.19.3.1" style="font-size:70%;">0.775</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.19.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.19.4.1" style="font-size:70%;">0.799</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.19.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.19.5.1" style="font-size:70%;">0.853</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.20.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.20.1.1" style="font-size:70%;">Baseline (global-only)</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.20.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.20.2.1" style="font-size:70%;">0.766</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.20.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.20.3.1" style="font-size:70%;">0.682</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.20.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.20.4.1" style="font-size:70%;">0.722</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.20.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.20.5.1" style="font-size:70%;">0.842</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.21">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.21.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S3.T2.4.1.21.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.21.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.21.2.1" style="font-size:70%;">0.841</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.21.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.21.3.1" style="font-size:70%;">0.798</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.21.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.21.4.1" style="font-size:70%;">0.819</span></td>
<td class="ltx_td ltx_align_center" id="S3.T2.4.1.21.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.4.1.21.5.1" style="font-size:70%;">0.867</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.4.1.22">
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.4.1.22.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td" id="S3.T2.4.1.22.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
<td class="ltx_td" id="S3.T2.4.1.22.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
<td class="ltx_td" id="S3.T2.4.1.22.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
<td class="ltx_td" id="S3.T2.4.1.22.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
<td class="ltx_td" id="S3.T2.4.1.22.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
</tr>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="252" id="S3.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.7.2.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.2.1" style="font-size:90%;"> Confusion matrix with different top-<math alttext="k" class="ltx_Math" display="inline" id="S3.F3.2.1.m1.1"><semantics id="S3.F3.2.1.m1.1b"><mi id="S3.F3.2.1.m1.1.1" xref="S3.F3.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.F3.2.1.m1.1c"><ci id="S3.F3.2.1.m1.1.1.cmml" xref="S3.F3.2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.2.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S3.F3.2.1.m1.1e">italic_k</annotation></semantics></math> strategies on the MIND-the-SKIN dataset. <span class="ltx_text ltx_font_bold" id="S3.F3.2.1.1">(a)</span> is the confusion matrix of top-1; <span class="ltx_text ltx_font_bold" id="S3.F3.2.1.2">(b)</span> shows the confusion matrix of top-5; <span class="ltx_text ltx_font_bold" id="S3.F3.2.1.3">(c)</span> represents the confusion matrix of top-15.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Datasets</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">We evaluated our methods using two datasets: MIND-the-SKIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib1" title="">1</a>]</cite> and SCIN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib33" title="">33</a>]</cite>. The MIND-the-SKIN project aims to address challenges in Neglected Tropical Diseases (NTDs), a diverse group of skin conditions prevalent in impoverished tropical communities, affecting over 1 billion people. A crucial aspect of the project involves data collection in rural West Africa using portable devices and the development of AI-based diagnostic tools. For our evaluation, we utilized a subset of this dataset comprising 1,731 clinical photos representing five common NTD conditions: leprosy, Buruli ulcers, yaws, scabies, and mycetoma.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Existing skin disease datasets featuring clinical photos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib11" title="">11</a>]</cite> are pre-processed to center at lesions and have reduced background noise. In contrast, the SCIN dataset retains its original noise and more closely mirrors real-world inputs, making it a valuable resource for our investigation. The SCIN dataset was collected through a voluntary image donation platform from Google Search users in the United States, with each case including up to three images, all diagnosed by up to three dermatologists. This process results in a weighted skin condition label for each case. To ensure label accuracy, we selected the condition with the highest weight as the final label, excluding ambiguous cases where multiple conditions had equal probabilities. Additionally, to facilitate reliable evaluation and maintain consistency with the NTD dataset in terms of the number of conditions, we focused on the five largest classes. In our experiments, we used a random split by cases for both datasets to prevent data leakage. Dataset details and class distributions are provided in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S3.T1" title="Table 1 ‣ 3.3 Cross-Attentive Fusion Model ‣ 3 Methods ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Implementation Details</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.2">We use the pretrained ViT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib8" title="">8</a>]</cite> as the backbone to extract global features from each input image. The global feature extracted by ViT, combined with our top-<math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p1.1.m1.1"><semantics id="S4.SS2.p1.1.m1.1a"><mi id="S4.SS2.p1.1.m1.1.1" xref="S4.SS2.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.1.m1.1b"><ci id="S4.SS2.p1.1.m1.1.1.cmml" xref="S4.SS2.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.1.m1.1d">italic_k</annotation></semantics></math> mechanism, serves as the baseline for comparison. ViT produces a 768-dimensional feature map with a resolution of 14 <math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.p1.2.m2.1"><semantics id="S4.SS2.p1.2.m2.1a"><mo id="S4.SS2.p1.2.m2.1.1" xref="S4.SS2.p1.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.p1.2.m2.1b"><times id="S4.SS2.p1.2.m2.1.1.cmml" xref="S4.SS2.p1.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p1.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p1.2.m2.1d">×</annotation></semantics></math> 14, which we pass through a linear layer to reduce the dimensionality to 256 for further refinement. For the local encoder, we extract visual concepts using our SAM variant, generating 256-dimensional vectors. These local features are refined with two convolutional layers, also outputting 256-dimensional vectors. To maintain consistent input, we fix the number of visual concepts at 30 for the NTD dataset and 25 for the SCIN dataset, based on the maximum number of captured concepts. For samples with fewer visual concepts, we use a random perturbation padding method to ensure consistency in input size. In the cross-attentive module, we implement a multi-head transformer with 8 heads, where each transformer module consists of 1 attention layer with a dimensionality of 256. The classification module includes 3 convolutional layers, with two dropout layers (dropout rate of 0.7) between them to regularize intermediate features.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">For hyperparameter choice, we train the whole model with a learning rate of 1e-4 using the Adam optimizer, and the batch size is set as 128 for training and 1 for testing. Considering the overfitting problem, we train all model parts simultaneously with 30 epochs. We choose <math alttext="k" class="ltx_Math" display="inline" id="S4.SS2.p2.1.m1.1"><semantics id="S4.SS2.p2.1.m1.1a"><mi id="S4.SS2.p2.1.m1.1.1" xref="S4.SS2.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p2.1.m1.1b"><ci id="S4.SS2.p2.1.m1.1.1.cmml" xref="S4.SS2.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.p2.1.m1.1d">italic_k</annotation></semantics></math> = 5 for the NTD dataset and 2 for the SCIN dataset. All experiments are conducted with one RTX 4090 GPU.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.2.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.3.2" style="font-size:90%;">Comparison of different methods on the SCIN dataset.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.4" style="width:431.1pt;height:328.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(73.7pt,-56.2pt) scale(1.52,1.52) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T3.4.1">
<tr class="ltx_tr" id="S4.T3.4.1.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.1.1.1" style="padding-top:0.3pt;padding-bottom:0.3pt;">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span><span class="ltx_text" id="S4.T3.4.1.1.1.1" style="font-size:70%;">
</span><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.1.2" style="font-size:70%;">Split Ratio</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.1.1.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.2.1" style="font-size:70%;">Method</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.1.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.3.1" style="font-size:70%;">Precision</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.1.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.4.1" style="font-size:70%;">Recall</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.1.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.5.1" style="font-size:70%;">F1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.1.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.6.1" style="font-size:70%;">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.2.1" rowspan="2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.2.1.1" style="font-size:70%;">train/total = 0.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.2.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.2.2.1" style="font-size:70%;">Baseline (global-only)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.2.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.2.3.1" style="font-size:70%;">0.196</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.2.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.2.4.1" style="font-size:70%;">0.204</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.2.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.2.5.1" style="font-size:70%;">0.200</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.2.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.2.6.1" style="font-size:70%;">0.385</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.1.3.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.3.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.3.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.3.2.1" style="font-size:70%;">0.233</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.3.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.3.3.1" style="font-size:70%;">0.209</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.3.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.3.4.1" style="font-size:70%;">0.220</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.3.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.3.5.1" style="font-size:70%;">0.393</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.4.1" rowspan="2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.4.1.1" style="font-size:70%;">train/total = 0.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.4.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.4.2.1" style="font-size:70%;">Baseline (global-only)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.4.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.4.3.1" style="font-size:70%;">0.070</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.4.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.4.4.1" style="font-size:70%;">0.199</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.4.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.4.5.1" style="font-size:70%;">0.104</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.4.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.4.6.1" style="font-size:70%;">0.394</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.1.5.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.5.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.5.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.5.2.1" style="font-size:70%;">0.190</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.5.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.5.3.1" style="font-size:70%;">0.221</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.5.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.5.4.1" style="font-size:70%;">0.204</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.5.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.5.5.1" style="font-size:70%;">0.395</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.6.1" rowspan="2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.6.1.1" style="font-size:70%;">train/total = 0.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.6.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.6.2.1" style="font-size:70%;">Baseline (global-only)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.6.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.6.3.1" style="font-size:70%;">0.145</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.6.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.6.4.1" style="font-size:70%;">0.209</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.6.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.6.5.1" style="font-size:70%;">0.171</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.6.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.6.6.1" style="font-size:70%;">0.395</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.1.7.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.7.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.7.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.7.2.1" style="font-size:70%;">0.155</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.7.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.7.3.1" style="font-size:70%;">0.208</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.7.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.7.4.1" style="font-size:70%;">0.177</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.7.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.7.5.1" style="font-size:70%;">0.397</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.8.1" rowspan="2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.8.1.1" style="font-size:70%;">train/total = 0.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.8.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.8.2.1" style="font-size:70%;">Baseline (global-only)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.8.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.8.3.1" style="font-size:70%;">0.207</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.8.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.8.4.1" style="font-size:70%;">0.249</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.8.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.8.5.1" style="font-size:70%;">0.226</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.8.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.8.6.1" style="font-size:70%;">0.403</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.1.9.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.9.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.9.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.9.2.1" style="font-size:70%;">0.275</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.9.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.9.3.1" style="font-size:70%;">0.255</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.9.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.9.4.1" style="font-size:70%;">0.265</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.9.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.9.5.1" style="font-size:70%;">0.394</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.10.1" rowspan="2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.10.1.1" style="font-size:70%;">train/total = 0.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.1.10.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.10.2.1" style="font-size:70%;">Baseline (global-only)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.10.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.10.3.1" style="font-size:70%;">0.325</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.10.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.10.4.1" style="font-size:70%;">0.307</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.10.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.10.5.1" style="font-size:70%;">0.316</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.1.10.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.10.6.1" style="font-size:70%;">0.436</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.11">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.1.11.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text" id="S4.T3.4.1.11.1.1" style="font-size:70%;">Ours</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.11.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.11.2.1" style="font-size:70%;">0.347</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.11.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.11.3.1" style="font-size:70%;">0.319</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.11.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.11.4.1" style="font-size:70%;">0.332</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.1.11.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.11.5.1" style="font-size:70%;">0.439</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.1.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.1.12.1" style="padding-top:0.3pt;padding-bottom:0.3pt;"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td" id="S4.T3.4.1.12.2" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
<td class="ltx_td" id="S4.T3.4.1.12.3" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
<td class="ltx_td" id="S4.T3.4.1.12.4" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
<td class="ltx_td" id="S4.T3.4.1.12.5" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
<td class="ltx_td" id="S4.T3.4.1.12.6" style="padding-top:0.3pt;padding-bottom:0.3pt;"></td>
</tr>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Comparison Results</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">We first tested our proposed method on the NTD dataset from the MIND-the-SKIN Project. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S3.T2" title="Table 2 ‣ 3.4 Interpretable Skin Diagnosis ‣ 3 Methods ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">2</span></a>, we conducted five random splits of cases into training and validation sets and compared our method with three other competitive methods. An immediate observation is that our method consistently outperforms the others in terms of classification accuracy for all splits, validating the effectiveness of leveraging both global and local features for skin lesion images. Notably, as the size of the training set increases, the margin of improvement gained by our method over the others also enlarges across the four metrics. This suggests that our method is more sensitive to new information than its competitors and can make full use of additional features to achieve superior performance. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S3.F3" title="Figure 3 ‣ 3.4 Interpretable Skin Diagnosis ‣ 3 Methods ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">3</span></a> reports the confusion matrix when we select different <math alttext="k" class="ltx_Math" display="inline" id="S4.SS3.p1.1.m1.1"><semantics id="S4.SS3.p1.1.m1.1a"><mi id="S4.SS3.p1.1.m1.1.1" xref="S4.SS3.p1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS3.p1.1.m1.1b"><ci id="S4.SS3.p1.1.m1.1.1.cmml" xref="S4.SS3.p1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS3.p1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS3.p1.1.m1.1d">italic_k</annotation></semantics></math> and train/total = 0.5, which aims to show the category-specific accuracy. Interestingly, we notice our model fails to recognize “Leprosy” and “Mycetoma”. The reason is that the lesion is not visually clear for those two diseases and they have the smallest sample sizes, thus easily misclassified. Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.F4" title="Figure 4 ‣ 4.3 Comparison Results ‣ 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">4</span></a> indicates the comparison of each category in the MIND-the-SKIN dataset. It is observed that though our fusion model shows a little performance degradation in the “Leprosy” disease, it shows an obvious boost in the “Scabies” disease. On average, our proposed fusion method achieves better performance than the baseline method.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="229" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.5.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.6.2" style="font-size:90%;">Performance comparison between baseline(global) and fusion(local + global) methods with the MIND-the-SKIN dataset. <span class="ltx_text ltx_font_bold" id="S4.F4.6.2.1">(a)</span> shows the confidence for each condition by the baseline method; <span class="ltx_text ltx_font_bold" id="S4.F4.6.2.2">(b)</span> the class-wise confidence by our fusion method; <span class="ltx_text ltx_font_bold" id="S4.F4.6.2.3">(c)</span> demonstrates one “Scabies” example that is wrongly recognized by the baseline method and correctly predicted by our proposed fusion model.</span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="321" id="S4.F5.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Visualization of data separability for the NTD (left) and SCIN (right) datasets after feature extraction by ViT.</span></figcaption>
</figure>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">Similarly, we performed five random splits for the SCIN dataset in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.T3" title="Table 3 ‣ 4.2 Implementation Details ‣ 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">3</span></a>, as no official splits are available. We observed a similar trend in the SCIN dataset, where our proposed method generally outperforms the baseline. However, we also observed that the overall performance on the SCIN dataset was suboptimal. To the best of our knowledge, no prior studies have experimented with the SCIN dataset, leaving us no references for comparison. One possible explanation for the suboptimal performance is the difficulty in distinguishing between the conditions represented in the dataset, as shown by the t-SNE plots after feature extraction using ViT (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.F5" title="Figure 5 ‣ 4.3 Comparison Results ‣ 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">5</span></a>). Additionally, the dataset does not provide definitive labels for each image, but rather a list of potential labels with associated confidence scores, leading to label noise. Thus, while SCIN is the best available public dataset for our experiments, it remains challenging to use effectively, and its optimal usage is still unclear.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="417" id="S4.F6.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="S4.F6.3.2" style="font-size:90%;">Qualitative results of skin disease diagnosis in the MIND-the-SKIN dataset. The first row represents one example from the “Buruli ulcer” disease; The second row demonstrates the example from “Yaws” disease. Both samples select the most contributed visual concepts and prove the effectiveness of our proposed method.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Interpretable Results</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.F6" title="Figure 6 ‣ 4.3 Comparison Results ‣ 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">6</span></a> shows examples of “Buruli ulcer” and “Yaws,” where the diseased areas are easily identifiable on the skin. The distribution of the row scores in the CAM indicates that both cases have a high probability of the correct disease and low confidence in others. The column score distribution demonstrates that our model focuses more on the lesion part rather than the whole body. The selected bounding box (third column of the figure) and segmentation mask (fourth column) provide professionals with an intuitive yet reliable explanation for the prediction. Note that we report the final prediction using the top-5 visual concepts, and we select the top-1 from these to represent the visual concept that contributes most to the prediction, considering that multiple concepts might confuse the medical diagnosis.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Ablation Study</h3>
<div class="ltx_para" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.6">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.T4" title="Table 4 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">4</span></a> indicates the performance comparison of different feature combinations on the MIND-the-SKIN dataset with train/total=0.5. The “Local-only” and “Baseline” denote the methods that only use local or global features individually for the final prediction. The “Concatenation-<math alttext="1" class="ltx_Math" display="inline" id="S4.SS5.p1.1.m1.1"><semantics id="S4.SS5.p1.1.m1.1a"><mn id="S4.SS5.p1.1.m1.1.1" xref="S4.SS5.p1.1.m1.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.1.m1.1b"><cn id="S4.SS5.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS5.p1.1.m1.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.1.m1.1c">1</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.1.m1.1d">1</annotation></semantics></math>” represents that we simply concatenate global feature to <math alttext="n" class="ltx_Math" display="inline" id="S4.SS5.p1.2.m2.1"><semantics id="S4.SS5.p1.2.m2.1a"><mi id="S4.SS5.p1.2.m2.1.1" xref="S4.SS5.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.2.m2.1b"><ci id="S4.SS5.p1.2.m2.1.1.cmml" xref="S4.SS5.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.2.m2.1d">italic_n</annotation></semantics></math> local features as <math alttext="n" class="ltx_Math" display="inline" id="S4.SS5.p1.3.m3.1"><semantics id="S4.SS5.p1.3.m3.1a"><mi id="S4.SS5.p1.3.m3.1.1" xref="S4.SS5.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.3.m3.1b"><ci id="S4.SS5.p1.3.m3.1.1.cmml" xref="S4.SS5.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.3.m3.1c">n</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.3.m3.1d">italic_n</annotation></semantics></math>+1 dimensional feature <math alttext="\mathbf{O^{\prime}}\in\mathbb{R}^{N\times(n+1)\times D}" class="ltx_Math" display="inline" id="S4.SS5.p1.4.m4.1"><semantics id="S4.SS5.p1.4.m4.1a"><mrow id="S4.SS5.p1.4.m4.1.2" xref="S4.SS5.p1.4.m4.1.2.cmml"><msup id="S4.SS5.p1.4.m4.1.2.2" xref="S4.SS5.p1.4.m4.1.2.2.cmml"><mi id="S4.SS5.p1.4.m4.1.2.2.2" xref="S4.SS5.p1.4.m4.1.2.2.2.cmml">𝐎</mi><mo id="S4.SS5.p1.4.m4.1.2.2.3" xref="S4.SS5.p1.4.m4.1.2.2.3.cmml">′</mo></msup><mo id="S4.SS5.p1.4.m4.1.2.1" xref="S4.SS5.p1.4.m4.1.2.1.cmml">∈</mo><msup id="S4.SS5.p1.4.m4.1.2.3" xref="S4.SS5.p1.4.m4.1.2.3.cmml"><mi id="S4.SS5.p1.4.m4.1.2.3.2" xref="S4.SS5.p1.4.m4.1.2.3.2.cmml">ℝ</mi><mrow id="S4.SS5.p1.4.m4.1.1.1" xref="S4.SS5.p1.4.m4.1.1.1.cmml"><mi id="S4.SS5.p1.4.m4.1.1.1.3" xref="S4.SS5.p1.4.m4.1.1.1.3.cmml">N</mi><mo id="S4.SS5.p1.4.m4.1.1.1.2" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.4.m4.1.1.1.2.cmml">×</mo><mrow id="S4.SS5.p1.4.m4.1.1.1.1.1" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.cmml"><mo id="S4.SS5.p1.4.m4.1.1.1.1.1.2" stretchy="false" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S4.SS5.p1.4.m4.1.1.1.1.1.1" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="S4.SS5.p1.4.m4.1.1.1.1.1.1.2" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.2.cmml">n</mi><mo id="S4.SS5.p1.4.m4.1.1.1.1.1.1.1" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.1.cmml">+</mo><mn id="S4.SS5.p1.4.m4.1.1.1.1.1.1.3" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="S4.SS5.p1.4.m4.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S4.SS5.p1.4.m4.1.1.1.2a" rspace="0.222em" xref="S4.SS5.p1.4.m4.1.1.1.2.cmml">×</mo><mi id="S4.SS5.p1.4.m4.1.1.1.4" xref="S4.SS5.p1.4.m4.1.1.1.4.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.4.m4.1b"><apply id="S4.SS5.p1.4.m4.1.2.cmml" xref="S4.SS5.p1.4.m4.1.2"><in id="S4.SS5.p1.4.m4.1.2.1.cmml" xref="S4.SS5.p1.4.m4.1.2.1"></in><apply id="S4.SS5.p1.4.m4.1.2.2.cmml" xref="S4.SS5.p1.4.m4.1.2.2"><csymbol cd="ambiguous" id="S4.SS5.p1.4.m4.1.2.2.1.cmml" xref="S4.SS5.p1.4.m4.1.2.2">superscript</csymbol><ci id="S4.SS5.p1.4.m4.1.2.2.2.cmml" xref="S4.SS5.p1.4.m4.1.2.2.2">𝐎</ci><ci id="S4.SS5.p1.4.m4.1.2.2.3.cmml" xref="S4.SS5.p1.4.m4.1.2.2.3">′</ci></apply><apply id="S4.SS5.p1.4.m4.1.2.3.cmml" xref="S4.SS5.p1.4.m4.1.2.3"><csymbol cd="ambiguous" id="S4.SS5.p1.4.m4.1.2.3.1.cmml" xref="S4.SS5.p1.4.m4.1.2.3">superscript</csymbol><ci id="S4.SS5.p1.4.m4.1.2.3.2.cmml" xref="S4.SS5.p1.4.m4.1.2.3.2">ℝ</ci><apply id="S4.SS5.p1.4.m4.1.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1.1"><times id="S4.SS5.p1.4.m4.1.1.1.2.cmml" xref="S4.SS5.p1.4.m4.1.1.1.2"></times><ci id="S4.SS5.p1.4.m4.1.1.1.3.cmml" xref="S4.SS5.p1.4.m4.1.1.1.3">𝑁</ci><apply id="S4.SS5.p1.4.m4.1.1.1.1.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1.1.1.1"><plus id="S4.SS5.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.1"></plus><ci id="S4.SS5.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.2">𝑛</ci><cn id="S4.SS5.p1.4.m4.1.1.1.1.1.1.3.cmml" type="integer" xref="S4.SS5.p1.4.m4.1.1.1.1.1.1.3">1</cn></apply><ci id="S4.SS5.p1.4.m4.1.1.1.4.cmml" xref="S4.SS5.p1.4.m4.1.1.1.4">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.4.m4.1c">\mathbf{O^{\prime}}\in\mathbb{R}^{N\times(n+1)\times D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.4.m4.1d">bold_O start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × ( italic_n + 1 ) × italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. “Concatenation-<math alttext="2" class="ltx_Math" display="inline" id="S4.SS5.p1.5.m5.1"><semantics id="S4.SS5.p1.5.m5.1a"><mn id="S4.SS5.p1.5.m5.1.1" xref="S4.SS5.p1.5.m5.1.1.cmml">2</mn><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.5.m5.1b"><cn id="S4.SS5.p1.5.m5.1.1.cmml" type="integer" xref="S4.SS5.p1.5.m5.1.1">2</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.5.m5.1c">2</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.5.m5.1d">2</annotation></semantics></math>” follows the feature fusion method in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#biba.bib35" title="">35</a>]</cite> to combine local and global features in feature space as <math alttext="\mathbf{O^{\prime\prime}}\in\mathbb{R}^{N\times n\times 2D}" class="ltx_Math" display="inline" id="S4.SS5.p1.6.m6.1"><semantics id="S4.SS5.p1.6.m6.1a"><mrow id="S4.SS5.p1.6.m6.1.1" xref="S4.SS5.p1.6.m6.1.1.cmml"><msup id="S4.SS5.p1.6.m6.1.1.2" xref="S4.SS5.p1.6.m6.1.1.2.cmml"><mi id="S4.SS5.p1.6.m6.1.1.2.2" xref="S4.SS5.p1.6.m6.1.1.2.2.cmml">𝐎</mi><mo id="S4.SS5.p1.6.m6.1.1.2.3" xref="S4.SS5.p1.6.m6.1.1.2.3.cmml">′′</mo></msup><mo id="S4.SS5.p1.6.m6.1.1.1" xref="S4.SS5.p1.6.m6.1.1.1.cmml">∈</mo><msup id="S4.SS5.p1.6.m6.1.1.3" xref="S4.SS5.p1.6.m6.1.1.3.cmml"><mi id="S4.SS5.p1.6.m6.1.1.3.2" xref="S4.SS5.p1.6.m6.1.1.3.2.cmml">ℝ</mi><mrow id="S4.SS5.p1.6.m6.1.1.3.3" xref="S4.SS5.p1.6.m6.1.1.3.3.cmml"><mrow id="S4.SS5.p1.6.m6.1.1.3.3.2" xref="S4.SS5.p1.6.m6.1.1.3.3.2.cmml"><mi id="S4.SS5.p1.6.m6.1.1.3.3.2.2" xref="S4.SS5.p1.6.m6.1.1.3.3.2.2.cmml">N</mi><mo id="S4.SS5.p1.6.m6.1.1.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.6.m6.1.1.3.3.2.1.cmml">×</mo><mi id="S4.SS5.p1.6.m6.1.1.3.3.2.3" xref="S4.SS5.p1.6.m6.1.1.3.3.2.3.cmml">n</mi><mo id="S4.SS5.p1.6.m6.1.1.3.3.2.1a" lspace="0.222em" rspace="0.222em" xref="S4.SS5.p1.6.m6.1.1.3.3.2.1.cmml">×</mo><mn id="S4.SS5.p1.6.m6.1.1.3.3.2.4" xref="S4.SS5.p1.6.m6.1.1.3.3.2.4.cmml">2</mn></mrow><mo id="S4.SS5.p1.6.m6.1.1.3.3.1" xref="S4.SS5.p1.6.m6.1.1.3.3.1.cmml">⁢</mo><mi id="S4.SS5.p1.6.m6.1.1.3.3.3" xref="S4.SS5.p1.6.m6.1.1.3.3.3.cmml">D</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p1.6.m6.1b"><apply id="S4.SS5.p1.6.m6.1.1.cmml" xref="S4.SS5.p1.6.m6.1.1"><in id="S4.SS5.p1.6.m6.1.1.1.cmml" xref="S4.SS5.p1.6.m6.1.1.1"></in><apply id="S4.SS5.p1.6.m6.1.1.2.cmml" xref="S4.SS5.p1.6.m6.1.1.2"><csymbol cd="ambiguous" id="S4.SS5.p1.6.m6.1.1.2.1.cmml" xref="S4.SS5.p1.6.m6.1.1.2">superscript</csymbol><ci id="S4.SS5.p1.6.m6.1.1.2.2.cmml" xref="S4.SS5.p1.6.m6.1.1.2.2">𝐎</ci><ci id="S4.SS5.p1.6.m6.1.1.2.3.cmml" xref="S4.SS5.p1.6.m6.1.1.2.3">′′</ci></apply><apply id="S4.SS5.p1.6.m6.1.1.3.cmml" xref="S4.SS5.p1.6.m6.1.1.3"><csymbol cd="ambiguous" id="S4.SS5.p1.6.m6.1.1.3.1.cmml" xref="S4.SS5.p1.6.m6.1.1.3">superscript</csymbol><ci id="S4.SS5.p1.6.m6.1.1.3.2.cmml" xref="S4.SS5.p1.6.m6.1.1.3.2">ℝ</ci><apply id="S4.SS5.p1.6.m6.1.1.3.3.cmml" xref="S4.SS5.p1.6.m6.1.1.3.3"><times id="S4.SS5.p1.6.m6.1.1.3.3.1.cmml" xref="S4.SS5.p1.6.m6.1.1.3.3.1"></times><apply id="S4.SS5.p1.6.m6.1.1.3.3.2.cmml" xref="S4.SS5.p1.6.m6.1.1.3.3.2"><times id="S4.SS5.p1.6.m6.1.1.3.3.2.1.cmml" xref="S4.SS5.p1.6.m6.1.1.3.3.2.1"></times><ci id="S4.SS5.p1.6.m6.1.1.3.3.2.2.cmml" xref="S4.SS5.p1.6.m6.1.1.3.3.2.2">𝑁</ci><ci id="S4.SS5.p1.6.m6.1.1.3.3.2.3.cmml" xref="S4.SS5.p1.6.m6.1.1.3.3.2.3">𝑛</ci><cn id="S4.SS5.p1.6.m6.1.1.3.3.2.4.cmml" type="integer" xref="S4.SS5.p1.6.m6.1.1.3.3.2.4">2</cn></apply><ci id="S4.SS5.p1.6.m6.1.1.3.3.3.cmml" xref="S4.SS5.p1.6.m6.1.1.3.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p1.6.m6.1c">\mathbf{O^{\prime\prime}}\in\mathbb{R}^{N\times n\times 2D}</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p1.6.m6.1d">bold_O start_POSTSUPERSCRIPT ′ ′ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_N × italic_n × 2 italic_D end_POSTSUPERSCRIPT</annotation></semantics></math>. “Average Sum” conducts average pooling and sum operation on local and global features. The comparison results demonstrate that our cross-attention fusion method achieves better performance than other fusion strategies.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T4.2.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text" id="S4.T4.3.2" style="font-size:90%;">Comparison of different feature combining methods on the MIND-the-SKIN dataset with train/total=0.5.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T4.4" style="width:245.1pt;height:122.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-21.6pt,10.8pt) scale(0.85,0.85) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.4.1">
<tr class="ltx_tr" id="S4.T4.4.1.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.1.1.1" style="padding-top:1pt;padding-bottom:1pt;">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span>
<span class="ltx_text ltx_font_bold" id="S4.T4.4.1.1.1.1">Methods</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.1.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.1.2.1">Precision</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.1.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.1.3.1">Recall</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.1.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.1.4.1">F1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.1.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.1.5.1">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.1.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T4.4.1.2.1" style="padding-top:1pt;padding-bottom:1pt;">Local-only</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.1.2.2" style="padding-top:1pt;padding-bottom:1pt;">0.598</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.1.2.3" style="padding-top:1pt;padding-bottom:1pt;">0.589</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.1.2.4" style="padding-top:1pt;padding-bottom:1pt;">0.593</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.4.1.2.5" style="padding-top:1pt;padding-bottom:1pt;">0.69</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.1.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.1.3.1" style="padding-top:1pt;padding-bottom:1pt;">Baseline (global-only)</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.3.2" style="padding-top:1pt;padding-bottom:1pt;">0.550</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.3.3" style="padding-top:1pt;padding-bottom:1pt;">0.565</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.3.4" style="padding-top:1pt;padding-bottom:1pt;">0.557</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.3.5" style="padding-top:1pt;padding-bottom:1pt;">0.764</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.1.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.1.4.1" style="padding-top:1pt;padding-bottom:1pt;">Concatenation-1</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.4.2" style="padding-top:1pt;padding-bottom:1pt;">0.576</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.4.3" style="padding-top:1pt;padding-bottom:1pt;">0.575</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.4.4" style="padding-top:1pt;padding-bottom:1pt;">0.57</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.4.5" style="padding-top:1pt;padding-bottom:1pt;">0.676</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.1.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.1.5.1" style="padding-top:1pt;padding-bottom:1pt;">Concatenation-2</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.5.2" style="padding-top:1pt;padding-bottom:1pt;">0.555</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.5.3" style="padding-top:1pt;padding-bottom:1pt;">0.571</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.5.4" style="padding-top:1pt;padding-bottom:1pt;">0.562</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.5.5" style="padding-top:1pt;padding-bottom:1pt;">0.671</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.1.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.1.6.1" style="padding-top:1pt;padding-bottom:1pt;">Average Sum</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.6.2" style="padding-top:1pt;padding-bottom:1pt;">0.609</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.6.3" style="padding-top:1pt;padding-bottom:1pt;">0.534</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.6.4" style="padding-top:1pt;padding-bottom:1pt;">0.536</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.6.5" style="padding-top:1pt;padding-bottom:1pt;">0.734</td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.1.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.1.7.1" style="padding-top:1pt;padding-bottom:1pt;">Ours</td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.7.2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.7.2.1">0.651</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.7.3" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.7.3.1">0.631</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.7.4" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.7.4.1">0.641</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.4.1.7.5" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.4.1.7.5.1">0.809</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.4.1.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T4.4.1.8.1" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td" id="S4.T4.4.1.8.2" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td class="ltx_td" id="S4.T4.4.1.8.3" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td class="ltx_td" id="S4.T4.4.1.8.4" style="padding-top:1pt;padding-bottom:1pt;"></td>
<td class="ltx_td" id="S4.T4.4.1.8.5" style="padding-top:1pt;padding-bottom:1pt;"></td>
</tr>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.3">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09520v1#S4.T5" title="Table 5 ‣ 4.5 Ablation Study ‣ 4 Experiments ‣ Enhancing Skin Disease Diagnosis: Interpretable Visual Concept Discovery with SAM Empowerment† indicates equal contribution, ‡ indicates the corresponding author zding1@tulane.edu"><span class="ltx_text ltx_ref_tag">5</span></a> presents a performance comparison using different top-<math alttext="k" class="ltx_Math" display="inline" id="S4.SS5.p2.1.m1.1"><semantics id="S4.SS5.p2.1.m1.1a"><mi id="S4.SS5.p2.1.m1.1.1" xref="S4.SS5.p2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.1.m1.1b"><ci id="S4.SS5.p2.1.m1.1.1.cmml" xref="S4.SS5.p2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.1.m1.1d">italic_k</annotation></semantics></math> strategies. It is observed that the model achieves optimal performance when <math alttext="k=5" class="ltx_Math" display="inline" id="S4.SS5.p2.2.m2.1"><semantics id="S4.SS5.p2.2.m2.1a"><mrow id="S4.SS5.p2.2.m2.1.1" xref="S4.SS5.p2.2.m2.1.1.cmml"><mi id="S4.SS5.p2.2.m2.1.1.2" xref="S4.SS5.p2.2.m2.1.1.2.cmml">k</mi><mo id="S4.SS5.p2.2.m2.1.1.1" xref="S4.SS5.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS5.p2.2.m2.1.1.3" xref="S4.SS5.p2.2.m2.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.2.m2.1b"><apply id="S4.SS5.p2.2.m2.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1"><eq id="S4.SS5.p2.2.m2.1.1.1.cmml" xref="S4.SS5.p2.2.m2.1.1.1"></eq><ci id="S4.SS5.p2.2.m2.1.1.2.cmml" xref="S4.SS5.p2.2.m2.1.1.2">𝑘</ci><cn id="S4.SS5.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS5.p2.2.m2.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.2.m2.1c">k=5</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.2.m2.1d">italic_k = 5</annotation></semantics></math>. While fewer visual concepts are selected, there is a significant drop in precision, recall, and F1 score, even though the accuracy does not vary much. This can be attributed to the fact that accuracy is the metric used to select the best model, and the limited samples from the training dataset can lead to overfitting on this metric. Furthermore, when <math alttext="k=15" class="ltx_Math" display="inline" id="S4.SS5.p2.3.m3.1"><semantics id="S4.SS5.p2.3.m3.1a"><mrow id="S4.SS5.p2.3.m3.1.1" xref="S4.SS5.p2.3.m3.1.1.cmml"><mi id="S4.SS5.p2.3.m3.1.1.2" xref="S4.SS5.p2.3.m3.1.1.2.cmml">k</mi><mo id="S4.SS5.p2.3.m3.1.1.1" xref="S4.SS5.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS5.p2.3.m3.1.1.3" xref="S4.SS5.p2.3.m3.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS5.p2.3.m3.1b"><apply id="S4.SS5.p2.3.m3.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1"><eq id="S4.SS5.p2.3.m3.1.1.1.cmml" xref="S4.SS5.p2.3.m3.1.1.1"></eq><ci id="S4.SS5.p2.3.m3.1.1.2.cmml" xref="S4.SS5.p2.3.m3.1.1.2">𝑘</ci><cn id="S4.SS5.p2.3.m3.1.1.3.cmml" type="integer" xref="S4.SS5.p2.3.m3.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS5.p2.3.m3.1c">k=15</annotation><annotation encoding="application/x-llamapun" id="S4.SS5.p2.3.m3.1d">italic_k = 15</annotation></semantics></math>, which is half the total number of concepts, there is a noticeable decline in all performance metrics. This decline occurs because including more visual concepts incorporates background noise from SAM into the final prediction, degrading performance.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T5.5.2.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text" id="S4.T5.2.1" style="font-size:90%;">Comparison of various <math alttext="k" class="ltx_Math" display="inline" id="S4.T5.2.1.m1.1"><semantics id="S4.T5.2.1.m1.1b"><mi id="S4.T5.2.1.m1.1.1" xref="S4.T5.2.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T5.2.1.m1.1c"><ci id="S4.T5.2.1.m1.1.1.cmml" xref="S4.T5.2.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.2.1.m1.1d">k</annotation><annotation encoding="application/x-llamapun" id="S4.T5.2.1.m1.1e">italic_k</annotation></semantics></math> on the MIND-the-SKIN dataset.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T5.3">
<tr class="ltx_tr" id="S4.T5.3.1">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.1.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">
<span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span>
<math alttext="k" class="ltx_Math" display="inline" id="S4.T5.3.1.1.m1.1"><semantics id="S4.T5.3.1.1.m1.1a"><mi id="S4.T5.3.1.1.m1.1.1" xref="S4.T5.3.1.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S4.T5.3.1.1.m1.1b"><ci id="S4.T5.3.1.1.m1.1.1.cmml" xref="S4.T5.3.1.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T5.3.1.1.m1.1c">k</annotation><annotation encoding="application/x-llamapun" id="S4.T5.3.1.1.m1.1d">italic_k</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.2.1">Precision</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.3.1">Recall</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.4.1">F1</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.1.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.1.5.1">Accuracy</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T5.3.2.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.2.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.621</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.2.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.612</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.2.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.616</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T5.3.2.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.806</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.3.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">2</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.615</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.624</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.619</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.3.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.807</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.4.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">5</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.4.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.4.2.1">0.651</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.4.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.4.3.1">0.631</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.4.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.4.4.1">0.640</span></td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.4.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T5.3.4.5.1">0.809</span></td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.5">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.5.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">6</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.647</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.634</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.640</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.5.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.808</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.6.1" style="padding-top:0.5pt;padding-bottom:0.5pt;">15</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.2" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.607</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.3" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.591</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.4" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.599</td>
<td class="ltx_td ltx_align_center" id="S4.T5.3.6.5" style="padding-top:0.5pt;padding-bottom:0.5pt;">0.804</td>
</tr>
<tr class="ltx_tr" id="S4.T5.3.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T5.3.7.1" style="padding-top:0.5pt;padding-bottom:0.5pt;"><span class="ltx_rule" style="width:100%;height:1.0pt;background:black;display:inline-block;"> </span></td>
<td class="ltx_td" id="S4.T5.3.7.2" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="S4.T5.3.7.3" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="S4.T5.3.7.4" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
<td class="ltx_td" id="S4.T5.3.7.5" style="padding-top:0.5pt;padding-bottom:0.5pt;"></td>
</tr>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this paper, we leverage the foundation AI model, SAM, to automatically segment visual skin images and propose a cross-attention model designed to harness complementary information between local visual concepts and global features in challenging clinical skin disease images. To effectively explain our model’s decision-making process, we integrate CAM and multi-instance learning to identify the most influential concepts, which are generated by SAM using stochastic text prompts. Our experiments demonstrate that the proposed method consistently outperforms competitive approaches across various metrics, underscoring the effectiveness of the dual-branch design. In addition, our method provides better interpretability, offering explainable predictions that enhance the reliability of AI-based diagnoses for medical professionals. This interpretability is crucial for building trust and improving diagnostic accessibility in future applications.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Mind-the-skin project: Multi-functional innovative digital toolkit for the skin diseases in lmics and beyond.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Segment and track anything.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:90%;">https://github.com/z-x-yang/Segment-and-Track-Anything</span><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Titus Brinker, Achim Hekler, Alexander Enk, Joachim Klode, Axel Hauschild, Carola Berking, Bastian Schilling, Sebastian Haferkamp, Jochen Utikal, Christof Kalle, Stefan Fröhling, and Michael Weichenthal.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">A convolutional neural network trained with dermoscopic images performed on par with 145 dermatologists in a clinical melanoma image classification task.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.3.1" style="font-size:90%;">European Journal of Cancer</span><span class="ltx_text" id="bib.bib3.4.2" style="font-size:90%;">, 111:148–154, 03 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
M Emre Celebi, Noel Codella, and Allan Halpern.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">Dermoscopy image analysis: overview and future directions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.3.1" style="font-size:90%;">IEEE journal of biomedical and health informatics</span><span class="ltx_text" id="bib.bib4.4.2" style="font-size:90%;">, 23(2):474–478, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.3.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span class="ltx_text" id="bib.bib5.4.2" style="font-size:90%;">, 40(4):834–848, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Alberto Coustasse, Raghav Sarkar, Bukola Abodunde, Brandon J. Metzger, and Chelsea M. Slater.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Use of teledermatology to improve dermatological access in rural areas.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">Telemedicine journal and e-health : the official journal of the American Telemedicine Association</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Roxana Daneshjou, Kailas Vodrahalli, Roberto A Novoa, Melissa Jenkins, Weixin Liang, Veronica Rotemberg, Justin Ko, Susan M Swetter, Elizabeth E Bailey, Olivier Gevaert, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Disparities in dermatology ai performance on a diverse, curated clinical image set.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">Science advances</span><span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">, 8(31):eabq6147, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin M. Ko, Susan M. Swetter, Helen M. Blau, and Sebastian Thrun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Dermatologist-level classification of skin cancer with deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib9.3.1" style="font-size:90%;">Nature</span><span class="ltx_text" id="bib.bib9.4.2" style="font-size:90%;">, 542:115–118, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Karoline Freeman, Jacqueline Dinnes, Naomi Chuchu, Yemisi Takwoingi, Sue E Bayliss, Rubeta N Matin, Abhilash Jain, Fiona M Walter, Hywel C Williams, and Jonathan J Deeks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Algorithm based smartphone apps to assess risk of skin cancer in adults: systematic review of diagnostic accuracy studies.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib10.3.1" style="font-size:90%;">bmj</span><span class="ltx_text" id="bib.bib10.4.2" style="font-size:90%;">, 368, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Ioannis Giotis, Nynke Molders, Sander Land, Michael Biehl, Marcel F Jonkman, and Nicolai Petkov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Med-node: A computer-assisted melanoma diagnosis system using non-dermoscopic images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.3.1" style="font-size:90%;">Expert systems with applications</span><span class="ltx_text" id="bib.bib11.4.2" style="font-size:90%;">, 42(19):6578–6585, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">Communications of the ACM</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, 63(11):139–144, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han, Aerin Kim, Arash Koochek, and Omar Badri.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib13.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib13.5.3" style="font-size:90%;">, pages 1820–1828, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Md Kamrul Hasan, Lavsen Dahal, Prasad N Samarakoon, Fakrul Islam Tushar, and Robert Martí.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Dsnet: Automatic dermoscopic skin lesion segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.3.1" style="font-size:90%;">Computers in biology and medicine</span><span class="ltx_text" id="bib.bib14.4.2" style="font-size:90%;">, 120:103738, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Masked autoencoders are scalable vision learners.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">, pages 16000–16009, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Galib Muhammad Shahriar Himel, Md Masudul Islam, Kh Abdullah Al-Aff, Shams Ibne Karim, and Md Kabir Uddin Sikder.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Skin cancer segmentation and classification using vision transformer for automatic analysis in dermatoscopy-based noninvasive digital system.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1" style="font-size:90%;">International Journal of Biomedical Imaging</span><span class="ltx_text" id="bib.bib18.4.2" style="font-size:90%;">, 2024(1):3022192, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Mingzhe Hu, Yuheng Li, and Xiaofeng Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Skinsam: Empowering skin cancer segmentation with segment anything model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.3.1" style="font-size:90%;">arXiv preprint arXiv:2304.13973</span><span class="ltx_text" id="bib.bib19.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Xin Hu, Kai Li, Deep Patel, Erik Kruus, Martin Renqiang Min, and Zhengming Ding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Weakly-supervised temporal action localization with multi-modal plateau transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib20.5.3" style="font-size:90%;">, pages 2704–2713, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Shubham Innani, Prasad Dutande, Ujjwal Baid, Venu Pokuri, Spyridon Bakas, Sanjay Talbar, Bhakti Baheti, and Sharath Chandra Guntuku.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">Generative adversarial networks based skin lesion segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.3.1" style="font-size:90%;">Scientific Reports</span><span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">, 13(1):13467, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Panoptic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, pages 9404–9413, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib23.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib23.5.3" style="font-size:90%;">, pages 4015–4026, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Baiying Lei, Zaimin Xia, Feng Jiang, Xudong Jiang, Zongyuan Ge, Yanwu Xu, Jing Qin, Siping Chen, Tianfu Wang, and Shuqiang Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Skin lesion segmentation via generative adversarial networks with dual discriminators.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">Medical Image Analysis</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 64:101716, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Grounding dino: Marrying dino with grounded pre-training for open-set object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">arXiv preprint arXiv:2303.05499</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Yuan Liu, Ayush Jain, Clara Eng, David H Way, Kang Lee, Peggy Bui, Kimberly Kanada, Guilherme de Oliveira Marinho, Jessica Gallegos, Sara Gabriele, Vishakha Gupta, Nalini Singh, Vivek Natarajan, Rainer Hofmann-Wellenhof, Greg S Corrado, Lily H Peng, Dale R Webster, Dennis Ai, Susan J Huang, Yun Liu, R Carter Dunn, and David Coz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">A deep learning system for differential diagnosis of skin diseases.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.3.1" style="font-size:90%;">Nature Medicine</span><span class="ltx_text" id="bib.bib26.4.2" style="font-size:90%;">, 26(6):900–908, June 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Şaban Öztürk and Umut Özkaya.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Skin lesion segmentation with improved convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.3.1" style="font-size:90%;">Journal of digital imaging</span><span class="ltx_text" id="bib.bib27.4.2" style="font-size:90%;">, 33:958–970, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Andre GC Pacheco, Gustavo R Lima, Amanda S Salomao, Breno Krohling, Igor P Biral, Gabriel G de Angelo, Fábio CR Alves Jr, José GM Esgario, Alana C Simora, Pedro BC Castro, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Pad-ufes-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib28.3.1" style="font-size:90%;">Data in brief</span><span class="ltx_text" id="bib.bib28.4.2" style="font-size:90%;">, 32:106221, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Luis R Soenksen, Timothy Kassis, Susan T Conover, Berta Marti-Fuster, Judith S Birkenfeld, Jason Tucker-Schwartz, Asif Naseem, Robert R Stavert, Caroline C Kim, Maryanne M Senna, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Using deep learning for dermatologist-level detection of suspicious pigmented skin lesions from wide-field images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">Science Translational Medicine</span><span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">, 13(581):eabb3652, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Sebastian Thrun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Is learning the n-th thing any easier than learning the first?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib30.4.2" style="font-size:90%;">, 8, 1995.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Philipp Tschandl.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Philipp Tschandl, Christoph Rinner, Zoe Apalla, Giuseppe Argenziano, Noel Codella, Allan Halpern, Monika Janda, Aimilios Lallas, Caterina Longo, Josep Malvehy, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Human–computer collaboration for skin cancer recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib32.3.1" style="font-size:90%;">Nature medicine</span><span class="ltx_text" id="bib.bib32.4.2" style="font-size:90%;">, 26(8):1229–1234, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Abbi Ward, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley Carrick, Bilson Campana, Jay Hartford, Pradeep Kumar S, Tiya Tiyasirichokchai, Sunny Virmani, Renee Wong, Yossi Matias, Greg S. Corrado, Dale R. Webster, Dawn Siegel, Steven Lin, Justin Ko, Alan Karthikesalingam, Christopher Semturs, and Pooja Rao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Crowdsourcing dermatology images with google search ads: Creating a real-world skin condition dataset, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Fengying Xie, Jiawen Yang, Jie Liu, Zhiguo Jiang, Yushan Zheng, and Yukun Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">Skin lesion segmentation using high-resolution convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.3.1" style="font-size:90%;">Computer methods and programs in biomedicine</span><span class="ltx_text" id="bib.bib34.4.2" style="font-size:90%;">, 186:105241, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Yiran Xu, Xiaoyin Yang, Lihang Gong, Hsuan-Chu Lin, Tz-Ying Wu, Yunsheng Li, and Nuno Vasconcelos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Explainable object-induced action decision for autonomous vehicles.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib35.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib35.5.3" style="font-size:90%;">, pages 9523–9532, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Jufeng Yang, Xiaoping Wu, Jie Liang, Xiaoxiao Sun, Ming-Ming Cheng, Paul L Rosin, and Liang Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Self-paced balance learning for clinical skin disease recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.3.1" style="font-size:90%;">IEEE transactions on neural networks and learning systems</span><span class="ltx_text" id="bib.bib36.4.2" style="font-size:90%;">, 31(8):2832–2846, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Kashan Zafar, Syed Omer Gilani, Asim Waris, Ali Ahmed, Mohsin Jamil, Muhammad Nasir Khan, and Amer Sohail Kashif.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">Skin lesion segmentation from dermoscopic images using convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">Sensors</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, 20(6):1601, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Haojie Zhang, Yongyi Su, Xun Xu, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Improving the generalization of segmentation foundation model under distribution shift via weakly supervised adaptation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib38.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib38.5.3" style="font-size:90%;">, pages 23385–23395, 2024.
</span>
</span>
</li>
</ul>
</section>
<section class="ltx_bibliography" id="biba">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="biba.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib1.1.1" style="font-size:90%;">
Mind-the-skin project: Multi-functional innovative digital toolkit for the skin diseases in lmics and beyond.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib2.1.1" style="font-size:90%;">
Segment and track anything.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self" style="font-size:90%;">https://github.com/z-x-yang/Segment-and-Track-Anything</span><span class="ltx_text" id="biba.bib2.2.1" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib3.1.1" style="font-size:90%;">
Titus Brinker, Achim Hekler, Alexander Enk, Joachim Klode, Axel Hauschild, Carola Berking, Bastian Schilling, Sebastian Haferkamp, Jochen Utikal, Christof Kalle, Stefan Fröhling, and Michael Weichenthal.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib3.2.1" style="font-size:90%;">A convolutional neural network trained with dermoscopic images performed on par with 145 dermatologists in a clinical melanoma image classification task.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib3.3.1" style="font-size:90%;">European Journal of Cancer</span><span class="ltx_text" id="biba.bib3.4.2" style="font-size:90%;">, 111:148–154, 03 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib4.1.1" style="font-size:90%;">
M Emre Celebi, Noel Codella, and Allan Halpern.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib4.2.1" style="font-size:90%;">Dermoscopy image analysis: overview and future directions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib4.3.1" style="font-size:90%;">IEEE journal of biomedical and health informatics</span><span class="ltx_text" id="biba.bib4.4.2" style="font-size:90%;">, 23(2):474–478, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib5.1.1" style="font-size:90%;">
Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib5.2.1" style="font-size:90%;">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib5.3.1" style="font-size:90%;">IEEE transactions on pattern analysis and machine intelligence</span><span class="ltx_text" id="biba.bib5.4.2" style="font-size:90%;">, 40(4):834–848, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib6.1.1" style="font-size:90%;">
Alberto Coustasse, Raghav Sarkar, Bukola Abodunde, Brandon J. Metzger, and Chelsea M. Slater.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib6.2.1" style="font-size:90%;">Use of teledermatology to improve dermatological access in rural areas.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib6.3.1" style="font-size:90%;">Telemedicine journal and e-health : the official journal of the American Telemedicine Association</span><span class="ltx_text" id="biba.bib6.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib7.1.1" style="font-size:90%;">
Roxana Daneshjou, Kailas Vodrahalli, Roberto A Novoa, Melissa Jenkins, Weixin Liang, Veronica Rotemberg, Justin Ko, Susan M Swetter, Elizabeth E Bailey, Olivier Gevaert, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib7.2.1" style="font-size:90%;">Disparities in dermatology ai performance on a diverse, curated clinical image set.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib7.3.1" style="font-size:90%;">Science advances</span><span class="ltx_text" id="biba.bib7.4.2" style="font-size:90%;">, 8(31):eabq6147, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib8.1.1" style="font-size:90%;">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib8.2.1" style="font-size:90%;">An image is worth 16x16 words: Transformers for image recognition at scale.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib8.3.1" style="font-size:90%;">arXiv preprint arXiv:2010.11929</span><span class="ltx_text" id="biba.bib8.4.2" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib9.1.1" style="font-size:90%;">
Andre Esteva, Brett Kuprel, Roberto A. Novoa, Justin M. Ko, Susan M. Swetter, Helen M. Blau, and Sebastian Thrun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib9.2.1" style="font-size:90%;">Dermatologist-level classification of skin cancer with deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib9.3.1" style="font-size:90%;">Nature</span><span class="ltx_text" id="biba.bib9.4.2" style="font-size:90%;">, 542:115–118, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib10.1.1" style="font-size:90%;">
Karoline Freeman, Jacqueline Dinnes, Naomi Chuchu, Yemisi Takwoingi, Sue E Bayliss, Rubeta N Matin, Abhilash Jain, Fiona M Walter, Hywel C Williams, and Jonathan J Deeks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib10.2.1" style="font-size:90%;">Algorithm based smartphone apps to assess risk of skin cancer in adults: systematic review of diagnostic accuracy studies.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib10.3.1" style="font-size:90%;">bmj</span><span class="ltx_text" id="biba.bib10.4.2" style="font-size:90%;">, 368, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib11.1.1" style="font-size:90%;">
Ioannis Giotis, Nynke Molders, Sander Land, Michael Biehl, Marcel F Jonkman, and Nicolai Petkov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib11.2.1" style="font-size:90%;">Med-node: A computer-assisted melanoma diagnosis system using non-dermoscopic images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib11.3.1" style="font-size:90%;">Expert systems with applications</span><span class="ltx_text" id="biba.bib11.4.2" style="font-size:90%;">, 42(19):6578–6585, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib12.1.1" style="font-size:90%;">
Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib12.2.1" style="font-size:90%;">Generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib12.3.1" style="font-size:90%;">Communications of the ACM</span><span class="ltx_text" id="biba.bib12.4.2" style="font-size:90%;">, 63(11):139–144, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib13.1.1" style="font-size:90%;">
Matthew Groh, Caleb Harris, Luis Soenksen, Felix Lau, Rachel Han, Aerin Kim, Arash Koochek, and Omar Badri.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib13.2.1" style="font-size:90%;">Evaluating deep neural networks trained on clinical images in dermatology with the fitzpatrick 17k dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib13.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="biba.bib13.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="biba.bib13.5.3" style="font-size:90%;">, pages 1820–1828, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib14.1.1" style="font-size:90%;">
Md Kamrul Hasan, Lavsen Dahal, Prasad N Samarakoon, Fakrul Islam Tushar, and Robert Martí.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib14.2.1" style="font-size:90%;">Dsnet: Automatic dermoscopic skin lesion segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib14.3.1" style="font-size:90%;">Computers in biology and medicine</span><span class="ltx_text" id="biba.bib14.4.2" style="font-size:90%;">, 120:103738, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib15.1.1" style="font-size:90%;">
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib15.2.1" style="font-size:90%;">Masked autoencoders are scalable vision learners.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib15.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="biba.bib15.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="biba.bib15.5.3" style="font-size:90%;">, pages 16000–16009, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib16.1.1" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib16.2.1" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib16.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="biba.bib16.4.2" style="font-size:90%;">Proceedings of the IEEE international conference on computer vision</span><span class="ltx_text" id="biba.bib16.5.3" style="font-size:90%;">, pages 2961–2969, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib17.1.1" style="font-size:90%;">
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib17.2.1" style="font-size:90%;">Deep residual learning for image recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="biba.bib17.4.2" style="font-size:90%;">Proceedings of the IEEE conference on computer vision and pattern recognition</span><span class="ltx_text" id="biba.bib17.5.3" style="font-size:90%;">, pages 770–778, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib18.1.1" style="font-size:90%;">
Galib Muhammad Shahriar Himel, Md Masudul Islam, Kh Abdullah Al-Aff, Shams Ibne Karim, and Md Kabir Uddin Sikder.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib18.2.1" style="font-size:90%;">Skin cancer segmentation and classification using vision transformer for automatic analysis in dermatoscopy-based noninvasive digital system.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib18.3.1" style="font-size:90%;">International Journal of Biomedical Imaging</span><span class="ltx_text" id="biba.bib18.4.2" style="font-size:90%;">, 2024(1):3022192, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib19.1.1" style="font-size:90%;">
Mingzhe Hu, Yuheng Li, and Xiaofeng Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib19.2.1" style="font-size:90%;">Skinsam: Empowering skin cancer segmentation with segment anything model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib19.3.1" style="font-size:90%;">arXiv preprint arXiv:2304.13973</span><span class="ltx_text" id="biba.bib19.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib20.1.1" style="font-size:90%;">
Xin Hu, Kai Li, Deep Patel, Erik Kruus, Martin Renqiang Min, and Zhengming Ding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib20.2.1" style="font-size:90%;">Weakly-supervised temporal action localization with multi-modal plateau transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib20.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="biba.bib20.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="biba.bib20.5.3" style="font-size:90%;">, pages 2704–2713, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib21.1.1" style="font-size:90%;">
Shubham Innani, Prasad Dutande, Ujjwal Baid, Venu Pokuri, Spyridon Bakas, Sanjay Talbar, Bhakti Baheti, and Sharath Chandra Guntuku.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib21.2.1" style="font-size:90%;">Generative adversarial networks based skin lesion segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib21.3.1" style="font-size:90%;">Scientific Reports</span><span class="ltx_text" id="biba.bib21.4.2" style="font-size:90%;">, 13(1):13467, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib22.1.1" style="font-size:90%;">
Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib22.2.1" style="font-size:90%;">Panoptic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="biba.bib22.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</span><span class="ltx_text" id="biba.bib22.5.3" style="font-size:90%;">, pages 9404–9413, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib23.1.1" style="font-size:90%;">
Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C Berg, Wan-Yen Lo, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib23.2.1" style="font-size:90%;">Segment anything.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib23.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="biba.bib23.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="biba.bib23.5.3" style="font-size:90%;">, pages 4015–4026, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib24.1.1" style="font-size:90%;">
Baiying Lei, Zaimin Xia, Feng Jiang, Xudong Jiang, Zongyuan Ge, Yanwu Xu, Jing Qin, Siping Chen, Tianfu Wang, and Shuqiang Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib24.2.1" style="font-size:90%;">Skin lesion segmentation via generative adversarial networks with dual discriminators.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib24.3.1" style="font-size:90%;">Medical Image Analysis</span><span class="ltx_text" id="biba.bib24.4.2" style="font-size:90%;">, 64:101716, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib25.1.1" style="font-size:90%;">
Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib25.2.1" style="font-size:90%;">Grounding dino: Marrying dino with grounded pre-training for open-set object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib25.3.1" style="font-size:90%;">arXiv preprint arXiv:2303.05499</span><span class="ltx_text" id="biba.bib25.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib26.1.1" style="font-size:90%;">
Yuan Liu, Ayush Jain, Clara Eng, David H Way, Kang Lee, Peggy Bui, Kimberly Kanada, Guilherme de Oliveira Marinho, Jessica Gallegos, Sara Gabriele, Vishakha Gupta, Nalini Singh, Vivek Natarajan, Rainer Hofmann-Wellenhof, Greg S Corrado, Lily H Peng, Dale R Webster, Dennis Ai, Susan J Huang, Yun Liu, R Carter Dunn, and David Coz.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib26.2.1" style="font-size:90%;">A deep learning system for differential diagnosis of skin diseases.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib26.3.1" style="font-size:90%;">Nature Medicine</span><span class="ltx_text" id="biba.bib26.4.2" style="font-size:90%;">, 26(6):900–908, June 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib27.1.1" style="font-size:90%;">
Şaban Öztürk and Umut Özkaya.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib27.2.1" style="font-size:90%;">Skin lesion segmentation with improved convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib27.3.1" style="font-size:90%;">Journal of digital imaging</span><span class="ltx_text" id="biba.bib27.4.2" style="font-size:90%;">, 33:958–970, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib28.1.1" style="font-size:90%;">
Andre GC Pacheco, Gustavo R Lima, Amanda S Salomao, Breno Krohling, Igor P Biral, Gabriel G de Angelo, Fábio CR Alves Jr, José GM Esgario, Alana C Simora, Pedro BC Castro, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib28.2.1" style="font-size:90%;">Pad-ufes-20: A skin lesion dataset composed of patient data and clinical images collected from smartphones.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib28.3.1" style="font-size:90%;">Data in brief</span><span class="ltx_text" id="biba.bib28.4.2" style="font-size:90%;">, 32:106221, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib29.1.1" style="font-size:90%;">
Luis R Soenksen, Timothy Kassis, Susan T Conover, Berta Marti-Fuster, Judith S Birkenfeld, Jason Tucker-Schwartz, Asif Naseem, Robert R Stavert, Caroline C Kim, Maryanne M Senna, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib29.2.1" style="font-size:90%;">Using deep learning for dermatologist-level detection of suspicious pigmented skin lesions from wide-field images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib29.3.1" style="font-size:90%;">Science Translational Medicine</span><span class="ltx_text" id="biba.bib29.4.2" style="font-size:90%;">, 13(581):eabb3652, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib30.1.1" style="font-size:90%;">
Sebastian Thrun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib30.2.1" style="font-size:90%;">Is learning the n-th thing any easier than learning the first?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib30.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="biba.bib30.4.2" style="font-size:90%;">, 8, 1995.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib31.1.1" style="font-size:90%;">
Philipp Tschandl.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib31.2.1" style="font-size:90%;">The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib32.1.1" style="font-size:90%;">
Philipp Tschandl, Christoph Rinner, Zoe Apalla, Giuseppe Argenziano, Noel Codella, Allan Halpern, Monika Janda, Aimilios Lallas, Caterina Longo, Josep Malvehy, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib32.2.1" style="font-size:90%;">Human–computer collaboration for skin cancer recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib32.3.1" style="font-size:90%;">Nature medicine</span><span class="ltx_text" id="biba.bib32.4.2" style="font-size:90%;">, 26(8):1229–1234, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib33.1.1" style="font-size:90%;">
Abbi Ward, Jimmy Li, Julie Wang, Sriram Lakshminarasimhan, Ashley Carrick, Bilson Campana, Jay Hartford, Pradeep Kumar S, Tiya Tiyasirichokchai, Sunny Virmani, Renee Wong, Yossi Matias, Greg S. Corrado, Dale R. Webster, Dawn Siegel, Steven Lin, Justin Ko, Alan Karthikesalingam, Christopher Semturs, and Pooja Rao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib33.2.1" style="font-size:90%;">Crowdsourcing dermatology images with google search ads: Creating a real-world skin condition dataset, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib34.1.1" style="font-size:90%;">
Fengying Xie, Jiawen Yang, Jie Liu, Zhiguo Jiang, Yushan Zheng, and Yukun Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib34.2.1" style="font-size:90%;">Skin lesion segmentation using high-resolution convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib34.3.1" style="font-size:90%;">Computer methods and programs in biomedicine</span><span class="ltx_text" id="biba.bib34.4.2" style="font-size:90%;">, 186:105241, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib35.1.1" style="font-size:90%;">
Yiran Xu, Xiaoyin Yang, Lihang Gong, Hsuan-Chu Lin, Tz-Ying Wu, Yunsheng Li, and Nuno Vasconcelos.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib35.2.1" style="font-size:90%;">Explainable object-induced action decision for autonomous vehicles.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib35.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="biba.bib35.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="biba.bib35.5.3" style="font-size:90%;">, pages 9523–9532, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib36.1.1" style="font-size:90%;">
Jufeng Yang, Xiaoping Wu, Jie Liang, Xiaoxiao Sun, Ming-Ming Cheng, Paul L Rosin, and Liang Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib36.2.1" style="font-size:90%;">Self-paced balance learning for clinical skin disease recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib36.3.1" style="font-size:90%;">IEEE transactions on neural networks and learning systems</span><span class="ltx_text" id="biba.bib36.4.2" style="font-size:90%;">, 31(8):2832–2846, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib37.1.1" style="font-size:90%;">
Kashan Zafar, Syed Omer Gilani, Asim Waris, Ali Ahmed, Mohsin Jamil, Muhammad Nasir Khan, and Amer Sohail Kashif.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib37.2.1" style="font-size:90%;">Skin lesion segmentation from dermoscopic images using convolutional neural network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="biba.bib37.3.1" style="font-size:90%;">Sensors</span><span class="ltx_text" id="biba.bib37.4.2" style="font-size:90%;">, 20(6):1601, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="biba.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib38.1.1" style="font-size:90%;">
Haojie Zhang, Yongyi Su, Xun Xu, and Kui Jia.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib38.2.1" style="font-size:90%;">Improving the generalization of segmentation foundation model under distribution shift via weakly supervised adaptation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="biba.bib38.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="biba.bib38.4.2" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="biba.bib38.5.3" style="font-size:90%;">, pages 23385–23395, 2024.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 14 20:07:51 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
