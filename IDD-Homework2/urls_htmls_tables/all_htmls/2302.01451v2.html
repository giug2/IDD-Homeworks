<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2302.01451] CTE: A Dataset for Contextualized Table Extraction</title><meta property="og:description" content="Relevant information in documents is often summarized in tables, helping the reader to identify useful facts.
Most benchmark datasets support either document layout analysis or table understanding, but lack in providin‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CTE: A Dataset for Contextualized Table Extraction">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="CTE: A Dataset for Contextualized Table Extraction">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2302.01451">

<!--Generated on Fri Mar  1 03:25:45 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<div id="p1" class="ltx_para">
<span id="p1.1" class="ltx_ERROR undefined">\copyrightclause</span>
<p id="p1.2" class="ltx_p">Copyright for this paper by its authors.
Use permitted under Creative Commons License Attribution 4.0
International (CC BY 4.0).</p>
</div>
<div id="p2" class="ltx_para">
<span id="p2.1" class="ltx_ERROR undefined">\conference</span>
<p id="p2.2" class="ltx_p">IRCDL 2023 ‚Äì XIX: The Conference on Information and Research science Connecting to Digital and Library science, 23-24 February 2023 - Bari, Italy</p>
</div>
<div id="p3" class="ltx_para">
<p id="p3.1" class="ltx_p">[orcid=0000-0002-6149-8282,
email=andrea.gemelli@unifi.it,
url=https://andreagemelli.github.io,
]
<span id="p3.1.1" class="ltx_ERROR undefined">\cormark</span>[1]
<span id="p3.1.2" class="ltx_ERROR undefined">\fnmark</span>[1]</p>
</div>
<div id="p4" class="ltx_para">
<p id="p4.1" class="ltx_p">[orcid=0000-0002-9971-8738,
email=emanuele.vivoli@unifi.it,
url=http://www.emanuelevivoli.me,
]
<span id="p4.1.1" class="ltx_ERROR undefined">\fnmark</span>[1]</p>
</div>
<div id="p5" class="ltx_para">
<p id="p5.1" class="ltx_p">[orcid=0000-0002-6702-2277,
email=simone.marinai@unifi.it,
url=https://tinyurl.com/simone-marinai,
]
<span id="p5.1.1" class="ltx_ERROR undefined">\fnmark</span>[1]</p>
</div>
<div id="p6" class="ltx_para">
<span id="p6.1" class="ltx_ERROR undefined">\cortext</span>
<p id="p6.2" class="ltx_p">[1]Corresponding author.
<span id="p6.2.1" class="ltx_ERROR undefined">\fntext</span>[1]The authors contributed equally.</p>
</div>
<h1 class="ltx_title ltx_title_document">CTE: A Dataset for Contextualized Table Extraction</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrea Gemelli
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Emanuele Vivoli
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Simone Marinai
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_address">Dipartimento di Ingegneria dell‚ÄôInformazione (DINFO)
Universit√† degli studi di Firenze, Italy
</span></span></span>
</div>
<div class="ltx_dates">(2022)</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Relevant information in documents is often summarized in tables, helping the reader to identify useful facts.
Most benchmark datasets support either document layout analysis or table understanding, but lack in providing data to apply both tasks in a unified way.
We define the task of Contextualized Table Extraction (CTE), which aims to extract and define the structure of tables considering the textual context of the document.
The dataset comprises 75k fully annotated pages of scientific papers, including more than 35k tables.
Data are gathered from PubMed Central, merging the information provided by annotations in the PubTables-1M and PubLayNet datasets. The dataset can support CTE and adds new classes to the original ones.
The generated annotations can be used to develop end-to-end pipelines for various tasks, including document layout analysis, table detection, structure recognition, and functional analysis.
We formally define CTE and evaluation metrics, showing which subtasks can be tackled, describing advantages, limitations, and future works of this collection of data.
Annotations and code will be accessible at <a target="_blank" href="https://github.com/AILab-UniFI/cte-dataset" title="" class="ltx_ref ltx_href">https://github.com/AILab-UniFI/cte-dataset</a>.</p>
</div>
<div class="ltx_classification">
<h6 class="ltx_title ltx_title_classification">keywords: </h6>
Dataset <span id="id2.id1" class="ltx_ERROR undefined">\sep</span>Table Extraction <span id="id3.id2" class="ltx_ERROR undefined">\sep</span>Scientific Paper Analysis <span id="id4.id3" class="ltx_ERROR undefined">\sep</span>Document Layout Analysis <span id="id5.id4" class="ltx_ERROR undefined">\sep</span>Benchmark

</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Nowadays, large collections of documents require a huge amount of human work to annotate documents and extract important information. In the last thirty years, the community of Document Analysis and Recognition (DAR) tried to overcome this challenge, exploiting suitable algorithms and artificial intelligence techniques to automatize the analysis of documents and reduce its costs. Among others, Document Classification (DC), Layout Analysis (DLA), and Table Understanding (TU) more broadly attracted the interest of researchers and companies.
DC is the first step of many DAR pipelines, since different kinds of documents require different strategies: given a document, either scanned or digital-born, the aim is to classify it into a specific category, e.g. invoice or magazine.
DLA <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> aims at recognizing homogeneous regions within the document, grouping smaller components close to each other such as regions of text, and, if required, assigning it a category (e.g. a title or an image caption). Finally, TU <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite> is an umbrella term for table detection and recognition: tables summarize important information within documents and their detection along with the recognition of their structure is crucial to automatically query collections of documents.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">During the past years, the interest in the detection and recognition of tables raised significantly, leading to the automation of important processes such as information extraction. In particular, for scientific literature, it is crucial to extract tabular data, e.g. to make the research comparable and help scholars to reconstruct the SOTA of the different fields of study <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>]</cite>. Moreover, collections of scientific papers such as arXiv and PubMed opened to the possibility of accessing a large number of documents along with their structural information represented in standard formats such as <span id="S1.p2.1.1" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="S1.p2.1.1.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="S1.p2.1.1.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span>‚ÄÑ and XML. That is why scientific literature parsing and scientific table analysis rapidly became one of the most prominent areas of research in DAR: large datasets have been released <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite>, allowing the community to develop deep learning models. Unfortunately, as we will describe in the next sections, these datasets come with partial information that forces the experimentation of layout analysis and table extraction separately. From this identified lack, we define Contextualized Table Extraction, a broad task that comes along with novel annotations for a collection of 75k scientific pages containing more than 35k tables, encouraging the development of new systems capable of tackling a multitude of tasks at once.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">In this paper, we introduce a new task called Contextualized Table Extraction that is a framework, which involves detecting tables, recognizing their structure, and performing functional analysis in an end-to-end manner.
CTE is formulated as a token and link classification task, which allows for multiple tasks to be addressed simultaneously overcoming common limitations such as being performed separately or lacking a comprehensive dataset. CTE is built on top of well known tasks in DAR. CTE is designed to be suitable for methods employing Graph Neural Networks, which are widely used in applications where the structure and layout in documents matter.
We provide a new set of labels structured in a way that allows us to merge information of selected scientific publications from other well known benchmark datasets. In this way we obtain a comprehensive dataset for the task of CTE.
We believe that the combination of methods applied to process the labeled documents and produce the merged information collected is a novel contribution to the field of document analysis as well.</p>
</div>
<figure id="S1.T1" class="ltx_table">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S1.T1.8.3.1" class="ltx_text" style="font-size:90%;">Table 1</span>: </span><span id="S1.T1.4.2" class="ltx_text" style="font-size:90%;">Comparison of CTE with related datasets: <math id="S1.T1.3.1.m1.1" class="ltx_Math" alttext="\clubsuit" display="inline"><semantics id="S1.T1.3.1.m1.1b"><mi mathvariant="normal" id="S1.T1.3.1.m1.1.1" xref="S1.T1.3.1.m1.1.1.cmml">‚ô£</mi><annotation-xml encoding="MathML-Content" id="S1.T1.3.1.m1.1c"><ci id="S1.T1.3.1.m1.1.1.cmml" xref="S1.T1.3.1.m1.1.1">‚ô£</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.3.1.m1.1d">\clubsuit</annotation></semantics></math> ¬†¬†denotes the datasets used to generate the new annotations. A dataset is <math id="S1.T1.4.2.m2.1" class="ltx_Math" alttext="S4G" display="inline"><semantics id="S1.T1.4.2.m2.1b"><mrow id="S1.T1.4.2.m2.1.1" xref="S1.T1.4.2.m2.1.1.cmml"><mi id="S1.T1.4.2.m2.1.1.2" xref="S1.T1.4.2.m2.1.1.2.cmml">S</mi><mo lspace="0em" rspace="0em" id="S1.T1.4.2.m2.1.1.1" xref="S1.T1.4.2.m2.1.1.1.cmml">‚Äã</mo><mn id="S1.T1.4.2.m2.1.1.3" xref="S1.T1.4.2.m2.1.1.3.cmml">4</mn><mo lspace="0em" rspace="0em" id="S1.T1.4.2.m2.1.1.1b" xref="S1.T1.4.2.m2.1.1.1.cmml">‚Äã</mo><mi id="S1.T1.4.2.m2.1.1.4" xref="S1.T1.4.2.m2.1.1.4.cmml">G</mi></mrow><annotation-xml encoding="MathML-Content" id="S1.T1.4.2.m2.1c"><apply id="S1.T1.4.2.m2.1.1.cmml" xref="S1.T1.4.2.m2.1.1"><times id="S1.T1.4.2.m2.1.1.1.cmml" xref="S1.T1.4.2.m2.1.1.1"></times><ci id="S1.T1.4.2.m2.1.1.2.cmml" xref="S1.T1.4.2.m2.1.1.2">ùëÜ</ci><cn type="integer" id="S1.T1.4.2.m2.1.1.3.cmml" xref="S1.T1.4.2.m2.1.1.3">4</cn><ci id="S1.T1.4.2.m2.1.1.4.cmml" xref="S1.T1.4.2.m2.1.1.4">ùê∫</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.4.2.m2.1d">S4G</annotation></semantics></math> (suitable for graphs) if a graph can be constructed directly with no further preprocessing. DLA (Document Layout Analysis), TD (Table Detection), TSR (Table Structure Recognition), and TFA (Table Functional Analysis) show which tasks models can be trained for.</span></figcaption>
<div id="S1.T1.6" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:159.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(20.9pt,-7.7pt) scale(1.10691718656413,1.10691718656413) ;">
<table id="S1.T1.6.2" class="ltx_tabular ltx_align_middle">
<tr id="S1.T1.6.2.3" class="ltx_tr">
<td id="S1.T1.6.2.3.1" class="ltx_td ltx_align_left ltx_border_r">Dataset</td>
<td id="S1.T1.6.2.3.2" class="ltx_td ltx_align_center">#pages</td>
<td id="S1.T1.6.2.3.3" class="ltx_td ltx_align_center">#tables</td>
<td id="S1.T1.6.2.3.4" class="ltx_td ltx_align_center ltx_border_r">#classes</td>
<td id="S1.T1.6.2.3.5" class="ltx_td ltx_align_center">DLA</td>
<td id="S1.T1.6.2.3.6" class="ltx_td ltx_align_center">TD</td>
<td id="S1.T1.6.2.3.7" class="ltx_td ltx_align_center">TSR</td>
<td id="S1.T1.6.2.3.8" class="ltx_td ltx_align_center ltx_border_r">TFA</td>
<td id="S1.T1.6.2.3.9" class="ltx_td ltx_align_center">S4G</td>
</tr>
<tr id="S1.T1.5.1.1" class="ltx_tr">
<td id="S1.T1.5.1.1.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">PubLayNet (<math id="S1.T1.5.1.1.1.m1.1" class="ltx_Math" alttext="\clubsuit" display="inline"><semantics id="S1.T1.5.1.1.1.m1.1a"><mi mathvariant="normal" id="S1.T1.5.1.1.1.m1.1.1" xref="S1.T1.5.1.1.1.m1.1.1.cmml">‚ô£</mi><annotation-xml encoding="MathML-Content" id="S1.T1.5.1.1.1.m1.1b"><ci id="S1.T1.5.1.1.1.m1.1.1.cmml" xref="S1.T1.5.1.1.1.m1.1.1">‚ô£</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.5.1.1.1.m1.1c">\clubsuit</annotation></semantics></math>)</td>
<td id="S1.T1.5.1.1.2" class="ltx_td ltx_align_center ltx_border_t">358k</td>
<td id="S1.T1.5.1.1.3" class="ltx_td ltx_align_center ltx_border_t">107k</td>
<td id="S1.T1.5.1.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">5</td>
<td id="S1.T1.5.1.1.5" class="ltx_td ltx_align_center ltx_border_t">‚úì</td>
<td id="S1.T1.5.1.1.6" class="ltx_td ltx_align_center ltx_border_t">‚úì</td>
<td id="S1.T1.5.1.1.7" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
<td id="S1.T1.5.1.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úó</td>
<td id="S1.T1.5.1.1.9" class="ltx_td ltx_align_center ltx_border_t">‚úó</td>
</tr>
<tr id="S1.T1.6.2.2" class="ltx_tr">
<td id="S1.T1.6.2.2.1" class="ltx_td ltx_align_left ltx_border_r">PubTables-1M (<math id="S1.T1.6.2.2.1.m1.1" class="ltx_Math" alttext="\clubsuit" display="inline"><semantics id="S1.T1.6.2.2.1.m1.1a"><mi mathvariant="normal" id="S1.T1.6.2.2.1.m1.1.1" xref="S1.T1.6.2.2.1.m1.1.1.cmml">‚ô£</mi><annotation-xml encoding="MathML-Content" id="S1.T1.6.2.2.1.m1.1b"><ci id="S1.T1.6.2.2.1.m1.1.1.cmml" xref="S1.T1.6.2.2.1.m1.1.1">‚ô£</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.T1.6.2.2.1.m1.1c">\clubsuit</annotation></semantics></math>)</td>
<td id="S1.T1.6.2.2.2" class="ltx_td ltx_align_center">574k</td>
<td id="S1.T1.6.2.2.3" class="ltx_td ltx_align_center">948k</td>
<td id="S1.T1.6.2.2.4" class="ltx_td ltx_align_center ltx_border_r">7</td>
<td id="S1.T1.6.2.2.5" class="ltx_td ltx_align_center">‚úó</td>
<td id="S1.T1.6.2.2.6" class="ltx_td ltx_align_center">‚úì</td>
<td id="S1.T1.6.2.2.7" class="ltx_td ltx_align_center">‚úì</td>
<td id="S1.T1.6.2.2.8" class="ltx_td ltx_align_center ltx_border_r">‚úì</td>
<td id="S1.T1.6.2.2.9" class="ltx_td ltx_align_center">‚úó</td>
</tr>
<tr id="S1.T1.6.2.4" class="ltx_tr">
<td id="S1.T1.6.2.4.1" class="ltx_td ltx_align_left ltx_border_r">DocBank</td>
<td id="S1.T1.6.2.4.2" class="ltx_td ltx_align_center">500k</td>
<td id="S1.T1.6.2.4.3" class="ltx_td ltx_align_center">417k*</td>
<td id="S1.T1.6.2.4.4" class="ltx_td ltx_align_center ltx_border_r">12</td>
<td id="S1.T1.6.2.4.5" class="ltx_td ltx_align_center">‚úì</td>
<td id="S1.T1.6.2.4.6" class="ltx_td ltx_align_center">‚úì</td>
<td id="S1.T1.6.2.4.7" class="ltx_td ltx_align_center">‚úì*</td>
<td id="S1.T1.6.2.4.8" class="ltx_td ltx_align_center ltx_border_r">‚úó</td>
<td id="S1.T1.6.2.4.9" class="ltx_td ltx_align_center">‚úì**</td>
</tr>
<tr id="S1.T1.6.2.5" class="ltx_tr">
<td id="S1.T1.6.2.5.1" class="ltx_td ltx_align_left ltx_border_r">SciTSR</td>
<td id="S1.T1.6.2.5.2" class="ltx_td ltx_align_center">0</td>
<td id="S1.T1.6.2.5.3" class="ltx_td ltx_align_center">15k</td>
<td id="S1.T1.6.2.5.4" class="ltx_td ltx_align_center ltx_border_r">-</td>
<td id="S1.T1.6.2.5.5" class="ltx_td ltx_align_center">‚úó</td>
<td id="S1.T1.6.2.5.6" class="ltx_td ltx_align_center">‚úó</td>
<td id="S1.T1.6.2.5.7" class="ltx_td ltx_align_center">‚úì</td>
<td id="S1.T1.6.2.5.8" class="ltx_td ltx_align_center ltx_border_r">‚úó</td>
<td id="S1.T1.6.2.5.9" class="ltx_td ltx_align_center">‚úì</td>
</tr>
<tr id="S1.T1.6.2.6" class="ltx_tr">
<td id="S1.T1.6.2.6.1" class="ltx_td ltx_align_left ltx_border_r ltx_border_t">CTE</td>
<td id="S1.T1.6.2.6.2" class="ltx_td ltx_align_center ltx_border_t">75k</td>
<td id="S1.T1.6.2.6.3" class="ltx_td ltx_align_center ltx_border_t">35k</td>
<td id="S1.T1.6.2.6.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">13</td>
<td id="S1.T1.6.2.6.5" class="ltx_td ltx_align_center ltx_border_t">‚úì</td>
<td id="S1.T1.6.2.6.6" class="ltx_td ltx_align_center ltx_border_t">‚úì</td>
<td id="S1.T1.6.2.6.7" class="ltx_td ltx_align_center ltx_border_t">‚úì</td>
<td id="S1.T1.6.2.6.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t">‚úì</td>
<td id="S1.T1.6.2.6.9" class="ltx_td ltx_align_center ltx_border_t">‚úì</td>
</tr>
<tr id="S1.T1.6.2.7" class="ltx_tr">
<td id="S1.T1.6.2.7.1" class="ltx_td ltx_align_left ltx_border_tt" colspan="9"><span id="S1.T1.6.2.7.1.1" class="ltx_text" style="font-size:70%;">*DocBank is an extension of TableBank, from which we gathered these information</span></td>
</tr>
<tr id="S1.T1.6.2.8" class="ltx_tr">
<td id="S1.T1.6.2.8.1" class="ltx_td ltx_align_left" colspan="9"><span id="S1.T1.6.2.8.1.1" class="ltx_text" style="font-size:70%;">**If tokens used as graph nodes, no information on edges</span></td>
</tr>
</table>
</span></div>
</figure>
<section id="S1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Related Work</h3>

<div id="S1.SS1.p1" class="ltx_para">
<p id="S1.SS1.p1.1" class="ltx_p">Despite the advances in the field, several challenges strongly limited the generalization of methods developed until a few years ago. In particular, we can mention: (i) data quality (e.g. scanned documents or images captured in-the-wild); (ii) contents, due to different languages and/or scripts; (iii) document layouts (which differentiate in, e.g. magazines, scientific papers, and invoices).
To address these challenges a large number of data need to be collected in order to fully exploit the power of Deep Learning models that achieve the state-of-the-art for the aforementioned tasks.
Unfortunately, creating such datasets is nothing but trivial since accurate annotations come at a high cost in terms of time and human effort¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>]</cite>. On the other hand, automatic annotation techniques are not always applicable since they require
a large number of documents shared together with their source files in standard formats such as <span id="S1.SS1.p1.1.1" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="S1.SS1.p1.1.1.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="S1.SS1.p1.1.1.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span>, XML, or HTML¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Additionally, these techniques usually generate weakly labeled collections and are more error-prone than manually annotated ones.</p>
</div>
<div id="S1.SS1.p2" class="ltx_para">
<p id="S1.SS1.p2.1" class="ltx_p">Since online archives of scientific papers are freely and publicly available along with the corresponding source information (e.g. arXiv and PubMed) several datasets have been proposed so far in the field of scientific literature parsing.
Among others, we summarize in Table <a href="#S1.T1" title="Table 1 ‚Ä£ 1 Introduction ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> some of the most important datasets proposed for layout analysis and table extraction.
PubLayNet and DocBank have been widely used to train object detectors <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>]</cite> and transformers <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> for DLA.
Overall, these datasets contain around half a million pages labeled into five and twelve different classes, respectively.
PubLayNet has been constructed merging the information extracted from PDFMiner (bounding box regions) and the XML files shared by the publishers (containing the region labels).
DocBank is built gathering the <span id="S1.SS1.p2.1.1" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="S1.SS1.p2.1.1.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="S1.SS1.p2.1.1.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span>source files and assigning labels taking into account the section tags.
For the Table Extraction task, a recent dataset has been released (PubTables-1M) which counts nearly one million tables, labeled to perform not only TD and TSR but also Table Functional Analysis (TFA) that provides additional information on table cells like table headers.
Even if it is smaller, SciTSR <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> introduced a collection of 15k tables generated from <span id="S1.SS1.p2.1.2" class="ltx_text ltx_LaTeX_logo" style="letter-spacing:-0.2em; margin-right:0.1em;">L<span id="S1.SS1.p2.1.2.1" class="ltx_text" style="position:relative; bottom:0.4ex;font-variant:small-caps;;">a</span>T<span id="S1.SS1.p2.1.2.2" class="ltx_text" style="position:relative; bottom:-0.2ex;font-variant:small-caps;font-size:120%;">e</span>X</span>‚ÄÑ to perform TSR, mainly using a Graph Neural Network (GNN). Despite this contribution, GNNs also have the advantage of being lightweight compared to transformer-based architectures while still retaining good performance, as shown in the framework Doc2Graph <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite> for document analysis.</p>
</div>
<div id="S1.SS1.p3" class="ltx_para">
<p id="S1.SS1.p3.1" class="ltx_p">As it is possible to notice in Table¬†<a href="#S1.T1" title="Table 1 ‚Ä£ 1 Introduction ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>, all these datasets lack a comprehensive and broader set of annotations, forcing the community to develop multiple systems that, in application scenarios, would lead to heavy and large pipelines.</p>
</div>
</section>
<section id="S1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.2 </span>Contributions</h3>

<div id="S1.SS2.p1" class="ltx_para">
<p id="S1.SS2.p1.1" class="ltx_p">Our ongoing work brings several novelties, that are discussed throughout the paper and are summarized as follows:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i1.p1" class="ltx_para">
<p id="S1.I1.i1.p1.1" class="ltx_p">We define the task of Contextualized Table Extraction, an extended version of table extraction as defined in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> that adds layout information and encourages the development of end-to-end systems that can tackle multiple tasks at once;</p>
</div>
</li>
<li id="S1.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i2.p1" class="ltx_para">
<p id="S1.I1.i2.p1.1" class="ltx_p">Novel annotations are created by merging subset of <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> that can be found in our repo<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/AILab-UniFI/cte-dataset" title="" class="ltx_ref ltx_href">https://github.com/AILab-UniFI/cte-dataset</a></span></span></span>. Our collection comprehends 75k scientific pages and more than 35k tables. Tokens at the basis of annotations correspond to words extracted from PDFs using PyMuPDF and labeled according to the region they belong to; table structure information is encoded as links between tokens;</p>
</div>
</li>
<li id="S1.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">‚Ä¢</span> 
<div id="S1.I1.i3.p1" class="ltx_para">
<p id="S1.I1.i3.p1.1" class="ltx_p">The dataset encourages the use and development of graph methods on documents, providing to the community a new set of labeled data to experiment with GNN-based techniques. The annotations do not require any further processing (either in labels or data themselves) to construct a graph over the scientific pages.</p>
</div>
</li>
</ul>
</div>
<div id="S1.SS2.p2" class="ltx_para">
<p id="S1.SS2.p2.1" class="ltx_p">The paper is organized as follows: in Section 2 we describe in detail how the dataset has been created and how the annotations are presented, along with some limitations we aim to address in the near future. Section 3 formalizes the CTE task by means of token and link classification. Finally, in Section 4 and 5 we
discuss future work and draw conclusions.</p>
</div>
</section>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Dataset Description</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">Contextualized Table Extraction (CTE), as we describe deeply in Section <a href="#S3" title="3 Contextualized Table Extraction ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, involves not only detecting tables, recognizing their layout and functional structure, but also takes into consideration their surrounding information. We formalize CTE to be accomplished through token and link classification, allowing multiple tasks to be tackled at once. The F1 score for CTE is defined as the average of F1 scores for token and link classification.</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p">Although it is easy to freely access large collections of scientific papers (i.e. from arXiv or PubMed Central) it is difficult to find documents labeled with complete information.
Most benchmark datasets support either DLA or TU.
However, as our aim is encouraging the development of systems capable of tackling more tasks at once, a new dataset is needed.
The proposed dataset for CTE is obtained by merging data and annotations given by PubLayNet and PubTables-1M datasets, both based on PubMed Central publications. As depicted in the next sections, firstly we identify the pages of scientific papers annotated in both datasets, then we merge the information and add two novel classes (captions and page information) and finally use PyMuPDF to extract text and position of tokens. We used a preliminary small version of this collection in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, applying a GNN to tackle CTE.
After the release of PubLayNet test set we updated the version of CTE dataset, now containing more annotated data.</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Subset of PubLayNet and PubTables-1M</h3>

<div id="S2.SS1.p1" class="ltx_para">
<p id="S2.SS1.p1.2" class="ltx_p">PubLayNet is a collection of <math id="S2.SS1.p1.1.m1.2" class="ltx_Math" alttext="358,353" display="inline"><semantics id="S2.SS1.p1.1.m1.2a"><mrow id="S2.SS1.p1.1.m1.2.3.2" xref="S2.SS1.p1.1.m1.2.3.1.cmml"><mn id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml">358</mn><mo id="S2.SS1.p1.1.m1.2.3.2.1" xref="S2.SS1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.1.m1.2.2" xref="S2.SS1.p1.1.m1.2.2.cmml">353</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.2b"><list id="S2.SS1.p1.1.m1.2.3.1.cmml" xref="S2.SS1.p1.1.m1.2.3.2"><cn type="integer" id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">358</cn><cn type="integer" id="S2.SS1.p1.1.m1.2.2.cmml" xref="S2.SS1.p1.1.m1.2.2">353</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.2c">358,353</annotation></semantics></math> PDF pages with five types of regions annotated (<span id="S2.SS1.p1.2.1" class="ltx_text ltx_font_italic">title, text, list, table, image</span>) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>.
PubTables-1M <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib5" title="" class="ltx_ref">5</a>]</cite> is a collection of <math id="S2.SS1.p1.2.m2.2" class="ltx_Math" alttext="947,642" display="inline"><semantics id="S2.SS1.p1.2.m2.2a"><mrow id="S2.SS1.p1.2.m2.2.3.2" xref="S2.SS1.p1.2.m2.2.3.1.cmml"><mn id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml">947</mn><mo id="S2.SS1.p1.2.m2.2.3.2.1" xref="S2.SS1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S2.SS1.p1.2.m2.2.2" xref="S2.SS1.p1.2.m2.2.2.cmml">642</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.2b"><list id="S2.SS1.p1.2.m2.2.3.1.cmml" xref="S2.SS1.p1.2.m2.2.3.2"><cn type="integer" id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">947</cn><cn type="integer" id="S2.SS1.p1.2.m2.2.2.cmml" xref="S2.SS1.p1.2.m2.2.2">642</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.2c">947,642</annotation></semantics></math> fully annotated tables, including information for table detection, recognition, and functional analysis (such as identifying <span id="S2.SS1.p1.2.2" class="ltx_text ltx_font_italic">column headers, projected rows</span>, and <span id="S2.SS1.p1.2.3" class="ltx_text ltx_font_italic">table cells</span>). The datasets are built to address different tasks, as summarized in Table <a href="#S1.T1" title="Table 1 ‚Ä£ 1 Introduction ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">To merge the datasets, we first identify the papers belonging to both collections.
From this subset, we keep pages with tables fully annotated in PubTables-1M and pages without tables: this filters out even more pages, since we found some PubTables-1M annotations to have only one annotated table in pages containing two or more tables. Following this step, we obtain approximately 75k pages.
The resulting merged dataset contains objects labeled into 13 different classes, having in addition to the regions annotated in PubLayNet the table annotations described in PubTables-1M (<span id="S2.SS1.p2.1.1" class="ltx_text ltx_font_italic">row, column, table header, projected header, table cell</span>, and <span id="S2.SS1.p2.1.2" class="ltx_text ltx_font_italic">grid cell</span>). Moreover, we added two classes: <span id="S2.SS1.p2.1.3" class="ltx_text ltx_font_italic">caption</span> and <span id="S2.SS1.p2.1.4" class="ltx_text ltx_font_italic">other</span>. <span id="S2.SS1.p2.1.5" class="ltx_text ltx_font_italic">Captions</span> are heuristically found taking into account the proximity with images and tables, while the <span id="S2.SS1.p2.1.6" class="ltx_text ltx_font_italic">other</span> class contains all the remaining not-labeled text regions (e.g. page headers and page numbers).</p>
</div>
<div id="S2.SS1.p3" class="ltx_para">
<p id="S2.SS1.p3.1" class="ltx_p">The GitHub repository of our dataset is at its second version, after adding the test-set released by PubLayNet <span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>From PubLayNet Github repo: ‚Äù07/Mar/2022 - We have released the ground truth of the test set for the ICDAR 2021 Scientific Literature Parsing competition available <span class="ltx_ref ltx_ref_self">here</span>.‚Äù</span></span></span>.
We followed PubLayNet for the train/val/test splits.</p>
</div>
<figure id="S2.F1" class="ltx_figure"><img src="/html/2302.01451/assets/data.png" id="S2.F1.g1" class="ltx_graphics ltx_centering ltx_img_square" width="538" height="538" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S2.F1.2.1.1" class="ltx_text" style="font-size:90%;">Figure 1</span>: </span><span id="S2.F1.3.2" class="ltx_text" style="font-size:90%;">Example page (content is intentionally concealed in ‚ÄôOriginal‚Äô) and corresponding CTE annotations.
Objects represent the layout regions. Tokens contain the word tokens labeled according to the class in the top of the figure.
Acronyms for table annotations are: THEAD (table headers), TSPAN (table sub-headers spanning along different columns), TGRID (table cells), TCOLS and TROWS (respectively columns and rows of the tables).</span></figcaption>
</figure>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Annotation procedure</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">Once a complete annotated list of pages is selected from the two datasets, we leverage an external tool to extract page tokens. After comparing several tools, we opted for PyMuPDF <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib15" title="" class="ltx_ref">15</a>]</cite> which is a Python open-source library backed by a large community and constantly maintained. Each element, visible or not visible, present in the PDF page is extracted and annotated based on the annotation bounding-box it appears in, as depicted in Figure <a href="#S2.F1" title="Figure 1 ‚Ä£ 2.1 Subset of PubLayNet and PubTables-1M ‚Ä£ 2 Dataset Description ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>: tokens are labeled according to their enclosing labeled region (upper part); links, instead, are presented as groups of tokens for visualization purposes (bottom part), but encoded as couples as described in details in the next Section and in Table <a href="#S2.T2.st3" title="In 2.3 Dataset structure and format ‚Ä£ 2 Dataset Description ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(c)</span></a>. By doing so, the resulting page is composed by extracting page tokens along with their position (bounding boxes coordinates) and their textual content (mostly single words).
This process heavily depends on original versions of the PDF files: even if the document name is the same along the two datasets annotations (PubLayNet and PubTables-1M) the PDF version of PubLayNet documents could differ. This is due to the two years gap between the datasets release date. To obtain reliable information, in our approach we discard all the pages (and tables) in which the content of the two sources does not correspond anymore.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Dataset structure and format</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">After the merging procedure, we end up with three JSON files (subset of the original PubLayNet one) splitting the data into train, val, and test.
Each one contains information regarding tokens extracted by PyMuPDF, their links and the regions that group them (larger objects).
Tokens have these information: <span id="S2.SS3.p1.1.1" class="ltx_text ltx_font_italic">token id, bounding box coordinates, text, class id</span>, and <span id="S2.SS3.p1.1.2" class="ltx_text ltx_font_italic">object id</span> (larger region to which it belongs).
Links between tokens (belonging to the same row, column or grid cell) have information such as <span id="S2.SS3.p1.1.3" class="ltx_text ltx_font_italic">link id, class id</span>, and <span id="S2.SS3.p1.1.4" class="ltx_text ltx_font_italic">token id</span> (list of tokens linked together). Finally, objects contain information such as <span id="S2.SS3.p1.1.5" class="ltx_text ltx_font_italic">object id, bounding box coordinates</span>¬†and <span id="S2.SS3.p1.1.6" class="ltx_text ltx_font_italic">class id</span>. A representation of the aforementioned annotation format is represented in Tables <a href="#S2.T2.st3" title="In 2.3 Dataset structure and format ‚Ä£ 2 Dataset Description ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(c)</span></a>.</p>
</div>
<figure id="S2.T2.st3" class="ltx_table">

<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.st3.2.1.1" class="ltx_text" style="font-size:90%;">Table 2</span>: </span><span id="S2.T2.st3.3.2" class="ltx_text" style="font-size:90%;">Annotation Format: each line contains different information in case of Objects, Tokens, or Links.</span></figcaption>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.st3.4.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S2.T2.st3.5.2" class="ltx_text" style="font-size:90%;">Object annotations</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S2.T2.st3.6" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S2.T2.st3.6.1" class="ltx_tr">
<td id="S2.T2.st3.6.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.st3.6.1.1.1" class="ltx_text ltx_font_bold">Index</span></td>
<td id="S2.T2.st3.6.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.6.1.2.1" class="ltx_text ltx_font_bold">0</span></td>
<td id="S2.T2.st3.6.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.6.1.3.1" class="ltx_text ltx_font_bold">1,0</span></td>
<td id="S2.T2.st3.6.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.6.1.4.1" class="ltx_text ltx_font_bold">1,1</span></td>
<td id="S2.T2.st3.6.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.6.1.5.1" class="ltx_text ltx_font_bold">1,2</span></td>
<td id="S2.T2.st3.6.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.6.1.6.1" class="ltx_text ltx_font_bold">1,3</span></td>
<td id="S2.T2.st3.6.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.6.1.7.1" class="ltx_text ltx_font_bold">2</span></td>
</tr>
<tr id="S2.T2.st3.6.2" class="ltx_tr">
<td id="S2.T2.st3.6.2.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.st3.6.2.1.1" class="ltx_text ltx_font_bold">Content</span></td>
<td id="S2.T2.st3.6.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">object id</td>
<td id="S2.T2.st3.6.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">x0</td>
<td id="S2.T2.st3.6.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">y0</td>
<td id="S2.T2.st3.6.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">x1</td>
<td id="S2.T2.st3.6.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">y1</td>
<td id="S2.T2.st3.6.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">class id</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.st3.7.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S2.T2.st3.8.2" class="ltx_text" style="font-size:90%;">Tokens annotations</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S2.T2.st3.9" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S2.T2.st3.9.1" class="ltx_tr">
<td id="S2.T2.st3.9.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.1.1.1" class="ltx_text ltx_font_bold">Index</span></td>
<td id="S2.T2.st3.9.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.1.2.1" class="ltx_text ltx_font_bold">0</span></td>
<td id="S2.T2.st3.9.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.1.3.1" class="ltx_text ltx_font_bold">1,0</span></td>
<td id="S2.T2.st3.9.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.1.4.1" class="ltx_text ltx_font_bold">1,1</span></td>
<td id="S2.T2.st3.9.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.1.5.1" class="ltx_text ltx_font_bold">1,2</span></td>
<td id="S2.T2.st3.9.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.1.6.1" class="ltx_text ltx_font_bold">1,3</span></td>
<td id="S2.T2.st3.9.1.7" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.1.7.1" class="ltx_text ltx_font_bold">2</span></td>
<td id="S2.T2.st3.9.1.8" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.1.8.1" class="ltx_text ltx_font_bold">3</span></td>
<td id="S2.T2.st3.9.1.9" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.1.9.1" class="ltx_text ltx_font_bold">4</span></td>
</tr>
<tr id="S2.T2.st3.9.2" class="ltx_tr">
<td id="S2.T2.st3.9.2.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.st3.9.2.1.1" class="ltx_text ltx_font_bold">Content</span></td>
<td id="S2.T2.st3.9.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">token id</td>
<td id="S2.T2.st3.9.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">x0</td>
<td id="S2.T2.st3.9.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">y0</td>
<td id="S2.T2.st3.9.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">x1</td>
<td id="S2.T2.st3.9.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">y1</td>
<td id="S2.T2.st3.9.2.7" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">text</td>
<td id="S2.T2.st3.9.2.8" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">class id</td>
<td id="S2.T2.st3.9.2.9" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">parent object id</td>
</tr>
</table>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span id="S2.T2.st3.10.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S2.T2.st3.11.2" class="ltx_text" style="font-size:90%;">Link annotations</span></figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table id="S2.T2.st3.12" class="ltx_tabular ltx_centering ltx_figure_panel ltx_align_middle">
<tr id="S2.T2.st3.12.1" class="ltx_tr">
<td id="S2.T2.st3.12.1.1" class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.st3.12.1.1.1" class="ltx_text ltx_font_bold">Index</span></td>
<td id="S2.T2.st3.12.1.2" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.12.1.2.1" class="ltx_text ltx_font_bold">0</span></td>
<td id="S2.T2.st3.12.1.3" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.12.1.3.1" class="ltx_text ltx_font_bold">1</span></td>
<td id="S2.T2.st3.12.1.4" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.12.1.4.1" class="ltx_text ltx_font_bold">2,0</span></td>
<td id="S2.T2.st3.12.1.5" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.12.1.5.1" class="ltx_text ltx_font_bold">2,1</span></td>
<td id="S2.T2.st3.12.1.6" class="ltx_td ltx_align_center ltx_border_r ltx_border_t"><span id="S2.T2.st3.12.1.6.1" class="ltx_text ltx_font_bold">2,n-1</span></td>
</tr>
<tr id="S2.T2.st3.12.2" class="ltx_tr">
<td id="S2.T2.st3.12.2.1" class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t"><span id="S2.T2.st3.12.2.1.1" class="ltx_text ltx_font_bold">Content</span></td>
<td id="S2.T2.st3.12.2.2" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">link id</td>
<td id="S2.T2.st3.12.2.3" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">class id</td>
<td id="S2.T2.st3.12.2.4" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">1st token id</td>
<td id="S2.T2.st3.12.2.5" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">2nd token id</td>
<td id="S2.T2.st3.12.2.6" class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t">n-th token id</td>
</tr>
</table>
</div>
</div>
</figure>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Limitations of the Dataset</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">We are aware that the proposed dataset, even if it is proposing a new benchmark to tackle CTE, has room for improvement. As such, in the following we list the limitations of the dataset:</p>
<ol id="S2.I1" class="ltx_enumerate">
<li id="S2.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S2.I1.i1.p1" class="ltx_para">
<p id="S2.I1.i1.p1.1" class="ltx_p">There is a small amount of data and tables compared to other datasets. Considering that adding more annotated data would be nothing but trivial, we believe this point could be addressed in two ways: i) as a starting pool of data to train generative models and getting new samples automatically labeled (e.g. using techniques similar to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib16" title="" class="ltx_ref">16</a>]</cite>); ii) using the CTE collection as a challenging benchmark to compare lightweight models, such as GNNs, along with state-of-the-art transformers (notably anger of huge amount of data).</p>
</div>
</li>
<li id="S2.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S2.I1.i2.p1" class="ltx_para">
<p id="S2.I1.i2.p1.1" class="ltx_p">The heuristics used for the the classes <span id="S2.I1.i2.p1.1.1" class="ltx_text ltx_font_italic">caption</span> and <span id="S2.I1.i2.p1.1.2" class="ltx_text ltx_font_italic">other</span> could affect the generalization of trained models, highly dependent on the paper format used in PubMed Central. On the other hand, we are enriching information about tables by recognizing captions, that contain valuable table descriptions and that otherwise would be discarded.</p>
</div>
</li>
<li id="S2.I1.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S2.I1.i3.p1" class="ltx_para">
<p id="S2.I1.i3.p1.1" class="ltx_p">We still lack additional information such as author, keywords, and equations. We are going to add these additional labels in the near future, considering Grobid <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite> in the annotation procedure, since it is a machine learning library for extracting technical information from scientific publications, from PDF to XML/TEI structured documents.</p>
</div>
</li>
<li id="S2.I1.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S2.I1.i4.p1" class="ltx_para">
<p id="S2.I1.i4.p1.1" class="ltx_p">The first attempts to define a baselines are reported in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, in which the task of TE and DLA are treated end-to-end. This paper aims at sharing the CTE dataset in a way that the scientific community can further propose baselines on this work.</p>
</div>
</li>
</ol>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Contextualized Table Extraction</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Contextualized Table Extraction (CTE) is the broader task of extracting tables (meaning their detection) recognizing their structure and performing functional analysis, along with other page layout information.
To do so, CTE is formulated as a token and link classification tasks, similarly to <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite>, since fine-grained objects like tokens permit to tackle multiple tasks at once.
For instance, recognizing the table headers and grid cells allows us to detect the tables (grouping tokens together through links) and add functional information.
In addition, through token and link classification the need for more components would be reduced since a method capable of successfully solving CTE would require to train only one model, extracting more information at once.</p>
</div>
<div id="S3.p2" class="ltx_para">
<p id="S3.p2.1" class="ltx_p">Given Precision and Recall for token and link classification, namely Token Precision (TP), Token Recall (TR), Link Precision (LP), and Link Recall (LR). We can define the <math id="S3.p2.1.m1.1" class="ltx_Math" alttext="F1_{CTE}" display="inline"><semantics id="S3.p2.1.m1.1a"><mrow id="S3.p2.1.m1.1.1" xref="S3.p2.1.m1.1.1.cmml"><mi id="S3.p2.1.m1.1.1.2" xref="S3.p2.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.p2.1.m1.1.1.1" xref="S3.p2.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S3.p2.1.m1.1.1.3" xref="S3.p2.1.m1.1.1.3.cmml"><mn id="S3.p2.1.m1.1.1.3.2" xref="S3.p2.1.m1.1.1.3.2.cmml">1</mn><mrow id="S3.p2.1.m1.1.1.3.3" xref="S3.p2.1.m1.1.1.3.3.cmml"><mi id="S3.p2.1.m1.1.1.3.3.2" xref="S3.p2.1.m1.1.1.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.p2.1.m1.1.1.3.3.1" xref="S3.p2.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.p2.1.m1.1.1.3.3.3" xref="S3.p2.1.m1.1.1.3.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.p2.1.m1.1.1.3.3.1a" xref="S3.p2.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.p2.1.m1.1.1.3.3.4" xref="S3.p2.1.m1.1.1.3.3.4.cmml">E</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.p2.1.m1.1b"><apply id="S3.p2.1.m1.1.1.cmml" xref="S3.p2.1.m1.1.1"><times id="S3.p2.1.m1.1.1.1.cmml" xref="S3.p2.1.m1.1.1.1"></times><ci id="S3.p2.1.m1.1.1.2.cmml" xref="S3.p2.1.m1.1.1.2">ùêπ</ci><apply id="S3.p2.1.m1.1.1.3.cmml" xref="S3.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.p2.1.m1.1.1.3.1.cmml" xref="S3.p2.1.m1.1.1.3">subscript</csymbol><cn type="integer" id="S3.p2.1.m1.1.1.3.2.cmml" xref="S3.p2.1.m1.1.1.3.2">1</cn><apply id="S3.p2.1.m1.1.1.3.3.cmml" xref="S3.p2.1.m1.1.1.3.3"><times id="S3.p2.1.m1.1.1.3.3.1.cmml" xref="S3.p2.1.m1.1.1.3.3.1"></times><ci id="S3.p2.1.m1.1.1.3.3.2.cmml" xref="S3.p2.1.m1.1.1.3.3.2">ùê∂</ci><ci id="S3.p2.1.m1.1.1.3.3.3.cmml" xref="S3.p2.1.m1.1.1.3.3.3">ùëá</ci><ci id="S3.p2.1.m1.1.1.3.3.4.cmml" xref="S3.p2.1.m1.1.1.3.3.4">ùê∏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p2.1.m1.1c">F1_{CTE}</annotation></semantics></math> metric as follows:</p>
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.1" class="ltx_Math" alttext="F1_{CTE}=\frac{F1_{Token}+F1_{Link}}{2}=\frac{TP\cdot TR}{TP+TR}+\frac{LP\cdot LR}{LP+LR}." display="block"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml"><mi id="S3.E1.m1.1.1.1.1.2.2" xref="S3.E1.m1.1.1.1.1.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.1" xref="S3.E1.m1.1.1.1.1.2.1.cmml">‚Äã</mo><msub id="S3.E1.m1.1.1.1.1.2.3" xref="S3.E1.m1.1.1.1.1.2.3.cmml"><mn id="S3.E1.m1.1.1.1.1.2.3.2" xref="S3.E1.m1.1.1.1.1.2.3.2.cmml">1</mn><mrow id="S3.E1.m1.1.1.1.1.2.3.3" xref="S3.E1.m1.1.1.1.1.2.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.2.3.3.2" xref="S3.E1.m1.1.1.1.1.2.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.3.3.1" xref="S3.E1.m1.1.1.1.1.2.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.2.3.3.3" xref="S3.E1.m1.1.1.1.1.2.3.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.2.3.3.1a" xref="S3.E1.m1.1.1.1.1.2.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.2.3.3.4" xref="S3.E1.m1.1.1.1.1.2.3.3.4.cmml">E</mi></mrow></msub></mrow><mo id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml">=</mo><mfrac id="S3.E1.m1.1.1.1.1.4" xref="S3.E1.m1.1.1.1.1.4.cmml"><mrow id="S3.E1.m1.1.1.1.1.4.2" xref="S3.E1.m1.1.1.1.1.4.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.4.2.2" xref="S3.E1.m1.1.1.1.1.4.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.4.2.2.2" xref="S3.E1.m1.1.1.1.1.4.2.2.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.2.1" xref="S3.E1.m1.1.1.1.1.4.2.2.1.cmml">‚Äã</mo><msub id="S3.E1.m1.1.1.1.1.4.2.2.3" xref="S3.E1.m1.1.1.1.1.4.2.2.3.cmml"><mn id="S3.E1.m1.1.1.1.1.4.2.2.3.2" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2.cmml">1</mn><mrow id="S3.E1.m1.1.1.1.1.4.2.2.3.3" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.4.2.2.3.3.2" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.2.3.3.1" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.4.2.2.3.3.3" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.3.cmml">o</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.2.3.3.1a" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.4.2.2.3.3.4" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.4.cmml">k</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.2.3.3.1b" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.4.2.2.3.3.5" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.2.3.3.1c" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.4.2.2.3.3.6" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.6.cmml">n</mi></mrow></msub></mrow><mo id="S3.E1.m1.1.1.1.1.4.2.1" xref="S3.E1.m1.1.1.1.1.4.2.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.4.2.3" xref="S3.E1.m1.1.1.1.1.4.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.4.2.3.2" xref="S3.E1.m1.1.1.1.1.4.2.3.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.3.1" xref="S3.E1.m1.1.1.1.1.4.2.3.1.cmml">‚Äã</mo><msub id="S3.E1.m1.1.1.1.1.4.2.3.3" xref="S3.E1.m1.1.1.1.1.4.2.3.3.cmml"><mn id="S3.E1.m1.1.1.1.1.4.2.3.3.2" xref="S3.E1.m1.1.1.1.1.4.2.3.3.2.cmml">1</mn><mrow id="S3.E1.m1.1.1.1.1.4.2.3.3.3" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.4.2.3.3.3.2" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.3.3.3.1" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.4.2.3.3.3.3" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.3.cmml">i</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.3.3.3.1a" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.4.2.3.3.3.4" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.4.cmml">n</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.4.2.3.3.3.1b" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.4.2.3.3.3.5" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.5.cmml">k</mi></mrow></msub></mrow></mrow><mn id="S3.E1.m1.1.1.1.1.4.3" xref="S3.E1.m1.1.1.1.1.4.3.cmml">2</mn></mfrac><mo id="S3.E1.m1.1.1.1.1.5" xref="S3.E1.m1.1.1.1.1.5.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.6" xref="S3.E1.m1.1.1.1.1.6.cmml"><mfrac id="S3.E1.m1.1.1.1.1.6.2" xref="S3.E1.m1.1.1.1.1.6.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.6.2.2" xref="S3.E1.m1.1.1.1.1.6.2.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.6.2.2.2" xref="S3.E1.m1.1.1.1.1.6.2.2.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.6.2.2.2.2" xref="S3.E1.m1.1.1.1.1.6.2.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.6.2.2.2.2.2" xref="S3.E1.m1.1.1.1.1.6.2.2.2.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.2.2.2.2.1" xref="S3.E1.m1.1.1.1.1.6.2.2.2.2.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.6.2.2.2.2.3" xref="S3.E1.m1.1.1.1.1.6.2.2.2.2.3.cmml">P</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.6.2.2.2.1" xref="S3.E1.m1.1.1.1.1.6.2.2.2.1.cmml">‚ãÖ</mo><mi id="S3.E1.m1.1.1.1.1.6.2.2.2.3" xref="S3.E1.m1.1.1.1.1.6.2.2.2.3.cmml">T</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.2.2.1" xref="S3.E1.m1.1.1.1.1.6.2.2.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.6.2.2.3" xref="S3.E1.m1.1.1.1.1.6.2.2.3.cmml">R</mi></mrow><mrow id="S3.E1.m1.1.1.1.1.6.2.3" xref="S3.E1.m1.1.1.1.1.6.2.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.6.2.3.2" xref="S3.E1.m1.1.1.1.1.6.2.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.6.2.3.2.2" xref="S3.E1.m1.1.1.1.1.6.2.3.2.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.2.3.2.1" xref="S3.E1.m1.1.1.1.1.6.2.3.2.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.6.2.3.2.3" xref="S3.E1.m1.1.1.1.1.6.2.3.2.3.cmml">P</mi></mrow><mo id="S3.E1.m1.1.1.1.1.6.2.3.1" xref="S3.E1.m1.1.1.1.1.6.2.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.6.2.3.3" xref="S3.E1.m1.1.1.1.1.6.2.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.6.2.3.3.2" xref="S3.E1.m1.1.1.1.1.6.2.3.3.2.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.2.3.3.1" xref="S3.E1.m1.1.1.1.1.6.2.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.6.2.3.3.3" xref="S3.E1.m1.1.1.1.1.6.2.3.3.3.cmml">R</mi></mrow></mrow></mfrac><mo id="S3.E1.m1.1.1.1.1.6.1" xref="S3.E1.m1.1.1.1.1.6.1.cmml">+</mo><mfrac id="S3.E1.m1.1.1.1.1.6.3" xref="S3.E1.m1.1.1.1.1.6.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.6.3.2" xref="S3.E1.m1.1.1.1.1.6.3.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.6.3.2.2" xref="S3.E1.m1.1.1.1.1.6.3.2.2.cmml"><mrow id="S3.E1.m1.1.1.1.1.6.3.2.2.2" xref="S3.E1.m1.1.1.1.1.6.3.2.2.2.cmml"><mi id="S3.E1.m1.1.1.1.1.6.3.2.2.2.2" xref="S3.E1.m1.1.1.1.1.6.3.2.2.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.3.2.2.2.1" xref="S3.E1.m1.1.1.1.1.6.3.2.2.2.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.6.3.2.2.2.3" xref="S3.E1.m1.1.1.1.1.6.3.2.2.2.3.cmml">P</mi></mrow><mo lspace="0.222em" rspace="0.222em" id="S3.E1.m1.1.1.1.1.6.3.2.2.1" xref="S3.E1.m1.1.1.1.1.6.3.2.2.1.cmml">‚ãÖ</mo><mi id="S3.E1.m1.1.1.1.1.6.3.2.2.3" xref="S3.E1.m1.1.1.1.1.6.3.2.2.3.cmml">L</mi></mrow><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.3.2.1" xref="S3.E1.m1.1.1.1.1.6.3.2.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.6.3.2.3" xref="S3.E1.m1.1.1.1.1.6.3.2.3.cmml">R</mi></mrow><mrow id="S3.E1.m1.1.1.1.1.6.3.3" xref="S3.E1.m1.1.1.1.1.6.3.3.cmml"><mrow id="S3.E1.m1.1.1.1.1.6.3.3.2" xref="S3.E1.m1.1.1.1.1.6.3.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.6.3.3.2.2" xref="S3.E1.m1.1.1.1.1.6.3.3.2.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.3.3.2.1" xref="S3.E1.m1.1.1.1.1.6.3.3.2.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.6.3.3.2.3" xref="S3.E1.m1.1.1.1.1.6.3.3.2.3.cmml">P</mi></mrow><mo id="S3.E1.m1.1.1.1.1.6.3.3.1" xref="S3.E1.m1.1.1.1.1.6.3.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.6.3.3.3" xref="S3.E1.m1.1.1.1.1.6.3.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.6.3.3.3.2" xref="S3.E1.m1.1.1.1.1.6.3.3.3.2.cmml">L</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.1.1.1.1.6.3.3.3.1" xref="S3.E1.m1.1.1.1.1.6.3.3.3.1.cmml">‚Äã</mo><mi id="S3.E1.m1.1.1.1.1.6.3.3.3.3" xref="S3.E1.m1.1.1.1.1.6.3.3.3.3.cmml">R</mi></mrow></mrow></mfrac></mrow></mrow><mo lspace="0em" id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><and id="S3.E1.m1.1.1.1.1a.cmml" xref="S3.E1.m1.1.1.1"></and><apply id="S3.E1.m1.1.1.1.1b.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"></eq><apply id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2"><times id="S3.E1.m1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.1.2.1"></times><ci id="S3.E1.m1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.1.1.1.1.2.2">ùêπ</ci><apply id="S3.E1.m1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.3">subscript</csymbol><cn type="integer" id="S3.E1.m1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.3.2">1</cn><apply id="S3.E1.m1.1.1.1.1.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3.3"><times id="S3.E1.m1.1.1.1.1.2.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.2.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.2.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.2.3.3.2">ùê∂</ci><ci id="S3.E1.m1.1.1.1.1.2.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.2.3.3.3">ùëá</ci><ci id="S3.E1.m1.1.1.1.1.2.3.3.4.cmml" xref="S3.E1.m1.1.1.1.1.2.3.3.4">ùê∏</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.4.cmml" xref="S3.E1.m1.1.1.1.1.4"><divide id="S3.E1.m1.1.1.1.1.4.1.cmml" xref="S3.E1.m1.1.1.1.1.4"></divide><apply id="S3.E1.m1.1.1.1.1.4.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2"><plus id="S3.E1.m1.1.1.1.1.4.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.1"></plus><apply id="S3.E1.m1.1.1.1.1.4.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2"><times id="S3.E1.m1.1.1.1.1.4.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.1"></times><ci id="S3.E1.m1.1.1.1.1.4.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.2">ùêπ</ci><apply id="S3.E1.m1.1.1.1.1.4.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.2.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3">subscript</csymbol><cn type="integer" id="S3.E1.m1.1.1.1.1.4.2.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.2">1</cn><apply id="S3.E1.m1.1.1.1.1.4.2.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3"><times id="S3.E1.m1.1.1.1.1.4.2.2.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.4.2.2.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.2">ùëá</ci><ci id="S3.E1.m1.1.1.1.1.4.2.2.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.3">ùëú</ci><ci id="S3.E1.m1.1.1.1.1.4.2.2.3.3.4.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.4">ùëò</ci><ci id="S3.E1.m1.1.1.1.1.4.2.2.3.3.5.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.5">ùëí</ci><ci id="S3.E1.m1.1.1.1.1.4.2.2.3.3.6.cmml" xref="S3.E1.m1.1.1.1.1.4.2.2.3.3.6">ùëõ</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.4.2.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3"><times id="S3.E1.m1.1.1.1.1.4.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.1"></times><ci id="S3.E1.m1.1.1.1.1.4.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.2">ùêπ</ci><apply id="S3.E1.m1.1.1.1.1.4.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.4.2.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3">subscript</csymbol><cn type="integer" id="S3.E1.m1.1.1.1.1.4.2.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3.2">1</cn><apply id="S3.E1.m1.1.1.1.1.4.2.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3"><times id="S3.E1.m1.1.1.1.1.4.2.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.4.2.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.2">ùêø</ci><ci id="S3.E1.m1.1.1.1.1.4.2.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.3">ùëñ</ci><ci id="S3.E1.m1.1.1.1.1.4.2.3.3.3.4.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.4">ùëõ</ci><ci id="S3.E1.m1.1.1.1.1.4.2.3.3.3.5.cmml" xref="S3.E1.m1.1.1.1.1.4.2.3.3.3.5">ùëò</ci></apply></apply></apply></apply><cn type="integer" id="S3.E1.m1.1.1.1.1.4.3.cmml" xref="S3.E1.m1.1.1.1.1.4.3">2</cn></apply></apply><apply id="S3.E1.m1.1.1.1.1c.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.5.cmml" xref="S3.E1.m1.1.1.1.1.5"></eq><share href="#S3.E1.m1.1.1.1.1.4.cmml" id="S3.E1.m1.1.1.1.1d.cmml" xref="S3.E1.m1.1.1.1"></share><apply id="S3.E1.m1.1.1.1.1.6.cmml" xref="S3.E1.m1.1.1.1.1.6"><plus id="S3.E1.m1.1.1.1.1.6.1.cmml" xref="S3.E1.m1.1.1.1.1.6.1"></plus><apply id="S3.E1.m1.1.1.1.1.6.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2"><divide id="S3.E1.m1.1.1.1.1.6.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2"></divide><apply id="S3.E1.m1.1.1.1.1.6.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2"><times id="S3.E1.m1.1.1.1.1.6.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.1"></times><apply id="S3.E1.m1.1.1.1.1.6.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.2"><ci id="S3.E1.m1.1.1.1.1.6.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.2.1">‚ãÖ</ci><apply id="S3.E1.m1.1.1.1.1.6.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.2.2"><times id="S3.E1.m1.1.1.1.1.6.2.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.2.2.1"></times><ci id="S3.E1.m1.1.1.1.1.6.2.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.2.2.2">ùëá</ci><ci id="S3.E1.m1.1.1.1.1.6.2.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.2.2.3">ùëÉ</ci></apply><ci id="S3.E1.m1.1.1.1.1.6.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.2.3">ùëá</ci></apply><ci id="S3.E1.m1.1.1.1.1.6.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.2.3">ùëÖ</ci></apply><apply id="S3.E1.m1.1.1.1.1.6.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3"><plus id="S3.E1.m1.1.1.1.1.6.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3.1"></plus><apply id="S3.E1.m1.1.1.1.1.6.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3.2"><times id="S3.E1.m1.1.1.1.1.6.2.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3.2.1"></times><ci id="S3.E1.m1.1.1.1.1.6.2.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3.2.2">ùëá</ci><ci id="S3.E1.m1.1.1.1.1.6.2.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3.2.3">ùëÉ</ci></apply><apply id="S3.E1.m1.1.1.1.1.6.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3.3"><times id="S3.E1.m1.1.1.1.1.6.2.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.6.2.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3.3.2">ùëá</ci><ci id="S3.E1.m1.1.1.1.1.6.2.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.6.2.3.3.3">ùëÖ</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.6.3.cmml" xref="S3.E1.m1.1.1.1.1.6.3"><divide id="S3.E1.m1.1.1.1.1.6.3.1.cmml" xref="S3.E1.m1.1.1.1.1.6.3"></divide><apply id="S3.E1.m1.1.1.1.1.6.3.2.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2"><times id="S3.E1.m1.1.1.1.1.6.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2.1"></times><apply id="S3.E1.m1.1.1.1.1.6.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2.2"><ci id="S3.E1.m1.1.1.1.1.6.3.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2.2.1">‚ãÖ</ci><apply id="S3.E1.m1.1.1.1.1.6.3.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2.2.2"><times id="S3.E1.m1.1.1.1.1.6.3.2.2.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2.2.2.1"></times><ci id="S3.E1.m1.1.1.1.1.6.3.2.2.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2.2.2.2">ùêø</ci><ci id="S3.E1.m1.1.1.1.1.6.3.2.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2.2.2.3">ùëÉ</ci></apply><ci id="S3.E1.m1.1.1.1.1.6.3.2.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2.2.3">ùêø</ci></apply><ci id="S3.E1.m1.1.1.1.1.6.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.3.2.3">ùëÖ</ci></apply><apply id="S3.E1.m1.1.1.1.1.6.3.3.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3"><plus id="S3.E1.m1.1.1.1.1.6.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3.1"></plus><apply id="S3.E1.m1.1.1.1.1.6.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3.2"><times id="S3.E1.m1.1.1.1.1.6.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3.2.1"></times><ci id="S3.E1.m1.1.1.1.1.6.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3.2.2">ùêø</ci><ci id="S3.E1.m1.1.1.1.1.6.3.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3.2.3">ùëÉ</ci></apply><apply id="S3.E1.m1.1.1.1.1.6.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3.3"><times id="S3.E1.m1.1.1.1.1.6.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.6.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3.3.2">ùêø</ci><ci id="S3.E1.m1.1.1.1.1.6.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.6.3.3.3.3">ùëÖ</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">F1_{CTE}=\frac{F1_{Token}+F1_{Link}}{2}=\frac{TP\cdot TR}{TP+TR}+\frac{LP\cdot LR}{LP+LR}.</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p3" class="ltx_para ltx_noindent">
<p id="S3.p3.1" class="ltx_p"><span id="S3.p3.1.1" class="ltx_text ltx_font_bold">Token classification
<br class="ltx_break"></span>The first step required to tackle CTE is the classification of tokens, extracted from PDF pages using PyMuPDF. Tokens contain textual and positional information, along with class information inherited from the larger region they belong to (details in Table <a href="#S2.T2.st3" title="In 2.3 Dataset structure and format ‚Ä£ 2 Dataset Description ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2(c)</span></a>, tokens annotations). This subtask exposes these properties:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I1.i1.p1" class="ltx_para">
<p id="S3.I1.i1.p1.1" class="ltx_p">Through token classification it is possible to achieve DLA, TD, and TFA at once.</p>
</div>
</li>
<li id="S3.I1.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I1.i2.p1" class="ltx_para">
<p id="S3.I1.i2.p1.1" class="ltx_p">If tackled along with link classification to achieve CTE the <math id="S3.I1.i2.p1.1.m1.1" class="ltx_Math" alttext="F1_{CTE}" display="inline"><semantics id="S3.I1.i2.p1.1.m1.1a"><mrow id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.1.m1.1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml"><mn id="S3.I1.i2.p1.1.m1.1.1.3.2" xref="S3.I1.i2.p1.1.m1.1.1.3.2.cmml">1</mn><mrow id="S3.I1.i2.p1.1.m1.1.1.3.3" xref="S3.I1.i2.p1.1.m1.1.1.3.3.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.3.3.2" xref="S3.I1.i2.p1.1.m1.1.1.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.1.m1.1.1.3.3.1" xref="S3.I1.i2.p1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.I1.i2.p1.1.m1.1.1.3.3.3" xref="S3.I1.i2.p1.1.m1.1.1.3.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.I1.i2.p1.1.m1.1.1.3.3.1a" xref="S3.I1.i2.p1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.I1.i2.p1.1.m1.1.1.3.3.4" xref="S3.I1.i2.p1.1.m1.1.1.3.3.4.cmml">E</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><times id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.1"></times><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">ùêπ</ci><apply id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3">subscript</csymbol><cn type="integer" id="S3.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.2">1</cn><apply id="S3.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.3"><times id="S3.I1.i2.p1.1.m1.1.1.3.3.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.3.1"></times><ci id="S3.I1.i2.p1.1.m1.1.1.3.3.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.3.2">ùê∂</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.3.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.3.3">ùëá</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.3.4.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.3.4">ùê∏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">F1_{CTE}</annotation></semantics></math> metric (Eq.¬†<a href="#S3.E1" title="In 3 Contextualized Table Extraction ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>) should be used. Instead, if tackled alone the metric proposed in <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> can be used as well.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p4" class="ltx_para ltx_noindent">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text ltx_font_bold">Link Classification
<br class="ltx_break"></span>In order to group together tokens belonging to tables into columns, rows, or grid cells, additional information on links among pairs of tokens is added. This subtask exposes these properties:</p>
<ol id="S3.I2" class="ltx_enumerate">
<li id="S3.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S3.I2.i1.p1" class="ltx_para">
<p id="S3.I2.i1.p1.1" class="ltx_p">Through link classification it is possible to perform TSR.</p>
</div>
</li>
<li id="S3.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S3.I2.i2.p1" class="ltx_para">
<p id="S3.I2.i2.p1.1" class="ltx_p">Similarly to token classification, F1 is preferred to evaluate link classification if tackled alone.</p>
</div>
</li>
<li id="S3.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S3.I2.i3.p1" class="ltx_para">
<p id="S3.I2.i3.p1.1" class="ltx_p">Links connecting non-tables items should be considered as an additional class ‚Äô<span id="S3.I2.i3.p1.1.1" class="ltx_text ltx_font_italic">none</span>‚Äô.</p>
</div>
</li>
</ol>
</div>
<div id="S3.p5" class="ltx_para ltx_noindent">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text ltx_font_bold">Object Recognition
<br class="ltx_break"></span>Even if not required to do CTE, the annotations include area information of different regions in the paper (as common for object detection). Grouping together tokens belonging to the same class via edges can be exploited to find such areas, e.g. extracting sub-graphs from the whole document. A recent paper¬†<cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib18" title="" class="ltx_ref">18</a>]</cite> exploited GNN to perform post-OCR paragraph recognition by grouping together similar items in the pages.</p>
</div>
<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Limitation of the Task</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">While we acknowledge that CTE has some limitations, we believe that it represents a significant step towards a more comprehensive solution for table extraction in documents.
In our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib14" title="" class="ltx_ref">14</a>]</cite>, we investigated different ways to achieve CTE through ablation studies, so as to analyze the impact of different components on the system‚Äôs performances. In this paper, we define a metric, (<math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="F1_{CTE}" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mrow id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.p1.1.m1.1.1.2" xref="S3.SS1.p1.1.m1.1.1.2.cmml">F</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.1" xref="S3.SS1.p1.1.m1.1.1.1.cmml">‚Äã</mo><msub id="S3.SS1.p1.1.m1.1.1.3" xref="S3.SS1.p1.1.m1.1.1.3.cmml"><mn id="S3.SS1.p1.1.m1.1.1.3.2" xref="S3.SS1.p1.1.m1.1.1.3.2.cmml">1</mn><mrow id="S3.SS1.p1.1.m1.1.1.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.cmml"><mi id="S3.SS1.p1.1.m1.1.1.3.3.2" xref="S3.SS1.p1.1.m1.1.1.3.3.2.cmml">C</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.3" xref="S3.SS1.p1.1.m1.1.1.3.3.3.cmml">T</mi><mo lspace="0em" rspace="0em" id="S3.SS1.p1.1.m1.1.1.3.3.1a" xref="S3.SS1.p1.1.m1.1.1.3.3.1.cmml">‚Äã</mo><mi id="S3.SS1.p1.1.m1.1.1.3.3.4" xref="S3.SS1.p1.1.m1.1.1.3.3.4.cmml">E</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><apply id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1"><times id="S3.SS1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1.1"></times><ci id="S3.SS1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.p1.1.m1.1.1.2">ùêπ</ci><apply id="S3.SS1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3">subscript</csymbol><cn type="integer" id="S3.SS1.p1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.2">1</cn><apply id="S3.SS1.p1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3"><times id="S3.SS1.p1.1.m1.1.1.3.3.1.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.1"></times><ci id="S3.SS1.p1.1.m1.1.1.3.3.2.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.2">ùê∂</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.3">ùëá</ci><ci id="S3.SS1.p1.1.m1.1.1.3.3.4.cmml" xref="S3.SS1.p1.1.m1.1.1.3.3.4">ùê∏</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">F1_{CTE}</annotation></semantics></math>), for the updated dataset regarding CTE. As the combination of two metrics, namely Token F1 and Link F1, they can be used to evaluate the performance of the system.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Future work</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p">In addition to providing a new dataset for contextualized table extraction, the CTE task can also serve as a basis for future research. One area of research is to investigate the effectiveness of using graph neural networks (GNNs) versus transformer architectures for the CTE task. The models might be pre-trained and fine-tuned on all the original data from <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib11" title="" class="ltx_ref">11</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>. Comparing a lightweight network, GNN-based, with a heavy network, such as transformer-based, can help determine which approach is best suited for the CTE task.
Another potential avenue for future work is to investigate the use of the CTE dataset for information extraction tasks, specifically in the context of scientific papers. Many papers include tables with important information that can be challenging to extract automatically, and incorporating external knowledge bases could further improve performance. With the CTE dataset, it would be possible to explore how to effectively combine table structure information with external knowledge to answer questions based on scientific papers.
Other open research questions that could be addressed using the CTE dataset include investigating cross-lingual performance, transfer learning, and developing techniques to handle different types of tables (e.g., nested tables, tables with merged cells).</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">In this work we presented a new dataset to tackle the task of Contextualized Table Extraction. The dataset is obtained by merging two well-known benchmark datasets (PubTables-1M and PubLayNet). Usually, table extraction pipelines involve several components to perform different tasks on tables, without considering other important information present in the document such as captions. Based on these limitations, the proposed collection of data aims at developing models capable of tackling more tasks at once, resulting in CTE. Moreover, the annotations format encourages the development of systems based on GNN, that lack of a common benchmark within the DAR community for tasks different from TSR. We are looking to extend the dataset by adding more information such as authors, keywords, and equations.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marinai [2013]</span>
<span class="ltx_bibblock">
S.¬†Marinai,

</span>
<span class="ltx_bibblock">Learning algorithms for document layout analysis,

</span>
<span class="ltx_bibblock">in: C.¬†Rao, V.¬†Govindaraju
(Eds.), Handbook of Statistics,
volume¬†31 of <span id="bib.bib1.1.1" class="ltx_text ltx_font_italic">Handbook of
Statistics</span>, Elsevier, .,
2013, pp. 400‚Äì419.
doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/B978-0-444-53859-8.00016-3" title="" class="ltx_ref">https://doi.org/10.1016/B978-0-444-53859-8.00016-3</a>.

</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hashmi et¬†al. [2021]</span>
<span class="ltx_bibblock">
K.¬†A. Hashmi, M.¬†Liwicki,
D.¬†Stricker, M.¬†A. Afzal,
M.¬†A. Afzal, M.¬†Z. Afzal,

</span>
<span class="ltx_bibblock">Current status and performance analysis of table
recognition in document images with deep neural networks,

</span>
<span class="ltx_bibblock">IEEE Access 9
(2021) 87663‚Äì87685. URL: <a target="_blank" href="https://doi.org/10.1109/ACCESS.2021.3087865" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ACCESS.2021.3087865</a>.
doi:<a target="_blank" href="https:/doi.org/10.1109/ACCESS.2021.3087865" title="" class="ltx_ref">10.1109/ACCESS.2021.3087865</a>.

</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kardas et¬†al. [2020]</span>
<span class="ltx_bibblock">
M.¬†Kardas, P.¬†Czapla,
P.¬†Stenetorp, S.¬†Ruder,
S.¬†Riedel, R.¬†Taylor,
R.¬†Stojnic,

</span>
<span class="ltx_bibblock">Axcell: Automatic extraction of results from machine
learning papers,

</span>
<span class="ltx_bibblock">in: B.¬†Webber, T.¬†Cohn,
Y.¬†He, Y.¬†Liu (Eds.),
Proceedings of the 2020 Conference on Empirical Methods
in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020,
Association for Computational Linguistics,
2020, pp. 8580‚Äì8594. URL: <a target="_blank" href="https://doi.org/10.18653/v1/2020.emnlp-main.692" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2020.emnlp-main.692</a>.
doi:<a target="_blank" href="https:/doi.org/10.18653/v1/2020.emnlp-main.692" title="" class="ltx_ref">10.18653/v1/2020.emnlp-main.692</a>.

</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et¬†al. [2019]</span>
<span class="ltx_bibblock">
X.¬†Zhong, J.¬†Tang,
A.¬†Jimeno-Yepes,

</span>
<span class="ltx_bibblock">Publaynet: Largest dataset ever for document layout
analysis,

</span>
<span class="ltx_bibblock">in: 2019 International Conference on Document
Analysis and Recognition, ICDAR 2019, Sydney, Australia, September 20-25,
2019, IEEE, 2019, pp.
1015‚Äì1022. URL: <a target="_blank" href="https://doi.org/10.1109/ICDAR.2019.00166" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICDAR.2019.00166</a>.
doi:<a target="_blank" href="https:/doi.org/10.1109/ICDAR.2019.00166" title="" class="ltx_ref">10.1109/ICDAR.2019.00166</a>.

</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Smock et¬†al. [2021]</span>
<span class="ltx_bibblock">
B.¬†Smock, R.¬†Pesala,
R.¬†Abraham,

</span>
<span class="ltx_bibblock">PubTables-1M: Towards a universal dataset and
metrics for training and evaluating table extraction models,

</span>
<span class="ltx_bibblock">CoRR abs/2110.00061
(2021). URL: <a target="_blank" href="https://arxiv.org/abs/2110.00061" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2110.00061</a>.
<a target="_blank" href="http://arxiv.org/abs/2110.00061" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:2110.00061</a>.

</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Siegel et¬†al. [2016]</span>
<span class="ltx_bibblock">
N.¬†Siegel, Z.¬†Horvitz,
R.¬†Levin, S.¬†Divvala,
A.¬†Farhadi,

</span>
<span class="ltx_bibblock">Figureseer: Parsing result-figures in research
papers,

</span>
<span class="ltx_bibblock">in: European Conference on Computer Vision
(ECCV), 2016.

</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pfitzmann et¬†al. [2022]</span>
<span class="ltx_bibblock">
B.¬†Pfitzmann, C.¬†Auer,
M.¬†Dolfi, A.¬†S. Nassar,
P.¬†W.¬†J. Staar,

</span>
<span class="ltx_bibblock">Doclaynet: A large human-annotated dataset for
document-layout analysis (2022). URL: <a target="_blank" href="https://arxiv.org/abs/2206.01062" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://arxiv.org/abs/2206.01062</a>.
doi:<a target="_blank" href="https:/doi.org/10.1145/3534678.353904" title="" class="ltx_ref">10.1145/3534678.353904</a>.

</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et¬†al. [2020]</span>
<span class="ltx_bibblock">
M.¬†Li, Y.¬†Xu, L.¬†Cui,
S.¬†Huang, F.¬†Wei,
Z.¬†Li, M.¬†Zhou,

</span>
<span class="ltx_bibblock">Docbank: A benchmark dataset for document layout
analysis,

</span>
<span class="ltx_bibblock">in: D.¬†Scott, N.¬†Bel,
C.¬†Zong (Eds.), Proceedings of the 28th
International Conference on Computational Linguistics, COLING 2020,
Barcelona, Spain (Online), December 8-13, 2020,
International Committee on Computational Linguistics,
2020, pp. 949‚Äì960. URL: <a target="_blank" href="https://doi.org/10.18653/v1/2020.coling-main.82" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.18653/v1/2020.coling-main.82</a>.
doi:<a target="_blank" href="https:/doi.org/10.18653/v1/2020.coling-main.82" title="" class="ltx_ref">10.18653/v1/2020.coling-main.82</a>.

</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ren et¬†al. [2015]</span>
<span class="ltx_bibblock">
S.¬†Ren, K.¬†He, R.¬†B.
Girshick, J.¬†Sun,

</span>
<span class="ltx_bibblock">Faster R-CNN: towards real-time object detection
with region proposal networks,

</span>
<span class="ltx_bibblock">in: C.¬†Cortes, N.¬†D. Lawrence,
D.¬†D. Lee, M.¬†Sugiyama,
R.¬†Garnett (Eds.), Advances in Neural
Information Processing Systems 28: Annual Conference on Neural Information
Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada,
2015, pp. 91‚Äì99. URL: <a target="_blank" href="https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html</a>.

</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">He et¬†al. [2017]</span>
<span class="ltx_bibblock">
K.¬†He, G.¬†Gkioxari,
P.¬†Doll√°r, R.¬†B. Girshick,

</span>
<span class="ltx_bibblock">Mask R-CNN,

</span>
<span class="ltx_bibblock">in: IEEE International Conference on Computer
Vision, ICCV 2017, Venice, Italy, October 22-29, 2017,
IEEE Computer Society, 2017, pp.
2980‚Äì2988. URL: <a target="_blank" href="https://doi.org/10.1109/ICCV.2017.322" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICCV.2017.322</a>.
doi:<a target="_blank" href="https:/doi.org/10.1109/ICCV.2017.322" title="" class="ltx_ref">10.1109/ICCV.2017.322</a>.

</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et¬†al. [2019]</span>
<span class="ltx_bibblock">
Y.¬†Xu, M.¬†Li, L.¬†Cui,
S.¬†Huang, F.¬†Wei,
M.¬†Zhou,

</span>
<span class="ltx_bibblock">Layoutlm: Pre-training of text and layout for
document image understanding,

</span>
<span class="ltx_bibblock">CoRR abs/1912.13318
(2019). URL: <a target="_blank" href="http://arxiv.org/abs/1912.13318" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1912.13318</a>.
<a target="_blank" href="http://arxiv.org/abs/1912.13318" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1912.13318</a>.

</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chi et¬†al. [2019]</span>
<span class="ltx_bibblock">
Z.¬†Chi, H.¬†Huang, H.¬†Xu,
H.¬†Yu, W.¬†Yin, X.¬†Mao,

</span>
<span class="ltx_bibblock">Complicated table structure recognition,

</span>
<span class="ltx_bibblock">CoRR abs/1908.04729
(2019). URL: <a target="_blank" href="http://arxiv.org/abs/1908.04729" title="" class="ltx_ref ltx_url ltx_font_typewriter">http://arxiv.org/abs/1908.04729</a>.
<a target="_blank" href="http://arxiv.org/abs/1908.04729" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1908.04729</a>.

</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemelli et¬†al. [2022a]</span>
<span class="ltx_bibblock">
A.¬†Gemelli, S.¬†Biswas,
E.¬†Civitelli, J.¬†Llad√≥s,
S.¬†Marinai,

</span>
<span class="ltx_bibblock">Doc2graph: a task agnostic document understanding
framework based on graph neural networks,

</span>
<span class="ltx_bibblock">arXiv preprint arXiv:2208.11168
(2022a).

</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gemelli et¬†al. [2022b]</span>
<span class="ltx_bibblock">
A.¬†Gemelli, E.¬†Vivoli,
S.¬†Marinai,

</span>
<span class="ltx_bibblock">Graph neural networks and representation embedding
for table extraction in PDF documents,

</span>
<span class="ltx_bibblock">in: 26th International Conference on Pattern
Recognition, ICPR 2022, Montreal, QC, Canada, August 21-25, 2022,
IEEE, 2022b, pp.
1719‚Äì1726. URL: <a target="_blank" href="https://doi.org/10.1109/ICPR56361.2022.9956590" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/ICPR56361.2022.9956590</a>.
doi:<a target="_blank" href="https:/doi.org/10.1109/ICPR56361.2022.9956590" title="" class="ltx_ref">10.1109/ICPR56361.2022.9956590</a>.

</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">PyMuPDF and McKie [2012]</span>
<span class="ltx_bibblock">
PyMuPDF, J.¬†X. McKie,
Pymupdf: Python bindings for mupdf‚Äôs rendering library.,
<a target="_blank" href="https://github.com/pymupdf/PyMuPDF" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/pymupdf/PyMuPDF</a>,
2012.

</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pisaneschi et¬†al. [2023]</span>
<span class="ltx_bibblock">
L.¬†Pisaneschi, A.¬†Gemelli,
S.¬†Marinai,

</span>
<span class="ltx_bibblock">Automatic generation of scientific papers for data
augmentation in document layout analysis,

</span>
<span class="ltx_bibblock">Pattern Recognition Letters 167
(2023) 38‚Äì44. URL: <a target="_blank" href="https://www.sciencedirect.com/science/article/pii/S0167865523000247" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://www.sciencedirect.com/science/article/pii/S0167865523000247</a>.
doi:<a target="_blank" href="https:/doi.org/https://doi.org/10.1016/j.patrec.2023.01.018" title="" class="ltx_ref">https://doi.org/10.1016/j.patrec.2023.01.018</a>.

</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">GROBID [2021]</span>
<span class="ltx_bibblock">
GROBID, Grobid,
<a target="_blank" href="https://github.com/kermitt2/grobid" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://github.com/kermitt2/grobid</a>,
2008‚Äì2021.
<a target="_blank" href="http://arxiv.org/abs/1:dir:dab86b296e3c3216e2241968f0d63b68e8209d3c" title="" class="ltx_ref ltx_href ltx_font_typewriter">arXiv:1:dir:dab86b296e3c3216e2241968f0d63b68e8209d3c</a>.

</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et¬†al. [2022]</span>
<span class="ltx_bibblock">
R.¬†Wang, Y.¬†Fujii, A.¬†C.
Popat,

</span>
<span class="ltx_bibblock">Post-ocr paragraph recognition by graph convolutional
networks,

</span>
<span class="ltx_bibblock">in: IEEE/CVF Winter Conference on Applications
of Computer Vision, WACV 2022, Waikoloa, HI, USA, January 3-8, 2022,
IEEE, 2022, pp.
2533‚Äì2542. URL: <a target="_blank" href="https://doi.org/10.1109/WACV51458.2022.00259" title="" class="ltx_ref ltx_url ltx_font_typewriter">https://doi.org/10.1109/WACV51458.2022.00259</a>.
doi:<a target="_blank" href="https:/doi.org/10.1109/WACV51458.2022.00259" title="" class="ltx_ref">10.1109/WACV51458.2022.00259</a>.

</span>
</li>
</ul>
</section>
<figure id="S5.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F2.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2302.01451/assets/esempi/tokens.png" id="S5.F2.sf1.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="798" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf1.2.1.1" class="ltx_text" style="font-size:90%;">(a)</span> </span><span id="S5.F2.sf1.3.2" class="ltx_text" style="font-size:90%;">Spanning rows.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F2.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2302.01451/assets/esempi/tokens2.png" id="S5.F2.sf2.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="794" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf2.2.1.1" class="ltx_text" style="font-size:90%;">(b)</span> </span><span id="S5.F2.sf2.3.2" class="ltx_text" style="font-size:90%;">More out-column tables.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F2.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2302.01451/assets/esempi/tokens4.png" id="S5.F2.sf3.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="846" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf3.2.1.1" class="ltx_text" style="font-size:90%;">(c)</span> </span><span id="S5.F2.sf3.3.2" class="ltx_text" style="font-size:90%;">Single page layout.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F2.sf4" class="ltx_figure ltx_figure_panel"><img src="/html/2302.01451/assets/esempi/tokens5.png" id="S5.F2.sf4.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="789" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf4.2.1.1" class="ltx_text" style="font-size:90%;">(d)</span> </span><span id="S5.F2.sf4.3.2" class="ltx_text" style="font-size:90%;">Title page.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F2.sf5" class="ltx_figure ltx_figure_panel"><img src="/html/2302.01451/assets/esempi/tokens7.png" id="S5.F2.sf5.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="794" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf5.2.1.1" class="ltx_text" style="font-size:90%;">(e)</span> </span><span id="S5.F2.sf5.3.2" class="ltx_text" style="font-size:90%;">More images per page.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F2.sf6" class="ltx_figure ltx_figure_panel"><img src="/html/2302.01451/assets/esempi/tokens8.png" id="S5.F2.sf6.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="797" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf6.2.1.1" class="ltx_text" style="font-size:90%;">(f)</span> </span><span id="S5.F2.sf6.3.2" class="ltx_text" style="font-size:90%;">Formulas labeled as others.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F2.sf7" class="ltx_figure ltx_figure_panel"><img src="/html/2302.01451/assets/esempi/tokens9.png" id="S5.F2.sf7.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="797" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf7.2.1.1" class="ltx_text" style="font-size:90%;">(g)</span> </span><span id="S5.F2.sf7.3.2" class="ltx_text" style="font-size:90%;">List example.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F2.sf8" class="ltx_figure ltx_figure_panel"><img src="/html/2302.01451/assets/esempi/tokens11.png" id="S5.F2.sf8.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="774" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf8.2.1.1" class="ltx_text" style="font-size:90%;">(h)</span> </span><span id="S5.F2.sf8.3.2" class="ltx_text" style="font-size:90%;">Full page table.</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="S5.F2.sf9" class="ltx_figure ltx_figure_panel"><img src="/html/2302.01451/assets/esempi/tokens12.png" id="S5.F2.sf9.g1" class="ltx_graphics ltx_centering ltx_img_portrait" width="598" height="797" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.sf9.2.1.1" class="ltx_text" style="font-size:90%;">(i)</span> </span><span id="S5.F2.sf9.3.2" class="ltx_text" style="font-size:90%;">More in-column tables.</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S5.F2.2.1.1" class="ltx_text" style="font-size:90%;">Figure 2</span>: </span><span id="S5.F2.3.2" class="ltx_text" style="font-size:90%;">Examples of labeled pages showing the different layouts available in the dataset.
There can be more tables and images per page (b, i, e) either aligned or not with columns.
There are single-column pages (c).
As mentioned in Section <a href="#S2.SS4" title="2.4 Limitations of the Dataset ‚Ä£ 2 Dataset Description ‚Ä£ CTE: A Dataset for Contextualized Table Extraction" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.4</span></a>, there are some limitations: equations not labeled (f), missing keywords, authors, abstract information (d), and subtitles that are not distinguished from the paper title.</span></figcaption>
</figure>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2302.01450" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2302.01451" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2302.01451">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2302.01451" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2302.01452" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Fri Mar  1 03:25:45 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
