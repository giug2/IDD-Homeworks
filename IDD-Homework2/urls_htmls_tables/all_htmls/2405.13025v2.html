<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>A survey on fairness of large language models in e-commerce: Progress, application, and challenge</title>
<!--Generated on Fri Jun 21 21:26:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2405.13025v2/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S1" title="In A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S2" title="In A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>The Principles of E-commerce LLMs</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S2.SS1" title="In 2 The Principles of E-commerce LLMs ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Pre-training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S2.SS2" title="In 2 The Principles of E-commerce LLMs ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Fine-tuning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S2.SS3" title="In 2 The Principles of E-commerce LLMs ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Prompt-tuning</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3" title="In A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Bias challenges</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS1" title="In 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Introduction to Fairness and Bias in LLM, E-commerce</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS2" title="In 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>What is fairness and bias?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS3" title="In 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>The challenges and biases in LLM</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS3.SSS1" title="In 3.3 The challenges and biases in LLM ‣ 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.1 </span>Gender bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS3.SSS2" title="In 3.3 The challenges and biases in LLM ‣ 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.2 </span>Racial bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS3.SSS3" title="In 3.3 The challenges and biases in LLM ‣ 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3.3 </span>Other types of social bias</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS4" title="In 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>What are the challenges and biases in e-commerce?</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS4.SSS1" title="In 3.4 What are the challenges and biases in e-commerce? ‣ 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.1 </span>Visibility and accessibility: popularity, exposure, and recommendation bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS4.SSS2" title="In 3.4 What are the challenges and biases in e-commerce? ‣ 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.2 </span>Search bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS4.SSS3" title="In 3.4 What are the challenges and biases in e-commerce? ‣ 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4.3 </span>Description bias</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS5" title="In 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>What are the challenges and biases in the application of LLM in the e-commerce field?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS6" title="In 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>Impacts of bias</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S3.SS7" title="In 3 Bias challenges ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.7 </span>Current approaches</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4" title="In A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>E-commerce Application</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.SS1" title="In 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Product Recommendation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.SS2" title="In 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Product Search</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.SS3" title="In 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Product Information Summary</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.SS4" title="In 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.4 </span>Product Information Translation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.SS5" title="In 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.5 </span>Product Information Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.SS6" title="In 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.6 </span>Product Q&amp;A</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.SS7" title="In 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7 </span>Fairness Analysis in E-commerce Application</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.SS7.SSS1" title="In 4.7 Fairness Analysis in E-commerce Application ‣ 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7.1 </span>Intrinsic Bias Evaluation Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.SS7.SSS2" title="In 4.7 Fairness Analysis in E-commerce Application ‣ 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.7.2 </span>Extrinsic Evaluation Metrics</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S5" title="In A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Future direction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S6" title="In A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">A survey on fairness of large language models in e-commerce: Progress, application, and challenge</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Qingyang Ren
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Department of Computer Science, Cornell Univerisity
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zilin Jiang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Carnegie Mellon University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinghan Cao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">San Francisco State University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sijia Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Carnegie Mellon University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chiqu Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Columbia University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yiyang Liu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">University of Southern California
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shuning Huo
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">Virginia Tech
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Tiange He
</span><span class="ltx_author_notes">Author contact Information: qr23@cornell.edu (Qingyang Ren), zilinjia@alumni.cmu.edu (Zilin Jiang), jcao3@alumni.sfsu.edu (Jinghan Cao), sijiali@alumni.cmu.edu (Sijia Li), chiqu.l@columbia.edu (Chiqu Li), ianl@alumni.usc.edu (Yiyang Liu), shuni93@vt.edu (Shuning Huo), he.ti@northeastern.edu (Tiange He)
<span class="ltx_contact ltx_role_affiliation">Khoury College of Computer Sciences, Northeastern University
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yuan Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">New York University
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">This survey explores the fairness of large language models (LLMs) in e-commerce, examining their progress, applications, and the challenges they face. LLMs have become pivotal in the e-commerce domain, offering innovative solutions and enhancing customer experiences. This work presents a comprehensive survey on the applications and challenges of LLMs in e-commerce.</p>
<p class="ltx_p" id="id2.id2">The paper begins by introducing the key principles underlying the use of LLMs in e-commerce, detailing the processes of pretraining, fine-tuning, and prompting that tailor these models to specific needs. It then explores the varied applications of LLMs in e-commerce, including product reviews, where they synthesize and analyze customer feedback; product recommendations, where they leverage consumer data to suggest relevant items; product information translation, enhancing global accessibility; and product question and answer sections, where they automate customer support.</p>
<p class="ltx_p" id="id3.id3">The paper critically addresses the fairness challenges in e-commerce, highlighting how biases in training data and algorithms can lead to unfair outcomes, such as reinforcing stereotypes or discriminating against certain groups. These issues not only undermine consumer trust, but also raise ethical and legal concerns.</p>
<p class="ltx_p" id="id4.id4">Finally, the work outlines future research directions, emphasizing the need for more equitable and transparent LLMs in e-commerce. It advocates for ongoing efforts to mitigate biases and improve the fairness of these systems, ensuring they serve diverse global markets effectively and ethically. Through this comprehensive analysis, the survey provides a holistic view of the current landscape of LLMs in e-commerce, offering insights into their potential and limitations, and guiding future endeavors in creating fairer and more inclusive e-commerce environments.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The rapid advancement of LLMs has initiated a new era of natural language processing (NLP), revolutionizing various fields with their remarkable capabilities. Among these domains, e-commerce has emerged as a promising arena for the application of LLMs, offering innovative solutions and enhancing customer experiences. This survey investigates the fairness of LLMs in the e-commerce landscape, exploring their progress, applications, and the challenges they face.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The emergence of general LLMs, such as LLaMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib1" title="">1</a>]</cite>, the GPT series <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib3" title="">3</a>]</cite>, and Claude <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib5" title="">5</a>]</cite>, has set new benchmarks in NLP tasks, including text classification, summarization, and question answering. Inspired by their remarkable success in general domains, the e-commerce sector has witnessed the rise of specialized LLMs tailored to its unique needs, such as understanding consumer behavior, optimizing search and recommendation systems, and automating content creation for product listings and marketing materials. Notable examples include EComGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib6" title="">6</a>]</cite> and E-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib7" title="">7</a>]</cite>, which have gained growing research interest in resolving pain points in both shopping and retailing experiences.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Despite the promising results achieved by existing e-commerce LLMs, several limitations and challenges remain to be addressed. These research gaps motivate the need for a comprehensive review that examines the fairness of LLMs in the e-commerce domain. There are concerns over existing e-commerce LLM regarding their potential to perpetuate harm. These models are typically trained on vast amounts of uncurated data sourced from the Internet, which can result in the inheritance of stereotypes, misrepresentations, derogatory language, and exclusionary behaviors. LLMs not only reflect existing biases but can also amplify them, leading to the automated perpetuation of injustice and the reinforcement of inequitable systems. The presence of biases in LLMs has been extensively documented, encompassing negative sentiment and toxicity towards specific social groups, stereotypical language associations, and a lack of recognition for certain language dialects <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib9" title="">9</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_tag">1</span></a>, this survey is organized as follows: Section 2 summarizes the principles underlying the development of existing e-commerce LLMs, detailing the processes of pretraining, fine-tuning, and prompting that tailor these models to specific e-commerce needs. Section 3 describes the common fairness challenges faced by both general and e-commerce LLMs, shedding light on the potential biases and discriminatory outcomes that can arise from the training data and algorithms employed.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Section 4 outlines the outstanding applications of e-commerce LLMs, showcasing their versatility in areas such as product reviews, where they synthesize and analyze customer feedback; product recommendations, where they leverage consumer data to suggest relevant items; product information translation, enhancing global accessibility; and product question and answer sections, where they automate customer support. However, this section also critically examines the specific fairness challenges that arise within each application domain, highlighting the potential for biases and unfair outcomes that can undermine consumer trust and raise ethical and legal concerns.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Finally, Section 5 explores future research directions, emphasizing the need for more equitable and transparent LLMs in e-commerce. It advocates for ongoing efforts to mitigate biases and improve the fairness of these systems, ensuring they serve diverse global markets effectively and ethically. Through rigorous evaluation and continuous improvement, e-commerce LLMs can foster inclusive and trustworthy online shopping experiences, benefiting both consumers and businesses alike.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S1.F1.g1" src="extracted/5684480/figures/graph.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>An overview of the fairness of e-commerce LLMs</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Through this comprehensive analysis, the survey provides a holistic view of the current landscape of LLMs in e-commerce, offering insights into their potential and limitations, and guiding future endeavors in creating fairer and more inclusive e-commerce environments. By addressing the fairness challenges directly and promoting responsible development and deployment of LLMs, the e-commerce sector can harness the full potential of these powerful models while upholding ethical principles and safeguarding consumer rights.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>The Principles of E-commerce LLMs</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">LLM training comprises three major different approaches: pre-training, fine-tuning and prompting. Given the system complexity in E-commerce domain, the relevant research is shifting from applying a single model training to an integration of multiple LLM model, of which is trained for specific tasks in a larger system.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Summary of existing LLMs in E-commerce</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S2.T1.1" style="width:433.6pt;height:211.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-356.3pt,173.4pt) scale(0.378330383264516,0.378330383264516) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1.1">Domain</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1.2">Model Type</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1.3">Model</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1.4">Base</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1.5">Param</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.1.1.1.1.6">Data Source</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.2.1.1">General</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.2.1.2">Pre-training</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.2.1.3">ALBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib10" title="">10</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.2.1.4">BERT</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.2.1.5">12M/18M/60M/235M</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.2.1.6">BooksCorpus, English Wikipedia</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.3.2">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.3.2.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.3.2.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.3.2.3">BERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib11" title="">11</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.3.2.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.3.2.5">110M/340M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.3.2.6">BooksCorpus, English Wikipedia<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib11" title="">11</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.4.3">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.4.3.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.4.3.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.4.3.3">BART <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib12" title="">12</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.4.3.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.4.3.5">140M/400M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.4.3.6">mix of books and Wikipedia data</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.5.4">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.5.4.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.5.4.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.5.4.3">ELECTRA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib13" title="">13</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.5.4.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.5.4.5">14M/110M/335M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.5.4.6">BooksCorpus, English Wikipedia</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.6.5">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.6.5.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.6.5.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.6.5.3">XLNet<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib14" title="">14</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.6.5.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.6.5.5">110M/340M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.6.5.6">Wikipedia, BookCorpus</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.7.6">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.7.6.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.7.6.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.7.6.3">ERNIE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib15" title="">15</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.7.6.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.7.6.5">110M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.7.6.6">Wikipedia, other texts</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.8.7">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.8.7.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.8.7.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.8.7.3">Galactica<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib16" title="">16</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.8.7.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.8.7.5">6.7B/30.0B/120.0B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.8.7.6">Scientific papers</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.9.8">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.9.8.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.9.8.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.9.8.3">GPT-2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib17" title="">17</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.9.8.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.9.8.5">1.5B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.9.8.6">WebText</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.10.9">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.10.9.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.10.9.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.10.9.3">DeBERTa<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib18" title="">18</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.10.9.4">BERT</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.10.9.5">1.5B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.10.9.6">BooksCorpus, English Wikipedia</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.11.10">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.11.10.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.11.10.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.11.10.3">LLaMA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib19" title="">19</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.11.10.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.11.10.5">7B/13B/33B/65B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.11.10.6">Diverse internet data</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.12.11">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.12.11.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.12.11.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.12.11.3">LLaMA-2<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib1" title="">1</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.12.11.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.12.11.5">7B/13B/34B/70B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.12.11.6">Larger dataset</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.13.12">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.13.12.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.13.12.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.13.12.3">GPT-3<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib20" title="">20</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.13.12.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.13.12.5">6.7B/13B/175B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.13.12.6">Extensive internet text</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.14.13">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.14.13.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.14.13.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.14.13.3">PaLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib21" title="">21</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.14.13.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.14.13.5">8B/62B/540B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.14.13.6">Public and proprietary data</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.15.14">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.15.14.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.15.14.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.15.14.3">Alpaca<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib22" title="">22</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.15.14.4">LLaMA</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.15.14.5">7B/13B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.15.14.6">LLaMA datasets, additional data</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.16.15">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.16.15.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.16.15.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.16.15.3">InstructGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib23" title="">23</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.16.15.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.16.15.5">175B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.16.15.6">Based on GPT-3</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.17.16">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.17.16.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.17.16.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.17.16.3">GPT-4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib3" title="">3</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.17.16.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.17.16.5">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.17.16.6">-</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.18.17">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.18.17.1">General</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.18.17.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.18.17.3">Mixtral <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib24" title="">24</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.18.17.4">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.18.17.5">8x7B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.18.17.6">multilingual data using a context size of 32k tokens</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.19.18">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.19.18.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.19.18.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.19.18.3">E-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib7" title="">7</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.19.18.4">BERT</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.19.18.5">110M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.19.18.6">Amazon Dataset</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.20.19">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.20.19.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.20.19.2">Pre-training</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.20.19.3">KG-FLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib25" title="">25</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.20.19.4">BLIP</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.20.19.5">224M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.20.19.6">Amazon Dataset</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.21.20">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.21.20.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.21.20.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.21.20.3">Ecom-GPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib6" title="">6</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.21.20.4">BLOOMZ</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.21.20.5">560M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.21.20.6">EcomInstruct</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.22.21">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.22.21.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.22.21.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.22.21.3">G2ST<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib26" title="">26</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.22.21.4">Qwen-14B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.22.21.5">14B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.22.21.6">Alibaba.com</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.23.22">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.23.22.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.23.22.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.23.22.3">eCeLLM-L<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.23.22.4">Flan-T5-XXL, Llama-2-13B-chat</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.23.22.5">11B-13B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.23.22.6">ECInstruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib27" title="">27</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.24.23">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.24.23.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.24.23.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.24.23.3">eCeLLM-M<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.24.23.4">Llama-2-7B-chat, Mistral-7B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.24.23.5">7B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.24.23.6">ECInstruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib27" title="">27</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.25.24">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.25.24.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.25.24.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.25.24.3">eCeLLM-S<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib27" title="">27</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.25.24.4">Flan-T5-XL-3B, Phi-2-3B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.25.24.5">3B</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.25.24.6">ECInstruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib27" title="">27</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.26.25">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.26.25.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.26.25.2">Fine-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.26.25.3">GPT4Rec<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib28" title="">28</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.26.25.4">GPT-2</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.26.25.5">117M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.26.25.6">Amazon Review: Beauty and Electronics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib28" title="">28</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.27.26">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.27.26.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.27.26.2">Prompt-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.27.26.3">MixPAVE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib29" title="">29</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.27.26.4">Pre-training Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib30" title="">30</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.27.26.5">0.445M</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.27.26.6">AE-110K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib31" title="">31</a>]</cite> , MAVE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib29" title="">29</a>]</cite>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.28.27">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.28.27.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.28.27.2">Prompt-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.28.27.3">CTM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib32" title="">32</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.28.27.4">characterBERT , BERT</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.28.27.5">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.28.27.6">Huski.ai</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.29.28">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.29.28.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.29.28.2">Prompt-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.29.28.3">Aspect Extraction LLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib33" title="">33</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.29.28.4">GPT-2,BERT</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.29.28.5">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.29.28.6">Amazon, Yelp, Tripadvisor</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.30.29">
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.30.29.1">E-commerce</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.30.29.2">Prompt-tuning</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.30.29.3">CF Recommender Enhancement Model<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib34" title="">34</a>]</cite>
</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.30.29.4">BERT,RoBERTa</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.30.29.5">-</td>
<td class="ltx_td ltx_align_left" id="S2.T1.1.1.30.29.6">Amazon US Reviews</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.1.31.30">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.1.31.30.1">E-commerce</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.1.31.30.2">Prompt-tuning</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.1.31.30.3">recGPT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib34" title="">34</a>]</cite>
</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.1.31.30.4">pre-trained ChatGPT</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.1.31.30.5">-</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.1.1.31.30.6">Amazon reviews and Yelp</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Pre-training</h3>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="384" id="S2.F2.g1" src="extracted/5684480/figures/LLM_Ecommerce_Model_diagram.png" width="628"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>LLM Dependency in E-commerce
</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">Pre-training involves training a large language model (LLM) from scratch on a substantial corpus of e-commerce texts. This foundational training equips the model with domain-specific knowledge necessary to tackle a wide range of tasks within the e-commerce sector. Utilizing foundational architectures such as the Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib35" title="">35</a>]</cite> and subsequent adaptations like BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib11" title="">11</a>]</cite> and GPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib2" title="">2</a>]</cite>, these models are pre-trained on extensive corpora, including product descriptions, customer reviews, and user interactions. These datasets enable the models to capture a variety of linguistic nuances and e-commerce specific terminologies. For instance, BERT’s bidirectional training structure is particularly effective for tasks like sentiment analysis and intent recognition, which are crucial for personalized product recommendations and customer service automation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib11" title="">11</a>]</cite> . Similarly, GPT’s autoregressive features are adept at generating coherent and engaging product descriptions that can significantly enhance search engine optimization (SEO) and user interaction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">In e-commerce field, specialized models such as E-BERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib7" title="">7</a>]</cite> and KG-FLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib25" title="">25</a>]</cite> further refine these capabilities. E-BERT, a derivative of BERT, is re-pre-trained on the Amazon Dataset to boost its efficacy in product-related tasks, thereby enhancing customer interaction quality and the accuracy of sentiment analysis. On the other hand, KG-FLIP extends this by integrating knowledge graphs, which enrich the model’s understanding of structured product information and customer data, leading to improved contextual awareness and precision in search functionalities.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">Future research is likely to focus on refining pre-training approaches to better handle the informal and varied nature of e-commerce text, expand multilingual support, and enhance context-aware systems, potentially incorporating newer models such as GPT-4, LLaMA-3 for even more robust applications.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Fine-tuning</h3>
<div class="ltx_para ltx_noindent" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">There’s a lack of consensus on the precise definition of "fine-tuning" (also known as model tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib11" title="">11</a>]</cite>) within the industry as it emerges with the iteration of researchers experimenting on pre-trained models. Fine-tuning is based on an existing model and then further trained with specific datasets of samples and parameter-efficient tuning approaches such as Lora <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib36" title="">36</a>]</cite>, Prefix-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib37" title="">37</a>]</cite> and full parameter tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib38" title="">38</a>]</cite>. In E-commerce, precedent researches have emphasized the feasibility of applying fine-tuned LLMs to address specific tasks. Li et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib6" title="">6</a>]</cite> (2023) proposed a Ecom-GPT model which was trained based on BLOOMZ<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib39" title="">39</a>]</cite> with instruct datasets. In zero-shot scenarios, this model shows outperformed metrics than other general LLMs in terms of attribution extraction, customer topic classification and product title generation. Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib26" title="">26</a>]</cite>(2024), targeting the translation tasks in E-commerce, offered a general-to-specialized paradigm based on Neural Machine Translation models. Two-step fine-tuning approaches are incorporated into the experiment with 14 billion parameters trained on bilingual datasets. The ROUGE-N and ROUGE-L metrics reveals a better results in translation tasks compared with LLaMA, Qwen and GTP. Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib27" title="">27</a>]</cite> (2024) has proposed a set of E-commerce models (eCeLLM) to strengthen the generalization abilities including product understanding, user request understanding, product matching and question answering. Particularly, three different model sizes are developed and compared with general-purpose LLMs, e-commerce LLMs and task-specific models given a comprehensive set of individual tasks. It is noted that the models demonstrate higher F1 scores and better generalization ability in out-of-domain test cases. Li et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib28" title="">28</a>]</cite> (2023) fine-tuned GPT-2 with a 2-step training process and then integrate it with a search engine. This framework aims to leverage LLM to generate recommended products given the customers previous purchase history.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Apart from relevant works on improving single model’s performance for either specific tasks or generalization ability in E-commerce, researchers also proposed novel systematic integration of multiple trained LLMs to handle customer requests in real-world applications. Zhou et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib40" title="">40</a>]</cite> (2023) proposed an approach to synthesize fine-tuned BERT and Llama 2 in a system to efficiently extract product attributes for customer queries in Walmart search functionality. When customers query for specific products, the most likely matched results will be returned. This system employs BERT to generate contextual embedding as the encoding phase and Conditional Random Field(CRFs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib41" title="">41</a>]</cite> layer to decode the tags. In parallel the encoding from BERT is utilized and trained to construct neural network providing decorative relation correction scrutinizing on the returned responses. This system not only incorporate a fine-tuned BERT as base model but also leverage LLAMA 2.0 with prompts to retrieve additional product attributes for customers. Another practice proposed by Zhao et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib42" title="">42</a>]</cite> (2024) also utilizes BERT-CRF model in encoding/decoding for entity extraction. The difference is that Zhao et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib42" title="">42</a>]</cite> (2024) builds a complementary graph (Entity Dict) to recommend the next products for customers. Cloude 2, as the pre-trained model, is fine-tuned to discern the complementary relationships in the graph construction.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Prompt-tuning</h3>
<div class="ltx_para ltx_noindent" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Prompt-tuning, as Lester et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib43" title="">43</a>]</cite> explained, is a mechanism of freezing language models and tuning model with task-specific prompt for each task. The whole process is effective in terms of only a few tun-able tokens prepend per downstream
task and grouping multiple adaptations of a pre-trained language model to achieve similar or even better performance than traditional fine-tuning approach (See Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S2.F3" title="Figure 3 ‣ 2.3 Prompt-tuning ‣ 2 The Principles of E-commerce LLMs ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_tag">3</span></a>).</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="291" id="S2.F3.g1" src="extracted/5684480/figures/prompting.jpg" width="354"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Prompt tuning of T5 performance as size increases 

<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib43" title="">43</a>]</cite></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">Lester et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib43" title="">43</a>]</cite>’s work reveals the lightweight parameter footprint and multi-task serving in prompt-tuning. Based on such findings, there have been growing interests in applying prompt-tuning in E-commerce. Yang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib29" title="">29</a>]</cite> (2023) proposed a mix-prompt trained model for product attribute value extraction(MixPAVE). The base pre-trained transformer model is frozen except the extraction head<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib44" title="">44</a>]</cite> to be trained with textual and key-value prompts. Additionally, two datasets: MAVE<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib45" title="">45</a>]</cite> with 3 million attribute-value annotations and AE-110K collected from AliExpress are used for training and partially for testing in few-shot scenarios. The experiment result shows that MixPAVE outperforms fine-tuning models regarding certain attributes extraction with fewer parameters trained over the process. In a similar study on prompt-tuning, Wang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib32" title="">32</a>]</cite> observed a significant rise in the number of new entities emerging in the E-commerce domain. To overcome limitations of existing LLM’s ability to handle emerging product entities and titles, Wang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib32" title="">32</a>]</cite> proposes a textual entailment model with continuous prompt-tuning approach to better classify entity types. The experiment result shows higher average F1 score in both addition (0.30%) and concatenation (0.38%) as fusion methods.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">In some research, prompt-tuning is jointly applied with fine-tuning approach. For instance, Li et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib33" title="">33</a>]</cite> (2023) fine-tuned GPT-2 with local offline datasets and then feed the model with soft prompts concatenated with embeddings from customer review texts to generate a list of aspect terms. These terms successively are fed into a neural network to generate aspect-based recommendations. With a chain of fine-tuned LLMs and prompt-tuning method connected to aspect-based recommender systems, the frameworks shows better metrics than state-of-the-art baseline methods in providing more meaningful recommendations for users.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.SS3.p4">
<p class="ltx_p" id="S2.SS3.p4.1">Moreover, some researcher intentionally blurs the boundary of prompt-tuning and fine-tuning to serve specific purpose. For example, Dang et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib34" title="">34</a>]</cite> fine-tuned BERT and RoBERTa with prompt-based learning paradigm to generate more sentiment data in order to tackle the insufficient rating data and data sparsity issues in collaborative filtering recommendation systems.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Bias challenges</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Introduction to Fairness and Bias in LLM, E-commerce</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">LLMs and AI systems have revolutionized the e-commerce industry, enabling personalized experiences and efficient decision-making processes. However, these advanced technologies also introduce challenges related to fairness and bias. LLMs can perpetuate and amplify societal biases present in their training data, leading to discriminatory outcomes and unfair treatment of certain demographics. In the e-commerce domain, biases can manifest in various forms, such as popularity bias in product recommendations, exposure imbalance among sellers, and skewed search results. These biases not only impact user experiences but also raise ethical concerns regarding transparency, accountability, and equity. Addressing fairness and bias in LLMs and e-commerce requires a multifaceted approach, including the development of fairness-aware algorithms, diverse and representative training data, and rigorous evaluation frameworks. Ongoing research efforts and interdisciplinary collaboration are crucial to mitigate biases, ensure fair outcomes, and build trust in AI-driven e-commerce systems. As the integration of LLMs and AI continues to shape the future of e-commerce, prioritizing fairness and addressing bias remains paramount for creating inclusive and equitable online marketplaces.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>What is fairness and bias?</h3>
<div class="ltx_para ltx_noindent" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Fairness and bias are two interrelated concepts that are crucial in the context of AI and machine learning systems. Fairness refers to the principle of ensuring equitable treatment and outcomes for all individuals or groups, regardless of their protected attributes such as race, gender, age, or socioeconomic status. It involves the absence of discrimination or unjustified disparities in the decisions or outputs generated by AI algorithms. On the other hand, bias refers to the systematic errors or prejudices that can be present in data, algorithms, or models, leading to skewed or unfair results. Bias can arise from various sources, including biased training data, flawed data collection processes, or the inherent limitations of the algorithms themselves. Biases can manifest in different forms, such as demographic biases, measurement biases, or representation biases, and can perpetuate or amplify existing societal inequalities. Ensuring fairness and mitigating bias in AI systems is essential to prevent discriminatory outcomes, promote equal opportunities, and build trust in the technology. It requires a proactive approach that involves careful data curation, algorithmic fairness techniques, rigorous testing and evaluation, and ongoing monitoring to identify and address any potential biases throughout the AI lifecycle.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>The challenges and biases in LLM</h3>
<div class="ltx_para ltx_noindent" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">LLMs have made remarkable advancements in natural language processing, but they also face significant challenges related to fairness and bias. One major challenge is the presence of various types of biases in LLMs, including gender bias, racial bias, religious bias, age bias, sexuality bias, country bias, and disease bias. These biases can manifest in the model’s outputs, leading to stereotypical or discriminatory associations. Another challenge lies in the sources of bias in LLMs, which can stem from biased training data, sampling biases, semantic biases encoded in the model’s representations, and the amplification of biases during the learning process. Addressing these challenges requires a comprehensive approach that involves careful data curation, bias mitigation techniques, and rigorous evaluation frameworks. Additionally, the development of explainable and interpretable LLMs is crucial to understand and mitigate biases effectively. Researchers and practitioners must also consider the ethical implications of deploying LLMs in real-world applications and ensure that the models align with principles of fairness, transparency, and accountability. Overcoming these challenges is essential to harness the full potential of LLMs while promoting fairness and reducing the risk of perpetuating societal biases.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.1 </span>Gender bias</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p1">
<p class="ltx_p" id="S3.SS3.SSS1.p1.1">LLMs can exhibit various types of biases that pose significant challenges to their fairness and reliability. Among them, gender bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib46" title="">46</a>]</cite> is a prominent issue, where LLMs may associate certain occupations or attributes with specific genders, perpetuating stereotypical assumptions. Gender bias has been demonstrated to be present in word embeddings, as well as in a wide range of models designed for diverse NLP tasks, including machine translation, sentiment analysis, auto-captioning, toxicity detection, and beyond. Since LLMs often failed to acknowledge the ambiguity in pronoun references unless explicitly prompted, LLMs often provided explanations that appeared logical but were factually inaccurate, potentially masking the biases.
One significant source that introduces gender bias is labelling <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib47" title="">47</a>]</cite>, which occurs when the training data contains biased or subjective labels provided by annotators, leading the model to learn and perpetuate those biases. If the training data for sentiment analysis predominantly associates certain genders with specific sentiments, such as associating women with emotions like "sensitivity" and men with "strength", an LLM might learn and reinforce these stereotypes. For instance, it may consistently associate pronouns referring to women with negative sentiments or pronouns referring to men with positive ones.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS1.p2">
<p class="ltx_p" id="S3.SS3.SSS1.p2.1">One potential solution to mitigate gender bias in LLM is to ensure that training datasets are diverse and representative of different genders, races, cultures, and backgrounds. This involves collecting data from a wide range of sources and demographics to minimize biases present in the data.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.2 </span>Racial bias</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p1">
<p class="ltx_p" id="S3.SS3.SSS2.p1.1">Racial bias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib48" title="">48</a>]</cite> can also be present, leading to biased outputs or decisions based on race-related information. The models tended to generate biased content for certain racial groups, including unwarranted details based on race. The models exhibited favoritism and has racially-skewed socio-economic projection towards a certain racial group in content recommendations. A important origin of this racial bias is sampling. The issue arises when the distribution of samples from different demographic groups in the training data differs from the actual population distribution, causing the model to exhibit biased behavior. Pre-existing racial prejudices and inequalities within the data can be reflected in the outputs of the language models. Additionally, the vulnerability of the models to prompt manipulation with malicious intent can lead to biased responses.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS2.p2">
<p class="ltx_p" id="S3.SS3.SSS2.p2.1">One potential solution to mitigate racial bias in LLM is to analyze the distribution of racial groups within the data and adjusting the sampling process to ensure equal representation. Random sampling techniques could be adopted to select data for training the language models, which helps reduce the risk of bias by ensuring that each data point has an equal chance of being selected, regardless of racial characteristics. Stratified sampling can be employed to ensure proportional representation of different racial groups in the training data, which involves dividing the dataset into strata based on race and then sampling proportionally from each stratum to ensure balanced representation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.3.3 </span>Other types of social bias</h4>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.p1">
<p class="ltx_p" id="S3.SS3.SSS3.p1.1">Religious bias occurs when LLM demonstrates favoritism or discrimination towards individuals or groups based on their religious beliefs or affiliations. It may originate from the generation of text that stereotypes or stigmatizes certain religions, promotes one religion over others, or misrepresents religious practices and beliefs. Similarly, LLM could cause and even amplify other social bias including age bias, sexuality bias, and country bias. Potential source of these social bias could be semantic bias, which can emerge during the language model encoding process, resulting in biased semantic representations that capture stereotypical associations. These social bias could also be amplified, where the model not only learns the biases present in the training data but also amplifies them during the learning process. They can persist and even intensify further when the model is fine-tuned for downstream tasks.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.SSS3.p2">
<p class="ltx_p" id="S3.SS3.SSS3.p2.1">To mitigate the bias in LLMs requires careful attention to data quality and representative sampling during both pre-training and fine-tuning stages. It also involves developing robust evaluation frameworks to detect and quantify biases, enabling researchers to identify and address them effectively. By understanding and tackling the sources of bias, we can work towards building more fair and unbiased LLMs that provide reliable and equitable outputs.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>What are the challenges and biases in e-commerce?</h3>
<div class="ltx_para ltx_noindent" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">E-commerce platforms face several challenges and biases that can impact the fairness and equity of the online marketplace. One significant challenge is the presence of various types of biases, such as popularity bias, where popular items or sellers receive disproportionate exposure in recommendation systems, hindering the visibility of less popular offerings. Exposure bias refers to the skewed distribution of visibility and opportunities among sellers, with a small percentage of popular sellers receiving the majority of user attention. This can lead to unfair competition and limit the growth potential of smaller or newer sellers. Recommendation bias can also arise, where the algorithms used to suggest products to users are influenced by factors beyond relevance or user preferences, leading to the promotion of certain products or sellers over others. Search bias can further compound these issues, as the search results on e-commerce platforms may be skewed towards certain products or sellers due to factors such as search optimization techniques or paid placements.Moreover, e-commerce platforms must grapple with description bias, where the textual metadata like product tags and descriptions provided by sellers may not accurately or comprehensively reflect the offerings. Addressing these challenges and biases, which also include ensuring seller-side fairness and providing fair and unbiased product reviews, requires the implementation of fair and transparent algorithms, robust evaluation frameworks, and a commitment to creating an equitable online marketplace. By promoting fairness and mitigating these varied biases, e-commerce platforms can build a more trustworthy and inclusive digital environment that benefits both sellers and consumers.</p>
</div>
<section class="ltx_subsubsection" id="S3.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.1 </span>Visibility and accessibility: popularity, exposure, and recommendation bias</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p1">
<p class="ltx_p" id="S3.SS4.SSS1.p1.1">Visibility and accessibility are crucial to products on e-commerce platforms. One prominent issue is popularity bias, where popular items or sellers receive disproportionate exposure and visibility in recommendation systems, overshadowing less popular offerings. This bias can limit the discoverability of new or niche products and hinder the growth of smaller sellers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib49" title="">49</a>]</cite>. Another type of bias is exposure bias, which refers to the skewed distribution of visibility and opportunities among sellers, with a small group of popular sellers receiving the majority of user attention and sales. Less exposed items pose the challenge of inaccurate reward function prediction in our e-commerce setting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib50" title="">50</a>]</cite>. This bias can create an uneven playing field and stifle competition. As discussed in prior research, the concept of higher-ranked items in recommendation lists commonly receiving more exposure and user attention, and being more likely to be consumed, was also addressed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib52" title="">52</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p2">
<p class="ltx_p" id="S3.SS4.SSS1.p2.1">Recommendation bias occurs when the algorithms used to suggest products to users are influenced by factors beyond relevance or user preferences, such as promotional partnerships or business objectives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib49" title="">49</a>]</cite>. This bias can lead to the promotion of certain products or sellers over others, potentially compromising the integrity of the recommendations and creating an uneven playing field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib51" title="">51</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib52" title="">52</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS1.p3">
<p class="ltx_p" id="S3.SS4.SSS1.p3.1">To address recommendation and other biases related to visibility and accessibility, recent research has proposed bias mitigation strategies that go beyond relying solely on binary rating matrices <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib53" title="">53</a>]</cite>. These more advanced techniques require complex model adjustments, expensive sampling methods, or heuristic propensity scores, and can struggle when users accept or reject multiple recommendations for the same item <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib53" title="">53</a>]</cite>. An alternative approach, as suggested in the literature, is a multi-process fusion method that combines pre-processing, in-processing, and post-processing techniques to alleviate popularity bias in recommendations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib54" title="">54</a>]</cite>. This approach embeds consumer preferences and product popularity information directly into the recommendation model, while also making adjustments to the dataset and recommendation lists, without imposing specific requirements on the underlying recommendation algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib54" title="">54</a>]</cite>. This multi-faceted debiasing strategy has been shown to improve recommendation accuracy and consumer interest, making it a promising solution for addressing recommendation bias in e-commerce and LLM-powered systems.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.2 </span>Search bias</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p1">
<p class="ltx_p" id="S3.SS4.SSS2.p1.1">Platforms facilitating digital commerce have long grappled with the challenge of search bias, where search results are skewed towards certain products or sellers due to factors such as search optimization techniques or paid placements. This bias in search can adversely impact the visibility and discoverability of products, thereby affecting consumer choice and fair competition. Addressing such biases necessitates the implementation of fair and transparent search algorithms, regular audits and evaluations, and a steadfast commitment to fostering an equitable e-commerce ecosystem for all participants.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS2.p2">
<p class="ltx_p" id="S3.SS4.SSS2.p2.1">To combat the issue of search bias, a novel model training framework dubbed "TripleLearn" was proposed <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib55" title="">55</a>]</cite>. The cornerstone of this solution is that TripleLearn iteratively learns from three distinct training datasets, deviating from the traditional approach of employing a single training set. By harnessing this iterative learning process, the authors were able to substantially enhance the model’s performance, boosting the F1 score from 69.5 to an impressive 93.3 on the holdout test data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib55" title="">55</a>]</cite>. This remarkable improvement underscores the efficacy of the TripleLearn approach in mitigating search bias and delivering high-quality search results for e-commerce platforms, thereby fostering a more equitable and transparent search experience.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S3.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.4.3 </span>Description bias</h4>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS3.p1">
<p class="ltx_p" id="S3.SS4.SSS3.p1.1">E-commerce platforms have long been grappled with the challenge of description bias, which stems from the manner in which sellers provide textual metadata, such as product tags, to characterize their offerings. As these digital marketplaces facilitate active user participation in the creation and categorization of product-related content, the textual features (e.g., titles, descriptions, tags) generated by sellers may not always be of sufficient quality or accurately reflect the nuances of the products <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib56" title="">56</a>]</cite>. Sellers, lacking the comprehensive training or domain expertise required to meticulously describe their wares, may instead resort to the use of "tag spam" – the employment of irrelevant yet popular keywords in a misguided attempt to promote their products <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib56" title="">56</a>]</cite>. This seller-generated bias in product descriptions can have detrimental impacts on the efficacy of crucial e-commerce services, such as search and recommendation systems, ultimately frustrating consumers’ ability to effectively locate their desired items <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib56" title="">56</a>]</cite>. Addressing this description bias is of paramount importance for enhancing the quality of textual product descriptors and, correspondingly, improving the overall e-commerce user experience.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS4.SSS3.p2">
<p class="ltx_p" id="S3.SS4.SSS3.p2.1">In an effort to alleviate the description bias introduced by seller-provided product tags, the scholarly work under consideration proposes the leveraging of automated tag recommendation techniques that harness search query and click data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib56" title="">56</a>]</cite>. The central hypothesis posits that "the set of queries collectively issued by the consumers of the e-marketplace, along with corresponding clicks, reflect a more trustworthy view of the products; thus those queries and clicks can be exploited as a source of high-quality (e.g., more diverse) tags to describe the products" <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib56" title="">56</a>]</cite>. Guided by this principle, the authors develop novel tag recommendation solutions, including deep learning-based approaches, that generate tags based on the insights gleaned from this search data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib56" title="">56</a>]</cite>. Rigorous evaluations revealed that the seller-provided tags often contain significant noise and bias, while the proposed search-boosted tag recommenders were able to substantially outperform the state-of-the-art, improving recommendation effectiveness by over 16 per cent <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib56" title="">56</a>]</cite>. The authors contend that these recommended tags, borne of the collective consumer search experience, can provide a more reliable data source for e-commerce search and information services than the original seller-provided descriptions, thereby helping to overcome the inherent biases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib56" title="">56</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>What are the challenges and biases in the application of LLM in the e-commerce field?</h3>
<div class="ltx_para ltx_noindent" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.1">The application of LLMs in the e-commerce field presents several challenges and biases that need to be addressed to ensure fairness and equity. One significant challenge is the potential for LLMs to perpetuate and amplify biases present in the training data, which can lead to discriminatory outcomes in e-commerce recommendations, search results, and customer interactions. For example, if an LLM is trained on biased product descriptions or customer reviews, it may generate biased outputs that favor certain products or sellers over others. One example is the Modern collaborative filtering algorithms seek to provide personalized product recommendations by uncovering patterns in consumer product interactions: Addressing Marketing Bias in Product Recommendations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib57" title="">57</a>]</cite>. Additionally, LLMs may struggle to capture the nuances and context-specific meanings of e-commerce terminology, leading to misinterpretations or inaccurate recommendations. Another challenge is the lack of transparency and explainability in LLM-based e-commerce systems, making it difficult to identify and mitigate biases. Moreover, the application of LLMs in e-commerce may raise privacy concerns, as these models require vast amounts of user data for training and operation. Ensuring the responsible and ethical use of user data while maintaining the benefits of personalization is a delicate balance. To address these challenges, e-commerce platforms must prioritize the development of fair and unbiased LLMs, incorporate diversity and inclusivity in training data, and implement robust evaluation and auditing mechanisms. Collaboration between e-commerce practitioners, researchers, and ethicists is crucial to navigate the ethical implications and ensure the responsible deployment of LLMs in the e-commerce field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib57" title="">57</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Impacts of bias</h3>
<div class="ltx_para ltx_noindent" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.1">Bias in AI systems and e-commerce platforms can have far-reaching and detrimental impacts on individuals, businesses, and society as a whole. One significant impact is the perpetuation and amplification of societal inequalities and discrimination. Biased algorithms can lead to unfair treatment of certain demographics, limiting their access to opportunities, resources, and services. This can result in a widening of the digital divide and the reinforcement of historical biases <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib57" title="">57</a>]</cite>. Moreover, biased AI systems in e-commerce can lead to discriminatory outcomes, such as skewed product recommendations, unfair pricing, or biased search results. This can harm the reputation and trust in e-commerce platforms, as consumers may feel misled or unfairly treated. Bias can also have economic consequences, stifling competition and innovation by favoring established or popular brands over newer or niche offerings. This can create barriers to entry for small businesses and limit consumer choice <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib57" title="">57</a>]</cite>. Additionally, biased AI systems can perpetuate stereotypes and contribute to the spread of misinformation, influencing public opinion and decision-making. The impacts of bias extend beyond the individual level, affecting society’s collective values, beliefs, and behaviors. Addressing the impacts of bias requires a proactive and multifaceted approach, including the development of fair and transparent AI systems, regular audits and assessments, and the promotion of diversity and inclusivity in the design and deployment of AI technologies.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.7 </span>Current approaches</h3>
<div class="ltx_para ltx_noindent" id="S3.SS7.p1">
<p class="ltx_p" id="S3.SS7.p1.1">Addressing the challenges of fairness and bias in LLMs and e-commerce platforms requires a multifaceted approach. Current efforts focus on developing fairness-aware algorithms that incorporate fairness metrics and constraints into the training and evaluation processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib58" title="">58</a>]</cite>. These algorithms aim to mitigate biases by ensuring equitable treatment of different groups and promoting diversity in the model’s outputs. Another approach is the use of adversarial debiasing techniques, which involve training the model to be invariant to sensitive attributes, such as gender or race, while still maintaining its predictive performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib58" title="">58</a>]</cite>. Researchers are also exploring the use of counterfactual fairness frameworks, which assess the fairness of a model by considering the outcomes under different hypothetical scenarios. In the e-commerce domain, current approaches include the development of fair ranking algorithms that ensure equitable exposure for all sellers and products, regardless of their popularity or historical performance. Collaborative filtering techniques are being adapted to incorporate fairness constraints and promote diversity in recommendations. Additionally, there is a growing emphasis on transparency and explainability in e-commerce algorithms, allowing stakeholders to understand and audit the decision-making processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib58" title="">58</a>]</cite>. Efforts are also being made to curate diverse and representative training data to reduce the impact of historical biases. Overall, the current approaches to tackling fairness and bias in LLMs and e-commerce involve a combination of algorithmic innovations, data curation strategies, and transparency initiatives to ensure equitable outcomes for all participants in the digital marketplace <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib58" title="">58</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>E-commerce Application</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">The integration of language models such as ChatGPT has transformed the customer-business interaction within e-commerce applications. By harnessing the extensive knowledge and linguistic capabilities of these models, e-commerce platforms can deliver personalized and interactive experiences to users. Language models facilitate natural language understanding, empowering customers to ask questions, receive product recommendations, and obtain detailed information in a conversational manner. These models contribute to product search functionalities, generate accurate summaries, and even facilitate translation to cater to a global user base. Moreover, language models can analyze customer sentiment expressed in reviews and feedback<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib59" title="">59</a>]</cite>, providing businesses with valuable insights into customer preferences and enhancing their offerings. By emulating human-like text generation and comprehension, language models elevate customer engagement, streamline the shopping experience, and ultimately drive sales in the dynamic landscape of e-commerce. In this section, we demonstrate real-world applications within e-commerce and discuss the potential fairness concern.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1">To further illustrate the impact of language models on e-commerce applications, the integration into various aspects of the e-commerce workflow can be visualized as Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.F4" title="Figure 4 ‣ 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_tag">4</span></a>. The following graph showcases how language models, such as ChatGPT, enhance customer-business interactions by enabling personalized experiences, natural language understanding, and conversational interactions. Through the graph, it explores how language models contribute to product recommendation, search functionalities, information summarization, translation services, sentiment analysis, and customer engagement within the e-commerce landscape. This visualization serves to highlight the transformative role of language models in optimizing the customer journey, improving user experience, and driving sales in the dynamic realm of e-commerce. Detail workflows can be illustrated by Figure <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#S4.F5" title="Figure 5 ‣ 4 E-commerce Application ‣ A survey on fairness of large language models in e-commerce: Progress, application, and challenge"><span class="ltx_text ltx_ref_tag">5</span></a></p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="275" id="S4.F4.g1" src="extracted/5684480/figures/llmonecommerce.jpg" width="200"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span> intergration of LLMs in e-commerce workflow</figcaption>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S4.F5.g1" src="extracted/5684480/figures/recommendationandsearch.jpg" width="390"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span> details of LLMs in e-commerce workflow</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Product Recommendation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Product recommendation systems play a critical role in assisting users in finding relevant and personalized items or content. With the emergence of LLMs in Natural Language Processing (NLP), there has been a growing interest in harnessing the power of these models to enhance recommendation systems. Different from traditional recommendation systems, the LLM-based models excel in capturing contextual information, comprehending user queries, item descriptions, and other textual data more effectively<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib60" title="">60</a>]</cite>. By understanding the context, LLM-based RS can improve the accuracy and relevance of recommendations, leading to enhanced user satisfaction. Meanwhile, facing the common data sparsity issue of limited historical interactions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib61" title="">61</a>]</cite>, LLMs also bring new possibilities to recommendation systems through zero/few-shot recommendation capabilities<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib62" title="">62</a>]</cite>. These models can generalize to unseen candidates due to the extensive pre-training with factual information, domain expertise, and common-sense reasoning, enabling them to provide reasonable recommendations even without prior exposure to specific items or users<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib63" title="">63</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="666" id="S4.F6.g1" src="extracted/5684480/figures/recommendation_system.png" width="855"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span> Three modeling paradigms of the research for LLMs on recommendation systems<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib64" title="">64</a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Product Search</h3>
<div class="ltx_para ltx_noindent" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">In e-commerce, product search involves retrieving catalog items that are semantically related to a customer’s query. The search algorithm evolved from relying primarily on lexical matching to semantic matching<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib65" title="">65</a>]</cite>. With the generalization capability of LLM, the use of language models like ChatGPT can greatly boost the search performance. Leveraging the power of a large language model, a product search system can understand and interpret natural language queries, making the search process more intuitive and efficient<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib66" title="">66</a>]</cite>. For instance, users can simply describe the product they are looking for in plain language, and the language model can analyze their query to identify relevant products<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib67" title="">67</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="436" id="S4.F7.g1" src="extracted/5684480/figures/productSearch.png" width="765"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span> LLM Search: Given a user query, it extracts relevant product information from
the internet and passes it to the LLM along with the query. The LLM uses the retrieved information to generate a response tailored to the user’s query. The circled numbers indicate the order of the steps. STS: The strategic text sequence is added to the target product’s information page to increase its chances of being recommended to the user.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib66" title="">66</a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span>Product Information Summary</h3>
<div class="ltx_para ltx_noindent" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Language models like ChatGPT have emerged as valuable tools in the e-commerce industry, particularly in generating concise and informative summaries of product information. By utilizing the model’s comprehensive knowledge and language processing capabilities, e-commerce platforms can effortlessly condense crucial product details into easily understandable summaries. These summaries encompass essential information such as product features, specifications, pricing, customer reviews, and availability<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib68" title="">68</a>]</cite>. With their aptitude for comprehending and analyzing textual data, language models can extract pertinent details from various sources, including product descriptions and reviews, to provide comprehensive and accurate summaries. This enables shoppers to swiftly evaluate the suitability of a product based on their specific requirements and preferences, without the need to sift through overwhelming amounts of information.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.4 </span>Product Information Translation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">Transformer-based Machine Translation (MT) models have achieved significant process in the general domain, with more training parameters and full richer bilingual parallel corpora <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib69" title="">69</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib70" title="">70</a>]</cite>. Especially for LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib39" title="">39</a>]</cite>, peculiar emergence greatly improves their generalization for precise text translation in various sources. Efforts are made into adapting LLM to e-commerce domain by creating linguistic pairs and introduce contrastive learning<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1">LLMs offer a transformative solution for e-commerce product information translation, leveraging their ability to comprehend vast datasets and multilingual proficiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib71" title="">71</a>]</cite>. Through sophisticated contextual understanding, LLMs accurately translate product descriptions, specifications, and reviews across different languages, catering to diverse global markets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib35" title="">35</a>]</cite>. Their adeptness in handling technical terminology and customization for specific e-commerce domains ensures precise translations that maintain brand voice and style <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib11" title="">11</a>]</cite>. Integrated into quality assurance workflows, LLMs facilitate rapid, scalable translation processes while continuously improving through user feedback integration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib2" title="">2</a>]</cite>, ultimately enabling businesses to reach and engage with global audiences effectively and efficiently.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.5 </span>Product Information Generation</h3>
<div class="ltx_para ltx_noindent" id="S4.SS5.p1">
<p class="ltx_p" id="S4.SS5.p1.1">In the evolving landscape of e-commerce, the integration of LLMs has demonstrated significant potential in enhancing user experience and product visibility. The work by Shanu Vashishtha and colleagues at Walmart Inc. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib72" title="">72</a>]</cite> highlights an innovative approach to generating personalized e-commerce banners using LLMs combined with text-to-image technologies like Stable Diffusion. This method effectively transforms user interaction data into visually appealing banners, validated through image quality metrics and human evaluations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS5.p2">
<p class="ltx_p" id="S4.SS5.p2.1">Concurrently, research by Aounon Kumar and Himabindu Lakkaraju <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib66" title="">66</a>]</cite> investigates manipulating LLMs to prioritize certain products in search results. By embedding strategic text sequences into product descriptions, they show that search algorithms can be influenced to favor these entries, enhancing product visibility and potentially skewing market dynamics. This raises important ethical questions about the manipulation of AI-driven tools in commercial settings.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS5.p3">
<p class="ltx_p" id="S4.SS5.p3.1">Both studies exemplify the dual use of LLMs in e-commerce—improving user engagement and challenging the fairness of AI applications. They collectively underscore the need for ethical guidelines and safeguards to ensure that these technologies are used responsibly in enhancing the digital marketplace.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="859" id="S4.F8.g1" src="extracted/5684480/figures/informationGeneration.png" width="675"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Table with product name and generated images with different approaches<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib72" title="">72</a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.6 </span>Product Q&amp;A</h3>
<div class="ltx_para" id="S4.SS6.p1">
<p class="ltx_p" id="S4.SS6.p1.1">LLMs can also positively influence the process of answering user queries. One research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib73" title="">73</a>]</cite> demonstrates the utility of LLMs in the domain of product question and answer (Q&amp;A) systems on e-commerce platforms. Specifically, the research focuses on utilizing models like XLNet and BERT to directly answer queries based on product specifications, rather than user reviews. The researchers developed a semi-supervised approach to create a large training dataset for fine-tuning these models, which significantly outperformed the baseline Siamese model in identifying relevant product specifications across various product categories. This method enhances the accuracy of product Q&amp;A systems by leveraging structured product information, showcasing the adaptability of LLMs to different data types within e-commerce. Another research on LLM and Conversational recommender systems (CRS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib74" title="">74</a>]</cite> has conducted experiments on a real-world dataset. It suggests that such collaborations significantly enhance the performance of pre-sales dialogues, offering a promising approach to refining customer interaction and satisfaction in e-commerce settings.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="173" id="S4.F9.g1" src="extracted/5684480/figures/productQ_A.png" width="747"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Trained model answering product questions. Top three specifications returned by different models for two questions. Correct specification is highlighted in bold. BERT and XLNet are able to retrieve the correct
specifications.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib73" title="">73</a>]</cite>.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS7">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.7 </span>Fairness Analysis in E-commerce Application</h3>
<div class="ltx_para ltx_noindent" id="S4.SS7.p1">
<p class="ltx_p" id="S4.SS7.p1.1">While language models (LLMs) have the potential to enhance e-commerce applications and improve user experience, multiple studies have confirmed that these models can inherit societal biases from the raw training data. Previous work has shown that LLMs tend to reinforce social biases in their generation outputs due to the bias in the large pre-training corpus, leading to unfair treatment of vulnerable groups <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib75" title="">75</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib76" title="">76</a>]</cite>. Specifically, an increasing concerns about the negative social impact of recommendation systems<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib77" title="">77</a>]</cite>, unfairness issues in recommendation have received significant attention in recent years <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib78" title="">78</a>, <a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib64" title="">64</a>]</cite>. Researchers conducted analysis over fairness in recommendation system, including defining the group/individual difference in recommendation results/qualities across different sensitive groups.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib78" title="">78</a>]</cite></p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS7.p2">
<p class="ltx_p" id="S4.SS7.p2.1">Researchers have been actively working on developing quantitative metrics to assess the importance of fairness.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS7.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.7.1 </span>Intrinsic Bias Evaluation Metrics</h4>
<div class="ltx_para ltx_noindent" id="S4.SS7.SSS1.p1">
<p class="ltx_p" id="S4.SS7.SSS1.p1.1">Similarity-based metrics, such as WEAT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib79" title="">79</a>]</cite>, SEAT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib80" title="">80</a>]</cite>, and CEAT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib81" title="">81</a>]</cite>, employ semantically bleached sentence templates to measure similarities between various demographic groups.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS7.SSS1.p2">
<p class="ltx_p" id="S4.SS7.SSS1.p2.4">The WEAT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib79" title="">79</a>]</cite> metric, quantifies the association between two sets of attribute words (e.g.gender pronouns) and two sets of target words (e.g., career). Formally, the sets of attribute words are indicated by <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p2.1.m1.1"><semantics id="S4.SS7.SSS1.p2.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p2.1.m1.1.1" xref="S4.SS7.SSS1.p2.1.m1.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p2.1.m1.1b"><ci id="S4.SS7.SSS1.p2.1.m1.1.1.cmml" xref="S4.SS7.SSS1.p2.1.m1.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p2.1.m1.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p2.1.m1.1d">caligraphic_A</annotation></semantics></math> and <math alttext="\mathcal{B}" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p2.2.m2.1"><semantics id="S4.SS7.SSS1.p2.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p2.2.m2.1.1" xref="S4.SS7.SSS1.p2.2.m2.1.1.cmml">ℬ</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p2.2.m2.1b"><ci id="S4.SS7.SSS1.p2.2.m2.1.1.cmml" xref="S4.SS7.SSS1.p2.2.m2.1.1">ℬ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p2.2.m2.1c">\mathcal{B}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p2.2.m2.1d">caligraphic_B</annotation></semantics></math>, and the sets of target words are denoted by <math alttext="\mathcal{X}" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p2.3.m3.1"><semantics id="S4.SS7.SSS1.p2.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p2.3.m3.1.1" xref="S4.SS7.SSS1.p2.3.m3.1.1.cmml">𝒳</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p2.3.m3.1b"><ci id="S4.SS7.SSS1.p2.3.m3.1.1.cmml" xref="S4.SS7.SSS1.p2.3.m3.1.1">𝒳</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p2.3.m3.1c">\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p2.3.m3.1d">caligraphic_X</annotation></semantics></math> and <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p2.4.m4.1"><semantics id="S4.SS7.SSS1.p2.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p2.4.m4.1.1" xref="S4.SS7.SSS1.p2.4.m4.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p2.4.m4.1b"><ci id="S4.SS7.SSS1.p2.4.m4.1.1.cmml" xref="S4.SS7.SSS1.p2.4.m4.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p2.4.m4.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p2.4.m4.1d">caligraphic_Y</annotation></semantics></math>. Then the WEAT test statistics are defined as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s(\mathcal{X},\mathcal{Y},\mathcal{A},\mathcal{B})=\sum_{x\in X}s(x,\mathcal{A%
},\mathcal{B})-\sum_{y\in\mathcal{Y}}s(y,\mathcal{A},\mathcal{B})," class="ltx_Math" display="block" id="S4.Ex1.m1.11"><semantics id="S4.Ex1.m1.11a"><mrow id="S4.Ex1.m1.11.11.1" xref="S4.Ex1.m1.11.11.1.1.cmml"><mrow id="S4.Ex1.m1.11.11.1.1" xref="S4.Ex1.m1.11.11.1.1.cmml"><mrow id="S4.Ex1.m1.11.11.1.1.2" xref="S4.Ex1.m1.11.11.1.1.2.cmml"><mi id="S4.Ex1.m1.11.11.1.1.2.2" xref="S4.Ex1.m1.11.11.1.1.2.2.cmml">s</mi><mo id="S4.Ex1.m1.11.11.1.1.2.1" xref="S4.Ex1.m1.11.11.1.1.2.1.cmml">⁢</mo><mrow id="S4.Ex1.m1.11.11.1.1.2.3.2" xref="S4.Ex1.m1.11.11.1.1.2.3.1.cmml"><mo id="S4.Ex1.m1.11.11.1.1.2.3.2.1" stretchy="false" xref="S4.Ex1.m1.11.11.1.1.2.3.1.cmml">(</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.1.1" xref="S4.Ex1.m1.1.1.cmml">𝒳</mi><mo id="S4.Ex1.m1.11.11.1.1.2.3.2.2" xref="S4.Ex1.m1.11.11.1.1.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.2.2" xref="S4.Ex1.m1.2.2.cmml">𝒴</mi><mo id="S4.Ex1.m1.11.11.1.1.2.3.2.3" xref="S4.Ex1.m1.11.11.1.1.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.3.3" xref="S4.Ex1.m1.3.3.cmml">𝒜</mi><mo id="S4.Ex1.m1.11.11.1.1.2.3.2.4" xref="S4.Ex1.m1.11.11.1.1.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.4.4" xref="S4.Ex1.m1.4.4.cmml">ℬ</mi><mo id="S4.Ex1.m1.11.11.1.1.2.3.2.5" stretchy="false" xref="S4.Ex1.m1.11.11.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex1.m1.11.11.1.1.1" rspace="0.111em" xref="S4.Ex1.m1.11.11.1.1.1.cmml">=</mo><mrow id="S4.Ex1.m1.11.11.1.1.3" xref="S4.Ex1.m1.11.11.1.1.3.cmml"><mrow id="S4.Ex1.m1.11.11.1.1.3.2" xref="S4.Ex1.m1.11.11.1.1.3.2.cmml"><munder id="S4.Ex1.m1.11.11.1.1.3.2.1" xref="S4.Ex1.m1.11.11.1.1.3.2.1.cmml"><mo id="S4.Ex1.m1.11.11.1.1.3.2.1.2" movablelimits="false" xref="S4.Ex1.m1.11.11.1.1.3.2.1.2.cmml">∑</mo><mrow id="S4.Ex1.m1.11.11.1.1.3.2.1.3" xref="S4.Ex1.m1.11.11.1.1.3.2.1.3.cmml"><mi id="S4.Ex1.m1.11.11.1.1.3.2.1.3.2" xref="S4.Ex1.m1.11.11.1.1.3.2.1.3.2.cmml">x</mi><mo id="S4.Ex1.m1.11.11.1.1.3.2.1.3.1" xref="S4.Ex1.m1.11.11.1.1.3.2.1.3.1.cmml">∈</mo><mi id="S4.Ex1.m1.11.11.1.1.3.2.1.3.3" xref="S4.Ex1.m1.11.11.1.1.3.2.1.3.3.cmml">X</mi></mrow></munder><mrow id="S4.Ex1.m1.11.11.1.1.3.2.2" xref="S4.Ex1.m1.11.11.1.1.3.2.2.cmml"><mi id="S4.Ex1.m1.11.11.1.1.3.2.2.2" xref="S4.Ex1.m1.11.11.1.1.3.2.2.2.cmml">s</mi><mo id="S4.Ex1.m1.11.11.1.1.3.2.2.1" xref="S4.Ex1.m1.11.11.1.1.3.2.2.1.cmml">⁢</mo><mrow id="S4.Ex1.m1.11.11.1.1.3.2.2.3.2" xref="S4.Ex1.m1.11.11.1.1.3.2.2.3.1.cmml"><mo id="S4.Ex1.m1.11.11.1.1.3.2.2.3.2.1" stretchy="false" xref="S4.Ex1.m1.11.11.1.1.3.2.2.3.1.cmml">(</mo><mi id="S4.Ex1.m1.5.5" xref="S4.Ex1.m1.5.5.cmml">x</mi><mo id="S4.Ex1.m1.11.11.1.1.3.2.2.3.2.2" xref="S4.Ex1.m1.11.11.1.1.3.2.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.6.6" xref="S4.Ex1.m1.6.6.cmml">𝒜</mi><mo id="S4.Ex1.m1.11.11.1.1.3.2.2.3.2.3" xref="S4.Ex1.m1.11.11.1.1.3.2.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.7.7" xref="S4.Ex1.m1.7.7.cmml">ℬ</mi><mo id="S4.Ex1.m1.11.11.1.1.3.2.2.3.2.4" stretchy="false" xref="S4.Ex1.m1.11.11.1.1.3.2.2.3.1.cmml">)</mo></mrow></mrow></mrow><mo id="S4.Ex1.m1.11.11.1.1.3.1" rspace="0.055em" xref="S4.Ex1.m1.11.11.1.1.3.1.cmml">−</mo><mrow id="S4.Ex1.m1.11.11.1.1.3.3" xref="S4.Ex1.m1.11.11.1.1.3.3.cmml"><munder id="S4.Ex1.m1.11.11.1.1.3.3.1" xref="S4.Ex1.m1.11.11.1.1.3.3.1.cmml"><mo id="S4.Ex1.m1.11.11.1.1.3.3.1.2" movablelimits="false" xref="S4.Ex1.m1.11.11.1.1.3.3.1.2.cmml">∑</mo><mrow id="S4.Ex1.m1.11.11.1.1.3.3.1.3" xref="S4.Ex1.m1.11.11.1.1.3.3.1.3.cmml"><mi id="S4.Ex1.m1.11.11.1.1.3.3.1.3.2" xref="S4.Ex1.m1.11.11.1.1.3.3.1.3.2.cmml">y</mi><mo id="S4.Ex1.m1.11.11.1.1.3.3.1.3.1" xref="S4.Ex1.m1.11.11.1.1.3.3.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.11.11.1.1.3.3.1.3.3" xref="S4.Ex1.m1.11.11.1.1.3.3.1.3.3.cmml">𝒴</mi></mrow></munder><mrow id="S4.Ex1.m1.11.11.1.1.3.3.2" xref="S4.Ex1.m1.11.11.1.1.3.3.2.cmml"><mi id="S4.Ex1.m1.11.11.1.1.3.3.2.2" xref="S4.Ex1.m1.11.11.1.1.3.3.2.2.cmml">s</mi><mo id="S4.Ex1.m1.11.11.1.1.3.3.2.1" xref="S4.Ex1.m1.11.11.1.1.3.3.2.1.cmml">⁢</mo><mrow id="S4.Ex1.m1.11.11.1.1.3.3.2.3.2" xref="S4.Ex1.m1.11.11.1.1.3.3.2.3.1.cmml"><mo id="S4.Ex1.m1.11.11.1.1.3.3.2.3.2.1" stretchy="false" xref="S4.Ex1.m1.11.11.1.1.3.3.2.3.1.cmml">(</mo><mi id="S4.Ex1.m1.8.8" xref="S4.Ex1.m1.8.8.cmml">y</mi><mo id="S4.Ex1.m1.11.11.1.1.3.3.2.3.2.2" xref="S4.Ex1.m1.11.11.1.1.3.3.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.9.9" xref="S4.Ex1.m1.9.9.cmml">𝒜</mi><mo id="S4.Ex1.m1.11.11.1.1.3.3.2.3.2.3" xref="S4.Ex1.m1.11.11.1.1.3.3.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex1.m1.10.10" xref="S4.Ex1.m1.10.10.cmml">ℬ</mi><mo id="S4.Ex1.m1.11.11.1.1.3.3.2.3.2.4" stretchy="false" xref="S4.Ex1.m1.11.11.1.1.3.3.2.3.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S4.Ex1.m1.11.11.1.2" xref="S4.Ex1.m1.11.11.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex1.m1.11b"><apply id="S4.Ex1.m1.11.11.1.1.cmml" xref="S4.Ex1.m1.11.11.1"><eq id="S4.Ex1.m1.11.11.1.1.1.cmml" xref="S4.Ex1.m1.11.11.1.1.1"></eq><apply id="S4.Ex1.m1.11.11.1.1.2.cmml" xref="S4.Ex1.m1.11.11.1.1.2"><times id="S4.Ex1.m1.11.11.1.1.2.1.cmml" xref="S4.Ex1.m1.11.11.1.1.2.1"></times><ci id="S4.Ex1.m1.11.11.1.1.2.2.cmml" xref="S4.Ex1.m1.11.11.1.1.2.2">𝑠</ci><vector id="S4.Ex1.m1.11.11.1.1.2.3.1.cmml" xref="S4.Ex1.m1.11.11.1.1.2.3.2"><ci id="S4.Ex1.m1.1.1.cmml" xref="S4.Ex1.m1.1.1">𝒳</ci><ci id="S4.Ex1.m1.2.2.cmml" xref="S4.Ex1.m1.2.2">𝒴</ci><ci id="S4.Ex1.m1.3.3.cmml" xref="S4.Ex1.m1.3.3">𝒜</ci><ci id="S4.Ex1.m1.4.4.cmml" xref="S4.Ex1.m1.4.4">ℬ</ci></vector></apply><apply id="S4.Ex1.m1.11.11.1.1.3.cmml" xref="S4.Ex1.m1.11.11.1.1.3"><minus id="S4.Ex1.m1.11.11.1.1.3.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.1"></minus><apply id="S4.Ex1.m1.11.11.1.1.3.2.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2"><apply id="S4.Ex1.m1.11.11.1.1.3.2.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.11.11.1.1.3.2.1.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.1">subscript</csymbol><sum id="S4.Ex1.m1.11.11.1.1.3.2.1.2.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.1.2"></sum><apply id="S4.Ex1.m1.11.11.1.1.3.2.1.3.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.1.3"><in id="S4.Ex1.m1.11.11.1.1.3.2.1.3.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.1.3.1"></in><ci id="S4.Ex1.m1.11.11.1.1.3.2.1.3.2.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.1.3.2">𝑥</ci><ci id="S4.Ex1.m1.11.11.1.1.3.2.1.3.3.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.1.3.3">𝑋</ci></apply></apply><apply id="S4.Ex1.m1.11.11.1.1.3.2.2.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.2"><times id="S4.Ex1.m1.11.11.1.1.3.2.2.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.2.1"></times><ci id="S4.Ex1.m1.11.11.1.1.3.2.2.2.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.2.2">𝑠</ci><vector id="S4.Ex1.m1.11.11.1.1.3.2.2.3.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.2.2.3.2"><ci id="S4.Ex1.m1.5.5.cmml" xref="S4.Ex1.m1.5.5">𝑥</ci><ci id="S4.Ex1.m1.6.6.cmml" xref="S4.Ex1.m1.6.6">𝒜</ci><ci id="S4.Ex1.m1.7.7.cmml" xref="S4.Ex1.m1.7.7">ℬ</ci></vector></apply></apply><apply id="S4.Ex1.m1.11.11.1.1.3.3.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3"><apply id="S4.Ex1.m1.11.11.1.1.3.3.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.1"><csymbol cd="ambiguous" id="S4.Ex1.m1.11.11.1.1.3.3.1.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.1">subscript</csymbol><sum id="S4.Ex1.m1.11.11.1.1.3.3.1.2.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.1.2"></sum><apply id="S4.Ex1.m1.11.11.1.1.3.3.1.3.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.1.3"><in id="S4.Ex1.m1.11.11.1.1.3.3.1.3.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.1.3.1"></in><ci id="S4.Ex1.m1.11.11.1.1.3.3.1.3.2.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.1.3.2">𝑦</ci><ci id="S4.Ex1.m1.11.11.1.1.3.3.1.3.3.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.1.3.3">𝒴</ci></apply></apply><apply id="S4.Ex1.m1.11.11.1.1.3.3.2.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.2"><times id="S4.Ex1.m1.11.11.1.1.3.3.2.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.2.1"></times><ci id="S4.Ex1.m1.11.11.1.1.3.3.2.2.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.2.2">𝑠</ci><vector id="S4.Ex1.m1.11.11.1.1.3.3.2.3.1.cmml" xref="S4.Ex1.m1.11.11.1.1.3.3.2.3.2"><ci id="S4.Ex1.m1.8.8.cmml" xref="S4.Ex1.m1.8.8">𝑦</ci><ci id="S4.Ex1.m1.9.9.cmml" xref="S4.Ex1.m1.9.9">𝒜</ci><ci id="S4.Ex1.m1.10.10.cmml" xref="S4.Ex1.m1.10.10">ℬ</ci></vector></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex1.m1.11c">s(\mathcal{X},\mathcal{Y},\mathcal{A},\mathcal{B})=\sum_{x\in X}s(x,\mathcal{A%
},\mathcal{B})-\sum_{y\in\mathcal{Y}}s(y,\mathcal{A},\mathcal{B}),</annotation><annotation encoding="application/x-llamapun" id="S4.Ex1.m1.11d">italic_s ( caligraphic_X , caligraphic_Y , caligraphic_A , caligraphic_B ) = ∑ start_POSTSUBSCRIPT italic_x ∈ italic_X end_POSTSUBSCRIPT italic_s ( italic_x , caligraphic_A , caligraphic_B ) - ∑ start_POSTSUBSCRIPT italic_y ∈ caligraphic_Y end_POSTSUBSCRIPT italic_s ( italic_y , caligraphic_A , caligraphic_B ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S4.SS7.SSS1.p2.9">where <math alttext="s(w,\mathcal{A},\mathcal{B})" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p2.5.m1.3"><semantics id="S4.SS7.SSS1.p2.5.m1.3a"><mrow id="S4.SS7.SSS1.p2.5.m1.3.4" xref="S4.SS7.SSS1.p2.5.m1.3.4.cmml"><mi id="S4.SS7.SSS1.p2.5.m1.3.4.2" xref="S4.SS7.SSS1.p2.5.m1.3.4.2.cmml">s</mi><mo id="S4.SS7.SSS1.p2.5.m1.3.4.1" xref="S4.SS7.SSS1.p2.5.m1.3.4.1.cmml">⁢</mo><mrow id="S4.SS7.SSS1.p2.5.m1.3.4.3.2" xref="S4.SS7.SSS1.p2.5.m1.3.4.3.1.cmml"><mo id="S4.SS7.SSS1.p2.5.m1.3.4.3.2.1" stretchy="false" xref="S4.SS7.SSS1.p2.5.m1.3.4.3.1.cmml">(</mo><mi id="S4.SS7.SSS1.p2.5.m1.1.1" xref="S4.SS7.SSS1.p2.5.m1.1.1.cmml">w</mi><mo id="S4.SS7.SSS1.p2.5.m1.3.4.3.2.2" xref="S4.SS7.SSS1.p2.5.m1.3.4.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p2.5.m1.2.2" xref="S4.SS7.SSS1.p2.5.m1.2.2.cmml">𝒜</mi><mo id="S4.SS7.SSS1.p2.5.m1.3.4.3.2.3" xref="S4.SS7.SSS1.p2.5.m1.3.4.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p2.5.m1.3.3" xref="S4.SS7.SSS1.p2.5.m1.3.3.cmml">ℬ</mi><mo id="S4.SS7.SSS1.p2.5.m1.3.4.3.2.4" stretchy="false" xref="S4.SS7.SSS1.p2.5.m1.3.4.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p2.5.m1.3b"><apply id="S4.SS7.SSS1.p2.5.m1.3.4.cmml" xref="S4.SS7.SSS1.p2.5.m1.3.4"><times id="S4.SS7.SSS1.p2.5.m1.3.4.1.cmml" xref="S4.SS7.SSS1.p2.5.m1.3.4.1"></times><ci id="S4.SS7.SSS1.p2.5.m1.3.4.2.cmml" xref="S4.SS7.SSS1.p2.5.m1.3.4.2">𝑠</ci><vector id="S4.SS7.SSS1.p2.5.m1.3.4.3.1.cmml" xref="S4.SS7.SSS1.p2.5.m1.3.4.3.2"><ci id="S4.SS7.SSS1.p2.5.m1.1.1.cmml" xref="S4.SS7.SSS1.p2.5.m1.1.1">𝑤</ci><ci id="S4.SS7.SSS1.p2.5.m1.2.2.cmml" xref="S4.SS7.SSS1.p2.5.m1.2.2">𝒜</ci><ci id="S4.SS7.SSS1.p2.5.m1.3.3.cmml" xref="S4.SS7.SSS1.p2.5.m1.3.3">ℬ</ci></vector></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p2.5.m1.3c">s(w,\mathcal{A},\mathcal{B})</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p2.5.m1.3d">italic_s ( italic_w , caligraphic_A , caligraphic_B )</annotation></semantics></math> represents the difference between the average of the cosine similarity of word <math alttext="w" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p2.6.m2.1"><semantics id="S4.SS7.SSS1.p2.6.m2.1a"><mi id="S4.SS7.SSS1.p2.6.m2.1.1" xref="S4.SS7.SSS1.p2.6.m2.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p2.6.m2.1b"><ci id="S4.SS7.SSS1.p2.6.m2.1.1.cmml" xref="S4.SS7.SSS1.p2.6.m2.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p2.6.m2.1c">w</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p2.6.m2.1d">italic_w</annotation></semantics></math> with all words in <math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p2.7.m3.1"><semantics id="S4.SS7.SSS1.p2.7.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p2.7.m3.1.1" xref="S4.SS7.SSS1.p2.7.m3.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p2.7.m3.1b"><ci id="S4.SS7.SSS1.p2.7.m3.1.1.cmml" xref="S4.SS7.SSS1.p2.7.m3.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p2.7.m3.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p2.7.m3.1d">caligraphic_A</annotation></semantics></math> and the average of the cosine similarity of word <math alttext="w" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p2.8.m4.1"><semantics id="S4.SS7.SSS1.p2.8.m4.1a"><mi id="S4.SS7.SSS1.p2.8.m4.1.1" xref="S4.SS7.SSS1.p2.8.m4.1.1.cmml">w</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p2.8.m4.1b"><ci id="S4.SS7.SSS1.p2.8.m4.1.1.cmml" xref="S4.SS7.SSS1.p2.8.m4.1.1">𝑤</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p2.8.m4.1c">w</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p2.8.m4.1d">italic_w</annotation></semantics></math> to all words in <math alttext="\mathcal{B}" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p2.9.m5.1"><semantics id="S4.SS7.SSS1.p2.9.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p2.9.m5.1.1" xref="S4.SS7.SSS1.p2.9.m5.1.1.cmml">ℬ</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p2.9.m5.1b"><ci id="S4.SS7.SSS1.p2.9.m5.1.1.cmml" xref="S4.SS7.SSS1.p2.9.m5.1.1">ℬ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p2.9.m5.1c">\mathcal{B}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p2.9.m5.1d">caligraphic_B</annotation></semantics></math>, and it is defined as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS7.SSS1.p3">
<table class="ltx_equation ltx_eqn_table" id="S4.Ex2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s(w,\mathcal{A},\mathcal{B})=\frac{1}{|\mathcal{A}|}\sum_{a\in\mathcal{A}}\cos%
(w,a)-\frac{1}{|\mathcal{B}|}\sum_{b\in\mathcal{B}}\cos(w,b)," class="ltx_Math" display="block" id="S4.Ex2.m1.12"><semantics id="S4.Ex2.m1.12a"><mrow id="S4.Ex2.m1.12.12.1" xref="S4.Ex2.m1.12.12.1.1.cmml"><mrow id="S4.Ex2.m1.12.12.1.1" xref="S4.Ex2.m1.12.12.1.1.cmml"><mrow id="S4.Ex2.m1.12.12.1.1.2" xref="S4.Ex2.m1.12.12.1.1.2.cmml"><mi id="S4.Ex2.m1.12.12.1.1.2.2" xref="S4.Ex2.m1.12.12.1.1.2.2.cmml">s</mi><mo id="S4.Ex2.m1.12.12.1.1.2.1" xref="S4.Ex2.m1.12.12.1.1.2.1.cmml">⁢</mo><mrow id="S4.Ex2.m1.12.12.1.1.2.3.2" xref="S4.Ex2.m1.12.12.1.1.2.3.1.cmml"><mo id="S4.Ex2.m1.12.12.1.1.2.3.2.1" stretchy="false" xref="S4.Ex2.m1.12.12.1.1.2.3.1.cmml">(</mo><mi id="S4.Ex2.m1.3.3" xref="S4.Ex2.m1.3.3.cmml">w</mi><mo id="S4.Ex2.m1.12.12.1.1.2.3.2.2" xref="S4.Ex2.m1.12.12.1.1.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex2.m1.4.4" xref="S4.Ex2.m1.4.4.cmml">𝒜</mi><mo id="S4.Ex2.m1.12.12.1.1.2.3.2.3" xref="S4.Ex2.m1.12.12.1.1.2.3.1.cmml">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex2.m1.5.5" xref="S4.Ex2.m1.5.5.cmml">ℬ</mi><mo id="S4.Ex2.m1.12.12.1.1.2.3.2.4" stretchy="false" xref="S4.Ex2.m1.12.12.1.1.2.3.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex2.m1.12.12.1.1.1" xref="S4.Ex2.m1.12.12.1.1.1.cmml">=</mo><mrow id="S4.Ex2.m1.12.12.1.1.3" xref="S4.Ex2.m1.12.12.1.1.3.cmml"><mrow id="S4.Ex2.m1.12.12.1.1.3.2" xref="S4.Ex2.m1.12.12.1.1.3.2.cmml"><mfrac id="S4.Ex2.m1.1.1" xref="S4.Ex2.m1.1.1.cmml"><mn id="S4.Ex2.m1.1.1.3" xref="S4.Ex2.m1.1.1.3.cmml">1</mn><mrow id="S4.Ex2.m1.1.1.1.3" xref="S4.Ex2.m1.1.1.1.2.cmml"><mo id="S4.Ex2.m1.1.1.1.3.1" stretchy="false" xref="S4.Ex2.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex2.m1.1.1.1.1" xref="S4.Ex2.m1.1.1.1.1.cmml">𝒜</mi><mo id="S4.Ex2.m1.1.1.1.3.2" stretchy="false" xref="S4.Ex2.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.Ex2.m1.12.12.1.1.3.2.1" xref="S4.Ex2.m1.12.12.1.1.3.2.1.cmml">⁢</mo><mrow id="S4.Ex2.m1.12.12.1.1.3.2.2" xref="S4.Ex2.m1.12.12.1.1.3.2.2.cmml"><munder id="S4.Ex2.m1.12.12.1.1.3.2.2.1" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.cmml"><mo id="S4.Ex2.m1.12.12.1.1.3.2.2.1.2" movablelimits="false" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.2.cmml">∑</mo><mrow id="S4.Ex2.m1.12.12.1.1.3.2.2.1.3" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.cmml"><mi id="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.2" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.2.cmml">a</mi><mo id="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.1" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.3" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.3.cmml">𝒜</mi></mrow></munder><mrow id="S4.Ex2.m1.12.12.1.1.3.2.2.2.2" xref="S4.Ex2.m1.12.12.1.1.3.2.2.2.1.cmml"><mi id="S4.Ex2.m1.6.6" xref="S4.Ex2.m1.6.6.cmml">cos</mi><mo id="S4.Ex2.m1.12.12.1.1.3.2.2.2.2a" xref="S4.Ex2.m1.12.12.1.1.3.2.2.2.1.cmml">⁡</mo><mrow id="S4.Ex2.m1.12.12.1.1.3.2.2.2.2.1" xref="S4.Ex2.m1.12.12.1.1.3.2.2.2.1.cmml"><mo id="S4.Ex2.m1.12.12.1.1.3.2.2.2.2.1.1" stretchy="false" xref="S4.Ex2.m1.12.12.1.1.3.2.2.2.1.cmml">(</mo><mi id="S4.Ex2.m1.7.7" xref="S4.Ex2.m1.7.7.cmml">w</mi><mo id="S4.Ex2.m1.12.12.1.1.3.2.2.2.2.1.2" xref="S4.Ex2.m1.12.12.1.1.3.2.2.2.1.cmml">,</mo><mi id="S4.Ex2.m1.8.8" xref="S4.Ex2.m1.8.8.cmml">a</mi><mo id="S4.Ex2.m1.12.12.1.1.3.2.2.2.2.1.3" stretchy="false" xref="S4.Ex2.m1.12.12.1.1.3.2.2.2.1.cmml">)</mo></mrow></mrow></mrow></mrow><mo id="S4.Ex2.m1.12.12.1.1.3.1" xref="S4.Ex2.m1.12.12.1.1.3.1.cmml">−</mo><mrow id="S4.Ex2.m1.12.12.1.1.3.3" xref="S4.Ex2.m1.12.12.1.1.3.3.cmml"><mfrac id="S4.Ex2.m1.2.2" xref="S4.Ex2.m1.2.2.cmml"><mn id="S4.Ex2.m1.2.2.3" xref="S4.Ex2.m1.2.2.3.cmml">1</mn><mrow id="S4.Ex2.m1.2.2.1.3" xref="S4.Ex2.m1.2.2.1.2.cmml"><mo id="S4.Ex2.m1.2.2.1.3.1" stretchy="false" xref="S4.Ex2.m1.2.2.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex2.m1.2.2.1.1" xref="S4.Ex2.m1.2.2.1.1.cmml">ℬ</mi><mo id="S4.Ex2.m1.2.2.1.3.2" stretchy="false" xref="S4.Ex2.m1.2.2.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.Ex2.m1.12.12.1.1.3.3.1" xref="S4.Ex2.m1.12.12.1.1.3.3.1.cmml">⁢</mo><mrow id="S4.Ex2.m1.12.12.1.1.3.3.2" xref="S4.Ex2.m1.12.12.1.1.3.3.2.cmml"><munder id="S4.Ex2.m1.12.12.1.1.3.3.2.1" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.cmml"><mo id="S4.Ex2.m1.12.12.1.1.3.3.2.1.2" movablelimits="false" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.2.cmml">∑</mo><mrow id="S4.Ex2.m1.12.12.1.1.3.3.2.1.3" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.cmml"><mi id="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.2" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.2.cmml">b</mi><mo id="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.1" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.3" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.3.cmml">ℬ</mi></mrow></munder><mrow id="S4.Ex2.m1.12.12.1.1.3.3.2.2.2" xref="S4.Ex2.m1.12.12.1.1.3.3.2.2.1.cmml"><mi id="S4.Ex2.m1.9.9" xref="S4.Ex2.m1.9.9.cmml">cos</mi><mo id="S4.Ex2.m1.12.12.1.1.3.3.2.2.2a" xref="S4.Ex2.m1.12.12.1.1.3.3.2.2.1.cmml">⁡</mo><mrow id="S4.Ex2.m1.12.12.1.1.3.3.2.2.2.1" xref="S4.Ex2.m1.12.12.1.1.3.3.2.2.1.cmml"><mo id="S4.Ex2.m1.12.12.1.1.3.3.2.2.2.1.1" stretchy="false" xref="S4.Ex2.m1.12.12.1.1.3.3.2.2.1.cmml">(</mo><mi id="S4.Ex2.m1.10.10" xref="S4.Ex2.m1.10.10.cmml">w</mi><mo id="S4.Ex2.m1.12.12.1.1.3.3.2.2.2.1.2" xref="S4.Ex2.m1.12.12.1.1.3.3.2.2.1.cmml">,</mo><mi id="S4.Ex2.m1.11.11" xref="S4.Ex2.m1.11.11.cmml">b</mi><mo id="S4.Ex2.m1.12.12.1.1.3.3.2.2.2.1.3" stretchy="false" xref="S4.Ex2.m1.12.12.1.1.3.3.2.2.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S4.Ex2.m1.12.12.1.2" xref="S4.Ex2.m1.12.12.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex2.m1.12b"><apply id="S4.Ex2.m1.12.12.1.1.cmml" xref="S4.Ex2.m1.12.12.1"><eq id="S4.Ex2.m1.12.12.1.1.1.cmml" xref="S4.Ex2.m1.12.12.1.1.1"></eq><apply id="S4.Ex2.m1.12.12.1.1.2.cmml" xref="S4.Ex2.m1.12.12.1.1.2"><times id="S4.Ex2.m1.12.12.1.1.2.1.cmml" xref="S4.Ex2.m1.12.12.1.1.2.1"></times><ci id="S4.Ex2.m1.12.12.1.1.2.2.cmml" xref="S4.Ex2.m1.12.12.1.1.2.2">𝑠</ci><vector id="S4.Ex2.m1.12.12.1.1.2.3.1.cmml" xref="S4.Ex2.m1.12.12.1.1.2.3.2"><ci id="S4.Ex2.m1.3.3.cmml" xref="S4.Ex2.m1.3.3">𝑤</ci><ci id="S4.Ex2.m1.4.4.cmml" xref="S4.Ex2.m1.4.4">𝒜</ci><ci id="S4.Ex2.m1.5.5.cmml" xref="S4.Ex2.m1.5.5">ℬ</ci></vector></apply><apply id="S4.Ex2.m1.12.12.1.1.3.cmml" xref="S4.Ex2.m1.12.12.1.1.3"><minus id="S4.Ex2.m1.12.12.1.1.3.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.1"></minus><apply id="S4.Ex2.m1.12.12.1.1.3.2.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2"><times id="S4.Ex2.m1.12.12.1.1.3.2.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.1"></times><apply id="S4.Ex2.m1.1.1.cmml" xref="S4.Ex2.m1.1.1"><divide id="S4.Ex2.m1.1.1.2.cmml" xref="S4.Ex2.m1.1.1"></divide><cn id="S4.Ex2.m1.1.1.3.cmml" type="integer" xref="S4.Ex2.m1.1.1.3">1</cn><apply id="S4.Ex2.m1.1.1.1.2.cmml" xref="S4.Ex2.m1.1.1.1.3"><abs id="S4.Ex2.m1.1.1.1.2.1.cmml" xref="S4.Ex2.m1.1.1.1.3.1"></abs><ci id="S4.Ex2.m1.1.1.1.1.cmml" xref="S4.Ex2.m1.1.1.1.1">𝒜</ci></apply></apply><apply id="S4.Ex2.m1.12.12.1.1.3.2.2.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.2"><apply id="S4.Ex2.m1.12.12.1.1.3.2.2.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.12.12.1.1.3.2.2.1.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1">subscript</csymbol><sum id="S4.Ex2.m1.12.12.1.1.3.2.2.1.2.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.2"></sum><apply id="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.3"><in id="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.1"></in><ci id="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.2.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.2">𝑎</ci><ci id="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.3.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.2.1.3.3">𝒜</ci></apply></apply><apply id="S4.Ex2.m1.12.12.1.1.3.2.2.2.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.2.2.2.2"><cos id="S4.Ex2.m1.6.6.cmml" xref="S4.Ex2.m1.6.6"></cos><ci id="S4.Ex2.m1.7.7.cmml" xref="S4.Ex2.m1.7.7">𝑤</ci><ci id="S4.Ex2.m1.8.8.cmml" xref="S4.Ex2.m1.8.8">𝑎</ci></apply></apply></apply><apply id="S4.Ex2.m1.12.12.1.1.3.3.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3"><times id="S4.Ex2.m1.12.12.1.1.3.3.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.1"></times><apply id="S4.Ex2.m1.2.2.cmml" xref="S4.Ex2.m1.2.2"><divide id="S4.Ex2.m1.2.2.2.cmml" xref="S4.Ex2.m1.2.2"></divide><cn id="S4.Ex2.m1.2.2.3.cmml" type="integer" xref="S4.Ex2.m1.2.2.3">1</cn><apply id="S4.Ex2.m1.2.2.1.2.cmml" xref="S4.Ex2.m1.2.2.1.3"><abs id="S4.Ex2.m1.2.2.1.2.1.cmml" xref="S4.Ex2.m1.2.2.1.3.1"></abs><ci id="S4.Ex2.m1.2.2.1.1.cmml" xref="S4.Ex2.m1.2.2.1.1">ℬ</ci></apply></apply><apply id="S4.Ex2.m1.12.12.1.1.3.3.2.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.2"><apply id="S4.Ex2.m1.12.12.1.1.3.3.2.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1"><csymbol cd="ambiguous" id="S4.Ex2.m1.12.12.1.1.3.3.2.1.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1">subscript</csymbol><sum id="S4.Ex2.m1.12.12.1.1.3.3.2.1.2.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.2"></sum><apply id="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.3"><in id="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.1"></in><ci id="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.2.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.2">𝑏</ci><ci id="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.3.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.2.1.3.3">ℬ</ci></apply></apply><apply id="S4.Ex2.m1.12.12.1.1.3.3.2.2.1.cmml" xref="S4.Ex2.m1.12.12.1.1.3.3.2.2.2"><cos id="S4.Ex2.m1.9.9.cmml" xref="S4.Ex2.m1.9.9"></cos><ci id="S4.Ex2.m1.10.10.cmml" xref="S4.Ex2.m1.10.10">𝑤</ci><ci id="S4.Ex2.m1.11.11.cmml" xref="S4.Ex2.m1.11.11">𝑏</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex2.m1.12c">s(w,\mathcal{A},\mathcal{B})=\frac{1}{|\mathcal{A}|}\sum_{a\in\mathcal{A}}\cos%
(w,a)-\frac{1}{|\mathcal{B}|}\sum_{b\in\mathcal{B}}\cos(w,b),</annotation><annotation encoding="application/x-llamapun" id="S4.Ex2.m1.12d">italic_s ( italic_w , caligraphic_A , caligraphic_B ) = divide start_ARG 1 end_ARG start_ARG | caligraphic_A | end_ARG ∑ start_POSTSUBSCRIPT italic_a ∈ caligraphic_A end_POSTSUBSCRIPT roman_cos ( italic_w , italic_a ) - divide start_ARG 1 end_ARG start_ARG | caligraphic_B | end_ARG ∑ start_POSTSUBSCRIPT italic_b ∈ caligraphic_B end_POSTSUBSCRIPT roman_cos ( italic_w , italic_b ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS7.SSS1.p4">
<p class="ltx_p" id="S4.SS7.SSS1.p4.3">where <math alttext="w\in\mathcal{X}" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p4.1.m1.1"><semantics id="S4.SS7.SSS1.p4.1.m1.1a"><mrow id="S4.SS7.SSS1.p4.1.m1.1.1" xref="S4.SS7.SSS1.p4.1.m1.1.1.cmml"><mi id="S4.SS7.SSS1.p4.1.m1.1.1.2" xref="S4.SS7.SSS1.p4.1.m1.1.1.2.cmml">w</mi><mo id="S4.SS7.SSS1.p4.1.m1.1.1.1" xref="S4.SS7.SSS1.p4.1.m1.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p4.1.m1.1.1.3" xref="S4.SS7.SSS1.p4.1.m1.1.1.3.cmml">𝒳</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p4.1.m1.1b"><apply id="S4.SS7.SSS1.p4.1.m1.1.1.cmml" xref="S4.SS7.SSS1.p4.1.m1.1.1"><in id="S4.SS7.SSS1.p4.1.m1.1.1.1.cmml" xref="S4.SS7.SSS1.p4.1.m1.1.1.1"></in><ci id="S4.SS7.SSS1.p4.1.m1.1.1.2.cmml" xref="S4.SS7.SSS1.p4.1.m1.1.1.2">𝑤</ci><ci id="S4.SS7.SSS1.p4.1.m1.1.1.3.cmml" xref="S4.SS7.SSS1.p4.1.m1.1.1.3">𝒳</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p4.1.m1.1c">w\in\mathcal{X}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p4.1.m1.1d">italic_w ∈ caligraphic_X</annotation></semantics></math> or <math alttext="\mathcal{Y}" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p4.2.m2.1"><semantics id="S4.SS7.SSS1.p4.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S4.SS7.SSS1.p4.2.m2.1.1" xref="S4.SS7.SSS1.p4.2.m2.1.1.cmml">𝒴</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p4.2.m2.1b"><ci id="S4.SS7.SSS1.p4.2.m2.1.1.cmml" xref="S4.SS7.SSS1.p4.2.m2.1.1">𝒴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p4.2.m2.1c">\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p4.2.m2.1d">caligraphic_Y</annotation></semantics></math>, and <math alttext="\cos(\cdot,\cdot)" class="ltx_Math" display="inline" id="S4.SS7.SSS1.p4.3.m3.3"><semantics id="S4.SS7.SSS1.p4.3.m3.3a"><mrow id="S4.SS7.SSS1.p4.3.m3.3.4.2" xref="S4.SS7.SSS1.p4.3.m3.3.4.1.cmml"><mi id="S4.SS7.SSS1.p4.3.m3.1.1" xref="S4.SS7.SSS1.p4.3.m3.1.1.cmml">cos</mi><mo id="S4.SS7.SSS1.p4.3.m3.3.4.2a" xref="S4.SS7.SSS1.p4.3.m3.3.4.1.cmml">⁡</mo><mrow id="S4.SS7.SSS1.p4.3.m3.3.4.2.1" xref="S4.SS7.SSS1.p4.3.m3.3.4.1.cmml"><mo id="S4.SS7.SSS1.p4.3.m3.3.4.2.1.1" stretchy="false" xref="S4.SS7.SSS1.p4.3.m3.3.4.1.cmml">(</mo><mo id="S4.SS7.SSS1.p4.3.m3.2.2" lspace="0em" rspace="0em" xref="S4.SS7.SSS1.p4.3.m3.2.2.cmml">⋅</mo><mo id="S4.SS7.SSS1.p4.3.m3.3.4.2.1.2" rspace="0em" xref="S4.SS7.SSS1.p4.3.m3.3.4.1.cmml">,</mo><mo id="S4.SS7.SSS1.p4.3.m3.3.3" lspace="0em" rspace="0em" xref="S4.SS7.SSS1.p4.3.m3.3.3.cmml">⋅</mo><mo id="S4.SS7.SSS1.p4.3.m3.3.4.2.1.3" stretchy="false" xref="S4.SS7.SSS1.p4.3.m3.3.4.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS1.p4.3.m3.3b"><apply id="S4.SS7.SSS1.p4.3.m3.3.4.1.cmml" xref="S4.SS7.SSS1.p4.3.m3.3.4.2"><cos id="S4.SS7.SSS1.p4.3.m3.1.1.cmml" xref="S4.SS7.SSS1.p4.3.m3.1.1"></cos><ci id="S4.SS7.SSS1.p4.3.m3.2.2.cmml" xref="S4.SS7.SSS1.p4.3.m3.2.2">⋅</ci><ci id="S4.SS7.SSS1.p4.3.m3.3.3.cmml" xref="S4.SS7.SSS1.p4.3.m3.3.3">⋅</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS1.p4.3.m3.3c">\cos(\cdot,\cdot)</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS1.p4.3.m3.3d">roman_cos ( ⋅ , ⋅ )</annotation></semantics></math> represents the cosine similarity. The normalized effect size is as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S4.Ex3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="d=\frac{\mu\left(\{s(x,\mathcal{A},\mathcal{B})\}_{x\in\mathcal{X}}\right)-\mu%
\left(\{s(y,\mathcal{A},\mathcal{B})\}_{y\in\mathcal{Y})}\right.}{\sigma\left(%
\{s(t,\mathcal{X},\mathcal{Y})\}_{t\in\mathcal{F}\cup\mathcal{B}}\right)}" class="ltx_math_unparsed" display="block" id="S4.Ex3.m1.10"><semantics id="S4.Ex3.m1.10a"><mrow id="S4.Ex3.m1.10.11"><mi id="S4.Ex3.m1.10.11.2">d</mi><mo id="S4.Ex3.m1.10.11.1">=</mo><mfrac id="S4.Ex3.m1.10.10"><mrow id="S4.Ex3.m1.6.6.6"><mi id="S4.Ex3.m1.6.6.6.7">μ</mi><mrow id="S4.Ex3.m1.6.6.6.8"><mo id="S4.Ex3.m1.6.6.6.8.1">(</mo><msub id="S4.Ex3.m1.6.6.6.8.2"><mrow id="S4.Ex3.m1.6.6.6.8.2.2"><mo id="S4.Ex3.m1.6.6.6.8.2.2.1" stretchy="false">{</mo><mi id="S4.Ex3.m1.6.6.6.8.2.2.2">s</mi><mrow id="S4.Ex3.m1.6.6.6.8.2.2.3"><mo id="S4.Ex3.m1.6.6.6.8.2.2.3.1" stretchy="false">(</mo><mi id="S4.Ex3.m1.1.1.1.1">x</mi><mo id="S4.Ex3.m1.6.6.6.8.2.2.3.2">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.2.2.2.2">𝒜</mi><mo id="S4.Ex3.m1.6.6.6.8.2.2.3.3">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.3.3.3.3">ℬ</mi><mo id="S4.Ex3.m1.6.6.6.8.2.2.3.4" stretchy="false">)</mo></mrow><mo id="S4.Ex3.m1.6.6.6.8.2.2.4" stretchy="false">}</mo></mrow><mrow id="S4.Ex3.m1.6.6.6.8.2.3"><mi id="S4.Ex3.m1.6.6.6.8.2.3.2">x</mi><mo id="S4.Ex3.m1.6.6.6.8.2.3.1">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.6.6.6.8.2.3.3">𝒳</mi></mrow></msub><mo id="S4.Ex3.m1.6.6.6.8.3">)</mo></mrow><mo id="S4.Ex3.m1.6.6.6.9">−</mo><mi id="S4.Ex3.m1.6.6.6.10">μ</mi><mrow id="S4.Ex3.m1.6.6.6.11"><mo id="S4.Ex3.m1.6.6.6.11.1">(</mo><msub id="S4.Ex3.m1.6.6.6.11.2"><mrow id="S4.Ex3.m1.6.6.6.11.2.2"><mo id="S4.Ex3.m1.6.6.6.11.2.2.1" stretchy="false">{</mo><mi id="S4.Ex3.m1.6.6.6.11.2.2.2">s</mi><mrow id="S4.Ex3.m1.6.6.6.11.2.2.3"><mo id="S4.Ex3.m1.6.6.6.11.2.2.3.1" stretchy="false">(</mo><mi id="S4.Ex3.m1.4.4.4.4">y</mi><mo id="S4.Ex3.m1.6.6.6.11.2.2.3.2">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.5.5.5.5">𝒜</mi><mo id="S4.Ex3.m1.6.6.6.11.2.2.3.3">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.6.6.6.6">ℬ</mi><mo id="S4.Ex3.m1.6.6.6.11.2.2.3.4" stretchy="false">)</mo></mrow><mo id="S4.Ex3.m1.6.6.6.11.2.2.4" stretchy="false">}</mo></mrow><mrow id="S4.Ex3.m1.6.6.6.11.2.3"><mi id="S4.Ex3.m1.6.6.6.11.2.3.1">y</mi><mo id="S4.Ex3.m1.6.6.6.11.2.3.2">∈</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.6.6.6.11.2.3.3">𝒴</mi><mo id="S4.Ex3.m1.6.6.6.11.2.3.4" stretchy="false">)</mo></mrow></msub></mrow></mrow><mrow id="S4.Ex3.m1.10.10.10"><mi id="S4.Ex3.m1.10.10.10.6">σ</mi><mo id="S4.Ex3.m1.10.10.10.5">⁢</mo><mrow id="S4.Ex3.m1.10.10.10.4.1"><mo id="S4.Ex3.m1.10.10.10.4.1.2">(</mo><msub id="S4.Ex3.m1.10.10.10.4.1.1"><mrow id="S4.Ex3.m1.10.10.10.4.1.1.1.1"><mo id="S4.Ex3.m1.10.10.10.4.1.1.1.1.2" stretchy="false">{</mo><mrow id="S4.Ex3.m1.10.10.10.4.1.1.1.1.1"><mi id="S4.Ex3.m1.10.10.10.4.1.1.1.1.1.2">s</mi><mo id="S4.Ex3.m1.10.10.10.4.1.1.1.1.1.1">⁢</mo><mrow id="S4.Ex3.m1.10.10.10.4.1.1.1.1.1.3.2"><mo id="S4.Ex3.m1.10.10.10.4.1.1.1.1.1.3.2.1" stretchy="false">(</mo><mi id="S4.Ex3.m1.7.7.7.1">t</mi><mo id="S4.Ex3.m1.10.10.10.4.1.1.1.1.1.3.2.2">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.8.8.8.2">𝒳</mi><mo id="S4.Ex3.m1.10.10.10.4.1.1.1.1.1.3.2.3">,</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.9.9.9.3">𝒴</mi><mo id="S4.Ex3.m1.10.10.10.4.1.1.1.1.1.3.2.4" stretchy="false">)</mo></mrow></mrow><mo id="S4.Ex3.m1.10.10.10.4.1.1.1.1.3" stretchy="false">}</mo></mrow><mrow id="S4.Ex3.m1.10.10.10.4.1.1.3"><mi id="S4.Ex3.m1.10.10.10.4.1.1.3.2">t</mi><mo id="S4.Ex3.m1.10.10.10.4.1.1.3.1">∈</mo><mrow id="S4.Ex3.m1.10.10.10.4.1.1.3.3"><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.10.10.10.4.1.1.3.3.2">ℱ</mi><mo id="S4.Ex3.m1.10.10.10.4.1.1.3.3.1">∪</mo><mi class="ltx_font_mathcaligraphic" id="S4.Ex3.m1.10.10.10.4.1.1.3.3.3">ℬ</mi></mrow></mrow></msub><mo id="S4.Ex3.m1.10.10.10.4.1.3">)</mo></mrow></mrow></mfrac></mrow><annotation encoding="application/x-tex" id="S4.Ex3.m1.10b">d=\frac{\mu\left(\{s(x,\mathcal{A},\mathcal{B})\}_{x\in\mathcal{X}}\right)-\mu%
\left(\{s(y,\mathcal{A},\mathcal{B})\}_{y\in\mathcal{Y})}\right.}{\sigma\left(%
\{s(t,\mathcal{X},\mathcal{Y})\}_{t\in\mathcal{F}\cup\mathcal{B}}\right)}</annotation><annotation encoding="application/x-llamapun" id="S4.Ex3.m1.10c">italic_d = divide start_ARG italic_μ ( { italic_s ( italic_x , caligraphic_A , caligraphic_B ) } start_POSTSUBSCRIPT italic_x ∈ caligraphic_X end_POSTSUBSCRIPT ) - italic_μ ( { italic_s ( italic_y , caligraphic_A , caligraphic_B ) } start_POSTSUBSCRIPT italic_y ∈ caligraphic_Y ) end_POSTSUBSCRIPT end_ARG start_ARG italic_σ ( { italic_s ( italic_t , caligraphic_X , caligraphic_Y ) } start_POSTSUBSCRIPT italic_t ∈ caligraphic_F ∪ caligraphic_B end_POSTSUBSCRIPT ) end_ARG</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS7.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.7.2 </span>Extrinsic Evaluation Metrics</h4>
<div class="ltx_para ltx_noindent" id="S4.SS7.SSS2.p1">
<p class="ltx_p" id="S4.SS7.SSS2.p1.1">Extrinsic bias evaluation metrics are employed to assess extrinsic bias by measuring the performance gap in the output of downstream tasks. These metrics are often accompanied by benchmark datasets that specifically measure bias in a particular task. Dhamala et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib82" title="">82</a>]</cite> introduces the Bias in Open-Ended Language Generation Dataset (BOLD), a comprehensive fairness benchmark dataset with a large scale. BOLD focuses on evaluating bias in five domains: gender, race, religion, profession, and political ideology, using natural prompts. By providing prompts that describe specific target populations, BOLD assesses the completions generated by language models over sentiment, toxicity, regard, emotion lexicons and gender polarity. Counterfactual Sentiment Bias (CSB) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib83" title="">83</a>]</cite> considers the fairness of the generated text under counterfactual evaluation, which inputs the conditions containing sensitive attributes to GPT-2, and then calculate the sentiment score of the generation. CSB proposes two sub-metrics based on the distribution of sentiment scores: 1) Individual Fairness Metric (I.F.) is the average of the Wasserstein-1<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib84" title="">84</a>]</cite> distance of the sentiment score distribution between each counterfactual sentence pair; 2) Group Fairness Metric (G.F.) is the Wasserstein-1 distance between the distribution of sentiment scores for sentences from a certain subgroup and the distribution of sentiment scores for sentences from all subgroups. They are formalized as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS7.SSS2.p2">
<table class="ltx_equation ltx_eqn_table" id="S4.Ex4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{ I.F. }=\frac{2}{M|A|(|A|-1)}\sum_{m=1}^{M}\sum_{a,\hat{a}\in A}W_{1}%
\left(P_{S}\left(x^{m}\right),P_{S}\left(\hat{x}^{m}\right)\right),\\
\text{ G.F. }=\frac{1}{|A|}\sum_{a\in A}W_{1}\left(P_{S}^{a},P_{S}^{*}\right)," class="ltx_Math" display="block" id="S4.Ex4.m1.7"><semantics id="S4.Ex4.m1.7a"><mrow id="S4.Ex4.m1.7.7.1"><mrow id="S4.Ex4.m1.7.7.1.1.2" xref="S4.Ex4.m1.7.7.1.1.3.cmml"><mrow id="S4.Ex4.m1.7.7.1.1.1.1" xref="S4.Ex4.m1.7.7.1.1.1.1.cmml"><mtext id="S4.Ex4.m1.7.7.1.1.1.1.4" xref="S4.Ex4.m1.7.7.1.1.1.1.4a.cmml"> I.F. </mtext><mo id="S4.Ex4.m1.7.7.1.1.1.1.3" xref="S4.Ex4.m1.7.7.1.1.1.1.3.cmml">=</mo><mrow id="S4.Ex4.m1.7.7.1.1.1.1.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.cmml"><mfrac id="S4.Ex4.m1.3.3" xref="S4.Ex4.m1.3.3.cmml"><mn id="S4.Ex4.m1.3.3.5" xref="S4.Ex4.m1.3.3.5.cmml">2</mn><mrow id="S4.Ex4.m1.3.3.3" xref="S4.Ex4.m1.3.3.3.cmml"><mi id="S4.Ex4.m1.3.3.3.5" xref="S4.Ex4.m1.3.3.3.5.cmml">M</mi><mo id="S4.Ex4.m1.3.3.3.4" xref="S4.Ex4.m1.3.3.3.4.cmml">⁢</mo><mrow id="S4.Ex4.m1.3.3.3.6.2" xref="S4.Ex4.m1.3.3.3.6.1.cmml"><mo id="S4.Ex4.m1.3.3.3.6.2.1" stretchy="false" xref="S4.Ex4.m1.3.3.3.6.1.1.cmml">|</mo><mi id="S4.Ex4.m1.1.1.1.1" xref="S4.Ex4.m1.1.1.1.1.cmml">A</mi><mo id="S4.Ex4.m1.3.3.3.6.2.2" stretchy="false" xref="S4.Ex4.m1.3.3.3.6.1.1.cmml">|</mo></mrow><mo id="S4.Ex4.m1.3.3.3.4a" xref="S4.Ex4.m1.3.3.3.4.cmml">⁢</mo><mrow id="S4.Ex4.m1.3.3.3.3.1" xref="S4.Ex4.m1.3.3.3.3.1.1.cmml"><mo id="S4.Ex4.m1.3.3.3.3.1.2" stretchy="false" xref="S4.Ex4.m1.3.3.3.3.1.1.cmml">(</mo><mrow id="S4.Ex4.m1.3.3.3.3.1.1" xref="S4.Ex4.m1.3.3.3.3.1.1.cmml"><mrow id="S4.Ex4.m1.3.3.3.3.1.1.2.2" xref="S4.Ex4.m1.3.3.3.3.1.1.2.1.cmml"><mo id="S4.Ex4.m1.3.3.3.3.1.1.2.2.1" stretchy="false" xref="S4.Ex4.m1.3.3.3.3.1.1.2.1.1.cmml">|</mo><mi id="S4.Ex4.m1.2.2.2.2" xref="S4.Ex4.m1.2.2.2.2.cmml">A</mi><mo id="S4.Ex4.m1.3.3.3.3.1.1.2.2.2" stretchy="false" xref="S4.Ex4.m1.3.3.3.3.1.1.2.1.1.cmml">|</mo></mrow><mo id="S4.Ex4.m1.3.3.3.3.1.1.1" xref="S4.Ex4.m1.3.3.3.3.1.1.1.cmml">−</mo><mn id="S4.Ex4.m1.3.3.3.3.1.1.3" xref="S4.Ex4.m1.3.3.3.3.1.1.3.cmml">1</mn></mrow><mo id="S4.Ex4.m1.3.3.3.3.1.3" stretchy="false" xref="S4.Ex4.m1.3.3.3.3.1.1.cmml">)</mo></mrow></mrow></mfrac><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.3.cmml">⁢</mo><mrow id="S4.Ex4.m1.7.7.1.1.1.1.2.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.cmml"><munderover id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.cmml"><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.2" movablelimits="false" rspace="0em" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.2.cmml">∑</mo><mrow id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.cmml"><mi id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.2.cmml">m</mi><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.1" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.1.cmml">=</mo><mn id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.3.cmml">1</mn></mrow><mi id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.3.cmml">M</mi></munderover><mrow id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.cmml"><munder id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3.cmml"><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3.2" movablelimits="false" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3.2.cmml">∑</mo><mrow id="S4.Ex4.m1.5.5.2" xref="S4.Ex4.m1.5.5.2.cmml"><mrow id="S4.Ex4.m1.5.5.2.4.2" xref="S4.Ex4.m1.5.5.2.4.1.cmml"><mi id="S4.Ex4.m1.4.4.1.1" xref="S4.Ex4.m1.4.4.1.1.cmml">a</mi><mo id="S4.Ex4.m1.5.5.2.4.2.1" xref="S4.Ex4.m1.5.5.2.4.1.cmml">,</mo><mover accent="true" id="S4.Ex4.m1.5.5.2.2" xref="S4.Ex4.m1.5.5.2.2.cmml"><mi id="S4.Ex4.m1.5.5.2.2.2" xref="S4.Ex4.m1.5.5.2.2.2.cmml">a</mi><mo id="S4.Ex4.m1.5.5.2.2.1" xref="S4.Ex4.m1.5.5.2.2.1.cmml">^</mo></mover></mrow><mo id="S4.Ex4.m1.5.5.2.3" xref="S4.Ex4.m1.5.5.2.3.cmml">∈</mo><mi id="S4.Ex4.m1.5.5.2.5" xref="S4.Ex4.m1.5.5.2.5.cmml">A</mi></mrow></munder><mrow id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.cmml"><msub id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.cmml"><mi id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.2.cmml">W</mi><mn id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.3.cmml">1</mn></msub><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.3.cmml">⁢</mo><mrow id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.3.cmml"><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.3.cmml">(</mo><mrow id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">P</mi><mi id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">S</mi></msub><mo id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">m</mi></msup><mo id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.4" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.3.cmml">,</mo><mrow id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.cmml"><msub id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.cmml"><mi id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.2.cmml">P</mi><mi id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.3.cmml">S</mi></msub><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.2.cmml">⁢</mo><mrow id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.cmml"><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.cmml">(</mo><msup id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.cmml"><mover accent="true" id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.cmml"><mi id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.2" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.2.cmml">x</mi><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.1" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.1.cmml">^</mo></mover><mi id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.3.cmml">m</mi></msup><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.3" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.5" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S4.Ex4.m1.7.7.1.1.2.3" xref="S4.Ex4.m1.7.7.1.1.3a.cmml">,</mo><mrow id="S4.Ex4.m1.7.7.1.1.2.2" xref="S4.Ex4.m1.7.7.1.1.2.2.cmml"><mtext id="S4.Ex4.m1.7.7.1.1.2.2.4" xref="S4.Ex4.m1.7.7.1.1.2.2.4a.cmml"> G.F. </mtext><mo id="S4.Ex4.m1.7.7.1.1.2.2.3" xref="S4.Ex4.m1.7.7.1.1.2.2.3.cmml">=</mo><mrow id="S4.Ex4.m1.7.7.1.1.2.2.2" xref="S4.Ex4.m1.7.7.1.1.2.2.2.cmml"><mfrac id="S4.Ex4.m1.6.6" xref="S4.Ex4.m1.6.6.cmml"><mn id="S4.Ex4.m1.6.6.3" xref="S4.Ex4.m1.6.6.3.cmml">1</mn><mrow id="S4.Ex4.m1.6.6.1.3" xref="S4.Ex4.m1.6.6.1.2.cmml"><mo id="S4.Ex4.m1.6.6.1.3.1" stretchy="false" xref="S4.Ex4.m1.6.6.1.2.1.cmml">|</mo><mi id="S4.Ex4.m1.6.6.1.1" xref="S4.Ex4.m1.6.6.1.1.cmml">A</mi><mo id="S4.Ex4.m1.6.6.1.3.2" stretchy="false" xref="S4.Ex4.m1.6.6.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S4.Ex4.m1.7.7.1.1.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.2.2.2.3.cmml">⁢</mo><mrow id="S4.Ex4.m1.7.7.1.1.2.2.2.2" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.cmml"><munder id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.cmml"><mo id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.2" movablelimits="false" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.2.cmml">∑</mo><mrow id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.cmml"><mi id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.2" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.2.cmml">a</mi><mo id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.1" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.1.cmml">∈</mo><mi id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.3" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.3.cmml">A</mi></mrow></munder><mrow id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.cmml"><msub id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.cmml"><mi id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.2" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.2.cmml">W</mi><mn id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.3" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.3.cmml">1</mn></msub><mo id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.3.cmml">⁢</mo><mrow id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.3.cmml"><mo id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.3.cmml">(</mo><msubsup id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.cmml"><mi id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.2" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.2.cmml">P</mi><mi id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.3" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.3.cmml">S</mi><mi id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.3" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.cmml">a</mi></msubsup><mo id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.4" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.3.cmml">,</mo><msubsup id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.cmml"><mi id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.2" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.2.cmml">P</mi><mi id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.3.cmml">S</mi><mo id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.3" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.3.cmml">∗</mo></msubsup><mo id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.5" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow></mrow></mrow></mrow><mo id="S4.Ex4.m1.7.7.1.2">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.Ex4.m1.7b"><apply id="S4.Ex4.m1.7.7.1.1.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.3a.cmml" xref="S4.Ex4.m1.7.7.1.1.2.3">formulae-sequence</csymbol><apply id="S4.Ex4.m1.7.7.1.1.1.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1"><eq id="S4.Ex4.m1.7.7.1.1.1.1.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.3"></eq><ci id="S4.Ex4.m1.7.7.1.1.1.1.4a.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.4"><mtext id="S4.Ex4.m1.7.7.1.1.1.1.4.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.4"> I.F. </mtext></ci><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2"><times id="S4.Ex4.m1.7.7.1.1.1.1.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.3"></times><apply id="S4.Ex4.m1.3.3.cmml" xref="S4.Ex4.m1.3.3"><divide id="S4.Ex4.m1.3.3.4.cmml" xref="S4.Ex4.m1.3.3"></divide><cn id="S4.Ex4.m1.3.3.5.cmml" type="integer" xref="S4.Ex4.m1.3.3.5">2</cn><apply id="S4.Ex4.m1.3.3.3.cmml" xref="S4.Ex4.m1.3.3.3"><times id="S4.Ex4.m1.3.3.3.4.cmml" xref="S4.Ex4.m1.3.3.3.4"></times><ci id="S4.Ex4.m1.3.3.3.5.cmml" xref="S4.Ex4.m1.3.3.3.5">𝑀</ci><apply id="S4.Ex4.m1.3.3.3.6.1.cmml" xref="S4.Ex4.m1.3.3.3.6.2"><abs id="S4.Ex4.m1.3.3.3.6.1.1.cmml" xref="S4.Ex4.m1.3.3.3.6.2.1"></abs><ci id="S4.Ex4.m1.1.1.1.1.cmml" xref="S4.Ex4.m1.1.1.1.1">𝐴</ci></apply><apply id="S4.Ex4.m1.3.3.3.3.1.1.cmml" xref="S4.Ex4.m1.3.3.3.3.1"><minus id="S4.Ex4.m1.3.3.3.3.1.1.1.cmml" xref="S4.Ex4.m1.3.3.3.3.1.1.1"></minus><apply id="S4.Ex4.m1.3.3.3.3.1.1.2.1.cmml" xref="S4.Ex4.m1.3.3.3.3.1.1.2.2"><abs id="S4.Ex4.m1.3.3.3.3.1.1.2.1.1.cmml" xref="S4.Ex4.m1.3.3.3.3.1.1.2.2.1"></abs><ci id="S4.Ex4.m1.2.2.2.2.cmml" xref="S4.Ex4.m1.2.2.2.2">𝐴</ci></apply><cn id="S4.Ex4.m1.3.3.3.3.1.1.3.cmml" type="integer" xref="S4.Ex4.m1.3.3.3.3.1.1.3">1</cn></apply></apply></apply><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2"><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3">superscript</csymbol><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3">subscript</csymbol><sum id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.2"></sum><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3"><eq id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.1"></eq><ci id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.2">𝑚</ci><cn id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.3.cmml" type="integer" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.2.3.3">1</cn></apply></apply><ci id="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.3.3">𝑀</ci></apply><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2"><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3">subscript</csymbol><sum id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.3.2"></sum><apply id="S4.Ex4.m1.5.5.2.cmml" xref="S4.Ex4.m1.5.5.2"><in id="S4.Ex4.m1.5.5.2.3.cmml" xref="S4.Ex4.m1.5.5.2.3"></in><list id="S4.Ex4.m1.5.5.2.4.1.cmml" xref="S4.Ex4.m1.5.5.2.4.2"><ci id="S4.Ex4.m1.4.4.1.1.cmml" xref="S4.Ex4.m1.4.4.1.1">𝑎</ci><apply id="S4.Ex4.m1.5.5.2.2.cmml" xref="S4.Ex4.m1.5.5.2.2"><ci id="S4.Ex4.m1.5.5.2.2.1.cmml" xref="S4.Ex4.m1.5.5.2.2.1">^</ci><ci id="S4.Ex4.m1.5.5.2.2.2.cmml" xref="S4.Ex4.m1.5.5.2.2.2">𝑎</ci></apply></list><ci id="S4.Ex4.m1.5.5.2.5.cmml" xref="S4.Ex4.m1.5.5.2.5">𝐴</ci></apply></apply><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2"><times id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.3"></times><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4">subscript</csymbol><ci id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.2">𝑊</ci><cn id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.3.cmml" type="integer" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.4.3">1</cn></apply><interval closure="open" id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2"><apply id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1"><times id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.2">𝑃</ci><ci id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.3.3">𝑆</ci></apply><apply id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑥</ci><ci id="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑚</ci></apply></apply><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2"><times id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.2"></times><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3">subscript</csymbol><ci id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.2">𝑃</ci><ci id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.3.3">𝑆</ci></apply><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1">superscript</csymbol><apply id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2"><ci id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.1.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.1">^</ci><ci id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.2.2">𝑥</ci></apply><ci id="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.3.cmml" xref="S4.Ex4.m1.7.7.1.1.1.1.2.2.2.2.2.2.2.1.1.1.3">𝑚</ci></apply></apply></interval></apply></apply></apply></apply></apply><apply id="S4.Ex4.m1.7.7.1.1.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2"><eq id="S4.Ex4.m1.7.7.1.1.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.3"></eq><ci id="S4.Ex4.m1.7.7.1.1.2.2.4a.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.4"><mtext id="S4.Ex4.m1.7.7.1.1.2.2.4.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.4"> G.F. </mtext></ci><apply id="S4.Ex4.m1.7.7.1.1.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2"><times id="S4.Ex4.m1.7.7.1.1.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.3"></times><apply id="S4.Ex4.m1.6.6.cmml" xref="S4.Ex4.m1.6.6"><divide id="S4.Ex4.m1.6.6.2.cmml" xref="S4.Ex4.m1.6.6"></divide><cn id="S4.Ex4.m1.6.6.3.cmml" type="integer" xref="S4.Ex4.m1.6.6.3">1</cn><apply id="S4.Ex4.m1.6.6.1.2.cmml" xref="S4.Ex4.m1.6.6.1.3"><abs id="S4.Ex4.m1.6.6.1.2.1.cmml" xref="S4.Ex4.m1.6.6.1.3.1"></abs><ci id="S4.Ex4.m1.6.6.1.1.cmml" xref="S4.Ex4.m1.6.6.1.1">𝐴</ci></apply></apply><apply id="S4.Ex4.m1.7.7.1.1.2.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2"><apply id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.1.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3">subscript</csymbol><sum id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.2"></sum><apply id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3"><in id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.1.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.1"></in><ci id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.2">𝑎</ci><ci id="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.3.3.3">𝐴</ci></apply></apply><apply id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2"><times id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.3"></times><apply id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.1.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4">subscript</csymbol><ci id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.2">𝑊</ci><cn id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.3.cmml" type="integer" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.4.3">1</cn></apply><interval closure="open" id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2"><apply id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.1.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1">superscript</csymbol><apply id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.1.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.2">𝑃</ci><ci id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.2.3">𝑆</ci></apply><ci id="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.1.1.1.1.1.1.3">𝑎</ci></apply><apply id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.1.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2">superscript</csymbol><apply id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.1.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2">subscript</csymbol><ci id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.2.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.2">𝑃</ci><ci id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.2.3">𝑆</ci></apply><times id="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.3.cmml" xref="S4.Ex4.m1.7.7.1.1.2.2.2.2.2.2.2.2.3"></times></apply></interval></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.Ex4.m1.7c">\text{ I.F. }=\frac{2}{M|A|(|A|-1)}\sum_{m=1}^{M}\sum_{a,\hat{a}\in A}W_{1}%
\left(P_{S}\left(x^{m}\right),P_{S}\left(\hat{x}^{m}\right)\right),\\
\text{ G.F. }=\frac{1}{|A|}\sum_{a\in A}W_{1}\left(P_{S}^{a},P_{S}^{*}\right),</annotation><annotation encoding="application/x-llamapun" id="S4.Ex4.m1.7d">I.F. = divide start_ARG 2 end_ARG start_ARG italic_M | italic_A | ( | italic_A | - 1 ) end_ARG ∑ start_POSTSUBSCRIPT italic_m = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_M end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_a , over^ start_ARG italic_a end_ARG ∈ italic_A end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) , italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( over^ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT ) ) , G.F. = divide start_ARG 1 end_ARG start_ARG | italic_A | end_ARG ∑ start_POSTSUBSCRIPT italic_a ∈ italic_A end_POSTSUBSCRIPT italic_W start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT , italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS7.SSS2.p3">
<p class="ltx_p" id="S4.SS7.SSS2.p3.11">where <math alttext="M" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.1.m1.1"><semantics id="S4.SS7.SSS2.p3.1.m1.1a"><mi id="S4.SS7.SSS2.p3.1.m1.1.1" xref="S4.SS7.SSS2.p3.1.m1.1.1.cmml">M</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.1.m1.1b"><ci id="S4.SS7.SSS2.p3.1.m1.1.1.cmml" xref="S4.SS7.SSS2.p3.1.m1.1.1">𝑀</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.1.m1.1c">M</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.1.m1.1d">italic_M</annotation></semantics></math> is the number of templates, <math alttext="A" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.2.m2.1"><semantics id="S4.SS7.SSS2.p3.2.m2.1a"><mi id="S4.SS7.SSS2.p3.2.m2.1.1" xref="S4.SS7.SSS2.p3.2.m2.1.1.cmml">A</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.2.m2.1b"><ci id="S4.SS7.SSS2.p3.2.m2.1.1.cmml" xref="S4.SS7.SSS2.p3.2.m2.1.1">𝐴</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.2.m2.1c">A</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.2.m2.1d">italic_A</annotation></semantics></math> is the set of all subgroups, <math alttext="x" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.3.m3.1"><semantics id="S4.SS7.SSS2.p3.3.m3.1a"><mi id="S4.SS7.SSS2.p3.3.m3.1.1" xref="S4.SS7.SSS2.p3.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.3.m3.1b"><ci id="S4.SS7.SSS2.p3.3.m3.1.1.cmml" xref="S4.SS7.SSS2.p3.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.3.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.3.m3.1d">italic_x</annotation></semantics></math> and <math alttext="\hat{x}" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.4.m4.1"><semantics id="S4.SS7.SSS2.p3.4.m4.1a"><mover accent="true" id="S4.SS7.SSS2.p3.4.m4.1.1" xref="S4.SS7.SSS2.p3.4.m4.1.1.cmml"><mi id="S4.SS7.SSS2.p3.4.m4.1.1.2" xref="S4.SS7.SSS2.p3.4.m4.1.1.2.cmml">x</mi><mo id="S4.SS7.SSS2.p3.4.m4.1.1.1" xref="S4.SS7.SSS2.p3.4.m4.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.4.m4.1b"><apply id="S4.SS7.SSS2.p3.4.m4.1.1.cmml" xref="S4.SS7.SSS2.p3.4.m4.1.1"><ci id="S4.SS7.SSS2.p3.4.m4.1.1.1.cmml" xref="S4.SS7.SSS2.p3.4.m4.1.1.1">^</ci><ci id="S4.SS7.SSS2.p3.4.m4.1.1.2.cmml" xref="S4.SS7.SSS2.p3.4.m4.1.1.2">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.4.m4.1c">\hat{x}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.4.m4.1d">over^ start_ARG italic_x end_ARG</annotation></semantics></math> are a pair of counterfactual sentences, <math alttext="a" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.5.m5.1"><semantics id="S4.SS7.SSS2.p3.5.m5.1a"><mi id="S4.SS7.SSS2.p3.5.m5.1.1" xref="S4.SS7.SSS2.p3.5.m5.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.5.m5.1b"><ci id="S4.SS7.SSS2.p3.5.m5.1.1.cmml" xref="S4.SS7.SSS2.p3.5.m5.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.5.m5.1c">a</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.5.m5.1d">italic_a</annotation></semantics></math> and <math alttext="\hat{a}" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.6.m6.1"><semantics id="S4.SS7.SSS2.p3.6.m6.1a"><mover accent="true" id="S4.SS7.SSS2.p3.6.m6.1.1" xref="S4.SS7.SSS2.p3.6.m6.1.1.cmml"><mi id="S4.SS7.SSS2.p3.6.m6.1.1.2" xref="S4.SS7.SSS2.p3.6.m6.1.1.2.cmml">a</mi><mo id="S4.SS7.SSS2.p3.6.m6.1.1.1" xref="S4.SS7.SSS2.p3.6.m6.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.6.m6.1b"><apply id="S4.SS7.SSS2.p3.6.m6.1.1.cmml" xref="S4.SS7.SSS2.p3.6.m6.1.1"><ci id="S4.SS7.SSS2.p3.6.m6.1.1.1.cmml" xref="S4.SS7.SSS2.p3.6.m6.1.1.1">^</ci><ci id="S4.SS7.SSS2.p3.6.m6.1.1.2.cmml" xref="S4.SS7.SSS2.p3.6.m6.1.1.2">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.6.m6.1c">\hat{a}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.6.m6.1d">over^ start_ARG italic_a end_ARG</annotation></semantics></math> are their sensitive attributes, <math alttext="P_{S}\left(x^{m}\right)" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.7.m7.1"><semantics id="S4.SS7.SSS2.p3.7.m7.1a"><mrow id="S4.SS7.SSS2.p3.7.m7.1.1" xref="S4.SS7.SSS2.p3.7.m7.1.1.cmml"><msub id="S4.SS7.SSS2.p3.7.m7.1.1.3" xref="S4.SS7.SSS2.p3.7.m7.1.1.3.cmml"><mi id="S4.SS7.SSS2.p3.7.m7.1.1.3.2" xref="S4.SS7.SSS2.p3.7.m7.1.1.3.2.cmml">P</mi><mi id="S4.SS7.SSS2.p3.7.m7.1.1.3.3" xref="S4.SS7.SSS2.p3.7.m7.1.1.3.3.cmml">S</mi></msub><mo id="S4.SS7.SSS2.p3.7.m7.1.1.2" xref="S4.SS7.SSS2.p3.7.m7.1.1.2.cmml">⁢</mo><mrow id="S4.SS7.SSS2.p3.7.m7.1.1.1.1" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.cmml"><mo id="S4.SS7.SSS2.p3.7.m7.1.1.1.1.2" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.cmml">(</mo><msup id="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.cmml"><mi id="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.2" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.2.cmml">x</mi><mi id="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.3" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.3.cmml">m</mi></msup><mo id="S4.SS7.SSS2.p3.7.m7.1.1.1.1.3" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.7.m7.1b"><apply id="S4.SS7.SSS2.p3.7.m7.1.1.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1"><times id="S4.SS7.SSS2.p3.7.m7.1.1.2.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1.2"></times><apply id="S4.SS7.SSS2.p3.7.m7.1.1.3.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1.3"><csymbol cd="ambiguous" id="S4.SS7.SSS2.p3.7.m7.1.1.3.1.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1.3">subscript</csymbol><ci id="S4.SS7.SSS2.p3.7.m7.1.1.3.2.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1.3.2">𝑃</ci><ci id="S4.SS7.SSS2.p3.7.m7.1.1.3.3.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1.3.3">𝑆</ci></apply><apply id="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.1.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1">superscript</csymbol><ci id="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.2.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.2">𝑥</ci><ci id="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.3.cmml" xref="S4.SS7.SSS2.p3.7.m7.1.1.1.1.1.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.7.m7.1c">P_{S}\left(x^{m}\right)</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.7.m7.1d">italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )</annotation></semantics></math> and <math alttext="P_{S}\left(\hat{x}^{m}\right)" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.8.m8.1"><semantics id="S4.SS7.SSS2.p3.8.m8.1a"><mrow id="S4.SS7.SSS2.p3.8.m8.1.1" xref="S4.SS7.SSS2.p3.8.m8.1.1.cmml"><msub id="S4.SS7.SSS2.p3.8.m8.1.1.3" xref="S4.SS7.SSS2.p3.8.m8.1.1.3.cmml"><mi id="S4.SS7.SSS2.p3.8.m8.1.1.3.2" xref="S4.SS7.SSS2.p3.8.m8.1.1.3.2.cmml">P</mi><mi id="S4.SS7.SSS2.p3.8.m8.1.1.3.3" xref="S4.SS7.SSS2.p3.8.m8.1.1.3.3.cmml">S</mi></msub><mo id="S4.SS7.SSS2.p3.8.m8.1.1.2" xref="S4.SS7.SSS2.p3.8.m8.1.1.2.cmml">⁢</mo><mrow id="S4.SS7.SSS2.p3.8.m8.1.1.1.1" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.cmml"><mo id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.2" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.cmml">(</mo><msup id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.cmml"><mover accent="true" id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.cmml"><mi id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.2" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.2.cmml">x</mi><mo id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.1" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.3" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.3.cmml">m</mi></msup><mo id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.3" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.8.m8.1b"><apply id="S4.SS7.SSS2.p3.8.m8.1.1.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1"><times id="S4.SS7.SSS2.p3.8.m8.1.1.2.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.2"></times><apply id="S4.SS7.SSS2.p3.8.m8.1.1.3.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.3"><csymbol cd="ambiguous" id="S4.SS7.SSS2.p3.8.m8.1.1.3.1.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.3">subscript</csymbol><ci id="S4.SS7.SSS2.p3.8.m8.1.1.3.2.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.3.2">𝑃</ci><ci id="S4.SS7.SSS2.p3.8.m8.1.1.3.3.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.3.3">𝑆</ci></apply><apply id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1"><csymbol cd="ambiguous" id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.1.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1">superscript</csymbol><apply id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2"><ci id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.1.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.1">^</ci><ci id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.2.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.2.2">𝑥</ci></apply><ci id="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.3.cmml" xref="S4.SS7.SSS2.p3.8.m8.1.1.1.1.1.3">𝑚</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.8.m8.1c">P_{S}\left(\hat{x}^{m}\right)</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.8.m8.1d">italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT ( over^ start_ARG italic_x end_ARG start_POSTSUPERSCRIPT italic_m end_POSTSUPERSCRIPT )</annotation></semantics></math> are their sentiment score distributions, as well as <math alttext="P_{S}^{a}" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.9.m9.1"><semantics id="S4.SS7.SSS2.p3.9.m9.1a"><msubsup id="S4.SS7.SSS2.p3.9.m9.1.1" xref="S4.SS7.SSS2.p3.9.m9.1.1.cmml"><mi id="S4.SS7.SSS2.p3.9.m9.1.1.2.2" xref="S4.SS7.SSS2.p3.9.m9.1.1.2.2.cmml">P</mi><mi id="S4.SS7.SSS2.p3.9.m9.1.1.2.3" xref="S4.SS7.SSS2.p3.9.m9.1.1.2.3.cmml">S</mi><mi id="S4.SS7.SSS2.p3.9.m9.1.1.3" xref="S4.SS7.SSS2.p3.9.m9.1.1.3.cmml">a</mi></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.9.m9.1b"><apply id="S4.SS7.SSS2.p3.9.m9.1.1.cmml" xref="S4.SS7.SSS2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS7.SSS2.p3.9.m9.1.1.1.cmml" xref="S4.SS7.SSS2.p3.9.m9.1.1">superscript</csymbol><apply id="S4.SS7.SSS2.p3.9.m9.1.1.2.cmml" xref="S4.SS7.SSS2.p3.9.m9.1.1"><csymbol cd="ambiguous" id="S4.SS7.SSS2.p3.9.m9.1.1.2.1.cmml" xref="S4.SS7.SSS2.p3.9.m9.1.1">subscript</csymbol><ci id="S4.SS7.SSS2.p3.9.m9.1.1.2.2.cmml" xref="S4.SS7.SSS2.p3.9.m9.1.1.2.2">𝑃</ci><ci id="S4.SS7.SSS2.p3.9.m9.1.1.2.3.cmml" xref="S4.SS7.SSS2.p3.9.m9.1.1.2.3">𝑆</ci></apply><ci id="S4.SS7.SSS2.p3.9.m9.1.1.3.cmml" xref="S4.SS7.SSS2.p3.9.m9.1.1.3">𝑎</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.9.m9.1c">P_{S}^{a}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.9.m9.1d">italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_a end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="P_{S}^{*}" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.10.m10.1"><semantics id="S4.SS7.SSS2.p3.10.m10.1a"><msubsup id="S4.SS7.SSS2.p3.10.m10.1.1" xref="S4.SS7.SSS2.p3.10.m10.1.1.cmml"><mi id="S4.SS7.SSS2.p3.10.m10.1.1.2.2" xref="S4.SS7.SSS2.p3.10.m10.1.1.2.2.cmml">P</mi><mi id="S4.SS7.SSS2.p3.10.m10.1.1.2.3" xref="S4.SS7.SSS2.p3.10.m10.1.1.2.3.cmml">S</mi><mo id="S4.SS7.SSS2.p3.10.m10.1.1.3" xref="S4.SS7.SSS2.p3.10.m10.1.1.3.cmml">∗</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.10.m10.1b"><apply id="S4.SS7.SSS2.p3.10.m10.1.1.cmml" xref="S4.SS7.SSS2.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS7.SSS2.p3.10.m10.1.1.1.cmml" xref="S4.SS7.SSS2.p3.10.m10.1.1">superscript</csymbol><apply id="S4.SS7.SSS2.p3.10.m10.1.1.2.cmml" xref="S4.SS7.SSS2.p3.10.m10.1.1"><csymbol cd="ambiguous" id="S4.SS7.SSS2.p3.10.m10.1.1.2.1.cmml" xref="S4.SS7.SSS2.p3.10.m10.1.1">subscript</csymbol><ci id="S4.SS7.SSS2.p3.10.m10.1.1.2.2.cmml" xref="S4.SS7.SSS2.p3.10.m10.1.1.2.2">𝑃</ci><ci id="S4.SS7.SSS2.p3.10.m10.1.1.2.3.cmml" xref="S4.SS7.SSS2.p3.10.m10.1.1.2.3">𝑆</ci></apply><times id="S4.SS7.SSS2.p3.10.m10.1.1.3.cmml" xref="S4.SS7.SSS2.p3.10.m10.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.10.m10.1c">P_{S}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.10.m10.1d">italic_P start_POSTSUBSCRIPT italic_S end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ∗ end_POSTSUPERSCRIPT</annotation></semantics></math> are the sentiment scores distributions over all generated sentences in subgroup <math alttext="a" class="ltx_Math" display="inline" id="S4.SS7.SSS2.p3.11.m11.1"><semantics id="S4.SS7.SSS2.p3.11.m11.1a"><mi id="S4.SS7.SSS2.p3.11.m11.1.1" xref="S4.SS7.SSS2.p3.11.m11.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S4.SS7.SSS2.p3.11.m11.1b"><ci id="S4.SS7.SSS2.p3.11.m11.1.1.cmml" xref="S4.SS7.SSS2.p3.11.m11.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS7.SSS2.p3.11.m11.1c">a</annotation><annotation encoding="application/x-llamapun" id="S4.SS7.SSS2.p3.11.m11.1d">italic_a</annotation></semantics></math> and all subgroups, respectively.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Future direction</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The future direction of addressing fairness and bias in LLMs and e-commerce platforms requires ongoing research, innovation, and collaboration among researchers, industry practitioners, and policymakers. One key direction is the development of more advanced and nuanced fairness metrics that capture the multifaceted nature of fairness and account for the complex dynamics of e-commerce ecosystems. This involves moving beyond simplistic notions of demographic parity and towards more context-specific and domain-aware fairness criteria. Another important direction is the integration of fairness considerations into the entire AI development pipeline, from data collection and preprocessing to model training, evaluation, and deployment. This requires the establishment of standardized fairness assessment frameworks and the incorporation of fairness checks at every stage of the development process. Researchers should also focus on developing explanatory models that provide insights into the decision-making processes of LLMs and e-commerce algorithms, enabling stakeholders to identify and mitigate sources of bias. Future work should explore the potential of using domain adaptation techniques to transfer fairness-aware models across different e-commerce platforms and contexts, promoting the widespread adoption of fair AI practices. Additionally, there is a need for interdisciplinary collaboration, bringing together experts from computer science, social sciences, ethics, and law to address the societal implications of biased AI systems and develop holistic solutions. By prioritizing fairness and transparency in the development and deployment of LLMs and e-commerce algorithms, we can work towards building a more equitable and trustworthy digital marketplace that benefits all participants.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">This review provides a comprehensive overview of the principles, applications, and fairness challenges of LLMs in e-commerce, intended to promote further research and exploration in this interdisciplinary field. With the rapid development, LLMs could significantly improve future e-commerce practices and innovations for the benefit of businesses and consumers. However, a critical aspect that demands attention is addressing the fairness challenges that may arise from the integration of LLMs into e-commerce platforms. Ensuring fairness and mitigating potential biases in these models is important for creating equitable and inclusive online shopping experiences for all users. This review highlights the need for sustained interdisciplinary collaboration between e-commerce practitioners, domain experts, and AI researchers. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2405.13025v2#bib.bib85" title="">85</a>]</cite>
</p>
<div class="ltx_pagination ltx_role_newpage"></div>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas
Scialom.

</span>
<span class="ltx_bibblock">Llama 2: Open foundation and fine-tuned chat models, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
OpenAI.

</span>
<span class="ltx_bibblock">Gpt-3: Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2005.14165</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford, Leo
Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa Lopez, Ryan
Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted, Henri Roussez,
Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, CJ Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin Zhang, Shengjia
Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.

</span>
<span class="ltx_bibblock">Gpt-4 technical report, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Claude’s constitution, May 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Anthropic.

</span>
<span class="ltx_bibblock">Model card and evaluations for claude models, July 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Yangning Li, Shirong Ma, Xiaobin Wang, Shen Huang, Chengyue Jiang, Hai-Tao Zheng, Pengjun Xie, Fei Huang, and Yong Jiang.

</span>
<span class="ltx_bibblock">Ecomgpt: Instruction-tuning large language models with chain-of-task tasks for e-commerce, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
D. Zhang, Z. Yuan, Y. Liu, F. Zhuang, C. H., and H. Xiong.

</span>
<span class="ltx_bibblock">E-bert: Adapting bert to e-commerce with adaptive hybrid masking and neighbor product reconstruction.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2009.02835" title="">https://arxiv.org/pdf/2009.02835</a>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Ben Hutchinson, Vinodkumar Prabhakaran, Emily Denton, Kellie Webster, Yu Zhong, and Stephen Denuyl.

</span>
<span class="ltx_bibblock">Social biases in nlp models as barriers for persons with disabilities, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Michal Měchura.

</span>
<span class="ltx_bibblock">A taxonomy of bias-causing ambiguities in machine translation.

</span>
<span class="ltx_bibblock">In Christian Hardmeier, Christine Basta, Marta R. Costa-jussà, Gabriel Stanovsky, and Hila Gonen, editors, <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP)</span>, pages 168–173, Seattle, Washington, July 2022. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.

</span>
<span class="ltx_bibblock">Albert: A lite bert for self-supervised learning of language representations.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">Proceedings of the International Conference on Learning Representations (ICLR)</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language understanding, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Mike Lewis et al.

</span>
<span class="ltx_bibblock">Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">ArXiv</span>, abs/1910.13461, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning.

</span>
<span class="ltx_bibblock">Electra: Pre-training text encoders as discriminators rather than generators.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">International Conference on Learning Representations</span>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le.

</span>
<span class="ltx_bibblock">Xlnet: Generalized autoregressive pretraining for language understanding.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">Advances in neural information processing systems</span>, volume 32, pages 5754–5764, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Hao Tian, Hua Wu, and Haifeng Wang.

</span>
<span class="ltx_bibblock">Ernie: Enhanced representation through knowledge integration.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:1904.09223</span>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Meta AI.

</span>
<span class="ltx_bibblock">Galactica: A large language model for science.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">ArXiv</span>, abs/2211.12522, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">OpenAI Blog</span>, 1(8), 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Pengcheng He, Xiaodong Liu, Weizhu Chen, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">Deberta: Decoding-enhanced bert with disentangled attention.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>. AAAI, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Meta AI.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">ArXiv</span>, abs/2302.13971, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Tom B. Brown et al.

</span>
<span class="ltx_bibblock">Language models are few-shot learners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib20.1.1">ArXiv</span>, abs/2005.14165, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Aakanksha Chowdhery et al.

</span>
<span class="ltx_bibblock">Palm: Scaling language modeling with pathways.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">ArXiv</span>, abs/2204.02311, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Stanford CRFM.

</span>
<span class="ltx_bibblock">Alpaca: A strong, replicable instruction-following model.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">Stanford Alpaca Project</span>, 2023.

</span>
<span class="ltx_bibblock">Available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://stanford-alpaca.gitbook.io/everything-you-need-to-know" title="">https://stanford-alpaca.gitbook.io/everything-you-need-to-know</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.

</span>
<span class="ltx_bibblock">Mixtral of experts, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Qinjin Jia, Yang Liu, Shaoyuan Xu, Huidong Liu, Daoping Wu, Jinmiao Fu, Roland Vollgraf, and Bryan Wang.

</span>
<span class="ltx_bibblock">Kg-flip: Knowledge-guided fashion-domain language-image pre-training for e-commerce.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">ACL 2023</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Kaidi Chen, Ben Chen, Dehong Gao, Huangyu Dai, Wen Jiang, Wei Ning, Shanqing Yu, Libin Yang, and Xiaoyan Cai.

</span>
<span class="ltx_bibblock">General2specialized llms translation for e-commerce, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Bo Peng, Xinyi Ling, Ziru Chen, Huan Sun, and Xia Ning.

</span>
<span class="ltx_bibblock">ecellm: Generalizing large language models for e-commerce from large-scale, high-quality instruction data, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Jinming Li, Wentao Zhang, Tian Wang, Guanglei Xiong, Alan Lu, and Gerard Medioni.

</span>
<span class="ltx_bibblock">Gpt4rec: A generative framework for personalized recommendation and user interests interpretation, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Li Yang, Qifan Wang, Jingang Wang, Xiaojun Quan, Fuli Feng, Yu Chen, Madian Khabsa, Sinong Wang, Zenglin Xu, and Dongfang Liu.

</span>
<span class="ltx_bibblock">MixPAVE: Mix-prompt tuning for few-shot product attribute value extraction.

</span>
<span class="ltx_bibblock">In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">Findings of the Association for Computational Linguistics: ACL 2023</span>, pages 9978–9991, Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Li Yang, Qifan Wang, Zac Yu, Anand Kulkarni, Sumit Sanghai, Bin Shu, Jon Elsas, and Bhargav Kanagal.

</span>
<span class="ltx_bibblock">Mave: A product dataset for multi-source attribute value extraction.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</span>, WSDM ’22, page 1256–1265, New York, NY, USA, 2022. Association for Computing Machinery.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Huimin Xu, Wenting Wang, Xin Mao, Xinyu Jiang, and Man Lan.

</span>
<span class="ltx_bibblock">Scaling up open tagging from tens to thousands: Comprehension empowered attribute value extraction from product title.

</span>
<span class="ltx_bibblock">In Anna Korhonen, David Traum, and Lluís Màrquez, editors, <span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</span>, pages 5214–5223, Florence, Italy, July 2019. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Yibo Wang, Congying Xia, Guan Wang, and Philip Yu.

</span>
<span class="ltx_bibblock">Continuous prompt tuning based textual entailment model for e-commerce entity typing, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Pan Li, Yuyan Wang, Ed H. Chi, and Minmin Chen.

</span>
<span class="ltx_bibblock">Prompt tuning large language models on personalized aspect extraction for recommendations, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Elliot Dang, Zheyuan Hu, and Tong Li.

</span>
<span class="ltx_bibblock">Enhancing collaborative filtering recommender with prompt-based sentiment analysis, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
A. Vaswani and et al.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">Advances in Neural Information Processing Systems 30 (NIPS 2017)</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Edward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">LoRA: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">International Conference on Learning Representations</span>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Xiang Lisa Li and Percy Liang.

</span>
<span class="ltx_bibblock">Prefix-tuning: Optimizing continuous prompts for generation.

</span>
<span class="ltx_bibblock">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</span>, pages 4582–4597, Online, August 2021. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu.

</span>
<span class="ltx_bibblock">Full parameter fine-tuning for large language models with limited resources, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Niklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel.

</span>
<span class="ltx_bibblock">Crosslingual generalization through multitask finetuning, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Jianghong Zhou, Weizhi Du, Md Omar Faruk Rokon, Zhaodong Wang, Jiaxuan Xu, Isha Shah, Kuang chih Lee, and Musen Wen.

</span>
<span class="ltx_bibblock">Enhanced e-commerce attribute extraction: Innovating with decorative relation correction and llama 2.0-based annotation, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Jenny Rose Finkel, Trond Grenager, and Christopher Manning.

</span>
<span class="ltx_bibblock">Incorporating non-local information into information extraction systems by gibbs sampling.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics</span>, ACL ’05, page 363–370, USA, 2005. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Qian Zhao, Hao Qian, Ziqi Liu, Gong-Duo Zhang, and Lihong Gu.

</span>
<span class="ltx_bibblock">Breaking the barrier: Utilizing large language models for industrial recommendation systems through an inferential knowledge graph, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Brian Lester, Rami Al-Rfou, and Noah Constant.

</span>
<span class="ltx_bibblock">The power of scale for parameter-efficient prompt tuning, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.

</span>
<span class="ltx_bibblock">How transferable are features in deep neural networks?

</span>
<span class="ltx_bibblock">In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, <span class="ltx_text ltx_font_italic" id="bib.bib44.1.1">Advances in Neural Information Processing Systems</span>, volume 27. Curran Associates, Inc., 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
google-research-datasets/mave.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/google-research-datasets/MAVE" title="">https://github.com/google-research-datasets/MAVE</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Hadas Kotek, Rikker Dockum, and David Sun.

</span>
<span class="ltx_bibblock">Gender bias and stereotypes in large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib46.1.1">Proceedings of The ACM Collective Intelligence Conference</span>, CI ’23. ACM, November 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
Yingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang.

</span>
<span class="ltx_bibblock">A survey on fairness in large language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
Yifan Yang, Xiaoyu Liu, Qiao Jin, Furong Huang, and Zhiyong Lu.

</span>
<span class="ltx_bibblock">Unmasking and quantifying racial bias of large language models in medical report generation, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
Anastasiia Klimashevskaia, Dietmar Jannach, Mehdi Elahi, and Christoph Trattner.

</span>
<span class="ltx_bibblock">A survey on popularity bias in recommender systems.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/pdf/2308.01118.pdf" title="">https://arxiv.org/pdf/2308.01118.pdf</a>, 2023.

</span>
<span class="ltx_bibblock">MediaFutures: Research Centre for Responsible Media Technology &amp; Innovation, University of Bergen, Norway; AAU Klagenfurt, Austria.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Zikun Ye, Reza Yousefi Maragheh, Lalitesh Morishetti, Shanu Vashishtha, Jason Cho, Kaushiki Nag, Sushant Kumar, and Kannan Achan.

</span>
<span class="ltx_bibblock">Seller-side outcome fairness in online marketplaces.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/html/2312.03253v1" title="">https://arxiv.org/html/2312.03253v1</a>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
Thorsten Joachims, Laura Granka, Bing Pan, Helene Hembrooke, Filip Radlinski, and Geri Gay.

</span>
<span class="ltx_bibblock">Evaluating the accuracy of implicit feedback from clicks and query reformulations in web search.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib51.1.1">ACM Transactions on Information Systems (TOIS)</span>, 25(2):7–es, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
Himan Abdollahpouri and Olfa Nasraoui.

</span>
<span class="ltx_bibblock">Addressing seller-side fairness in recommender systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib52.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</span>, 34(03):13805–13806, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Aravind et al. Cikm.

</span>
<span class="ltx_bibblock">Beyond binary ratings: Debiasing recommender systems with exposure adjustment.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib53.1.1">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</span>, pages 2749–2758, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
Guan Tece, Yiwei Kang, Jingyi Jiang, and Xin Wang.

</span>
<span class="ltx_bibblock">Alleviating popularity bias in recommender systems: A multi-process fusion approach.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib54.1.1">Transactions on Emerging Telecommunications Technologies</span>, page e4443, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Xiang Cheng, Mitchell Bowden, Bhushan Ramesh Bhange, Priyanka Goyal, Thomas Packer, and Faizan Javed.

</span>
<span class="ltx_bibblock">An end-to-end solution for named entity recognition in ecommerce search.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 16th ACM International Conference on Web Search and Data Mining</span>, pages 1099–1107, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
Fabiano M Bel’em, Rodrigo M Silva, Claudio MV de Andrade, Gabriel Pessoa, Felipe Mingote, Raphael Ballet, Helton Alpontı, Henrique P de Oliveira, Jussara M Almeida, and Marcos A Gonçalves.

</span>
<span class="ltx_bibblock">Fixing the curse of the bad product descriptions: Search-boosted tag recommendation for e-commerce products.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib56.1.1">Information Processing &amp; Management</span>, 60(1):102912, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Menting Wan, Jianmo Ni, Rishabh Misra, and Julian McAuley.

</span>
<span class="ltx_bibblock">Addressing marketing bias in product recommendations.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nijianmo.github.io/paper/wsdm19.pdf" title="">https://nijianmo.github.io/paper/wsdm19.pdf</a>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
Abhisek Dash, Abhijnan Chakraborty, Saptarshi Ghosh, Animesh Mukherjee, and Krishna P Gummadi.

</span>
<span class="ltx_bibblock">Alexa, in you, i trust! fairness and interpretability issues in e-commerce search through smart speakers.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib58.1.1">Proceedings of the ACM Web Conference 2022</span>, pages 3695–3705, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
Shaochen Xu, Zihao Wu, Huaqin Zhao, Peng Shu, Zhengliang Liu, Wenxiong Liao, Sheng Li, Andrea Sikora, Tianming Liu, and Xiang Li.

</span>
<span class="ltx_bibblock">Reasoning before comparison: Llm-enhanced semantic similarity metrics for domain specialized text analysis, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Shijie Geng, Shuchang Liu, Zuohui Fu, Yingqiang Ge, and Yongfeng Zhang.

</span>
<span class="ltx_bibblock">Recommendation as language processing (rlp): A unified pretrain, personalized prompt &amp; predict paradigm (p5), 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
Aminu Da’u and Naomie Salim.

</span>
<span class="ltx_bibblock">Recommendation system based on deep learning methods: a systematic review and new directions.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib61.1.1">Artificial Intelligence Review</span>, 53:2709 – 2748, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Damien Sileo, Wout Vossen, and Robbe Raymaekers.

</span>
<span class="ltx_bibblock">Zero-shot recommendation as language modeling, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
Likang Wu, Zhi Zheng, Zhaopeng Qiu, Hao Wang, Hongchao Gu, Tingjia Shen, Chuan Qin, Chen Zhu, Hengshu Zhu, Qi Liu, Hui Xiong, and Enhong Chen.

</span>
<span class="ltx_bibblock">A survey on large language models for recommendation, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
Yifan Wang, Weizhi Ma, Min Zhang, Yiqun Liu, and Shaoping Ma.

</span>
<span class="ltx_bibblock">A survey on the fairness of recommender systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib64.1.1">ACM Transactions on Information Systems</span>, 41(3):1–43, February 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
Priyanka Nigam, Yiwei Song, Vijai Mohan, Vihan Lakshman, Weitian, Ding, Ankit Shingavi, Choon Hui Teo, Hao Gu, and Bing Yin.

</span>
<span class="ltx_bibblock">Semantic product search, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
Aounon Kumar and Himabindu Lakkaraju.

</span>
<span class="ltx_bibblock">Manipulating large language models to increase product visibility, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
Haixun Wang and Taesik Na.

</span>
<span class="ltx_bibblock">Rethinking e-commerce search, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
Alexander Brinkmann, Roee Shraga, and Christian Bizer.

</span>
<span class="ltx_bibblock">Product attribute value extraction using large language models, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, John Hoffman, Semarley Jarrett, Kaushik Ram Sadagopan, Dirk Rowe, Shannon Spruit, Chau Tran, Pierre Andrews, Necip Fazil Ayan, Shruti Bhosale, Sergey Edunov, Angela Fan, Cynthia Gao, Vedanuj Goswami, Francisco Guzmán, Philipp Koehn, Alexandre Mourachko, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, and Jeff Wang.

</span>
<span class="ltx_bibblock">No language left behind: Scaling human-centered machine translation, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer.

</span>
<span class="ltx_bibblock">Multilingual denoising pre-training for neural machine translation, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
S. Liu and J. Liu.

</span>
<span class="ltx_bibblock">A survey of large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib71.1.1">Proceedings of the 2021 International Conference on Data Science and Business Analytics (DSBA 2021)</span>. ACM, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Shanu Vashishtha, Abhinav Prakash, Lalitesh Morishetti, Kaushiki Nag, Yokila Arora, Sushant Kumar, and Kannan Achan.

</span>
<span class="ltx_bibblock">Chaining text-to-image and large language model: A novel approach for generating personalized e-commerce banners, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
Kalyani Roy, Smit Shah, Nithish Pai, Jaidam Ramtej, Prajit Prashant Nadkarn, Jyotirmoy Banerjee, Pawan Goyal, and Surender Kumar.

</span>
<span class="ltx_bibblock">Using large pretrained language models for answering user queries from product specifications, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
Yuanxing Liu, Wei-Nan Zhang, Yifan Chen, Yuchi Zhang, Haopeng Bai, Fan Feng, Hengbin Cui, Yongbin Li, and Wanxiang Che.

</span>
<span class="ltx_bibblock">Conversational recommender system and large language model are made for each other in e-commerce pre-sales dialogue, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Abubakar Abid, Maheen Farooqi, and James Zou.

</span>
<span class="ltx_bibblock">Persistent anti-muslim bias in large language models, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal Ndousse, Andy Jones, Sam Bowman, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Nelson Elhage, Sheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds, Tom Henighan, Danny Hernandez, Tristan Hume, Josh Jacobson, Scott Johnston, Shauna Kravec, Catherine Olsson, Sam Ringer, Eli Tran-Johnson, Dario Amodei, Tom Brown, Nicholas Joseph, Sam McCandlish, Chris Olah, Jared Kaplan, and Jack Clark.

</span>
<span class="ltx_bibblock">Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
Tien T. Nguyen, Pik Mai Hui, F. Maxwell Harper, Loren Terveen, and Joseph A. Konstan.

</span>
<span class="ltx_bibblock">Exploring the filter bubble: The effect of using recommender systems on content diversity.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib77.1.1">WWW 2014 - Proceedings of the 23rd International Conference on World Wide Web</span>, WWW 2014 - Proceedings of the 23rd International Conference on World Wide Web, pages 677–686. Association for Computing Machinery, April 2014.

</span>
<span class="ltx_bibblock">23rd International Conference on World Wide Web, WWW 2014 ; Conference date: 07-04-2014 Through 11-04-2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
Yunqi Li, Hanxiong Chen, Shuyuan Xu, Yingqiang Ge, Juntao Tan, Shuchang Liu, and Yongfeng Zhang.

</span>
<span class="ltx_bibblock">Fairness in recommendation: Foundations, methods and applications, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
Aylin Caliskan, Joanna J. Bryson, and Arvind Narayanan.

</span>
<span class="ltx_bibblock">Semantics derived automatically from language corpora contain human-like biases.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib79.1.1">Science</span>, 356(6334):183–186, April 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Chandler May, Alex Wang, Shikha Bordia, Samuel R. Bowman, and Rachel Rudinger.

</span>
<span class="ltx_bibblock">On measuring social biases in sentence encoders.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio, editors, <span class="ltx_text ltx_font_italic" id="bib.bib80.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</span>, pages 622–628, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Wei Guo and Aylin Caliskan.

</span>
<span class="ltx_bibblock">Detecting emergent intersectional biases: Contextualized word embeddings contain a distribution of human-like biases.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib81.1.1">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society</span>, AIES ’21. ACM, July 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta.

</span>
<span class="ltx_bibblock">Bold: Dataset and metrics for measuring biases in open-ended language generation.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib82.1.1">Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</span>, FAccT ’21. ACM, March 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
Po-Sen Huang, Huan Zhang, Ray Jiang, Robert Stanforth, Johannes Welbl, Jack Rae, Vishal Maini, Dani Yogatama, and Pushmeet Kohli.

</span>
<span class="ltx_bibblock">Reducing sentiment bias in language models via counterfactual evaluation.

</span>
<span class="ltx_bibblock">In Trevor Cohn, Yulan He, and Yang Liu, editors, <span class="ltx_text ltx_font_italic" id="bib.bib83.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</span>, pages 65–83, Online, November 2020. Association for Computational Linguistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
Ray Jiang, Aldo Pacchiano, Tom Stepleton, Heinrich Jiang, and Silvia Chiappa.

</span>
<span class="ltx_bibblock">Wasserstein fair classification, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
Zhenglin Li, Haibei Zhu, Houze Liu, Jintong Song, and Qishuo Cheng.

</span>
<span class="ltx_bibblock">Comprehensive evaluation of mal-api-2019 dataset by machine learning in malware detection.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib85.1.1">International Journal of Computer Science and Information Technology</span>, 2(1):1–9, March 2024.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Jun 21 21:26:30 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
