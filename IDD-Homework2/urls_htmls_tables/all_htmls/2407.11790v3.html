<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Characterizing and Understanding HGNN Training on GPUs</title>
<!--Generated on Fri Aug 16 01:11:27 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Heterogeneous Graph Neural Networks,  Graph Neural Networks Training,  Characterization,  Quantitative Analysis,  Optimization Guidelines" lang="en" name="keywords"/>
<base href="/html/2407.11790v3/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S1" title="In Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2" title="In Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.SS1" title="In 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Heterogeneous Graphs and Semantic Graphs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.SS2" title="In 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Heterogeneous Graph Neural Networks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.SS3" title="In 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>HGNN Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.SS3.SSS1" title="In 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Full-batch and Mini-batch Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.SS3.SSS2" title="In 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Single-node and Distributed Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.SS3.SSS3" title="In 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.3 </span>Workload Distribution</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S3" title="In Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Characterization Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S3.SS1" title="In 3. Characterization Methodology ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S3.SS1.SSS1" title="In 3.1. Experimental Setup ‣ 3. Characterization Methodology ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.1 </span>Platforms</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S3.SS1.SSS2" title="In 3.1. Experimental Setup ‣ 3. Characterization Methodology ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.2 </span>HGNN Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S3.SS1.SSS3" title="In 3.1. Experimental Setup ‣ 3. Characterization Methodology ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1.3 </span>Benchmark Datasets</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S3.SS2" title="In 3. Characterization Methodology ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Evaluation Methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4" title="In Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Single-GPU Training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS1" title="In 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Full-batch Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS1" title="In 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.1 </span>Execution Time Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS2" title="In 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.2 </span>Execution Bounds Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS3" title="In 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.3 </span>Memory Pattern Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS4" title="In 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.4 </span>Issue Stall Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS5" title="In 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1.5 </span>Exploring Metapath Changes</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS2" title="In 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Mini-batch Training</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS2.SSS1" title="In 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.1 </span>Execution Analysis</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS2.SSS2" title="In 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2.2 </span>Exploring Metapath Changes</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5" title="In Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Multi-GPU Training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.SS1" title="In 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Overall Profiling Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.SS1.SSS1" title="In 5.1. Overall Profiling Results ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Performance Comparison with Different Number of GPUs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.SS1.SSS2" title="In 5.1. Overall Profiling Results ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.2 </span>Execution Time Breakdown</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.SS2" title="In 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>In-depth Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.SS2.SSS1" title="In 5.2. In-depth Analysis ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.1 </span>CPU Resources Competition During Sampling</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.SS2.SSS2" title="In 5.2. In-depth Analysis ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2.2 </span>Bandwidth Competition During Data Loading and Synchronization</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6" title="In Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Comparison with GNN Training</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6.SS1" title="In 6. Comparison with GNN Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Difference in execution process</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6.SS1.SSS1" title="In 6.1. Difference in execution process ‣ 6. Comparison with GNN Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Unique Process of Metapath Instance Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6.SS1.SSS2" title="In 6.1. Difference in execution process ‣ 6. Comparison with GNN Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Separate Feature Projection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6.SS1.SSS3" title="In 6.1. Difference in execution process ‣ 6. Comparison with GNN Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Intricate Two-level Aggregation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6.SS2" title="In 6. Comparison with GNN Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Changes in execution bottlenecks</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6.SS2.SSS1" title="In 6.2. Changes in execution bottlenecks ‣ 6. Comparison with GNN Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>More Intricate Hybrid Execution Pattern</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6.SS2.SSS2" title="In 6.2. Changes in execution bottlenecks ‣ 6. Comparison with GNN Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Bottleneck Migration in Distributed Training</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S7" title="In Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Optimization Guidelines</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S7.SS1" title="In 7. Optimization Guidelines ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Software Perspective</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S7.SS1.SSS1" title="In 7.1. Software Perspective ‣ 7. Optimization Guidelines ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.1 </span>Reasonable Overlapping of Phases</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S7.SS1.SSS2" title="In 7.1. Software Perspective ‣ 7. Optimization Guidelines ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.2 </span>Recomputing to Reduce Memory Cost</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S7.SS1.SSS3" title="In 7.1. Software Perspective ‣ 7. Optimization Guidelines ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.3 </span>Scheduling Based on Semantic Graphs for Data Reuse</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S7.SS2" title="In 7. Optimization Guidelines ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Hardware Acceleration</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S7.SS2.SSS1" title="In 7.2. Hardware Acceleration ‣ 7. Optimization Guidelines ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.1 </span>Independent Parallel Neighbor Traversal Unit</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S7.SS2.SSS2" title="In 7.2. Hardware Acceleration ‣ 7. Optimization Guidelines ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.2 </span>Unified Reconfigurable Execution Unit</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S7.SS2.SSS3" title="In 7.2. Hardware Acceleration ‣ 7. Optimization Guidelines ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.3 </span>Multi-lane Architecture Supporting Semantic-graph-level Parallelism</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S8" title="In Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">Characterizing and Understanding HGNN Training on GPUs</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dengke Han
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:handengke21s@ict.ac.cn">handengke21s@ict.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-0641-5779" title="ORCID identifier">0000-0003-0641-5779</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">SKLP, ICT, CAS; UCAS</span><span class="ltx_text ltx_affiliation_streetaddress" id="id2.2.id2">6 Kexueyuan Nanlu, Zhongguancun, Haidian, Beijing, China</span><span class="ltx_text ltx_affiliation_city" id="id3.3.id3">Beijing</span><span class="ltx_text ltx_affiliation_state" id="id4.4.id4">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id5.5.id5">China</span><span class="ltx_text ltx_affiliation_postcode" id="id6.6.id6">100190</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mingyu Yan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yanmingyu@ict.ac.cn">yanmingyu@ict.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0002-6915-955X" title="ORCID identifier">0000-0002-6915-955X</a></span>
</span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaochun Ye
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:yexiaochun@ict.ac.cn">yexiaochun@ict.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0003-4598-1685" title="ORCID identifier">0000-0003-4598-1685</a></span>
</span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Dongrui Fan
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:fandr@ict.ac.cn">fandr@ict.ac.cn</a>
</span>
<span class="ltx_contact ltx_role_orcid"><a class="ltx_ref" href="https://orcid.org/0000-0001-5219-0908" title="ORCID identifier">0000-0001-5219-0908</a></span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">SKLP, ICT, CAS</span><span class="ltx_text ltx_affiliation_streetaddress" id="id8.2.id2">6 Kexueyuan Nanlu, Zhongguancun, Haidian, Beijing, China</span><span class="ltx_text ltx_affiliation_city" id="id9.3.id3">Beijing</span><span class="ltx_text ltx_affiliation_state" id="id10.4.id4">Beijing</span><span class="ltx_text ltx_affiliation_country" id="id11.5.id5">China</span><span class="ltx_text ltx_affiliation_postcode" id="id12.6.id6">100190</span>
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id13.id1">Owing to their remarkable representation capabilities for heterogeneous graph data, Heterogeneous Graph Neural Networks (HGNNs) have been widely adopted in many critical real-world domains such as recommendation systems and medical analysis. Prior to their practical application, identifying the optimal HGNN model parameters tailored to specific tasks through extensive training is a time-consuming and costly process.
To enhance the efficiency of HGNN training, it is essential to characterize and analyze the execution semantics and patterns within the training process to identify performance bottlenecks. In this study, we conduct an in-depth quantification and analysis of two mainstream HGNN training scenarios, including single-GPU and multi-GPU distributed training. Based on the characterization results, we disclose the performance bottlenecks and their underlying causes in different HGNN training scenarios and provide optimization guidelines from both software and hardware perspectives.</p>
</div>
<div class="ltx_keywords">Heterogeneous Graph Neural Networks, Graph Neural Networks Training, Characterization, Quantitative Analysis, Optimization Guidelines
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_journal" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journal: </span>TACO</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>General and reference General literature</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computer systems organization Architectures</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Theory of computation Graph algorithms analysis</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Artificial intelligence</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In recent years, Graph Neural Networks (GNNs) have demonstrated exceptional capabilities in representing and processing graph data in non-Euclidean spaces <cite class="ltx_cite ltx_citemacro_citep">(Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib36" title="">2020a</a>; Xu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib38" title="">2018</a>; Zhou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib52" title="">2020</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib35" title="">2020b</a>)</cite>. The early successes of GNNs are predominantly in the domain of homogeneous graphs (HomoGs), characterized by a single type of entity and adjacency relation <cite class="ltx_cite ltx_citemacro_citep">(Kipf and Welling, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib16" title="">2017</a>; Veličković et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib29" title="">2018</a>; Hamilton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib10" title="">2017</a>)</cite>. However, many real-world data in complex systems are more aptly represented as heterogeneous graphs (HetGs) which consist of multiple types of entities and relations embodied by various types of vertices and edges, respectively. In contrast to HomoGs, HetGs possess not only structural information inherent in graph data but also rich semantic information harbored in the relations <cite class="ltx_cite ltx_citemacro_citep">(Shi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib27" title="">2016</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib32" title="">2020a</a>)</cite>. Due to the powerful representation ability of HetGs, Heterogeneous Graph Neural Networks (HGNNs) have been developed and widely adopted in many critical fields including recommendation systems <cite class="ltx_cite ltx_citemacro_citep">(Zhao and Yu, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib49" title="">2021</a>; Altaf et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib2" title="">2019</a>; xia Liang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib37" title="">2022</a>)</cite>, cybersecurity <cite class="ltx_cite ltx_citemacro_citep">(Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib12" title="">2017</a>; Ye et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib45" title="">2019</a>; Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib5" title="">2018</a>)</cite>, medical analysis <cite class="ltx_cite ltx_citemacro_citep">(Luo et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib21" title="">2021</a>; Kim et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib15" title="">2023</a>)</cite>, traffic prediction <cite class="ltx_cite ltx_citemacro_citep">(Nazzal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib24" title="">2024</a>; Jin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib14" title="">2021</a>; Zhong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib51" title="">2021</a>)</cite> and many others.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.4">Unlike GNNs, which recursively aggregate the feature vectors of neighboring vertices <cite class="ltx_cite ltx_citemacro_citep">(Kipf and Welling, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib16" title="">2017</a>; Wu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib36" title="">2020a</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib18" title="">2023</a>)</cite> to obtain structural information in HomoGs, HGNNs employ a different execution semantics to extract both structural and semantic information. Specifically, most of the mainstream HGNN models can be partitioned into four primary execution stages <cite class="ltx_cite ltx_citemacro_citep">(Schlichtkrull et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib26" title="">2018</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib30" title="">2020b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib33" title="">2019a</a>; Lv et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib22" title="">2021</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib44" title="">2023</a>)</cite>:  <svg class="ltx_picture" height="12.66" id="S1.p2.1.pic1" overflow="visible" version="1.1" width="12.66"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,12.66) matrix(1 0 0 -1 0 0) translate(6.33,0) translate(0,6.33)"><g stroke="#000000"><path d="M 6.05 0 C 6.05 3.34 3.34 6.05 0 6.05 C -3.34 6.05 -6.05 3.34 -6.05 0 C -6.05 -3.34 -3.34 -6.05 0 -6.05 C 3.34 -6.05 6.05 -3.34 6.05 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.23"><span class="ltx_text" id="S1.p2.1.pic1.1.1.1.1.1" style="font-size:90%;">1</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S1.p2.4.1">Semantic Graph Build</span> (SGB);  <svg class="ltx_picture" height="12.66" id="S1.p2.2.pic2" overflow="visible" version="1.1" width="12.66"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,12.66) matrix(1 0 0 -1 0 0) translate(6.33,0) translate(0,6.33)"><g stroke="#000000"><path d="M 6.05 0 C 6.05 3.34 3.34 6.05 0 6.05 C -3.34 6.05 -6.05 3.34 -6.05 0 C -6.05 -3.34 -3.34 -6.05 0 -6.05 C 3.34 -6.05 6.05 -3.34 6.05 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.23"><span class="ltx_text" id="S1.p2.2.pic2.1.1.1.1.1" style="font-size:90%;">2</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S1.p2.4.2">Feature Projection</span> (FP);  <svg class="ltx_picture" height="12.66" id="S1.p2.3.pic3" overflow="visible" version="1.1" width="12.66"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,12.66) matrix(1 0 0 -1 0 0) translate(6.33,0) translate(0,6.33)"><g stroke="#000000"><path d="M 6.05 0 C 6.05 3.34 3.34 6.05 0 6.05 C -3.34 6.05 -6.05 3.34 -6.05 0 C -6.05 -3.34 -3.34 -6.05 0 -6.05 C 3.34 -6.05 6.05 -3.34 6.05 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.23"><span class="ltx_text" id="S1.p2.3.pic3.1.1.1.1.1" style="font-size:90%;">3</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S1.p2.4.3">Neighbor Aggregation</span> (NA);  <svg class="ltx_picture" height="12.66" id="S1.p2.4.pic4" overflow="visible" version="1.1" width="12.66"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,12.66) matrix(1 0 0 -1 0 0) translate(6.33,0) translate(0,6.33)"><g stroke="#000000"><path d="M 6.05 0 C 6.05 3.34 3.34 6.05 0 6.05 C -3.34 6.05 -6.05 3.34 -6.05 0 C -6.05 -3.34 -3.34 -6.05 0 -6.05 C 3.34 -6.05 6.05 -3.34 6.05 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.23"><span class="ltx_text" id="S1.p2.4.pic4.1.1.1.1.1" style="font-size:90%;">4</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S1.p2.4.4">Semantic Fusion</span> (SF). The SGB stage partitions the original HetGs into multiple semantic graphs. The FP and NA stages perform conventional GNN processes, operating independently within each semantic graph. Subsequently, the SF stage fuses the results of the NA stage across different semantic graphs.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Applying HGNNs to downstream tasks such as vertex classification or link prediction requires a comprehensive training process to find the optimal model parameters. Inherited from conventional GNNs, training HGNNs on computing nodes like GPUs primarily involves two methods: full-batch and mini-batch training. Unlike full-batch training on the entire graph, mini-batch training iteratively trains on mini-batches generated from a group of target vertices, significantly reducing memory footprints and speeding up convergence <cite class="ltx_cite ltx_citemacro_citep">(Hamilton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib10" title="">2017</a>)</cite>. Moreover, distributed training across multiple computing nodes distributes the training load, improving the overall performance of the training process. Although the introduction of mini-batch training and distributed training solutions has improved the scalability and training efficiency of GNN models respectively, the training process itself remains extremely time-consuming and resource-intensive <cite class="ltx_cite ltx_citemacro_citep">(Thorpe et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib28" title="">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib46" title="">2022a</a>)</cite>. HGNNs, due to their higher algorithmic complexity compared to GNNs <cite class="ltx_cite ltx_citemacro_citep">(Bing et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib3" title="">2023</a>)</cite>, exhibit even higher costs than GNNs.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Quantitatively analyzing and characterizing the execution behaviors and patterns of HGNN models is crucial for enhancing their execution efficiency. Currently, extensive efforts have been dedicated to characterizing GNNs <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib41" title="">2020</a>; Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib17" title="">2022</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib46" title="">2022a</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib34" title="">2021</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib48" title="">2020</a>)</cite>, contributing to a substantial understanding within the research community of their execution patterns. However, there is limited effort focused on HGNN characterization. On one hand, GNNs and HGNNs exhibit distinct execution semantics and patterns, with HGNNs demonstrating more intricate execution behaviors. Consequently, characterization results for GNNs cannot be directly applied to optimize HGNN efficiency. On the other hand, existing characterization study on HGNNs <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib43" title="">2022</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib4" title="">2023</a>; Xue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib39" title="">2024a</a>)</cite> have primarily focused on inference tasks. Yet, the training process is significantly more complex than inference, rendering existing inference-focused characterizations insufficient for guiding training efficiency optimizations.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address these gaps, we conduct a comprehensive characterization and quantitative analysis of HGNN training across single-node and multi-node distributed platforms, incorporating both full-batch and mini-batch training methods. Specifically, we undertake a quantitative characterization from various perspectives, encompassing the intrinsic characteristics of the HGNN execution stages, a comparative analysis of forward and backward propagation, the overall attributes of the training process, and the influence of metapath properties on training performance. Furthermore, a comparative analysis of HGNN and GNN training is carried out. Finally, we propose optimization guidelines from both software and hardware perspectives. We systematically summarize our 22 findings based on the four aforementioned characterization perspectives as follows:</p>
</div>
<div class="ltx_para" id="S1.p6">
<ul class="ltx_itemize" id="S1.p6.1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i1.1.1.m1.1"><semantics id="S1.I1.i1.1.1.m1.1b"><mo id="S1.I1.i1.1.1.m1.1.1" xref="S1.I1.i1.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i1.1.1.m1.1c"><ci id="S1.I1.i1.1.1.m1.1.1.cmml" xref="S1.I1.i1.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i1.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i1.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i1.p1.1.1">Execution Characteristics of HGNN Models</span>: (1) The NA stage dominates the most execution time both in forward and backward propagation, which is considered the predominant stage across all the execution stages of HGNNs; (2) There exists a distinct hybrid execution pattern between HGNN stages, each characterized by unique execution bounds, resulting in varying demands on hardware resources.; (3) Mini-batch sampling accounts for the majority of the execution time in each epoch of mini-batch training in both single-GPU and multi-GPU distributed scenarios.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i2.1.1.m1.1"><semantics id="S1.I1.i2.1.1.m1.1b"><mo id="S1.I1.i2.1.1.m1.1.1" xref="S1.I1.i2.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i2.1.1.m1.1c"><ci id="S1.I1.i2.1.1.m1.1.1.cmml" xref="S1.I1.i2.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i2.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i2.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i2.p1.1.1">Comparison between Backward and Forward Propagation</span>: (1) Forward propagation is generally more time-consuming than backward propagation; (2) Although the kernels used during backward propagation are similar to those in forward propagation, there are significant differences in time distribution across kernel types and in the execution characteristics of the same kernels; (3) Backward propagation demands greater memory access and exhibits lower data locality than the forward pass. However, execution stages characterized by a high number of additions are anticipated to incur a lower computational workload in backward propagation.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i3.1.1.m1.1"><semantics id="S1.I1.i3.1.1.m1.1b"><mo id="S1.I1.i3.1.1.m1.1.1" xref="S1.I1.i3.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i3.1.1.m1.1c"><ci id="S1.I1.i3.1.1.m1.1.1.cmml" xref="S1.I1.i3.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i3.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i3.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.1">Overall Analysis of End-to-end Training Process</span>: (1) In comparison to pure inference, the training process necessitates greater memory footprint, with the majority memory allocation lies on the NA stage; (2) The vast majority of kernel stalls during the training process originate from memory dependency except for <span class="ltx_text ltx_font_italic" id="S1.I1.i3.p1.1.2">sgemm</span> kernel; (3) The principal factor affecting the speedup of multi-GPU distributed training is the competition for various shared hardware resources.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><math alttext="\bullet" class="ltx_Math" display="inline" id="S1.I1.i4.1.1.m1.1"><semantics id="S1.I1.i4.1.1.m1.1b"><mo id="S1.I1.i4.1.1.m1.1.1" xref="S1.I1.i4.1.1.m1.1.1.cmml">∙</mo><annotation-xml encoding="MathML-Content" id="S1.I1.i4.1.1.m1.1c"><ci id="S1.I1.i4.1.1.m1.1.1.cmml" xref="S1.I1.i4.1.1.m1.1.1">∙</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.I1.i4.1.1.m1.1d">\bullet</annotation><annotation encoding="application/x-llamapun" id="S1.I1.i4.1.1.m1.1e">∙</annotation></semantics></math></span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1"><span class="ltx_text ltx_font_italic" id="S1.I1.i4.p1.1.1">Exploration Related to Metapaths</span>: (1) An increase in both the length and number of metapaths significantly extends execution time during the NA stage, while the FP and SF stages are sensitive only to variations in the number of metapaths. (2) For large-scale datasets, an increase in both the length and number of metapaths leads to a substantial rise in sampling time. While sampling time is primarily influenced by changes in the number of metapaths for small datasets.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Background</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Heterogeneous Graphs and Semantic Graphs</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.3">Fig <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F1" title="Figure 1 ‣ 2.1. Heterogeneous Graphs and Semantic Graphs ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates a simple example of a HetG from the ACM dataset, which includes three types of vertices, A (Author), P (Paper), and S (Subject), along with three types of adjacency relations between them: author<math alttext="\mathrel{\mathop{\xrightarrow{}}\limits^{\raisebox{-1.5pt}[0.0pt][0.0pt]{%
\scriptsize\rm writes}}}" class="ltx_Math" display="inline" id="S2.SS1.p1.1.m1.1"><semantics id="S2.SS1.p1.1.m1.1a"><mover id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mover accent="true" id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml"><mo id="S2.SS1.p1.1.m1.1.1.2.2" stretchy="false" xref="S2.SS1.p1.1.m1.1.1.2.2.cmml">→</mo><mi id="S2.SS1.p1.1.m1.1.1.2.1" xref="S2.SS1.p1.1.m1.1.1.2.1.cmml"></mi></mover><mpadded id="S2.SS1.p1.1.m1.1.1.3" voffset="-1.5pt" xref="S2.SS1.p1.1.m1.1.1.3b.cmml"><mtext id="S2.SS1.p1.1.m1.1.1.3a" xref="S2.SS1.p1.1.m1.1.1.3b.cmml">writes</mtext></mpadded></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1">superscript</csymbol><apply id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2"><csymbol cd="latexml" id="S2.SS1.p1.1.m1.1.1.2.1.cmml" xref="S2.SS1.p1.1.m1.1.1.2.1">absent</csymbol><ci id="S2.SS1.p1.1.m1.1.1.2.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2.2">→</ci></apply><ci id="S2.SS1.p1.1.m1.1.1.3b.cmml" xref="S2.SS1.p1.1.m1.1.1.3"><mpadded id="S2.SS1.p1.1.m1.1.1.3.cmml" voffset="-1.5pt" xref="S2.SS1.p1.1.m1.1.1.3"><mtext id="S2.SS1.p1.1.m1.1.1.3a.cmml" mathsize="70%" xref="S2.SS1.p1.1.m1.1.1.3">writes</mtext></mpadded></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">\mathrel{\mathop{\xrightarrow{}}\limits^{\raisebox{-1.5pt}[0.0pt][0.0pt]{%
\scriptsize\rm writes}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.1.m1.1d">start_BIGOP start_OVERACCENT end_OVERACCENT → end_BIGOP start_POSTSUPERSCRIPT writes end_POSTSUPERSCRIPT</annotation></semantics></math>paper, paper<math alttext="\mathrel{\mathop{\xrightarrow{}}\limits^{\raisebox{-1.5pt}[0.0pt][0.0pt]{%
\scriptsize\rm cites}}}" class="ltx_Math" display="inline" id="S2.SS1.p1.2.m2.1"><semantics id="S2.SS1.p1.2.m2.1a"><mover id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mover accent="true" id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml"><mo id="S2.SS1.p1.2.m2.1.1.2.2" stretchy="false" xref="S2.SS1.p1.2.m2.1.1.2.2.cmml">→</mo><mi id="S2.SS1.p1.2.m2.1.1.2.1" xref="S2.SS1.p1.2.m2.1.1.2.1.cmml"></mi></mover><mpadded id="S2.SS1.p1.2.m2.1.1.3" voffset="-1.5pt" xref="S2.SS1.p1.2.m2.1.1.3b.cmml"><mtext id="S2.SS1.p1.2.m2.1.1.3a" xref="S2.SS1.p1.2.m2.1.1.3b.cmml">cites</mtext></mpadded></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">superscript</csymbol><apply id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2"><csymbol cd="latexml" id="S2.SS1.p1.2.m2.1.1.2.1.cmml" xref="S2.SS1.p1.2.m2.1.1.2.1">absent</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2.2">→</ci></apply><ci id="S2.SS1.p1.2.m2.1.1.3b.cmml" xref="S2.SS1.p1.2.m2.1.1.3"><mpadded id="S2.SS1.p1.2.m2.1.1.3.cmml" voffset="-1.5pt" xref="S2.SS1.p1.2.m2.1.1.3"><mtext id="S2.SS1.p1.2.m2.1.1.3a.cmml" mathsize="70%" xref="S2.SS1.p1.2.m2.1.1.3">cites</mtext></mpadded></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">\mathrel{\mathop{\xrightarrow{}}\limits^{\raisebox{-1.5pt}[0.0pt][0.0pt]{%
\scriptsize\rm cites}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.2.m2.1d">start_BIGOP start_OVERACCENT end_OVERACCENT → end_BIGOP start_POSTSUPERSCRIPT cites end_POSTSUPERSCRIPT</annotation></semantics></math>paper, and paper<math alttext="\mathrel{\mathop{\xrightarrow{}}\limits^{\raisebox{-1.5pt}[0.0pt][0.0pt]{%
\scriptsize\rm belongs\ to}}}" class="ltx_Math" display="inline" id="S2.SS1.p1.3.m3.1"><semantics id="S2.SS1.p1.3.m3.1a"><mover id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml"><mover accent="true" id="S2.SS1.p1.3.m3.1.1.2" xref="S2.SS1.p1.3.m3.1.1.2.cmml"><mo id="S2.SS1.p1.3.m3.1.1.2.2" stretchy="false" xref="S2.SS1.p1.3.m3.1.1.2.2.cmml">→</mo><mi id="S2.SS1.p1.3.m3.1.1.2.1" xref="S2.SS1.p1.3.m3.1.1.2.1.cmml"></mi></mover><mpadded id="S2.SS1.p1.3.m3.1.1.3" voffset="-1.5pt" xref="S2.SS1.p1.3.m3.1.1.3b.cmml"><mtext id="S2.SS1.p1.3.m3.1.1.3a" xref="S2.SS1.p1.3.m3.1.1.3b.cmml">belongs to</mtext></mpadded></mover><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><apply id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.3.m3.1.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">superscript</csymbol><apply id="S2.SS1.p1.3.m3.1.1.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2"><csymbol cd="latexml" id="S2.SS1.p1.3.m3.1.1.2.1.cmml" xref="S2.SS1.p1.3.m3.1.1.2.1">absent</csymbol><ci id="S2.SS1.p1.3.m3.1.1.2.2.cmml" xref="S2.SS1.p1.3.m3.1.1.2.2">→</ci></apply><ci id="S2.SS1.p1.3.m3.1.1.3b.cmml" xref="S2.SS1.p1.3.m3.1.1.3"><mpadded id="S2.SS1.p1.3.m3.1.1.3.cmml" voffset="-1.5pt" xref="S2.SS1.p1.3.m3.1.1.3"><mtext id="S2.SS1.p1.3.m3.1.1.3a.cmml" mathsize="70%" xref="S2.SS1.p1.3.m3.1.1.3">belongs to</mtext></mpadded></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">\mathrel{\mathop{\xrightarrow{}}\limits^{\raisebox{-1.5pt}[0.0pt][0.0pt]{%
\scriptsize\rm belongs\ to}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p1.3.m3.1d">start_BIGOP start_OVERACCENT end_OVERACCENT → end_BIGOP start_POSTSUPERSCRIPT belongs to end_POSTSUPERSCRIPT</annotation></semantics></math>subject (abbreviated as AP, PP and PS). Moreover, the inverse of these relations also holds significant meaning like PA and SP. In addition to direct relations like AP, different combinations of these direct relations can form higher-order relations, referred to as metapaths. For instance, in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F1" title="Figure 1 ‣ 2.1. Heterogeneous Graphs and Semantic Graphs ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">1</span></a>, PSP represents a metapath composed of PS and SP, signifying that two papers are linked by a shared subject, implying a strong likelihood that the two papers pertain to the same research area. Each type of relation or metapath represents a unique semantic information between the two endpoints connected.</p>
</div>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="331" id="S2.F1.g1" src="x1.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Illustration of HetGs and HGNNs.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">The semantic graphs are derived from the original heterogeneous graph based on specific relationships or metapaths. Each semantic graph contains only one type of relationship or metapath. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F1" title="Figure 1 ‣ 2.1. Heterogeneous Graphs and Semantic Graphs ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">1</span></a>, we build a semantic graph based on AP, PP, PS, and PSP, respectively. Semantic graphs separate multiple semantics in the original HetG, facilitating information extraction. Numerous HGNN models <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib33" title="">2019a</a>; Fu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib7" title="">2020</a>; Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib44" title="">2023</a>)</cite> utilize metapaths to construct semantic graphs, while others <cite class="ltx_cite ltx_citemacro_citep">(Schlichtkrull et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib26" title="">2018</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib30" title="">2020b</a>)</cite> employ direct relations, i.e., edge types, for semantic graph construction.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Heterogeneous Graph Neural Networks</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.4">Given the inefficiencies of traditional GNNs in extracting semantic information from HetGs, a multitude of specialized HGNNs have recently emerged <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib33" title="">2019a</a>; Schlichtkrull et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib26" title="">2018</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib30" title="">2020b</a>; Fu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib7" title="">2020</a>; Lv et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib22" title="">2021</a>)</cite>, which incorporate both structural and semantic information from HetGs through distinct neighborhood aggregation and semantic fusion schemes. Specifically, these schemes involve separate aggregation of neighbors with different semantics followed by their fusion. In general, most prevalent HGNNs usually contain four major execution stages as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F1" title="Figure 1 ‣ 2.1. Heterogeneous Graphs and Semantic Graphs ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">1</span></a>:  <svg class="ltx_picture" height="12.66" id="S2.SS2.p1.1.pic1" overflow="visible" version="1.1" width="12.66"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,12.66) matrix(1 0 0 -1 0 0) translate(6.33,0) translate(0,6.33)"><g stroke="#000000"><path d="M 6.05 0 C 6.05 3.34 3.34 6.05 0 6.05 C -3.34 6.05 -6.05 3.34 -6.05 0 C -6.05 -3.34 -3.34 -6.05 0 -6.05 C 3.34 -6.05 6.05 -3.34 6.05 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.23"><span class="ltx_text" id="S2.SS2.p1.1.pic1.1.1.1.1.1" style="font-size:90%;">1</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.4.1">Semantic Graph Build</span> stage partitions the original HetG into several semantic graphs;  <svg class="ltx_picture" height="12.66" id="S2.SS2.p1.2.pic2" overflow="visible" version="1.1" width="12.66"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,12.66) matrix(1 0 0 -1 0 0) translate(6.33,0) translate(0,6.33)"><g stroke="#000000"><path d="M 6.05 0 C 6.05 3.34 3.34 6.05 0 6.05 C -3.34 6.05 -6.05 3.34 -6.05 0 C -6.05 -3.34 -3.34 -6.05 0 -6.05 C 3.34 -6.05 6.05 -3.34 6.05 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.23"><span class="ltx_text" id="S2.SS2.p1.2.pic2.1.1.1.1.1" style="font-size:90%;">2</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.4.2">Feature Projection</span> stage transforms the feature vectors of vertices in each semantic graph to new ones using multi-layer perceptrons;  <svg class="ltx_picture" height="12.66" id="S2.SS2.p1.3.pic3" overflow="visible" version="1.1" width="12.66"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,12.66) matrix(1 0 0 -1 0 0) translate(6.33,0) translate(0,6.33)"><g stroke="#000000"><path d="M 6.05 0 C 6.05 3.34 3.34 6.05 0 6.05 C -3.34 6.05 -6.05 3.34 -6.05 0 C -6.05 -3.34 -3.34 -6.05 0 -6.05 C 3.34 -6.05 6.05 -3.34 6.05 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.23"><span class="ltx_text" id="S2.SS2.p1.3.pic3.1.1.1.1.1" style="font-size:90%;">3</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.4.3">Neighbor Aggregation</span> stage aggregates features from neighbors for each target vertex within semantic graphs;  <svg class="ltx_picture" height="12.66" id="S2.SS2.p1.4.pic4" overflow="visible" version="1.1" width="12.66"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,12.66) matrix(1 0 0 -1 0 0) translate(6.33,0) translate(0,6.33)"><g stroke="#000000"><path d="M 6.05 0 C 6.05 3.34 3.34 6.05 0 6.05 C -3.34 6.05 -6.05 3.34 -6.05 0 C -6.05 -3.34 -3.34 -6.05 0 -6.05 C 3.34 -6.05 6.05 -3.34 6.05 0 Z M 0 0" style="fill:none"></path></g><g fill="#000000" stroke="#000000" transform="matrix(1.0 0.0 0.0 1.0 -3.11 -4.01)"><foreignobject height="8.03" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="6.23"><span class="ltx_text" id="S2.SS2.p1.4.pic4.1.1.1.1.1" style="font-size:90%;">4</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.4.4">Semantic Fusion</span> stage fuses the results of the NA stage across different semantic graphs for each vertex to aggregate the semantic information.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>HGNN Training</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F2" title="Figure 2 ‣ 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a> depicts the training process of the HGNN models. The process of a single training epoch can generally be divided into four main steps as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F2" title="Figure 2 ‣ 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a>(c). Firstly, during forward propagation, the embeddings of target vertices are computed according to the procedural steps as in model formula. Secondly, the loss computation (LC) stage transforms these embeddings into the vector space of classification categories, subsequently generating a probability distribution used to compute the loss function. Thirdly, backward propagation calculates the gradient of each model parameter relative to the loss function, employing the chain rule to determine the direction for parameter adjustments that yield the most rapid loss reduction. Finally, in the parameters update (PU) stage, model parameters are adjusted based on these gradients and a predefined learning rate.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="272" id="S2.F2.g1" src="x2.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Illustration of HGNN training: (a) SGB stage; (b) Mini-batch Sampling stage; (c) Training process on a single computing node; (d) Distributed training process.</figcaption>
</figure>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1. </span>Full-batch and Mini-batch Training</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">From the perspective of training methods, HGNN training can be categorized into two primary approaches: full-batch training, where the entire graph dataset is processed per epoch; and mini-batch training, which entails iterative processing of multiple mini-batches consisting of sets of target vertices and their respective neighbors within each epoch. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F2" title="Figure 2 ‣ 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a>(a), semantic graphs are first constructed from the original HetG based on the relation type or predefined metapaths when performing full-batch training. Subsequently, multiple epochs are executed to iteratively update the parameters as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F2" title="Figure 2 ‣ 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a>(c).</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS1.p2">
<p class="ltx_p" id="S2.SS3.SSS1.p2.1">Mini-batch training differs in that the input data during execution does not encompass the entire graph structure. Instead, target vertices are partitioned into multiple groups, and neighbors of these vertices are sampled based on specified traversal depth and the number of sampled neighbors, forming individual mini-batches as depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F2" title="Figure 2 ‣ 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a>(b). Notably, when employing the mini-batch training method, the necessity of performing the SGB stage to construct a complete semantic graph is obviated. Instead, direct sampling is conducted, which can be regarded as a subset of the SGB execution process. Each training epoch involves the independent execution of multiple batches, with gradient updates performed autonomously for each batch, thus reducing resource requirements and improving the model’s ability to handle large-scale datasets.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2. </span>Single-node and Distributed Training</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">Distributed training has emerged as a solution to the limitations associated with the memory and computational capacity of a single machine. In practical applications, distributed training commonly employs mini-batch methods to achieve rapid convergence while preserving model accuracy <cite class="ltx_cite ltx_citemacro_citep">(Zheng et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib50" title="">2022</a>)</cite>. As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F2" title="Figure 2 ‣ 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a>(d), each computing node in distributed training independently processes one batch. Following backward propagation to obtain gradients, synchronization across nodes is essential through an <span class="ltx_text ltx_font_italic" id="S2.SS3.SSS2.p1.1.1">All-reduce</span> operation, which ensures consistency of model parameters across all nodes at the outset of each epoch.</p>
</div>
<div class="ltx_para" id="S2.SS3.SSS2.p2">
<p class="ltx_p" id="S2.SS3.SSS2.p2.1">The principal benefits of distributed training encompass enhanced performance, facilitating parallel processing across multiple nodes; and expanded model scalability, enabling the training of more intricate models and processing of large-scale datasets. Moreover, this approach provides fault tolerance, as the failure of a single node does not necessarily disrupt the entire training process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.3. </span>Workload Distribution</h4>
<div class="ltx_para" id="S2.SS3.SSS3.p1">
<p class="ltx_p" id="S2.SS3.SSS3.p1.1">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F2" title="Figure 2 ‣ 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a>(a) and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2.F2" title="Figure 2 ‣ 2.3. HGNN Training ‣ 2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a>(b), the processes of SGB and mini-batch sampling are integral components of input data preparation, typically classified as data preprocessing.
In contemporary mainstream training platforms, a heterogeneous configuration is frequently adopted, comprising a central host CPU and peripheral computing nodes, typically GPUs. Preprocessing tasks are typically executed by the CPU, which subsequently transfers the prepared data to the computing nodes for execution. These nodes are tasked with executing the entire training iterations and producing the final well-optimized model parameters.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Characterization Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">To comprehensively and thoroughly characterize the training process of HGNNs, we adopt a rational and rigorous evaluation approach. In this section, we initially outline our experimental setup, encompassing the experimental platform, software framework, model and dataset selection. Then we delineate our evaluation methods.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Experimental Setup</h3>
<section class="ltx_subsubsection" id="S3.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.1. </span>Platforms</h4>
<div class="ltx_para" id="S3.SS1.SSS1.p1">
<p class="ltx_p" id="S3.SS1.SSS1.p1.1">The configuration of our experimental platform is detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S3.T1" title="Table 1 ‣ 3.1.1. Platforms ‣ 3.1. Experimental Setup ‣ 3. Characterization Methodology ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">1</span></a>, featuring computing nodes equipped with four advanced A100 GPUs. The four GPUs are organized into two groups, with intra-group connections facilitated by NVLink buses, while inter-group connections are established through PCIe buses. We utilize Nsight Systems and Nsight Compute tools to capture detailed performance metrics data of execution on the GPUs. Regarding the software framework, we choose DGL <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib31" title="">2019b</a>)</cite>, which emerges as one of the most prominent GNN frameworks, typically outperforming PyG <cite class="ltx_cite ltx_citemacro_citep">(Fey and Lenssen, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib6" title="">2019</a>)</cite> in terms of runtime efficiency and energy consumption <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib13" title="">2022</a>)</cite>. All the experiments are conducted utilizing 32-bit floating-point data format.</p>
</div>
<figure class="ltx_table ltx_align_floatleft" id="S3.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Platform Configurations</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.1" style="width:216.8pt;height:107.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-18.8pt,9.3pt) scale(0.851997278426807,0.851997278426807) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.1.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1.1" style="padding-top:1pt;padding-bottom:1pt;"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T1.1.1.1.1.2" style="padding-top:1pt;padding-bottom:1pt;">Configuration</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.1" style="padding-top:1pt;padding-bottom:1pt;">CPU</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.2.1.2" style="padding-top:1pt;padding-bottom:1pt;">Intel(R) Xeon(R) Platinum 8350C CPU</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.1" style="padding-top:1pt;padding-bottom:1pt;">GPU</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.3.2.2" style="padding-top:1pt;padding-bottom:1pt;">NVIDIA A100 80GB SXM GPU</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.4.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.4.3.1" style="padding-top:1pt;padding-bottom:1pt;">OS</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.4.3.2" style="padding-top:1pt;padding-bottom:1pt;">Ubuntu 20.04.5 LTS</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.5.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.5.4.1" style="padding-top:1pt;padding-bottom:1pt;">Framework</th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.5.4.2" style="padding-top:1pt;padding-bottom:1pt;">Deep Graph Library 1.0.2</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.6.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T1.1.1.6.5.1" rowspan="2" style="padding-top:1pt;padding-bottom:1pt;"><span class="ltx_text" id="S3.T1.1.1.6.5.1.1">Tools</span></th>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.1.1.6.5.2" style="padding-top:1pt;padding-bottom:1pt;">Nsight Compute 2021.2.1.0</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S3.T1.1.1.7.6.1" style="padding-top:1pt;padding-bottom:1pt;">Nsight Systems 2023.4.1.97</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.2. </span>HGNN Models</h4>
<div class="ltx_para" id="S3.SS1.SSS2.p1">
<p class="ltx_p" id="S3.SS1.SSS2.p1.1">We conduct experiments on three mainstream HGNN models, namely HAN <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib33" title="">2019a</a>)</cite>, RGCN <cite class="ltx_cite ltx_citemacro_citep">(Schlichtkrull et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib26" title="">2018</a>)</cite> and RGAT <cite class="ltx_cite ltx_citemacro_citep">(Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib30" title="">2020b</a>)</cite> as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S3.T2" title="Table 2 ‣ 3.1.2. HGNN Models ‣ 3.1. Experimental Setup ‣ 3. Characterization Methodology ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a>, each with its own representative features. Specifically, RGCN first extends the conventional GCN <cite class="ltx_cite ltx_citemacro_citep">(Kipf and Welling, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib16" title="">2017</a>)</cite> to handle HetGs by applying separate GCN convolutions to each semantic graph and aggregating the results through summation (SUM). RGAT builds upon this by introducing attention mechanisms in NA stage, with semantic fusion performed via SUM as well. HAN further enhances this by introducing attention mechanisms in SF stage, enabling the model to focus on important semantic graphs. In summary, the selected models are representative and widely adopted within the field, offering a comprehensive reflection of HGNN models as a whole.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Information of HGNN models.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:433.6pt;height:63.1pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-56.6pt,8.1pt) scale(0.792943918266984,0.792943918266984) ;">
<table class="ltx_tabular ltx_align_middle" id="S3.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.1" style="padding:1pt 2.0pt;">Model</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.2" style="padding:1pt 2.0pt;">#Layers</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.3" style="padding:1pt 2.0pt;">#Hidden Dimension</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.4" style="padding:1pt 2.0pt;">#Attention Heads</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.5" style="padding:1pt 2.0pt;">SGB</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.6" style="padding:1pt 2.0pt;">FP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.7" style="padding:1pt 2.0pt;">NA</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.2.1.8" style="padding:1pt 2.0pt;">SF</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.1" style="padding:1pt 2.0pt;">HAN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.2" style="padding:1pt 2.0pt;">1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.3" style="padding:1pt 2.0pt;">64</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.4" style="padding:1pt 2.0pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.5" style="padding:1pt 2.0pt;">Metapath</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.6" style="padding:1pt 2.0pt;">Linear Transformation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.7" style="padding:1pt 2.0pt;">Attention SUM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.8" style="padding:1pt 2.0pt;">Attention SUM</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2" style="padding:1pt 2.0pt;">RGCN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.3" style="padding:1pt 2.0pt;">3</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.4" style="padding:1pt 2.0pt;">64</td>
<td class="ltx_td ltx_nopad ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.1" style="padding:1pt 2.0pt;"><svg height="0" overflow="visible" version="1.1" width="0"><g transform="translate(0,0) scale(1,-1)"><path d="M 0,0 0,0" stroke="#000000" stroke-width="0.4"></path><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject height="1.383700013837" overflow="visible" width="1.383700013837">
<span class="ltx_inline-block" id="S3.T2.1.1.1.1.pic1.1.1">
<span class="ltx_inline-block ltx_align_left" id="S3.T2.1.1.1.1.pic1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.pic1.1.1.1.1"></span>
</span>
</span></foreignobject></g></g><g class="ltx_svg_fog" transform="translate(0,0)"><g transform="translate(0,1.383700013837) scale(1, -1)"><foreignobject height="1.383700013837" overflow="visible" width="1.383700013837">
<span class="ltx_inline-block" id="S3.T2.1.1.1.1.pic1.2.1">
<span class="ltx_inline-block ltx_align_right" id="S3.T2.1.1.1.1.pic1.2.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.pic1.2.1.1.1"></span>
</span>
</span></foreignobject></g></g></g></svg></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.5" style="padding:1pt 2.0pt;">Relation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.6" style="padding:1pt 2.0pt;">Linear Transformation</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.7" style="padding:1pt 2.0pt;">MEAN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.1.1.8" style="padding:1pt 2.0pt;">SUM</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.1" style="padding:1pt 2.0pt;">RGAT</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.2" style="padding:1pt 2.0pt;">3</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.3" style="padding:1pt 2.0pt;">64</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.4" style="padding:1pt 2.0pt;">8</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.5" style="padding:1pt 2.0pt;">Relation</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.6" style="padding:1pt 2.0pt;">Linear Transformation</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.7" style="padding:1pt 2.0pt;">Attention SUM</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.1.4.3.8" style="padding:1pt 2.0pt;">SUM</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_subsubsection" id="S3.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">3.1.3. </span>Benchmark Datasets</h4>
<div class="ltx_para" id="S3.SS1.SSS3.p1">
<p class="ltx_p" id="S3.SS1.SSS3.p1.1">We employ four widely-used HetG datasets as benchmark datasets: ACM, IMDB, DBLP, and OGBN-MAG (MAG), as detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S3.T3" title="Table 3 ‣ 3.1.3. Benchmark Datasets ‣ 3.1. Experimental Setup ‣ 3. Characterization Methodology ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">3</span></a>. ACM, DBLP, and MAG are citation datasets, while IMDB represents a movie dataset. These datasets range in size from tens of thousands to tens of millions and billions of edges, providing a comprehensive representation of graph data across diverse real-world application scenarios.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>Information of HetG datasets.</figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T3.1" style="width:346.9pt;height:252.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-36.4pt,26.5pt) scale(0.826600282467593,0.826600282467593) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T3.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1.1" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.1.1">Dataset</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1.2" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.2.1">#Vertex</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1.3" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.3.1">#Feature</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1.4" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.4.1">#Edge of Relations</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S3.T3.1.1.1.1.5" style="padding:1pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1.5.1">#Edge of Metapaths</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1.1" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.2.1.1.1">ACM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1.2" style="padding:1pt 2.0pt;">paper (P): 3025</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1.3" style="padding:1pt 2.0pt;">P:1902</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1.4" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.2.1.4.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.2.1.4.1.1">
<span class="ltx_tr" id="S3.T3.1.1.2.1.4.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.2.1.4.1.1.1.1" style="padding:1pt 2.0pt;">AP: 9936 PA: 9936</span></span>
<span class="ltx_tr" id="S3.T3.1.1.2.1.4.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.2.1.4.1.1.2.1" style="padding:1pt 2.0pt;">PS: 3025 SP: 3025</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.2.1.5" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.2.1.5.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.2.1.5.1.1">
<span class="ltx_tr" id="S3.T3.1.1.2.1.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.2.1.5.1.1.1.1" style="padding:1pt 2.0pt;">PAP: 29436</span></span>
<span class="ltx_tr" id="S3.T3.1.1.2.1.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.2.1.5.1.1.2.1" style="padding:1pt 2.0pt;">PSP: 2200581</span></span>
<span class="ltx_tr" id="S3.T3.1.1.2.1.5.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.2.1.5.1.1.3.1" style="padding:1pt 2.0pt;">PAPSP: 3666289</span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.2.1" style="padding:1pt 2.0pt;">author (A): 5912</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.3.2.2" style="padding:1pt 2.0pt;">A: 1902</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.4.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.3.1" style="padding:1pt 2.0pt;">subject (S): 56</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.4.3.2" style="padding:1pt 2.0pt;">S: 1902</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.5.4">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.4.1" style="padding:1pt 2.0pt;">term (T):1902</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.5.4.2" style="padding:1pt 2.0pt;">T: -</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.6.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.6.5.1" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.6.5.1.1">IMDB</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.5.2" style="padding:1pt 2.0pt;">movie (M): 4278</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.5.3" style="padding:1pt 2.0pt;">M: 3066</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.5.4" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.6.5.4.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.6.5.4.1.1">
<span class="ltx_tr" id="S3.T3.1.1.6.5.4.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.5.4.1.1.1.1" style="padding:1pt 2.0pt;">AM: 12828 DM: 4278</span></span>
<span class="ltx_tr" id="S3.T3.1.1.6.5.4.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.5.4.1.1.2.1" style="padding:1pt 2.0pt;">MA: 12828 MD: 4278</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.6.5.5" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.6.5.5.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.6.5.5.1.1">
<span class="ltx_tr" id="S3.T3.1.1.6.5.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.5.5.1.1.1.1" style="padding:1pt 2.0pt;">MDM: 17446</span></span>
<span class="ltx_tr" id="S3.T3.1.1.6.5.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.5.5.1.1.2.1" style="padding:1pt 2.0pt;">MAM: 85358</span></span>
<span class="ltx_tr" id="S3.T3.1.1.6.5.5.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.6.5.5.1.1.3.1" style="padding:1pt 2.0pt;">MAMDM: 338517</span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.7.6">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.7.6.1" style="padding:1pt 2.0pt;">director (D):2081</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.7.6.2" style="padding:1pt 2.0pt;">D: 2081</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.8.7">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.8.7.1" style="padding:1pt 2.0pt;">actor(A):5257</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.8.7.2" style="padding:1pt 2.0pt;">A: 5257</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.9.8.1" style="padding:1pt 2.0pt;">keyword (K):7971</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.9.8.2" style="padding:1pt 2.0pt;">K: -</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.10.9">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.10.9.1" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.10.9.1.1">DBLP</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.10.9.2" style="padding:1pt 2.0pt;">author (A): 4057</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.10.9.3" style="padding:1pt 2.0pt;">A: 334</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.10.9.4" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.10.9.4.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.10.9.4.1.1">
<span class="ltx_tr" id="S3.T3.1.1.10.9.4.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.10.9.4.1.1.1.1" style="padding:1pt 2.0pt;">AP: 19645 PA: 19645</span></span>
<span class="ltx_tr" id="S3.T3.1.1.10.9.4.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.10.9.4.1.1.2.1" style="padding:1pt 2.0pt;">VP: 14328 PV: 14328</span></span>
<span class="ltx_tr" id="S3.T3.1.1.10.9.4.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.10.9.4.1.1.3.1" style="padding:1pt 2.0pt;">TP: 85810 PT: 85810</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.10.9.5" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.10.9.5.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.10.9.5.1.1">
<span class="ltx_tr" id="S3.T3.1.1.10.9.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.10.9.5.1.1.1.1" style="padding:1pt 2.0pt;">APA: 11113</span></span>
<span class="ltx_tr" id="S3.T3.1.1.10.9.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.10.9.5.1.1.2.1" style="padding:1pt 2.0pt;">APVPA: 5000495</span></span>
<span class="ltx_tr" id="S3.T3.1.1.10.9.5.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.10.9.5.1.1.3.1" style="padding:1pt 2.0pt;">APTPA: 7043571</span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.11.10">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.11.10.1" style="padding:1pt 2.0pt;">paper (P): 14328</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.11.10.2" style="padding:1pt 2.0pt;">P:14328</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.12.11.1" style="padding:1pt 2.0pt;">term (T):7723</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.12.11.2" style="padding:1pt 2.0pt;">T: 7723</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.13.12">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.13.12.1" style="padding:1pt 2.0pt;">venue(V):20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.13.12.2" style="padding:1pt 2.0pt;">V: 20</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.14.13">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T3.1.1.14.13.1" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.14.13.1.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.14.13.1.1.1">
<span class="ltx_tr" id="S3.T3.1.1.14.13.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.14.13.1.1.1.1.1" style="padding:1pt 2.0pt;">OGBN-MAG</span></span>
<span class="ltx_tr" id="S3.T3.1.1.14.13.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.14.13.1.1.1.2.1" style="padding:1pt 2.0pt;">(MAG)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.14.13.2" style="padding:1pt 2.0pt;">author (A):1134649</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.14.13.3" style="padding:1pt 2.0pt;">A: 129</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.1.14.13.4" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.14.13.4.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.14.13.4.1.1">
<span class="ltx_tr" id="S3.T3.1.1.14.13.4.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.14.13.4.1.1.1.1" style="padding:1pt 2.0pt;">AI: 1043998 IA: 1043998</span></span>
<span class="ltx_tr" id="S3.T3.1.1.14.13.4.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.14.13.4.1.1.2.1" style="padding:1pt 2.0pt;">AP: 7145660 PA: 7145660</span></span>
<span class="ltx_tr" id="S3.T3.1.1.14.13.4.1.1.3">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.14.13.4.1.1.3.1" style="padding:1pt 2.0pt;">PF: 7505078 FP: 7505078</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.1.14.13.5" rowspan="4" style="padding:1pt 2.0pt;"><span class="ltx_text" id="S3.T3.1.1.14.13.5.1">
<span class="ltx_tabular ltx_align_middle" id="S3.T3.1.1.14.13.5.1.1">
<span class="ltx_tr" id="S3.T3.1.1.14.13.5.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.14.13.5.1.1.1.1" style="padding:1pt 2.0pt;">PAP: 65933339</span></span>
<span class="ltx_tr" id="S3.T3.1.1.14.13.5.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S3.T3.1.1.14.13.5.1.1.2.1" style="padding:1pt 2.0pt;">PPAP: 614471897</span></span>
</span></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.15.14">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.15.14.1" style="padding:1pt 2.0pt;">paper (P): 736389</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.15.14.2" style="padding:1pt 2.0pt;">P: 14328</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.16.15">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.16.15.1" style="padding:1pt 2.0pt;">feld (F): 59965</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T3.1.1.16.15.2" style="padding:1pt 2.0pt;">T: 7723</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.1.17.16">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.1.17.16.1" style="padding:1pt 2.0pt;">institute (l):8740</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T3.1.1.17.16.2" style="padding:1pt 2.0pt;">V: 20</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Evaluation Methods</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">A fundamental and crucial step in profiling involves delineating the scope of the profiling process. We utilize the CUDA interface provided by PyTorch to initiate and terminate the <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.1">cudaProfiler</span>. Additionally, we employ <span class="ltx_text ltx_font_italic" id="S3.SS2.p1.1.2">NVTX</span> tags to encapsulate the code regions designated for profiling, thereby differentiating between distinct execution phases.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1">Nsight Systems is utilized to provide a holistic analysis of system-level performance, offering insights into the entire application execution process, including both GPU and CPU activities, as well as their interactions with system resources. Meanwhile, Nsight Compute specializes in conducting detailed analysis specifically on the execution of CUDA kernels on the GPU. It offers granular performance metric data at the GPU instruction level, such as execution time, memory access patterns, and instruction efficiency for each CUDA kernel function.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">Due to the GPU initialization processes involved in the initial training epochs, unless otherwise specified, the data presented in this paper are the geometric mean (GM) of results from the 5 epochs after excluding the first 3 epochs, which ensures the accuracy of the obtained results.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Single-GPU Training</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Understanding the execution process on a single GPU is essential to comprehending the execution behavior and characteristics of HGNNs. In this section, we conduct a detailed performance analysis of two principal training methods, full-batch and mini-batch training, executed on a single GPU to identify the execution bottlenecks of HGNNs under different training methods.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Full-batch Training</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Full-batch training enables the model to access the entire graph for thorough parameter updates, leading to accurate and stable gradient estimates; however, it necessitates substantial memory resources. In this section, we present a detailed quantitative analysis of execution time, execution bounds, memory patterns, and instruction issue stalls during full-batch training. Additionally, we examine the influence of variations in metapath properties on execution performance. Due to an Out of Memory (OOM) issue encountered while conducting full-batch training on the MAG dataset with the A100 GPU, this section includes results solely for the three smaller datasets. Furthermore, as the SGB stage is executed only once throughout the entire training process, its associated analysis will be presented in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6.SS1.SSS1" title="6.1.1. Unique Process of Metapath Instance Generation ‣ 6.1. Difference in execution process ‣ 6. Comparison with GNN Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">6.1.1</span></a>.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.1. </span>Execution Time Analysis</h4>
<div class="ltx_para" id="S4.SS1.SSS1.p1">
<p class="ltx_p" id="S4.SS1.SSS1.p1.1">In this section, we provide a comprehensive analysis of the execution time during full-batch HGNN training. Our analysis focuses on identifying the principal execution components from two perspectives encompassing various execution stages and the distinct kernels within each stage.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p2">
<p class="ltx_p" id="S4.SS1.SSS1.p2.2"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p2.2.1">Time Breakdown by Stage.</span> Breaking down the execution time by stages allows us to understand the time proportion of each execution stage, thereby identifying the main execution stages for targeted optimization. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F3" title="Figure 3 ‣ 4.1.1. Execution Time Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">3</span></a> shows the profiling results of the time breakdown by stages. From the overall training perspective,  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS1.p2.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS1.p2.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS1.p2.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS1.p2.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS1.p2.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">1</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p2.2.2">forward propagation is more time-consuming than backward propagation</span>. This phenomenon occurs because, although the backward stage involves the reverse operations of forward propagation, it can directly reuse numerous intermediate results from the forward process. The experimental result in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F3" title="Figure 3 ‣ 4.1.1. Execution Time Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">3</span></a>(a) indicates that forward propagation accounts for an average of 48.33% of the total execution time across different HGNN models and datasets, while backward propagation averages 42.37%. In terms of HGNN execution stages,  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS1.p2.2.pic2" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS1.p2.2.pic2.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS1.p2.2.pic2.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS1.p2.2.pic2.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS1.p2.2.pic2.1.1.1.1.1.2.1" style="font-size:70%;">2</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p2.2.3">the NA stage dominates the most execution time both in forward and backward propagation, which is considered the predominant stage across all the HGNN stages</span>. This is because each edge in every semantic graph requires one execution process during the NA stage, making the load during NA stage the heaviest. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F3" title="Figure 3 ‣ 4.1.1. Execution Time Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">3</span></a>(b) and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F3" title="Figure 3 ‣ 4.1.1. Execution Time Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">3</span></a>(c), the NA stage accounts for an average of 77.80% of the execution time in the forward propagation and 77.53% in the backward propagation.</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="220" id="S4.F3.g1" src="x3.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Time breakdown of HGNN training by phase: (a) The whole training phase; (b) Forward; (c) Backward.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p3">
<p class="ltx_p" id="S4.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS1.p3.1.1">Time Breakdown by Kernel.</span> Analyzing the execution time distribution of CUDA kernels offers valuable insights into the primary kernels utilized during each stage of execution, thereby elucidating their core execution characteristics. We categorize these kernels into four distinct groups based on their specific computational tasks: dense-dense matrix multiplication (DeMM) kernel (DM-Type), topology-based matrix operation kernel (TB-Type), element-wise computation kernel (EW-Type), and data rearrangement kernel (DR-Type), as detailed in our prior work <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib43" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p4">
<p class="ltx_p" id="S4.SS1.SSS1.p4.1">The DM-Type kernels execute dense-dense matrix multiplication (DeMM) tasks, such as <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.1">sgemm</span>, and typically exhibit a regular execution pattern with a high compute-to-memory access ratio. The TB-Type kernels handle computational operations based on the irregular topologies of graphs, exemplified by operations like <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.2">SpMMCsr</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.3">SDDMMCoo</span> (sampled dense-dense matrix multiplication, <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.4">SDDMM</span>), often demonstrating an irregular execution pattern induced by the irregular neighbor connection patterns in graphs. The EW-Type kernels perform element-wise computational operations on sets of vectors or matrices, represented by <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.5">elementwise_kernel</span> (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.6">EleWise</span>), <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.7">matrix_scalar_kernel</span> (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.8">MatScla</span>), and <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.9">reduce_kernel</span> (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.10">Reduce</span>), typically characterized by a low compute-to-memory access ratio. The DR-Type kernels specialize in data rearrangement tasks on matrices, such as <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.11">CatArrayBatchedCopy</span> (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.12">Concat</span>) and <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.13">DeviceRadixSortSingleTileKernel</span> (<span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p4.1.14">Sort</span>), involving a substantial amount of data movement operations.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="209" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>Time breakdown of HGNN training by kernel: (a) Forward; (b) Backward (”NONE” indicates that there are no CUDA kernels invoked here).</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS1.p5">
<p class="ltx_p" id="S4.SS1.SSS1.p5.1">From an overall perspective,  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS1.p5.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS1.p5.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS1.p5.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS1.p5.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS1.p5.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">3</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p5.1.1">the distribution of primary executing kernel types during the forward and backward propagation of the same stage exhibits significant similarity, whereas the time proportion of different kernel types varies</span>. The resemblance in the distribution of kernel types stems from backward propagation being the inverse operation of forward propagation in model computations. The disparities in the distribution of kernel execution times originate from discrepancy in input data and computational loads during backward propagation. For example, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F4" title="Figure 4 ‣ 4.1.1. Execution Time Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4</span></a>(a), in the forward propagation, the FP stage is predominantly occupied by the DM-Type kernel (mainly <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p5.1.2">sgemm</span>), taking up an average of 96.48% of the total kernel execution time across various models and datasets. The EW-Type kernels (mainly <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p5.1.3">EleWise</span>) only occupy a small fraction which is less than 4%. However, during the backward propagation, the proportion of time spent on EW-Type kernels notably escalates, averaging 25.78% of the total kernel execution duration. The underlying cause of this phenomenon lies in the vertex-type-specific nature of the weight matrix employed during the FP stage which will be explained further in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S6.SS1.SSS2" title="6.1.2. Separate Feature Projection ‣ 6.1. Difference in execution process ‣ 6. Comparison with GNN Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">6.1.2</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS1.p6">
<p class="ltx_p" id="S4.SS1.SSS1.p6.2">Note that the RGCN and RGAT models do not invoke any CUDA kernels during the backward propagation of SF stage as in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F4" title="Figure 4 ‣ 4.1.1. Execution Time Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4</span></a>(b). This is due to their adoption of a direct summation aggregation approach in the forward propagation of SF stage. Moreover, in a broader context,  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS1.p6.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS1.p6.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS1.p6.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS1.p6.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS1.p6.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">4</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS1.p6.2.1">addition operations during forward propagation necessitate no computational effort during in the backward pass</span>. This is determined by the nature of the chain rule for gradient propagation, which can be explained by the following formula: <math alttext="Loss=f(C)=f(A+B)\ \rightarrow\frac{\partial L}{\partial B}=\frac{\partial L}{%
\partial A}=\frac{\partial L}{\partial C}" class="ltx_Math" display="inline" id="S4.SS1.SSS1.p6.2.m1.2"><semantics id="S4.SS1.SSS1.p6.2.m1.2a"><mrow id="S4.SS1.SSS1.p6.2.m1.2.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.cmml"><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.3" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.cmml"><mi id="S4.SS1.SSS1.p6.2.m1.2.2.3.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.2.cmml">L</mi><mo id="S4.SS1.SSS1.p6.2.m1.2.2.3.1" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.1.cmml">⁢</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.3.3" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.3.cmml">o</mi><mo id="S4.SS1.SSS1.p6.2.m1.2.2.3.1a" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.1.cmml">⁢</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.3.4" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.4.cmml">s</mi><mo id="S4.SS1.SSS1.p6.2.m1.2.2.3.1b" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.1.cmml">⁢</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.3.5" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.5.cmml">s</mi></mrow><mo id="S4.SS1.SSS1.p6.2.m1.2.2.4" xref="S4.SS1.SSS1.p6.2.m1.2.2.4.cmml">=</mo><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.5" xref="S4.SS1.SSS1.p6.2.m1.2.2.5.cmml"><mi id="S4.SS1.SSS1.p6.2.m1.2.2.5.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.5.2.cmml">f</mi><mo id="S4.SS1.SSS1.p6.2.m1.2.2.5.1" xref="S4.SS1.SSS1.p6.2.m1.2.2.5.1.cmml">⁢</mo><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.5.3.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.5.cmml"><mo id="S4.SS1.SSS1.p6.2.m1.2.2.5.3.2.1" stretchy="false" xref="S4.SS1.SSS1.p6.2.m1.2.2.5.cmml">(</mo><mi id="S4.SS1.SSS1.p6.2.m1.1.1" xref="S4.SS1.SSS1.p6.2.m1.1.1.cmml">C</mi><mo id="S4.SS1.SSS1.p6.2.m1.2.2.5.3.2.2" stretchy="false" xref="S4.SS1.SSS1.p6.2.m1.2.2.5.cmml">)</mo></mrow></mrow><mo id="S4.SS1.SSS1.p6.2.m1.2.2.6" xref="S4.SS1.SSS1.p6.2.m1.2.2.6.cmml">=</mo><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.1" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.cmml"><mi id="S4.SS1.SSS1.p6.2.m1.2.2.1.3" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.3.cmml">f</mi><mo id="S4.SS1.SSS1.p6.2.m1.2.2.1.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.2.cmml">⁢</mo><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.cmml"><mo id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.2" stretchy="false" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.cmml">(</mo><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.cmml"><mi id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.2.cmml">A</mi><mo id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.1" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.1.cmml">+</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.3" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.3.cmml">B</mi></mrow><mo id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.3" rspace="0.500em" stretchy="false" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S4.SS1.SSS1.p6.2.m1.2.2.7" stretchy="false" xref="S4.SS1.SSS1.p6.2.m1.2.2.7.cmml">→</mo><mfrac id="S4.SS1.SSS1.p6.2.m1.2.2.8" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.cmml"><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.8.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.2.cmml"><mo id="S4.SS1.SSS1.p6.2.m1.2.2.8.2.1" rspace="0em" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.2.1.cmml">∂</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.8.2.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.2.2.cmml">L</mi></mrow><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.8.3" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.3.cmml"><mo id="S4.SS1.SSS1.p6.2.m1.2.2.8.3.1" rspace="0em" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.3.1.cmml">∂</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.8.3.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.3.2.cmml">B</mi></mrow></mfrac><mo id="S4.SS1.SSS1.p6.2.m1.2.2.9" xref="S4.SS1.SSS1.p6.2.m1.2.2.9.cmml">=</mo><mfrac id="S4.SS1.SSS1.p6.2.m1.2.2.10" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.cmml"><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.10.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.2.cmml"><mo id="S4.SS1.SSS1.p6.2.m1.2.2.10.2.1" rspace="0em" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.2.1.cmml">∂</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.10.2.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.2.2.cmml">L</mi></mrow><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.10.3" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.3.cmml"><mo id="S4.SS1.SSS1.p6.2.m1.2.2.10.3.1" rspace="0em" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.3.1.cmml">∂</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.10.3.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.3.2.cmml">A</mi></mrow></mfrac><mo id="S4.SS1.SSS1.p6.2.m1.2.2.11" xref="S4.SS1.SSS1.p6.2.m1.2.2.11.cmml">=</mo><mfrac id="S4.SS1.SSS1.p6.2.m1.2.2.12" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.cmml"><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.12.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.2.cmml"><mo id="S4.SS1.SSS1.p6.2.m1.2.2.12.2.1" rspace="0em" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.2.1.cmml">∂</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.12.2.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.2.2.cmml">L</mi></mrow><mrow id="S4.SS1.SSS1.p6.2.m1.2.2.12.3" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.3.cmml"><mo id="S4.SS1.SSS1.p6.2.m1.2.2.12.3.1" rspace="0em" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.3.1.cmml">∂</mo><mi id="S4.SS1.SSS1.p6.2.m1.2.2.12.3.2" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.3.2.cmml">C</mi></mrow></mfrac></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS1.p6.2.m1.2b"><apply id="S4.SS1.SSS1.p6.2.m1.2.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"><and id="S4.SS1.SSS1.p6.2.m1.2.2a.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"></and><apply id="S4.SS1.SSS1.p6.2.m1.2.2b.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"><eq id="S4.SS1.SSS1.p6.2.m1.2.2.4.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.4"></eq><apply id="S4.SS1.SSS1.p6.2.m1.2.2.3.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.3"><times id="S4.SS1.SSS1.p6.2.m1.2.2.3.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.1"></times><ci id="S4.SS1.SSS1.p6.2.m1.2.2.3.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.2">𝐿</ci><ci id="S4.SS1.SSS1.p6.2.m1.2.2.3.3.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.3">𝑜</ci><ci id="S4.SS1.SSS1.p6.2.m1.2.2.3.4.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.4">𝑠</ci><ci id="S4.SS1.SSS1.p6.2.m1.2.2.3.5.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.3.5">𝑠</ci></apply><apply id="S4.SS1.SSS1.p6.2.m1.2.2.5.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.5"><times id="S4.SS1.SSS1.p6.2.m1.2.2.5.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.5.1"></times><ci id="S4.SS1.SSS1.p6.2.m1.2.2.5.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.5.2">𝑓</ci><ci id="S4.SS1.SSS1.p6.2.m1.1.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.1.1">𝐶</ci></apply></apply><apply id="S4.SS1.SSS1.p6.2.m1.2.2c.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"><eq id="S4.SS1.SSS1.p6.2.m1.2.2.6.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.6"></eq><share href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS1.p6.2.m1.2.2.5.cmml" id="S4.SS1.SSS1.p6.2.m1.2.2d.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"></share><apply id="S4.SS1.SSS1.p6.2.m1.2.2.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.1"><times id="S4.SS1.SSS1.p6.2.m1.2.2.1.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.2"></times><ci id="S4.SS1.SSS1.p6.2.m1.2.2.1.3.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.3">𝑓</ci><apply id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1"><plus id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.1"></plus><ci id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.2">𝐴</ci><ci id="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.3.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.1.1.1.1.3">𝐵</ci></apply></apply></apply><apply id="S4.SS1.SSS1.p6.2.m1.2.2e.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"><ci id="S4.SS1.SSS1.p6.2.m1.2.2.7.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.7">→</ci><share href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS1.p6.2.m1.2.2.1.cmml" id="S4.SS1.SSS1.p6.2.m1.2.2f.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"></share><apply id="S4.SS1.SSS1.p6.2.m1.2.2.8.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.8"><divide id="S4.SS1.SSS1.p6.2.m1.2.2.8.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.8"></divide><apply id="S4.SS1.SSS1.p6.2.m1.2.2.8.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.2"><partialdiff id="S4.SS1.SSS1.p6.2.m1.2.2.8.2.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.2.1"></partialdiff><ci id="S4.SS1.SSS1.p6.2.m1.2.2.8.2.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.2.2">𝐿</ci></apply><apply id="S4.SS1.SSS1.p6.2.m1.2.2.8.3.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.3"><partialdiff id="S4.SS1.SSS1.p6.2.m1.2.2.8.3.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.3.1"></partialdiff><ci id="S4.SS1.SSS1.p6.2.m1.2.2.8.3.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.8.3.2">𝐵</ci></apply></apply></apply><apply id="S4.SS1.SSS1.p6.2.m1.2.2g.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"><eq id="S4.SS1.SSS1.p6.2.m1.2.2.9.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.9"></eq><share href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS1.p6.2.m1.2.2.8.cmml" id="S4.SS1.SSS1.p6.2.m1.2.2h.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"></share><apply id="S4.SS1.SSS1.p6.2.m1.2.2.10.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.10"><divide id="S4.SS1.SSS1.p6.2.m1.2.2.10.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.10"></divide><apply id="S4.SS1.SSS1.p6.2.m1.2.2.10.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.2"><partialdiff id="S4.SS1.SSS1.p6.2.m1.2.2.10.2.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.2.1"></partialdiff><ci id="S4.SS1.SSS1.p6.2.m1.2.2.10.2.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.2.2">𝐿</ci></apply><apply id="S4.SS1.SSS1.p6.2.m1.2.2.10.3.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.3"><partialdiff id="S4.SS1.SSS1.p6.2.m1.2.2.10.3.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.3.1"></partialdiff><ci id="S4.SS1.SSS1.p6.2.m1.2.2.10.3.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.10.3.2">𝐴</ci></apply></apply></apply><apply id="S4.SS1.SSS1.p6.2.m1.2.2i.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"><eq id="S4.SS1.SSS1.p6.2.m1.2.2.11.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.11"></eq><share href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS1.p6.2.m1.2.2.10.cmml" id="S4.SS1.SSS1.p6.2.m1.2.2j.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2"></share><apply id="S4.SS1.SSS1.p6.2.m1.2.2.12.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.12"><divide id="S4.SS1.SSS1.p6.2.m1.2.2.12.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.12"></divide><apply id="S4.SS1.SSS1.p6.2.m1.2.2.12.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.2"><partialdiff id="S4.SS1.SSS1.p6.2.m1.2.2.12.2.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.2.1"></partialdiff><ci id="S4.SS1.SSS1.p6.2.m1.2.2.12.2.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.2.2">𝐿</ci></apply><apply id="S4.SS1.SSS1.p6.2.m1.2.2.12.3.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.3"><partialdiff id="S4.SS1.SSS1.p6.2.m1.2.2.12.3.1.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.3.1"></partialdiff><ci id="S4.SS1.SSS1.p6.2.m1.2.2.12.3.2.cmml" xref="S4.SS1.SSS1.p6.2.m1.2.2.12.3.2">𝐶</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS1.p6.2.m1.2c">Loss=f(C)=f(A+B)\ \rightarrow\frac{\partial L}{\partial B}=\frac{\partial L}{%
\partial A}=\frac{\partial L}{\partial C}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS1.p6.2.m1.2d">italic_L italic_o italic_s italic_s = italic_f ( italic_C ) = italic_f ( italic_A + italic_B ) → divide start_ARG ∂ italic_L end_ARG start_ARG ∂ italic_B end_ARG = divide start_ARG ∂ italic_L end_ARG start_ARG ∂ italic_A end_ARG = divide start_ARG ∂ italic_L end_ARG start_ARG ∂ italic_C end_ARG</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.2. </span>Execution Bounds Analysis</h4>
<div class="ltx_para" id="S4.SS1.SSS2.p1">
<p class="ltx_p" id="S4.SS1.SSS2.p1.1">In this section, we conduct a detailed analysis of the performance metrics of CUDA kernels predominantly invoked during each execution stage to identify the hardware resource bounds encountered at various stages.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p2">
<p class="ltx_p" id="S4.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p2.1.1">Forward Propagation Analysis.</span> The forward propagation process constitutes the primary execution phase of HGNN model training. Characterizing and analyzing the execution bounds during this phase is essential for understanding the requirements of various hardware resources throughout the model’s execution. Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.T4" title="Table 4 ‣ 4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4</span></a> presents the performance metrics of the primary kernels at each execution stage during the training of the HAN model on the DBLP dataset. Based on the data in the table, we formulate the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p2.1.2">Roofline</span> model shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F5" title="Figure 5 ‣ 4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">5</span></a>, which elucidates the execution bounds of each stage encountered during both forward and backward propagation.</p>
</div>
<figure class="ltx_table" id="S4.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4. </span>Profiling results of major CUDA kernels on HAN model with DBLP dataset.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T4.1" style="width:429.3pt;height:256.5pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-122.0pt,72.7pt) scale(0.637605166094144,0.637605166094144) ;">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T4.1.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1.1" style="padding:-0.75pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.1.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.1.1.1.1" style="padding:-0.75pt 5.0pt;">Stage</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1.2" style="padding:-0.75pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.2.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.2.1.1.1" style="padding:-0.75pt 5.0pt;">Kernel</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.2.1.2.1" style="padding:-0.75pt 5.0pt;">Name</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1.3" style="padding:-0.75pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.3.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.3.1.1.1" style="padding:-0.75pt 5.0pt;">Kernel</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.3.1.2.1" style="padding:-0.75pt 5.0pt;">Type</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1.4" style="padding:-0.75pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.4.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.4.1.1.1" style="padding:-0.75pt 5.0pt;">Time</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.4.1.2.1" style="padding:-0.75pt 5.0pt;">(%)</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1.5" style="padding:-0.75pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.5.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.5.1.1.1" style="padding:-0.75pt 5.0pt;">Achieved Peak</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.5.1.2.1" style="padding:-0.75pt 5.0pt;">Performance (%)</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1.6" style="padding:-0.75pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.6.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.6.1.1.1" style="padding:-0.75pt 5.0pt;">DRAM BW</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1.1.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.6.1.2.1" style="padding:-0.75pt 5.0pt;">Utilization (%)</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1.7" style="padding:-0.75pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.7.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.7.1.1.1" style="padding:-0.75pt 5.0pt;">Shared Memory BW</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1.1.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.7.1.2.1" style="padding:-0.75pt 5.0pt;">Utilization (%)</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1.8" style="padding:-0.75pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.8.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.8.1.1.1" style="padding:-0.75pt 5.0pt;">L2 Cache</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1.1.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.8.1.2.1" style="padding:-0.75pt 5.0pt;">Hit Rate (%)</td>
</tr>
</table>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.1.1.9" style="padding:-0.75pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T4.1.1.1.1.9.1">
<tr class="ltx_tr" id="S4.T4.1.1.1.1.9.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.9.1.1.1" style="padding:-0.75pt 5.0pt;">Arithmetic</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.1.1.9.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T4.1.1.1.1.9.1.2.1" style="padding:-0.75pt 5.0pt;">Intensity (FLOP/Byte)</td>
</tr>
</table>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.2.2">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="9" id="S4.T4.1.1.2.2.1" style="padding:-0.75pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.2.2.1.1">Feature Projection</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.3.3">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3.3.1" style="padding:-0.75pt 5.0pt;">Forward</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3.3.2" style="padding:-0.75pt 5.0pt;">sgemm</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3.3.3" style="padding:-0.75pt 5.0pt;">DM</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3.3.4" style="padding:-0.75pt 5.0pt;">98.94%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3.3.5" style="padding:-0.75pt 5.0pt;">56.15%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3.3.6" style="padding:-0.75pt 5.0pt;">10.15%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3.3.7" style="padding:-0.75pt 5.0pt;">38.17%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3.3.8" style="padding:-0.75pt 5.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.3.8.1">86.14</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.3.3.9" style="padding:-0.75pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.3.3.9.1">111.75</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.4.4">
<td class="ltx_td ltx_border_t" id="S4.T4.1.1.4.4.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.2" style="padding:-0.75pt 5.0pt;">sgemm</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.3" style="padding:-0.75pt 5.0pt;">DM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.4" style="padding:-0.75pt 5.0pt;">45.30%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.5" style="padding:-0.75pt 5.0pt;">57.26%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.6" style="padding:-0.75pt 5.0pt;">14.14%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.7" style="padding:-0.75pt 5.0pt;">37.16%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.8" style="padding:-0.75pt 5.0pt;">62.43%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.4.4.9" style="padding:-0.75pt 5.0pt;">30.57</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.5.5">
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.1" style="padding:-0.75pt 5.0pt;">Backward</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.2" style="padding:-0.75pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.5.5.2.1">EleWise</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.3" style="padding:-0.75pt 5.0pt;">EW</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.4" style="padding:-0.75pt 5.0pt;">36.76%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.5" style="padding:-0.75pt 5.0pt;">0.51%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.6" style="padding:-0.75pt 5.0pt;">15.88%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.7" style="padding:-0.75pt 5.0pt;">0.80%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.8" style="padding:-0.75pt 5.0pt;">76.25%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.5.5.9" style="padding:-0.75pt 5.0pt;">2.68</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.6.6">
<td class="ltx_td" id="S4.T4.1.1.6.6.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.2" style="padding:-0.75pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.6.6.2.1">Reduce</span></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.3" style="padding:-0.75pt 5.0pt;">EW</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.4" style="padding:-0.75pt 5.0pt;">17.94%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.5" style="padding:-0.75pt 5.0pt;">0.50%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.6" style="padding:-0.75pt 5.0pt;">21.14%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.7" style="padding:-0.75pt 5.0pt;">0.72%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.8" style="padding:-0.75pt 5.0pt;">29.91%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.6.6.9" style="padding:-0.75pt 5.0pt;">0.29</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.7.7">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="9" id="S4.T4.1.1.7.7.1" style="padding:-0.75pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.7.7.1.1">Neighbor Aggregation</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.8.8">
<td class="ltx_td ltx_border_tt" id="S4.T4.1.1.8.8.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.8.8.2" style="padding:-0.75pt 5.0pt;">SpMMCsr</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.8.8.3" style="padding:-0.75pt 5.0pt;">TB</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.8.8.4" style="padding:-0.75pt 5.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.8.8.4.1">84.16</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.8.8.5" style="padding:-0.75pt 5.0pt;">1.00%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.8.8.6" style="padding:-0.75pt 5.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.8.8.6.1">53.11</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.8.8.7" style="padding:-0.75pt 5.0pt;">0.23%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.8.8.8" style="padding:-0.75pt 5.0pt;">63.19%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.8.8.9" style="padding:-0.75pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.8.8.9.1">2.19</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.9.9">
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.1" style="padding:-0.75pt 5.0pt;">Forward</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.2" style="padding:-0.75pt 5.0pt;">SDDMMCoo</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.3" style="padding:-0.75pt 5.0pt;">TB</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.4" style="padding:-0.75pt 5.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.9.9.4.1">8.21</span>%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.5" style="padding:-0.75pt 5.0pt;">2.32%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.6" style="padding:-0.75pt 5.0pt;">32.10%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.7" style="padding:-0.75pt 5.0pt;">1.78%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.8" style="padding:-0.75pt 5.0pt;">71.17%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.9.9.9" style="padding:-0.75pt 5.0pt;">0.74</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.10.10">
<td class="ltx_td" id="S4.T4.1.1.10.10.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.2" style="padding:-0.75pt 5.0pt;">EleWise</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.3" style="padding:-0.75pt 5.0pt;">EW</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.4" style="padding:-0.75pt 5.0pt;">5.42%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.5" style="padding:-0.75pt 5.0pt;">0.89%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.6" style="padding:-0.75pt 5.0pt;">31.60%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.7" style="padding:-0.75pt 5.0pt;">1.30%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.8" style="padding:-0.75pt 5.0pt;">86.54%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.10.10.9" style="padding:-0.75pt 5.0pt;">0.21</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.11.11">
<td class="ltx_td ltx_border_t" id="S4.T4.1.1.11.11.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.11.11.2" style="padding:-0.75pt 5.0pt;">SpMMCsr</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.11.11.3" style="padding:-0.75pt 5.0pt;">TB</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.11.11.4" style="padding:-0.75pt 5.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.11.11.4.1">48.58</span>%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.11.11.5" style="padding:-0.75pt 5.0pt;">7.90%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.11.11.6" style="padding:-0.75pt 5.0pt;">5.79%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.11.11.7" style="padding:-0.75pt 5.0pt;">0.18%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.11.11.8" style="padding:-0.75pt 5.0pt;">59.73%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.11.11.9" style="padding:-0.75pt 5.0pt;">1.90</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.12.12">
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.1" style="padding:-0.75pt 5.0pt;">Backward</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.2" style="padding:-0.75pt 5.0pt;">SDDMMCoo</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.3" style="padding:-0.75pt 5.0pt;">TB</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.4" style="padding:-0.75pt 5.0pt;">
<span class="ltx_text ltx_font_bold" id="S4.T4.1.1.12.12.4.1">46.21</span>%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.5" style="padding:-0.75pt 5.0pt;">2.79%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.6" style="padding:-0.75pt 5.0pt;">27.92%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.7" style="padding:-0.75pt 5.0pt;">10.96%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.8" style="padding:-0.75pt 5.0pt;">70.13%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.12.12.9" style="padding:-0.75pt 5.0pt;">1.52</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.13.13">
<td class="ltx_td" id="S4.T4.1.1.13.13.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.13.13.2" style="padding:-0.75pt 5.0pt;">EleWise</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.13.13.3" style="padding:-0.75pt 5.0pt;">EW</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.13.13.4" style="padding:-0.75pt 5.0pt;">5.04%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.13.13.5" style="padding:-0.75pt 5.0pt;">0.63%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.13.13.6" style="padding:-0.75pt 5.0pt;">30.32%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.13.13.7" style="padding:-0.75pt 5.0pt;">0.95%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.13.13.8" style="padding:-0.75pt 5.0pt;">75.11%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.13.13.9" style="padding:-0.75pt 5.0pt;">1.09</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.14.14">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="9" id="S4.T4.1.1.14.14.1" style="padding:-0.75pt 5.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T4.1.1.14.14.1.1">Semantic Fusion</span></td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.15.15">
<td class="ltx_td ltx_border_tt" id="S4.T4.1.1.15.15.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.15.15.2" style="padding:-0.75pt 5.0pt;">sgemm</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.15.15.3" style="padding:-0.75pt 5.0pt;">DM</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.15.15.4" style="padding:-0.75pt 5.0pt;">46.43%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.15.15.5" style="padding:-0.75pt 5.0pt;">73.64%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.15.15.6" style="padding:-0.75pt 5.0pt;">11.12%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.15.15.7" style="padding:-0.75pt 5.0pt;">24.35%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.15.15.8" style="padding:-0.75pt 5.0pt;">78.94%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T4.1.1.15.15.9" style="padding:-0.75pt 5.0pt;">51.10</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.16.16">
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.1" style="padding:-0.75pt 5.0pt;">Forward</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.2" style="padding:-0.75pt 5.0pt;">EleWise</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.3" style="padding:-0.75pt 5.0pt;">EW</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.4" style="padding:-0.75pt 5.0pt;">21.02%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.5" style="padding:-0.75pt 5.0pt;">3.2%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.6" style="padding:-0.75pt 5.0pt;">26.97%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.7" style="padding:-0.75pt 5.0pt;">1.78%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.8" style="padding:-0.75pt 5.0pt;">75.03%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.16.16.9" style="padding:-0.75pt 5.0pt;">0.97</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.17.17">
<td class="ltx_td" id="S4.T4.1.1.17.17.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.17.17.2" style="padding:-0.75pt 5.0pt;">Concat</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.17.17.3" style="padding:-0.75pt 5.0pt;">DR</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.17.17.4" style="padding:-0.75pt 5.0pt;">14.87%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.17.17.5" style="padding:-0.75pt 5.0pt;">0.00%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.17.17.6" style="padding:-0.75pt 5.0pt;">45.78%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.17.17.7" style="padding:-0.75pt 5.0pt;">0.23%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.17.17.8" style="padding:-0.75pt 5.0pt;">62.96%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.17.17.9" style="padding:-0.75pt 5.0pt;">0.00</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.18.18">
<td class="ltx_td" id="S4.T4.1.1.18.18.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.18.18.2" style="padding:-0.75pt 5.0pt;">Reduce</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.18.18.3" style="padding:-0.75pt 5.0pt;">EW</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.18.18.4" style="padding:-0.75pt 5.0pt;">13.16%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.18.18.5" style="padding:-0.75pt 5.0pt;">1.31%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.18.18.6" style="padding:-0.75pt 5.0pt;">29.14%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.18.18.7" style="padding:-0.75pt 5.0pt;">1.29%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.18.18.8" style="padding:-0.75pt 5.0pt;">54.00%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.18.18.9" style="padding:-0.75pt 5.0pt;">0.40</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.19.19">
<td class="ltx_td ltx_border_t" id="S4.T4.1.1.19.19.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.19.19.2" style="padding:-0.75pt 5.0pt;">sgemm</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.19.19.3" style="padding:-0.75pt 5.0pt;">DM</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.19.19.4" style="padding:-0.75pt 5.0pt;">52.22%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.19.19.5" style="padding:-0.75pt 5.0pt;">61.41%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.19.19.6" style="padding:-0.75pt 5.0pt;">4.33%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.19.19.7" style="padding:-0.75pt 5.0pt;">39.99%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.19.19.8" style="padding:-0.75pt 5.0pt;">91.79%</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T4.1.1.19.19.9" style="padding:-0.75pt 5.0pt;">301.92</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.20.20">
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.20.20.1" style="padding:-0.75pt 5.0pt;">Backward</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.20.20.2" style="padding:-0.75pt 5.0pt;">EleWise</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.20.20.3" style="padding:-0.75pt 5.0pt;">EW</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.20.20.4" style="padding:-0.75pt 5.0pt;">28.72%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.20.20.5" style="padding:-0.75pt 5.0pt;">0.53%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.20.20.6" style="padding:-0.75pt 5.0pt;">21.07%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.20.20.7" style="padding:-0.75pt 5.0pt;">0.63%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.20.20.8" style="padding:-0.75pt 5.0pt;">82.31%</td>
<td class="ltx_td ltx_align_center" id="S4.T4.1.1.20.20.9" style="padding:-0.75pt 5.0pt;">2.57</td>
</tr>
<tr class="ltx_tr" id="S4.T4.1.1.21.21">
<td class="ltx_td ltx_border_bb" id="S4.T4.1.1.21.21.1" style="padding:-0.75pt 5.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.21.21.2" style="padding:-0.75pt 5.0pt;">Reduce</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.21.21.3" style="padding:-0.75pt 5.0pt;">EW</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.21.21.4" style="padding:-0.75pt 5.0pt;">16.39%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.21.21.5" style="padding:-0.75pt 5.0pt;">0.47%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.21.21.6" style="padding:-0.75pt 5.0pt;">14.13%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.21.21.7" style="padding:-0.75pt 5.0pt;">0.94%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.21.21.8" style="padding:-0.75pt 5.0pt;">47.32%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T4.1.1.21.21.9" style="padding:-0.75pt 5.0pt;">0.29</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="254" id="S4.F5.g1" src="x5.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>The roofline model for kernels under single-precision floating-point operations.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS2.p3">
<svg class="ltx_picture" height="14.25" id="S4.SS1.SSS2.p3.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS2.p3.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS2.p3.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS2.p3.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS2.p3.pic1.1.1.1.1.1.2.1" style="font-size:70%;">5</span></span>
</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S4.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p3.1.1">During the forward propagation, a distinct hybrid execution pattern emerges between stages, each characterized by unique execution bounds, resulting in varying demands on hardware resources.</span> This is due to the distinct execution behaviors exhibited by different stages of the HGNN models. To be specific, for the FP stage, the kernels that occupy the majority of execution during forward propagtion is mainly <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p3.1.2">sgemm</span>, which primarily performs DeMM operations. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.T4" title="Table 4 ‣ 4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4</span></a>, due to its regular memory access pattern, it achieves an L2 cache hit rate of 86.14%, while its high arithmetic density of 111.75 FLOP/Byte reflects its elevated computational demand. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F5" title="Figure 5 ‣ 4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">5</span></a>, the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p3.1.3">sgemm</span> kernel during the forward propagation is situated in the compute-bound region. The NA stage primarily involves graph-topology-based and element-wise operations, mainly invoking <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p3.1.4">SpMMCsr</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p3.1.5">SDDMMCoo</span> kernels as presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.T4" title="Table 4 ‣ 4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4</span></a>. Taking the former as an example, its DRAM bandwidth utilization is 53.11%, but its arithmetic density is only 2.19 FLOP/Byte, indicating its high demand for memory resources. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F5" title="Figure 5 ‣ 4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">5</span></a>, it lies within the memory-bound region. In the SF stage, the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p3.1.6">sgemm</span> kernel is initially utilized for computing attention weights, subsequent to which <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p3.1.7">EleWise</span> and <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p3.1.8">Reduce</span> kernels are employed for aggregating features from diverse semantic graphs. According to the aforementioned analysis, this stage demonstrates an initial manifestation of compute-bound behavior, succeeded by memory-bound characteristic as depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F5" title="Figure 5 ‣ 4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS2.p4">
<p class="ltx_p" id="S4.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.SSS2.p4.1.1">Comparison of Backward and Forward.</span> As a crucial component of the training process, backward propagation shares some similarities with but also exhibits significant differences from forward propagation. Contrasting them aids in a deeper understanding of the varying hardware resource demands during HGNN training. Overall,  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS2.p4.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS2.p4.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS2.p4.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS2.p4.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS2.p4.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">6</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p4.1.2">the execution bounds in the backward and forward propagation exhibit similarity, yet the hybrid execution pattern in backward propagation is notably more intricate</span>. This phenomenon arises because, while backward propagation reverses the process of forward propagation, the application of the chain rule in gradient propagation may introduce operations within the same stage that deviate from those encountered in forward propagation. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.T4" title="Table 4 ‣ 4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4</span></a>, in FP-Backward compared to FP-Forward, the proportion of time spent on <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p4.1.3">sgemm</span> decreases from 98.94% to 45.40% , while the time occupied by <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p4.1.4">EleWise</span> kernels facing memory-bound notably increases. In both forward and backward propagation, the NA stage primarily exhibits memory-bound. However, what differs is that in the NA-Backward stage, the proportion of time taken by the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS2.p4.1.5">SDDMMCoo</span> kernel increases from 8.21% to 46.21%, which indicates the matrices corresponding to the operations performed in the NA stage during backward propagation are denser compared to those in the forward process. The execution bounds exhibited by the SF stage in both backward and forward propagation are fundamentally similar.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.3. </span>Memory Pattern Analysis</h4>
<div class="ltx_para" id="S4.SS1.SSS3.p1">
<p class="ltx_p" id="S4.SS1.SSS3.p1.1">In this section, we focus on the memory aspects, encompassing DRAM access, memory footprint, and cache hit rates, to elucidate the memory characteristics of various execution stages during the HGNN training process.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p2">
<p class="ltx_p" id="S4.SS1.SSS3.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS3.p2.1.1">DRAM Access Breakdown.</span> Memory access latency is a critical factor contributing to performance degradation during model execution, and it is associated with high energy consumption. A comprehensive understanding of memory access patterns across various stages of HGNN training facilitates both performance and energy efficiency improvement. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F6" title="Figure 6 ‣ 4.1.3. Memory Pattern Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">6</span></a> illustrates the DRAM access breakdown of various models during the training process of HGNN on different datasets. From a training perspective,  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS3.p2.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS3.p2.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS3.p2.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS3.p2.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS3.p2.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">7</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p2.1.2">the backward propagation generally incurs more memory access compared to the forward propagation</span>. This phenomenon arises due to the reason that during the backward propagation, the model must access gradient data that matches the size of the dataset accessed in forward propagation. Additionally, the computation of gradients requires the accesses of numerous intermediate results preserved from the forward propagation. As presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F6" title="Figure 6 ‣ 4.1.3. Memory Pattern Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">6</span></a> (a), the backward propagation accounts for 55.38% of the total memory access during the entire training process, while the forward propagation averages only 42.87%.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="193" id="S4.F6.g1" src="x6.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>DRAM access breakdown of HGNN training by phase: (a) Training; (b) Forward; (c) Backward.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS3.p3">
<p class="ltx_p" id="S4.SS1.SSS3.p3.1">Looking specifically at the internal stages of the forward and backward propagation, there are significant differences in memory access distribution among different models. However, from the perspective of the same execution workload (same model and dataset),  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS3.p3.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS3.p3.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS3.p3.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS3.p3.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS3.p3.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">8</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p3.1.1">the stages dominating memory accesses during both backward and forward propagation remain consistent, with the proportions of memory accesses in corresponding stages between forward and backward propagation exhibiting notable similarity</span>. This phenomenon still stems from the fact that backward propagation largely mirrors the computation flow of forward propagation in reverse for most operations. Specifically, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F6" title="Figure 6 ‣ 4.1.3. Memory Pattern Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">6</span></a> (b) and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F6" title="Figure 6 ‣ 4.1.3. Memory Pattern Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">6</span></a>(c), in both forward and backward propagation, the HAN model primarily incurs memory accesses during the NA stage across various datasets, whereas for the RGCN and RGAT models, the primary memory accesses originate from the FP stage.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p4">
<p class="ltx_p" id="S4.SS1.SSS3.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS3.p4.1.1">Memory Footprint Analysis.</span> As the scale of real-world graph data continues to grow, memory utilization emerges as a pivotal factor influencing the scalability of models applied to large-scale datasets. Conducting an analysis of the memory footprint facilitates the optimization of model scalability. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F7" title="Figure 7 ‣ 4.1.3. Memory Pattern Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">7</span></a>(a) illustrates the memory allocation in each execution stage during the training process of HGNN along the timeline. For the sake of convenience, only the specific cases of the HAN model on three different datasets are shown here, while the situations for the other two models are similar. Here, we compare the situations between the pure inference stage and the complete training stage to highlight the more urgent memory demands during the training process.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS3.p5">
<svg class="ltx_picture" height="14.25" id="S4.SS1.SSS3.p5.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS3.p5.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS3.p5.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS3.p5.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS3.p5.pic1.1.1.1.1.1.2.1" style="font-size:70%;">9</span></span>
</span></foreignobject></g></g></svg>
<p class="ltx_p" id="S4.SS1.SSS3.p5.1"><span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p5.1.1">In comparison to pure inference, the training process necessitates greater memory footprint, with the majority memory allocation lies on the NA stage</span>. This distinction arises from inference’s singular execution of a forward propagation, which does not involve gradient computation and thereby obviates the storage of intermediate computational results essential for training during the forward propagation. Furthermore, the inference process does not require backward propagation, thus eliminating the associated memory allocation needed during training. And given that the numerous intermediate computation results requiring storage are associated with edges, the NA stage necessitates the most substantial memory allocation. Considering the case of DBLP dataset in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F7" title="Figure 7 ‣ 4.1.3. Memory Pattern Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">7</span></a>(a), in comparison to the inference process, the total memory allocation during training increases by 1.92<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.SSS3.p5.1.m1.1"><semantics id="S4.SS1.SSS3.p5.1.m1.1a"><mo id="S4.SS1.SSS3.p5.1.m1.1.1" xref="S4.SS1.SSS3.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS3.p5.1.m1.1b"><times id="S4.SS1.SSS3.p5.1.m1.1.1.cmml" xref="S4.SS1.SSS3.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS3.p5.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS3.p5.1.m1.1d">×</annotation></semantics></math>, with the NA stage constituting 66.63% of the total memory consumption.</p>
</div>
<figure class="ltx_figure" id="S4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="241" id="S4.F7.g1" src="x7.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Memory-related profiling results of HGNN Training: (a) Memory Footprint; (b) Cache Hit Rate.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS3.p6">
<p class="ltx_p" id="S4.SS1.SSS3.p6.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS3.p6.1.1">Cache Hit Rate Analysis.</span> The cache hit rate effectively reflects data locality, thereby serving as a starting point for optimizing models from this perspective. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F7" title="Figure 7 ‣ 4.1.3. Memory Pattern Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">7</span></a>(b) illustrates the differences in cache hit rates for forward and backward propagation across various stages of execution on different models of the HAN model. As shown in the figure, in nearly all scenarios,  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS3.p6.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS3.p6.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS3.p6.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS3.p6.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS3.p6.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">10</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS3.p6.1.2">the data locality during backward propagation tends to be lower compared to that observed during forward propagation</span>. This discrepancy primarily arises because backward propagation involves not only accessing graph data but also retrieving stored gradient information and various intermediate variables, necessitating a broader range of memory accesses and leading to diminished data locality. For the FP, NA, and SF stages, the L2 cache hit rate during backward propagation is on average 23.71%, 3.57%, and 4.34% lower than during forward propagation, respectively.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.4. </span>Issue Stall Analysis</h4>
<div class="ltx_para" id="S4.SS1.SSS4.p1">
<p class="ltx_p" id="S4.SS1.SSS4.p1.1">This section provides a detailed analysis of GPU instruction issue stalls encountered during the training process. The focus is on elucidating the temporal distribution and underlying causes of these stalls at each stage. Analyzing instruction issue stalls can reveal operational bottlenecks from an alternative perspective, thereby offering pathways for targeted optimization strategies.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS4.p2">
<p class="ltx_p" id="S4.SS1.SSS4.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS4.p2.1.1">Overall Profiling Results.</span> Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F8" title="Figure 8 ‣ 4.1.4. Issue Stall Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">8</span></a>(a) presents the ratio of stall time to execution time for different execution stages of the training process. From a comprehensive viewpoint, the prevalence of instruction issue stall across different stages throughout the training process is notable and merits careful consideration, averaging 33.21% across different stages. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F8" title="Figure 8 ‣ 4.1.4. Issue Stall Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">8</span></a>(c) illustrates the breakdown of instruction issue stall reasons for the main CUDA kernels invoked at each execution stage. It can be observed that  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS4.p2.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS4.p2.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS4.p2.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS4.p2.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS4.p2.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">11</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.2">the vast majority of kernel stalls originate from memory dependency except for sgemm kernel, and the proportion of stall time attributable to memory dependency during the NA stage is the highest among all execution stages</span>. This is because, during the training process of HGNN, other than <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.3">sgemm</span>, the kernels involve a large number of reads and writes to irregular graph data. These access patterns are usually highly irregular, making it difficult for the accessed data to be fully cached. Therefore, stalls caused by memory dependencies dominate. Furthermore, the NA stage, which predominantly accesses irregular graph data during aggregation, thereby resulting in the most significant instruction issue stalls attributable to memory dependency. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F8" title="Figure 8 ‣ 4.1.4. Issue Stall Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">8</span></a>(c), for kernels other than <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.4">sgemm</span>, memory dependency accounts for an average of 74.11% of the instruction issue stalls. In contrast, for the <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p2.1.5">sgemm</span> kernel, memory dependency accounts for an average of only 15.51%, with the dominant stalls being caused by execution dependency, which average 18.86%. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F8" title="Figure 8 ‣ 4.1.4. Issue Stall Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">8</span></a>(b) illustrates that the NA stage exhibits the highest ratio of stalls caused by memory dependency relative to total elapsed time across all execution stages during both forward and backward propagation.</p>
</div>
<figure class="ltx_figure" id="S4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="270" id="S4.F8.g1" src="x8.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>Issue stall of HAN model on DBLP dataset: (a) The ratio of stall time to elapsed time.; (b) Memory dependency stall ratio to elapsed time; (c) Breakdown of issue stall reasons of different stages.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS4.p3">
<p class="ltx_p" id="S4.SS1.SSS4.p3.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS4.p3.2.1">Comparison of Backward and Forward.</span> Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F8" title="Figure 8 ‣ 4.1.4. Issue Stall Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">8</span></a>(b) presents the proportion of stalls attributed to memory dependency in relation to the total elapsed time for each execution stage of HGNN training during both forward and backward propagation.  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS4.p3.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS4.p3.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS4.p3.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS4.p3.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS4.p3.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">12</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS4.p3.2.2">The fraction of overall execution time attributed to memory dependency during each stage of backward propagation surpasses that observed in forward propagation</span>. The heightened memory access volume during the backward propagation stage, coupled with increased data type diversity, leads to diminished data locality and reduced cache hit rates, as detailed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS3" title="4.1.3. Memory Pattern Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4.1.3</span></a>. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F8" title="Figure 8 ‣ 4.1.4. Issue Stall Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">8</span></a>(b), the average proportion of memory dependency during each stage of backward propagation is 2.26<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.SSS4.p3.2.m1.1"><semantics id="S4.SS1.SSS4.p3.2.m1.1a"><mo id="S4.SS1.SSS4.p3.2.m1.1.1" xref="S4.SS1.SSS4.p3.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS4.p3.2.m1.1b"><times id="S4.SS1.SSS4.p3.2.m1.1.1.cmml" xref="S4.SS1.SSS4.p3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS4.p3.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS4.p3.2.m1.1d">×</annotation></semantics></math> higher than that during forward propagation.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS1.SSS5">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.1.5. </span>Exploring Metapath Changes</h4>
<div class="ltx_para" id="S4.SS1.SSS5.p1">
<p class="ltx_p" id="S4.SS1.SSS5.p1.1">In this section, we explore the performance impact on each execution stage as the length and number of metapaths vary. As the number of metapaths increases significantly, it can greatly expand the size of the graph data, potentially leading to an OOM issue on a single GPU. Therefore, in the experiments in this section, we use the IMDB dataset as a representative example, focusing on the HAN model. And the performance metrics for each stage covered in this section are obtained by averaging forward and backward propagation.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS5.p2">
<p class="ltx_p" id="S4.SS1.SSS5.p2.2"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS5.p2.2.1">Increase in Length of Metapaths.</span> Exploring the performance changes at different stages due to variations in metapath length is crucial, as longer metapaths can assist models in capturing more complex relationship patterns and context information over greater distances. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F9" title="Figure 9 ‣ 4.1.5. Exploring Metapath Changes ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">9</span></a>(a) shows the execution time of different stages across varying metapath lengths. Obviously,  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS5.p2.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS5.p2.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS5.p2.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS5.p2.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS5.p2.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">13</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS5.p2.2.2">increase in the length of the metapath only significantly increase the execution time of NA stage, while the FP and SF stages are almost unaffected.</span> The rationale behind this lies in the fact that the length increase of the metapath does not alter the vertex types or the number of semantic graphs, thereby maintaining the workloads on the FP and SF stages. However, elongating the metapath results in denser semantic graphs <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib43" title="">2022</a>)</cite>, characterized by a higher volume of edges. This directly translates into increased execution times during the NA stage. As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F9" title="Figure 9 ‣ 4.1.5. Exploring Metapath Changes ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">9</span></a>(a), an increase in metapath length from 3 to 9 correlates with a 4.12<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.SSS5.p2.2.m1.1"><semantics id="S4.SS1.SSS5.p2.2.m1.1a"><mo id="S4.SS1.SSS5.p2.2.m1.1.1" xref="S4.SS1.SSS5.p2.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS5.p2.2.m1.1b"><times id="S4.SS1.SSS5.p2.2.m1.1.1.cmml" xref="S4.SS1.SSS5.p2.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS5.p2.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS5.p2.2.m1.1d">×</annotation></semantics></math> rise in execution time for the NA stage, while the FP and SF stages exhibit negligible change in execution times.</p>
</div>
<figure class="ltx_figure" id="S4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S4.F9.g1" src="x9.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9. </span>Execution time breakdown as metapath changes: (a) Length of metapath increases; (b) Number of metapath increases.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.SSS5.p3">
<p class="ltx_p" id="S4.SS1.SSS5.p3.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS1.SSS5.p3.4.1">Increase in Number of Metapaths.</span> Exploring the impact of varying the number of metapaths on the execution performance of different stages is essential, as a greater number of metapaths can provide the model with richer semantic graph information, thereby enhancing model accuracy. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F9" title="Figure 9 ‣ 4.1.5. Exploring Metapath Changes ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">9</span></a>(b) illustrates the impact of increasing the number of metapaths while keeping the metapath length constant on the performance at various execution stages. According to the figure,  <svg class="ltx_picture" height="14.25" id="S4.SS1.SSS5.p3.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS1.SSS5.p3.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS1.SSS5.p3.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS1.SSS5.p3.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS1.SSS5.p3.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">14</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS1.SSS5.p3.4.2">increase in the number of metapaths leads to increased execution times across all stages of HGNN, with the NA stage showing the most noticeable growth trend</span>. As the number of metapaths increases, so does the number of semantic graphs, resulting in a higher volume of vertices and edges to process. Consequently, each execution stage experiences a proportional increase in workload. Notably, the NA stage shows the most pronounced growth, primarily because the rise in edges outpaces that of nodes and even semantic graphs. As illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F9" title="Figure 9 ‣ 4.1.5. Exploring Metapath Changes ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">9</span></a>(b), expanding the metapath count from 1 to 8 leads to execution time increases of 4.71<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.SSS5.p3.2.m1.1"><semantics id="S4.SS1.SSS5.p3.2.m1.1a"><mo id="S4.SS1.SSS5.p3.2.m1.1.1" xref="S4.SS1.SSS5.p3.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS5.p3.2.m1.1b"><times id="S4.SS1.SSS5.p3.2.m1.1.1.cmml" xref="S4.SS1.SSS5.p3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS5.p3.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS5.p3.2.m1.1d">×</annotation></semantics></math> for FP, 9.34<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.SSS5.p3.3.m2.1"><semantics id="S4.SS1.SSS5.p3.3.m2.1a"><mo id="S4.SS1.SSS5.p3.3.m2.1.1" xref="S4.SS1.SSS5.p3.3.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS5.p3.3.m2.1b"><times id="S4.SS1.SSS5.p3.3.m2.1.1.cmml" xref="S4.SS1.SSS5.p3.3.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS5.p3.3.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS5.p3.3.m2.1d">×</annotation></semantics></math> for NA, and 1.40<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS1.SSS5.p3.4.m3.1"><semantics id="S4.SS1.SSS5.p3.4.m3.1a"><mo id="S4.SS1.SSS5.p3.4.m3.1.1" xref="S4.SS1.SSS5.p3.4.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS5.p3.4.m3.1b"><times id="S4.SS1.SSS5.p3.4.m3.1.1.cmml" xref="S4.SS1.SSS5.p3.4.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS5.p3.4.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS5.p3.4.m3.1d">×</annotation></semantics></math> for SF.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Mini-batch Training</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">As the size of real-world graph datasets continues to expand, mini-batch training has increasingly become the predominant method for model training, which leads to faster convergence and improved generalization performance compared to full-batch training. Aside from the additional mini-batch sampling stage, the execution behavior and characteristics are identical to those in full-batch training. Consequently, this section primarily focuses on profiling the mini-batch sampling stage. Unless otherwise specified, the batch size used for sampling is 256 for ACM, IMDB and DBLP datasets. Due to the large scale of the MAG dataset, its batch size is set to 1024. The sampling method employed is <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">Random Walk Neighbor Sampler</span> <cite class="ltx_cite ltx_citemacro_citep">(Hamilton et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib10" title="">2017</a>)</cite>. Unless specified otherwise, the sampling process is executed on the CPU.</p>
</div>
<section class="ltx_subsubsection" id="S4.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.1. </span>Execution Analysis</h4>
<div class="ltx_para" id="S4.SS2.SSS1.p1">
<p class="ltx_p" id="S4.SS2.SSS1.p1.1">This section provides a detailed analysis of the execution time breakdown during mini-batch training, compares the performance of GPU sampling with that of CPU sampling, and examines various performance metrics of the CUDA kernels employed in GPU sampling to elucidate the characteristics of the sampling operation.</p>
</div>
<figure class="ltx_figure" id="S4.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="227" id="S4.F10.g1" src="x10.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10. </span>Execution analysis: (a) Time breakdown; (b) Speedup of GPU sampling; (c) Speedup on MAG dataset.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p2">
<p class="ltx_p" id="S4.SS2.SSS1.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS2.SSS1.p2.1.1">Execution Time Breakdown.</span> Breaking down the execution time facilitates the identification of the primary execution processes during mini-batch training. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F10" title="Figure 10 ‣ 4.2.1. Execution Analysis ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">10</span></a>(a) shows the breakdown of execution time across various execution stages.  <svg class="ltx_picture" height="14.25" id="S4.SS2.SSS1.p2.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS2.SSS1.p2.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS2.SSS1.p2.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS2.SSS1.p2.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS2.SSS1.p2.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">15</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p2.1.2">Mini-batch sampling occupies the majority of the execution time in each epoch of mini-batch training, even exceeding the combined time of all other execution stages.</span> This is due to mini-batch sampling in HGNN models requiring sampling operations across multiple semantic graphs not just one graph as in conventional GNNs, entailing traversal of intricate and irregular graph structures. Furthermore, this procedure is carried out on CPU, which is markedly time-intensive in contrast to the highly parallelized computational workload executed on GPU. As presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F10" title="Figure 10 ‣ 4.2.1. Execution Analysis ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">10</span></a>(a), mini-batch sampling accounts for an average of 51.01% of the total time across different models and datasets. Especially for the HAN model on the MAG dataset, the time elpsed on sampling in each epoch accounts for as much as 88.92%.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p3">
<p class="ltx_p" id="S4.SS2.SSS1.p3.7"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS2.SSS1.p3.7.1">Performance Comparison of GPU and CPU Sampling.</span> In general, the mini-batch sampling process is categorized as a preprocessing step, managed by the CPU as introduced in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S2" title="2. Background ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">2</span></a>. However, compared to CPU sampling,  <svg class="ltx_picture" height="14.25" id="S4.SS2.SSS1.p3.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS2.SSS1.p3.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS2.SSS1.p3.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS2.SSS1.p3.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS2.SSS1.p3.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">16</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p3.7.2">employing GPUs for sampling yields a substantial enhancement in sampling performance, particularly discernible in the context of large-scale datasets</span>. This is attributed to GPUs offering ample bandwidth resources for graph traversal, coupled with their inherently highly parallel architecture that facilitates simultaneous sampling of multiple target vertices. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F10" title="Figure 10 ‣ 4.2.1. Execution Analysis ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">10</span></a>(b) demonstrates that using GPU for mini-batch sampling directly can achieve an average acceleration of 2.43<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.2.m1.1"><semantics id="S4.SS2.SSS1.p3.2.m1.1a"><mo id="S4.SS2.SSS1.p3.2.m1.1.1" xref="S4.SS2.SSS1.p3.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.2.m1.1b"><times id="S4.SS2.SSS1.p3.2.m1.1.1.cmml" xref="S4.SS2.SSS1.p3.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.2.m1.1d">×</annotation></semantics></math>, 2.06<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.3.m2.1"><semantics id="S4.SS2.SSS1.p3.3.m2.1a"><mo id="S4.SS2.SSS1.p3.3.m2.1.1" xref="S4.SS2.SSS1.p3.3.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.3.m2.1b"><times id="S4.SS2.SSS1.p3.3.m2.1.1.cmml" xref="S4.SS2.SSS1.p3.3.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.3.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.3.m2.1d">×</annotation></semantics></math>, and 1.21<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.4.m3.1"><semantics id="S4.SS2.SSS1.p3.4.m3.1a"><mo id="S4.SS2.SSS1.p3.4.m3.1.1" xref="S4.SS2.SSS1.p3.4.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.4.m3.1b"><times id="S4.SS2.SSS1.p3.4.m3.1.1.cmml" xref="S4.SS2.SSS1.p3.4.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.4.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.4.m3.1d">×</annotation></semantics></math> compared to CPU, under different batch sizes of 64, 128, and 256, respectively. For larger-scale datasets like MAG, GPU sampling achieves an average speedup of 5.78<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.5.m4.1"><semantics id="S4.SS2.SSS1.p3.5.m4.1a"><mo id="S4.SS2.SSS1.p3.5.m4.1.1" xref="S4.SS2.SSS1.p3.5.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.5.m4.1b"><times id="S4.SS2.SSS1.p3.5.m4.1.1.cmml" xref="S4.SS2.SSS1.p3.5.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.5.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.5.m4.1d">×</annotation></semantics></math>, 6.35<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.6.m5.1"><semantics id="S4.SS2.SSS1.p3.6.m5.1a"><mo id="S4.SS2.SSS1.p3.6.m5.1.1" xref="S4.SS2.SSS1.p3.6.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.6.m5.1b"><times id="S4.SS2.SSS1.p3.6.m5.1.1.cmml" xref="S4.SS2.SSS1.p3.6.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.6.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.6.m5.1d">×</annotation></semantics></math>, and 8.62<math alttext="\times" class="ltx_Math" display="inline" id="S4.SS2.SSS1.p3.7.m6.1"><semantics id="S4.SS2.SSS1.p3.7.m6.1a"><mo id="S4.SS2.SSS1.p3.7.m6.1.1" xref="S4.SS2.SSS1.p3.7.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS1.p3.7.m6.1b"><times id="S4.SS2.SSS1.p3.7.m6.1.1.cmml" xref="S4.SS2.SSS1.p3.7.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS1.p3.7.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS1.p3.7.m6.1d">×</annotation></semantics></math> with batch sizes of 512, 1024, and 2048, respectively, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F10" title="Figure 10 ‣ 4.2.1. Execution Analysis ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">10</span></a>(c).</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS1.p4">
<p class="ltx_p" id="S4.SS2.SSS1.p4.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS2.SSS1.p4.1.1">Characteristics Analysis of the Sampling Process.</span> We conduct an in-depth analysis of the mini-batch sampling stage utilizing GPU-based sampling techniques, and present the performance metrics of the primary kernels in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.T5" title="Table 5 ‣ 4.2.1. Execution Analysis ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">5</span></a>. The performance metrics detailed here pertain exclusively to the CUDA kernels specific to the mini-batch sampling stage, distinguishing them from those involved in the main execution stages.</p>
</div>
<figure class="ltx_table" id="S4.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5. </span>Profiling results of mini-batch sampling of HAN model on MAG datasets.</figcaption>
<div class="ltx_inline-block ltx_transformed_outer" id="S4.T5.1" style="width:416.3pt;height:99.6pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-181.3pt,43.2pt) scale(0.534431150510657,0.534431150510657) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T5.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T5.1.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1.1" style="padding:-0.5pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.1.1.1">
<tr class="ltx_tr" id="S4.T5.1.1.1.1.1.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.1.1.1.1" style="padding:-0.5pt 5.0pt;">Kernel</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.1.1.1.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.1.1.2.1" style="padding:-0.5pt 5.0pt;">Name</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1.2" style="padding:-0.5pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.1.2.1">
<tr class="ltx_tr" id="S4.T5.1.1.1.1.2.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.2.1.1.1" style="padding:-0.5pt 5.0pt;">Time</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.1.1.2.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.2.1.2.1" style="padding:-0.5pt 5.0pt;">(%)</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1.3" style="padding:-0.5pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.1.3.1">
<tr class="ltx_tr" id="S4.T5.1.1.1.1.3.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.3.1.1.1" style="padding:-0.5pt 5.0pt;">Achieved Peak</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.1.1.3.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.3.1.2.1" style="padding:-0.5pt 5.0pt;">Performance (%)</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1.4" style="padding:-0.5pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.1.4.1">
<tr class="ltx_tr" id="S4.T5.1.1.1.1.4.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.4.1.1.1" style="padding:-0.5pt 5.0pt;">DRAM BW</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.1.1.4.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.4.1.2.1" style="padding:-0.5pt 5.0pt;">Utilization (%)</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1.5" style="padding:-0.5pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.1.5.1">
<tr class="ltx_tr" id="S4.T5.1.1.1.1.5.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.5.1.1.1" style="padding:-0.5pt 5.0pt;">Shared Memory BW</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.1.1.5.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.5.1.2.1" style="padding:-0.5pt 5.0pt;">Utilization (%)</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1.6" style="padding:-0.5pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.1.6.1">
<tr class="ltx_tr" id="S4.T5.1.1.1.1.6.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.6.1.1.1" style="padding:-0.5pt 5.0pt;">L2 Cache</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.1.1.6.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.6.1.2.1" style="padding:-0.5pt 5.0pt;">Hit Rate (%)</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1.7" style="padding:-0.5pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.1.7.1">
<tr class="ltx_tr" id="S4.T5.1.1.1.1.7.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.7.1.1.1" style="padding:-0.5pt 5.0pt;">Integer</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.1.1.7.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.7.1.2.1" style="padding:-0.5pt 5.0pt;">Instructions (Amount)</td>
</tr>
</table>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T5.1.1.1.1.8" style="padding:-0.5pt 5.0pt;">
<table class="ltx_tabular ltx_align_middle" id="S4.T5.1.1.1.1.8.1">
<tr class="ltx_tr" id="S4.T5.1.1.1.1.8.1.1">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.8.1.1.1" style="padding:-0.5pt 5.0pt;">Float</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.1.1.8.1.2">
<td class="ltx_td ltx_nopad_r ltx_align_center" id="S4.T5.1.1.1.1.8.1.2.1" style="padding:-0.5pt 5.0pt;">Instructions (Amount)</td>
</tr>
</table>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T5.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.2.1.1" style="padding:-0.5pt 5.0pt;">pick_data</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.2.1.2" style="padding:-0.5pt 5.0pt;">11.11%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.2.1.3" style="padding:-0.5pt 5.0pt;">0.21%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.2.1.4" style="padding:-0.5pt 5.0pt;">0.22%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.2.1.5" style="padding:-0.5pt 5.0pt;">0.00%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.2.1.6" style="padding:-0.5pt 5.0pt;">91.28%</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.2.1.7" style="padding:-0.5pt 5.0pt;">793156</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T5.1.1.2.1.8" style="padding:-0.5pt 5.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.3.2">
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.2.1" style="padding:-0.5pt 5.0pt;">RandomWalkKernel</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.2.2" style="padding:-0.5pt 5.0pt;">4.54%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.2.3" style="padding:-0.5pt 5.0pt;">17.26%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.2.4" style="padding:-0.5pt 5.0pt;">34.47%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.2.5" style="padding:-0.5pt 5.0pt;">0.06%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.2.6" style="padding:-0.5pt 5.0pt;">86.92%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.2.7" style="padding:-0.5pt 5.0pt;">901362365</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.3.2.8" style="padding:-0.5pt 5.0pt;">18032000</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.4.3">
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.4.3.1" style="padding:-0.5pt 5.0pt;">count_frequency</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.4.3.2" style="padding:-0.5pt 5.0pt;">36.72%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.4.3.3" style="padding:-0.5pt 5.0pt;">9.16%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.4.3.4" style="padding:-0.5pt 5.0pt;">0.82%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.4.3.5" style="padding:-0.5pt 5.0pt;">0.22%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.4.3.6" style="padding:-0.5pt 5.0pt;">94.64%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.4.3.7" style="padding:-0.5pt 5.0pt;">457740460</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.4.3.8" style="padding:-0.5pt 5.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.5.4">
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.5.4.1" style="padding:-0.5pt 5.0pt;">compact_frequency</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.5.4.2" style="padding:-0.5pt 5.0pt;">1.78%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.5.4.3" style="padding:-0.5pt 5.0pt;">15.57%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.5.4.4" style="padding:-0.5pt 5.0pt;">16.89%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.5.4.5" style="padding:-0.5pt 5.0pt;">8.69%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.5.4.6" style="padding:-0.5pt 5.0pt;">43.61%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.5.4.7" style="padding:-0.5pt 5.0pt;">165723693</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.5.4.8" style="padding:-0.5pt 5.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.6.5">
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.6.5.1" style="padding:-0.5pt 5.0pt;">count_hashmap</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.6.5.2" style="padding:-0.5pt 5.0pt;">0.26%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.6.5.3" style="padding:-0.5pt 5.0pt;">6.92%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.6.5.4" style="padding:-0.5pt 5.0pt;">32.11%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.6.5.5" style="padding:-0.5pt 5.0pt;">1.22%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.6.5.6" style="padding:-0.5pt 5.0pt;">56.42%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.6.5.7" style="padding:-0.5pt 5.0pt;">19747489</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.6.5.8" style="padding:-0.5pt 5.0pt;">593850</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.7.6">
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.7.6.1" style="padding:-0.5pt 5.0pt;">compact_hashmap</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.7.6.2" style="padding:-0.5pt 5.0pt;">0.34%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.7.6.3" style="padding:-0.5pt 5.0pt;">5.98%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.7.6.4" style="padding:-0.5pt 5.0pt;">21.94%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.7.6.5" style="padding:-0.5pt 5.0pt;">2.31%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.7.6.6" style="padding:-0.5pt 5.0pt;">67.7%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.7.6.7" style="padding:-0.5pt 5.0pt;">21827916</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.7.6.8" style="padding:-0.5pt 5.0pt;">476162</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.8.7">
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.8.7.1" style="padding:-0.5pt 5.0pt;">RangeKernel</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.8.7.2" style="padding:-0.5pt 5.0pt;">0.35%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.8.7.3" style="padding:-0.5pt 5.0pt;">11.45%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.8.7.4" style="padding:-0.5pt 5.0pt;">0.00%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.8.7.5" style="padding:-0.5pt 5.0pt;">1.55%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.8.7.6" style="padding:-0.5pt 5.0pt;">100.00%</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.8.7.7" style="padding:-0.5pt 5.0pt;">31911248</td>
<td class="ltx_td ltx_align_center" id="S4.T5.1.1.8.7.8" style="padding:-0.5pt 5.0pt;">0</td>
</tr>
<tr class="ltx_tr" id="S4.T5.1.1.9.8">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.9.8.1" style="padding:-0.5pt 5.0pt;">DeviceSegmentedRadixSortKernel</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.9.8.2" style="padding:-0.5pt 5.0pt;">26.92%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.9.8.3" style="padding:-0.5pt 5.0pt;">29.56%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.9.8.4" style="padding:-0.5pt 5.0pt;">1.43%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.9.8.5" style="padding:-0.5pt 5.0pt;">42.56%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.9.8.6" style="padding:-0.5pt 5.0pt;">66.39%</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.9.8.7" style="padding:-0.5pt 5.0pt;">5815247002</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T5.1.1.9.8.8" style="padding:-0.5pt 5.0pt;">0</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S4.SS2.SSS1.p5">
<p class="ltx_p" id="S4.SS2.SSS1.p5.1">The kernels employed for sampling demonstrate modest computational demands, primarily executing integer instructions for tasks such as vertex ID comparison and indexing. Although the DRAM bandwidth utilization may appear low, this is attributed to the substantial bandwidth provided by the GPU’s HBM memory, which is 2039 GB/s on our platform. For instance, the <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS1.p5.1.1">RandomWalkKernel</span> achieves an absolute bandwidth value of 702.84 GB/s, significantly exceeding the maximum bandwidth attainable with the CPU’s DDR4 memory. The relatively low bandwidth utilization can be attributed to the substantial on-chip cache available on the GPU, which is capable of storing a significant portion of the graph data during the sampling process. This is particularly reflected in the high L2 Cache hit rate, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.T5" title="Table 5 ‣ 4.2.1. Execution Analysis ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">5</span></a>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.2.2. </span>Exploring Metapath Changes</h4>
<div class="ltx_para" id="S4.SS2.SSS2.p1">
<p class="ltx_p" id="S4.SS2.SSS2.p1.1">In this section, we explore the impact of changes in metapath properties on the efficiency of mini-batch sampling, considering both the length and number of metapaths. We conduct experiments using two datasets, DBLP and MAG, to reflect the varying impacts of metapath property changes across different dataset scales.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS2.p2">
<p class="ltx_p" id="S4.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS2.SSS2.p2.1.1">Increase in Length of Metapath.</span> Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F11" title="Figure 11 ‣ 4.2.2. Exploring Metapath Changes ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">11</span></a>(a) and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F11" title="Figure 11 ‣ 4.2.2. Exploring Metapath Changes ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">11</span></a>(b) illustrate the impact on the execution time of mini-batch sampling as metapath length varies.  <svg class="ltx_picture" height="14.25" id="S4.SS2.SSS2.p2.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS2.SSS2.p2.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS2.SSS2.p2.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS2.SSS2.p2.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS2.SSS2.p2.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">17</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p2.1.2">For smaller datasets, the time required for mini-batch sampling exhibits minimal sensitivity to variations in metapath length. Conversely, large-scale datasets experience prolonged execution times as metapath length increases.</span> Increasing the length of metapath augments the number of edges within the semantic graph, consequently elevating the number of edges sampled per batch. However, thread-level parallelism is utilized during sampling process. The additional sampling workload, facilitated by adequate hardware resources, does not significantly affect overall performance. Notably, sampling time increases only when the workload surpasses the hardware’s maximum parallel processing capacity. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F11" title="Figure 11 ‣ 4.2.2. Exploring Metapath Changes ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">11</span></a>(a), compared to a metapath length of 3, the sampling time ratio remains around 1 as the metapath length increases on DBLP dataset. However, as depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F11" title="Figure 11 ‣ 4.2.2. Exploring Metapath Changes ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">11</span></a>(b), for larger datasets MAG, as the metapath length increases, the number of sampled edges in each batch grows more significantly, and the mini-batch sampling time also increases accordingly.</p>
</div>
<figure class="ltx_figure" id="S4.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="188" id="S4.F11.g1" src="x11.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11. </span>Treands as metapath changes: (a) DBLP dataset; (b) MAG dataset; (c) DBLP dataset; (d) MAG dataset.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.SSS2.p3">
<p class="ltx_p" id="S4.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.SS2.SSS2.p3.1.1">Increase in Number of Metapaths.</span> Increasing the number of metapaths directly leads to an increase in the number of semantic graphs. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F11" title="Figure 11 ‣ 4.2.2. Exploring Metapath Changes ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">11</span></a>(c) and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F11" title="Figure 11 ‣ 4.2.2. Exploring Metapath Changes ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">11</span></a>(d) illustrate the variation in mini-batch sampling time with an increasing number of metapaths.  <svg class="ltx_picture" height="14.25" id="S4.SS2.SSS2.p3.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S4.SS2.SSS2.p3.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S4.SS2.SSS2.p3.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S4.SS2.SSS2.p3.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S4.SS2.SSS2.p3.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">18</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS2.p3.1.2">For both small-scale and large-scale datasets, the sampling time of mini-batches is sensitive to changes in the number of metapaths.</span> The increase in the number of semantic graphs due to the growth in metapath number results in an increased sampling load. However, unlike the increase in sampling load caused by denser semantic graphs due to longer metapaths discussed earlier, the sampling process between semantic graphs can only proceed sequentially and is difficult to parallelize within existing programming frameworks. Therefore, even for small datasets, there is a noticeable increasing trend in sampling time as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F11" title="Figure 11 ‣ 4.2.2. Exploring Metapath Changes ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">11</span></a>(c) and Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F11" title="Figure 11 ‣ 4.2.2. Exploring Metapath Changes ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">11</span></a>(d).</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Multi-GPU Training</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Distributed training, leveraging multiple GPUs for model training, is driven by the increasing complexity of real-world datasets and models, which can be implemented using either full-batch or mini-batch methods. In comparison to full-batch distributed training, mini-batch distributed training can markedly reduce the convergence time of the training process while preserving model accuracy. Furthermore, it demands fewer hardware resources, rendering it superior in terms of both performance and energy efficiency. Consequently, we focuse exclusively on the analysis of the mini-batch distributed training method in this section.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Overall Profiling Results</h3>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Performance Comparison with Different Number of GPUs</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.7">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.F12" title="Figure 12 ‣ 5.1.1. Performance Comparison with Different Number of GPUs ‣ 5.1. Overall Profiling Results ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">12</span></a>(a) illustrates the normalized execution time of distributed training using multiple GPUs compared to training on a single GPU. As depicted in the figure,  <svg class="ltx_picture" height="14.25" id="S5.SS1.SSS1.p1.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S5.SS1.SSS1.p1.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S5.SS1.SSS1.p1.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S5.SS1.SSS1.p1.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S5.SS1.SSS1.p1.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">19</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS1.p1.7.1">the performance improvement ratio in multi-GPU distributed training scenarios diverges from the ideal ratio.</span> For smaller-scale datasets such as DBLP, increasing the number of devices may even result in a significant decline in overall training performance. This is mainly because an increase in the number of devices leads to competition for shared resources such as cache and bandwidth, thereby reducing performance. Relevant analysis will be provided in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.SS2" title="5.2. In-depth Analysis ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">5.2</span></a>. As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.F12" title="Figure 12 ‣ 5.1.1. Performance Comparison with Different Number of GPUs ‣ 5.1. Overall Profiling Results ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">12</span></a>(a), the overall execution time of two-GPUs and four-GPUs training are an average 1.37<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.2.m1.1"><semantics id="S5.SS1.SSS1.p1.2.m1.1a"><mo id="S5.SS1.SSS1.p1.2.m1.1.1" xref="S5.SS1.SSS1.p1.2.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.2.m1.1b"><times id="S5.SS1.SSS1.p1.2.m1.1.1.cmml" xref="S5.SS1.SSS1.p1.2.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.2.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.2.m1.1d">×</annotation></semantics></math> and 8.69<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.3.m2.1"><semantics id="S5.SS1.SSS1.p1.3.m2.1a"><mo id="S5.SS1.SSS1.p1.3.m2.1.1" xref="S5.SS1.SSS1.p1.3.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.3.m2.1b"><times id="S5.SS1.SSS1.p1.3.m2.1.1.cmml" xref="S5.SS1.SSS1.p1.3.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.3.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.3.m2.1d">×</annotation></semantics></math> compared to training on a single GPU for DBLP dataset. While linear improvements are evident in larger-scale datasets like MAG, scenarios involving two GPUs and four GPUs yield average performance enhancements of merely 1.66<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.4.m3.1"><semantics id="S5.SS1.SSS1.p1.4.m3.1a"><mo id="S5.SS1.SSS1.p1.4.m3.1.1" xref="S5.SS1.SSS1.p1.4.m3.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.4.m3.1b"><times id="S5.SS1.SSS1.p1.4.m3.1.1.cmml" xref="S5.SS1.SSS1.p1.4.m3.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.4.m3.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.4.m3.1d">×</annotation></semantics></math> and 2.13<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.5.m4.1"><semantics id="S5.SS1.SSS1.p1.5.m4.1a"><mo id="S5.SS1.SSS1.p1.5.m4.1.1" xref="S5.SS1.SSS1.p1.5.m4.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.5.m4.1b"><times id="S5.SS1.SSS1.p1.5.m4.1.1.cmml" xref="S5.SS1.SSS1.p1.5.m4.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.5.m4.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.5.m4.1d">×</annotation></semantics></math> respectively, falling short of the anticipated 2<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.6.m5.1"><semantics id="S5.SS1.SSS1.p1.6.m5.1a"><mo id="S5.SS1.SSS1.p1.6.m5.1.1" xref="S5.SS1.SSS1.p1.6.m5.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.6.m5.1b"><times id="S5.SS1.SSS1.p1.6.m5.1.1.cmml" xref="S5.SS1.SSS1.p1.6.m5.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.6.m5.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.6.m5.1d">×</annotation></semantics></math> and 4<math alttext="\times" class="ltx_Math" display="inline" id="S5.SS1.SSS1.p1.7.m6.1"><semantics id="S5.SS1.SSS1.p1.7.m6.1a"><mo id="S5.SS1.SSS1.p1.7.m6.1.1" xref="S5.SS1.SSS1.p1.7.m6.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS1.p1.7.m6.1b"><times id="S5.SS1.SSS1.p1.7.m6.1.1.cmml" xref="S5.SS1.SSS1.p1.7.m6.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS1.p1.7.m6.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS1.p1.7.m6.1d">×</annotation></semantics></math> improvements.</p>
</div>
<figure class="ltx_figure" id="S5.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="242" id="S5.F12.g1" src="x12.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12. </span>Overall resutls: (a) Normalized performance compared to trin on one GPU; (b) Execution time breakdown on DBLP dataset.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.2. </span>Execution Time Breakdown</h4>
<div class="ltx_para" id="S5.SS1.SSS2.p1">
<p class="ltx_p" id="S5.SS1.SSS2.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.F12" title="Figure 12 ‣ 5.1.1. Performance Comparison with Different Number of GPUs ‣ 5.1. Overall Profiling Results ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">12</span></a>(b) presents the execution time breakdown for distributed training with different numbers of GPUs. For clarity, we only present results on the MAG dataset. According to the figure,  <svg class="ltx_picture" height="14.25" id="S5.SS1.SSS2.p1.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S5.SS1.SSS2.p1.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S5.SS1.SSS2.p1.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S5.SS1.SSS2.p1.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S5.SS1.SSS2.p1.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">20</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS2.p1.1.1">mini-batch sampling is the predominant execution stage during the distributed training process of HGNN, occupying the vast majority of the execution time.</span> This is consistent with the mini-batch training scenario on a single GPU, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS2.SSS1" title="4.2.1. Execution Analysis ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4.2.1</span></a>. Due to the complexity of HGNN sampling and the inefficiency of sampling on the CPU, mini-batch sampling has become the primary performance bottleneck.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>In-depth Analysis</h3>
<section class="ltx_subsubsection" id="S5.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.1. </span>CPU Resources Competition During Sampling</h4>
<div class="ltx_para" id="S5.SS2.SSS1.p1">
<p class="ltx_p" id="S5.SS2.SSS1.p1.1">To better highlight the decrease in sampling efficiency caused by CPU resource contention, we adopt an adaptive batch size approach here. Specifically, for each epoch, we shuffle the order of target vertices and evenly divide them into <math alttext="num\_dst\_vertices/num\_gpus" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p1.1.m1.1"><semantics id="S5.SS2.SSS1.p1.1.m1.1a"><mrow id="S5.SS2.SSS1.p1.1.m1.1.1" xref="S5.SS2.SSS1.p1.1.m1.1.1.cmml"><mrow id="S5.SS2.SSS1.p1.1.m1.1.1.2" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.cmml"><mrow id="S5.SS2.SSS1.p1.1.m1.1.1.2.2" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.cmml"><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.2" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.2.cmml">n</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.3" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.3.cmml">u</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1a" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.4" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.4.cmml">m</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1b" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.5" mathvariant="normal" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.5.cmml">_</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1c" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.6" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.6.cmml">d</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1d" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.7" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.7.cmml">s</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1e" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.8" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.8.cmml">t</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1f" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.9" mathvariant="normal" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.9.cmml">_</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1g" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.10" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.10.cmml">v</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1h" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.11" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.11.cmml">e</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1i" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.12" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.12.cmml">r</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1j" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.13" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.13.cmml">t</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1k" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.14" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.14.cmml">i</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1l" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.15" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.15.cmml">c</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1m" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.16" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.16.cmml">e</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1n" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.17" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.17.cmml">s</mi></mrow><mo id="S5.SS2.SSS1.p1.1.m1.1.1.2.1" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.1.cmml">/</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.2.3" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.3.cmml">n</mi></mrow><mo id="S5.SS2.SSS1.p1.1.m1.1.1.1" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.3" xref="S5.SS2.SSS1.p1.1.m1.1.1.3.cmml">u</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.1a" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.4" xref="S5.SS2.SSS1.p1.1.m1.1.1.4.cmml">m</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.1b" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.5" mathvariant="normal" xref="S5.SS2.SSS1.p1.1.m1.1.1.5.cmml">_</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.1c" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.6" xref="S5.SS2.SSS1.p1.1.m1.1.1.6.cmml">g</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.1d" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.7" xref="S5.SS2.SSS1.p1.1.m1.1.1.7.cmml">p</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.1e" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.8" xref="S5.SS2.SSS1.p1.1.m1.1.1.8.cmml">u</mi><mo id="S5.SS2.SSS1.p1.1.m1.1.1.1f" xref="S5.SS2.SSS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S5.SS2.SSS1.p1.1.m1.1.1.9" xref="S5.SS2.SSS1.p1.1.m1.1.1.9.cmml">s</mi></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p1.1.m1.1b"><apply id="S5.SS2.SSS1.p1.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1"><times id="S5.SS2.SSS1.p1.1.m1.1.1.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.1"></times><apply id="S5.SS2.SSS1.p1.1.m1.1.1.2.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2"><divide id="S5.SS2.SSS1.p1.1.m1.1.1.2.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.1"></divide><apply id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2"><times id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.1"></times><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.2.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.2">𝑛</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.3.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.3">𝑢</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.4.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.4">𝑚</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.5.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.5">_</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.6.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.6">𝑑</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.7.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.7">𝑠</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.8.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.8">𝑡</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.9.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.9">_</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.10.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.10">𝑣</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.11.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.11">𝑒</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.12.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.12">𝑟</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.13.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.13">𝑡</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.14.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.14">𝑖</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.15.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.15">𝑐</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.16.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.16">𝑒</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.2.17.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.2.17">𝑠</ci></apply><ci id="S5.SS2.SSS1.p1.1.m1.1.1.2.3.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.2.3">𝑛</ci></apply><ci id="S5.SS2.SSS1.p1.1.m1.1.1.3.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.3">𝑢</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.4.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.4">𝑚</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.5.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.5">_</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.6.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.6">𝑔</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.7.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.7">𝑝</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.8.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.8">𝑢</ci><ci id="S5.SS2.SSS1.p1.1.m1.1.1.9.cmml" xref="S5.SS2.SSS1.p1.1.m1.1.1.9">𝑠</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p1.1.m1.1c">num\_dst\_vertices/num\_gpus</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p1.1.m1.1d">italic_n italic_u italic_m _ italic_d italic_s italic_t _ italic_v italic_e italic_r italic_t italic_i italic_c italic_e italic_s / italic_n italic_u italic_m _ italic_g italic_p italic_u italic_s</annotation></semantics></math> groups according to the number of GPUs. Each group is then sampled in parallel, and the complete batch after sampling is sent to a separate GPU for execution. We only present the relevant profiling results on the HAN model, with the results on other two models exhibit similar characteristics.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS1.p2">
<p class="ltx_p" id="S5.SS2.SSS1.p2.3">As shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.F13" title="Figure 13 ‣ 5.2.1. CPU Resources Competition During Sampling ‣ 5.2. In-depth Analysis ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">13</span></a>(a), as the number of devices increases, the average number of edges sampled per batch for each GPU decreases. In theory, the average sampling time should also decrease, but in reality, the sampling time shows an increasing trend. Compared to a single GPU, in scenarios with two GPUs and four GPUs, the average time for mini-batch sampling is 1.29 <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p2.1.m1.1"><semantics id="S5.SS2.SSS1.p2.1.m1.1a"><mo id="S5.SS2.SSS1.p2.1.m1.1.1" xref="S5.SS2.SSS1.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.1.m1.1b"><times id="S5.SS2.SSS1.p2.1.m1.1.1.cmml" xref="S5.SS2.SSS1.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p2.1.m1.1d">×</annotation></semantics></math> and 6.49 <math alttext="\times" class="ltx_Math" display="inline" id="S5.SS2.SSS1.p2.2.m2.1"><semantics id="S5.SS2.SSS1.p2.2.m2.1a"><mo id="S5.SS2.SSS1.p2.2.m2.1.1" xref="S5.SS2.SSS1.p2.2.m2.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S5.SS2.SSS1.p2.2.m2.1b"><times id="S5.SS2.SSS1.p2.2.m2.1.1.cmml" xref="S5.SS2.SSS1.p2.2.m2.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.SSS1.p2.2.m2.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.SSS1.p2.2.m2.1d">×</annotation></semantics></math> that of a single GPU, respectively.  <svg class="ltx_picture" height="14.25" id="S5.SS2.SSS1.p2.3.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S5.SS2.SSS1.p2.3.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S5.SS2.SSS1.p2.3.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S5.SS2.SSS1.p2.3.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S5.SS2.SSS1.p2.3.pic1.1.1.1.1.1.2.1" style="font-size:70%;">21</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS1.p2.3.1">The primary factor limiting multi-process parallel sampling is the competition for shared CPU resources.</span> As in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.F13" title="Figure 13 ‣ 5.2.1. CPU Resources Competition During Sampling ‣ 5.2. In-depth Analysis ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">13</span></a>(b), an increase in the number of context switches as the number of GPU increase indicates that due to the concurrent execution of multiple sampling processes, CPU cores become more finely divided in terms of time slice allocation, leading to increased overhead of context switching. The decrease in LLC hit rate indicates that contention for the shared cache by multiple sampling processes reduces the locality of cache data in each sampling process.</p>
</div>
<figure class="ltx_figure" id="S5.F13"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="210" id="S5.F13.g1" src="x13.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13. </span>CPU resources competition during mini-batch sampling: (a) Average nuumber of edges in one batch and related sampling time; (b) Number of CPU context switches and LLC hit rate.</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.2.2. </span>Bandwidth Competition During Data Loading and Synchronization</h4>
<div class="ltx_para" id="S5.SS2.SSS2.p1">
<p class="ltx_p" id="S5.SS2.SSS2.p1.1">Data loading and gradient synchronization are two critical steps in distributed training.  <svg class="ltx_picture" height="14.25" id="S5.SS2.SSS2.p1.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S5.SS2.SSS2.p1.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S5.SS2.SSS2.p1.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S5.SS2.SSS2.p1.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S5.SS2.SSS2.p1.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">22</span></span>
</span></foreignobject></g></g></svg> <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p1.1.1">The factors limiting the performance of data loading and synchronization in multi-device scenarios are primarily the contention for shared bandwidth.</span></p>
</div>
<div class="ltx_para" id="S5.SS2.SSS2.p2">
<p class="ltx_p" id="S5.SS2.SSS2.p2.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.F14" title="Figure 14 ‣ 5.2.2. Bandwidth Competition During Data Loading and Synchronization ‣ 5.2. In-depth Analysis ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">14</span></a>(a) illustrates the trend of PCIe bus communication bandwidth between the host CPU and GPU devices during distributed training as the number of devices increases. Based on the geometric mean across various model and dataset scenarios, an increase in the number of devices from one to two results in an 8.37% reduction in the average PCIe bus bandwidth connecting the host and devices. This reduction further extends to 22.62% when the number of devices increases to four. The decline in PCIe bandwidth suggests that data transfers between the CPU and GPU, particularly the data loading process, encounter increased latency, consequently diminishing the overall performance of distributed training.</p>
</div>
<figure class="ltx_figure" id="S5.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="217" id="S5.F14.g1" src="x14.png" width="797"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14. </span>Bandwidth competition between devices: (a) Host-Device PCIe bandwidth; (b) Device-Device NVLink bandwidth.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.SSS2.p3">
<p class="ltx_p" id="S5.SS2.SSS2.p3.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.F14" title="Figure 14 ‣ 5.2.2. Bandwidth Competition During Data Loading and Synchronization ‣ 5.2. In-depth Analysis ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">14</span></a>(b) shows the changes in data transfer bandwidth through NVLink between GPUs in distributed training as the number of devices increases. From the geometric mean across different model and dataset scenarios, when the number of devices increases from one to two, the average NVLink bus bandwidth connecting Device-Device decreases by 26.07%, and when it increases to four devices, the average NVLink data transfer bandwidth decreases by 46.03%. This phenomenon occurs because the NVIDIA Collective Communications Library library, utilized for gradient synchronization in this experiment, employs the <span class="ltx_text ltx_font_italic" id="S5.SS2.SSS2.p3.1.1">All-Reduce</span> algorithm to synchronize gradient information across devices. This process is executed concurrently on multiple devices. As the number of devices increases, the communication network’s complexity escalates, resulting in heightened contention for communication bandwidth among the devices.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Comparison with GNN Training</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this section, we mainly discuss the comparison between HGNN training and GNN training, focusing on the differences in their execution stages and the transfer of performance bottlenecks during the training process.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Difference in execution process</h3>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1. </span>Unique Process of Metapath Instance Generation</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1">Given that HGNNs primarily operate on semantic graphs during execution, it is imperative to construct these semantic graphs from the original HetG, which is unique to HGNNs. Our experimental results for the HAN model on different datasets indicate that the execution time of the SGB stage is, on average, 11.46<math alttext="\times" class="ltx_Math" display="inline" id="S6.SS1.SSS1.p1.1.m1.1"><semantics id="S6.SS1.SSS1.p1.1.m1.1a"><mo id="S6.SS1.SSS1.p1.1.m1.1.1" xref="S6.SS1.SSS1.p1.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.SSS1.p1.1.m1.1b"><times id="S6.SS1.SSS1.p1.1.m1.1.1.cmml" xref="S6.SS1.SSS1.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.SSS1.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.SSS1.p1.1.m1.1d">×</annotation></semantics></math> the execution time of one single training epoch. Furthermore, as the length and number of metapaths increase, the execution time of the SGB stage exhibits a marked increase, irrespective of the dataset’s scale. For large-scale HetG datasets, SGB will be a considerably time-consuming stage during in the training process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2. </span>Separate Feature Projection</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1">The raw feature vectors of vertices in HomoGs reside in the same vector space and share identical dimensionality, allowing for joint projection. In contrast, HGNNs utilize separate feature projection matrices for each vertex type or each semantic graph, necessitating the reduction of gradients of the projection weight matrices for the same type of vertices appearing in different semantic graphs. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.T4" title="Table 4 ‣ 4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4</span></a>, during backward propagation in the FP stage, the percentage of time occupied by the <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS2.p1.1.1">sgemm</span> kernel decreased from 98.94% to 45.3%, while the <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS2.p1.1.2">EleWise</span> kernel increased from less than 1% to 36.76%. This shift causes the FP stage in backward propagation to transition from being initially compute-bound to becoming memory-bound, further exacerbating the hybrid execution patterns during the training process.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3. </span>Intricate Two-level Aggregation</h4>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1">GNNs perform a single aggregation step for neighboring vertices within a singular type of relation. In contrast, HGNNs aggregate features from neighbors in each semantic graph generated according to corresponding semantics (relations or metapaths), and then fuse intermediate results of each semantic graph for each vertex. As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.F3" title="Figure 3 ‣ 4.1.1. Execution Time Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">3</span></a>, the incorporation of a secondary aggregation level, denoted as the SF stage, incurs an approximate 15% increase in time overhead per epoch.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Changes in execution bottlenecks</h3>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1. </span>More Intricate Hybrid Execution Pattern</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1">Prior work <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib41" title="">2020</a>)</cite> highlights the presence of hybrid execution pattern in the execution of the typical GCN model. Specifically, the FP stage (<span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p1.1.1">Combination</span> stage in work <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib41" title="">2020</a>)</cite>) exhibits a more regular pattern and demonstrates compute-bound execution modes, whereas the NA (<span class="ltx_text ltx_font_italic" id="S6.SS2.SSS1.p1.1.2">Aggregation</span> stage) stage involves numerous random accesses, resulting in memory-bound behavior. According to the analysis in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS1.SSS2" title="4.1.2. Execution Bounds Analysis ‣ 4.1. Full-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4.1.2</span></a>, compared to GNNs, HGNNs exhibit more pronounced and complex hybrid execution pattern, which stems from their more intricate model structure.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2. </span>Bottleneck Migration in Distributed Training</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1">Prior work <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib17" title="">2022</a>)</cite> demonstrates that the data loading stage dominates the most execution time of GNN distributed training. Conversely, in HGNNs, the mini-batch sampling process incurs markedly higher overhead than data loading and is considered the predominant execution process. This shift arises primarily from the necessity in HGNNs to sample multiple semantic graphs to form a batch, compared to GNNs which sample from a single graph as surveyed in work <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib20" title="">2022</a>; Lv et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib23" title="">2023</a>)</cite>. Moreover, models employing metapath-based graph construction experience significant overhead in neighbor sampling due to the intricate traversal of multi-hop neighbors following each meatapath.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Optimization Guidelines</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this section, leveraging the findings previously outlined, we provide guidences for optimizing HGNN training from both software and hardware perspectives.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Software Perspective</h3>
<section class="ltx_subsubsection" id="S7.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1. </span>Reasonable Overlapping of Phases</h4>
<div class="ltx_para" id="S7.SS1.SSS1.p1">
<p class="ltx_p" id="S7.SS1.SSS1.p1.2">On one hand, a bound-aware kernel fusion method can be proposed to facilitate the overlapping execution of stage with differing bounds. Observations  <svg class="ltx_picture" height="14.25" id="S7.SS1.SSS1.p1.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS1.SSS1.p1.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS1.SSS1.p1.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS1.SSS1.p1.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS1.SSS1.p1.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">5</span></span>
</span></foreignobject></g></g></svg> and  <svg class="ltx_picture" height="14.25" id="S7.SS1.SSS1.p1.2.pic2" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS1.SSS1.p1.2.pic2.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS1.SSS1.p1.2.pic2.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS1.SSS1.p1.2.pic2.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS1.SSS1.p1.2.pic2.1.1.1.1.1.2.1" style="font-size:70%;">6</span></span>
</span></foreignobject></g></g></svg> underscore the presence of intricate hybrid execution patterns during HGNN training. These stages, distinguished by diverse execution bounds, frequently alternate, facilitating overlap execution to harness multiple hardware resources concurrently. This approach enhances overall execution performance by optimizing hardware resource utilization. For example, <span class="ltx_text ltx_font_italic" id="S7.SS1.SSS1.p1.2.1">Graphite</span> <cite class="ltx_cite ltx_citemacro_citep">(Gong et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib8" title="">2022</a>)</cite> adopts similar approach to accelerate GNNs on the CPU.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p2">
<p class="ltx_p" id="S7.SS1.SSS1.p2.2">Moreover, training phases executed on different devices can be overlapped to reduce overall time overhead. Observation  <svg class="ltx_picture" height="14.25" id="S7.SS1.SSS1.p2.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS1.SSS1.p2.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS1.SSS1.p2.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS1.SSS1.p2.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS1.SSS1.p2.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">15</span></span>
</span></foreignobject></g></g></svg> and  <svg class="ltx_picture" height="14.25" id="S7.SS1.SSS1.p2.2.pic2" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS1.SSS1.p2.2.pic2.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS1.SSS1.p2.2.pic2.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS1.SSS1.p2.2.pic2.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS1.SSS1.p2.2.pic2.1.1.1.1.1.2.1" style="font-size:70%;">20</span></span>
</span></foreignobject></g></g></svg> indicate that using a mini-batch-based training method makes the mini-batch sampling process the primary execution stage. Fortunately, there is inherent parallelism between the sampling process executed on the CPU and the workload computation process executed on the GPU. The training paradigm can be adjusted to start the sampling process for the next epoch while performing the computation for the current epoch, like <span class="ltx_text ltx_font_italic" id="S7.SS1.SSS1.p2.2.1">PaGraph</span> <cite class="ltx_cite ltx_citemacro_citep">(Lin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib19" title="">2020</a>)</cite> which overlaps mini-batch sampling with data loading to eliminate sampling overhead during GNN training.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2. </span>Recomputing to Reduce Memory Cost</h4>
<div class="ltx_para" id="S7.SS1.SSS2.p1">
<p class="ltx_p" id="S7.SS1.SSS2.p1.3">Observation  <svg class="ltx_picture" height="14.25" id="S7.SS1.SSS2.p1.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS1.SSS2.p1.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS1.SSS2.p1.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS1.SSS2.p1.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS1.SSS2.p1.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">7</span></span>
</span></foreignobject></g></g></svg> and  <svg class="ltx_picture" height="14.25" id="S7.SS1.SSS2.p1.2.pic2" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS1.SSS2.p1.2.pic2.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS1.SSS2.p1.2.pic2.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS1.SSS2.p1.2.pic2.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS1.SSS2.p1.2.pic2.1.1.1.1.1.2.1" style="font-size:70%;">9</span></span>
</span></foreignobject></g></g></svg> indicate that compared to the inference process alone, the training process requires more memory storage, primarily due to the need for direct memory access to reuse a large number of intermediate results preserved during the forward propagation. However, observation  <svg class="ltx_picture" height="14.25" id="S7.SS1.SSS2.p1.3.pic3" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS1.SSS2.p1.3.pic3.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS1.SSS2.p1.3.pic3.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS1.SSS2.p1.3.pic3.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS1.SSS2.p1.3.pic3.1.1.1.1.1.2.1" style="font-size:70%;">4</span></span>
</span></foreignobject></g></g></svg> suggests that operations involving addition in the forward propagation process incur no computational cost in the backward propagation process. This characteristic of backward propagation, reducing computation while increasing memory usage compared to forward propagation, enables the possibility of recomputing certain intermediate results to conserve memory. Prior work <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib47" title="">2022b</a>)</cite> proposes an evaluation method to strike a reasonable trade-off between recomputation cost and memory overhead, aiming to judiciously recompute certain intermediate variables during backward propagation to reduce storage expenses in GNN training. Additionally, <span class="ltx_text ltx_font_italic" id="S7.SS1.SSS2.p1.3.1">TT-GNN</span> <cite class="ltx_cite ltx_citemacro_citep">(Qu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib25" title="">2023</a>)</cite> also employs a method of recomputing prefix arrays to reduce memory cost during GNN training.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3. </span>Scheduling Based on Semantic Graphs for Data Reuse</h4>
<div class="ltx_para" id="S7.SS1.SSS3.p1">
<p class="ltx_p" id="S7.SS1.SSS3.p1.1">Processing multiple semantic graphs is an important feature of HGNNs compared to GNNs. Determining the optimal execution order of semantic graphs based on the number of shared vertices between different semantic graphs can maximize data reuse between the graphs, thereby reducing off-chip memory accesses. Pioneered by <span class="ltx_text ltx_font_italic" id="S7.SS1.SSS3.p1.1.1">HiHGNN</span> <cite class="ltx_cite ltx_citemacro_citep">(Xue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib39" title="">2024a</a>)</cite>, the concept of semantic graph similarity was introduced to maximize reusable data in HGNN inference processes. This can also be extended and applied to HGNN training, exploring optimal execution orders across multiple layers and epochs to maximize data reuse and reduce off-chip memory accesses. Besides, <span class="ltx_text ltx_font_italic" id="S7.SS1.SSS3.p1.1.2">GDR-HGNN<cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text ltx_font_upright" id="S7.SS1.SSS3.p1.1.2.1.1">(</span>Xue et al<span class="ltx_text">.</span><span class="ltx_text ltx_font_upright" id="S7.SS1.SSS3.p1.1.2.2.2.1.1">, </span><a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib40" title="">2024b</a><span class="ltx_text ltx_font_upright" id="S7.SS1.SSS3.p1.1.2.3.3">)</span></cite></span> utilizes the bipartite nature of semantic graphs to decompose them in order to enhance data locality.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Hardware Acceleration</h3>
<section class="ltx_subsubsection" id="S7.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1. </span>Independent Parallel Neighbor Traversal Unit</h4>
<div class="ltx_para" id="S7.SS2.SSS1.p1">
<p class="ltx_p" id="S7.SS2.SSS1.p1.1">Both the extremely time-consuming mini-batch sampling and SGB stages involve traversing neighbors of target vertices. As indicated by the analysis in Section <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S4.SS2.SSS2" title="4.2.2. Exploring Metapath Changes ‣ 4.2. Mini-batch Training ‣ 4. Single-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">4.2.2</span></a>, this traversal process can be effectively parallelized across various target vertices. However, Section <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#S5.SS2.SSS1" title="5.2.1. CPU Resources Competition During Sampling ‣ 5.2. In-depth Analysis ‣ 5. Multi-GPU Training ‣ Characterizing and Understanding HGNN Training on GPUs"><span class="ltx_text ltx_ref_tag">5.2.1</span></a> and observation  <svg class="ltx_picture" height="14.25" id="S7.SS2.SSS1.p1.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS2.SSS1.p1.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS2.SSS1.p1.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS2.SSS1.p1.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS2.SSS1.p1.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">21</span></span>
</span></foreignobject></g></g></svg> underscore that concurrent sampling processes frequently contend for cache resources, leading to notable performance degradation in neighbor sampling tasks. Consequently, it is advisable for researchers to design specialized neighbor traversal units. Each unit should independently cache relevant neighbor information specific to its assigned target vertex, thereby leveraging sampling parallelism while mitigating cache contention. Moreover, optimization structures tailored for graph processing with irregular memory access patterns such as <span class="ltx_text ltx_font_italic" id="S7.SS2.SSS1.p1.1.1">Graphicinado</span> <cite class="ltx_cite ltx_citemacro_citep">(Ham et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib9" title="">2016</a>)</cite> and <span class="ltx_text ltx_font_italic" id="S7.SS2.SSS1.p1.1.2">GraphDynS</span> <cite class="ltx_cite ltx_citemacro_citep">(Yan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib42" title="">2019</a>)</cite> could also be contemplated for enhancing neighbor traversal efficiency.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2. </span>Unified Reconfigurable Execution Unit</h4>
<div class="ltx_para" id="S7.SS2.SSS2.p1">
<p class="ltx_p" id="S7.SS2.SSS2.p1.2">Observations  <svg class="ltx_picture" height="14.25" id="S7.SS2.SSS2.p1.1.pic1" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS2.SSS2.p1.1.pic1.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS2.SSS2.p1.1.pic1.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS2.SSS2.p1.1.pic1.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS2.SSS2.p1.1.pic1.1.1.1.1.1.2.1" style="font-size:70%;">5</span></span>
</span></foreignobject></g></g></svg> and  <svg class="ltx_picture" height="14.25" id="S7.SS2.SSS2.p1.2.pic2" overflow="visible" version="1.1" width="14.25"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,14.25) matrix(1 0 0 -1 0 0) translate(7.13,0) translate(0,7.13)"><g fill="#000000"><path d="M 6.85 0 C 6.85 3.78 3.78 6.85 0 6.85 C -3.78 6.85 -6.85 3.78 -6.85 0 C -6.85 -3.78 -3.78 -6.85 0 -6.85 C 3.78 -6.85 6.85 -3.78 6.85 0 Z M 0 0"></path></g><g fill="#FFFFFF" stroke="#FFFFFF" transform="matrix(1.0 0.0 0.0 1.0 -4.84 -4.84)"><foreignobject color="#FFFFFF" height="9.69" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="9.69">
<span class="ltx_inline-block ltx_minipage ltx_align_top" id="S7.SS2.SSS2.p1.2.pic2.1.1.1.1.1" style="width:7.0pt;">
<span class="ltx_p" id="S7.SS2.SSS2.p1.2.pic2.1.1.1.1.1.1"></span>
<span class="ltx_p" id="S7.SS2.SSS2.p1.2.pic2.1.1.1.1.1.2"><span class="ltx_text" id="S7.SS2.SSS2.p1.2.pic2.1.1.1.1.1.2.1" style="font-size:70%;">6</span></span>
</span></foreignobject></g></g></svg> underscore the presence of intricate hybrid execution patterns during HGNN training, resulting in varied utilization of hardware resources across different stages and an inability to fully harness the maximum efficiency of the hardware platform. <span class="ltx_text ltx_font_italic" id="S7.SS2.SSS2.p1.2.1">ADE-HGNN</span> <cite class="ltx_cite ltx_citemacro_citep">(Han et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib11" title="">2024</a>)</cite> advocates for employing reconfigurable architectures to streamline HGNN inference execution. Given the complex nature of the training process, unified architectures hold the potential to substantially optimize hardware resource utilization.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.3. </span>Multi-lane Architecture Supporting Semantic-graph-level Parallelism</h4>
<div class="ltx_para" id="S7.SS2.SSS3.p1">
<p class="ltx_p" id="S7.SS2.SSS3.p1.1">Besides enabling data reuse, multiple semantic graphs offer a form of parallelism unique to traditional GNNs, termed semantic graph parallelism. In HGNN execution, the operations within each semantic graph prior to the SF stage are independent of other graphs, inherently possessing parallelism. Researchers can improve training efficiency by devising multi-channel hardware architectures that facilitate the concurrent execution of various semantic graphs like <span class="ltx_text ltx_font_italic" id="S7.SS2.SSS3.p1.1.1">HiHGNN</span> <cite class="ltx_cite ltx_citemacro_citep">(Xue et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2407.11790v3#bib.bib39" title="">2024a</a>)</cite> proposed for HGNN inference acceleration.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>Conclusion</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">The complex and costly training process is crucial for effectively utilizing HGNNs. In this work, we comprehensively analyze different training methods and scenarios using NVIDIA GPU A100 platform, revealing the execution semantics and patterns of the HGNN training process, and uncovering the performance bottlenecks. Additionally, we compare some similarities and differences between HGNN and GNN training. Finally, we provide optimization guidelines from both software and hardware perspectives.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Altaf et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Basmah Altaf, Uchenna Akujuobi, Lu Yu, and Xiangliang Zhang. 2019.

</span>
<span class="ltx_bibblock">Dataset Recommendation via Variational Graph Autoencoder. In <em class="ltx_emph ltx_font_italic" id="bib.bib2.3.1">2019 IEEE International Conference on Data Mining (ICDM)</em>. 11–20.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bing et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Rui Bing, Guan Yuan, Mu Zhu, Fanrong Meng, Huifang Ma, and Shaojie Qiao. 2023.

</span>
<span class="ltx_bibblock">Heterogeneous graph neural networks analysis: a survey of techniques, evaluations and applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">Artificial Intelligence Review</em> 56, 8 (2023), 8003–8042.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Dan Chen, Haiheng He, Hai Jin, et al<span class="ltx_text" id="bib.bib4.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">MetaNMP: Leveraging Cartesian-Like Product to Accelerate HGNNs with Near-Memory Processing. In <em class="ltx_emph ltx_font_italic" id="bib.bib4.4.1">Proceedings of the 50th Annual International Symposium on Computer Architecture</em> (Orlando, FL, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib4.5.2">(ISCA ’23)</em>. Association for Computing Machinery, New York, NY, USA, Article 56, 13 pages.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Shaohua Fan, Chuan Shi, and Xiao Wang. 2018.

</span>
<span class="ltx_bibblock">Abnormal Event Detection via Heterogeneous Information Network Embedding. In <em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</em> (Torino, Italy) <em class="ltx_emph ltx_font_italic" id="bib.bib5.4.2">(CIKM ’18)</em>. Association for Computing Machinery, New York, NY, USA, 1483–1486.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fey and Lenssen (2019)</span>
<span class="ltx_bibblock">
Matthias Fey and Jan E. Lenssen. 2019.

</span>
<span class="ltx_bibblock">Fast Graph Representation Learning with PyTorch Geometric. In <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">ICLR Workshop on Representation Learning on Graphs and Manifolds</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Xinyu Fu, Jiani Zhang, Ziqiao Meng, and Irwin King. 2020.

</span>
<span class="ltx_bibblock">Magnn: Metapath aggregated graph neural network for heterogeneous graph embedding. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">Proceedings of The Web Conference 2020</em>. 2331–2341.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gong et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Zhangxiaowen Gong, Houxiang Ji, Yao Yao, Christopher W. Fletcher, Christopher J. Hughes, and Josep Torrellas. 2022.

</span>
<span class="ltx_bibblock">Graphite: optimizing graph neural networks on CPUs through cooperative software-hardware techniques. In <em class="ltx_emph ltx_font_italic" id="bib.bib8.3.1">Proceedings of the 49th Annual International Symposium on Computer Architecture</em> (New York, New York) <em class="ltx_emph ltx_font_italic" id="bib.bib8.4.2">(ISCA ’22)</em>. Association for Computing Machinery, New York, NY, USA, 916–931.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ham et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
T. J. Ham, L. Wu, N. Sundaram, N. Satish, and M. Martonosi. 2016.

</span>
<span class="ltx_bibblock">Graphicionado: A High-Performance and Energy-Efficient Accelerator for Graph Analytics. In <em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamilton et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017.

</span>
<span class="ltx_bibblock">Inductive representation learning on large graphs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.3.1">Advances in neural information processing systems</em> 30 (2017).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al<span class="ltx_text" id="bib.bib11.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Dengke Han, Meng Wu, Runzhen Xue, Mingyu Yan, Xiaochun Ye, and Dongrui Fan. 2024.

</span>
<span class="ltx_bibblock">ADE-HGNN: Accelerating HGNNs through Attention Disparity Exploitation. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.3.1">Euro-Par 2024: Parallel Processing - 30th International Conference on Parallel and Distributed Computing, Madrid, Spain, August 25 - August 30, 2024, Proceedings</em> <em class="ltx_emph ltx_font_italic" id="bib.bib11.4.2">(Lecture Notes in Computer Science)</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib12.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Shifu Hou, Yanfang Ye, Yangqiu Song, and Melih Abdulhayoglu. 2017.

</span>
<span class="ltx_bibblock">HinDroid: An Intelligent Android Malware Detection System Based on Structured Heterogeneous Information Network. In <em class="ltx_emph ltx_font_italic" id="bib.bib12.3.1">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (Halifax, NS, Canada) <em class="ltx_emph ltx_font_italic" id="bib.bib12.4.2">(KDD ’17)</em>. Association for Computing Machinery, New York, NY, USA, 1507–1515.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3097983.3098026" title="">https://doi.org/10.1145/3097983.3098026</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xin Huang, Jongryool Kim, Bradley Rees, and Chul-Ho Lee. 2022.

</span>
<span class="ltx_bibblock">Characterizing the Efficiency of Graph Neural Network Frameworks with a Magnifying Glass. In <em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">2022 IEEE International Symposium on Workload Characterization (IISWC)</em>. 160–170.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Canghong Jin, Tao Ruan, Dexing Wu, Lei Xu, Tengran Dong, Tianyi Chen, Shuoping Wang, Yi Du, and Minghui Wu. 2021.

</span>
<span class="ltx_bibblock">HetGAT: a heterogeneous graph attention network for freeway traffic speed prediction.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Journal of Ambient Intelligence and Humanized Computing</em> (01 2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1007/s12652-020-02807-0" title="">https://doi.org/10.1007/s12652-020-02807-0</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kim et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Sein Kim, Namkyeong Lee, Junseok Lee, Dongmin Hyun, and Chanyoung Park. 2023.

</span>
<span class="ltx_bibblock">Heterogeneous Graph Learning for Multi-Modal Medical Data Analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em> 37, 4 (Jun. 2023), 5141–5150.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kipf and Welling (2017)</span>
<span class="ltx_bibblock">
Thomas N. Kipf and Max Welling. 2017.

</span>
<span class="ltx_bibblock">Semi-Supervised Classification with Graph Convolutional Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">International Conference on Learning Representations, ICLR 2017</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Haiyang Lin, Mingyu Yan, Xiaocheng Yang, et al<span class="ltx_text" id="bib.bib17.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Characterizing and Understanding Distributed GNN Training on GPUs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.4.1">IEEE Computer Architecture Letters</em> 21, 1 (2022), 21–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Haiyang Lin, Mingyu Yan, Xiaochun Ye, Dongrui Fan, Shirui Pan, Wenguang Chen, and Yuan Xie. 2023.

</span>
<span class="ltx_bibblock">A Comprehensive Survey on Distributed Training of Graph Neural Networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.3.1">Proc. IEEE</em> 111, 12 (2023), 1572–1606.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhiqi Lin, Cheng Li, Youshan Miao, Yunxin Liu, and Yinlong Xu. 2020.

</span>
<span class="ltx_bibblock">PaGraph: Scaling GNN training on large graphs via computation-aware caching. In <em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">Proceedings of the 11th ACM Symposium on Cloud Computing</em> (Virtual Event, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib19.4.2">(SoCC ’20)</em>. Association for Computing Machinery, New York, NY, USA, 401–415.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Xin Liu, Mingyu Yan, Lei Deng, Guoqi Li, Xiaochun Ye, and Dongrui Fan. 2022.

</span>
<span class="ltx_bibblock">Sampling Methods for Efficient Training of Graph Convolutional Networks: A Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">IEEE/CAA Journal of Automatica Sinica</em> 9, 2 (2022), 205–234.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Feng Luo, Yue Zhang, and Xiaoli Wang. 2021.

</span>
<span class="ltx_bibblock">IMAS++ An Intelligent Medical Analysis System Enhanced with Deep Graph Neural Networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</em>. 4754–4758.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Qingsong Lv, Ming Ding, Qiang Liu, et al<span class="ltx_text" id="bib.bib22.3.1">.</span> 2021.

</span>
<span class="ltx_bibblock">Are we really making much progress? Revisiting, benchmarking and refining heterogeneous graph neural networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib22.4.1">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</em>. 1150–1160.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lv et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhengyang Lv, Mingyu Yan, Xin Liu, Mengyao Dong, Xiaochun Ye, Dongrui Fan, and Ninghui Sun. 2023.

</span>
<span class="ltx_bibblock">A Survey of Graph Pre-processing Methods: From Algorithmic to Hardware Perspectives.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2309.07581 [cs.AR]

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nazzal et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Mahmoud Nazzal, Abdallah Khreishah, Joyoung Lee, Shaahin Angizi, Ala Al-Fuqaha, and Mohsen Guizani. 2024.

</span>
<span class="ltx_bibblock">Semi-decentralized Inference in Heterogeneous Graph Neural Networks for Traffic Demand Forecasting: An Edge-Computing Approach.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">IEEE Transactions on Vehicular Technology</em> (2024), 1–16.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qu et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zheng Qu, Dimin Niu, Shuangchen Li, Hongzhong Zheng, and Yuan Xie. 2023.

</span>
<span class="ltx_bibblock">TT-GNN: Efficient On-Chip Graph Neural Network Training via Embedding Reformation and Hardware Optimization. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture</em> (Toronto, ON, Canada) <em class="ltx_emph ltx_font_italic" id="bib.bib25.4.2">(MICRO ’23)</em>. Association for Computing Machinery, New York, NY, USA, 452–464.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schlichtkrull et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne van den Berg, Ivan Titov, and Max Welling. 2018.

</span>
<span class="ltx_bibblock">Modeling relational data with graph convolutional networks. In <em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">European semantic web conference</em>. Springer, 593–607.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shi et al<span class="ltx_text" id="bib.bib27.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Chuan Shi, Yitong Li, Jiawei Zhang, Yizhou Sun, and S Yu Philip. 2016.

</span>
<span class="ltx_bibblock">A survey of heterogeneous information network analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.3.1">IEEE Transactions on Knowledge and Data Engineering</em> 29, 1 (2016), 17–37.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Thorpe et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
John Thorpe, Yifan Qiao, Jon Eyolfson, Shen Teng, Guanzhou Hu, Zhihao Jia, Jinliang Wei, Keval Vora, Ravi Netravali, Miryung Kim, and Guoqing Harry Xu. 2021.

</span>
<span class="ltx_bibblock">Dorylus: Affordable, Scalable, and Accurate GNN Training with Distributed CPU Servers and Serverless Threads. In <em class="ltx_emph ltx_font_italic" id="bib.bib28.3.1">USENIX Symposium on Operating Systems Design and Implementation</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Veličković et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Petar Veličković, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua Bengio. 2018.

</span>
<span class="ltx_bibblock">Graph Attention Networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">International Conference on Learning Representations, ICLR 2018</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, and Rui Wang. 2020b.

</span>
<span class="ltx_bibblock">Relational Graph Attention Network for Aspect-based Sentiment Analysis. In <em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>. 3229–3238.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2019b)</span>
<span class="ltx_bibblock">
Minjie Wang, Lingfan Yu, Zheng Da, Gan Quan, Gai Yu, Ye Zihao, et al<span class="ltx_text" id="bib.bib31.3.1">.</span> 2019b.

</span>
<span class="ltx_bibblock">Deep graph library: Towards efficient and scalable deep learning on graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.4.1">ICLR</em>.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Xiao Wang, Deyu Bo, Chuan Shi, Shaohua Fan, Yanfang Ye, and Philip S Yu. 2020a.

</span>
<span class="ltx_bibblock">A survey on heterogeneous graph embedding: methods, techniques, applications and sources.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">arXiv preprint arXiv:2011.14867</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2019a)</span>
<span class="ltx_bibblock">
Xiao Wang, Houye Ji, Chuan Shi, et al<span class="ltx_text" id="bib.bib33.3.1">.</span> 2019a.

</span>
<span class="ltx_bibblock">Heterogeneous graph attention network. In <em class="ltx_emph ltx_font_italic" id="bib.bib33.4.1">The world wide web conference</em>. 2022–2032.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Zhaokang Wang, Yunpan Wang, Chunfeng Yuan, Rong Gu, and Yihua Huang. 2021.

</span>
<span class="ltx_bibblock">Empirical analysis of performance bottlenecks in graph neural network training and inference with GPUs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Neurocomputing</em> 446 (2021), 165–191.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1016/j.neucom.2021.03.015" title="">https://doi.org/10.1016/j.neucom.2021.03.015</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2020b)</span>
<span class="ltx_bibblock">
Shiwen Wu, Fei Sun, Wentao Zhang, Xu Xie, and Bin Cui. 2020b.

</span>
<span class="ltx_bibblock">Graph neural networks in recommender systems: a survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.3.1">ACM Computing Surveys (CSUR)</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2020a)</span>
<span class="ltx_bibblock">
Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. 2020a.

</span>
<span class="ltx_bibblock">A comprehensive survey on graph neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">IEEE transactions on neural networks and learning systems</em> 32, 1 (2020), 4–24.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">xia Liang et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Ru xia Liang, Qian Zhang, Jianqiang Wang, and Jie Lu. 2022.

</span>
<span class="ltx_bibblock">A Hierarchical Attention Network for Cross-Domain Group Recommendation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">IEEE Transactions on Neural Networks and Learning Systems</em> 35 (2022), 3859–3873.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018.

</span>
<span class="ltx_bibblock">How powerful are graph neural networks?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib38.3.1">arXiv preprint arXiv:1810.00826</em> (2018).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Runzhen Xue, Dengke Han, Mingyu Yan, Mo Zou, Xiaocheng Yang, Duo Wang, Wenming Li, Zhimin Tang, John Kim, Xiaochun Ye, and Dongrui Fan. 2024a.

</span>
<span class="ltx_bibblock">HiHGNN: Accelerating HGNNs Through Parallelism and Data Reusability Exploitation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.3.1">IEEE Transactions on Parallel and Distributed Systems</em> 35, 7 (2024), 1122–1138.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xue et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Runzhen Xue, Mingyu Yan, Dengke Han, Yihan Teng, Zhimin Tang, Xiaochun Ye, and Dongrui Fan. 2024b.

</span>
<span class="ltx_bibblock">GDR-HGNN: A Heterogeneous Graph Neural Networks Accelerator Frontend with Graph Decoupling and Recoupling.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.3.1">ArXiv</em> abs/2404.04792 (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Mingyu Yan, Zhaodong Chen, Lei Deng, et al<span class="ltx_text" id="bib.bib41.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Characterizing and understanding GCNs on GPU.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.4.1">IEEE Computer Architecture Letters</em> 19, 1 (2020), 22–25.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Mingyu Yan, Xing Hu, Shuangchen Li, et al<span class="ltx_text" id="bib.bib42.3.1">.</span> 2019.

</span>
<span class="ltx_bibblock">Alleviating Irregularity in Graph Analytics Acceleration: A Hardware/Software Co-Design Approach. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.4.1">Proceedings of the 52nd Annual IEEE/ACM International Symposium on Microarchitecture</em> (Columbus, OH, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib42.5.2">(MICRO ’52)</em>. Association for Computing Machinery, New York, NY, USA, 615–628.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yan et al<span class="ltx_text" id="bib.bib43.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Mingyu Yan, Mo Zou, Xiaocheng Yang, et al<span class="ltx_text" id="bib.bib43.3.1">.</span> 2022.

</span>
<span class="ltx_bibblock">Characterizing and Understanding HGNNs on GPUs.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.4.1">IEEE Computer Architecture Letters</em> 21, 2 (2022), 69–72.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Xiaocheng Yang, Mingyu Yan, Shirui Pan, Xiaochun Ye, and Dongrui Fan. 2023.

</span>
<span class="ltx_bibblock">Simple and Efficient Heterogeneous Graph Neural Network. In <em class="ltx_emph ltx_font_italic" id="bib.bib44.3.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, Vol. 37.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al<span class="ltx_text" id="bib.bib45.2.2.1">.</span> (2019)</span>
<span class="ltx_bibblock">
Yanfang Ye, Shifu Hou, Lingwei Chen, Jingwei Lei, Wenqiang Wan, Jiabin Wang, Qi Xiong, and Fudong Shao. 2019.

</span>
<span class="ltx_bibblock">Out-of-sample node representation learning for heterogeneous graph in real-time android malware detection. In <em class="ltx_emph ltx_font_italic" id="bib.bib45.3.1">Proceedings of the 28th International Joint Conference on Artificial Intelligence</em> (Macao, China) <em class="ltx_emph ltx_font_italic" id="bib.bib45.4.2">(IJCAI’19)</em>. AAAI Press, 4150–4156.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2022a)</span>
<span class="ltx_bibblock">
Hengrui Zhang, Zhongming Yu, Guohao Dai, Guyue Huang, Yufei Ding, Yuan Xie, and Yu Wang. 2022a.

</span>
<span class="ltx_bibblock">Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective. In <em class="ltx_emph ltx_font_italic" id="bib.bib46.3.1">Proceedings of Machine Learning and Systems 2022, MLSys 2022, Santa Clara, CA, USA, August 29 - September 1, 2022</em>, Diana Marculescu, Yuejie Chi, and Carole-Jean Wu (Eds.). mlsys.org.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib47.2.2.1">.</span> (2022b)</span>
<span class="ltx_bibblock">
Hengrui Zhang, Zhongming Yu, Guohao Dai, Guyue Huang, Yufei Ding, Yuan Xie, and Yu Wang. 2022b.

</span>
<span class="ltx_bibblock">Understanding GNN Computational Graph: A Coordinated Computation, IO, and Memory Perspective. In <em class="ltx_emph ltx_font_italic" id="bib.bib47.3.1">Proceedings of the Fifth Conference on Machine Learning and Systems, MLSys 2022, Santa Clara, CA, USA, August 29 - September 1, 2022</em>, Diana Marculescu, Yuejie Chi, and Carole-Jean Wu (Eds.). mlsys.org.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib48.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Zhihui Zhang, Jingwen Leng, Lingxiao Ma, Youshan Miao, Chao Li, and Minyi Guo. 2020.

</span>
<span class="ltx_bibblock">Architectural implications of graph neural networks.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.3.1">IEEE Computer architecture letters</em> 19, 1 (2020), 59–62.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhao and Yu (2021)</span>
<span class="ltx_bibblock">
Anping Zhao and Yu Yu. 2021.

</span>
<span class="ltx_bibblock">Context Aware Sentiment Link Prediction in Heterogeneous Social Network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Cognitive Computation</em> 14 (2021), 300 – 309.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zheng et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Da Zheng, Xiang Song, Chengru Yang, Dominique LaSalle, and George Karypis. 2022.

</span>
<span class="ltx_bibblock">Distributed Hybrid CPU and GPU training for Graph Neural Networks on Billion-Scale Heterogeneous Graphs. In <em class="ltx_emph ltx_font_italic" id="bib.bib50.3.1">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</em> (Washington DC, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib50.4.2">(KDD ’22)</em>. Association for Computing Machinery, New York, NY, USA, 4582–4591.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhong et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Weida Zhong, Qiuling Suo, Xiaowei Jia, Aidong Zhang, and Lu Su. 2021.

</span>
<span class="ltx_bibblock">Heterogeneous Spatio-Temporal Graph Convolution Network for Traffic Forecasting with Missing Values. In <em class="ltx_emph ltx_font_italic" id="bib.bib51.3.1">2021 IEEE 41st International Conference on Distributed Computing Systems (ICDCS)</em>. 707–717.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Jie Zhou, Ganqu Cui, Shengding Hu, et al<span class="ltx_text" id="bib.bib52.3.1">.</span> 2020.

</span>
<span class="ltx_bibblock">Graph neural networks: A review of methods and applications.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.4.1">AI Open</em> 1 (2020), 57–81.

</span>
<span class="ltx_bibblock">
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Aug 16 01:11:27 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
