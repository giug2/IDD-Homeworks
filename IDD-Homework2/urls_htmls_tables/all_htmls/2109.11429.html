<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2109.11429] Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data</title><meta property="og:description" content="Generative models trained with Differential Privacy (DP) can be used to generate synthetic data while minimizing privacy risks.
We analyze the impact of DP on these models vis-à-vis underrepresented classes/subgroups o…">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2109.11429">

<!--Generated on Wed Mar  6 21:19:49 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_font_bold ltx_title_document">
<span id="id1.id1" class="ltx_text ltx_font_italic">Robin Hood</span> and <span id="id2.id2" class="ltx_text ltx_font_italic">Matthew</span> Effects: Differential Privacy Has
<br class="ltx_break">Disparate Impact on Synthetic Data</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Georgi Ganev
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bristena Oprisanu
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Emiliano De Cristofaro
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id3.id1" class="ltx_p">Generative models trained with Differential Privacy (DP) can be used to generate synthetic data while minimizing privacy risks.
We analyze the impact of DP on these models vis-à-vis <span id="id3.id1.1" class="ltx_text ltx_font_italic">underrepresented</span> classes/subgroups of data, specifically, studying: 1) the <span id="id3.id1.2" class="ltx_text ltx_font_italic">size</span> of classes/subgroups in the synthetic data, and 2) the <span id="id3.id1.3" class="ltx_text ltx_font_italic">accuracy</span> of classification tasks run on them.
We also evaluate the effect of various levels of imbalance and privacy budgets.
Our analysis uses three state-of-the-art DP models (PrivBayes, DP-WGAN, and PATE-GAN) and shows that DP yields opposite size distributions in the generated synthetic data.
It affects the gap between the majority and minority classes/subgroups; in some cases by reducing it (a “Robin Hood” effect) and, in others, by increasing it (a “Matthew” effect).
Either way, this leads to (similar) disparate impacts on the accuracy of classification tasks on the synthetic data, affecting disproportionately more the underrepresented subparts of the data.
Consequently, when training models on synthetic data, one might incur the risk of treating different subpopulations unevenly, leading to unreliable or unfair conclusions.</p>
</div>
<div id="p2" class="ltx_para">
<br class="ltx_break">
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Releasing synthetic data is an increasingly advocated and adopted approach to reduce privacy risks while sharing data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Van Der Schaar &amp; Maxfield</span>, <a href="#bib.bib54" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>.
Synthetic data initiatives have been promoted by, e.g., the US Census Bureau <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Benedetto et al.</span>, <a href="#bib.bib7" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, England’s National Health Service <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">NHS England</span>, <a href="#bib.bib37" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, and NIST <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">NIST</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018a</span></a>, <a href="#bib.bib39" title="" class="ltx_ref">b</a>)</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">The idea is to train <span id="S1.p2.1.1" class="ltx_text ltx_font_italic">generative</span> machine learning models to learn the probabilistic distribution of the (real) data and then sample from the model to generate new (synthetic) data records.
However, real-world datasets often contain personal and sensitive information about individuals <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Thompson &amp; Warzel</span>, <a href="#bib.bib49" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> that could leak into/through models that are trained on them.
Generative models can overfit or memorize individual data points <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Carlini et al.</span>, <a href="#bib.bib9" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Webster et al.</span>, <a href="#bib.bib55" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, which facilitates privacy attacks such as membership or property inference attacks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hayes et al.</span>, <a href="#bib.bib25" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Chen et al.</span>, <a href="#bib.bib11" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020a</span></a>; <span class="ltx_text" style="font-size:90%;">Stadler et al.</span>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>.</p>
</div>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">The state-of-the-art method for training models that provably minimize inferences is to do while satisfying Differential Privacy (DP) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dwork et al.</span>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite>.
DP provides a mathematical guarantee on the privacy of all records in the training dataset by bounding their individual contribution.
This can be achieved by applying noise (e.g., using the Laplace Mechanism <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dwork et al.</span>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2006b</span></a>)</cite>), relying on techniques such as DP-Stochastic Gradient Descent (DP-SGD) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Abadi et al.</span>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>, or Private Aggregation of Teacher Ensembles (PATE) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Papernot et al.</span>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p">Naturally, as they rely on perturbation, DP methods inherently reduce accuracy in the task the data is used for.
Incidentally, this degradation is often disproportionate; for instance, the accuracy of <span id="S1.p4.1.1" class="ltx_text ltx_font_italic">DP classifiers</span> often drops more for the underrepresented classes and subgroups of the dataset.
Prior work <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bagdasaryan et al.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Farrand et al.</span>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Uniyal et al.</span>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> illustrates this effect when deep neural networks are trained with DP-SGD or PATE on imbalanced datasets.
Moreover, <span id="S1.p4.1.2" class="ltx_text ltx_font_italic">DP statistics</span> have also been shown to lead to disproportionate biases <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kuppam et al.</span>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para ltx_noindent">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_bold">Problem Statement.</span>
So far, this “disparate effect” caused by DP and its applications have only been analyzed in the context of discriminative models.
This paper focuses on DP generative models and tabular synthetic data. We look at the problem from two angles: 1) <span id="S1.p5.1.2" class="ltx_text ltx_font_italic">counts comparisons</span> and 2) <span id="S1.p5.1.3" class="ltx_text ltx_font_italic">downstream tasks</span> such as classification.
We analyze three widely used DP generative models: PrivBayes <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et al.</span>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>, DP-WGAN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Alzantot &amp; Srivastava</span>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, and PATE-GAN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jordon et al.</span>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite>, which rely, respectively, on the Laplace Mechanism, DP-SGD, and PATE.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p">Our work aims to answer the following research questions:</p>
<ul id="S1.I1" class="ltx_itemize">
<li id="S1.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S1.I1.ix1.p1" class="ltx_para">
<p id="S1.I1.ix1.p1.1" class="ltx_p"><span id="S1.I1.ix1.p1.1.1" class="ltx_text ltx_font_bold">RQ1:</span> Do DP generative models generate data in similar classes and subgroups proportions to the real data?</p>
</div>
</li>
<li id="S1.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S1.I1.ix2.p1" class="ltx_para">
<p id="S1.I1.ix2.p1.1" class="ltx_p"><span id="S1.I1.ix2.p1.1.1" class="ltx_text ltx_font_bold">RQ2:</span> Does training a classifier on DP synthetic data lead to the same disparate impact on accuracy as training a DP classifier on the real data?</p>
</div>
</li>
<li id="S1.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">–</span> 
<div id="S1.I1.ix3.p1" class="ltx_para">
<p id="S1.I1.ix3.p1.1" class="ltx_p"><span id="S1.I1.ix3.p1.1.1" class="ltx_text ltx_font_bold">RQ3:</span> Do different DP mechanisms for DP synthetic data behave similarly under different privacy and data imbalance levels?</p>
</div>
</li>
</ul>
</div>
<div id="S1.p7" class="ltx_para ltx_noindent">
<p id="S1.p7.1" class="ltx_p"><span id="S1.p7.1.1" class="ltx_text ltx_font_bold">Main Findings.</span>
Overall, our experiments show that:</p>
<ol id="S1.I2" class="ltx_enumerate">
<li id="S1.I2.i1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span> 
<div id="S1.I2.i1.p1" class="ltx_para">
<p id="S1.I2.i1.p1.1" class="ltx_p">There is a disparate effect on the classes and subgroups sizes in the synthetic data generated by all DP generative models.
This effect is dependent on the generative model and DP mechanism; e.g., PrivBayes evens the data, while PATE-GAN increases the imbalance.</p>
</div>
</li>
<li id="S1.I2.i2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span> 
<div id="S1.I2.i2.p1" class="ltx_para">
<p id="S1.I2.i2.p1.1" class="ltx_p">There also is a disparate effect on the accuracy of classifiers trained on synthetic data generated by all generative models; for instance, underrepresented classes and subgroups suffer bigger and/or more variable drops.
Furthermore, majority classes with similar characteristics to minority classes could also suffer from a disproportionate drop in utility.</p>
</div>
</li>
<li id="S1.I2.i3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span> 
<div id="S1.I2.i3.p1" class="ltx_para">
<p id="S1.I2.i3.p1.1" class="ltx_p">The magnitude of these effects on size and accuracy increases when stronger privacy guarantees are imposed.
Higher data imbalance levels further intensify them.
Also, some generative models are better suited for specific privacy budgets and imbalance levels.</p>
</div>
</li>
<li id="S1.I2.i4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span> 
<div id="S1.I2.i4.p1" class="ltx_para">
<p id="S1.I2.i4.p1.1" class="ltx_p">While classifiers trained on data generated by PATE-GAN perform much better than, or on par with, DP-WGAN, we observe some undesirable behaviors: PATE-GAN completely fails to learn some subparts of the data with highly imbalanced multi-class data. With low privacy budgets, it also generates synthetic data with artificially enhanced correlation between the subgroup and the target columns.</p>
</div>
</li>
</ol>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Preliminaries</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p">In this section, we present some background information, then, we introduce the datasets, the classifiers used for baselines, and the generative models used for producing synthetic data. (The source of the implementations we use, when applicable, is also reported.)</p>
</div>
<section id="S2.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Background</h3>

<div id="S2.SS1.p1" class="ltx_para ltx_noindent">
<p id="S2.SS1.p1.11" class="ltx_p"><span id="S2.SS1.p1.11.1" class="ltx_text ltx_font_bold">Generative Models and Synthetic Data.</span>
During fitting, the generative model training algorithm <math id="S2.SS1.p1.1.m1.1" class="ltx_Math" alttext="GM(D^{n})" display="inline"><semantics id="S2.SS1.p1.1.m1.1a"><mrow id="S2.SS1.p1.1.m1.1.1" xref="S2.SS1.p1.1.m1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.3.cmml">G</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.2.cmml">​</mo><mi id="S2.SS1.p1.1.m1.1.1.4" xref="S2.SS1.p1.1.m1.1.1.4.cmml">M</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.1.m1.1.1.2a" xref="S2.SS1.p1.1.m1.1.1.2.cmml">​</mo><mrow id="S2.SS1.p1.1.m1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.1.m1.1.1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.1.1.1.cmml">(</mo><msup id="S2.SS1.p1.1.m1.1.1.1.1.1" xref="S2.SS1.p1.1.m1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.1.m1.1.1.1.1.1.2" xref="S2.SS1.p1.1.m1.1.1.1.1.1.2.cmml">D</mi><mi id="S2.SS1.p1.1.m1.1.1.1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.1.1.1.3.cmml">n</mi></msup><mo stretchy="false" id="S2.SS1.p1.1.m1.1.1.1.1.3" xref="S2.SS1.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.1.m1.1b"><apply id="S2.SS1.p1.1.m1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1"><times id="S2.SS1.p1.1.m1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.2"></times><ci id="S2.SS1.p1.1.m1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.3">𝐺</ci><ci id="S2.SS1.p1.1.m1.1.1.4.cmml" xref="S2.SS1.p1.1.m1.1.1.4">𝑀</ci><apply id="S2.SS1.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.2">𝐷</ci><ci id="S2.SS1.p1.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.1.m1.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.1.m1.1c">GM(D^{n})</annotation></semantics></math> takes in input <math id="S2.SS1.p1.2.m2.1" class="ltx_Math" alttext="D^{n}" display="inline"><semantics id="S2.SS1.p1.2.m2.1a"><msup id="S2.SS1.p1.2.m2.1.1" xref="S2.SS1.p1.2.m2.1.1.cmml"><mi id="S2.SS1.p1.2.m2.1.1.2" xref="S2.SS1.p1.2.m2.1.1.2.cmml">D</mi><mi id="S2.SS1.p1.2.m2.1.1.3" xref="S2.SS1.p1.2.m2.1.1.3.cmml">n</mi></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.2.m2.1b"><apply id="S2.SS1.p1.2.m2.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.2.m2.1.1.1.cmml" xref="S2.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="S2.SS1.p1.2.m2.1.1.2.cmml" xref="S2.SS1.p1.2.m2.1.1.2">𝐷</ci><ci id="S2.SS1.p1.2.m2.1.1.3.cmml" xref="S2.SS1.p1.2.m2.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.2.m2.1c">D^{n}</annotation></semantics></math> (a sample dataset consisting of <math id="S2.SS1.p1.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.SS1.p1.3.m3.1a"><mi id="S2.SS1.p1.3.m3.1.1" xref="S2.SS1.p1.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.3.m3.1b"><ci id="S2.SS1.p1.3.m3.1.1.cmml" xref="S2.SS1.p1.3.m3.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.3.m3.1c">n</annotation></semantics></math> records drawn iid from the population <math id="S2.SS1.p1.4.m4.1" class="ltx_Math" alttext="D^{n}{\sim}P(\mathbb{D})" display="inline"><semantics id="S2.SS1.p1.4.m4.1a"><mrow id="S2.SS1.p1.4.m4.1.2" xref="S2.SS1.p1.4.m4.1.2.cmml"><msup id="S2.SS1.p1.4.m4.1.2.2" xref="S2.SS1.p1.4.m4.1.2.2.cmml"><mi id="S2.SS1.p1.4.m4.1.2.2.2" xref="S2.SS1.p1.4.m4.1.2.2.2.cmml">D</mi><mi id="S2.SS1.p1.4.m4.1.2.2.3" xref="S2.SS1.p1.4.m4.1.2.2.3.cmml">n</mi></msup><mo id="S2.SS1.p1.4.m4.1.2.1" xref="S2.SS1.p1.4.m4.1.2.1.cmml">∼</mo><mrow id="S2.SS1.p1.4.m4.1.2.3" xref="S2.SS1.p1.4.m4.1.2.3.cmml"><mi id="S2.SS1.p1.4.m4.1.2.3.2" xref="S2.SS1.p1.4.m4.1.2.3.2.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.4.m4.1.2.3.1" xref="S2.SS1.p1.4.m4.1.2.3.1.cmml">​</mo><mrow id="S2.SS1.p1.4.m4.1.2.3.3.2" xref="S2.SS1.p1.4.m4.1.2.3.cmml"><mo stretchy="false" id="S2.SS1.p1.4.m4.1.2.3.3.2.1" xref="S2.SS1.p1.4.m4.1.2.3.cmml">(</mo><mi mathvariant="normal" id="S2.SS1.p1.4.m4.1.1" xref="S2.SS1.p1.4.m4.1.1.cmml">𝔻</mi><mo stretchy="false" id="S2.SS1.p1.4.m4.1.2.3.3.2.2" xref="S2.SS1.p1.4.m4.1.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.4.m4.1b"><apply id="S2.SS1.p1.4.m4.1.2.cmml" xref="S2.SS1.p1.4.m4.1.2"><csymbol cd="latexml" id="S2.SS1.p1.4.m4.1.2.1.cmml" xref="S2.SS1.p1.4.m4.1.2.1">similar-to</csymbol><apply id="S2.SS1.p1.4.m4.1.2.2.cmml" xref="S2.SS1.p1.4.m4.1.2.2"><csymbol cd="ambiguous" id="S2.SS1.p1.4.m4.1.2.2.1.cmml" xref="S2.SS1.p1.4.m4.1.2.2">superscript</csymbol><ci id="S2.SS1.p1.4.m4.1.2.2.2.cmml" xref="S2.SS1.p1.4.m4.1.2.2.2">𝐷</ci><ci id="S2.SS1.p1.4.m4.1.2.2.3.cmml" xref="S2.SS1.p1.4.m4.1.2.2.3">𝑛</ci></apply><apply id="S2.SS1.p1.4.m4.1.2.3.cmml" xref="S2.SS1.p1.4.m4.1.2.3"><times id="S2.SS1.p1.4.m4.1.2.3.1.cmml" xref="S2.SS1.p1.4.m4.1.2.3.1"></times><ci id="S2.SS1.p1.4.m4.1.2.3.2.cmml" xref="S2.SS1.p1.4.m4.1.2.3.2">𝑃</ci><ci id="S2.SS1.p1.4.m4.1.1.cmml" xref="S2.SS1.p1.4.m4.1.1">𝔻</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.4.m4.1c">D^{n}{\sim}P(\mathbb{D})</annotation></semantics></math>), updates its internal parameters to learn <math id="S2.SS1.p1.5.m5.1" class="ltx_Math" alttext="P_{g}(D^{n})" display="inline"><semantics id="S2.SS1.p1.5.m5.1a"><mrow id="S2.SS1.p1.5.m5.1.1" xref="S2.SS1.p1.5.m5.1.1.cmml"><msub id="S2.SS1.p1.5.m5.1.1.3" xref="S2.SS1.p1.5.m5.1.1.3.cmml"><mi id="S2.SS1.p1.5.m5.1.1.3.2" xref="S2.SS1.p1.5.m5.1.1.3.2.cmml">P</mi><mi id="S2.SS1.p1.5.m5.1.1.3.3" xref="S2.SS1.p1.5.m5.1.1.3.3.cmml">g</mi></msub><mo lspace="0em" rspace="0em" id="S2.SS1.p1.5.m5.1.1.2" xref="S2.SS1.p1.5.m5.1.1.2.cmml">​</mo><mrow id="S2.SS1.p1.5.m5.1.1.1.1" xref="S2.SS1.p1.5.m5.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.5.m5.1.1.1.1.2" xref="S2.SS1.p1.5.m5.1.1.1.1.1.cmml">(</mo><msup id="S2.SS1.p1.5.m5.1.1.1.1.1" xref="S2.SS1.p1.5.m5.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.5.m5.1.1.1.1.1.2" xref="S2.SS1.p1.5.m5.1.1.1.1.1.2.cmml">D</mi><mi id="S2.SS1.p1.5.m5.1.1.1.1.1.3" xref="S2.SS1.p1.5.m5.1.1.1.1.1.3.cmml">n</mi></msup><mo stretchy="false" id="S2.SS1.p1.5.m5.1.1.1.1.3" xref="S2.SS1.p1.5.m5.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.5.m5.1b"><apply id="S2.SS1.p1.5.m5.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1"><times id="S2.SS1.p1.5.m5.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.2"></times><apply id="S2.SS1.p1.5.m5.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.3.1.cmml" xref="S2.SS1.p1.5.m5.1.1.3">subscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.3.2.cmml" xref="S2.SS1.p1.5.m5.1.1.3.2">𝑃</ci><ci id="S2.SS1.p1.5.m5.1.1.3.3.cmml" xref="S2.SS1.p1.5.m5.1.1.3.3">𝑔</ci></apply><apply id="S2.SS1.p1.5.m5.1.1.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.5.m5.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.5.m5.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p1.5.m5.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.5.m5.1.1.1.1.1.2">𝐷</ci><ci id="S2.SS1.p1.5.m5.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.5.m5.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.5.m5.1c">P_{g}(D^{n})</annotation></semantics></math>, a (lower-dimensional) representation of the joint probability distribution of the sample dataset <math id="S2.SS1.p1.6.m6.1" class="ltx_Math" alttext="P(D^{n})" display="inline"><semantics id="S2.SS1.p1.6.m6.1a"><mrow id="S2.SS1.p1.6.m6.1.1" xref="S2.SS1.p1.6.m6.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.3" xref="S2.SS1.p1.6.m6.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.6.m6.1.1.2" xref="S2.SS1.p1.6.m6.1.1.2.cmml">​</mo><mrow id="S2.SS1.p1.6.m6.1.1.1.1" xref="S2.SS1.p1.6.m6.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.6.m6.1.1.1.1.2" xref="S2.SS1.p1.6.m6.1.1.1.1.1.cmml">(</mo><msup id="S2.SS1.p1.6.m6.1.1.1.1.1" xref="S2.SS1.p1.6.m6.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.6.m6.1.1.1.1.1.2" xref="S2.SS1.p1.6.m6.1.1.1.1.1.2.cmml">D</mi><mi id="S2.SS1.p1.6.m6.1.1.1.1.1.3" xref="S2.SS1.p1.6.m6.1.1.1.1.1.3.cmml">n</mi></msup><mo stretchy="false" id="S2.SS1.p1.6.m6.1.1.1.1.3" xref="S2.SS1.p1.6.m6.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.6.m6.1b"><apply id="S2.SS1.p1.6.m6.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1"><times id="S2.SS1.p1.6.m6.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.2"></times><ci id="S2.SS1.p1.6.m6.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.3">𝑃</ci><apply id="S2.SS1.p1.6.m6.1.1.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.6.m6.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p1.6.m6.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.6.m6.1.1.1.1.1.2">𝐷</ci><ci id="S2.SS1.p1.6.m6.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.6.m6.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.6.m6.1c">P(D^{n})</annotation></semantics></math>, and outputs a trained model <math id="S2.SS1.p1.7.m7.1" class="ltx_Math" alttext="g(D^{n})" display="inline"><semantics id="S2.SS1.p1.7.m7.1a"><mrow id="S2.SS1.p1.7.m7.1.1" xref="S2.SS1.p1.7.m7.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.3" xref="S2.SS1.p1.7.m7.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.7.m7.1.1.2" xref="S2.SS1.p1.7.m7.1.1.2.cmml">​</mo><mrow id="S2.SS1.p1.7.m7.1.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.7.m7.1.1.1.1.2" xref="S2.SS1.p1.7.m7.1.1.1.1.1.cmml">(</mo><msup id="S2.SS1.p1.7.m7.1.1.1.1.1" xref="S2.SS1.p1.7.m7.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.7.m7.1.1.1.1.1.2" xref="S2.SS1.p1.7.m7.1.1.1.1.1.2.cmml">D</mi><mi id="S2.SS1.p1.7.m7.1.1.1.1.1.3" xref="S2.SS1.p1.7.m7.1.1.1.1.1.3.cmml">n</mi></msup><mo stretchy="false" id="S2.SS1.p1.7.m7.1.1.1.1.3" xref="S2.SS1.p1.7.m7.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.7.m7.1b"><apply id="S2.SS1.p1.7.m7.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1"><times id="S2.SS1.p1.7.m7.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.2"></times><ci id="S2.SS1.p1.7.m7.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.3">𝑔</ci><apply id="S2.SS1.p1.7.m7.1.1.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.7.m7.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.7.m7.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p1.7.m7.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.7.m7.1.1.1.1.1.2">𝐷</ci><ci id="S2.SS1.p1.7.m7.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.7.m7.1.1.1.1.1.3">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.7.m7.1c">g(D^{n})</annotation></semantics></math>.
Then, one can sample from the trained model to generate a synthetic dataset of size <math id="S2.SS1.p1.8.m8.1" class="ltx_Math" alttext="m" display="inline"><semantics id="S2.SS1.p1.8.m8.1a"><mi id="S2.SS1.p1.8.m8.1.1" xref="S2.SS1.p1.8.m8.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.8.m8.1b"><ci id="S2.SS1.p1.8.m8.1.1.cmml" xref="S2.SS1.p1.8.m8.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.8.m8.1c">m</annotation></semantics></math>, <math id="S2.SS1.p1.9.m9.1" class="ltx_Math" alttext="S^{m}{\sim}P(g(D^{n}))" display="inline"><semantics id="S2.SS1.p1.9.m9.1a"><mrow id="S2.SS1.p1.9.m9.1.1" xref="S2.SS1.p1.9.m9.1.1.cmml"><msup id="S2.SS1.p1.9.m9.1.1.3" xref="S2.SS1.p1.9.m9.1.1.3.cmml"><mi id="S2.SS1.p1.9.m9.1.1.3.2" xref="S2.SS1.p1.9.m9.1.1.3.2.cmml">S</mi><mi id="S2.SS1.p1.9.m9.1.1.3.3" xref="S2.SS1.p1.9.m9.1.1.3.3.cmml">m</mi></msup><mo id="S2.SS1.p1.9.m9.1.1.2" xref="S2.SS1.p1.9.m9.1.1.2.cmml">∼</mo><mrow id="S2.SS1.p1.9.m9.1.1.1" xref="S2.SS1.p1.9.m9.1.1.1.cmml"><mi id="S2.SS1.p1.9.m9.1.1.1.3" xref="S2.SS1.p1.9.m9.1.1.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.9.m9.1.1.1.2" xref="S2.SS1.p1.9.m9.1.1.1.2.cmml">​</mo><mrow id="S2.SS1.p1.9.m9.1.1.1.1.1" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.9.m9.1.1.1.1.1.2" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.p1.9.m9.1.1.1.1.1.1" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.9.m9.1.1.1.1.1.1.3" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S2.SS1.p1.9.m9.1.1.1.1.1.1.2" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.2" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml">(</mo><msup id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.2" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.2.cmml">D</mi><mi id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.3" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.3.cmml">n</mi></msup><mo stretchy="false" id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.3" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo stretchy="false" id="S2.SS1.p1.9.m9.1.1.1.1.1.3" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.9.m9.1b"><apply id="S2.SS1.p1.9.m9.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1"><csymbol cd="latexml" id="S2.SS1.p1.9.m9.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.2">similar-to</csymbol><apply id="S2.SS1.p1.9.m9.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.3.1.cmml" xref="S2.SS1.p1.9.m9.1.1.3">superscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.3.2.cmml" xref="S2.SS1.p1.9.m9.1.1.3.2">𝑆</ci><ci id="S2.SS1.p1.9.m9.1.1.3.3.cmml" xref="S2.SS1.p1.9.m9.1.1.3.3">𝑚</ci></apply><apply id="S2.SS1.p1.9.m9.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1.1"><times id="S2.SS1.p1.9.m9.1.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.1.2"></times><ci id="S2.SS1.p1.9.m9.1.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.1.3">𝑃</ci><apply id="S2.SS1.p1.9.m9.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1"><times id="S2.SS1.p1.9.m9.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.2"></times><ci id="S2.SS1.p1.9.m9.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.3">𝑔</ci><apply id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.2">𝐷</ci><ci id="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.3.cmml" xref="S2.SS1.p1.9.m9.1.1.1.1.1.1.1.1.1.3">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.9.m9.1c">S^{m}{\sim}P(g(D^{n}))</annotation></semantics></math>.
Both the fitting and generation steps are stochastic; in order to get confidence intervals, one can train the generative model <math id="S2.SS1.p1.10.m10.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S2.SS1.p1.10.m10.1a"><mi id="S2.SS1.p1.10.m10.1.1" xref="S2.SS1.p1.10.m10.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.10.m10.1b"><ci id="S2.SS1.p1.10.m10.1.1.cmml" xref="S2.SS1.p1.10.m10.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.10.m10.1c">l</annotation></semantics></math> times and sample <math id="S2.SS1.p1.11.m11.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS1.p1.11.m11.1a"><mi id="S2.SS1.p1.11.m11.1.1" xref="S2.SS1.p1.11.m11.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p1.11.m11.1b"><ci id="S2.SS1.p1.11.m11.1.1.cmml" xref="S2.SS1.p1.11.m11.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p1.11.m11.1c">k</annotation></semantics></math> synthetic datasets for each trained model.</p>
</div>
<div id="S2.SS1.p2" class="ltx_para">
<p id="S2.SS1.p2.1" class="ltx_p">While several different approaches exist to build generative models, in this paper, we focus on two of them, specifically: 1) Bayesian networks <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Koller &amp; Friedman</span>, <a href="#bib.bib31" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2009</span></a>; <span class="ltx_text" style="font-size:90%;">Barber</span>, <a href="#bib.bib6" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2012</span></a>)</cite>, and 2) Generative Adversarial Networks (GANs) <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Goodfellow et al.</span>, <a href="#bib.bib23" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite>.
The former is a graphical model that breaks down the joint distribution by explicit lower-dimensional conditional distributions.
The latter approximates the dataset distribution implicitly by iteratively optimizing a min-max “game” between two neural networks: a generator, producing synthetic data, and a discriminator, trying to distinguish real from synthetic samples.</p>
</div>
<div id="S2.SS1.p3" class="ltx_para ltx_noindent">
<p id="S2.SS1.p3.8" class="ltx_p"><span id="S2.SS1.p3.8.1" class="ltx_text ltx_font_bold">Differential Privacy (DP).</span>
Let <math id="S2.SS1.p3.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS1.p3.1.m1.1a"><mi id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><ci id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">\epsilon</annotation></semantics></math> be a positive and real number and <math id="S2.SS1.p3.2.m2.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S2.SS1.p3.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.2.m2.1.1" xref="S2.SS1.p3.2.m2.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.2.m2.1b"><ci id="S2.SS1.p3.2.m2.1.1.cmml" xref="S2.SS1.p3.2.m2.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.2.m2.1c">\mathcal{A}</annotation></semantics></math> a randomized algorithm.
<math id="S2.SS1.p3.3.m3.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S2.SS1.p3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.3.m3.1.1" xref="S2.SS1.p3.3.m3.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.3.m3.1b"><ci id="S2.SS1.p3.3.m3.1.1.cmml" xref="S2.SS1.p3.3.m3.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.3.m3.1c">\mathcal{A}</annotation></semantics></math> satisfies <math id="S2.SS1.p3.4.m4.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS1.p3.4.m4.1a"><mi id="S2.SS1.p3.4.m4.1.1" xref="S2.SS1.p3.4.m4.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.4.m4.1b"><ci id="S2.SS1.p3.4.m4.1.1.cmml" xref="S2.SS1.p3.4.m4.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.4.m4.1c">\epsilon</annotation></semantics></math>-DP if, for all neighboring datasets <math id="S2.SS1.p3.5.m5.1" class="ltx_Math" alttext="D_{1}" display="inline"><semantics id="S2.SS1.p3.5.m5.1a"><msub id="S2.SS1.p3.5.m5.1.1" xref="S2.SS1.p3.5.m5.1.1.cmml"><mi id="S2.SS1.p3.5.m5.1.1.2" xref="S2.SS1.p3.5.m5.1.1.2.cmml">D</mi><mn id="S2.SS1.p3.5.m5.1.1.3" xref="S2.SS1.p3.5.m5.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.5.m5.1b"><apply id="S2.SS1.p3.5.m5.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.5.m5.1.1.1.cmml" xref="S2.SS1.p3.5.m5.1.1">subscript</csymbol><ci id="S2.SS1.p3.5.m5.1.1.2.cmml" xref="S2.SS1.p3.5.m5.1.1.2">𝐷</ci><cn type="integer" id="S2.SS1.p3.5.m5.1.1.3.cmml" xref="S2.SS1.p3.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.5.m5.1c">D_{1}</annotation></semantics></math> and <math id="S2.SS1.p3.6.m6.1" class="ltx_Math" alttext="D_{2}" display="inline"><semantics id="S2.SS1.p3.6.m6.1a"><msub id="S2.SS1.p3.6.m6.1.1" xref="S2.SS1.p3.6.m6.1.1.cmml"><mi id="S2.SS1.p3.6.m6.1.1.2" xref="S2.SS1.p3.6.m6.1.1.2.cmml">D</mi><mn id="S2.SS1.p3.6.m6.1.1.3" xref="S2.SS1.p3.6.m6.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.6.m6.1b"><apply id="S2.SS1.p3.6.m6.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS1.p3.6.m6.1.1.1.cmml" xref="S2.SS1.p3.6.m6.1.1">subscript</csymbol><ci id="S2.SS1.p3.6.m6.1.1.2.cmml" xref="S2.SS1.p3.6.m6.1.1.2">𝐷</ci><cn type="integer" id="S2.SS1.p3.6.m6.1.1.3.cmml" xref="S2.SS1.p3.6.m6.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.6.m6.1c">D_{2}</annotation></semantics></math> (differing in a single data record), and all possible outputs <math id="S2.SS1.p3.7.m7.1" class="ltx_Math" alttext="S" display="inline"><semantics id="S2.SS1.p3.7.m7.1a"><mi id="S2.SS1.p3.7.m7.1.1" xref="S2.SS1.p3.7.m7.1.1.cmml">S</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.7.m7.1b"><ci id="S2.SS1.p3.7.m7.1.1.cmml" xref="S2.SS1.p3.7.m7.1.1">𝑆</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.7.m7.1c">S</annotation></semantics></math> of <math id="S2.SS1.p3.8.m8.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S2.SS1.p3.8.m8.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p3.8.m8.1.1" xref="S2.SS1.p3.8.m8.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.8.m8.1b"><ci id="S2.SS1.p3.8.m8.1.1.cmml" xref="S2.SS1.p3.8.m8.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.8.m8.1c">\mathcal{A}</annotation></semantics></math>, the following holds <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dwork et al.</span>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite>:</p>
<table id="S2.Ex1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S2.Ex1.m1.4" class="ltx_Math" alttext="P[{\mathcal{A}}(D_{1})\in S]\leq\exp\left(\epsilon\right)\cdot P[{\mathcal{A}}(D_{2})\in S]" display="block"><semantics id="S2.Ex1.m1.4a"><mrow id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.4.cmml"><mrow id="S2.Ex1.m1.3.3.1" xref="S2.Ex1.m1.3.3.1.cmml"><mi id="S2.Ex1.m1.3.3.1.3" xref="S2.Ex1.m1.3.3.1.3.cmml">P</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.1.2" xref="S2.Ex1.m1.3.3.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.3.3.1.1.1" xref="S2.Ex1.m1.3.3.1.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.2.1.cmml">[</mo><mrow id="S2.Ex1.m1.3.3.1.1.1.1" xref="S2.Ex1.m1.3.3.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.3.3.1.1.1.1.1" xref="S2.Ex1.m1.3.3.1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.3.3.1.1.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.1.1.1.3.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.3.3.1.1.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.1.1.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml">D</mi><mn id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.3.3.1.1.1.1.2" xref="S2.Ex1.m1.3.3.1.1.1.1.2.cmml">∈</mo><mi id="S2.Ex1.m1.3.3.1.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.1.1.3.cmml">S</mi></mrow><mo stretchy="false" id="S2.Ex1.m1.3.3.1.1.1.3" xref="S2.Ex1.m1.3.3.1.1.2.1.cmml">]</mo></mrow></mrow><mo id="S2.Ex1.m1.4.4.3" xref="S2.Ex1.m1.4.4.3.cmml">≤</mo><mrow id="S2.Ex1.m1.4.4.2" xref="S2.Ex1.m1.4.4.2.cmml"><mrow id="S2.Ex1.m1.4.4.2.3" xref="S2.Ex1.m1.4.4.2.3.cmml"><mrow id="S2.Ex1.m1.4.4.2.3.2.2" xref="S2.Ex1.m1.4.4.2.3.2.1.cmml"><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">exp</mi><mo id="S2.Ex1.m1.4.4.2.3.2.2a" xref="S2.Ex1.m1.4.4.2.3.2.1.cmml">⁡</mo><mrow id="S2.Ex1.m1.4.4.2.3.2.2.1" xref="S2.Ex1.m1.4.4.2.3.2.1.cmml"><mo id="S2.Ex1.m1.4.4.2.3.2.2.1.1" xref="S2.Ex1.m1.4.4.2.3.2.1.cmml">(</mo><mi id="S2.Ex1.m1.2.2" xref="S2.Ex1.m1.2.2.cmml">ϵ</mi><mo rspace="0.055em" id="S2.Ex1.m1.4.4.2.3.2.2.1.2" xref="S2.Ex1.m1.4.4.2.3.2.1.cmml">)</mo></mrow></mrow><mo rspace="0.222em" id="S2.Ex1.m1.4.4.2.3.1" xref="S2.Ex1.m1.4.4.2.3.1.cmml">⋅</mo><mi id="S2.Ex1.m1.4.4.2.3.3" xref="S2.Ex1.m1.4.4.2.3.3.cmml">P</mi></mrow><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.2.2" xref="S2.Ex1.m1.4.4.2.2.cmml">​</mo><mrow id="S2.Ex1.m1.4.4.2.1.1" xref="S2.Ex1.m1.4.4.2.1.2.cmml"><mo stretchy="false" id="S2.Ex1.m1.4.4.2.1.1.2" xref="S2.Ex1.m1.4.4.2.1.2.1.cmml">[</mo><mrow id="S2.Ex1.m1.4.4.2.1.1.1" xref="S2.Ex1.m1.4.4.2.1.1.1.cmml"><mrow id="S2.Ex1.m1.4.4.2.1.1.1.1" xref="S2.Ex1.m1.4.4.2.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S2.Ex1.m1.4.4.2.1.1.1.1.3" xref="S2.Ex1.m1.4.4.2.1.1.1.1.3.cmml">𝒜</mi><mo lspace="0em" rspace="0em" id="S2.Ex1.m1.4.4.2.1.1.1.1.2" xref="S2.Ex1.m1.4.4.2.1.1.1.1.2.cmml">​</mo><mrow id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.cmml"><mo stretchy="false" id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.2" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.cmml"><mi id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.2.cmml">D</mi><mn id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.3.cmml">2</mn></msub><mo stretchy="false" id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.3" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.Ex1.m1.4.4.2.1.1.1.2" xref="S2.Ex1.m1.4.4.2.1.1.1.2.cmml">∈</mo><mi id="S2.Ex1.m1.4.4.2.1.1.1.3" xref="S2.Ex1.m1.4.4.2.1.1.1.3.cmml">S</mi></mrow><mo stretchy="false" id="S2.Ex1.m1.4.4.2.1.1.3" xref="S2.Ex1.m1.4.4.2.1.2.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.4b"><apply id="S2.Ex1.m1.4.4.cmml" xref="S2.Ex1.m1.4.4"><leq id="S2.Ex1.m1.4.4.3.cmml" xref="S2.Ex1.m1.4.4.3"></leq><apply id="S2.Ex1.m1.3.3.1.cmml" xref="S2.Ex1.m1.3.3.1"><times id="S2.Ex1.m1.3.3.1.2.cmml" xref="S2.Ex1.m1.3.3.1.2"></times><ci id="S2.Ex1.m1.3.3.1.3.cmml" xref="S2.Ex1.m1.3.3.1.3">𝑃</ci><apply id="S2.Ex1.m1.3.3.1.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.3.3.1.1.2.1.cmml" xref="S2.Ex1.m1.3.3.1.1.1.2">delimited-[]</csymbol><apply id="S2.Ex1.m1.3.3.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1"><in id="S2.Ex1.m1.3.3.1.1.1.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.2"></in><apply id="S2.Ex1.m1.3.3.1.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1"><times id="S2.Ex1.m1.3.3.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.2"></times><ci id="S2.Ex1.m1.3.3.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.3">𝒜</ci><apply id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.2">𝐷</ci><cn type="integer" id="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.1.1.1.1.3">1</cn></apply></apply><ci id="S2.Ex1.m1.3.3.1.1.1.1.3.cmml" xref="S2.Ex1.m1.3.3.1.1.1.1.3">𝑆</ci></apply></apply></apply><apply id="S2.Ex1.m1.4.4.2.cmml" xref="S2.Ex1.m1.4.4.2"><times id="S2.Ex1.m1.4.4.2.2.cmml" xref="S2.Ex1.m1.4.4.2.2"></times><apply id="S2.Ex1.m1.4.4.2.3.cmml" xref="S2.Ex1.m1.4.4.2.3"><ci id="S2.Ex1.m1.4.4.2.3.1.cmml" xref="S2.Ex1.m1.4.4.2.3.1">⋅</ci><apply id="S2.Ex1.m1.4.4.2.3.2.1.cmml" xref="S2.Ex1.m1.4.4.2.3.2.2"><exp id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1"></exp><ci id="S2.Ex1.m1.2.2.cmml" xref="S2.Ex1.m1.2.2">italic-ϵ</ci></apply><ci id="S2.Ex1.m1.4.4.2.3.3.cmml" xref="S2.Ex1.m1.4.4.2.3.3">𝑃</ci></apply><apply id="S2.Ex1.m1.4.4.2.1.2.cmml" xref="S2.Ex1.m1.4.4.2.1.1"><csymbol cd="latexml" id="S2.Ex1.m1.4.4.2.1.2.1.cmml" xref="S2.Ex1.m1.4.4.2.1.1.2">delimited-[]</csymbol><apply id="S2.Ex1.m1.4.4.2.1.1.1.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1"><in id="S2.Ex1.m1.4.4.2.1.1.1.2.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1.2"></in><apply id="S2.Ex1.m1.4.4.2.1.1.1.1.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1.1"><times id="S2.Ex1.m1.4.4.2.1.1.1.1.2.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1.1.2"></times><ci id="S2.Ex1.m1.4.4.2.1.1.1.1.3.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1.1.3">𝒜</ci><apply id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1">subscript</csymbol><ci id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.2">𝐷</ci><cn type="integer" id="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1.1.1.1.1.3">2</cn></apply></apply><ci id="S2.Ex1.m1.4.4.2.1.1.1.3.cmml" xref="S2.Ex1.m1.4.4.2.1.1.1.3">𝑆</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.4c">P[{\mathcal{A}}(D_{1})\in S]\leq\exp\left(\epsilon\right)\cdot P[{\mathcal{A}}(D_{2})\in S]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p id="S2.SS1.p3.9" class="ltx_p">In other words, looking at the output of the algorithm, one cannot distinguish whether any individual’s data was included in the input dataset or not.
The level of that indistinguishability is measured by <math id="S2.SS1.p3.9.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS1.p3.9.m1.1a"><mi id="S2.SS1.p3.9.m1.1.1" xref="S2.SS1.p3.9.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.9.m1.1b"><ci id="S2.SS1.p3.9.m1.1.1.cmml" xref="S2.SS1.p3.9.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.9.m1.1c">\epsilon</annotation></semantics></math>, also called a privacy budget.</p>
</div>
<div id="S2.SS1.p4" class="ltx_para">
<p id="S2.SS1.p4.4" class="ltx_p">In the context of machine learning, <math id="S2.SS1.p4.1.m1.1" class="ltx_Math" alttext="\mathcal{A}" display="inline"><semantics id="S2.SS1.p4.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS1.p4.1.m1.1.1" xref="S2.SS1.p4.1.m1.1.1.cmml">𝒜</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.1.m1.1b"><ci id="S2.SS1.p4.1.m1.1.1.cmml" xref="S2.SS1.p4.1.m1.1.1">𝒜</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.1.m1.1c">\mathcal{A}</annotation></semantics></math> is usually the training procedure. In this paper, we focus on three DP techniques: the Laplace mechanism <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dwork et al.</span>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2006b</span></a>)</cite>, DP-SGD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Abadi et al.</span>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>, and PATE <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Papernot et al.</span>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> (for more details, see Sec. <a href="#S2.SS3" title="2.3 Generative Models ‣ 2 Preliminaries ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2.3</span></a>).
The last two techniques use a relaxation of DP called (<math id="S2.SS1.p4.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS1.p4.2.m2.1a"><mi id="S2.SS1.p4.2.m2.1.1" xref="S2.SS1.p4.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.2.m2.1b"><ci id="S2.SS1.p4.2.m2.1.1.cmml" xref="S2.SS1.p4.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.2.m2.1c">\epsilon</annotation></semantics></math>, <math id="S2.SS1.p4.3.m3.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.SS1.p4.3.m3.1a"><mi id="S2.SS1.p4.3.m3.1.1" xref="S2.SS1.p4.3.m3.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.3.m3.1b"><ci id="S2.SS1.p4.3.m3.1.1.cmml" xref="S2.SS1.p4.3.m3.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.3.m3.1c">\delta</annotation></semantics></math>)-DP <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dwork et al.</span>, <a href="#bib.bib18" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2014</span></a>)</cite>; here, <math id="S2.SS1.p4.4.m4.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.SS1.p4.4.m4.1a"><mi id="S2.SS1.p4.4.m4.1.1" xref="S2.SS1.p4.4.m4.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p4.4.m4.1b"><ci id="S2.SS1.p4.4.m4.1.1.cmml" xref="S2.SS1.p4.4.m4.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p4.4.m4.1c">\delta</annotation></semantics></math>, usually a small number, denotes a probability of failure.
Finally, due to its robustness to post-processing, DP allows for DP-trained models to be re-used without further privacy leakage.</p>
</div>
<div id="S2.SS1.p5" class="ltx_para ltx_noindent">
<p id="S2.SS1.p5.1" class="ltx_p"><span id="S2.SS1.p5.1.1" class="ltx_text ltx_font_bold">Disparate Impact Metrics.</span>
For the downstream task evaluation (see Sec. <a href="#S3.SS1" title="3.1 Evaluation Methodology ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.1</span></a>), we follow the disparate impact metrics proposed in <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bagdasaryan et al.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> and use <span id="S2.SS1.p5.1.2" class="ltx_text ltx_font_italic">accuracy parity</span>, a weaker form of “equal odds” <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Hardt et al.</span>, <a href="#bib.bib24" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>.
Specifically, we focus on model accuracy on imbalanced classes (and multi-classes) and imbalanced subgroups (with balanced classes) of the dataset.
Similarly to <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bagdasaryan et al.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, we do not consider (other) fairness evaluations, leaving them as items for future work.</p>
</div>
</section>
<section id="S2.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Datasets</h3>

<div id="S2.SS2.p1" class="ltx_para">
<p id="S2.SS2.p1.1" class="ltx_p">We consider several tabular and one image datasets from different domains, which are widely used in the ML research community.
All have an associated classification task or have slightly been modified for this purpose.</p>
</div>
<div id="S2.SS2.p2" class="ltx_para ltx_noindent">
<p id="S2.SS2.p2.1" class="ltx_p"><span id="S2.SS2.p2.1.1" class="ltx_text ltx_font_bold">Adult.</span>
The Adult dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dua &amp; Graff</span>, <a href="#bib.bib15" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> is extracted from the 1994 Census database, consisting of 32,561 training and 16,281 testing records.
It has 15 attributes: 6 numerical, including age, and 9 categorical, including sex and race.
The target column indicates whether the individual’s income exceeds $50K/year.</p>
</div>
<div id="S2.SS2.p3" class="ltx_para ltx_noindent">
<p id="S2.SS2.p3.1" class="ltx_p"><span id="S2.SS2.p3.1.1" class="ltx_text ltx_font_bold">Texas.</span>
The Texas Hospital Inpatient Discharge dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">DSHS</span>, <a href="#bib.bib14" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2013</span></a>)</cite> contains data on discharges from Texas hospitals.
As done in previous work <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Stadler et al.</span>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>, we sample 49,983 records from 2013 and select 12 attributes, 1 numerical and 11 categorical, including age, sex, and race.
To create a classification task, we convert the numerical attribute, indicating the length of stay in the hospital, into a categorical one by specifying whether the person’s hospitalization was a week or longer.</p>
</div>
<div id="S2.SS2.p4" class="ltx_para ltx_noindent">
<p id="S2.SS2.p4.1" class="ltx_p"><span id="S2.SS2.p4.1.1" class="ltx_text ltx_font_bold">Purchases.</span>
The Purchases dataset is based on Kaggle’s “Acquire Valued Shoppers Challenge” <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kaggle</span>, <a href="#bib.bib30" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2013</span></a>)</cite>, aimed at predicting whether customers would become loyal to products based on incentives.
As done in previous work <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Shokri et al.</span>, <a href="#bib.bib45" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>, we modify the main task to be predicting customers’ purchase style.
First, we filter products purchased at least 750,000 times and customers who made at least 500 purchases.
Then, we summarize the data so that each row represents a customer with 108 binary features, corresponding to whether the customer has bought that product or not.
Finally, to create the different purchasing styles, we cluster the customers into 20 clusters using a Mixture of Gaussian models.
This yields a dataset with 152,369 customers and 109 attributes, including the style.
Unlike the previous datasets, the classification task here is multi-class.</p>
</div>
<div id="S2.SS2.p5" class="ltx_para ltx_noindent">
<p id="S2.SS2.p5.1" class="ltx_p"><span id="S2.SS2.p5.1.1" class="ltx_text ltx_font_bold">MNIST.</span>
The MNIST dataset <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">LeCun et al.</span>, <a href="#bib.bib33" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2010</span></a>)</cite> consist of 60,000 training and 10,000 testing black and white handwritten 784-pixel digits.
The goal is to classify the digit, making it a multi-class problem with 10 classes.</p>
</div>
</section>
<section id="S2.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Generative Models</h3>

<div id="S2.SS3.p1" class="ltx_para">
<p id="S2.SS3.p1.1" class="ltx_p">Our evaluation includes three of the most popular DP generative models: a statistical one based on Bayesian networks and two GANs incorporating DP mechanisms.
Unless stated otherwise, we use the default hyperparameters, as provided by the authors.</p>
</div>
<div id="S2.SS3.p2" class="ltx_para ltx_noindent">
<p id="S2.SS3.p2.1" class="ltx_p"><span id="S2.SS3.p2.1.1" class="ltx_text ltx_font_bold">PrivBayes.</span> PrivBayes <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Zhang et al.</span>, <a href="#bib.bib58" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>; <span class="ltx_text" style="font-size:90%;">Ping et al.</span>, <a href="#bib.bib44" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite> first constructs an optimal Bayesian network that approximates the joint data distribution by low-dimensional conditional distributions and then estimates them.
Both of these steps are done with <math id="S2.SS3.p2.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS3.p2.1.m1.1a"><mi id="S2.SS3.p2.1.m1.1.1" xref="S2.SS3.p2.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p2.1.m1.1b"><ci id="S2.SS3.p2.1.m1.1.1.cmml" xref="S2.SS3.p2.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p2.1.m1.1c">\epsilon</annotation></semantics></math>-DP guarantees, respectively, using the Exponential Mechanism <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">McSherry &amp; Talwar</span>, <a href="#bib.bib36" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2007</span></a>)</cite> to choose the parents for each child node and the Laplace Mechanism <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dwork et al.</span>, <a href="#bib.bib17" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2006b</span></a>)</cite> to construct noisy contingency tables before converting them to distributions.
Looking at the step involving Laplace Mechanism in more detail, any negative noisy counts are clipped at 0 before being normalized to a distribution, potentially leading to a biased estimator.
We discretize numerical columns to 50 bins, as opposed to 20, and set the degree of the network to 3 for all datasets except for Purchases, where it is 2.
Furthermore, we identified an industry-wide bug in the open-source package violating the DP guarantees and fixed it.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a target="_blank" href="https://github.com/DataResponsibly/DataSynthesizer/issues/34" title="" class="ltx_ref ltx_url">https://github.com/DataResponsibly/DataSynthesizer/issues/34</a></span></span></span></p>
</div>
<div id="S2.SS3.p3" class="ltx_para ltx_noindent">
<p id="S2.SS3.p3.4" class="ltx_p"><span id="S2.SS3.p3.4.1" class="ltx_text ltx_font_bold">DP-WGAN.</span> DP-WGAN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Alzantot &amp; Srivastava</span>, <a href="#bib.bib3" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> is one of the top 5 solutions to the 2018 NIST Contest <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">NIST</span>, <a href="#bib.bib38" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018a</span></a>)</cite>.
It relies on the WGAN architecture <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Arjovsky et al.</span>, <a href="#bib.bib4" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2017</span></a>)</cite>, which improves training stability and performance by using the Wasserstein distance instead of the Jensen-Shannon divergence as in GANs.
Furthermore, (<math id="S2.SS3.p3.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS3.p3.1.m1.1a"><mi id="S2.SS3.p3.1.m1.1.1" xref="S2.SS3.p3.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.1.m1.1b"><ci id="S2.SS3.p3.1.m1.1.1.cmml" xref="S2.SS3.p3.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.1.m1.1c">\epsilon</annotation></semantics></math>, <math id="S2.SS3.p3.2.m2.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.SS3.p3.2.m2.1a"><mi id="S2.SS3.p3.2.m2.1.1" xref="S2.SS3.p3.2.m2.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.2.m2.1b"><ci id="S2.SS3.p3.2.m2.1.1.cmml" xref="S2.SS3.p3.2.m2.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.2.m2.1c">\delta</annotation></semantics></math>)-DP of the output is achieved using DP-SGD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Abadi et al.</span>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>, which sanitizes the gradients (clips the <math id="S2.SS3.p3.3.m3.1" class="ltx_Math" alttext="\ell_{2}" display="inline"><semantics id="S2.SS3.p3.3.m3.1a"><msub id="S2.SS3.p3.3.m3.1.1" xref="S2.SS3.p3.3.m3.1.1.cmml"><mi mathvariant="normal" id="S2.SS3.p3.3.m3.1.1.2" xref="S2.SS3.p3.3.m3.1.1.2.cmml">ℓ</mi><mn id="S2.SS3.p3.3.m3.1.1.3" xref="S2.SS3.p3.3.m3.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.3.m3.1b"><apply id="S2.SS3.p3.3.m3.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS3.p3.3.m3.1.1.1.cmml" xref="S2.SS3.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS3.p3.3.m3.1.1.2.cmml" xref="S2.SS3.p3.3.m3.1.1.2">ℓ</ci><cn type="integer" id="S2.SS3.p3.3.m3.1.1.3.cmml" xref="S2.SS3.p3.3.m3.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.3.m3.1c">\ell_{2}</annotation></semantics></math> norm of the individual gradients and applies Gaussian Mechanism <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Dwork et al.</span>, <a href="#bib.bib16" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2006a</span></a>)</cite> to the sum) of the discriminator during training.
The privacy budget is tracked using the moments accountant method <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Abadi et al.</span>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite>.
To be consistent with PATE-GAN (see below), we set <math id="S2.SS3.p3.4.m4.1" class="ltx_Math" alttext="\delta=10^{-5}" display="inline"><semantics id="S2.SS3.p3.4.m4.1a"><mrow id="S2.SS3.p3.4.m4.1.1" xref="S2.SS3.p3.4.m4.1.1.cmml"><mi id="S2.SS3.p3.4.m4.1.1.2" xref="S2.SS3.p3.4.m4.1.1.2.cmml">δ</mi><mo id="S2.SS3.p3.4.m4.1.1.1" xref="S2.SS3.p3.4.m4.1.1.1.cmml">=</mo><msup id="S2.SS3.p3.4.m4.1.1.3" xref="S2.SS3.p3.4.m4.1.1.3.cmml"><mn id="S2.SS3.p3.4.m4.1.1.3.2" xref="S2.SS3.p3.4.m4.1.1.3.2.cmml">10</mn><mrow id="S2.SS3.p3.4.m4.1.1.3.3" xref="S2.SS3.p3.4.m4.1.1.3.3.cmml"><mo id="S2.SS3.p3.4.m4.1.1.3.3a" xref="S2.SS3.p3.4.m4.1.1.3.3.cmml">−</mo><mn id="S2.SS3.p3.4.m4.1.1.3.3.2" xref="S2.SS3.p3.4.m4.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p3.4.m4.1b"><apply id="S2.SS3.p3.4.m4.1.1.cmml" xref="S2.SS3.p3.4.m4.1.1"><eq id="S2.SS3.p3.4.m4.1.1.1.cmml" xref="S2.SS3.p3.4.m4.1.1.1"></eq><ci id="S2.SS3.p3.4.m4.1.1.2.cmml" xref="S2.SS3.p3.4.m4.1.1.2">𝛿</ci><apply id="S2.SS3.p3.4.m4.1.1.3.cmml" xref="S2.SS3.p3.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p3.4.m4.1.1.3.1.cmml" xref="S2.SS3.p3.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS3.p3.4.m4.1.1.3.2.cmml" xref="S2.SS3.p3.4.m4.1.1.3.2">10</cn><apply id="S2.SS3.p3.4.m4.1.1.3.3.cmml" xref="S2.SS3.p3.4.m4.1.1.3.3"><minus id="S2.SS3.p3.4.m4.1.1.3.3.1.cmml" xref="S2.SS3.p3.4.m4.1.1.3.3"></minus><cn type="integer" id="S2.SS3.p3.4.m4.1.1.3.3.2.cmml" xref="S2.SS3.p3.4.m4.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p3.4.m4.1c">\delta=10^{-5}</annotation></semantics></math> for all experiments.</p>
</div>
<div id="S2.SS3.p4" class="ltx_para ltx_noindent">
<p id="S2.SS3.p4.4" class="ltx_p"><span id="S2.SS3.p4.4.1" class="ltx_text ltx_font_bold">PATE-GAN.</span> PATE-GAN <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jordon et al.</span>, <a href="#bib.bib29" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> adapts the PATE framework <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Papernot et al.</span>, <a href="#bib.bib40" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>, <a href="#bib.bib41" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>)</cite> for training GANs.
Instead of a single discriminator, there are <math id="S2.SS3.p4.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S2.SS3.p4.1.m1.1a"><mi id="S2.SS3.p4.1.m1.1.1" xref="S2.SS3.p4.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.1.m1.1b"><ci id="S2.SS3.p4.1.m1.1.1.cmml" xref="S2.SS3.p4.1.m1.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.1.m1.1c">k</annotation></semantics></math> teacher-discriminators and a student-discriminator.
The teacher-discriminators only see a disjoint partition of the real data. They are trained to minimize the classification loss when classifying samples as real or fake. In contrast, the student-discriminator is trained using noisy labels (using the Laplace Mechanism) predicted by the teachers.
As before, the privacy budget of the algorithm is calculated using the moments accountant <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Abadi et al.</span>, <a href="#bib.bib1" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2016</span></a>)</cite> and the output is (<math id="S2.SS3.p4.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS3.p4.2.m2.1a"><mi id="S2.SS3.p4.2.m2.1.1" xref="S2.SS3.p4.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.2.m2.1b"><ci id="S2.SS3.p4.2.m2.1.1.cmml" xref="S2.SS3.p4.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.2.m2.1c">\epsilon</annotation></semantics></math>, <math id="S2.SS3.p4.3.m3.1" class="ltx_Math" alttext="\delta" display="inline"><semantics id="S2.SS3.p4.3.m3.1a"><mi id="S2.SS3.p4.3.m3.1.1" xref="S2.SS3.p4.3.m3.1.1.cmml">δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.3.m3.1b"><ci id="S2.SS3.p4.3.m3.1.1.cmml" xref="S2.SS3.p4.3.m3.1.1">𝛿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.3.m3.1c">\delta</annotation></semantics></math>)-DP, with <math id="S2.SS3.p4.4.m4.1" class="ltx_Math" alttext="\delta=10^{-5}" display="inline"><semantics id="S2.SS3.p4.4.m4.1a"><mrow id="S2.SS3.p4.4.m4.1.1" xref="S2.SS3.p4.4.m4.1.1.cmml"><mi id="S2.SS3.p4.4.m4.1.1.2" xref="S2.SS3.p4.4.m4.1.1.2.cmml">δ</mi><mo id="S2.SS3.p4.4.m4.1.1.1" xref="S2.SS3.p4.4.m4.1.1.1.cmml">=</mo><msup id="S2.SS3.p4.4.m4.1.1.3" xref="S2.SS3.p4.4.m4.1.1.3.cmml"><mn id="S2.SS3.p4.4.m4.1.1.3.2" xref="S2.SS3.p4.4.m4.1.1.3.2.cmml">10</mn><mrow id="S2.SS3.p4.4.m4.1.1.3.3" xref="S2.SS3.p4.4.m4.1.1.3.3.cmml"><mo id="S2.SS3.p4.4.m4.1.1.3.3a" xref="S2.SS3.p4.4.m4.1.1.3.3.cmml">−</mo><mn id="S2.SS3.p4.4.m4.1.1.3.3.2" xref="S2.SS3.p4.4.m4.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS3.p4.4.m4.1b"><apply id="S2.SS3.p4.4.m4.1.1.cmml" xref="S2.SS3.p4.4.m4.1.1"><eq id="S2.SS3.p4.4.m4.1.1.1.cmml" xref="S2.SS3.p4.4.m4.1.1.1"></eq><ci id="S2.SS3.p4.4.m4.1.1.2.cmml" xref="S2.SS3.p4.4.m4.1.1.2">𝛿</ci><apply id="S2.SS3.p4.4.m4.1.1.3.cmml" xref="S2.SS3.p4.4.m4.1.1.3"><csymbol cd="ambiguous" id="S2.SS3.p4.4.m4.1.1.3.1.cmml" xref="S2.SS3.p4.4.m4.1.1.3">superscript</csymbol><cn type="integer" id="S2.SS3.p4.4.m4.1.1.3.2.cmml" xref="S2.SS3.p4.4.m4.1.1.3.2">10</cn><apply id="S2.SS3.p4.4.m4.1.1.3.3.cmml" xref="S2.SS3.p4.4.m4.1.1.3.3"><minus id="S2.SS3.p4.4.m4.1.1.3.3.1.cmml" xref="S2.SS3.p4.4.m4.1.1.3.3"></minus><cn type="integer" id="S2.SS3.p4.4.m4.1.1.3.3.2.cmml" xref="S2.SS3.p4.4.m4.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p4.4.m4.1c">\delta=10^{-5}</annotation></semantics></math>.</p>
</div>
</section>
<section id="S2.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.4 </span>Discriminative Models</h3>

<div id="S2.SS4.p1" class="ltx_para">
<p id="S2.SS4.p1.1" class="ltx_p">As mentioned, our experiments include a downstream task (classification) run on the synthetic data.
We use Logistic Regression (LR) to avoid another layer of stochasticity; specifically two versions of LR: the standard one in Scikit-Learn
 <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Pedregosa et al.</span>, <a href="#bib.bib42" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2011</span></a>)</cite> and one with DP guarantees <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Chaudhuri et al.</span>, <a href="#bib.bib10" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2011</span></a>; <span class="ltx_text" style="font-size:90%;">Holohan et al.</span>, <a href="#bib.bib26" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>.
The latter achieves <math id="S2.SS4.p1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S2.SS4.p1.1.m1.1a"><mi id="S2.SS4.p1.1.m1.1.1" xref="S2.SS4.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S2.SS4.p1.1.m1.1b"><ci id="S2.SS4.p1.1.m1.1.1.cmml" xref="S2.SS4.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS4.p1.1.m1.1c">\epsilon</annotation></semantics></math>-DP by perturbing the objective function before optimization.</p>
</div>
</section>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experimental Evaluation</h2>

<section id="S3.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Evaluation Methodology</h3>

<div id="S3.SS1.p1" class="ltx_para">
<p id="S3.SS1.p1.1" class="ltx_p">Broadly, our goal is to empirically measure the impact of generative models with different DP mechanisms, <math id="S3.SS1.p1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS1.p1.1.m1.1a"><mi id="S3.SS1.p1.1.m1.1.1" xref="S3.SS1.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\epsilon</annotation></semantics></math> levels, and data imbalance ratios have on class/subgroups distributions in the generated synthetic data and downstream task performance.
We consider four settings:</p>
<ol id="S3.I1" class="ltx_enumerate">
<li id="S3.I1.ix1" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">S1)</span> 
<div id="S3.I1.ix1.p1" class="ltx_para">
<p id="S3.I1.ix1.p1.1" class="ltx_p"><span id="S3.I1.ix1.p1.1.1" class="ltx_text ltx_font_italic">Binary class size, precision, and recall.</span> We focus on the effect on binary classes,
reporting class recall and precision because the target columns in all datasets are imbalanced.</p>
</div>
</li>
<li id="S3.I1.ix2" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">S2)</span> 
<div id="S3.I1.ix2.p1" class="ltx_para">
<p id="S3.I1.ix2.p1.1" class="ltx_p"><span id="S3.I1.ix2.p1.1.1" class="ltx_text ltx_font_italic">Multi-class size, precision, and recall.</span> We study the effect on multi-classes.
As in the previous setting, we report class recall and precision.</p>
</div>
</li>
<li id="S3.I1.ix3" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">S3)</span> 
<div id="S3.I1.ix3.p1" class="ltx_para">
<p id="S3.I1.ix3.p1.1" class="ltx_p"><span id="S3.I1.ix3.p1.1.1" class="ltx_text ltx_font_italic">Single-attribute subgroup size, accuracy, and correlation.</span> We analyze the effect on a single-attribute subgroup.
Here, we treat a single feature (e.g., sex) as a subgroup.
We imbalance the dataset, so the minority subgroup comprises the desired ratio of the population while keeping the class per subgroup balanced.</p>
</div>
</li>
<li id="S3.I1.ix4" class="ltx_item" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">S4)</span> 
<div id="S3.I1.ix4.p1" class="ltx_para">
<p id="S3.I1.ix4.p1.1" class="ltx_p"><span id="S3.I1.ix4.p1.1.1" class="ltx_text ltx_font_italic">Multi-attribute subgroup size, accuracy, and correlation.</span> We focus on the effect on multi-attribute subgroups.
We treat an intersection of features (e.g., age, sex, and race) as small fine-grained subgroups.
As in the previous setting, we balance the data only according to a single-attribute subgroup; otherwise, we risk throwing too much data out.
Thus, we discard subgroups with fewer than 25 members.</p>
</div>
</li>
</ol>
</div>
<div id="S3.SS1.p2" class="ltx_para">
<p id="S3.SS1.p2.1" class="ltx_p">All evaluation settings follow three steps: dataset preparation, synthetic data generation, and prediction – see below.</p>
</div>
<div id="S3.SS1.p3" class="ltx_para ltx_noindent">
<p id="S3.SS1.p3.1" class="ltx_p"><span id="S3.SS1.p3.1.1" class="ltx_text ltx_font_bold">Dataset Preparation.</span>
First, we split the dataset into training and testing if the latter is not explicitly provided.
If the subgroup imbalance level is provided (S3 and S4), for both training and testing datasets, we balance the subgroup by class, so there are 50% of each class per subgroup (we only consider binary classes in these settings).</p>
</div>
<div id="S3.SS1.p4" class="ltx_para">
<p id="S3.SS1.p4.1" class="ltx_p">Then, we imbalance the datasets to the desired subgroup imbalance level (while maintaining class parity), where the level represents the ratio of minority subgroup to the total size of the dataset.</p>
</div>
<div id="S3.SS1.p5" class="ltx_para ltx_noindent">
<p id="S3.SS1.p5.6" class="ltx_p"><span id="S3.SS1.p5.6.1" class="ltx_text ltx_font_bold">Synthetic Data Generation.</span>
For a given generative model and privacy budget <math id="S3.SS1.p5.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS1.p5.1.m1.1a"><mi id="S3.SS1.p5.1.m1.1.1" xref="S3.SS1.p5.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.m1.1b"><ci id="S3.SS1.p5.1.m1.1.1.cmml" xref="S3.SS1.p5.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.m1.1c">\epsilon</annotation></semantics></math>, we train <math id="S3.SS1.p5.2.m2.1" class="ltx_Math" alttext="l" display="inline"><semantics id="S3.SS1.p5.2.m2.1a"><mi id="S3.SS1.p5.2.m2.1.1" xref="S3.SS1.p5.2.m2.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.2.m2.1b"><ci id="S3.SS1.p5.2.m2.1.1.cmml" xref="S3.SS1.p5.2.m2.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.2.m2.1c">l</annotation></semantics></math> (we set <math id="S3.SS1.p5.3.m3.1" class="ltx_Math" alttext="l=10" display="inline"><semantics id="S3.SS1.p5.3.m3.1a"><mrow id="S3.SS1.p5.3.m3.1.1" xref="S3.SS1.p5.3.m3.1.1.cmml"><mi id="S3.SS1.p5.3.m3.1.1.2" xref="S3.SS1.p5.3.m3.1.1.2.cmml">l</mi><mo id="S3.SS1.p5.3.m3.1.1.1" xref="S3.SS1.p5.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS1.p5.3.m3.1.1.3" xref="S3.SS1.p5.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.3.m3.1b"><apply id="S3.SS1.p5.3.m3.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1"><eq id="S3.SS1.p5.3.m3.1.1.1.cmml" xref="S3.SS1.p5.3.m3.1.1.1"></eq><ci id="S3.SS1.p5.3.m3.1.1.2.cmml" xref="S3.SS1.p5.3.m3.1.1.2">𝑙</ci><cn type="integer" id="S3.SS1.p5.3.m3.1.1.3.cmml" xref="S3.SS1.p5.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.3.m3.1c">l=10</annotation></semantics></math>) generators and generate <math id="S3.SS1.p5.4.m4.1" class="ltx_Math" alttext="k" display="inline"><semantics id="S3.SS1.p5.4.m4.1a"><mi id="S3.SS1.p5.4.m4.1.1" xref="S3.SS1.p5.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.4.m4.1b"><ci id="S3.SS1.p5.4.m4.1.1.cmml" xref="S3.SS1.p5.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.4.m4.1c">k</annotation></semantics></math> (we set <math id="S3.SS1.p5.5.m5.1" class="ltx_Math" alttext="k=10" display="inline"><semantics id="S3.SS1.p5.5.m5.1a"><mrow id="S3.SS1.p5.5.m5.1.1" xref="S3.SS1.p5.5.m5.1.1.cmml"><mi id="S3.SS1.p5.5.m5.1.1.2" xref="S3.SS1.p5.5.m5.1.1.2.cmml">k</mi><mo id="S3.SS1.p5.5.m5.1.1.1" xref="S3.SS1.p5.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS1.p5.5.m5.1.1.3" xref="S3.SS1.p5.5.m5.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.5.m5.1b"><apply id="S3.SS1.p5.5.m5.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1"><eq id="S3.SS1.p5.5.m5.1.1.1.cmml" xref="S3.SS1.p5.5.m5.1.1.1"></eq><ci id="S3.SS1.p5.5.m5.1.1.2.cmml" xref="S3.SS1.p5.5.m5.1.1.2">𝑘</ci><cn type="integer" id="S3.SS1.p5.5.m5.1.1.3.cmml" xref="S3.SS1.p5.5.m5.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.5.m5.1c">k=10</annotation></semantics></math>) synthetic datasets with size equal to the input dataset.
This results in <math id="S3.SS1.p5.6.m6.1" class="ltx_Math" alttext="l\cdot{k}" display="inline"><semantics id="S3.SS1.p5.6.m6.1a"><mrow id="S3.SS1.p5.6.m6.1.1" xref="S3.SS1.p5.6.m6.1.1.cmml"><mi id="S3.SS1.p5.6.m6.1.1.2" xref="S3.SS1.p5.6.m6.1.1.2.cmml">l</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p5.6.m6.1.1.1" xref="S3.SS1.p5.6.m6.1.1.1.cmml">⋅</mo><mi id="S3.SS1.p5.6.m6.1.1.3" xref="S3.SS1.p5.6.m6.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.6.m6.1b"><apply id="S3.SS1.p5.6.m6.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1"><ci id="S3.SS1.p5.6.m6.1.1.1.cmml" xref="S3.SS1.p5.6.m6.1.1.1">⋅</ci><ci id="S3.SS1.p5.6.m6.1.1.2.cmml" xref="S3.SS1.p5.6.m6.1.1.2">𝑙</ci><ci id="S3.SS1.p5.6.m6.1.1.3.cmml" xref="S3.SS1.p5.6.m6.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.6.m6.1c">l\cdot{k}</annotation></semantics></math> synthetic datasets.
We measure the class/subgroups distributions.
If single/multi-attribute subgroup is provided (S3 and S4), we also measure correlation between the subgroup and target columns by calculating the mutual information.</p>
</div>
<div id="S3.SS1.p6" class="ltx_para ltx_noindent">
<p id="S3.SS1.p6.2" class="ltx_p"><span id="S3.SS1.p6.2.1" class="ltx_text ltx_font_bold">Classifiers Prediction.</span>
We capture the performance of three types of classifiers: 1) <em id="S3.SS1.p6.2.2" class="ltx_emph ltx_font_italic">real classifier</em> – we train a single LR on the real dataset and predict on the test dataset to serve as an overall baseline; 2) <em id="S3.SS1.p6.2.3" class="ltx_emph ltx_font_italic">DP classifiers</em> – we train <math id="S3.SS1.p6.1.m1.1" class="ltx_Math" alttext="l\cdot{k}" display="inline"><semantics id="S3.SS1.p6.1.m1.1a"><mrow id="S3.SS1.p6.1.m1.1.1" xref="S3.SS1.p6.1.m1.1.1.cmml"><mi id="S3.SS1.p6.1.m1.1.1.2" xref="S3.SS1.p6.1.m1.1.1.2.cmml">l</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p6.1.m1.1.1.1" xref="S3.SS1.p6.1.m1.1.1.1.cmml">⋅</mo><mi id="S3.SS1.p6.1.m1.1.1.3" xref="S3.SS1.p6.1.m1.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.m1.1b"><apply id="S3.SS1.p6.1.m1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1"><ci id="S3.SS1.p6.1.m1.1.1.1.cmml" xref="S3.SS1.p6.1.m1.1.1.1">⋅</ci><ci id="S3.SS1.p6.1.m1.1.1.2.cmml" xref="S3.SS1.p6.1.m1.1.1.2">𝑙</ci><ci id="S3.SS1.p6.1.m1.1.1.3.cmml" xref="S3.SS1.p6.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.m1.1c">l\cdot{k}</annotation></semantics></math> (equal to the number of synthetic datasets) DP LRs on the real dataset and predict on the test dataset; 3) <em id="S3.SS1.p6.2.4" class="ltx_emph ltx_font_italic">synth classifiers</em> – we train a single LR per synthetic dataset (in total <math id="S3.SS1.p6.2.m2.1" class="ltx_Math" alttext="l\cdot{k}" display="inline"><semantics id="S3.SS1.p6.2.m2.1a"><mrow id="S3.SS1.p6.2.m2.1.1" xref="S3.SS1.p6.2.m2.1.1.cmml"><mi id="S3.SS1.p6.2.m2.1.1.2" xref="S3.SS1.p6.2.m2.1.1.2.cmml">l</mi><mo lspace="0.222em" rspace="0.222em" id="S3.SS1.p6.2.m2.1.1.1" xref="S3.SS1.p6.2.m2.1.1.1.cmml">⋅</mo><mi id="S3.SS1.p6.2.m2.1.1.3" xref="S3.SS1.p6.2.m2.1.1.3.cmml">k</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.2.m2.1b"><apply id="S3.SS1.p6.2.m2.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1"><ci id="S3.SS1.p6.2.m2.1.1.1.cmml" xref="S3.SS1.p6.2.m2.1.1.1">⋅</ci><ci id="S3.SS1.p6.2.m2.1.1.2.cmml" xref="S3.SS1.p6.2.m2.1.1.2">𝑙</ci><ci id="S3.SS1.p6.2.m2.1.1.3.cmml" xref="S3.SS1.p6.2.m2.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.2.m2.1c">l\cdot{k}</annotation></semantics></math>) and predict on the test dataset.</p>
</div>
<figure id="S3.F1" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x1.png" id="S3.F1.sf1.g1" class="ltx_graphics ltx_img_landscape" width="156" height="96" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F1.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F1.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x2.png" id="S3.F1.sf2.g1" class="ltx_graphics ltx_img_landscape" width="157" height="96" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F1.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F1.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x3.png" id="S3.F1.sf3.g1" class="ltx_graphics ltx_img_landscape" width="157" height="96" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F1.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S3.F1.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Synthetic data class size (top) and real, DP, and synthetic classifiers precision (bottom) for different levels of <math id="S3.F1.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.F1.2.m1.1b"><mi id="S3.F1.2.m1.1.1" xref="S3.F1.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.F1.2.m1.1c"><ci id="S3.F1.2.m1.1.1.cmml" xref="S3.F1.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F1.2.m1.1d">\epsilon</annotation></semantics></math>, <span id="S3.F1.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Adult</span>, (<span id="S3.F1.6.2" class="ltx_text ltx_font_bold ltx_font_italic">S1</span>).</figcaption>
</figure>
</section>
<section id="S3.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>S1: Binary Class Size and Precision</h3>

<div id="S3.SS2.p1" class="ltx_para">
<p id="S3.SS2.p1.1" class="ltx_p">We consider privacy budgets (<math id="S3.SS2.p1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\epsilon</annotation></semantics></math>) of 0.01, 0.1, 1, 10, 100, and infinity (“no-DP”) for the binary classification datasets (Adult and Texas).
We do not imbalance the data because all datasets already have imbalanced classes – specifically, the proportion of the minority class to the total number of records is 0.24 in Adult and 0.195 in Texas.</p>
</div>
<div id="S3.SS2.p2" class="ltx_para ltx_noindent">
<p id="S3.SS2.p2.5" class="ltx_p"><span id="S3.SS2.p2.5.1" class="ltx_text ltx_font_bold">Size.</span>
In the first row of Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.1 Evaluation Methodology ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (and <a href="#A1.F12.11" title="Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> in Appendix <a href="#A1.SS1" title="A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>), we plot the class size distribution in the synthetic data for Adult and Texas, respectively.
For both datasets, for PrivBayes, we observe that decreasing <math id="S3.SS2.p2.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p2.1.m1.1a"><mi id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><ci id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">\epsilon</annotation></semantics></math> results in synthetic data with reduced class imbalance; for PATE-GAN, the opposite is true – decreasing <math id="S3.SS2.p2.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p2.2.m2.1a"><mi id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><ci id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">\epsilon</annotation></semantics></math> leads to increased class imbalance (except for <math id="S3.SS2.p2.3.m3.1" class="ltx_Math" alttext="\epsilon=0.01" display="inline"><semantics id="S3.SS2.p2.3.m3.1a"><mrow id="S3.SS2.p2.3.m3.1.1" xref="S3.SS2.p2.3.m3.1.1.cmml"><mi id="S3.SS2.p2.3.m3.1.1.2" xref="S3.SS2.p2.3.m3.1.1.2.cmml">ϵ</mi><mo id="S3.SS2.p2.3.m3.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS2.p2.3.m3.1.1.3" xref="S3.SS2.p2.3.m3.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.1b"><apply id="S3.SS2.p2.3.m3.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1"><eq id="S3.SS2.p2.3.m3.1.1.1.cmml" xref="S3.SS2.p2.3.m3.1.1.1"></eq><ci id="S3.SS2.p2.3.m3.1.1.2.cmml" xref="S3.SS2.p2.3.m3.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS2.p2.3.m3.1.1.3.cmml" xref="S3.SS2.p2.3.m3.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.1c">\epsilon=0.01</annotation></semantics></math> for Texas).
These results are consistent with the disparate effects from applying Laplace to DP statistics <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Kuppam et al.</span>, <a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> as well as PATE to DP neural networks classifiers <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Uniyal et al.</span>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
Interestingly, DP-WGAN preserves the imbalance for <math id="S3.SS2.p2.4.m4.1" class="ltx_Math" alttext="\epsilon&gt;0.1" display="inline"><semantics id="S3.SS2.p2.4.m4.1a"><mrow id="S3.SS2.p2.4.m4.1.1" xref="S3.SS2.p2.4.m4.1.1.cmml"><mi id="S3.SS2.p2.4.m4.1.1.2" xref="S3.SS2.p2.4.m4.1.1.2.cmml">ϵ</mi><mo id="S3.SS2.p2.4.m4.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.cmml">&gt;</mo><mn id="S3.SS2.p2.4.m4.1.1.3" xref="S3.SS2.p2.4.m4.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.1b"><apply id="S3.SS2.p2.4.m4.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1"><gt id="S3.SS2.p2.4.m4.1.1.1.cmml" xref="S3.SS2.p2.4.m4.1.1.1"></gt><ci id="S3.SS2.p2.4.m4.1.1.2.cmml" xref="S3.SS2.p2.4.m4.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS2.p2.4.m4.1.1.3.cmml" xref="S3.SS2.p2.4.m4.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.1c">\epsilon&gt;0.1</annotation></semantics></math>.
As expected, there is an increased standard deviation for smaller values of <math id="S3.SS2.p2.5.m5.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p2.5.m5.1a"><mi id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><ci id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">\epsilon</annotation></semantics></math> for all synthetic datasets.</p>
</div>
<div id="S3.SS2.p3" class="ltx_para ltx_noindent">
<p id="S3.SS2.p3.1" class="ltx_p"><span id="S3.SS2.p3.1.1" class="ltx_text ltx_font_bold">Precision.</span>
In the bottom rows of Fig. <a href="#S3.F1" title="Figure 1 ‣ 3.1 Evaluation Methodology ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> (and <a href="#A1.F12.11" title="Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> in Appendix <a href="#A1.SS1" title="A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>) we plot the precision of the real, DP, and synth classifiers on the two datasets (recall plots are also in Appendix <a href="#A1.SS1" title="A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.1</span></a>).
For the DP classifiers, we find that precision drops disproportionately more for the underrepresented class when decreasing <math id="S3.SS2.p3.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS2.p3.1.m1.1a"><mi id="S3.SS2.p3.1.m1.1.1" xref="S3.SS2.p3.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.m1.1b"><ci id="S3.SS2.p3.1.m1.1.1.cmml" xref="S3.SS2.p3.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.m1.1c">\epsilon</annotation></semantics></math> for all datasets.</p>
</div>
<div id="S3.SS2.p4" class="ltx_para">
<p id="S3.SS2.p4.1" class="ltx_p">Synth classifiers follow very similar behavior even with small privacy budgets (<math id="S3.SS2.p4.1.m1.1" class="ltx_Math" alttext="\epsilon&lt;1" display="inline"><semantics id="S3.SS2.p4.1.m1.1a"><mrow id="S3.SS2.p4.1.m1.1.1" xref="S3.SS2.p4.1.m1.1.1.cmml"><mi id="S3.SS2.p4.1.m1.1.1.2" xref="S3.SS2.p4.1.m1.1.1.2.cmml">ϵ</mi><mo id="S3.SS2.p4.1.m1.1.1.1" xref="S3.SS2.p4.1.m1.1.1.1.cmml">&lt;</mo><mn id="S3.SS2.p4.1.m1.1.1.3" xref="S3.SS2.p4.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.m1.1b"><apply id="S3.SS2.p4.1.m1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1"><lt id="S3.SS2.p4.1.m1.1.1.1.cmml" xref="S3.SS2.p4.1.m1.1.1.1"></lt><ci id="S3.SS2.p4.1.m1.1.1.2.cmml" xref="S3.SS2.p4.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS2.p4.1.m1.1.1.3.cmml" xref="S3.SS2.p4.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.m1.1c">\epsilon&lt;1</annotation></semantics></math>), regardless of the direction of class distortion in the synthetic datasets.</p>
</div>
<figure id="S3.F2" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x4.png" id="S3.F2.sf1.g1" class="ltx_graphics ltx_img_landscape" width="156" height="96" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F2.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="S3.F2.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x5.png" id="S3.F2.sf2.g1" class="ltx_graphics ltx_img_landscape" width="156" height="96" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F2.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F2.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x6.png" id="S3.F2.sf3.g1" class="ltx_graphics ltx_img_landscape" width="156" height="96" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F2.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S3.F2.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Synthetic data class (multi-class) size relative to real (top) (each bubble denotes a distinct class while the size its relative count in the real data) and DP and synthetic classifiers recall relative to real (bottom) for different levels of <math id="S3.F2.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.F2.2.m1.1b"><mi id="S3.F2.2.m1.1.1" xref="S3.F2.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.F2.2.m1.1c"><ci id="S3.F2.2.m1.1.1.cmml" xref="S3.F2.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.2.m1.1d">\epsilon</annotation></semantics></math>, <span id="S3.F2.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Purchases</span>, (<span id="S3.F2.6.2" class="ltx_text ltx_font_bold ltx_font_italic">S2</span>).</figcaption>
</figure>
<figure id="S3.F3" class="ltx_figure ltx_align_center"><img src="/html/2109.11429/assets/x7.png" id="S3.F3.1.g1" class="ltx_graphics ltx_img_landscape" width="456" height="148" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>(S2) Synthetic data class (multi-class) size (top) and real, DP, and synthetic classifiers recall (bottom) for different digits and levels of <math id="S3.F3.3.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.F3.3.m1.1b"><mi id="S3.F3.3.m1.1.1" xref="S3.F3.3.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.F3.3.m1.1c"><ci id="S3.F3.3.m1.1.1.cmml" xref="S3.F3.3.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F3.3.m1.1d">\epsilon</annotation></semantics></math>, <span id="S3.F3.6.1" class="ltx_text ltx_font_bold ltx_font_italic">MNIST</span> with class “8” downsampled to 0.1 its count, (<span id="S3.F3.7.2" class="ltx_text ltx_font_bold ltx_font_italic">S2</span>).</figcaption>
</figure>
</section>
<section id="S3.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>S2: Multi-Class Size and Recall</h3>

<div id="S3.SS3.p1" class="ltx_para">
<p id="S3.SS3.p1.1" class="ltx_p">We experiment with privacy budgets (<math id="S3.SS3.p1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\epsilon</annotation></semantics></math>) of 0.1, 10, and infinity (“no-DP”) for Purchases and 0.5, 5, 15, and infinity (“no-DP”) for MNIST (to maintain consistency with previous work <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Uniyal et al.</span>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>).
While for Purchases we do not imbalance the dataset, since it is already imbalanced (the proportion of the smallest to the largest class is 0.015), we imbalance the class “8” in MNIST to be 0.25 and 0.1 of the largest class.
Furthermore, for MNIST, we only compare DP-SGD and PATE-GAN because the data has too many dimensions for PrivBayes.</p>
</div>
<div id="S3.SS3.p2" class="ltx_para ltx_noindent">
<p id="S3.SS3.p2.1" class="ltx_p"><span id="S3.SS3.p2.1.1" class="ltx_text ltx_font_bold">Size.</span>
The size of the synthetic data is shown in the top rows of Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 S1: Binary Class Size and Precision ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#S3.F3" title="Figure 3 ‣ 3.2 S1: Binary Class Size and Precision ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, (and <a href="#A1.F12.11" title="Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> in Appendix <a href="#A1.SS2" title="A.2 S2: Further Purchases and MNIST Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>).
For the Purchases dataset, we see similar trends with PrivBayes and PATE-GAN as in S1; the former evens the classes, while the latter increases the gap by “transferring” counts from the minority to the majority classes.
PATE-GAN exhibits the strongest disparity, even with “no-DP,” which contradicts findings from <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Uniyal et al.</span>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.However, the data imbalance here is of different nature as we have two classes with much higher counts than the others rather than a single underrepresented class.
In turn, this could potentially bias the generator towards these classes as the teacher-discriminators are exposed predominantly to them and thus, learn to distinguish them from fake examples better.
DP-WGAN does not preserve the class sizes as successfully as in S1.</p>
</div>
<div id="S3.SS3.p3" class="ltx_para">
<p id="S3.SS3.p3.1" class="ltx_p">For MNIST, PATE-GAN exhibits far better performance for both imbalances and preserves the counts even for lower <math id="S3.SS3.p3.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.p3.1.m1.1a"><mi id="S3.SS3.p3.1.m1.1.1" xref="S3.SS3.p3.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p3.1.m1.1b"><ci id="S3.SS3.p3.1.m1.1.1.cmml" xref="S3.SS3.p3.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p3.1.m1.1c">\epsilon</annotation></semantics></math> budgets.
Looking at the minority class “8,” however, PATE-GAN fails to generate any digits for imbalance 0.1 (even for “no-DP” as well).
This could be because the teachers fail to pass samples “8” labeled as real to the student even though when applied to classification, PATE is more robust under similar imbalance levels <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Uniyal et al.</span>, <a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.</p>
</div>
<div id="S3.SS3.p4" class="ltx_para ltx_noindent">
<p id="S3.SS3.p4.1" class="ltx_p"><span id="S3.SS3.p4.1.1" class="ltx_text ltx_font_bold">Recall.</span>
In the bottom two rows of Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 S1: Binary Class Size and Precision ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a>, <a href="#S3.F3" title="Figure 3 ‣ 3.2 S1: Binary Class Size and Precision ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a>, (and <a href="#A1.F12.11" title="Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> in Appendix <a href="#A1.SS2" title="A.2 S2: Further Purchases and MNIST Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>), we report the recall of the real, DP, and synth classifiers on the two datasets.
For Purchases, the synth classifiers trained on data from PrivBayes far outperform both the DP classifiers and the other synth classifiers.
Even with “no-DP,” the synth classifiers trained on DP-WGAN and PATE-GAN incur a severe recall drop on smaller subgroups.</p>
</div>
<div id="S3.SS3.p5" class="ltx_para">
<p id="S3.SS3.p5.4" class="ltx_p">For MNIST, again PATE-GAN performs much better than DP-WGAN and DP classifiers – the recall drops are not so acute, and their standard deviations are much lower.
DP-WGAN follows closely DP classifiers but is slightly worse for all levels of <math id="S3.SS3.p5.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.p5.1.m1.1a"><mi id="S3.SS3.p5.1.m1.1.1" xref="S3.SS3.p5.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.1.m1.1b"><ci id="S3.SS3.p5.1.m1.1.1.cmml" xref="S3.SS3.p5.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.1.m1.1c">\epsilon</annotation></semantics></math> and imbalances.
DP-WGAN’s performance looks random for <math id="S3.SS3.p5.2.m2.1" class="ltx_Math" alttext="\epsilon=0.5" display="inline"><semantics id="S3.SS3.p5.2.m2.1a"><mrow id="S3.SS3.p5.2.m2.1.1" xref="S3.SS3.p5.2.m2.1.1.cmml"><mi id="S3.SS3.p5.2.m2.1.1.2" xref="S3.SS3.p5.2.m2.1.1.2.cmml">ϵ</mi><mo id="S3.SS3.p5.2.m2.1.1.1" xref="S3.SS3.p5.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS3.p5.2.m2.1.1.3" xref="S3.SS3.p5.2.m2.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.2.m2.1b"><apply id="S3.SS3.p5.2.m2.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1"><eq id="S3.SS3.p5.2.m2.1.1.1.cmml" xref="S3.SS3.p5.2.m2.1.1.1"></eq><ci id="S3.SS3.p5.2.m2.1.1.2.cmml" xref="S3.SS3.p5.2.m2.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS3.p5.2.m2.1.1.3.cmml" xref="S3.SS3.p5.2.m2.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.2.m2.1c">\epsilon=0.5</annotation></semantics></math>, which means that the classifiers failed to learn anything, most likely due to bad quality of the synthetic data (Fig. <a href="#A1.F13" title="Figure 13 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> in Appendix <a href="#A1.SS2" title="A.2 S2: Further Purchases and MNIST Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.2</span></a>).
While expectedly DP-WGAN monotonically drops in terms of recall with decreasing <math id="S3.SS3.p5.3.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS3.p5.3.m3.1a"><mi id="S3.SS3.p5.3.m3.1.1" xref="S3.SS3.p5.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.3.m3.1b"><ci id="S3.SS3.p5.3.m3.1.1.cmml" xref="S3.SS3.p5.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.3.m3.1c">\epsilon</annotation></semantics></math>, PATE-GAN’s performance actually increases when DP is applied, e.g., <math id="S3.SS3.p5.4.m4.1" class="ltx_Math" alttext="\epsilon=15" display="inline"><semantics id="S3.SS3.p5.4.m4.1a"><mrow id="S3.SS3.p5.4.m4.1.1" xref="S3.SS3.p5.4.m4.1.1.cmml"><mi id="S3.SS3.p5.4.m4.1.1.2" xref="S3.SS3.p5.4.m4.1.1.2.cmml">ϵ</mi><mo id="S3.SS3.p5.4.m4.1.1.1" xref="S3.SS3.p5.4.m4.1.1.1.cmml">=</mo><mn id="S3.SS3.p5.4.m4.1.1.3" xref="S3.SS3.p5.4.m4.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p5.4.m4.1b"><apply id="S3.SS3.p5.4.m4.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1"><eq id="S3.SS3.p5.4.m4.1.1.1.cmml" xref="S3.SS3.p5.4.m4.1.1.1"></eq><ci id="S3.SS3.p5.4.m4.1.1.2.cmml" xref="S3.SS3.p5.4.m4.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS3.p5.4.m4.1.1.3.cmml" xref="S3.SS3.p5.4.m4.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p5.4.m4.1c">\epsilon=15</annotation></semantics></math> and 5 yield marginally better results than “no-DP” for both imbalances.
This is most likely due to the fact that the teacher-discriminators are exposed to different subsets of the real data, and as result, do not learn exactly the same distributions as well as the noise added to their votes, which further enables generalization.
Interestingly, the performance on some digits suffers a lot more than others (e.g., “2,” “5,” “9”), which could be explained because they are visually close to “8.”
This phenomenon is displayed in Fig. <a href="#S3.F4" title="Figure 4 ‣ 3.3 S2: Multi-Class Size and Recall ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a>.
The observation allows us to speculate that applying DP could not only lead to worse performance for the underrepresented subparts of the data but also for those with similar characteristics as well.</p>
</div>
<figure id="S3.F4" class="ltx_figure ltx_align_center"><img src="/html/2109.11429/assets/x8.png" id="S3.F4.1.g1" class="ltx_graphics ltx_img_landscape" width="231" height="112" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>(S2) PATE-GAN synthetic classifier class recall drop relative to real for different digits and levels of <math id="S3.F4.3.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.F4.3.m1.1b"><mi id="S3.F4.3.m1.1.1" xref="S3.F4.3.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.F4.3.m1.1c"><ci id="S3.F4.3.m1.1.1.cmml" xref="S3.F4.3.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.3.m1.1d">\epsilon</annotation></semantics></math>, <span id="S3.F4.6.1" class="ltx_text ltx_font_bold ltx_font_italic">MNIST</span> with class “8” downsampled to 0.25 its count, (<span id="S3.F4.7.2" class="ltx_text ltx_font_bold ltx_font_italic">S2</span>).</figcaption>
</figure>
</section>
<section id="S3.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>S3: Single-Attribute Subgroup Size, Accuracy, and Correlation</h3>

<div id="S3.SS4.p1" class="ltx_para">
<p id="S3.SS4.p1.1" class="ltx_p">We treat a single feature – namely, sex – as a subgroup in the Adult and Texas datasets.
We consider privacy budgets (<math id="S3.SS4.p1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS4.p1.1.m1.1a"><mi id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><ci id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">\epsilon</annotation></semantics></math>) of 0.01, 0.1, 1, 10, 100, and infinity (“no-DP”) as well as imbalance ratios of 0.01, 0.05, 0.1, 0.25, and 0.5.</p>
</div>
<div id="S3.SS4.p2" class="ltx_para ltx_noindent">
<p id="S3.SS4.p2.5" class="ltx_p"><span id="S3.SS4.p2.5.1" class="ltx_text ltx_font_bold">Size.</span>
In the top rows of the plots in Fig. <a href="#A1.F14" title="Figure 14 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> in Appendix <a href="#A1.SS3" title="A.3 S3: Full Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>, we report the full experiments for <math id="S3.SS4.p2.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS4.p2.1.m1.1a"><mi id="S3.SS4.p2.1.m1.1.1" xref="S3.SS4.p2.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.1.m1.1b"><ci id="S3.SS4.p2.1.m1.1.1.cmml" xref="S3.SS4.p2.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.1.m1.1c">\epsilon</annotation></semantics></math> and imbalance effects on subgroup size on the two datasets while in Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.4 S3: Single-Attribute Subgroup Size, Accuracy, and Correlation ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> we summarize the trends for Texas.
Once again, we find that, with PrivBayes, decreasing <math id="S3.SS4.p2.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS4.p2.2.m2.1a"><mi id="S3.SS4.p2.2.m2.1.1" xref="S3.SS4.p2.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.2.m2.1b"><ci id="S3.SS4.p2.2.m2.1.1.cmml" xref="S3.SS4.p2.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.2.m2.1c">\epsilon</annotation></semantics></math> results in synthetic data with reduced subgroup imbalance for all datasets – the higher the initial imbalance, the more PrivBayes balances the subgroups (could be seen in the slope of the blue lines).
As before, this effect could be attributed to the truncation of negative noisy counts after the Laplace mechanism is applied.
PATE-GAN synthetic datasets follow the opposite trend.
The gap becomes so large that, for imbalances lower than 0.1 for Adult and 0.25 for Texas, PATE-GAN barely generates the underrepresented subgroup for all <math id="S3.SS4.p2.3.m3.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS4.p2.3.m3.1a"><mi id="S3.SS4.p2.3.m3.1.1" xref="S3.SS4.p2.3.m3.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.3.m3.1b"><ci id="S3.SS4.p2.3.m3.1.1.cmml" xref="S3.SS4.p2.3.m3.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.3.m3.1c">\epsilon</annotation></semantics></math> values except 0.01.
DP-WGAN is again the most successful at preserving the imbalance for <math id="S3.SS4.p2.4.m4.1" class="ltx_Math" alttext="\epsilon&gt;0.1" display="inline"><semantics id="S3.SS4.p2.4.m4.1a"><mrow id="S3.SS4.p2.4.m4.1.1" xref="S3.SS4.p2.4.m4.1.1.cmml"><mi id="S3.SS4.p2.4.m4.1.1.2" xref="S3.SS4.p2.4.m4.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p2.4.m4.1.1.1" xref="S3.SS4.p2.4.m4.1.1.1.cmml">&gt;</mo><mn id="S3.SS4.p2.4.m4.1.1.3" xref="S3.SS4.p2.4.m4.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.4.m4.1b"><apply id="S3.SS4.p2.4.m4.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1"><gt id="S3.SS4.p2.4.m4.1.1.1.cmml" xref="S3.SS4.p2.4.m4.1.1.1"></gt><ci id="S3.SS4.p2.4.m4.1.1.2.cmml" xref="S3.SS4.p2.4.m4.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS4.p2.4.m4.1.1.3.cmml" xref="S3.SS4.p2.4.m4.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.4.m4.1c">\epsilon&gt;0.1</annotation></semantics></math>.
For <math id="S3.SS4.p2.5.m5.1" class="ltx_Math" alttext="\epsilon=0.1" display="inline"><semantics id="S3.SS4.p2.5.m5.1a"><mrow id="S3.SS4.p2.5.m5.1.1" xref="S3.SS4.p2.5.m5.1.1.cmml"><mi id="S3.SS4.p2.5.m5.1.1.2" xref="S3.SS4.p2.5.m5.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p2.5.m5.1.1.1" xref="S3.SS4.p2.5.m5.1.1.1.cmml">=</mo><mn id="S3.SS4.p2.5.m5.1.1.3" xref="S3.SS4.p2.5.m5.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p2.5.m5.1b"><apply id="S3.SS4.p2.5.m5.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1"><eq id="S3.SS4.p2.5.m5.1.1.1.cmml" xref="S3.SS4.p2.5.m5.1.1.1"></eq><ci id="S3.SS4.p2.5.m5.1.1.2.cmml" xref="S3.SS4.p2.5.m5.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS4.p2.5.m5.1.1.3.cmml" xref="S3.SS4.p2.5.m5.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p2.5.m5.1c">\epsilon=0.1</annotation></semantics></math> and 0.01, the subgroup size appears random, as the DP-WGAN models are trained only for a few iterations before the full privacy budget is spent.</p>
</div>
<div id="S3.SS4.p3" class="ltx_para ltx_noindent">
<p id="S3.SS4.p3.1" class="ltx_p"><span id="S3.SS4.p3.1.1" class="ltx_text ltx_font_bold">Accuracy.</span>
The bottom rows of Fig. <a href="#A1.F14" title="Figure 14 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> in Appendix <a href="#A1.SS3" title="A.3 S3: Full Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a> report the accuracy of the various classifiers, while Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.4 S3: Single-Attribute Subgroup Size, Accuracy, and Correlation ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> displays a summary for Texas.
Interestingly, we find that the real classifier, with Adult, achieves higher accuracy on the underrepresented subgroup “Female” than the overrepresented “Male” for all imbalances.</p>
</div>
<figure id="S3.F5" class="ltx_figure ltx_align_center"><img src="/html/2109.11429/assets/x9.png" id="S3.F5.1.g1" class="ltx_graphics ltx_img_landscape" width="230" height="159" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Minority single-attribute (sex) subgroup imbalance level difference (top) and minority subgroup accuracy drop difference (bottom) relative to majority for different subgroup imbalance and <math id="S3.F5.3.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.F5.3.m1.1b"><mi id="S3.F5.3.m1.1.1" xref="S3.F5.3.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.F5.3.m1.1c"><ci id="S3.F5.3.m1.1.1.cmml" xref="S3.F5.3.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F5.3.m1.1d">\epsilon</annotation></semantics></math> levels, <span id="S3.F5.6.1" class="ltx_text ltx_font_bold ltx_font_italic">Texas</span>, (<span id="S3.F5.7.2" class="ltx_text ltx_font_bold ltx_font_italic">S3</span>).</figcaption>
</figure>
<div id="S3.SS4.p4" class="ltx_para">
<p id="S3.SS4.p4.5" class="ltx_p">As for DP classifiers, over a certain <math id="S3.SS4.p4.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS4.p4.1.m1.1a"><mi id="S3.SS4.p4.1.m1.1.1" xref="S3.SS4.p4.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.1.m1.1b"><ci id="S3.SS4.p4.1.m1.1.1.cmml" xref="S3.SS4.p4.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.1.m1.1c">\epsilon</annotation></semantics></math>, decreasing it further reduces the accuracy of the minority subgroup more than that of the majority.
Additionally, this reduction in accuracy is more accentuated with increasing subgroup imbalance.
For example, looking at the Adult plots, the accuracy on “Female” drops more than “Male” for <math id="S3.SS4.p4.2.m2.1" class="ltx_Math" alttext="\epsilon\leq 0.1" display="inline"><semantics id="S3.SS4.p4.2.m2.1a"><mrow id="S3.SS4.p4.2.m2.1.1" xref="S3.SS4.p4.2.m2.1.1.cmml"><mi id="S3.SS4.p4.2.m2.1.1.2" xref="S3.SS4.p4.2.m2.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p4.2.m2.1.1.1" xref="S3.SS4.p4.2.m2.1.1.1.cmml">≤</mo><mn id="S3.SS4.p4.2.m2.1.1.3" xref="S3.SS4.p4.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.2.m2.1b"><apply id="S3.SS4.p4.2.m2.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1"><leq id="S3.SS4.p4.2.m2.1.1.1.cmml" xref="S3.SS4.p4.2.m2.1.1.1"></leq><ci id="S3.SS4.p4.2.m2.1.1.2.cmml" xref="S3.SS4.p4.2.m2.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS4.p4.2.m2.1.1.3.cmml" xref="S3.SS4.p4.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.2.m2.1c">\epsilon\leq 0.1</annotation></semantics></math> and imbalance 0.5.
For increasing imbalances, the drop on the minority subgroup overtakes the majority for larger privacy budgets, i.e., <math id="S3.SS4.p4.3.m3.1" class="ltx_Math" alttext="\epsilon\leq 1" display="inline"><semantics id="S3.SS4.p4.3.m3.1a"><mrow id="S3.SS4.p4.3.m3.1.1" xref="S3.SS4.p4.3.m3.1.1.cmml"><mi id="S3.SS4.p4.3.m3.1.1.2" xref="S3.SS4.p4.3.m3.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p4.3.m3.1.1.1" xref="S3.SS4.p4.3.m3.1.1.1.cmml">≤</mo><mn id="S3.SS4.p4.3.m3.1.1.3" xref="S3.SS4.p4.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.3.m3.1b"><apply id="S3.SS4.p4.3.m3.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1"><leq id="S3.SS4.p4.3.m3.1.1.1.cmml" xref="S3.SS4.p4.3.m3.1.1.1"></leq><ci id="S3.SS4.p4.3.m3.1.1.2.cmml" xref="S3.SS4.p4.3.m3.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS4.p4.3.m3.1.1.3.cmml" xref="S3.SS4.p4.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.3.m3.1c">\epsilon\leq 1</annotation></semantics></math> for imbalances 0.25 and 0.1, <math id="S3.SS4.p4.4.m4.1" class="ltx_Math" alttext="\epsilon\leq 10" display="inline"><semantics id="S3.SS4.p4.4.m4.1a"><mrow id="S3.SS4.p4.4.m4.1.1" xref="S3.SS4.p4.4.m4.1.1.cmml"><mi id="S3.SS4.p4.4.m4.1.1.2" xref="S3.SS4.p4.4.m4.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p4.4.m4.1.1.1" xref="S3.SS4.p4.4.m4.1.1.1.cmml">≤</mo><mn id="S3.SS4.p4.4.m4.1.1.3" xref="S3.SS4.p4.4.m4.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.4.m4.1b"><apply id="S3.SS4.p4.4.m4.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1"><leq id="S3.SS4.p4.4.m4.1.1.1.cmml" xref="S3.SS4.p4.4.m4.1.1.1"></leq><ci id="S3.SS4.p4.4.m4.1.1.2.cmml" xref="S3.SS4.p4.4.m4.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS4.p4.4.m4.1.1.3.cmml" xref="S3.SS4.p4.4.m4.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.4.m4.1c">\epsilon\leq 10</annotation></semantics></math> for imbalance 0.05, and finally <math id="S3.SS4.p4.5.m5.1" class="ltx_Math" alttext="\epsilon\leq 100" display="inline"><semantics id="S3.SS4.p4.5.m5.1a"><mrow id="S3.SS4.p4.5.m5.1.1" xref="S3.SS4.p4.5.m5.1.1.cmml"><mi id="S3.SS4.p4.5.m5.1.1.2" xref="S3.SS4.p4.5.m5.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p4.5.m5.1.1.1" xref="S3.SS4.p4.5.m5.1.1.1.cmml">≤</mo><mn id="S3.SS4.p4.5.m5.1.1.3" xref="S3.SS4.p4.5.m5.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p4.5.m5.1b"><apply id="S3.SS4.p4.5.m5.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1"><leq id="S3.SS4.p4.5.m5.1.1.1.cmml" xref="S3.SS4.p4.5.m5.1.1.1"></leq><ci id="S3.SS4.p4.5.m5.1.1.2.cmml" xref="S3.SS4.p4.5.m5.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS4.p4.5.m5.1.1.3.cmml" xref="S3.SS4.p4.5.m5.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p4.5.m5.1c">\epsilon\leq 100</annotation></semantics></math> for imbalance 0.01.</p>
</div>
<div id="S3.SS4.p5" class="ltx_para">
<p id="S3.SS4.p5.3" class="ltx_p">The synth classifiers incur a bigger accuracy drop in the underrepresented subgroup—regardless of the subgroup sizes in the synthetic data (as observed in the overall positive slopes of all lines in the bottom row of Fig. <a href="#S3.F5" title="Figure 5 ‣ 3.4 S3: Single-Attribute Subgroup Size, Accuracy, and Correlation ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a>).
Classifiers trained on PrivBayes synthetic data follow the behavior of DP classifiers the closest.
Overall, DP-WGAN synth classifiers perform worse than the others as they are more unstable, with some noticeable accuracy drops (e.g., <math id="S3.SS4.p5.1.m1.1" class="ltx_Math" alttext="\epsilon=100" display="inline"><semantics id="S3.SS4.p5.1.m1.1a"><mrow id="S3.SS4.p5.1.m1.1.1" xref="S3.SS4.p5.1.m1.1.1.cmml"><mi id="S3.SS4.p5.1.m1.1.1.2" xref="S3.SS4.p5.1.m1.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p5.1.m1.1.1.1" xref="S3.SS4.p5.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS4.p5.1.m1.1.1.3" xref="S3.SS4.p5.1.m1.1.1.3.cmml">100</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.1.m1.1b"><apply id="S3.SS4.p5.1.m1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1"><eq id="S3.SS4.p5.1.m1.1.1.1.cmml" xref="S3.SS4.p5.1.m1.1.1.1"></eq><ci id="S3.SS4.p5.1.m1.1.1.2.cmml" xref="S3.SS4.p5.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS4.p5.1.m1.1.1.3.cmml" xref="S3.SS4.p5.1.m1.1.1.3">100</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.1.m1.1c">\epsilon=100</annotation></semantics></math> for imbalance 0.5 in Adult, <math id="S3.SS4.p5.2.m2.1" class="ltx_Math" alttext="\epsilon=1" display="inline"><semantics id="S3.SS4.p5.2.m2.1a"><mrow id="S3.SS4.p5.2.m2.1.1" xref="S3.SS4.p5.2.m2.1.1.cmml"><mi id="S3.SS4.p5.2.m2.1.1.2" xref="S3.SS4.p5.2.m2.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p5.2.m2.1.1.1" xref="S3.SS4.p5.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS4.p5.2.m2.1.1.3" xref="S3.SS4.p5.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.2.m2.1b"><apply id="S3.SS4.p5.2.m2.1.1.cmml" xref="S3.SS4.p5.2.m2.1.1"><eq id="S3.SS4.p5.2.m2.1.1.1.cmml" xref="S3.SS4.p5.2.m2.1.1.1"></eq><ci id="S3.SS4.p5.2.m2.1.1.2.cmml" xref="S3.SS4.p5.2.m2.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS4.p5.2.m2.1.1.3.cmml" xref="S3.SS4.p5.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.2.m2.1c">\epsilon=1</annotation></semantics></math> for imbalances 0.1 and 0.05 in Texas).
PATE-GAN synth classifiers have better accuracy than DP classifiers for both subgroups for <math id="S3.SS4.p5.3.m3.1" class="ltx_Math" alttext="\epsilon&lt;10" display="inline"><semantics id="S3.SS4.p5.3.m3.1a"><mrow id="S3.SS4.p5.3.m3.1.1" xref="S3.SS4.p5.3.m3.1.1.cmml"><mi id="S3.SS4.p5.3.m3.1.1.2" xref="S3.SS4.p5.3.m3.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p5.3.m3.1.1.1" xref="S3.SS4.p5.3.m3.1.1.1.cmml">&lt;</mo><mn id="S3.SS4.p5.3.m3.1.1.3" xref="S3.SS4.p5.3.m3.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p5.3.m3.1b"><apply id="S3.SS4.p5.3.m3.1.1.cmml" xref="S3.SS4.p5.3.m3.1.1"><lt id="S3.SS4.p5.3.m3.1.1.1.cmml" xref="S3.SS4.p5.3.m3.1.1.1"></lt><ci id="S3.SS4.p5.3.m3.1.1.2.cmml" xref="S3.SS4.p5.3.m3.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS4.p5.3.m3.1.1.3.cmml" xref="S3.SS4.p5.3.m3.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p5.3.m3.1c">\epsilon&lt;10</annotation></semantics></math>; this is perhaps surprising, especially for the underrepresented subgroup, as the synth classifiers are trained on synthetic data containing only a small number of the minority subgroup.</p>
</div>
<div id="S3.SS4.p6" class="ltx_para ltx_noindent">
<p id="S3.SS4.p6.2" class="ltx_p"><span id="S3.SS4.p6.2.1" class="ltx_text ltx_font_bold">Correlation.</span>
The mutual information between the subgroup and the target columns are displayed in Fig. <a href="#A1.F15" title="Figure 15 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> in Appendix <a href="#A1.SS3" title="A.3 S3: Full Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.3</span></a>.
Since in the preparation step the subgroups were balanced by class, the expected (and the real) mutual information is 0.
We observe that, DP-WGAN manages best to maintain this relationship, closely followed by PrivBayes which, however, has a few noisy exceptions for small privacy budgets <math id="S3.SS4.p6.1.m1.1" class="ltx_Math" alttext="\epsilon&lt;=0.1" display="inline"><semantics id="S3.SS4.p6.1.m1.1a"><mrow id="S3.SS4.p6.1.m1.1.1" xref="S3.SS4.p6.1.m1.1.1.cmml"><mi id="S3.SS4.p6.1.m1.1.1.2" xref="S3.SS4.p6.1.m1.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p6.1.m1.1.1.1" xref="S3.SS4.p6.1.m1.1.1.1.cmml">&lt;=</mo><mn id="S3.SS4.p6.1.m1.1.1.3" xref="S3.SS4.p6.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.1.m1.1b"><apply id="S3.SS4.p6.1.m1.1.1.cmml" xref="S3.SS4.p6.1.m1.1.1"><leq id="S3.SS4.p6.1.m1.1.1.1.cmml" xref="S3.SS4.p6.1.m1.1.1.1"></leq><ci id="S3.SS4.p6.1.m1.1.1.2.cmml" xref="S3.SS4.p6.1.m1.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS4.p6.1.m1.1.1.3.cmml" xref="S3.SS4.p6.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.1.m1.1c">\epsilon&lt;=0.1</annotation></semantics></math>.
In contrast, PATE-GAN introduces undesirable artifact in the synthetic data for <math id="S3.SS4.p6.2.m2.1" class="ltx_Math" alttext="\epsilon&lt;=0.1" display="inline"><semantics id="S3.SS4.p6.2.m2.1a"><mrow id="S3.SS4.p6.2.m2.1.1" xref="S3.SS4.p6.2.m2.1.1.cmml"><mi id="S3.SS4.p6.2.m2.1.1.2" xref="S3.SS4.p6.2.m2.1.1.2.cmml">ϵ</mi><mo id="S3.SS4.p6.2.m2.1.1.1" xref="S3.SS4.p6.2.m2.1.1.1.cmml">&lt;=</mo><mn id="S3.SS4.p6.2.m2.1.1.3" xref="S3.SS4.p6.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS4.p6.2.m2.1b"><apply id="S3.SS4.p6.2.m2.1.1.cmml" xref="S3.SS4.p6.2.m2.1.1"><leq id="S3.SS4.p6.2.m2.1.1.1.cmml" xref="S3.SS4.p6.2.m2.1.1.1"></leq><ci id="S3.SS4.p6.2.m2.1.1.2.cmml" xref="S3.SS4.p6.2.m2.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS4.p6.2.m2.1.1.3.cmml" xref="S3.SS4.p6.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p6.2.m2.1c">\epsilon&lt;=0.1</annotation></semantics></math> for both datasets.
In other words, the model creates data with stronger relationship between the subgroup column and the target column.</p>
</div>
<figure id="S3.F6" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x10.png" id="S3.F6.sf1.g1" class="ltx_graphics ltx_img_landscape" width="228" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F6.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x11.png" id="S3.F6.sf2.g1" class="ltx_graphics ltx_img_landscape" width="228" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F6.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F6.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x12.png" id="S3.F6.sf3.g1" class="ltx_graphics ltx_img_landscape" width="228" height="87" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F6.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S3.F6.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Synthetic data multi-attribute (intersection of age, sex, and race) subgroup size relative to real (top) (each bubble denotes a distinct subgroup while the size its relative count in the real data) and DP and synthetic classifiers accuracy relative to real (bottom) for different single-attribute (sex) subgroup imbalance and <math id="S3.F6.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.F6.2.m1.1b"><mi id="S3.F6.2.m1.1.1" xref="S3.F6.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.F6.2.m1.1c"><ci id="S3.F6.2.m1.1.1.cmml" xref="S3.F6.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F6.2.m1.1d">\epsilon</annotation></semantics></math> levels, <span id="S3.F6.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Adult</span>, (<span id="S3.F6.6.2" class="ltx_text ltx_font_bold ltx_font_italic">S4</span>).</figcaption>
</figure>
<figure id="S3.F7" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F7.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x13.png" id="S3.F7.sf1.g1" class="ltx_graphics ltx_img_landscape" width="228" height="42" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="S3.F7.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes, Adult</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F7.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x14.png" id="S3.F7.sf2.g1" class="ltx_graphics ltx_img_landscape" width="228" height="42" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="S3.F7.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN, Adult</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F7.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x15.png" id="S3.F7.sf3.g1" class="ltx_graphics ltx_img_landscape" width="228" height="42" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="S3.F7.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN, Adult</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F7.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x16.png" id="S3.F7.sf4.g1" class="ltx_graphics ltx_img_landscape" width="228" height="42" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.sf4.3.1.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="S3.F7.sf4.4.2" class="ltx_text" style="font-size:70%;">PrivBayes, Texas</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F7.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x17.png" id="S3.F7.sf5.g1" class="ltx_graphics ltx_img_landscape" width="228" height="42" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.sf5.3.1.1" class="ltx_text" style="font-size:80%;">(e)</span> </span><span id="S3.F7.sf5.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN, Texas</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="S3.F7.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x18.png" id="S3.F7.sf6.g1" class="ltx_graphics ltx_img_landscape" width="228" height="42" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="S3.F7.sf6.3.1.1" class="ltx_text" style="font-size:80%;">(f)</span> </span><span id="S3.F7.sf6.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN, Texas</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Mutual information between the multi-attribute (intersection of age, sex, and race) subgroup and the target (income/length of stay) columns for different single-attribute subgroup imbalance (sex) and <math id="S3.F7.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.F7.2.m1.1b"><mi id="S3.F7.2.m1.1.1" xref="S3.F7.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.F7.2.m1.1c"><ci id="S3.F7.2.m1.1.1.cmml" xref="S3.F7.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F7.2.m1.1d">\epsilon</annotation></semantics></math> levels, <span id="S3.F7.6.1" class="ltx_text ltx_font_bold ltx_font_italic">Adult</span> (top 3) and <span id="S3.F7.7.2" class="ltx_text ltx_font_bold ltx_font_italic">Texas</span> (bottom 3), (<span id="S3.F7.8.3" class="ltx_text ltx_font_bold ltx_font_italic">S4</span>).</figcaption>
</figure>
</section>
<section id="S3.SS5" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>S4: Multi-Attribute Subgroup Size, Accuracy, and Correlation</h3>

<div id="S3.SS5.p1" class="ltx_para">
<p id="S3.SS5.p1.1" class="ltx_p">In our last set of experiments, we treat the intersection between three features – age, sex, and race – as complex subgroups in the Adult and Texas datasets.
We consider privacy budgets (<math id="S3.SS5.p1.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS5.p1.1.m1.1a"><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">\epsilon</annotation></semantics></math>) of 0.1, 10, and infinity (“no-DP”) as well as imbalance ratios of 0.05 and 0.25.
This results in 16 subgroups for Adult, 21 for Texas for imbalance 0.05 and 19, and 27 for 0.25, respectively.</p>
</div>
<div id="S3.SS5.p2" class="ltx_para ltx_noindent">
<p id="S3.SS5.p2.1" class="ltx_p"><span id="S3.SS5.p2.1.1" class="ltx_text ltx_font_bold">Size.</span>
In the top rows of the plots in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.4 S3: Single-Attribute Subgroup Size, Accuracy, and Correlation ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (also see Fig. <a href="#A1.F17" title="Figure 17 ‣ A.4 S4: Texas Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> in Appendix <a href="#A1.SS4" title="A.4 S4: Texas Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">A.4</span></a>), we report the sizes of the subgroups, with the three different models and the two datasets.
Once again, PrivBayes reduces the gap between majority and minority subgroups in the synthetic data, whereas PATE-GAN increases it.
DP-WGAN behaves similarly to PrivBayes, which is inconsistent with the multi-class case discussed in Sec. <a href="#S3.SS4" title="3.4 S3: Single-Attribute Subgroup Size, Accuracy, and Correlation ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.
For Adult, DP-WGAN does not manage to keep the subgroups distribution, even for “no-DP.”
Finally, the effect of increased subgroup imbalance is evident in the higher disparity for all models and datasets.</p>
</div>
<div id="S3.SS5.p3" class="ltx_para ltx_noindent">
<p id="S3.SS5.p3.1" class="ltx_p"><span id="S3.SS5.p3.1.1" class="ltx_text ltx_font_bold">Accuracy.</span>
The bottom rows of the plots in Fig. <a href="#S3.F6" title="Figure 6 ‣ 3.4 S3: Single-Attribute Subgroup Size, Accuracy, and Correlation ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> (and <a href="#A1.F17" title="Figure 17 ‣ A.4 S4: Texas Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a>) report the performance of the classifiers.
In contrast to previous work <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bagdasaryan et al.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite>, we do not observe “the rich get richer, the poor get poorer” effect for either DP or synth classifiers; instead, “everybody gets poorer,” with very few exceptions.
This should not come as a surprise, as it could not be expected from a generative model to synthesize data with better utility than the real data.
The mentioned exceptions are only for subgroups with small sizes; however, the accuracy on these subgroups has a much higher standard deviation compared to larger subgroups.</p>
</div>
<div id="S3.SS5.p4" class="ltx_para">
<p id="S3.SS5.p4.2" class="ltx_p">There is also a clear distinction between classifiers with <math id="S3.SS5.p4.1.m1.1" class="ltx_Math" alttext="\epsilon=10" display="inline"><semantics id="S3.SS5.p4.1.m1.1a"><mrow id="S3.SS5.p4.1.m1.1.1" xref="S3.SS5.p4.1.m1.1.1.cmml"><mi id="S3.SS5.p4.1.m1.1.1.2" xref="S3.SS5.p4.1.m1.1.1.2.cmml">ϵ</mi><mo id="S3.SS5.p4.1.m1.1.1.1" xref="S3.SS5.p4.1.m1.1.1.1.cmml">=</mo><mn id="S3.SS5.p4.1.m1.1.1.3" xref="S3.SS5.p4.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.1.m1.1b"><apply id="S3.SS5.p4.1.m1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1"><eq id="S3.SS5.p4.1.m1.1.1.1.cmml" xref="S3.SS5.p4.1.m1.1.1.1"></eq><ci id="S3.SS5.p4.1.m1.1.1.2.cmml" xref="S3.SS5.p4.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS5.p4.1.m1.1.1.3.cmml" xref="S3.SS5.p4.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.1.m1.1c">\epsilon=10</annotation></semantics></math> and 0.1, both DP and synth.
Synth classifiers trained on PATE-GAN and PrivBayes synthetic data incur a smaller drop than DP-WGAN synthetic data.
Similar to S3, PATE-GAN trained synth classifiers have better accuracy than DP classifiers for <math id="S3.SS5.p4.2.m2.1" class="ltx_Math" alttext="\epsilon=0.1" display="inline"><semantics id="S3.SS5.p4.2.m2.1a"><mrow id="S3.SS5.p4.2.m2.1.1" xref="S3.SS5.p4.2.m2.1.1.cmml"><mi id="S3.SS5.p4.2.m2.1.1.2" xref="S3.SS5.p4.2.m2.1.1.2.cmml">ϵ</mi><mo id="S3.SS5.p4.2.m2.1.1.1" xref="S3.SS5.p4.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS5.p4.2.m2.1.1.3" xref="S3.SS5.p4.2.m2.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p4.2.m2.1b"><apply id="S3.SS5.p4.2.m2.1.1.cmml" xref="S3.SS5.p4.2.m2.1.1"><eq id="S3.SS5.p4.2.m2.1.1.1.cmml" xref="S3.SS5.p4.2.m2.1.1.1"></eq><ci id="S3.SS5.p4.2.m2.1.1.2.cmml" xref="S3.SS5.p4.2.m2.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS5.p4.2.m2.1.1.3.cmml" xref="S3.SS5.p4.2.m2.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p4.2.m2.1c">\epsilon=0.1</annotation></semantics></math>.</p>
</div>
<div id="S3.SS5.p5" class="ltx_para ltx_noindent">
<p id="S3.SS5.p5.3" class="ltx_p"><span id="S3.SS5.p5.3.1" class="ltx_text ltx_font_bold">Correlation.</span>
In Fig. <a href="#S3.F7" title="Figure 7 ‣ 3.4 S3: Single-Attribute Subgroup Size, Accuracy, and Correlation ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">7</span></a>, we display the mutual information between the multi-attribute subgroup and the target.
Unlike the single-attribute scenario, the baseline mutual information here is not 0 because only one of the attributes (sex) was balanced by class.
For both datasets, PrivBayes exhibits the most expected behavior – increasing the privacy budget <math id="S3.SS5.p5.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="S3.SS5.p5.1.m1.1a"><mi id="S3.SS5.p5.1.m1.1.1" xref="S3.SS5.p5.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.1.m1.1b"><ci id="S3.SS5.p5.1.m1.1.1.cmml" xref="S3.SS5.p5.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.1.m1.1c">\epsilon</annotation></semantics></math> results in more distorted synthetic data, thus reducing the mutual information between the subgroup and target columns.
On the other hand, PATE-GAN displays similar to u-shaped behavior: incorporating some privacy (<math id="S3.SS5.p5.2.m2.1" class="ltx_Math" alttext="\epsilon=10" display="inline"><semantics id="S3.SS5.p5.2.m2.1a"><mrow id="S3.SS5.p5.2.m2.1.1" xref="S3.SS5.p5.2.m2.1.1.cmml"><mi id="S3.SS5.p5.2.m2.1.1.2" xref="S3.SS5.p5.2.m2.1.1.2.cmml">ϵ</mi><mo id="S3.SS5.p5.2.m2.1.1.1" xref="S3.SS5.p5.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS5.p5.2.m2.1.1.3" xref="S3.SS5.p5.2.m2.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.2.m2.1b"><apply id="S3.SS5.p5.2.m2.1.1.cmml" xref="S3.SS5.p5.2.m2.1.1"><eq id="S3.SS5.p5.2.m2.1.1.1.cmml" xref="S3.SS5.p5.2.m2.1.1.1"></eq><ci id="S3.SS5.p5.2.m2.1.1.2.cmml" xref="S3.SS5.p5.2.m2.1.1.2">italic-ϵ</ci><cn type="integer" id="S3.SS5.p5.2.m2.1.1.3.cmml" xref="S3.SS5.p5.2.m2.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.2.m2.1c">\epsilon=10</annotation></semantics></math>) initially reduces the mutual information but adding more privacy (<math id="S3.SS5.p5.3.m3.1" class="ltx_Math" alttext="\epsilon=0.1" display="inline"><semantics id="S3.SS5.p5.3.m3.1a"><mrow id="S3.SS5.p5.3.m3.1.1" xref="S3.SS5.p5.3.m3.1.1.cmml"><mi id="S3.SS5.p5.3.m3.1.1.2" xref="S3.SS5.p5.3.m3.1.1.2.cmml">ϵ</mi><mo id="S3.SS5.p5.3.m3.1.1.1" xref="S3.SS5.p5.3.m3.1.1.1.cmml">=</mo><mn id="S3.SS5.p5.3.m3.1.1.3" xref="S3.SS5.p5.3.m3.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p5.3.m3.1b"><apply id="S3.SS5.p5.3.m3.1.1.cmml" xref="S3.SS5.p5.3.m3.1.1"><eq id="S3.SS5.p5.3.m3.1.1.1.cmml" xref="S3.SS5.p5.3.m3.1.1.1"></eq><ci id="S3.SS5.p5.3.m3.1.1.2.cmml" xref="S3.SS5.p5.3.m3.1.1.2">italic-ϵ</ci><cn type="float" id="S3.SS5.p5.3.m3.1.1.3.cmml" xref="S3.SS5.p5.3.m3.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p5.3.m3.1c">\epsilon=0.1</annotation></semantics></math>) increases it to level even higher than when “no-DP” is applied.
In particular, for the Texas dataset, PATE-GAN enforces the dependency between the subgroups and target columns for all privacy budgets.
Finally, DP-WGAN performs the worst in the Adult dataset.</p>
</div>
</section>
<section id="S3.SS6" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>Main Take-Aways</h3>

<div id="S3.SS6.p1" class="ltx_para ltx_noindent">
<p id="S3.SS6.p1.1" class="ltx_p"><span id="S3.SS6.p1.1.1" class="ltx_text ltx_font_bold">Disparate Effects.</span> Overall, our experiments provide an empirical demonstration that DP generative models <span id="S3.SS6.p1.1.2" class="ltx_text ltx_font_italic">do</span> have different disparate effects on synthetic data.
Analyzing the size of the classes and subgroups in the generated data, we consistently observe that PrivBayes reduces the gap between the majority and minority classes/subgroups (thus exhibiting a “Robin Hood” effect), PATE-GAN increases it (exhibiting a “Matthew” effect), and DP-WGAN has a mixed one.</p>
</div>
<div id="S3.SS6.p2" class="ltx_para ltx_noindent">
<p id="S3.SS6.p2.1" class="ltx_p"><span id="S3.SS6.p2.1.1" class="ltx_text ltx_font_bold">Downstream Classification.</span> When performing classification tasks on data produced by generative models, one faces an even bigger (or more variable) accuracy drop on minority classes and subgroups.
We also see that higher privacy guarantees and more imbalanced datasets result in more substantial disparate effects.
For example, even though PATE-GAN displays better overall behavior than DP-WGAN, an imbalance of 0.1 prevents it from learning an entire class of the data even for low privacy settings.
High privacy guarantees also result in PATE-GAN generating undesirable artifacts in the synthetic data in the form of a much stronger correlation between low correlated columns.</p>
</div>
<div id="S3.SS6.p3" class="ltx_para ltx_noindent">
<p id="S3.SS6.p3.1" class="ltx_p"><span id="S3.SS6.p3.1.1" class="ltx_text ltx_font_bold">Revisiting Our Research Questions.</span> Recall from Section <a href="#S1" title="1 Introduction ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a> that our work aimed to answer three main research questions; we now summarize some concise answers.</p>
</div>
<div id="S3.SS6.p4" class="ltx_para ltx_noindent">
<p id="S3.SS6.p4.1" class="ltx_p"><span id="S3.SS6.p4.1.1" class="ltx_text ltx_font_italic">RQ1: Do DP generative models generate data in similar classes and subgroups proportions to the real data?
<br class="ltx_break"></span>Not really.
DP distorts the proportions, yielding Robin Hood vs Matthew effects depending on the DP generative model.</p>
</div>
<div id="S3.SS6.p5" class="ltx_para ltx_noindent">
<p id="S3.SS6.p5.1" class="ltx_p"><span id="S3.SS6.p5.1.1" class="ltx_text ltx_font_italic">RQ2: Does training a classifier on DP synthetic data lead to the same disparate impact on accuracy as training a DP classifier on the real data?
<br class="ltx_break"></span>Overall, yes.
Smaller classes/subgroups suffer more similarly to DP classifiers.
However, we do not see the rich get richer, the poor get poorer; everybody gets poorer.
Incidentally, sometimes synthetic classifiers are better than DP classifiers; studying this in detail is left to future work.</p>
</div>
<div id="S3.SS6.p6" class="ltx_para ltx_noindent">
<p id="S3.SS6.p6.1" class="ltx_p"><span id="S3.SS6.p6.1.1" class="ltx_text ltx_font_italic">RQ3: Do different DP mechanisms for DP synthetic data behave similarly under different privacy and data imbalance levels?
<br class="ltx_break"></span>No, different DP generative models behave differently.
For example, PATE-GAN performs better than DP-WGAN, with some very specific exceptions, while PrivBayes is the only one that manages to maintain the data utility for the multi-class tabular data Purchases.</p>
</div>
</section>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>

<div id="S4.p1" class="ltx_para">
<p id="S4.p1.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Kuppam et al.</span> (<a href="#bib.bib32" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>)</cite> show that, if resource allocation is decided based on DP statistics, smaller districts could get more funding and larger ones less. Prior work has also studied deep neural network classifiers trained using DP-SGD <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Bagdasaryan et al.</span>, <a href="#bib.bib5" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Farrand et al.</span>, <a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Suriyakumar et al.</span>, <a href="#bib.bib47" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> on imbalanced datasets (mainly images).
Essentially, they show that underrepresented groups in a dataset that already incur lower accuracy end up losing even more accuracy when DP is applied.
In particular, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Farrand et al.</span> (<a href="#bib.bib19" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> show that even small imbalances and loose privacy guarantees can cause disparate impacts.</p>
</div>
<div id="S4.p2" class="ltx_para">
<p id="S4.p2.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Uniyal et al.</span> (<a href="#bib.bib52" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> find that classifiers trained with PATE exhibit disparate drops in performance but less severely than with DP-SGD.
Furthermore, <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Feldman</span> (<a href="#bib.bib20" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite> formalizes the need for accurate discriminative models to memorize training data and studies the disparate effects of privacy and model compression on subgroups.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Chen et al.</span> (<a href="#bib.bib12" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020b</span></a>)</cite> provide a theoretical analysis that quantifies the clipping bias on convergence with a disparity measure between the gradient and a geometrically symmetric distribution.
There are also papers focusing on learning DP classifiers with fairness constraints <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Jagielski et al.</span>, <a href="#bib.bib28" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Tran et al.</span>, <a href="#bib.bib51" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021b</span></a>)</cite> and on analyzing the PATE framework from a fairness point of view <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Tran et al.</span>, <a href="#bib.bib50" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021a</span></a>)</cite>.
Unlike our work, these efforts focus on discriminative (rather than generative) models.</p>
</div>
<div id="S4.p3" class="ltx_para">
<p id="S4.p3.1" class="ltx_p"><cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Cheng et al.</span> (<a href="#bib.bib13" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> show that training classifiers on DP synthetic images can result in significant utility degradation and increased majority subgroup influence, but not worse group unfairness measures.
However, they only use a single generative model and only look at the utility of balanced DP synthetic datasets.</p>
</div>
<div id="S4.p4" class="ltx_para">
<p id="S4.p4.1" class="ltx_p">Finally, recent work has focused on tabular DP synthetic data.
<cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Ghalebikesabi et al.</span> (<a href="#bib.bib22" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> introduce novel bias mitigation techniques, which, unfortunately, lead to reduced usefulness of the synthetic data.
Perhaps closer to our work is that by <cite class="ltx_cite ltx_citemacro_citet"><span class="ltx_text" style="font-size:90%;">Pereira et al.</span> (<a href="#bib.bib43" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>, who mainly look at single-attribute subgroup fairness and overall classification performance.
Their work, however, does not investigate the utility disparity on different single and multi-attribute subgroups of the data, nor the effect of data imbalance.
Furthermore, we consider the size disparities in the generated synthetic data as we do not use conditional generative models.
We also experiment with a far wider range of epsilon budgets.
Overall, we are the first to highlight and analyze the disparate effects of several factors: generative model type, DP mechanism, privacy budget, class and single/multi-attribute subgroup imbalance on the resulting synthetic data in terms of both statistical analysis and downstream classification.
We do so through experiments geared to isolate the effect of these factors.</p>
</div>
<div id="S4.p5" class="ltx_para">
<p id="S4.p5.1" class="ltx_p">There is a rich literature with DP generative models for tabular data <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Acs et al.</span>, <a href="#bib.bib2" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Xie et al.</span>, <a href="#bib.bib57" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2018</span></a>; <span class="ltx_text" style="font-size:90%;">Tantipongpipat et al.</span>, <a href="#bib.bib48" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Frigerio et al.</span>, <a href="#bib.bib21" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2019</span></a>; <span class="ltx_text" style="font-size:90%;">Zhang et al.</span>, <a href="#bib.bib59" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">McKenna et al.</span>, <a href="#bib.bib35" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite>.
Our goal is, however, not to benchmark all possible models but to focus on the best known and accessible state-of-the-art models relying on different/well-studied DP mechanisms (Laplace, DP-SGD, PATE).</p>
</div>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>

<div id="S5.p1" class="ltx_para">
<p id="S5.p1.1" class="ltx_p">This work analyzed the effects of privacy-preserving generative models, using different DP methods,
on 1) class/subgroups distributions in the generated synthetic data and 2) the performance of downstream tasks.
We found that applying DP to synthetic data generation disparately affects the minority subpopulations.
As for the class/subgroup distribution in the synthetic data, DP can have opposing effects depending on the underlying DP method; e.g., PrivBayes reduces the imbalance, PATE-GAN increases it.
However, when training a classifier on the synthetic data, minority subpopulations suffer stronger and/or more varying decreases in accuracy.
We also showed that the privacy budget and data imbalance are important factors and further intensify these effects.</p>
</div>
<div id="S5.p2" class="ltx_para">
<p id="S5.p2.1" class="ltx_p">Overall, our work motivates the need for practitioners and companies to take the disparate effects into consideration and adopt more extensive testing before deploying synthetic data, given that the studied technologies are already in production in the real world <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Brown</span>, <a href="#bib.bib8" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">US Census Bureau</span>, <a href="#bib.bib53" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>)</cite> and there are concerns and scepticism from the public <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Wezerek &amp; Van Riper</span>, <a href="#bib.bib56" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>; <span class="ltx_text" style="font-size:90%;">Hong</span>, <a href="#bib.bib27" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2020</span></a>)</cite>.</p>
</div>
<div id="S5.p3" class="ltx_para">
<p id="S5.p3.1" class="ltx_p">We are confident that our results will motivate further research (including theoretical contributions) at the intersection of generative models, DP, and fairness.
Hopefully, this will include novel generative models with modified/new DP learning algorithms that could reproduce the original data in a privacy-preserving manner and without disparate loss in utility.
Another interesting direction would be to examine the conditions under which classifiers trained on DP synthetic data achieve better utility than DP classifiers trained on real data as observed in this paper.</p>
</div>
<div id="S5.p4" class="ltx_para">
<p id="S5.p4.1" class="ltx_p">To facilitate further research in this space, including reproducibility of our results and analysis a wider set of hyperparameters, additional generative models, datasets, and/or tasks beyond classification, we are also open-sourcing our code.<span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a target="_blank" href="https://github.com/ganevgv/dp-gen-disparate" title="" class="ltx_ref ltx_url">https://github.com/ganevgv/dp-gen-disparate</a></span></span></span>
As part of future work, we plan to explore the relationship between disparate effects and fairness, support additional experiments, as well as release a re-usable, modular framework that integrates with other security and privacy evaluations of both discriminative and generative models <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_text" style="font-size:90%;">Liu et al.</span>, <a href="#bib.bib34" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2021</span></a>; <span class="ltx_text" style="font-size:90%;">Stadler et al.</span>, <a href="#bib.bib46" title="" class="ltx_ref"><span class="ltx_text" style="font-size:90%;">2022</span></a>)</cite>.</p>
</div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib1.5.5.1" class="ltx_text" style="font-size:90%;">Abadi et al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib1.7.1" class="ltx_text" style="font-size:90%;">
Abadi, M., Chu, A., Goodfellow, I., McMahan, H. B., Mironov, I., Talwar, K.,
and Zhang, L.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.8.1" class="ltx_text" style="font-size:90%;">Deep learning with differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib1.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM CCS</em><span id="bib.bib1.11.3" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib2.5.5.1" class="ltx_text" style="font-size:90%;">Acs et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib2.7.1" class="ltx_text" style="font-size:90%;">
Acs, G., Melis, L., Castelluccia, C., and De Cristofaro, E.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.8.1" class="ltx_text" style="font-size:90%;">Differentially private mixture of generative neural networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib2.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE TKDE</em><span id="bib.bib2.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib3.5.5.1" class="ltx_text" style="font-size:90%;">Alzantot &amp; Srivastava (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib3.7.1" class="ltx_text" style="font-size:90%;">
Alzantot, M. and Srivastava, M.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.8.1" class="ltx_text" style="font-size:90%;">Differential Privacy Synthetic Data Generation using WGANs.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/nesl/nist_differential_privacy_synthetic_data_challenge/" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://github.com/nesl/nist˙differential˙privacy˙synthetic˙data˙challenge/</a><span id="bib.bib3.9.1" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib4.5.5.1" class="ltx_text" style="font-size:90%;">Arjovsky et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib4.7.1" class="ltx_text" style="font-size:90%;">
Arjovsky, M., Chintala, S., and Bottou, L.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.8.1" class="ltx_text" style="font-size:90%;">Wasserstein generative adversarial networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib4.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib4.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib5.5.5.1" class="ltx_text" style="font-size:90%;">Bagdasaryan et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib5.7.1" class="ltx_text" style="font-size:90%;">
Bagdasaryan, E., Poursaeed, O., and Shmatikov, V.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.8.1" class="ltx_text" style="font-size:90%;">Differential privacy has disparate impact on model accuracy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib5.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bib5.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib6.4.4.1" class="ltx_text" style="font-size:90%;">Barber (2012)</span></span>
<span class="ltx_bibblock"><span id="bib.bib6.6.1" class="ltx_text" style="font-size:90%;">
Barber, D.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib6.7.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Bayesian reasoning and machine learning</em><span id="bib.bib6.8.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.9.1" class="ltx_text" style="font-size:90%;">Cambridge University Press, 2012.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib7.5.5.1" class="ltx_text" style="font-size:90%;">Benedetto et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib7.7.1" class="ltx_text" style="font-size:90%;">
Benedetto, G., Stanley, J. C., Totty, E., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.8.1" class="ltx_text" style="font-size:90%;">The creation and use of the SIPP synthetic Beta v7. 0.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib7.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">US Census Bureau</em><span id="bib.bib7.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib8.4.4.1" class="ltx_text" style="font-size:90%;">Brown (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib8.6.1" class="ltx_text" style="font-size:90%;">
Brown, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.7.1" class="ltx_text" style="font-size:90%;">Synthetic Data Promises Fair AI And Privacy Compliance, But How
Exactly Does It Work?
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://tinyurl.com/yc5vtrhb" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://tinyurl.com/yc5vtrhb</a><span id="bib.bib8.8.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib9.5.5.1" class="ltx_text" style="font-size:90%;">Carlini et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib9.7.1" class="ltx_text" style="font-size:90%;">
Carlini, N., Liu, C., Erlingsson, Ú., Kos, J., and Song, D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.8.1" class="ltx_text" style="font-size:90%;">The secret sharer: Evaluating and testing unintended memorization in
neural networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib9.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">USENIX Security</em><span id="bib.bib9.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib10.5.5.1" class="ltx_text" style="font-size:90%;">Chaudhuri et al. (2011)</span></span>
<span class="ltx_bibblock"><span id="bib.bib10.7.1" class="ltx_text" style="font-size:90%;">
Chaudhuri, K., Monteleoni, C., and Sarwate, A. D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.8.1" class="ltx_text" style="font-size:90%;">Differentially private empirical risk minimization.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib10.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">JMLR</em><span id="bib.bib10.10.2" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib11.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. (2020a)</span></span>
<span class="ltx_bibblock"><span id="bib.bib11.7.1" class="ltx_text" style="font-size:90%;">
Chen, D., Yu, N., Zhang, Y., and Fritz, M.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.8.1" class="ltx_text" style="font-size:90%;">Gan-leaks: A taxonomy of membership inference attacks against
generative models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib11.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM CCS</em><span id="bib.bib11.11.3" class="ltx_text" style="font-size:90%;">, 2020a.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib12.5.5.1" class="ltx_text" style="font-size:90%;">Chen et al. (2020b)</span></span>
<span class="ltx_bibblock"><span id="bib.bib12.7.1" class="ltx_text" style="font-size:90%;">
Chen, X., Wu, S. Z., and Hong, M.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.8.1" class="ltx_text" style="font-size:90%;">Understanding gradient clipping in private SGD: A geometric
perspective.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib12.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bib12.10.2" class="ltx_text" style="font-size:90%;">, 2020b.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib13.5.5.1" class="ltx_text" style="font-size:90%;">Cheng et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib13.7.1" class="ltx_text" style="font-size:90%;">
Cheng, V., Suriyakumar, V. M., Dullerud, N., Joshi, S., and Ghassemi, M.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.8.1" class="ltx_text" style="font-size:90%;">Can You Fake It Until You Make It? Impacts of Differentially Private
Synthetic Data on Downstream Classification Fairness.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib13.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM FAccT</em><span id="bib.bib13.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib14.4.4.1" class="ltx_text" style="font-size:90%;">DSHS (2013)</span></span>
<span class="ltx_bibblock"><span id="bib.bib14.6.1" class="ltx_text" style="font-size:90%;">
DSHS.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.7.1" class="ltx_text" style="font-size:90%;">Texas Hospital Inpatient Discharge Public Use Data File Q1-Q4,
2013.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.dshs.texas.gov/THCIC/Hospitals/Download.shtm" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://www.dshs.texas.gov/THCIC/Hospitals/Download.shtm</a><span id="bib.bib14.8.1" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib15.5.5.1" class="ltx_text" style="font-size:90%;">Dua &amp; Graff (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib15.7.1" class="ltx_text" style="font-size:90%;">
Dua, D. and Graff, C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.8.1" class="ltx_text" style="font-size:90%;">UCI Machine Learning Repository.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://archive.ics.uci.edu/ml/datasets/adult" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://archive.ics.uci.edu/ml/datasets/adult</a><span id="bib.bib15.9.1" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib16.5.5.1" class="ltx_text" style="font-size:90%;">Dwork et al. (2006a)</span></span>
<span class="ltx_bibblock"><span id="bib.bib16.7.1" class="ltx_text" style="font-size:90%;">
Dwork, C., Kenthapadi, K., McSherry, F., Mironov, I., and Naor, M.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.8.1" class="ltx_text" style="font-size:90%;">Our data, ourselves: Privacy via distributed noise generation.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib16.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">EuroCrypt</em><span id="bib.bib16.11.3" class="ltx_text" style="font-size:90%;">, 2006a.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib17.5.5.1" class="ltx_text" style="font-size:90%;">Dwork et al. (2006b)</span></span>
<span class="ltx_bibblock"><span id="bib.bib17.7.1" class="ltx_text" style="font-size:90%;">
Dwork, C., McSherry, F., Nissim, K., and Smith, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.8.1" class="ltx_text" style="font-size:90%;">Calibrating noise to sensitivity in private data analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib17.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">TCC</em><span id="bib.bib17.11.3" class="ltx_text" style="font-size:90%;">, 2006b.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib18.5.5.1" class="ltx_text" style="font-size:90%;">Dwork et al. (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib18.7.1" class="ltx_text" style="font-size:90%;">
Dwork, C., Roth, A., et al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.8.1" class="ltx_text" style="font-size:90%;">The algorithmic foundations of differential privacy.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib18.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Foundations and Trends in Theoretical Computer Science</em><span id="bib.bib18.10.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib19.5.5.1" class="ltx_text" style="font-size:90%;">Farrand et al. (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib19.7.1" class="ltx_text" style="font-size:90%;">
Farrand, T., Mireshghallah, F., Singh, S., and Trask, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.8.1" class="ltx_text" style="font-size:90%;">Neither private nor fair: Impact of data imbalance on utility and
fairness in differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib19.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Workshop on Privacy-Preserving Machine Learning in
Practice</em><span id="bib.bib19.11.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib20.4.4.1" class="ltx_text" style="font-size:90%;">Feldman (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib20.6.1" class="ltx_text" style="font-size:90%;">
Feldman, V.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.7.1" class="ltx_text" style="font-size:90%;">Does learning require memorization? a short tale about a long tail.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.8.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib20.9.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">STOC</em><span id="bib.bib20.10.3" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib21.5.5.1" class="ltx_text" style="font-size:90%;">Frigerio et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib21.7.1" class="ltx_text" style="font-size:90%;">
Frigerio, L., de Oliveira, A. S., Gomez, L., and Duverger, P.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.8.1" class="ltx_text" style="font-size:90%;">Differentially private generative adversarial networks for time
series, continuous, and discrete open data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib21.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IFIP SEC</em><span id="bib.bib21.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib22.5.5.1" class="ltx_text" style="font-size:90%;">Ghalebikesabi et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib22.7.1" class="ltx_text" style="font-size:90%;">
Ghalebikesabi, S., Wilde, H., Jewson, J., Doucet, A., Vollmer, S., and Holmes,
C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.8.1" class="ltx_text" style="font-size:90%;">Bias Mitigated Learning from Differentially Private Synthetic Data:
A Cautionary Tale.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib22.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2108.10934</em><span id="bib.bib22.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib23.5.5.1" class="ltx_text" style="font-size:90%;">Goodfellow et al. (2014)</span></span>
<span class="ltx_bibblock"><span id="bib.bib23.7.1" class="ltx_text" style="font-size:90%;">
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
S., Courville, A., and Bengio, Y.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.8.1" class="ltx_text" style="font-size:90%;">Generative adversarial nets.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib23.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bib23.10.2" class="ltx_text" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib24.5.5.1" class="ltx_text" style="font-size:90%;">Hardt et al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib24.7.1" class="ltx_text" style="font-size:90%;">
Hardt, M., Price, E., and Srebro, N.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.8.1" class="ltx_text" style="font-size:90%;">Equality of opportunity in supervised learning.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib24.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">NeurIPS</em><span id="bib.bib24.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib25.5.5.1" class="ltx_text" style="font-size:90%;">Hayes et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib25.7.1" class="ltx_text" style="font-size:90%;">
Hayes, J., Melis, L., Danezis, G., and De Cristofaro, E.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.8.1" class="ltx_text" style="font-size:90%;">Logan: Membership inference attacks against generative models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib25.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">PoPETs</em><span id="bib.bib25.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib26.5.5.1" class="ltx_text" style="font-size:90%;">Holohan et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib26.7.1" class="ltx_text" style="font-size:90%;">
Holohan, N., Braghin, S., Mac Aonghusa, P., and Levacher, K.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.8.1" class="ltx_text" style="font-size:90%;">Diffprivlib: the IBM differential privacy library.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib26.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1907.02444</em><span id="bib.bib26.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib27.4.4.1" class="ltx_text" style="font-size:90%;">Hong (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib27.6.1" class="ltx_text" style="font-size:90%;">
Hong, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.7.1" class="ltx_text" style="font-size:90%;">Census 2020 +/- 2: Census, Differential Privacy, and the Future of
Data.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.hawaiidata.org/news/2020/9/24/census2020-census-differential-privacy-future-of-data" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://www.hawaiidata.org/news/2020/9/24/census2020-census-differential-privacy-future-of-data</a><span id="bib.bib27.8.1" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib28.5.5.1" class="ltx_text" style="font-size:90%;">Jagielski et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib28.7.1" class="ltx_text" style="font-size:90%;">
Jagielski, M., Kearns, M., Mao, J., Oprea, A., Roth, A., Sharifi-Malvajerdi,
S., and Ullman, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.8.1" class="ltx_text" style="font-size:90%;">Differentially private fair learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib28.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICML</em><span id="bib.bib28.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib29.5.5.1" class="ltx_text" style="font-size:90%;">Jordon et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib29.7.1" class="ltx_text" style="font-size:90%;">
Jordon, J., Yoon, J., and Van Der Schaar, M.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.8.1" class="ltx_text" style="font-size:90%;">PATE-GAN: Generating synthetic data with differential privacy
guarantees.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib29.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ICLR</em><span id="bib.bib29.11.3" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib30.4.4.1" class="ltx_text" style="font-size:90%;">Kaggle (2013)</span></span>
<span class="ltx_bibblock"><span id="bib.bib30.6.1" class="ltx_text" style="font-size:90%;">
Kaggle.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.7.1" class="ltx_text" style="font-size:90%;">Acquire Valued Shoppers Challenge.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data</a><span id="bib.bib30.8.1" class="ltx_text" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib31.5.5.1" class="ltx_text" style="font-size:90%;">Koller &amp; Friedman (2009)</span></span>
<span class="ltx_bibblock"><span id="bib.bib31.7.1" class="ltx_text" style="font-size:90%;">
Koller, D. and Friedman, N.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib31.8.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Probabilistic graphical models: principles and techniques</em><span id="bib.bib31.9.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.10.1" class="ltx_text" style="font-size:90%;">MIT Press, 2009.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib32.5.5.1" class="ltx_text" style="font-size:90%;">Kuppam et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib32.7.1" class="ltx_text" style="font-size:90%;">
Kuppam, S., McKenna, R., Pujol, D., Hay, M., Machanavajjhala, A., and Miklau,
G.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.8.1" class="ltx_text" style="font-size:90%;">Fair decision making using privacy-protected data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib32.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1905.12744</em><span id="bib.bib32.10.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib33.5.5.1" class="ltx_text" style="font-size:90%;">LeCun et al. (2010)</span></span>
<span class="ltx_bibblock"><span id="bib.bib33.7.1" class="ltx_text" style="font-size:90%;">
LeCun, Y., Cortes, C., and Burges, C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.8.1" class="ltx_text" style="font-size:90%;">MNIST handwritten digit database.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib33.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ATT Labs</em><span id="bib.bib33.10.2" class="ltx_text" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib34.5.5.1" class="ltx_text" style="font-size:90%;">Liu et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib34.7.1" class="ltx_text" style="font-size:90%;">
Liu, Y., Wen, R., He, X., Salem, A., Zhang, Z., Backes, M., De Cristofaro, E.,
Fritz, M., and Zhang, Y.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.8.1" class="ltx_text" style="font-size:90%;">ML-Doctor: Holistic Risk Assessment of Inference Attacks Against
Machine Learning Models.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib34.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2102.02551</em><span id="bib.bib34.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib35.5.5.1" class="ltx_text" style="font-size:90%;">McKenna et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib35.7.1" class="ltx_text" style="font-size:90%;">
McKenna, R., Miklau, G., and Sheldon, D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.8.1" class="ltx_text" style="font-size:90%;">Winning the NIST Contest: A scalable and general approach to
differentially private synthetic data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib35.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2108.04978</em><span id="bib.bib35.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib36.5.5.1" class="ltx_text" style="font-size:90%;">McSherry &amp; Talwar (2007)</span></span>
<span class="ltx_bibblock"><span id="bib.bib36.7.1" class="ltx_text" style="font-size:90%;">
McSherry, F. and Talwar, K.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.8.1" class="ltx_text" style="font-size:90%;">Mechanism design via differential privacy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib36.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">FOCS</em><span id="bib.bib36.11.3" class="ltx_text" style="font-size:90%;">, 2007.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib37.4.4.1" class="ltx_text" style="font-size:90%;">NHS England (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib37.6.1" class="ltx_text" style="font-size:90%;">
NHS England.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.7.1" class="ltx_text" style="font-size:90%;">A&amp;E Synthetic Data.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://data.england.nhs.uk/dataset/a-e-synthetic-data" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://data.england.nhs.uk/dataset/a-e-synthetic-data</a><span id="bib.bib37.8.1" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib38.4.4.1" class="ltx_text" style="font-size:90%;">NIST (2018a)</span></span>
<span class="ltx_bibblock"><span id="bib.bib38.6.1" class="ltx_text" style="font-size:90%;">
NIST.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.7.1" class="ltx_text" style="font-size:90%;">2018 Differential Privacy Synthetic Data Challenge.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2018-differential-privacy-synthetic" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2018-differential-privacy-synthetic</a><span id="bib.bib38.8.1" class="ltx_text" style="font-size:90%;">,
2018a.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib39.4.4.1" class="ltx_text" style="font-size:90%;">NIST (2018b)</span></span>
<span class="ltx_bibblock"><span id="bib.bib39.6.1" class="ltx_text" style="font-size:90%;">
NIST.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.7.1" class="ltx_text" style="font-size:90%;">2018 The Unlinkable Data Challenge.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2018-unlinkable-data-challenge" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://www.nist.gov/ctl/pscr/open-innovation-prize-challenges/past-prize-challenges/2018-unlinkable-data-challenge</a><span id="bib.bib39.8.1" class="ltx_text" style="font-size:90%;">,
2018b.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib40.5.5.1" class="ltx_text" style="font-size:90%;">Papernot et al. (2016)</span></span>
<span class="ltx_bibblock"><span id="bib.bib40.7.1" class="ltx_text" style="font-size:90%;">
Papernot, N., Abadi, M., Erlingsson, U., Goodfellow, I., and Talwar, K.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.8.1" class="ltx_text" style="font-size:90%;">Semi-supervised knowledge transfer for deep learning from private
training data.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib40.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1610.05755</em><span id="bib.bib40.10.2" class="ltx_text" style="font-size:90%;">, 2016.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib41.5.5.1" class="ltx_text" style="font-size:90%;">Papernot et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib41.7.1" class="ltx_text" style="font-size:90%;">
Papernot, N., Song, S., Mironov, I., Raghunathan, A., Talwar, K., and
Erlingsson, Ú.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.8.1" class="ltx_text" style="font-size:90%;">Scalable private learning with pate.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib41.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1802.08908</em><span id="bib.bib41.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib42.5.5.1" class="ltx_text" style="font-size:90%;">Pedregosa et al. (2011)</span></span>
<span class="ltx_bibblock"><span id="bib.bib42.7.1" class="ltx_text" style="font-size:90%;">
Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel,
O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J.,
Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.8.1" class="ltx_text" style="font-size:90%;">Scikit-learn: Machine learning in Python.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib42.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">Journal of Machine Learning Research</em><span id="bib.bib42.10.2" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib43.5.5.1" class="ltx_text" style="font-size:90%;">Pereira et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib43.7.1" class="ltx_text" style="font-size:90%;">
Pereira, M., Kshirsagar, M., Mukherjee, S., Dodhia, R., and Ferres, J. L.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.8.1" class="ltx_text" style="font-size:90%;">An Analysis of the Deployment of Models Trained on Private Tabular
Synthetic Data: Unexpected Surprises.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib43.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2106.10241</em><span id="bib.bib43.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib44.5.5.1" class="ltx_text" style="font-size:90%;">Ping et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib44.7.1" class="ltx_text" style="font-size:90%;">
Ping, H., Stoyanovich, J., and Howe, B.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.8.1" class="ltx_text" style="font-size:90%;">DataSynthesizer.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/DataResponsibly/DataSynthesizer" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://github.com/DataResponsibly/DataSynthesizer</a><span id="bib.bib44.9.1" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib45.5.5.1" class="ltx_text" style="font-size:90%;">Shokri et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib45.7.1" class="ltx_text" style="font-size:90%;">
Shokri, R., Stronati, M., Song, C., and Shmatikov, V.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.8.1" class="ltx_text" style="font-size:90%;">Membership inference attacks against machine learning models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib45.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE S&amp;P</em><span id="bib.bib45.11.3" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib46.5.5.1" class="ltx_text" style="font-size:90%;">Stadler et al. (2022)</span></span>
<span class="ltx_bibblock"><span id="bib.bib46.7.1" class="ltx_text" style="font-size:90%;">
Stadler, T., Oprisanu, B., and Troncoso, C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.8.1" class="ltx_text" style="font-size:90%;">Synthetic Data – Anonymization Groundhog Day.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib46.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">Usenix Security</em><span id="bib.bib46.11.3" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib47.5.5.1" class="ltx_text" style="font-size:90%;">Suriyakumar et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib47.7.1" class="ltx_text" style="font-size:90%;">
Suriyakumar, V. M., Papernot, N., Goldenberg, A., and Ghassemi, M.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.8.1" class="ltx_text" style="font-size:90%;">Chasing Your Long Tails: Differentially Private Prediction in Health
Care Settings.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib47.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM FAccT</em><span id="bib.bib47.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib48.5.5.1" class="ltx_text" style="font-size:90%;">Tantipongpipat et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib48.7.1" class="ltx_text" style="font-size:90%;">
Tantipongpipat, U., Waites, C., Boob, D., Siva, A., and Cummings, R.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.8.1" class="ltx_text" style="font-size:90%;">Differentially private mixed-type data generation for unsupervised
learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.9.1" class="ltx_text" style="font-size:90%;">2021.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib49.5.5.1" class="ltx_text" style="font-size:90%;">Thompson &amp; Warzel (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib49.7.1" class="ltx_text" style="font-size:90%;">
Thompson, S. A. and Warzel, C.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.8.1" class="ltx_text" style="font-size:90%;">The Privacy Project: Twelve Million Phones, One Dataset, Zero
Privacy.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nytimes.com/interactive/2019/12/19/opinion/location-tracking-cell-phone.html" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://www.nytimes.com/interactive/2019/12/19/opinion/location-tracking-cell-phone.html</a><span id="bib.bib49.9.1" class="ltx_text" style="font-size:90%;">,
2019.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib50.5.5.1" class="ltx_text" style="font-size:90%;">Tran et al. (2021a)</span></span>
<span class="ltx_bibblock"><span id="bib.bib50.7.1" class="ltx_text" style="font-size:90%;">
Tran, C., Dinh, M. H., Beiter, K., and Fioretto, F.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.8.1" class="ltx_text" style="font-size:90%;">A Fairness Analysis on Private Aggregation of Teacher Ensembles.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib50.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2109.08630</em><span id="bib.bib50.10.2" class="ltx_text" style="font-size:90%;">, 2021a.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib51.5.5.1" class="ltx_text" style="font-size:90%;">Tran et al. (2021b)</span></span>
<span class="ltx_bibblock"><span id="bib.bib51.7.1" class="ltx_text" style="font-size:90%;">
Tran, C., Fioretto, F., and Van Hentenryck, P.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.8.1" class="ltx_text" style="font-size:90%;">Differentially Private and Fair Deep Learning: A Lagrangian Dual
Approach.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib51.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">AAAI</em><span id="bib.bib51.10.2" class="ltx_text" style="font-size:90%;">, 2021b.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib52.5.5.1" class="ltx_text" style="font-size:90%;">Uniyal et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib52.7.1" class="ltx_text" style="font-size:90%;">
Uniyal, A., Naidu, R., Kotti, S., Singh, S., Kenfack, P. J., Mireshghallah, F.,
and Trask, A.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.8.1" class="ltx_text" style="font-size:90%;">DP-SGD vs PATE: Which Has Less Disparate Impact on Model Accuracy?
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib52.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:2106.12576</em><span id="bib.bib52.10.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib53.4.4.1" class="ltx_text" style="font-size:90%;">US Census Bureau (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib53.6.1" class="ltx_text" style="font-size:90%;">
US Census Bureau.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.7.1" class="ltx_text" style="font-size:90%;">Differential Privacy and the 2020 Census.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.census.gov/library/fact-sheets/2021/differential-privacy-and-the-2020-census.html" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://www.census.gov/library/fact-sheets/2021/differential-privacy-and-the-2020-census.html</a><span id="bib.bib53.8.1" class="ltx_text" style="font-size:90%;">,
2021.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib54.5.5.1" class="ltx_text" style="font-size:90%;">Van Der Schaar &amp; Maxfield (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib54.7.1" class="ltx_text" style="font-size:90%;">
Van Der Schaar, M. and Maxfield, N.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.8.1" class="ltx_text" style="font-size:90%;">Synthetic data: Breaking the data logjam in machine learning for
healthcare.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://tinyurl.com/2hr4atnn" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://tinyurl.com/2hr4atnn</a><span id="bib.bib54.9.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib55.5.5.1" class="ltx_text" style="font-size:90%;">Webster et al. (2019)</span></span>
<span class="ltx_bibblock"><span id="bib.bib55.7.1" class="ltx_text" style="font-size:90%;">
Webster, R., Rabin, J., Simon, L., and Jurie, F.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.8.1" class="ltx_text" style="font-size:90%;">Detecting overfitting of deep generative networks via latent
recovery.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib55.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">IEEE CVPR</em><span id="bib.bib55.11.3" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib56.5.5.1" class="ltx_text" style="font-size:90%;">Wezerek &amp; Van Riper (2020)</span></span>
<span class="ltx_bibblock"><span id="bib.bib56.7.1" class="ltx_text" style="font-size:90%;">
Wezerek, G. and Van Riper, D.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.8.1" class="ltx_text" style="font-size:90%;">Changes to the Census Could Make Small Towns Disappear.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.html" title="" class="ltx_ref ltx_url" style="font-size:90%;">https://www.nytimes.com/interactive/2020/02/06/opinion/census-algorithm-privacy.html</a><span id="bib.bib56.9.1" class="ltx_text" style="font-size:90%;">,
2020.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib57.5.5.1" class="ltx_text" style="font-size:90%;">Xie et al. (2018)</span></span>
<span class="ltx_bibblock"><span id="bib.bib57.7.1" class="ltx_text" style="font-size:90%;">
Xie, L., Lin, K., Wang, S., Wang, F., and Zhou, J.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.8.1" class="ltx_text" style="font-size:90%;">Differentially private generative adversarial network.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib57.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">arXiv:1802.06739</em><span id="bib.bib57.10.2" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib58.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. (2017)</span></span>
<span class="ltx_bibblock"><span id="bib.bib58.7.1" class="ltx_text" style="font-size:90%;">
Zhang, J., Cormode, G., Procopiuc, C. M., Srivastava, D., and Xiao, X.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.8.1" class="ltx_text" style="font-size:90%;">Privbayes: Private data release via bayesian networks.
</span>
</span>
<span class="ltx_bibblock"><em id="bib.bib58.9.1" class="ltx_emph ltx_font_italic" style="font-size:90%;">ACM Transactions on Database Systems</em><span id="bib.bib58.10.2" class="ltx_text" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem"><span id="bib.bib59.5.5.1" class="ltx_text" style="font-size:90%;">Zhang et al. (2021)</span></span>
<span class="ltx_bibblock"><span id="bib.bib59.7.1" class="ltx_text" style="font-size:90%;">
Zhang, Z., Wang, T., Li, N., Honorio, J., Backes, M., He, S., Chen, J., and
Zhang, Y.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.8.1" class="ltx_text" style="font-size:90%;">Privsyn: Differentially private data synthesis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.9.1" class="ltx_text" style="font-size:90%;">In </span><em id="bib.bib59.10.2" class="ltx_emph ltx_font_italic" style="font-size:90%;">USENIX Security</em><span id="bib.bib59.11.3" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
</ul>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Additional Results and Plots</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>S1: Texas and Recall Plots</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p">In Fig. <a href="#A1.F12.11" title="Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> we show the size of the binary class in the real and synthetic data as well as the precision of the real, DP, and synth classifiers for the Texas dataset, they are discussed in Section <a href="#S3.SS2" title="3.2 S1: Binary Class Size and Precision ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.2</span></a>.</p>
</div>
<div id="A1.SS1.p2" class="ltx_para">
<p id="A1.SS1.p2.1" class="ltx_p">In Fig. <a href="#A1.F12.11" title="Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> and <a href="#A1.F12.11" title="Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> we plot the recall of the real, DP, and synth classifiers on the Adult and Texas datasets.
For the DP classifiers, recall follows similar patterns as precision for Adult, while, for Texas, there is close to no drop for the underrepresented class and a small drop for the overrepresented class for <math id="A1.SS1.p2.1.m1.1" class="ltx_Math" alttext="\epsilon&lt;0.1" display="inline"><semantics id="A1.SS1.p2.1.m1.1a"><mrow id="A1.SS1.p2.1.m1.1.1" xref="A1.SS1.p2.1.m1.1.1.cmml"><mi id="A1.SS1.p2.1.m1.1.1.2" xref="A1.SS1.p2.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.SS1.p2.1.m1.1.1.1" xref="A1.SS1.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="A1.SS1.p2.1.m1.1.1.3" xref="A1.SS1.p2.1.m1.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p2.1.m1.1b"><apply id="A1.SS1.p2.1.m1.1.1.cmml" xref="A1.SS1.p2.1.m1.1.1"><lt id="A1.SS1.p2.1.m1.1.1.1.cmml" xref="A1.SS1.p2.1.m1.1.1.1"></lt><ci id="A1.SS1.p2.1.m1.1.1.2.cmml" xref="A1.SS1.p2.1.m1.1.1.2">italic-ϵ</ci><cn type="float" id="A1.SS1.p2.1.m1.1.1.3.cmml" xref="A1.SS1.p2.1.m1.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p2.1.m1.1c">\epsilon&lt;0.1</annotation></semantics></math>.</p>
</div>
<div id="A1.SS1.p3" class="ltx_para">
<p id="A1.SS1.p3.4" class="ltx_p">For all synth classifiers in the two datasets recall looks more noisy than precision; for most cases (except for PATE-GAN with <math id="A1.SS1.p3.1.m1.1" class="ltx_Math" alttext="\epsilon=0.01" display="inline"><semantics id="A1.SS1.p3.1.m1.1a"><mrow id="A1.SS1.p3.1.m1.1.1" xref="A1.SS1.p3.1.m1.1.1.cmml"><mi id="A1.SS1.p3.1.m1.1.1.2" xref="A1.SS1.p3.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.SS1.p3.1.m1.1.1.1" xref="A1.SS1.p3.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS1.p3.1.m1.1.1.3" xref="A1.SS1.p3.1.m1.1.1.3.cmml">0.01</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.1.m1.1b"><apply id="A1.SS1.p3.1.m1.1.1.cmml" xref="A1.SS1.p3.1.m1.1.1"><eq id="A1.SS1.p3.1.m1.1.1.1.cmml" xref="A1.SS1.p3.1.m1.1.1.1"></eq><ci id="A1.SS1.p3.1.m1.1.1.2.cmml" xref="A1.SS1.p3.1.m1.1.1.2">italic-ϵ</ci><cn type="float" id="A1.SS1.p3.1.m1.1.1.3.cmml" xref="A1.SS1.p3.1.m1.1.1.3">0.01</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.1.m1.1c">\epsilon=0.01</annotation></semantics></math> in Adult), after initially declining with decreasing <math id="A1.SS1.p3.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.SS1.p3.2.m2.1a"><mi id="A1.SS1.p3.2.m2.1.1" xref="A1.SS1.p3.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.2.m2.1b"><ci id="A1.SS1.p3.2.m2.1.1.cmml" xref="A1.SS1.p3.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.2.m2.1c">\epsilon</annotation></semantics></math> values, the recall of underrepresented class actually starts increasing for <math id="A1.SS1.p3.3.m3.1" class="ltx_Math" alttext="\epsilon&lt;0.1" display="inline"><semantics id="A1.SS1.p3.3.m3.1a"><mrow id="A1.SS1.p3.3.m3.1.1" xref="A1.SS1.p3.3.m3.1.1.cmml"><mi id="A1.SS1.p3.3.m3.1.1.2" xref="A1.SS1.p3.3.m3.1.1.2.cmml">ϵ</mi><mo id="A1.SS1.p3.3.m3.1.1.1" xref="A1.SS1.p3.3.m3.1.1.1.cmml">&lt;</mo><mn id="A1.SS1.p3.3.m3.1.1.3" xref="A1.SS1.p3.3.m3.1.1.3.cmml">0.1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.3.m3.1b"><apply id="A1.SS1.p3.3.m3.1.1.cmml" xref="A1.SS1.p3.3.m3.1.1"><lt id="A1.SS1.p3.3.m3.1.1.1.cmml" xref="A1.SS1.p3.3.m3.1.1.1"></lt><ci id="A1.SS1.p3.3.m3.1.1.2.cmml" xref="A1.SS1.p3.3.m3.1.1.2">italic-ϵ</ci><cn type="float" id="A1.SS1.p3.3.m3.1.1.3.cmml" xref="A1.SS1.p3.3.m3.1.1.3">0.1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.3.m3.1c">\epsilon&lt;0.1</annotation></semantics></math>.
This is most likely because the generated synthetic data is more random; indeed, this is also evident from the large standard deviations in the class size and recall values.
It is interesting to observe that for PATE-GAN in Fig. <a href="#A1.F10.sf3" title="Figure 10(c) ‣ Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">10(c)</span></a>, the underrepresented class recall is actually larger than the real baseline for <math id="A1.SS1.p3.4.m4.1" class="ltx_Math" alttext="\epsilon&gt;1" display="inline"><semantics id="A1.SS1.p3.4.m4.1a"><mrow id="A1.SS1.p3.4.m4.1.1" xref="A1.SS1.p3.4.m4.1.1.cmml"><mi id="A1.SS1.p3.4.m4.1.1.2" xref="A1.SS1.p3.4.m4.1.1.2.cmml">ϵ</mi><mo id="A1.SS1.p3.4.m4.1.1.1" xref="A1.SS1.p3.4.m4.1.1.1.cmml">&gt;</mo><mn id="A1.SS1.p3.4.m4.1.1.3" xref="A1.SS1.p3.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p3.4.m4.1b"><apply id="A1.SS1.p3.4.m4.1.1.cmml" xref="A1.SS1.p3.4.m4.1.1"><gt id="A1.SS1.p3.4.m4.1.1.1.cmml" xref="A1.SS1.p3.4.m4.1.1.1"></gt><ci id="A1.SS1.p3.4.m4.1.1.2.cmml" xref="A1.SS1.p3.4.m4.1.1.2">italic-ϵ</ci><cn type="integer" id="A1.SS1.p3.4.m4.1.1.3.cmml" xref="A1.SS1.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p3.4.m4.1c">\epsilon&gt;1</annotation></semantics></math>.</p>
</div>
<figure id="A1.F12.11" class="ltx_figure ltx_minipage ltx_align_middle" style="width:433.6pt;">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F8.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x19.png" id="A1.F8.sf1.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A1.F8.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F8.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x20.png" id="A1.F8.sf2.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A1.F8.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F8.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x21.png" id="A1.F8.sf3.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F8.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="A1.F8.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Synthetic data class size (top) and real, DP, and synthetic classifiers precision (bottom) for different levels of <math id="A1.F12.2.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F12.2.2.m1.1b"><mi id="A1.F12.2.2.m1.1.1" xref="A1.F12.2.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F12.2.2.m1.1c"><ci id="A1.F12.2.2.m1.1.1.cmml" xref="A1.F12.2.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F12.2.2.m1.1d">\epsilon</annotation></semantics></math>, <span id="A1.F12.11.14.1" class="ltx_text ltx_font_bold ltx_font_italic">Texas</span>, (<span id="A1.F12.11.15.2" class="ltx_text ltx_font_bold ltx_font_italic">S1</span>).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F9.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x22.png" id="A1.F9.sf1.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F9.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A1.F9.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F9.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x23.png" id="A1.F9.sf2.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F9.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A1.F9.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F9.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x24.png" id="A1.F9.sf3.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F9.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="A1.F9.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Real, DP, and synthetic classifiers recall for different levels of <math id="A1.F12.4.4.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F12.4.4.m1.1a"><mi id="A1.F12.4.4.m1.1.1" xref="A1.F12.4.4.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F12.4.4.m1.1b"><ci id="A1.F12.4.4.m1.1.1.cmml" xref="A1.F12.4.4.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F12.4.4.m1.1c">\epsilon</annotation></semantics></math>, <span id="A1.F12.11.18.1" class="ltx_text ltx_font_bold ltx_font_italic">Adult</span>, (<span id="A1.F12.11.19.2" class="ltx_text ltx_font_bold ltx_font_italic">S1</span>).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F10.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x25.png" id="A1.F10.sf1.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F10.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A1.F10.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F10.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x26.png" id="A1.F10.sf2.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F10.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A1.F10.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F10.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x27.png" id="A1.F10.sf3.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F10.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="A1.F10.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Real, DP, and synthetic classifiers recall for different levels of <math id="A1.F12.6.6.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F12.6.6.m1.1a"><mi id="A1.F12.6.6.m1.1.1" xref="A1.F12.6.6.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F12.6.6.m1.1b"><ci id="A1.F12.6.6.m1.1.1.cmml" xref="A1.F12.6.6.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F12.6.6.m1.1c">\epsilon</annotation></semantics></math>, <span id="A1.F12.11.22.1" class="ltx_text ltx_font_bold ltx_font_italic">Texas</span>, (<span id="A1.F12.11.23.2" class="ltx_text ltx_font_bold ltx_font_italic">S1</span>).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F11.sf1" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x28.png" id="A1.F11.sf1.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F11.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A1.F11.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure id="A1.F11.sf2" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x29.png" id="A1.F11.sf2.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F11.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A1.F11.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F11.sf3" class="ltx_figure ltx_figure_panel"><img src="/html/2109.11429/assets/x30.png" id="A1.F11.sf3.g1" class="ltx_graphics ltx_img_landscape" width="152" height="93" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F11.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="A1.F11.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Synthetic data class (multi-class) size relative to real (top) (each bubble denotes a distinct class while the size its relative count in the real data) and DP and synthetic classifiers recall relative to real (bottom) for different levels of <math id="A1.F12.8.8.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F12.8.8.m1.1a"><mi id="A1.F12.8.8.m1.1.1" xref="A1.F12.8.8.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F12.8.8.m1.1b"><ci id="A1.F12.8.8.m1.1.1.cmml" xref="A1.F12.8.8.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F12.8.8.m1.1c">\epsilon</annotation></semantics></math>, <span id="A1.F12.11.26.1" class="ltx_text ltx_font_bold ltx_font_italic">Purchases</span>, (<span id="A1.F12.11.27.2" class="ltx_text ltx_font_bold ltx_font_italic">S2</span>).</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F12.9.9" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x31.png" id="A1.F12.9.9.g1" class="ltx_graphics ltx_img_landscape" width="456" height="148" alt="Refer to caption">
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>Synthetic data class (multi-class) size (top) and real, DP, and synthetic classifiers recall (bottom) for different digits and levels of <math id="A1.F12.11.11.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F12.11.11.m1.1a"><mi id="A1.F12.11.11.m1.1.1" xref="A1.F12.11.11.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F12.11.11.m1.1b"><ci id="A1.F12.11.11.m1.1.1.cmml" xref="A1.F12.11.11.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F12.11.11.m1.1c">\epsilon</annotation></semantics></math>, <span id="A1.F12.11.30.1" class="ltx_text ltx_font_bold ltx_font_italic">MNIST</span> with class “8” downsampled to 0.25 its count, (<span id="A1.F12.11.31.2" class="ltx_text ltx_font_bold ltx_font_italic">S2</span>).</figcaption>
</figure>
<figure id="A1.F13" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F13.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x32.png" id="A1.F13.sf1.g1" class="ltx_graphics ltx_img_landscape" width="228" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F13.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A1.F13.sf1.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN, no-DP</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F13.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x33.png" id="A1.F13.sf2.g1" class="ltx_graphics ltx_img_landscape" width="228" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F13.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A1.F13.sf2.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN, no-DP</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F13.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x34.png" id="A1.F13.sf3.g1" class="ltx_graphics ltx_img_landscape" width="228" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F13.sf3.4.2.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="A1.F13.sf3.2.1" class="ltx_text" style="font-size:70%;">DP-WGAN, <math id="A1.F13.sf3.2.1.m1.1" class="ltx_Math" alttext="\epsilon=15" display="inline"><semantics id="A1.F13.sf3.2.1.m1.1b"><mrow id="A1.F13.sf3.2.1.m1.1.1" xref="A1.F13.sf3.2.1.m1.1.1.cmml"><mi id="A1.F13.sf3.2.1.m1.1.1.2" xref="A1.F13.sf3.2.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.F13.sf3.2.1.m1.1.1.1" xref="A1.F13.sf3.2.1.m1.1.1.1.cmml">=</mo><mn id="A1.F13.sf3.2.1.m1.1.1.3" xref="A1.F13.sf3.2.1.m1.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F13.sf3.2.1.m1.1c"><apply id="A1.F13.sf3.2.1.m1.1.1.cmml" xref="A1.F13.sf3.2.1.m1.1.1"><eq id="A1.F13.sf3.2.1.m1.1.1.1.cmml" xref="A1.F13.sf3.2.1.m1.1.1.1"></eq><ci id="A1.F13.sf3.2.1.m1.1.1.2.cmml" xref="A1.F13.sf3.2.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="A1.F13.sf3.2.1.m1.1.1.3.cmml" xref="A1.F13.sf3.2.1.m1.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F13.sf3.2.1.m1.1d">\epsilon=15</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F13.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x35.png" id="A1.F13.sf4.g1" class="ltx_graphics ltx_img_landscape" width="228" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F13.sf4.4.2.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="A1.F13.sf4.2.1" class="ltx_text" style="font-size:70%;">PATE-GAN, <math id="A1.F13.sf4.2.1.m1.1" class="ltx_Math" alttext="\epsilon=15" display="inline"><semantics id="A1.F13.sf4.2.1.m1.1b"><mrow id="A1.F13.sf4.2.1.m1.1.1" xref="A1.F13.sf4.2.1.m1.1.1.cmml"><mi id="A1.F13.sf4.2.1.m1.1.1.2" xref="A1.F13.sf4.2.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.F13.sf4.2.1.m1.1.1.1" xref="A1.F13.sf4.2.1.m1.1.1.1.cmml">=</mo><mn id="A1.F13.sf4.2.1.m1.1.1.3" xref="A1.F13.sf4.2.1.m1.1.1.3.cmml">15</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F13.sf4.2.1.m1.1c"><apply id="A1.F13.sf4.2.1.m1.1.1.cmml" xref="A1.F13.sf4.2.1.m1.1.1"><eq id="A1.F13.sf4.2.1.m1.1.1.1.cmml" xref="A1.F13.sf4.2.1.m1.1.1.1"></eq><ci id="A1.F13.sf4.2.1.m1.1.1.2.cmml" xref="A1.F13.sf4.2.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="A1.F13.sf4.2.1.m1.1.1.3.cmml" xref="A1.F13.sf4.2.1.m1.1.1.3">15</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F13.sf4.2.1.m1.1d">\epsilon=15</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F13.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x36.png" id="A1.F13.sf5.g1" class="ltx_graphics ltx_img_landscape" width="228" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F13.sf5.4.2.1" class="ltx_text" style="font-size:80%;">(e)</span> </span><span id="A1.F13.sf5.2.1" class="ltx_text" style="font-size:70%;">DP-WGAN, <math id="A1.F13.sf5.2.1.m1.1" class="ltx_Math" alttext="\epsilon=5" display="inline"><semantics id="A1.F13.sf5.2.1.m1.1b"><mrow id="A1.F13.sf5.2.1.m1.1.1" xref="A1.F13.sf5.2.1.m1.1.1.cmml"><mi id="A1.F13.sf5.2.1.m1.1.1.2" xref="A1.F13.sf5.2.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.F13.sf5.2.1.m1.1.1.1" xref="A1.F13.sf5.2.1.m1.1.1.1.cmml">=</mo><mn id="A1.F13.sf5.2.1.m1.1.1.3" xref="A1.F13.sf5.2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F13.sf5.2.1.m1.1c"><apply id="A1.F13.sf5.2.1.m1.1.1.cmml" xref="A1.F13.sf5.2.1.m1.1.1"><eq id="A1.F13.sf5.2.1.m1.1.1.1.cmml" xref="A1.F13.sf5.2.1.m1.1.1.1"></eq><ci id="A1.F13.sf5.2.1.m1.1.1.2.cmml" xref="A1.F13.sf5.2.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="A1.F13.sf5.2.1.m1.1.1.3.cmml" xref="A1.F13.sf5.2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F13.sf5.2.1.m1.1d">\epsilon=5</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F13.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x37.png" id="A1.F13.sf6.g1" class="ltx_graphics ltx_img_landscape" width="228" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F13.sf6.4.2.1" class="ltx_text" style="font-size:80%;">(f)</span> </span><span id="A1.F13.sf6.2.1" class="ltx_text" style="font-size:70%;">PATE-GAN, <math id="A1.F13.sf6.2.1.m1.1" class="ltx_Math" alttext="\epsilon=5" display="inline"><semantics id="A1.F13.sf6.2.1.m1.1b"><mrow id="A1.F13.sf6.2.1.m1.1.1" xref="A1.F13.sf6.2.1.m1.1.1.cmml"><mi id="A1.F13.sf6.2.1.m1.1.1.2" xref="A1.F13.sf6.2.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.F13.sf6.2.1.m1.1.1.1" xref="A1.F13.sf6.2.1.m1.1.1.1.cmml">=</mo><mn id="A1.F13.sf6.2.1.m1.1.1.3" xref="A1.F13.sf6.2.1.m1.1.1.3.cmml">5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F13.sf6.2.1.m1.1c"><apply id="A1.F13.sf6.2.1.m1.1.1.cmml" xref="A1.F13.sf6.2.1.m1.1.1"><eq id="A1.F13.sf6.2.1.m1.1.1.1.cmml" xref="A1.F13.sf6.2.1.m1.1.1.1"></eq><ci id="A1.F13.sf6.2.1.m1.1.1.2.cmml" xref="A1.F13.sf6.2.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="A1.F13.sf6.2.1.m1.1.1.3.cmml" xref="A1.F13.sf6.2.1.m1.1.1.3">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F13.sf6.2.1.m1.1d">\epsilon=5</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F13.sf7" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x38.png" id="A1.F13.sf7.g1" class="ltx_graphics ltx_img_landscape" width="228" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F13.sf7.4.2.1" class="ltx_text" style="font-size:80%;">(g)</span> </span><span id="A1.F13.sf7.2.1" class="ltx_text" style="font-size:70%;">DP-WGAN, <math id="A1.F13.sf7.2.1.m1.1" class="ltx_Math" alttext="\epsilon=0.5" display="inline"><semantics id="A1.F13.sf7.2.1.m1.1b"><mrow id="A1.F13.sf7.2.1.m1.1.1" xref="A1.F13.sf7.2.1.m1.1.1.cmml"><mi id="A1.F13.sf7.2.1.m1.1.1.2" xref="A1.F13.sf7.2.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.F13.sf7.2.1.m1.1.1.1" xref="A1.F13.sf7.2.1.m1.1.1.1.cmml">=</mo><mn id="A1.F13.sf7.2.1.m1.1.1.3" xref="A1.F13.sf7.2.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F13.sf7.2.1.m1.1c"><apply id="A1.F13.sf7.2.1.m1.1.1.cmml" xref="A1.F13.sf7.2.1.m1.1.1"><eq id="A1.F13.sf7.2.1.m1.1.1.1.cmml" xref="A1.F13.sf7.2.1.m1.1.1.1"></eq><ci id="A1.F13.sf7.2.1.m1.1.1.2.cmml" xref="A1.F13.sf7.2.1.m1.1.1.2">italic-ϵ</ci><cn type="float" id="A1.F13.sf7.2.1.m1.1.1.3.cmml" xref="A1.F13.sf7.2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F13.sf7.2.1.m1.1d">\epsilon=0.5</annotation></semantics></math></span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F13.sf8" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x39.png" id="A1.F13.sf8.g1" class="ltx_graphics ltx_img_landscape" width="228" height="113" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F13.sf8.4.2.1" class="ltx_text" style="font-size:80%;">(h)</span> </span><span id="A1.F13.sf8.2.1" class="ltx_text" style="font-size:70%;">PATE-GAN, <math id="A1.F13.sf8.2.1.m1.1" class="ltx_Math" alttext="\epsilon=0.5" display="inline"><semantics id="A1.F13.sf8.2.1.m1.1b"><mrow id="A1.F13.sf8.2.1.m1.1.1" xref="A1.F13.sf8.2.1.m1.1.1.cmml"><mi id="A1.F13.sf8.2.1.m1.1.1.2" xref="A1.F13.sf8.2.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.F13.sf8.2.1.m1.1.1.1" xref="A1.F13.sf8.2.1.m1.1.1.1.cmml">=</mo><mn id="A1.F13.sf8.2.1.m1.1.1.3" xref="A1.F13.sf8.2.1.m1.1.1.3.cmml">0.5</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.F13.sf8.2.1.m1.1c"><apply id="A1.F13.sf8.2.1.m1.1.1.cmml" xref="A1.F13.sf8.2.1.m1.1.1"><eq id="A1.F13.sf8.2.1.m1.1.1.1.cmml" xref="A1.F13.sf8.2.1.m1.1.1.1"></eq><ci id="A1.F13.sf8.2.1.m1.1.1.2.cmml" xref="A1.F13.sf8.2.1.m1.1.1.2">italic-ϵ</ci><cn type="float" id="A1.F13.sf8.2.1.m1.1.1.3.cmml" xref="A1.F13.sf8.2.1.m1.1.1.3">0.5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.F13.sf8.2.1.m1.1d">\epsilon=0.5</annotation></semantics></math></span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Synthetic samples (ordered from “0” to “9” in each subplot) generated by DP-WGAN (left) and PATE-GAN (right) for different <math id="A1.F13.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F13.2.m1.1b"><mi id="A1.F13.2.m1.1.1" xref="A1.F13.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F13.2.m1.1c"><ci id="A1.F13.2.m1.1.1.cmml" xref="A1.F13.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F13.2.m1.1d">\epsilon</annotation></semantics></math> levels, <span id="A1.F13.5.1" class="ltx_text ltx_font_bold ltx_font_italic">MNIST</span> with class “8” downsampled to 0.25 its count, (<span id="A1.F13.6.2" class="ltx_text ltx_font_bold ltx_font_italic">S2</span>).</figcaption>
</figure>
<figure id="A1.F14" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F14.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x40.png" id="A1.F14.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F14.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A1.F14.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes, Adult</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F14.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x41.png" id="A1.F14.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F14.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A1.F14.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN, Adult</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F14.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x42.png" id="A1.F14.sf3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F14.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="A1.F14.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN, Adult</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F14.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x43.png" id="A1.F14.sf4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F14.sf4.3.1.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="A1.F14.sf4.4.2" class="ltx_text" style="font-size:70%;">PrivBayes, Texas</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F14.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x44.png" id="A1.F14.sf5.g1" class="ltx_graphics ltx_img_landscape" width="461" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F14.sf5.3.1.1" class="ltx_text" style="font-size:80%;">(e)</span> </span><span id="A1.F14.sf5.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN, Texas</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F14.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x45.png" id="A1.F14.sf6.g1" class="ltx_graphics ltx_img_landscape" width="461" height="79" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F14.sf6.3.1.1" class="ltx_text" style="font-size:80%;">(f)</span> </span><span id="A1.F14.sf6.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN, Texas</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Synthetic data single-attribute (sex) subgroup size (top) and real, DP, and synthetic classifiers recall accuracy (bottom) for different single-attribute subgroup imbalance and <math id="A1.F14.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F14.2.m1.1b"><mi id="A1.F14.2.m1.1.1" xref="A1.F14.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F14.2.m1.1c"><ci id="A1.F14.2.m1.1.1.cmml" xref="A1.F14.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F14.2.m1.1d">\epsilon</annotation></semantics></math> levels, <span id="A1.F14.6.1" class="ltx_text ltx_font_bold ltx_font_italic">Adult</span> (top 3) and <span id="A1.F14.7.2" class="ltx_text ltx_font_bold ltx_font_italic">Texas</span> (bottom 3), (<span id="A1.F14.8.3" class="ltx_text ltx_font_bold ltx_font_italic">S3</span>).</figcaption>
</figure>
<figure id="A1.F15" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F15.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x46.png" id="A1.F15.sf1.g1" class="ltx_graphics ltx_img_landscape" width="461" height="61" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F15.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A1.F15.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes, Adult</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F15.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x47.png" id="A1.F15.sf2.g1" class="ltx_graphics ltx_img_landscape" width="461" height="61" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F15.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A1.F15.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN, Adult</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure id="A1.F15.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x48.png" id="A1.F15.sf3.g1" class="ltx_graphics ltx_img_landscape" width="461" height="61" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F15.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="A1.F15.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN, Adult</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F15.sf4" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x49.png" id="A1.F15.sf4.g1" class="ltx_graphics ltx_img_landscape" width="461" height="61" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F15.sf4.3.1.1" class="ltx_text" style="font-size:80%;">(d)</span> </span><span id="A1.F15.sf4.4.2" class="ltx_text" style="font-size:70%;">PrivBayes, Texas</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F15.sf5" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x50.png" id="A1.F15.sf5.g1" class="ltx_graphics ltx_img_landscape" width="461" height="61" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F15.sf5.3.1.1" class="ltx_text" style="font-size:80%;">(e)</span> </span><span id="A1.F15.sf5.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN, Texas</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F15.sf6" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x51.png" id="A1.F15.sf6.g1" class="ltx_graphics ltx_img_landscape" width="461" height="61" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F15.sf6.3.1.1" class="ltx_text" style="font-size:80%;">(f)</span> </span><span id="A1.F15.sf6.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN, Texas</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Mutual information between the single-attribute subgroup (sex) and the target (income/length of stay) columns for different single-attribute subgroup imbalance and <math id="A1.F15.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F15.2.m1.1b"><mi id="A1.F15.2.m1.1.1" xref="A1.F15.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F15.2.m1.1c"><ci id="A1.F15.2.m1.1.1.cmml" xref="A1.F15.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F15.2.m1.1d">\epsilon</annotation></semantics></math> levels, <span id="A1.F15.6.1" class="ltx_text ltx_font_bold ltx_font_italic">Adult</span> (top 3) and <span id="A1.F15.7.2" class="ltx_text ltx_font_bold ltx_font_italic">Texas</span> (bottom 3), (<span id="A1.F15.8.3" class="ltx_text ltx_font_bold ltx_font_italic">S3</span>).</figcaption>
</figure>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>S2: Further Purchases and MNIST Plots</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p">For the Purchases dataset, Fig. <a href="#A1.F12.11" title="Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> is duplicate of Fig. <a href="#S3.F2" title="Figure 2 ‣ 3.2 S1: Binary Class Size and Precision ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> but with included error bars.
Looking at the top row, where we display the size of the classes, we observe that PrivBayes has lowest standard deviation (approximately none for “no-DP” and <math id="A1.SS2.p1.1.m1.1" class="ltx_Math" alttext="\epsilon=10" display="inline"><semantics id="A1.SS2.p1.1.m1.1a"><mrow id="A1.SS2.p1.1.m1.1.1" xref="A1.SS2.p1.1.m1.1.1.cmml"><mi id="A1.SS2.p1.1.m1.1.1.2" xref="A1.SS2.p1.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.SS2.p1.1.m1.1.1.1" xref="A1.SS2.p1.1.m1.1.1.1.cmml">=</mo><mn id="A1.SS2.p1.1.m1.1.1.3" xref="A1.SS2.p1.1.m1.1.1.3.cmml">10</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS2.p1.1.m1.1b"><apply id="A1.SS2.p1.1.m1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1"><eq id="A1.SS2.p1.1.m1.1.1.1.cmml" xref="A1.SS2.p1.1.m1.1.1.1"></eq><ci id="A1.SS2.p1.1.m1.1.1.2.cmml" xref="A1.SS2.p1.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="A1.SS2.p1.1.m1.1.1.3.cmml" xref="A1.SS2.p1.1.m1.1.1.3">10</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p1.1.m1.1c">\epsilon=10</annotation></semantics></math>), while DP-WGAN the highest.
For PATE-GAN, unlike the other two models, bigger classes exhibit larger standard deviation.
Finally, observing the recall in the bottom row, PATE-GAN classifiers have the lowest variation.</p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p">For MNIST, Fig. <a href="#A1.F12.11" title="Figure 12 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">12</span></a> displays the size and recall on all digits for MNIST with class “8” undersampled to 0.25 its original size.
In Fig. <a href="#A1.F13" title="Figure 13 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">13</span></a> we can see random synthetic samples produced by DP-WGAN and PATE-GAN for various <math id="A1.SS2.p2.1.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.SS2.p2.1.m1.1a"><mi id="A1.SS2.p2.1.m1.1.1" xref="A1.SS2.p2.1.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.1.m1.1b"><ci id="A1.SS2.p2.1.m1.1.1.cmml" xref="A1.SS2.p2.1.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.1.m1.1c">\epsilon</annotation></semantics></math> budgets.
The results from both plots are analyzed in Section <a href="#S3.SS3" title="3.3 S2: Multi-Class Size and Recall ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
We do not plot or analyze the precision for S2, as the trends are almost identical to recall.</p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>S3: Full Plots</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p">Due to space limitation, in Fig. <a href="#A1.F14" title="Figure 14 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">14</span></a> and <a href="#A1.F15" title="Figure 15 ‣ A.1 S1: Texas and Recall Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">15</span></a> we plot the full set of experiments that are discussed in detail in Section <a href="#S3.SS4" title="3.4 S3: Single-Attribute Subgroup Size, Accuracy, and Correlation ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.4</span></a>.</p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p id="A1.SS3.p2.2" class="ltx_p">Additionally, in Fig. <a href="#A1.F16" title="Figure 16 ‣ A.4 S4: Texas Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">16</span></a> we plot a summary of the overall trends for the Adult dataset.
From the top row, it could be seen that if there is imbalance in the subgroups PrivBayes balances the data set (dark blue lines have positive slope), DP-WGAN maintains well the imbalance for <math id="A1.SS3.p2.1.m1.1" class="ltx_Math" alttext="\epsilon&lt;1" display="inline"><semantics id="A1.SS3.p2.1.m1.1a"><mrow id="A1.SS3.p2.1.m1.1.1" xref="A1.SS3.p2.1.m1.1.1.cmml"><mi id="A1.SS3.p2.1.m1.1.1.2" xref="A1.SS3.p2.1.m1.1.1.2.cmml">ϵ</mi><mo id="A1.SS3.p2.1.m1.1.1.1" xref="A1.SS3.p2.1.m1.1.1.1.cmml">&lt;</mo><mn id="A1.SS3.p2.1.m1.1.1.3" xref="A1.SS3.p2.1.m1.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p2.1.m1.1b"><apply id="A1.SS3.p2.1.m1.1.1.cmml" xref="A1.SS3.p2.1.m1.1.1"><lt id="A1.SS3.p2.1.m1.1.1.1.cmml" xref="A1.SS3.p2.1.m1.1.1.1"></lt><ci id="A1.SS3.p2.1.m1.1.1.2.cmml" xref="A1.SS3.p2.1.m1.1.1.2">italic-ϵ</ci><cn type="integer" id="A1.SS3.p2.1.m1.1.1.3.cmml" xref="A1.SS3.p2.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p2.1.m1.1c">\epsilon&lt;1</annotation></semantics></math> (orange dashed lines stay around 0), while PATE-GAN increases it (dotted green lines are negative).
Looking at the bottom row, it could be seen that almost all lines have a positive slope, meaning that with decreased <math id="A1.SS3.p2.2.m2.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.SS3.p2.2.m2.1a"><mi id="A1.SS3.p2.2.m2.1.1" xref="A1.SS3.p2.2.m2.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.p2.2.m2.1b"><ci id="A1.SS3.p2.2.m2.1.1.cmml" xref="A1.SS3.p2.2.m2.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p2.2.m2.1c">\epsilon</annotation></semantics></math> the accuracy of the minority subgroup drop quicker than the majority.
Furthermore, darker lines (i.e., more imbalanced datasets) tend to be on top of lighter (i.e., more balanced datasets) which means that increasing the imbalance results in even larger/quicker minority subgroup accuracy drop relative to the majority.</p>
</div>
</section>
<section id="A1.SS4" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.4 </span>S4: Texas Plots</h3>

<div id="A1.SS4.p1" class="ltx_para">
<p id="A1.SS4.p1.1" class="ltx_p">Due to space limitation, in Fig. <a href="#A1.F17" title="Figure 17 ‣ A.4 S4: Texas Plots ‣ Appendix A Additional Results and Plots ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">17</span></a> we plot the multi-attribute subgroup experiments for the Texas dataset, they are discussed in Section <a href="#S3.SS5" title="3.5 S4: Multi-Attribute Subgroup Size, Accuracy, and Correlation ‣ 3 Experimental Evaluation ‣ Robin Hood and Matthew Effects: Differential Privacy Has Disparate Impact on Synthetic Data" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3.5</span></a>.</p>
</div>
<figure id="A1.F16" class="ltx_figure ltx_align_center"><img src="/html/2109.11429/assets/x52.png" id="A1.F16.1.g1" class="ltx_graphics ltx_img_landscape" width="228" height="158" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Minority single-attribute (sex) subgroup imbalance level difference (top) and minority subgroup accuracy drop difference (bottom) relative to majority for different subgroup imbalance and <math id="A1.F16.3.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F16.3.m1.1b"><mi id="A1.F16.3.m1.1.1" xref="A1.F16.3.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F16.3.m1.1c"><ci id="A1.F16.3.m1.1.1.cmml" xref="A1.F16.3.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F16.3.m1.1d">\epsilon</annotation></semantics></math> levels, <span id="A1.F16.6.1" class="ltx_text ltx_font_bold ltx_font_italic">Adult</span>, (<span id="A1.F16.7.2" class="ltx_text ltx_font_bold ltx_font_italic">S3</span>).</figcaption>
</figure>
<figure id="A1.F17" class="ltx_figure">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F17.sf1" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x53.png" id="A1.F17.sf1.g1" class="ltx_graphics ltx_img_landscape" width="300" height="114" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F17.sf1.3.1.1" class="ltx_text" style="font-size:80%;">(a)</span> </span><span id="A1.F17.sf1.4.2" class="ltx_text" style="font-size:70%;">PrivBayes</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F17.sf2" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x54.png" id="A1.F17.sf2.g1" class="ltx_graphics ltx_img_landscape" width="300" height="114" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F17.sf2.3.1.1" class="ltx_text" style="font-size:80%;">(b)</span> </span><span id="A1.F17.sf2.4.2" class="ltx_text" style="font-size:70%;">DP-WGAN</span></figcaption>
</figure>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<figure id="A1.F17.sf3" class="ltx_figure ltx_figure_panel ltx_align_center"><img src="/html/2109.11429/assets/x55.png" id="A1.F17.sf3.g1" class="ltx_graphics ltx_img_landscape" width="300" height="114" alt="Refer to caption">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span id="A1.F17.sf3.3.1.1" class="ltx_text" style="font-size:80%;">(c)</span> </span><span id="A1.F17.sf3.4.2" class="ltx_text" style="font-size:70%;">PATE-GAN</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Synthetic data multi-attribute (intersection of age, sex, and race) subgroup size relative to real (top) (each bubble denotes a distinct subgroup while the size its relative count in the real data) and DP and synthetic classifiers accuracy relative to real (bottom) for different single-attribute (sex) subgroup imbalance and <math id="A1.F17.2.m1.1" class="ltx_Math" alttext="\epsilon" display="inline"><semantics id="A1.F17.2.m1.1b"><mi id="A1.F17.2.m1.1.1" xref="A1.F17.2.m1.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="A1.F17.2.m1.1c"><ci id="A1.F17.2.m1.1.1.cmml" xref="A1.F17.2.m1.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.F17.2.m1.1d">\epsilon</annotation></semantics></math> levels, <span id="A1.F17.5.1" class="ltx_text ltx_font_bold ltx_font_italic">Texas</span>, (<span id="A1.F17.6.2" class="ltx_text ltx_font_bold ltx_font_italic">S4</span>).</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2109.11428" class="ar5iv-nav-button ar5iv-nav-button-prev">◄</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2109.11429" class="ar5iv-text-button ar5iv-severity-ok">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2109.11429">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2109.11429" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2109.11430" class="ar5iv-nav-button ar5iv-nav-button-next">►</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Wed Mar  6 21:19:49 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "×";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
