<!DOCTYPE html><html lang="en">
<head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
<title>[2304.03464] Linking Representations with Multimodal Contrastive Learning</title><meta property="og:description" content="Many applications require grouping instances contained in diverse document datasets into classes. Most widely used methods do not employ deep learning and do not exploit the inherently multimodal nature of documents. N‚Ä¶">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Linking Representations with Multimodal Contrastive Learning">
<meta name="twitter:image:src" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta name="twitter:image:alt" content="ar5iv logo">
<meta property="og:title" content="Linking Representations with Multimodal Contrastive Learning">
<meta property="og:site_name" content="ar5iv">
<meta property="og:image" content="https://ar5iv.labs.arxiv.org/assets/ar5iv_card.png">
<meta property="og:type" content="article">
<meta property="og:url" content="https://ar5iv.labs.arxiv.org/html/2304.03464">

<!--Generated on Thu Feb 29 15:52:22 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<script>
  function detectColorScheme(){
    var theme="light";
    var current_theme = localStorage.getItem("ar5iv_theme");
    if(current_theme){
      if(current_theme == "dark"){
        theme = "dark";
      } }
    else if(!window.matchMedia) { return false; }
    else if(window.matchMedia("(prefers-color-scheme: dark)").matches) {
      theme = "dark"; }
    if (theme=="dark") {
      document.documentElement.setAttribute("data-theme", "dark");
    } else {
      document.documentElement.setAttribute("data-theme", "light"); } }

  detectColorScheme();

  function toggleColorScheme(){
    var current_theme = localStorage.getItem("ar5iv_theme");
    if (current_theme) {
      if (current_theme == "light") {
        localStorage.setItem("ar5iv_theme", "dark"); }
      else {
        localStorage.setItem("ar5iv_theme", "light"); } }
    else {
        localStorage.setItem("ar5iv_theme", "dark"); }
    detectColorScheme(); }
</script>
<link media="all" rel="stylesheet" href="/assets/ar5iv-fonts.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv.0.8.0.min.css"><link media="all" rel="stylesheet" href="/assets/ar5iv-site.0.2.2.css">
</head>
<body>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Linking Representations with Multimodal Contrastive Learning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Abhishek Arora
<br class="ltx_break">Harvard University 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xinmei Yang 
<br class="ltx_break">Renmin University 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Shao-Yu Jheng 
<br class="ltx_break">Harvard University 
<br class="ltx_break">
</span></span>
<span class="ltx_author_before">‚ÄÉ‚ÄÉ</span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Melissa Dell 
<br class="ltx_break">Harvard University
<br class="ltx_break">NBER
</span></span>
</div>

<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p id="id1.id1" class="ltx_p">Many applications require grouping instances contained in diverse document datasets into classes. Most widely used methods do not employ deep learning and do not exploit the inherently multimodal nature of documents. Notably, record linkage is typically conceptualized as a probabilistic or deterministic string matching problem. In record linkage problems, the number of classes is often extremely large, unknown ex ante, or constantly evolving. To address these challenges, this study develops
<span id="id1.id1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> (<span id="id1.id1.2" class="ltx_text ltx_font_bold">C</span>ontrastively <span id="id1.id1.3" class="ltx_text ltx_font_bold">LI</span>nking <span id="id1.id1.4" class="ltx_text ltx_font_bold">P</span>ooled <span id="id1.id1.5" class="ltx_text ltx_font_bold">P</span>re-trained Embedd<span id="id1.id1.6" class="ltx_text ltx_font_bold">ings</span>). <span id="id1.id1.7" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> employs end-to-end training of symmetric vision and language bi-encoders, aligned through contrastive language-image pre-training, to learn a metric space where the pooled image-text representation for a given instance is close to representations in the same class and distant from representations in different classes. At inference time, instances can be linked by retrieving their nearest neighbor from an offline exemplar embedding index or by clustering their representations. The study examines two challenging applications: constructing comprehensive supply chains for mid-20th century Japan through linking firm level financial records - with each firm name represented by its crop in the document image and the corresponding OCR - and detecting which image-caption pairs in a massive corpus of historical U.S. newspapers came from the same underlying photo wire source. <span id="id1.id1.8" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> outperforms widely used string matching methods by a wide margin and also outperforms unimodal methods. Moreover, a purely self-supervised model trained on only image-OCR pairs - while below the supervised gold-standard - still outperforms popular string matching methods without requiring any labels.</p>
</div>
<section id="S1" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>

<div id="S1.p1" class="ltx_para">
<p id="S1.p1.1" class="ltx_p">Linking information across diverse sources is fundamental to many pipelines. To name a few examples, researchers and businesses frequently link individuals or firms across censuses and company records, governments de-duplicate benefit or voter rolls across locations, and analysts seek to identify how information from the same source spreads through media. In large swathes of the relevant literatures, deep neural methods have made few inroads. For example, a recent comprehensive review of the computer science, social science, and statistical record linkage literatures in <span id="S1.p1.1.1" class="ltx_text ltx_font_italic">Science Advances</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> concludes
that other methods are preferred over deep neural models for record linkage in structured data.
This contrasts to some seemingly similar problems, such as disambiguating entities in unstructured texts to a knowledgebase like Wikipedia, where transformer language models overwhelmingly predominate, <span id="S1.p1.1.2" class="ltx_text ltx_font_italic">e.g.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>.</p>
</div>
<div id="S1.p2" class="ltx_para">
<p id="S1.p2.1" class="ltx_p">This study examines whether deep learning can significantly enhance record linkage in contexts where it has not traditionally been applied, developing a novel multimodal framework - <span id="S1.p2.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> (<span id="S1.p2.1.2" class="ltx_text ltx_font_bold">C</span>ontrastively <span id="S1.p2.1.3" class="ltx_text ltx_font_bold">LI</span>nking <span id="S1.p2.1.4" class="ltx_text ltx_font_bold">P</span>ooled <span id="S1.p2.1.5" class="ltx_text ltx_font_bold">P</span>re-trained Embedd<span id="S1.p2.1.6" class="ltx_text ltx_font_bold">ings</span>) - that can be applied to a wide variety of cross-document linking tasks.
<span id="S1.p2.1.7" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> is motivated by the insight that many record linkage applications can be conceptualized as a multimodal classification problem, in which the document image crop of an entity‚Äôs name and its corresponding text from optical character recognition (OCR) are the inputs, and each unique entity is a class.
More generally, document texts and photographs that appear in the document could be used for cross-document linkage. Two example applications are illustrated in Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>. The first consists of the localized names of Japanese firms in historical document image scans and their OCR. The second consists of image-caption clippings from historical U.S. newspapers.</p>
</div>
<figure id="S1.F1" class="ltx_figure"><img src="/html/2304.03464/assets/figures/dataset_v5.png" id="S1.F1.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="176" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Data: This figure shows representative linked records from the Japanese firm and historical newspaper image-caption datasets.</figcaption>
</figure>
<div id="S1.p3" class="ltx_para">
<p id="S1.p3.1" class="ltx_p">Standard powerful methods for image-text classification - used for applications such as hateful memes <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib32" title="" class="ltx_ref">32</a>]</cite> or product descriptions <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib39" title="" class="ltx_ref">39</a>, <a href="#bib.bib31" title="" class="ltx_ref">31</a>]</cite> - use a supervised classification objective that cannot be applied to problems like record linkage that have a very large number (potentially many millions) of classes, have classes that are unknown ex ante, or have classes that are constantly evolving as databases are updated. Much as in unimodal applications, such settings can be tackled by learning a metric space where classes can be assigned using kNN classification or clustering.</p>
</div>
<div id="S1.p4" class="ltx_para">
<p id="S1.p4.1" class="ltx_p"><span id="S1.p4.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> employs end-to-end training of symmetric vision and language bi-encoders to learn a metric space where the pooled image-text representation for an instance is close to representations in the same class and distant from representations in different classes.
We first implement self-supervised contrastive language-image pre-training <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, starting with CLIP embeddings <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>, <a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite>, to align the image and text encoders. The self-supervised model can be used off-the-shelf, or contrastively tuned using the <span id="S1.p4.1.2" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> framework on labeled linked data.
At inference time, instances can be linked by retrieving their nearest neighbor from an offline exemplar embedding index or by clustering their representations. The contrastive framework can flexibly incorporate blocking on database characteristics - common in record linkage - by using a type-specific loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>.</p>
</div>
<div id="S1.p5" class="ltx_para">
<p id="S1.p5.1" class="ltx_p"><span id="S1.p5.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> significantly outperforms string matching methods - which predominate in the record linkage literature - as well as unimodal methods. The primary context examined is a traditional record linkage application, that constructs comprehensive supply chains for mid-20th century Japan through linking firm level financial records. The firm is represented by the image crop of its name in the document scan and the corresponding OCR (Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Supply chains are fundamental to the transmission of economic shocks <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib3" title="" class="ltx_ref">3</a>, <a href="#bib.bib4" title="" class="ltx_ref">4</a>]</cite>, agglomeration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib17" title="" class="ltx_ref">17</a>]</cite>, and economic development <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib20" title="" class="ltx_ref">20</a>, <a href="#bib.bib42" title="" class="ltx_ref">42</a>, <a href="#bib.bib47" title="" class="ltx_ref">47</a>, <a href="#bib.bib7" title="" class="ltx_ref">7</a>, <a href="#bib.bib34" title="" class="ltx_ref">34</a>]</cite>. Their role in long-run economic development has been difficult to study quantitatively, due largely to the difficulties of accurately linking large-scale historical records, as sampling in networks can significantly bias statistical analyses <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib13" title="" class="ltx_ref">13</a>]</cite>.</p>
</div>
<div id="S1.p6" class="ltx_para">
<p id="S1.p6.1" class="ltx_p"><span id="S1.p6.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> is trained on a modest number of linked firms using supervised contrastive learning, following self-supervised language-image pre-training on the image crop-OCR pairs. At inference time, customer-supplier records <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib25" title="" class="ltx_ref">25</a>]</cite> are linked to their nearest neighbor in an exemplar offline embeddings index, computed using a comprehensive directory that contains rich firm-level data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib56" title="" class="ltx_ref">56</a>]</cite>.
<span id="S1.p6.1.2" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> achieves a 94.5% record linkage accuracy, compared to a string matching maximum accuracy of 73.1%.</p>
</div>
<div id="S1.p7" class="ltx_para">
<p id="S1.p7.1" class="ltx_p">Particularly in academic research, fine-tuning on a modest amount of labeled data can be more of a feature than a bug, providing the fine-grained control that is typical of scientific research.
Yet when we forgo supervised contrastive training - using only self-supervised language-image pre-training on the image-OCR pairs - this also outperforms traditional string matching, with an accuracy of 84.9%.
Moreover, supervised contrastive training of the aligned vision encoder on only images outperforms the same training on an encoder with unimodal rather than multimodal pre-training, as the aligned vision encoder acquired some language understanding through multimodal pre-training. This underscores the power of multimodal methods for document analyses.</p>
</div>
<div id="S1.p8" class="ltx_para">
<p id="S1.p8.1" class="ltx_p"><span id="S1.p8.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> does not model cross-modal attention, because in record linkage applications where the inputs are an image of a text and the corresponding OCR, cross-modal attention is unlikely to lead to significantly richer representations.
If desired, though, it would be straightforward to extend the framework to include cross-modal attention.</p>
</div>
<div id="S1.p9" class="ltx_para">
<p id="S1.p9.1" class="ltx_p">To test whether <span id="S1.p9.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> can apply to not only document images clips and their OCR but also to images and text more generally, we use it to measure which image-caption pairs in a large corpus of historical U.S. off-copyright newspapers came from the same underlying photo news wire source (Figure <a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">1</span></a>). Local papers obtained many of their images from news wire such as the Associated Press. Captions from the wire could be re-written or abridged. Images, on the other hand, can be very poor resolution (transmitted via telegraph), and there is also noise from cropping, scanning, etc. <span id="S1.p9.1.2" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> again significantly outperforms popular string matching methods.</p>
</div>
<div id="S1.p10" class="ltx_para">
<p id="S1.p10.1" class="ltx_p"><span id="S1.p10.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> can be trained and deployed with modest compute, which is central given its relevance to applications in academic research and government. This also makes it a feasible tool for removing noisy duplicates from massive image-caption training corpora, important for reducing the likelihood of training data regeneration <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite>.</p>
</div>
<div id="S1.p11" class="ltx_para">
<p id="S1.p11.1" class="ltx_p">The <span id="S1.p11.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> models and training data will be publicly released, in order to encourage further work on deep, multimodal record linkage.</p>
</div>
<div id="S1.p12" class="ltx_para">
<p id="S1.p12.1" class="ltx_p">The rest of this study is organized as follows:
Section <a href="#S2" title="2 Literature ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> discusses the literature, and
Section <a href="#S3" title="3 Model Architecture ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">3</span></a> describes the <span id="S1.p12.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> architecture. Section <a href="#S4" title="4 Record Linkage ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">4</span></a> applies <span id="S1.p12.1.2" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> to the firm record linkage problem, and Section <a href="#S5" title="5 Noisy Image-Caption Duplicates ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">5</span></a> applies it to detecting noisy image-caption duplicates in historical newspapers. Section <a href="#S6" title="6 Conclusion ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">6</span></a> concludes.</p>
</div>
</section>
<section id="S2" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Literature</h2>

<div id="S2.p1" class="ltx_para">
<p id="S2.p1.1" class="ltx_p"><span id="S2.p1.1.1" class="ltx_text ltx_font_bold">Contrastive learning</span>:</p>
</div>
<div id="S2.p2" class="ltx_para">
<p id="S2.p2.1" class="ltx_p"><span id="S2.p2.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> learns a metric space for pooled image-text representations that can be applied to linkage problems even when the number of classes is extremely large, unknown ex ante, or constantly changes as databases are updated. It is reminiscent of a variety of unimodal bi-encoder applications, such as semantic similarity <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib48" title="" class="ltx_ref">48</a>]</cite>, passage retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib28" title="" class="ltx_ref">28</a>]</cite>, and entity disambiguation <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>]</cite> and co-reference resolution <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib21" title="" class="ltx_ref">21</a>]</cite> in unstructured text. In order to create pooled image-text representations, it is necessary to have an aligned space.
Contrastive Language-Image Pre-training (CLIP) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> contrastively trained aligned text and image encoders using 400 million image-caption pairs.
<span id="S2.p2.1.2" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> begins with pre-trained Japanese <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> and English <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite> CLIP image and text encoders.</p>
</div>
<div id="S2.p3" class="ltx_para">
<p id="S2.p3.1" class="ltx_p"><span id="S2.p3.1.1" class="ltx_text ltx_font_bold">Record Linkage:</span></p>
</div>
<div id="S2.p4" class="ltx_para">
<p id="S2.p4.1" class="ltx_p">A variety of disciplines - including computer science, statistics, database management, economics, and political science - have made extensive methodological contributions to record linkage, alternatively referred to as entity resolution, fuzzy matching, approximate dictionary matching, and string matching.</p>
</div>
<div id="S2.p5" class="ltx_para">
<p id="S2.p5.1" class="ltx_p">Within this sprawling landscape, the literatures on entity resolution in structured databases <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> versus natural language text remain largely divorced.
While deep learning has transformed methods for disambiguating entities in unstructured texts to a knowledgebase, <span id="S2.p5.1.1" class="ltx_text ltx_font_italic">e.g.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib61" title="" class="ltx_ref">61</a>, <a href="#bib.bib15" title="" class="ltx_ref">15</a>, <a href="#bib.bib63" title="" class="ltx_ref">63</a>]</cite>, deep models have made few inroads in the large literature on record linkage using structured databases.
This literature emphasizes linking noisy text fields that contain information such as individual names, firm names, organizations, or locations.
Edit distance metrics are commonly used, <span id="S2.p5.1.2" class="ltx_text ltx_font_italic">e.g.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>, <a href="#bib.bib23" title="" class="ltx_ref">23</a>, <a href="#bib.bib60" title="" class="ltx_ref">60</a>]</cite>.</p>
</div>
<div id="S2.p6" class="ltx_para">
<p id="S2.p6.3" class="ltx_p">Another widespread approach computes the cosine similarity between <math id="S2.p6.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p6.1.m1.1a"><mi id="S2.p6.1.m1.1.1" xref="S2.p6.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p6.1.m1.1b"><ci id="S2.p6.1.m1.1.1.cmml" xref="S2.p6.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.1.m1.1c">n</annotation></semantics></math>-gram representations of strings, where <math id="S2.p6.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p6.2.m2.1a"><mi id="S2.p6.2.m2.1.1" xref="S2.p6.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p6.2.m2.1b"><ci id="S2.p6.2.m2.1.1.cmml" xref="S2.p6.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.2.m2.1c">n</annotation></semantics></math>-grams are defined as all substrings of size <math id="S2.p6.3.m3.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p6.3.m3.1a"><mi id="S2.p6.3.m3.1.1" xref="S2.p6.3.m3.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p6.3.m3.1b"><ci id="S2.p6.3.m3.1.1.cmml" xref="S2.p6.3.m3.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p6.3.m3.1c">n</annotation></semantics></math> in a string <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib44" title="" class="ltx_ref">44</a>]</cite>. There have also been some efforts to estimate machine learning models for record linkage. For example, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib58" title="" class="ltx_ref">58</a>]</cite> use a random forest classifier trained on labeled data to disambiguate authors of U.S. patents, applying clustering to the resulting dissimilarity scores to enforce transitivity.</p>
</div>
<figure id="S2.F2" class="ltx_figure"><img src="/html/2304.03464/assets/figures/model_architecture.png" id="S2.F2.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="143" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Model Architecture: This figure illustrates the <span id="S2.F2.2.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> model architecture.</figcaption>
</figure>
<div id="S2.p7" class="ltx_para">
<p id="S2.p7.1" class="ltx_p">Because labeled record linkage datasets are very small compared to the massive corpora used for training transformer models from scratch, a comprehensive 2022 review of the record linkage literature in <span id="S2.p7.1.1" class="ltx_text ltx_font_italic">Science Advances</span> <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib8" title="" class="ltx_ref">8</a>]</cite> concludes that deep neural models are unlikely to be applicable to entity resolution using structured data. Constructing training data for record linkage is indeed highly labor intensive, but much of the knowledge needed to improve record linkage is plausibly already encapsulated in pre-trained image and language encoders such as
CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>, or can be gleaned from the further self-supervised language-image pre-training pursued in this study.
In their simplest form, approximate string matching methods simply count the required number of edits (insertions, deletions, substitutions) to transform one string into another. In practice, not all substitutions are equally probable, leading to efforts to construct rule-based lists that adjust the costs of substitutions.
For example, the fuzzychinese <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib65" title="" class="ltx_ref">65</a>]</cite> package uses strokes or radicals as the fundamental unit for <math id="S2.p7.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p7.1.m1.1a"><mi id="S2.p7.1.m1.1.1" xref="S2.p7.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p7.1.m1.1b"><ci id="S2.p7.1.m1.1.1.cmml" xref="S2.p7.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p7.1.m1.1c">n</annotation></semantics></math>-grams substring representations of entities, where these strokes and radicals are drawn from an external database <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib29" title="" class="ltx_ref">29</a>]</cite> covering a subset of the CJK script. Alternatively, the masala merge package <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib43" title="" class="ltx_ref">43</a>]</cite> adjusts Levenshtein distance <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib37" title="" class="ltx_ref">37</a>]</cite> to impose smaller penalties for common alternative spellings in Hindi. Soundex, first developed in 1918 - together with the updated 1970 New York State Identification and Intelligence System (NYSIIS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib51" title="" class="ltx_ref">51</a>]</cite> - account for the fact that similar sounding substitutions are more likely since census enumerators misspelled names according to their sound. These remain a bedrock for record linkage in historical U.S. census data <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib2" title="" class="ltx_ref">2</a>]</cite>.</p>
</div>
<div id="S2.p8" class="ltx_para">
<p id="S2.p8.1" class="ltx_p">Such rule-based methods may perform well in the contexts to which they are tailored. However, they can be brittle and are labor-intensive to extend to new settings, due to the use of hand-crafted features. This heavily skews linked datasets towards a few high resource settings for which existing methods have been tailored, in turn skewing downstream knowledge. Even in high resource settings, low accuracy in some applications can require extensive human intervention in the matching process to achieve the desired accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib6" title="" class="ltx_ref">6</a>]</cite>, limiting the scale of problems.</p>
</div>
<div id="S2.p9" class="ltx_para">
<p id="S2.p9.1" class="ltx_p">An additional limitation of string matching is that it is unimodal, despite a growing emphasis on processing documents with multimodal models or OCR-free vision-only models <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib46" title="" class="ltx_ref">46</a>, <a href="#bib.bib18" title="" class="ltx_ref">18</a>, <a href="#bib.bib41" title="" class="ltx_ref">41</a>, <a href="#bib.bib49" title="" class="ltx_ref">49</a>, <a href="#bib.bib62" title="" class="ltx_ref">62</a>, <a href="#bib.bib33" title="" class="ltx_ref">33</a>, <a href="#bib.bib24" title="" class="ltx_ref">24</a>, <a href="#bib.bib5" title="" class="ltx_ref">5</a>, <a href="#bib.bib38" title="" class="ltx_ref">38</a>, <a href="#bib.bib22" title="" class="ltx_ref">22</a>]</cite>.</p>
</div>
<div id="S2.p10" class="ltx_para">
<p id="S2.p10.1" class="ltx_p"><span id="S2.p10.1.1" class="ltx_text ltx_font_bold">Noisy Duplicates:</span>
Identifying noisy duplicates in large corpora may be of inherent interest for downstream questions, for example, studying how the dissemination of iconic images through print media transformed public opinion. Second, duplicates significantly increase the regeneration of training data by large language models and Stable Diffusion <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib35" title="" class="ltx_ref">35</a>, <a href="#bib.bib27" title="" class="ltx_ref">27</a>, <a href="#bib.bib10" title="" class="ltx_ref">10</a>, <a href="#bib.bib9" title="" class="ltx_ref">9</a>]</cite> raising copyright and plagiarism risks.
Third, duplicates can contribute to test set leakage. <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> find that GPT-3 performs substantially better on samples from ReCoRD that have near duplicates in RealNews, a subset of Common Crawl included in the GPT-3 training data, compared to those that did not.
While most de-duplication of noisy text focuses on <math id="S2.p10.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S2.p10.1.m1.1a"><mi id="S2.p10.1.m1.1.1" xref="S2.p10.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.p10.1.m1.1b"><ci id="S2.p10.1.m1.1.1.cmml" xref="S2.p10.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.p10.1.m1.1c">n</annotation></semantics></math>-grams or locally sensitive hashing, <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib52" title="" class="ltx_ref">52</a>]</cite> show that neural de-duplication using a contrastively trained S-BERT bi-encoder significantly outperforms these methods and is highly scalable, consistent with <span id="S2.p10.1.2" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> being a potential tool for de-duplicating large image-caption datasets.</p>
</div>
</section>
<section id="S3" class="ltx_section">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Model Architecture</h2>

<div id="S3.p1" class="ltx_para">
<p id="S3.p1.1" class="ltx_p">Figure <a href="#S2.F2" title="Figure 2 ‚Ä£ 2 Literature ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref"><span class="ltx_text ltx_ref_tag">2</span></a> shows the <span id="S3.p1.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> architecture. <span id="S3.p1.1.2" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> employs end-to-end training of symmetric vision and language bi-encoders to learn a metric space where the pooled image-text representation for a given instance is close to representations in the same class and distant from representations in different classes. The baseline uses a supervised contrastive loss <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib30" title="" class="ltx_ref">30</a>]</cite> on the pooled representations:</p>
</div>
<div id="S3.p2" class="ltx_para">
<table id="S3.E1" class="ltx_equation ltx_eqn_table">

<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math id="S3.E1.m1.7" class="ltx_Math" alttext="\mathcal{L}=-\sum_{i\in\mathcal{B}}\frac{1}{|\mathcal{P}(i)|}\sum_{k\in\mathcal{P}(i)}\log\frac{\exp\left(\tau\boldsymbol{(}{z}_{i})^{T}({z}_{k})\right)}{\sum_{j\in\mathcal{B}}\exp\left(\tau\boldsymbol{(}{z}_{i})^{T}\boldsymbol{(}{z}_{j})\right)}" display="block"><semantics id="S3.E1.m1.7a"><mrow id="S3.E1.m1.7.8" xref="S3.E1.m1.7.8.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.7.8.2" xref="S3.E1.m1.7.8.2.cmml">‚Ñí</mi><mo mathsize="90%" id="S3.E1.m1.7.8.1" xref="S3.E1.m1.7.8.1.cmml">=</mo><mrow id="S3.E1.m1.7.8.3" xref="S3.E1.m1.7.8.3.cmml"><mo mathsize="90%" id="S3.E1.m1.7.8.3a" xref="S3.E1.m1.7.8.3.cmml">‚àí</mo><mrow id="S3.E1.m1.7.8.3.2" xref="S3.E1.m1.7.8.3.2.cmml"><munder id="S3.E1.m1.7.8.3.2.1" xref="S3.E1.m1.7.8.3.2.1.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" stretchy="true" id="S3.E1.m1.7.8.3.2.1.2" xref="S3.E1.m1.7.8.3.2.1.2.cmml">‚àë</mo><mrow id="S3.E1.m1.7.8.3.2.1.3" xref="S3.E1.m1.7.8.3.2.1.3.cmml"><mi mathsize="90%" id="S3.E1.m1.7.8.3.2.1.3.2" xref="S3.E1.m1.7.8.3.2.1.3.2.cmml">i</mi><mo mathsize="90%" id="S3.E1.m1.7.8.3.2.1.3.1" xref="S3.E1.m1.7.8.3.2.1.3.1.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.7.8.3.2.1.3.3" xref="S3.E1.m1.7.8.3.2.1.3.3.cmml">‚Ñ¨</mi></mrow></munder><mrow id="S3.E1.m1.7.8.3.2.2" xref="S3.E1.m1.7.8.3.2.2.cmml"><mfrac id="S3.E1.m1.2.2" xref="S3.E1.m1.2.2.cmml"><mn mathsize="90%" id="S3.E1.m1.2.2.4" xref="S3.E1.m1.2.2.4.cmml">1</mn><mrow id="S3.E1.m1.2.2.2.2" xref="S3.E1.m1.2.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.2.2.2.2.2" xref="S3.E1.m1.2.2.2.3.1.cmml">|</mo><mrow id="S3.E1.m1.2.2.2.2.1" xref="S3.E1.m1.2.2.2.2.1.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.2.2.2.2.1.2" xref="S3.E1.m1.2.2.2.2.1.2.cmml">ùí´</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.2.2.2.2.1.1" xref="S3.E1.m1.2.2.2.2.1.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.2.2.2.2.1.3.2" xref="S3.E1.m1.2.2.2.2.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.2.2.2.2.1.3.2.1" xref="S3.E1.m1.2.2.2.2.1.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">i</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.2.2.2.2.1.3.2.2" xref="S3.E1.m1.2.2.2.2.1.cmml">)</mo></mrow></mrow><mo maxsize="90%" minsize="90%" id="S3.E1.m1.2.2.2.2.3" xref="S3.E1.m1.2.2.2.3.1.cmml">|</mo></mrow></mfrac><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.8.3.2.2.1" xref="S3.E1.m1.7.8.3.2.2.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.7.8.3.2.2.2" xref="S3.E1.m1.7.8.3.2.2.2.cmml"><munder id="S3.E1.m1.7.8.3.2.2.2.1" xref="S3.E1.m1.7.8.3.2.2.2.1.cmml"><mo maxsize="90%" minsize="90%" movablelimits="false" stretchy="true" id="S3.E1.m1.7.8.3.2.2.2.1.2" xref="S3.E1.m1.7.8.3.2.2.2.1.2.cmml">‚àë</mo><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mi mathsize="90%" id="S3.E1.m1.3.3.1.3" xref="S3.E1.m1.3.3.1.3.cmml">k</mi><mo mathsize="90%" id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">‚àà</mo><mrow id="S3.E1.m1.3.3.1.4" xref="S3.E1.m1.3.3.1.4.cmml"><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.3.3.1.4.2" xref="S3.E1.m1.3.3.1.4.2.cmml">ùí´</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.3.3.1.4.1" xref="S3.E1.m1.3.3.1.4.1.cmml">‚Äã</mo><mrow id="S3.E1.m1.3.3.1.4.3.2" xref="S3.E1.m1.3.3.1.4.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.4.3.2.1" xref="S3.E1.m1.3.3.1.4.cmml">(</mo><mi mathsize="90%" id="S3.E1.m1.3.3.1.1" xref="S3.E1.m1.3.3.1.1.cmml">i</mi><mo maxsize="90%" minsize="90%" id="S3.E1.m1.3.3.1.4.3.2.2" xref="S3.E1.m1.3.3.1.4.cmml">)</mo></mrow></mrow></mrow></munder><mrow id="S3.E1.m1.7.8.3.2.2.2.2" xref="S3.E1.m1.7.8.3.2.2.2.2.cmml"><mi mathsize="90%" id="S3.E1.m1.7.8.3.2.2.2.2.1" xref="S3.E1.m1.7.8.3.2.2.2.2.1.cmml">log</mi><mo lspace="0.167em" id="S3.E1.m1.7.8.3.2.2.2.2a" xref="S3.E1.m1.7.8.3.2.2.2.2.cmml">‚Å°</mo><mfrac id="S3.E1.m1.7.7" xref="S3.E1.m1.7.7.cmml"><mrow id="S3.E1.m1.5.5.2.2" xref="S3.E1.m1.5.5.2.3.cmml"><mi mathsize="90%" id="S3.E1.m1.4.4.1.1" xref="S3.E1.m1.4.4.1.1.cmml">exp</mi><mo id="S3.E1.m1.5.5.2.2a" xref="S3.E1.m1.5.5.2.3.cmml">‚Å°</mo><mrow id="S3.E1.m1.5.5.2.2.1" xref="S3.E1.m1.5.5.2.3.cmml"><mo id="S3.E1.m1.5.5.2.2.1.2" xref="S3.E1.m1.5.5.2.3.cmml">(</mo><mrow id="S3.E1.m1.5.5.2.2.1.1" xref="S3.E1.m1.5.5.2.2.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.5.5.2.2.1.1.4" xref="S3.E1.m1.5.5.2.2.1.1.4.cmml">œÑ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.1.1.3" xref="S3.E1.m1.5.5.2.2.1.1.3.cmml">‚Äã</mo><msup id="S3.E1.m1.5.5.2.2.1.1.1" xref="S3.E1.m1.5.5.2.2.1.1.1.cmml"><mrow id="S3.E1.m1.5.5.2.2.1.1.1.1.1" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold" maxsize="90%" minsize="90%" id="S3.E1.m1.5.5.2.2.1.1.1.1.1.2" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.5.5.2.2.1.1.1.1.1.1" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.2" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.2.cmml">z</mi><mi mathsize="90%" id="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.3" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.3.cmml">i</mi></msub><mo maxsize="90%" minsize="90%" id="S3.E1.m1.5.5.2.2.1.1.1.1.1.3" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.cmml">)</mo></mrow><mi mathsize="90%" id="S3.E1.m1.5.5.2.2.1.1.1.3" xref="S3.E1.m1.5.5.2.2.1.1.1.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.5.5.2.2.1.1.3a" xref="S3.E1.m1.5.5.2.2.1.1.3.cmml">‚Äã</mo><mrow id="S3.E1.m1.5.5.2.2.1.1.2.1" xref="S3.E1.m1.5.5.2.2.1.1.2.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.E1.m1.5.5.2.2.1.1.2.1.2" xref="S3.E1.m1.5.5.2.2.1.1.2.1.1.cmml">(</mo><msub id="S3.E1.m1.5.5.2.2.1.1.2.1.1" xref="S3.E1.m1.5.5.2.2.1.1.2.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.5.5.2.2.1.1.2.1.1.2" xref="S3.E1.m1.5.5.2.2.1.1.2.1.1.2.cmml">z</mi><mi mathsize="90%" id="S3.E1.m1.5.5.2.2.1.1.2.1.1.3" xref="S3.E1.m1.5.5.2.2.1.1.2.1.1.3.cmml">k</mi></msub><mo maxsize="90%" minsize="90%" id="S3.E1.m1.5.5.2.2.1.1.2.1.3" xref="S3.E1.m1.5.5.2.2.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.5.5.2.2.1.3" xref="S3.E1.m1.5.5.2.3.cmml">)</mo></mrow></mrow><mrow id="S3.E1.m1.7.7.4" xref="S3.E1.m1.7.7.4.cmml"><msub id="S3.E1.m1.7.7.4.3" xref="S3.E1.m1.7.7.4.3.cmml"><mo maxsize="90%" minsize="90%" stretchy="true" id="S3.E1.m1.7.7.4.3.2" xref="S3.E1.m1.7.7.4.3.2.cmml">‚àë</mo><mrow id="S3.E1.m1.7.7.4.3.3" xref="S3.E1.m1.7.7.4.3.3.cmml"><mi mathsize="90%" id="S3.E1.m1.7.7.4.3.3.2" xref="S3.E1.m1.7.7.4.3.3.2.cmml">j</mi><mo mathsize="90%" id="S3.E1.m1.7.7.4.3.3.1" xref="S3.E1.m1.7.7.4.3.3.1.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="S3.E1.m1.7.7.4.3.3.3" xref="S3.E1.m1.7.7.4.3.3.3.cmml">‚Ñ¨</mi></mrow></msub><mrow id="S3.E1.m1.7.7.4.2.1" xref="S3.E1.m1.7.7.4.2.2.cmml"><mi mathsize="90%" id="S3.E1.m1.6.6.3.1" xref="S3.E1.m1.6.6.3.1.cmml">exp</mi><mo id="S3.E1.m1.7.7.4.2.1a" xref="S3.E1.m1.7.7.4.2.2.cmml">‚Å°</mo><mrow id="S3.E1.m1.7.7.4.2.1.1" xref="S3.E1.m1.7.7.4.2.2.cmml"><mo id="S3.E1.m1.7.7.4.2.1.1.2" xref="S3.E1.m1.7.7.4.2.2.cmml">(</mo><mrow id="S3.E1.m1.7.7.4.2.1.1.1" xref="S3.E1.m1.7.7.4.2.1.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.4" xref="S3.E1.m1.7.7.4.2.1.1.1.4.cmml">œÑ</mi><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.4.2.1.1.1.3" xref="S3.E1.m1.7.7.4.2.1.1.1.3.cmml">‚Äã</mo><msup id="S3.E1.m1.7.7.4.2.1.1.1.1" xref="S3.E1.m1.7.7.4.2.1.1.1.1.cmml"><mrow id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold" maxsize="90%" minsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.2" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.2.cmml">z</mi><mi mathsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo maxsize="90%" minsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.3" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi mathsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.1.3" xref="S3.E1.m1.7.7.4.2.1.1.1.1.3.cmml">T</mi></msup><mo lspace="0em" rspace="0em" id="S3.E1.m1.7.7.4.2.1.1.1.3a" xref="S3.E1.m1.7.7.4.2.1.1.1.3.cmml">‚Äã</mo><mrow id="S3.E1.m1.7.7.4.2.1.1.1.2.1" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.cmml"><mo class="ltx_mathvariant_bold" mathvariant="bold" maxsize="90%" minsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.2.1.2" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.cmml">(</mo><msub id="S3.E1.m1.7.7.4.2.1.1.1.2.1.1" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.cmml"><mi mathsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.2" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.2.cmml">z</mi><mi mathsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.3" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.3.cmml">j</mi></msub><mo maxsize="90%" minsize="90%" id="S3.E1.m1.7.7.4.2.1.1.1.2.1.3" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E1.m1.7.7.4.2.1.1.3" xref="S3.E1.m1.7.7.4.2.2.cmml">)</mo></mrow></mrow></mrow></mfrac></mrow></mrow></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.7b"><apply id="S3.E1.m1.7.8.cmml" xref="S3.E1.m1.7.8"><eq id="S3.E1.m1.7.8.1.cmml" xref="S3.E1.m1.7.8.1"></eq><ci id="S3.E1.m1.7.8.2.cmml" xref="S3.E1.m1.7.8.2">‚Ñí</ci><apply id="S3.E1.m1.7.8.3.cmml" xref="S3.E1.m1.7.8.3"><minus id="S3.E1.m1.7.8.3.1.cmml" xref="S3.E1.m1.7.8.3"></minus><apply id="S3.E1.m1.7.8.3.2.cmml" xref="S3.E1.m1.7.8.3.2"><apply id="S3.E1.m1.7.8.3.2.1.cmml" xref="S3.E1.m1.7.8.3.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.8.3.2.1.1.cmml" xref="S3.E1.m1.7.8.3.2.1">subscript</csymbol><sum id="S3.E1.m1.7.8.3.2.1.2.cmml" xref="S3.E1.m1.7.8.3.2.1.2"></sum><apply id="S3.E1.m1.7.8.3.2.1.3.cmml" xref="S3.E1.m1.7.8.3.2.1.3"><in id="S3.E1.m1.7.8.3.2.1.3.1.cmml" xref="S3.E1.m1.7.8.3.2.1.3.1"></in><ci id="S3.E1.m1.7.8.3.2.1.3.2.cmml" xref="S3.E1.m1.7.8.3.2.1.3.2">ùëñ</ci><ci id="S3.E1.m1.7.8.3.2.1.3.3.cmml" xref="S3.E1.m1.7.8.3.2.1.3.3">‚Ñ¨</ci></apply></apply><apply id="S3.E1.m1.7.8.3.2.2.cmml" xref="S3.E1.m1.7.8.3.2.2"><times id="S3.E1.m1.7.8.3.2.2.1.cmml" xref="S3.E1.m1.7.8.3.2.2.1"></times><apply id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2"><divide id="S3.E1.m1.2.2.3.cmml" xref="S3.E1.m1.2.2"></divide><cn type="integer" id="S3.E1.m1.2.2.4.cmml" xref="S3.E1.m1.2.2.4">1</cn><apply id="S3.E1.m1.2.2.2.3.cmml" xref="S3.E1.m1.2.2.2.2"><abs id="S3.E1.m1.2.2.2.3.1.cmml" xref="S3.E1.m1.2.2.2.2.2"></abs><apply id="S3.E1.m1.2.2.2.2.1.cmml" xref="S3.E1.m1.2.2.2.2.1"><times id="S3.E1.m1.2.2.2.2.1.1.cmml" xref="S3.E1.m1.2.2.2.2.1.1"></times><ci id="S3.E1.m1.2.2.2.2.1.2.cmml" xref="S3.E1.m1.2.2.2.2.1.2">ùí´</ci><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ùëñ</ci></apply></apply></apply><apply id="S3.E1.m1.7.8.3.2.2.2.cmml" xref="S3.E1.m1.7.8.3.2.2.2"><apply id="S3.E1.m1.7.8.3.2.2.2.1.cmml" xref="S3.E1.m1.7.8.3.2.2.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.8.3.2.2.2.1.1.cmml" xref="S3.E1.m1.7.8.3.2.2.2.1">subscript</csymbol><sum id="S3.E1.m1.7.8.3.2.2.2.1.2.cmml" xref="S3.E1.m1.7.8.3.2.2.2.1.2"></sum><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><in id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></in><ci id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3">ùëò</ci><apply id="S3.E1.m1.3.3.1.4.cmml" xref="S3.E1.m1.3.3.1.4"><times id="S3.E1.m1.3.3.1.4.1.cmml" xref="S3.E1.m1.3.3.1.4.1"></times><ci id="S3.E1.m1.3.3.1.4.2.cmml" xref="S3.E1.m1.3.3.1.4.2">ùí´</ci><ci id="S3.E1.m1.3.3.1.1.cmml" xref="S3.E1.m1.3.3.1.1">ùëñ</ci></apply></apply></apply><apply id="S3.E1.m1.7.8.3.2.2.2.2.cmml" xref="S3.E1.m1.7.8.3.2.2.2.2"><log id="S3.E1.m1.7.8.3.2.2.2.2.1.cmml" xref="S3.E1.m1.7.8.3.2.2.2.2.1"></log><apply id="S3.E1.m1.7.7.cmml" xref="S3.E1.m1.7.7"><divide id="S3.E1.m1.7.7.5.cmml" xref="S3.E1.m1.7.7"></divide><apply id="S3.E1.m1.5.5.2.3.cmml" xref="S3.E1.m1.5.5.2.2"><exp id="S3.E1.m1.4.4.1.1.cmml" xref="S3.E1.m1.4.4.1.1"></exp><apply id="S3.E1.m1.5.5.2.2.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1"><times id="S3.E1.m1.5.5.2.2.1.1.3.cmml" xref="S3.E1.m1.5.5.2.2.1.1.3"></times><ci id="S3.E1.m1.5.5.2.2.1.1.4.cmml" xref="S3.E1.m1.5.5.2.2.1.1.4">ùúè</ci><apply id="S3.E1.m1.5.5.2.2.1.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.2.2.1.1.1.2.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1">superscript</csymbol><apply id="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.2">ùëß</ci><ci id="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1.1.1.1.3">ùëñ</ci></apply><ci id="S3.E1.m1.5.5.2.2.1.1.1.3.cmml" xref="S3.E1.m1.5.5.2.2.1.1.1.3">ùëá</ci></apply><apply id="S3.E1.m1.5.5.2.2.1.1.2.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.2.2.1.1.2.1.1.1.cmml" xref="S3.E1.m1.5.5.2.2.1.1.2.1">subscript</csymbol><ci id="S3.E1.m1.5.5.2.2.1.1.2.1.1.2.cmml" xref="S3.E1.m1.5.5.2.2.1.1.2.1.1.2">ùëß</ci><ci id="S3.E1.m1.5.5.2.2.1.1.2.1.1.3.cmml" xref="S3.E1.m1.5.5.2.2.1.1.2.1.1.3">ùëò</ci></apply></apply></apply><apply id="S3.E1.m1.7.7.4.cmml" xref="S3.E1.m1.7.7.4"><apply id="S3.E1.m1.7.7.4.3.cmml" xref="S3.E1.m1.7.7.4.3"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.4.3.1.cmml" xref="S3.E1.m1.7.7.4.3">subscript</csymbol><sum id="S3.E1.m1.7.7.4.3.2.cmml" xref="S3.E1.m1.7.7.4.3.2"></sum><apply id="S3.E1.m1.7.7.4.3.3.cmml" xref="S3.E1.m1.7.7.4.3.3"><in id="S3.E1.m1.7.7.4.3.3.1.cmml" xref="S3.E1.m1.7.7.4.3.3.1"></in><ci id="S3.E1.m1.7.7.4.3.3.2.cmml" xref="S3.E1.m1.7.7.4.3.3.2">ùëó</ci><ci id="S3.E1.m1.7.7.4.3.3.3.cmml" xref="S3.E1.m1.7.7.4.3.3.3">‚Ñ¨</ci></apply></apply><apply id="S3.E1.m1.7.7.4.2.2.cmml" xref="S3.E1.m1.7.7.4.2.1"><exp id="S3.E1.m1.6.6.3.1.cmml" xref="S3.E1.m1.6.6.3.1"></exp><apply id="S3.E1.m1.7.7.4.2.1.1.1.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1"><times id="S3.E1.m1.7.7.4.2.1.1.1.3.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.3"></times><ci id="S3.E1.m1.7.7.4.2.1.1.1.4.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.4">ùúè</ci><apply id="S3.E1.m1.7.7.4.2.1.1.1.1.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.4.2.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.2">ùëß</ci><ci id="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.1.1.1.1.3">ùëñ</ci></apply><ci id="S3.E1.m1.7.7.4.2.1.1.1.1.3.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.1.3">ùëá</ci></apply><apply id="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.1.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1">subscript</csymbol><ci id="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.2.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.2">ùëß</ci><ci id="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.3.cmml" xref="S3.E1.m1.7.7.4.2.1.1.1.2.1.1.3">ùëó</ci></apply></apply></apply></apply></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.7c">\mathcal{L}=-\sum_{i\in\mathcal{B}}\frac{1}{|\mathcal{P}(i)|}\sum_{k\in\mathcal{P}(i)}\log\frac{\exp\left(\tau\boldsymbol{(}{z}_{i})^{T}({z}_{k})\right)}{\sum_{j\in\mathcal{B}}\exp\left(\tau\boldsymbol{(}{z}_{i})^{T}\boldsymbol{(}{z}_{j})\right)}</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td rowspan="1" class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div id="S3.p3" class="ltx_para">
<p id="S3.p3.4" class="ltx_p"><span id="S3.p3.4.4" class="ltx_text" style="font-size:90%;">where <math id="S3.p3.1.1.m1.2" class="ltx_Math" alttext="z_{i}=\dfrac{f(x_{i})+g(t_{i})}{2}" display="inline"><semantics id="S3.p3.1.1.m1.2a"><mrow id="S3.p3.1.1.m1.2.3" xref="S3.p3.1.1.m1.2.3.cmml"><msub id="S3.p3.1.1.m1.2.3.2" xref="S3.p3.1.1.m1.2.3.2.cmml"><mi id="S3.p3.1.1.m1.2.3.2.2" xref="S3.p3.1.1.m1.2.3.2.2.cmml">z</mi><mi id="S3.p3.1.1.m1.2.3.2.3" xref="S3.p3.1.1.m1.2.3.2.3.cmml">i</mi></msub><mo id="S3.p3.1.1.m1.2.3.1" xref="S3.p3.1.1.m1.2.3.1.cmml">=</mo><mstyle displaystyle="true" id="S3.p3.1.1.m1.2.2" xref="S3.p3.1.1.m1.2.2.cmml"><mfrac id="S3.p3.1.1.m1.2.2a" xref="S3.p3.1.1.m1.2.2.cmml"><mrow id="S3.p3.1.1.m1.2.2.2" xref="S3.p3.1.1.m1.2.2.2.cmml"><mrow id="S3.p3.1.1.m1.1.1.1.1" xref="S3.p3.1.1.m1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.p3.1.1.m1.1.1.1.1.3" xref="S3.p3.1.1.m1.1.1.1.1.3.cmml">f</mi><mo lspace="0em" rspace="0em" id="S3.p3.1.1.m1.1.1.1.1.2" xref="S3.p3.1.1.m1.1.1.1.1.2.cmml">‚Äã</mo><mrow id="S3.p3.1.1.m1.1.1.1.1.1.1" xref="S3.p3.1.1.m1.1.1.1.1.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.p3.1.1.m1.1.1.1.1.1.1.2" xref="S3.p3.1.1.m1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.p3.1.1.m1.1.1.1.1.1.1.1" xref="S3.p3.1.1.m1.1.1.1.1.1.1.1.cmml"><mi mathsize="90%" id="S3.p3.1.1.m1.1.1.1.1.1.1.1.2" xref="S3.p3.1.1.m1.1.1.1.1.1.1.1.2.cmml">x</mi><mi id="S3.p3.1.1.m1.1.1.1.1.1.1.1.3" xref="S3.p3.1.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo maxsize="90%" minsize="90%" id="S3.p3.1.1.m1.1.1.1.1.1.1.3" xref="S3.p3.1.1.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo mathsize="90%" id="S3.p3.1.1.m1.2.2.2.3" xref="S3.p3.1.1.m1.2.2.2.3.cmml">+</mo><mrow id="S3.p3.1.1.m1.2.2.2.2" xref="S3.p3.1.1.m1.2.2.2.2.cmml"><mi mathsize="90%" id="S3.p3.1.1.m1.2.2.2.2.3" xref="S3.p3.1.1.m1.2.2.2.2.3.cmml">g</mi><mo lspace="0em" rspace="0em" id="S3.p3.1.1.m1.2.2.2.2.2" xref="S3.p3.1.1.m1.2.2.2.2.2.cmml">‚Äã</mo><mrow id="S3.p3.1.1.m1.2.2.2.2.1.1" xref="S3.p3.1.1.m1.2.2.2.2.1.1.1.cmml"><mo maxsize="90%" minsize="90%" id="S3.p3.1.1.m1.2.2.2.2.1.1.2" xref="S3.p3.1.1.m1.2.2.2.2.1.1.1.cmml">(</mo><msub id="S3.p3.1.1.m1.2.2.2.2.1.1.1" xref="S3.p3.1.1.m1.2.2.2.2.1.1.1.cmml"><mi mathsize="90%" id="S3.p3.1.1.m1.2.2.2.2.1.1.1.2" xref="S3.p3.1.1.m1.2.2.2.2.1.1.1.2.cmml">t</mi><mi id="S3.p3.1.1.m1.2.2.2.2.1.1.1.3" xref="S3.p3.1.1.m1.2.2.2.2.1.1.1.3.cmml">i</mi></msub><mo maxsize="90%" minsize="90%" id="S3.p3.1.1.m1.2.2.2.2.1.1.3" xref="S3.p3.1.1.m1.2.2.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mn mathsize="90%" id="S3.p3.1.1.m1.2.2.4" xref="S3.p3.1.1.m1.2.2.4.cmml">2</mn></mfrac></mstyle></mrow><annotation-xml encoding="MathML-Content" id="S3.p3.1.1.m1.2b"><apply id="S3.p3.1.1.m1.2.3.cmml" xref="S3.p3.1.1.m1.2.3"><eq id="S3.p3.1.1.m1.2.3.1.cmml" xref="S3.p3.1.1.m1.2.3.1"></eq><apply id="S3.p3.1.1.m1.2.3.2.cmml" xref="S3.p3.1.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.p3.1.1.m1.2.3.2.1.cmml" xref="S3.p3.1.1.m1.2.3.2">subscript</csymbol><ci id="S3.p3.1.1.m1.2.3.2.2.cmml" xref="S3.p3.1.1.m1.2.3.2.2">ùëß</ci><ci id="S3.p3.1.1.m1.2.3.2.3.cmml" xref="S3.p3.1.1.m1.2.3.2.3">ùëñ</ci></apply><apply id="S3.p3.1.1.m1.2.2.cmml" xref="S3.p3.1.1.m1.2.2"><divide id="S3.p3.1.1.m1.2.2.3.cmml" xref="S3.p3.1.1.m1.2.2"></divide><apply id="S3.p3.1.1.m1.2.2.2.cmml" xref="S3.p3.1.1.m1.2.2.2"><plus id="S3.p3.1.1.m1.2.2.2.3.cmml" xref="S3.p3.1.1.m1.2.2.2.3"></plus><apply id="S3.p3.1.1.m1.1.1.1.1.cmml" xref="S3.p3.1.1.m1.1.1.1.1"><times id="S3.p3.1.1.m1.1.1.1.1.2.cmml" xref="S3.p3.1.1.m1.1.1.1.1.2"></times><ci id="S3.p3.1.1.m1.1.1.1.1.3.cmml" xref="S3.p3.1.1.m1.1.1.1.1.3">ùëì</ci><apply id="S3.p3.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.p3.1.1.m1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.p3.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.p3.1.1.m1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.p3.1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.p3.1.1.m1.1.1.1.1.1.1.1.2">ùë•</ci><ci id="S3.p3.1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.p3.1.1.m1.1.1.1.1.1.1.1.3">ùëñ</ci></apply></apply><apply id="S3.p3.1.1.m1.2.2.2.2.cmml" xref="S3.p3.1.1.m1.2.2.2.2"><times id="S3.p3.1.1.m1.2.2.2.2.2.cmml" xref="S3.p3.1.1.m1.2.2.2.2.2"></times><ci id="S3.p3.1.1.m1.2.2.2.2.3.cmml" xref="S3.p3.1.1.m1.2.2.2.2.3">ùëî</ci><apply id="S3.p3.1.1.m1.2.2.2.2.1.1.1.cmml" xref="S3.p3.1.1.m1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="S3.p3.1.1.m1.2.2.2.2.1.1.1.1.cmml" xref="S3.p3.1.1.m1.2.2.2.2.1.1">subscript</csymbol><ci id="S3.p3.1.1.m1.2.2.2.2.1.1.1.2.cmml" xref="S3.p3.1.1.m1.2.2.2.2.1.1.1.2">ùë°</ci><ci id="S3.p3.1.1.m1.2.2.2.2.1.1.1.3.cmml" xref="S3.p3.1.1.m1.2.2.2.2.1.1.1.3">ùëñ</ci></apply></apply></apply><cn type="integer" id="S3.p3.1.1.m1.2.2.4.cmml" xref="S3.p3.1.1.m1.2.2.4">2</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.1.m1.2c">z_{i}=\dfrac{f(x_{i})+g(t_{i})}{2}</annotation></semantics></math> is the mean of the image and text embeddings for instance <math id="S3.p3.2.2.m2.1" class="ltx_Math" alttext="i" display="inline"><semantics id="S3.p3.2.2.m2.1a"><mi id="S3.p3.2.2.m2.1.1" xref="S3.p3.2.2.m2.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.2.m2.1b"><ci id="S3.p3.2.2.m2.1.1.cmml" xref="S3.p3.2.2.m2.1.1">ùëñ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.2.m2.1c">i</annotation></semantics></math>.
<math id="S3.p3.3.3.m3.1" class="ltx_Math" alttext="\mathcal{B}" display="inline"><semantics id="S3.p3.3.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.p3.3.3.m3.1.1" xref="S3.p3.3.3.m3.1.1.cmml">‚Ñ¨</mi><annotation-xml encoding="MathML-Content" id="S3.p3.3.3.m3.1b"><ci id="S3.p3.3.3.m3.1.1.cmml" xref="S3.p3.3.3.m3.1.1">‚Ñ¨</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.3.m3.1c">\mathcal{B}</annotation></semantics></math> denotes the batch and <math id="S3.p3.4.4.m4.1" class="ltx_Math" alttext="\tau" display="inline"><semantics id="S3.p3.4.4.m4.1a"><mi id="S3.p3.4.4.m4.1.1" xref="S3.p3.4.4.m4.1.1.cmml">œÑ</mi><annotation-xml encoding="MathML-Content" id="S3.p3.4.4.m4.1b"><ci id="S3.p3.4.4.m4.1.1.cmml" xref="S3.p3.4.4.m4.1.1">ùúè</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.4.4.m4.1c">\tau</annotation></semantics></math> is a temperature hyperparameter. This loss incentivizes alignment of image-image, text-text, image-text, and text-image representations across positive instances. It has the flavor of combining contrastive learning on text, contrastive learning on images, and UniCL <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib64" title="" class="ltx_ref">64</a>]</cite>, which has a bi-directional image-text and text-image similarity objective.</span></p>
</div>
<div id="S3.p4" class="ltx_para">
<p id="S3.p4.1" class="ltx_p"><span id="S3.p4.1.1" class="ltx_text" style="font-size:90%;">We begin with pre-trained, aligned image and text embeddings. Japanese language CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib40" title="" class="ltx_ref">40</a>]</cite> was trained on image-caption pairs, using captions machine-translated into Japanese. For the newspapers, we use OpenAI‚Äôs CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib1" title="" class="ltx_ref">1</a>]</cite>.
We continue self-supervised pre-training of the CLIP vision and text encoders on firm image crop-OCR pairs and newspaper image-caption pairs.
<span id="S3.p4.1.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> then uses these encoders to initialize symmetric vision and language bi-encoders trained end-to-end on the pooled representations.
It learns a metric space where the pooled image-text representation for an instance is close to representations in the same class and distant from representations in different classes. Hyperparameters, batching, and hard negative mining are detailed in the supplementary materials. All training was done on an A6000 40 GB GPU.</span></p>
</div>
<div id="S3.p5" class="ltx_para">
<p id="S3.p5.1" class="ltx_p"><span id="S3.p5.1.1" class="ltx_text" style="font-size:90%;">At inference time, instances can be linked by retrieving their nearest neighbor from an offline exemplar embedding index or by clustering their representations. The former is used for the Japanese record linkage and the latter - with highly scalable single linkage clustering - is used for detecting noisy duplicates in the newspaper clippings dataset. Facebook Artificial Intelligence Similarly Search (FAISS) <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib26" title="" class="ltx_ref">26</a>]</cite>, with <span id="S3.p5.1.1.1" class="ltx_text ltx_font_typewriter">IndexFlatIP</span>, is used to calculate pairwise exact distances between the embeddings.<span id="footnote1" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note"><span id="footnote1.1.1.1" class="ltx_text" style="font-size:111%;">1</span></span><span id="footnote1.5" class="ltx_text" style="font-size:111%;">Because FAISS range search is not GPU-supported, we implement </span><math id="footnote1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="footnote1.m1.1b"><mi mathsize="111%" id="footnote1.m1.1.1" xref="footnote1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="footnote1.m1.1c"><ci id="footnote1.m1.1.1.cmml" xref="footnote1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m1.1d">k</annotation></semantics></math><span id="footnote1.6" class="ltx_text" style="font-size:111%;"> nearest neighbor search, conservatively setting </span><math id="footnote1.m2.1" class="ltx_Math" alttext="k" display="inline"><semantics id="footnote1.m2.1b"><mi mathsize="111%" id="footnote1.m2.1.1" xref="footnote1.m2.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="footnote1.m2.1c"><ci id="footnote1.m2.1.1.cmml" xref="footnote1.m2.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="footnote1.m2.1d">k</annotation></semantics></math><span id="footnote1.7" class="ltx_text" style="font-size:111%;"> to 900.</span></span></span></span></span></p>
</div>
<div id="S3.p6" class="ltx_para">
<p id="S3.p6.1" class="ltx_p"><span id="S3.p6.1.1" class="ltx_text" style="font-size:90%;">Blocking - which incorporates type information from structured databases into record linkage - is important in many applications and has been the subject of extensive research (see <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib55" title="" class="ltx_ref">55</a>, <a href="#bib.bib45" title="" class="ltx_ref">45</a>]</cite> for reviews). While not applicable to this study‚Äôs applications, blocking is natural to incorporate into a contrastive setup, through using a type-specific loss function <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib36" title="" class="ltx_ref">36</a>]</cite>. This will allow for matches to be made even when there is some noise in the type, a common scenario. For example, there might be noise in a firm‚Äôs measured establishment date, used to link across censuses.</span></p>
</div>
<div id="S3.p7" class="ltx_para">
<p id="S3.p7.1" class="ltx_p"><span id="S3.p7.1.1" class="ltx_text" style="font-size:90%;">To examine how <span id="S3.p7.1.1.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> compares to an analogous framework trained using a state-of-the-art unimocal encoder, we train a symmetric DINO vision transformer <cite class="ltx_cite ltx_citemacro_cite">[<a href="#bib.bib12" title="" class="ltx_ref">12</a>]</cite> using the Japanese firm linked image crop data.
Details on self-supervised pre-training and hyperparameters for the self-supervised and supervised training are detailed in the supplementary materials.</span></p>
</div>
</section>
<section id="S4" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Record Linkage</h2>

<section id="S4.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data</h3>

<div id="S4.SS1.p1" class="ltx_para">
<p id="S4.SS1.p1.1" class="ltx_p"><span id="S4.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">This study‚Äôs first application is constructing historical Japanese supply chains.
This entails matching suppliers and customers recorded in firm level records collected in 1956 for over 7,000 large Japanese firms </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib25" title="" class="ltx_ref">25</a><span id="S4.SS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.4" class="ltx_text" style="font-size:90%;"> to a firm level directory that provides additional rich information about nearly 70,000 firms </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib56" title="" class="ltx_ref">56</a><span id="S4.SS1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.7" class="ltx_text" style="font-size:90%;">. The former are written horizontally and the latter vertically, making a potentially challenging case for visual matching. Firm name crops were localized using a Mask R-CNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S4.SS1.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.10" class="ltx_text" style="font-size:90%;"> model custom-trained with Layout Parser </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p1.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S4.SS1.p1.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p1.1.13" class="ltx_text" style="font-size:90%;">.
To create labeled data for training and evaluation, the customers and suppliers of randomly selected firms were hand merged with the firm index. Two highly skilled annotators completed this task and resolved all discrepancies. Many firms appear as customers and suppliers of multiple randomly selected firms, and the data were de-duplicated such that each customer or supplier appears only once, in order to avoid test set leakage. Sometimes a single firm is linked to multiple entries in the firm directory, as firms can appear there more than once if they were registered in multiple prefectures.</span></p>
</div>
<div id="S4.SS1.p2" class="ltx_para">
<p id="S4.SS1.p2.1" class="ltx_p"><span id="S4.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">We used a 60-20-20 split to divide classes into a training set (772 examples), validation set (257 examples), and test set (257 examples).
The test data links the customer-supplier list to all matching firms in the index (with multiple matches occurring when a firm is registered in multiple prefectures), whereas this costly labeling was not needed for the training data, where each firm has a single match. In the main text, we report results using a dataset that drops customers and suppliers like ‚Äúthe government‚Äù that do not appear in the firm index. In the supplementary materials, we report analyses including these instances, with similar patterns emerging. We also trained on 19,793 synthetically generated Japanese names, with an 80-20 train-val split. Each are rendered using different fonts and OCRed with two different OCR engines, that make different errors </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a>, <a href="#bib.bib16" title="" class="ltx_ref">16</a><span id="S4.SS1.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p2.1.4" class="ltx_text" style="font-size:90%;">. Each epoch involved sampling 3 ‚Äùviews‚Äù of each image crop-ocr pair.</span></p>
</div>
<div id="S4.SS1.p3" class="ltx_para">
<p id="S4.SS1.p3.1" class="ltx_p"><span id="S4.SS1.p3.1.1" class="ltx_text" style="font-size:90%;">Additionally, we conducted self-supervised language-image pre-training of the Japanese CLIP encoders, using 111,750 firm image crops and their corresponding OCR, as well as the same 19,793 synthetically generated names and their OCR, with an 80-20 test-val split.</span></p>
</div>
<figure id="S4.F3" class="ltx_figure"><img src="/html/2304.03464/assets/figures/matching_errors.png" id="S4.F3.g1" class="ltx_graphics ltx_centering ltx_img_square" width="598" height="549" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Errors: This figure shows errors in record linkage made by different models.</figcaption>
</figure>
<div id="S4.SS1.p4" class="ltx_para">
<p id="S4.SS1.p4.1" class="ltx_p"><span id="S4.SS1.p4.1.1" class="ltx_text" style="font-size:90%;">As record linkage accuracy with string matching is likely to relate to the quality of the OCR, we apply string matching comparisons using two different OCRs of the Japanese firm names: one created by using Google Cloud Vision off-the-shelf, as off-the-shelf OCR usage is the overwhelming norm (Google Cloud Vision does not support fine-tuning) and is typically noisier, as well as a custom-trained OCR that achieves a character error rate of 0.006 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS1.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib11" title="" class="ltx_ref">11</a><span id="S4.SS1.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS1.p4.1.4" class="ltx_text" style="font-size:90%;">, a near best case scenario.</span></p>
</div>
</section>
<section id="S4.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>

<figure id="S4.T1" class="ltx_table">
<div id="S4.T1.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:701.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(122.3pt,-197.9pt) scale(2.29334696862511,2.29334696862511) ;">
<table id="S4.T1.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="S4.T1.1.1.2.1" class="ltx_tr">
<th id="S4.T1.1.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="S4.T1.1.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.2.1.2.1" class="ltx_text" style="font-size:90%;">Noisy</span></td>
<td id="S4.T1.1.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="S4.T1.1.1.2.1.3.1" class="ltx_text" style="font-size:90%;">Clean</span></td>
</tr>
<tr id="S4.T1.1.1.3.2" class="ltx_tr">
<th id="S4.T1.1.1.3.2.1" class="ltx_td ltx_th ltx_th_row"></th>
<td id="S4.T1.1.1.3.2.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.3.2.2.1" class="ltx_text" style="font-size:90%;">OCR</span></td>
<td id="S4.T1.1.1.3.2.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.3.2.3.1" class="ltx_text" style="font-size:90%;">OCR</span></td>
</tr>
<tr id="S4.T1.1.1.4.3" class="ltx_tr">
<th id="S4.T1.1.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="3"><span id="S4.T1.1.1.4.3.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel A: String-Matching</span></th>
</tr>
<tr id="S4.T1.1.1.5.4" class="ltx_tr">
<th id="S4.T1.1.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.5.4.1.1" class="ltx_text" style="font-size:90%;">Levenshtein distance</span></th>
<td id="S4.T1.1.1.5.4.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.5.4.2.1" class="ltx_text" style="font-size:90%;">0.630</span></td>
<td id="S4.T1.1.1.5.4.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.5.4.3.1" class="ltx_text" style="font-size:90%;">0.731</span></td>
</tr>
<tr id="S4.T1.1.1.1" class="ltx_tr">
<th id="S4.T1.1.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="S4.T1.1.1.1.1.1" class="ltx_text" style="font-size:90%;">Stroke </span><math id="S4.T1.1.1.1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.T1.1.1.1.1.m1.1a"><mi mathsize="90%" id="S4.T1.1.1.1.1.m1.1.1" xref="S4.T1.1.1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.T1.1.1.1.1.m1.1b"><ci id="S4.T1.1.1.1.1.m1.1.1.cmml" xref="S4.T1.1.1.1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.T1.1.1.1.1.m1.1c">n</annotation></semantics></math><span id="S4.T1.1.1.1.1.2" class="ltx_text" style="font-size:90%;">-gram similarity</span>
</th>
<td id="S4.T1.1.1.1.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">0.689</span></td>
<td id="S4.T1.1.1.1.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.1.3.1" class="ltx_text" style="font-size:90%;">0.731</span></td>
</tr>
<tr id="S4.T1.1.1.6.5" class="ltx_tr">
<th id="S4.T1.1.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="S4.T1.1.1.6.5.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel B: Language-Image Self-Supervised Training</span></th>
</tr>
<tr id="S4.T1.1.1.7.6" class="ltx_tr">
<th id="S4.T1.1.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.7.6.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="S4.T1.1.1.7.6.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.7.6.2.1" class="ltx_text" style="font-size:90%;">0.769</span></td>
<td id="S4.T1.1.1.7.6.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.7.6.3.1" class="ltx_text" style="font-size:90%;">0.769</span></td>
</tr>
<tr id="S4.T1.1.1.8.7" class="ltx_tr">
<th id="S4.T1.1.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.8.7.1.1" class="ltx_text" style="font-size:90%;">Language Linking</span></th>
<td id="S4.T1.1.1.8.7.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.8.7.2.1" class="ltx_text" style="font-size:90%;">0.740</span></td>
<td id="S4.T1.1.1.8.7.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.8.7.3.1" class="ltx_text" style="font-size:90%;">0.790</span></td>
</tr>
<tr id="S4.T1.1.1.9.8" class="ltx_tr">
<th id="S4.T1.1.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.9.8.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="S4.T1.1.1.9.8.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.9.8.2.1" class="ltx_text" style="font-size:90%;">0.845</span></td>
<td id="S4.T1.1.1.9.8.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.9.8.3.1" class="ltx_text" style="font-size:90%;">0.849</span></td>
</tr>
<tr id="S4.T1.1.1.10.9" class="ltx_tr">
<th id="S4.T1.1.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="S4.T1.1.1.10.9.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel C: Supervised Training on Linked Data</span></th>
</tr>
<tr id="S4.T1.1.1.11.10" class="ltx_tr">
<th id="S4.T1.1.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="S4.T1.1.1.11.10.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">with Vision Pre-training</span></th>
</tr>
<tr id="S4.T1.1.1.12.11" class="ltx_tr">
<th id="S4.T1.1.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.12.11.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="S4.T1.1.1.12.11.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.12.11.2.1" class="ltx_text" style="font-size:90%;">0.878</span></td>
<td id="S4.T1.1.1.12.11.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.12.11.3.1" class="ltx_text" style="font-size:90%;">0.878</span></td>
</tr>
<tr id="S4.T1.1.1.13.12" class="ltx_tr">
<th id="S4.T1.1.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="S4.T1.1.1.13.12.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel D: Supervised Training on Linked Data</span></th>
</tr>
<tr id="S4.T1.1.1.14.13" class="ltx_tr">
<th id="S4.T1.1.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="S4.T1.1.1.14.13.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">with Language-Image Pre-training</span></th>
</tr>
<tr id="S4.T1.1.1.15.14" class="ltx_tr">
<th id="S4.T1.1.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.15.14.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="S4.T1.1.1.15.14.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.15.14.2.1" class="ltx_text" style="font-size:90%;">0.924</span></td>
<td id="S4.T1.1.1.15.14.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.15.14.3.1" class="ltx_text" style="font-size:90%;">0.924</span></td>
</tr>
<tr id="S4.T1.1.1.16.15" class="ltx_tr">
<th id="S4.T1.1.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S4.T1.1.1.16.15.1.1" class="ltx_text" style="font-size:90%;">Language Linking</span></th>
<td id="S4.T1.1.1.16.15.2" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.16.15.2.1" class="ltx_text" style="font-size:90%;">0.790</span></td>
<td id="S4.T1.1.1.16.15.3" class="ltx_td ltx_align_center"><span id="S4.T1.1.1.16.15.3.1" class="ltx_text" style="font-size:90%;">0.882</span></td>
</tr>
<tr id="S4.T1.1.1.17.16" class="ltx_tr">
<th id="S4.T1.1.1.17.16.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S4.T1.1.1.17.16.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="S4.T1.1.1.17.16.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.17.16.2.1" class="ltx_text" style="font-size:90%;">0.937</span></td>
<td id="S4.T1.1.1.17.16.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="S4.T1.1.1.17.16.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.945</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 1: </span>Baseline Matching Results: This table reports accuracy on the test set using a variety of different methods for linking Japanese firms from supply chain records to a large firm directory. <span id="S4.T1.8.1" class="ltx_text ltx_font_italic">Noisy OCR</span> uses off-the-shelf Google Cloud Vision for OCR, and <span id="S4.T1.9.2" class="ltx_text ltx_font_italic">Clean OCR</span> uses an accurate custom-trained OCR.</figcaption>
</figure>
<div id="S4.SS2.p1" class="ltx_para">
<p id="S4.SS2.p1.1" class="ltx_p"><span id="S4.SS2.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="S4.SS2.p1.1.2" class="ltx_text" style="font-size:90%;"> achieves a record linkage accuracy of 94.5%, significantly outperforming string matching metrics on both noisy and clean OCR. Using the pooled representations also edges out tuning only the pre-trained image or pre-trained language encoders, which achieve an accuracy of 92.4% and 88.2%, respectively. When using multimodal representations, the accuracy of the OCR has only a very modest impact: 93.7% with noisy OCR versus 94.5% with clean OCR for the supervised multimodal </span><span id="S4.SS2.p1.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="S4.SS2.p1.1.4" class="ltx_text" style="font-size:90%;">. When supervised tuning is performed using only the linked OCR‚Äôed texts, the performance deterioration from using a noisier OCR is larger (79.0% versus 88.2%), illustrating how working directly with images relieves the OCR information bottleneck.</span></p>
</div>
<figure id="S4.F4" class="ltx_figure"><img src="/html/2304.03464/assets/figures/IOnet.png" id="S4.F4.g1" class="ltx_graphics ltx_centering ltx_img_landscape" width="598" height="324" alt="Refer to caption">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Input-Output Networks: This figure plots the average supply chain distance of Japanese firms to Mitsui, Mitsubishi, and Sumitomo, the three most prominent Japanese firms.</figcaption>
</figure>
<div id="S4.SS2.p2" class="ltx_para">
<p id="S4.SS2.p2.1" class="ltx_p"><span id="S4.SS2.p2.1.1" class="ltx_text" style="font-size:90%;">Figure </span><a href="#S4.F3" title="Figure 3 ‚Ä£ 4.1 Data ‚Ä£ 4 Record Linkage ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS2.p2.1.2" class="ltx_text" style="font-size:90%;">, Panel A shows representative errors made by the supervised vision only, language only, and multimodal encoders, that use language-image pre-training.
In the vision-encoder error, the ground truth is Meiji Seika, a firm producing snacks and chocolates. Its prediction is Meiji pharmaceutical company, as Ëèì and Ëñ¨ look similar.</span></p>
</div>
<div id="S4.SS2.p3" class="ltx_para">
<p id="S4.SS2.p3.1" class="ltx_p"><span id="S4.SS2.p3.1.1" class="ltx_text" style="font-size:90%;">In the language-only encoder error, the ground truth is ‚ÄúJapanese Processing and Manufacturing of Paper‚ÄùÔºàÊó•Êú¨Âä†Â∑•Ë£ΩÁ¥ô), and the prediction is ‚ÄúJapanese Paper Processing‚ÄùÔºàÊó•Êú¨Á¥ôÂä†Â∑•), very similar in meaning. The multimodal encoder predicts both cases correctly.
Finally, the multimodal error is a company whose name is only two characters. The ground truth is ‰∏∏Ê∞∏, and the prediction is ‰∏∏Ê∞¥, with Ê∞∏ and Ê∞¥ very visually similar. These are names without language context, so the language encoder cannot distinguish them either.</span></p>
</div>
<div id="S4.SS2.p4" class="ltx_para">
<p id="S4.SS2.p4.1" class="ltx_p"><span id="S4.SS2.p4.1.1" class="ltx_text" style="font-size:90%;">Fascinatingly, the tuned vision only encoder with language-image pre-training gets some matches that require language understanding correct (accuracy 92.4%), that the tuned ViT model with vision-only pre-training gets wrong (accuracy 87.8%).</span><span id="footnote2" class="ltx_note ltx_role_footnote"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Tuning a Japanese S-BERT language bi-encoder does even worse than the vision only encoder, but is less of an apples-to-apples comparisons to the <span id="footnote2.1" class="ltx_text ltx_font_typewriter">CLIPPINGS</span> text-only model due to the challenges of devising self-supervised language-only pre-training recipes in this context.</span></span></span><span id="S4.SS2.p4.1.2" class="ltx_text" style="font-size:90%;">
Figure </span><a href="#S4.F3" title="Figure 3 ‚Ä£ 4.1 Data ‚Ä£ 4 Record Linkage ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">3</span></a><span id="S4.SS2.p4.1.3" class="ltx_text" style="font-size:90%;">, Panel B provides several examples.
In the first example, „É¨„Éä„Ç¶„É≥Èù¥‰∏ãÔºàRenown Sock) is matched by the ViT model to ‚Äú„É¨„Éä„Ç¶„É≥ÂïÜ‰∫ã(Renown Business), whereas the multimodally pre-trained encoder matches it correctly to „É¨„Éä„Ç¶„É≥Èù¥‰∏ãÂ∑•Ê•≠ÔºàRenown Sock Industry), despite the extra two characters.
In the second example, the ground truth is Â§™ÈôΩÈëõÂ∑• (Sun Mineral Manufacturing), whereas the ViT prediction is Â§™ÈôΩÁ¥ôÂ∑•ÔºàSun Paper Manufacturing).
The third example results from an error in our custom trained layout detection model, that concatenates two customer-suppliers. The detected firm is ‚Äú„Ç®„Éå„ÉÜ„Ç£„Ç®„ÉåË≤©Ë≥£ÊúÉÁ§æ[Ë≤©Ë≥£]ÂõΩÈâÑ,‚Äù which translates as ‚ÄúNTN Sales Company[Sales]The Government-owned Railway‚Äù. The ViT simply predicts a company of similar length, whereas the encoder with multimodal pre-training matches it to NTN Sales Company.
These examples show that the vision encoder gains language understanding through the contrastive language-image self-supervised pre-training.</span></p>
</div>
<div id="S4.SS2.p5" class="ltx_para">
<p id="S4.SS2.p5.1" class="ltx_p"><span id="S4.SS2.p5.1.1" class="ltx_text" style="font-size:90%;">The purely self-supervised multimodal model also outperforms the string matching methods, with an accuracy of 84.9%. </span><math id="S4.SS2.p5.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S4.SS2.p5.1.m1.1a"><mi mathsize="90%" id="S4.SS2.p5.1.m1.1.1" xref="S4.SS2.p5.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.p5.1.m1.1b"><ci id="S4.SS2.p5.1.m1.1.1.cmml" xref="S4.SS2.p5.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.p5.1.m1.1c">n</annotation></semantics></math><span id="S4.SS2.p5.1.2" class="ltx_text" style="font-size:90%;">-gram similarity at the stroke level with fuzzychinese </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p5.1.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib65" title="" class="ltx_ref">65</a><span id="S4.SS2.p5.1.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p5.1.5" class="ltx_text" style="font-size:90%;"> achieves an accuracy of 73.1% on clean OCR, as coincidentally does Levenshtein distance. When only vision (76.9% accuracy) or language (79.0% accuracy) self-supervised embeddings are used at test time, the performance also exceeds that of standard string matching techniques. This shows that without any linked training data at all, neural methods can still outperform widely used rule-based methods.</span></p>
</div>
<div id="S4.SS2.p6" class="ltx_para">
<p id="S4.SS2.p6.1" class="ltx_p"><span id="S4.SS2.p6.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="S4.SS2.p6.1.2" class="ltx_text" style="font-size:90%;"> plausibly outperforms string matching for several reasons. Since it leverages transfer learning from language-image pre-training, it can be precisely tailored to different settings with small, cheap-to-create training sets, or used as a purely self-supervised solution.
Moreover, hard negatives in contrastive training encourage separation between different instance representations even if the instances are very similar.
Our application does not incorporate blocking, as it is not necessary in this setting, but in the many settings where it is we conjecture that the expressiveness of a type-specific contrastive loss could be another benefit.</span></p>
</div>
<div id="S4.SS2.p7" class="ltx_para">
<p id="S4.SS2.p7.1" class="ltx_p"><span id="S4.SS2.p7.1.1" class="ltx_text" style="font-size:90%;">Deployment costs are important for extensibility, as record linkage problems often entail linking millions of entities on a limited compute budget. Experiments on the full dataset of 36,673 customer-suppliers, using an 18 core Intel(R) i9-9980XE CPU @ 3.00GHz and a single NVIDIA A6000 GPU, show that </span><span id="S4.SS2.p7.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPING</span><span id="S4.SS2.p7.1.3" class="ltx_text" style="font-size:90%;">‚Äôs deployment costs are modest, making it a highly scalable solution.
Mutlimodal </span><span id="S4.SS2.p7.1.4" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="S4.SS2.p7.1.5" class="ltx_text" style="font-size:90%;"> takes 6 minutes, 21 seconds to run on the full data with a single GPU, all but one second of which are embedding the crops and OCR. This compares to
54 minutes to implement our optimized CPU Levenshtein distance calculations (which is an order of magnitude faster than R string matching, </span><span id="S4.SS2.p7.1.6" class="ltx_text ltx_font_italic" style="font-size:90%;">e.g.</span><span id="S4.SS2.p7.1.7" class="ltx_text" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p7.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib57" title="" class="ltx_ref">57</a><span id="S4.SS2.p7.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p7.1.10" class="ltx_text" style="font-size:90%;">) and 3 minutes and 7 seconds for stroke matching with fuzzychinese </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p7.1.11.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib65" title="" class="ltx_ref">65</a><span id="S4.SS2.p7.1.12.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p7.1.13" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="S4.SS2.p8" class="ltx_para">
<p id="S4.SS2.p8.1" class="ltx_p"><span id="S4.SS2.p8.1.1" class="ltx_text" style="font-size:90%;">Figure </span><a href="#S4.F4" title="Figure 4 ‚Ä£ 4.2 Results ‚Ä£ 4 Record Linkage ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">4</span></a><span id="S4.SS2.p8.1.2" class="ltx_text" style="font-size:90%;"> illustrates the supply chain networks created by applying </span><span id="S4.SS2.p8.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="S4.SS2.p8.1.4" class="ltx_text" style="font-size:90%;"> to the full dataset, with the shading showing the average distance in the supply chain networks to the three largest Japanese conglomerates: Mitsui, Mitsubishi, and Sumitomo. Node position and scaling are fixed, with scaling showing average degree centrality. Using off-the-shelf OCR and Levenshtein distance - a prevalent approach in the literature - creates a visibly different network (left) than the multimodal method (right), which is much closer to the ground truth. 7,352 firm nodes (56%) are in the supply chains of these big-3 firms. A study of the Japanese economy based on the noisier network is likely to produce biased results </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S4.SS2.p8.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib13" title="" class="ltx_ref">13</a><span id="S4.SS2.p8.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S4.SS2.p8.1.7" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
</section>
</section>
<section id="S5" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Noisy Image-Caption Duplicates</h2>

<section id="S5.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Data</h3>

<div id="S5.SS1.p1" class="ltx_para">
<p id="S5.SS1.p1.1" class="ltx_p"><span id="S5.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">To examine whether the </span><span id="S5.SS1.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="S5.SS1.p1.1.3" class="ltx_text" style="font-size:90%;"> framework can be applied more broadly to cross-document vision-language linking, we examine the detection of noisy duplicate image-caption pairs in a massive corpora of off-copyright historical U.S. newspapers that we have digitized using a Mask R-CNN </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS1.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib19" title="" class="ltx_ref">19</a><span id="S5.SS1.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS1.p1.1.6" class="ltx_text" style="font-size:90%;"> model custom-trained with Layout Parser </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS1.p1.1.7.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib50" title="" class="ltx_ref">50</a><span id="S5.SS1.p1.1.8.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS1.p1.1.9" class="ltx_text" style="font-size:90%;"> and Tesseract OCR. Images and captions are associated with their coordinates using a rule-based method, given that captions almost always appear directly below their corresponding images. The classes are unknown ex ante, and so we use single linkage clustering to detect groups of near-duplicate image-caption pairs that came from the same underlying photo news wire source.</span></p>
</div>
<div id="S5.SS1.p2" class="ltx_para">
<p id="S5.SS1.p2.1" class="ltx_p"><span id="S5.SS1.p2.1.1" class="ltx_text" style="font-size:90%;">Images can be significantly cropped (Figure </span><a href="#S1.F1" title="Figure 1 ‚Ä£ 1 Introduction ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">1</span></a><span id="S5.SS1.p2.1.2" class="ltx_text" style="font-size:90%;">), flipped, or nearly illegible due to the low resolution and noise from scanning. (39 images that were entirely black due to poor scanner settings were removed from the evaluation data.)
While the news wire included captions, papers frequently abridged or otherwise modified them to fit their space and appeal to their local audience. Frequently, different images also have the same generic caption (</span><span id="S5.SS1.p2.1.3" class="ltx_text ltx_font_italic" style="font-size:90%;">e.g.</span><span id="S5.SS1.p2.1.4" class="ltx_text" style="font-size:90%;">, ‚ÄúMain Street‚Äù or ‚ÄúJohn Smith‚Äù).</span></p>
</div>
<div id="S5.SS1.p3" class="ltx_para">
<p id="S5.SS1.p3.1" class="ltx_p"><span id="S5.SS1.p3.1.1" class="ltx_text" style="font-size:90%;">The training dataset is drawn from 5 random days per odd year, between 1941 and 1989.
The training set contains 910 classes, with all classes having at least two instances, and the validation set contains 255 classes.
To create a test sample, as well as a validation sample for choosing the optimal threshold for single linkage clustering, we labeled a full day sample of every image-caption pair appearing on a randomly selected day, October 3rd, 1968. The test set has 606 classes (60% split), including singletons, and the validation set for choosing the single linkage clustering threshold has 405 classes (40% split), including singletons.
Self-supervised pre-training is performed on 85,000 image-caption pairs, drawing from 1941 to 1989. 1968 is excluded to avoid leakage. Hyperparameters are provided in the supplementary materials.</span></p>
</div>
</section>
<section id="S5.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Results</h3>

<div id="S5.SS2.p1" class="ltx_para">
<p id="S5.SS2.p1.1" class="ltx_p"><span id="S5.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">Multimodal </span><span id="S5.SS2.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="S5.SS2.p1.1.3" class="ltx_text" style="font-size:90%;"> achieves an adjusted rand index (ARI) - a widely used measure of clustering performance - of 61.5.
This outperforms contrastively tuning just the multimodally pre-trained vision encoder (ARI of 59.7), whereas using just the language encoder performs substantially worse (ARI of 38.9).</span></p>
</div>
<figure id="S5.T2" class="ltx_table">
<div id="S5.T2.1" class="ltx_inline-block ltx_align_center ltx_transformed_outer" style="width:433.6pt;height:712.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(151.1pt,-248.1pt) scale(3.29748548299564,3.29748548299564) ;">
<table id="S5.T2.1.1" class="ltx_tabular ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="S5.T2.1.1.1.1" class="ltx_tr">
<th id="S5.T2.1.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="S5.T2.1.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="S5.T2.1.1.1.1.2.1" class="ltx_text" style="font-size:90%;">ARI</span></th>
</tr>
<tr id="S5.T2.1.1.2.2" class="ltx_tr">
<th id="S5.T2.1.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><span id="S5.T2.1.1.2.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel A: String-Matching</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="S5.T2.1.1.3.1" class="ltx_tr">
<th id="S5.T2.1.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.1.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Jaccard similarity</span></th>
<td id="S5.T2.1.1.3.1.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.3.1.2.1" class="ltx_text" style="font-size:90%;">40.3</span></td>
</tr>
<tr id="S5.T2.1.1.4.2" class="ltx_tr">
<th id="S5.T2.1.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="2"><span id="S5.T2.1.1.4.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel B: Language-Image Self-Supervised Training</span></th>
</tr>
<tr id="S5.T2.1.1.5.3" class="ltx_tr">
<th id="S5.T2.1.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.1.1.5.3.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="S5.T2.1.1.5.3.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.5.3.2.1" class="ltx_text" style="font-size:90%;">40.7</span></td>
</tr>
<tr id="S5.T2.1.1.6.4" class="ltx_tr">
<th id="S5.T2.1.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.1.1.6.4.1.1" class="ltx_text" style="font-size:90%;">Language Linking</span></th>
<td id="S5.T2.1.1.6.4.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.6.4.2.1" class="ltx_text" style="font-size:90%;">31.0</span></td>
</tr>
<tr id="S5.T2.1.1.7.5" class="ltx_tr">
<th id="S5.T2.1.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.1.1.7.5.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="S5.T2.1.1.7.5.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.7.5.2.1" class="ltx_text" style="font-size:90%;">43.0</span></td>
</tr>
<tr id="S5.T2.1.1.8.6" class="ltx_tr">
<th id="S5.T2.1.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="2"><span id="S5.T2.1.1.8.6.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel C: Supervised Training on Linked Data</span></th>
</tr>
<tr id="S5.T2.1.1.9.7" class="ltx_tr">
<th id="S5.T2.1.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="2"><span id="S5.T2.1.1.9.7.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">with Language-Image Pre-training</span></th>
</tr>
<tr id="S5.T2.1.1.10.8" class="ltx_tr">
<th id="S5.T2.1.1.10.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.1.1.10.8.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="S5.T2.1.1.10.8.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.10.8.2.1" class="ltx_text" style="font-size:90%;">59.7</span></td>
</tr>
<tr id="S5.T2.1.1.11.9" class="ltx_tr">
<th id="S5.T2.1.1.11.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="S5.T2.1.1.11.9.1.1" class="ltx_text" style="font-size:90%;">Language Linking</span></th>
<td id="S5.T2.1.1.11.9.2" class="ltx_td ltx_align_center"><span id="S5.T2.1.1.11.9.2.1" class="ltx_text" style="font-size:90%;">38.9</span></td>
</tr>
<tr id="S5.T2.1.1.12.10" class="ltx_tr">
<th id="S5.T2.1.1.12.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="S5.T2.1.1.12.10.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="S5.T2.1.1.12.10.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="S5.T2.1.1.12.10.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">61.5</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 2: </span>Detecting Noisy Image-Caption Duplicates: This table reports adjusted rand index on the test set using a variety of different methods for detecting noisy duplicated image-caption pairs in a historical newspaper corpus.</figcaption>
</figure>
<div id="S5.SS2.p2" class="ltx_para">
<p id="S5.SS2.p2.2" class="ltx_p"><span id="S5.SS2.p2.2.1" class="ltx_text" style="font-size:90%;">For comparison, we also examine </span><math id="S5.SS2.p2.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS2.p2.1.m1.1a"><mi mathsize="90%" id="S5.SS2.p2.1.m1.1.1" xref="S5.SS2.p2.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.1.m1.1b"><ci id="S5.SS2.p2.1.m1.1.1.cmml" xref="S5.SS2.p2.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.1.m1.1c">n</annotation></semantics></math><span id="S5.SS2.p2.2.2" class="ltx_text" style="font-size:90%;">-gram based methods, predominant in the literature measuring the spread of content in historical newspapers </span><cite class="ltx_cite ltx_citemacro_cite"><span id="S5.SS2.p2.2.3.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib59" title="" class="ltx_ref">59</a>, <a href="#bib.bib14" title="" class="ltx_ref">14</a>, <a href="#bib.bib53" title="" class="ltx_ref">53</a><span id="S5.SS2.p2.2.4.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="S5.SS2.p2.2.5" class="ltx_text" style="font-size:90%;">.
Texts are compared using Jaccard similarity , given by </span><math id="S5.SS2.p2.2.m2.2" class="ltx_Math" alttext="\frac{|A\cap B|}{|A\cup B|}" display="inline"><semantics id="S5.SS2.p2.2.m2.2a"><mfrac id="S5.SS2.p2.2.m2.2.2" xref="S5.SS2.p2.2.m2.2.2.cmml"><mrow id="S5.SS2.p2.2.m2.1.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.2.cmml"><mo maxsize="90%" minsize="90%" id="S5.SS2.p2.2.m2.1.1.1.1.2" xref="S5.SS2.p2.2.m2.1.1.1.2.1.cmml">|</mo><mrow id="S5.SS2.p2.2.m2.1.1.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.2.m2.1.1.1.1.1.2" xref="S5.SS2.p2.2.m2.1.1.1.1.1.2.cmml">A</mi><mo mathsize="90%" id="S5.SS2.p2.2.m2.1.1.1.1.1.1" xref="S5.SS2.p2.2.m2.1.1.1.1.1.1.cmml">‚à©</mo><mi mathsize="90%" id="S5.SS2.p2.2.m2.1.1.1.1.1.3" xref="S5.SS2.p2.2.m2.1.1.1.1.1.3.cmml">B</mi></mrow><mo maxsize="90%" minsize="90%" id="S5.SS2.p2.2.m2.1.1.1.1.3" xref="S5.SS2.p2.2.m2.1.1.1.2.1.cmml">|</mo></mrow><mrow id="S5.SS2.p2.2.m2.2.2.2.1" xref="S5.SS2.p2.2.m2.2.2.2.2.cmml"><mo maxsize="90%" minsize="90%" id="S5.SS2.p2.2.m2.2.2.2.1.2" xref="S5.SS2.p2.2.m2.2.2.2.2.1.cmml">|</mo><mrow id="S5.SS2.p2.2.m2.2.2.2.1.1" xref="S5.SS2.p2.2.m2.2.2.2.1.1.cmml"><mi mathsize="90%" id="S5.SS2.p2.2.m2.2.2.2.1.1.2" xref="S5.SS2.p2.2.m2.2.2.2.1.1.2.cmml">A</mi><mo mathsize="90%" id="S5.SS2.p2.2.m2.2.2.2.1.1.1" xref="S5.SS2.p2.2.m2.2.2.2.1.1.1.cmml">‚à™</mo><mi mathsize="90%" id="S5.SS2.p2.2.m2.2.2.2.1.1.3" xref="S5.SS2.p2.2.m2.2.2.2.1.1.3.cmml">B</mi></mrow><mo maxsize="90%" minsize="90%" id="S5.SS2.p2.2.m2.2.2.2.1.3" xref="S5.SS2.p2.2.m2.2.2.2.2.1.cmml">|</mo></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S5.SS2.p2.2.m2.2b"><apply id="S5.SS2.p2.2.m2.2.2.cmml" xref="S5.SS2.p2.2.m2.2.2"><divide id="S5.SS2.p2.2.m2.2.2.3.cmml" xref="S5.SS2.p2.2.m2.2.2"></divide><apply id="S5.SS2.p2.2.m2.1.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1"><abs id="S5.SS2.p2.2.m2.1.1.1.2.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1.2"></abs><apply id="S5.SS2.p2.2.m2.1.1.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1.1"><intersect id="S5.SS2.p2.2.m2.1.1.1.1.1.1.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1.1.1"></intersect><ci id="S5.SS2.p2.2.m2.1.1.1.1.1.2.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1.1.2">ùê¥</ci><ci id="S5.SS2.p2.2.m2.1.1.1.1.1.3.cmml" xref="S5.SS2.p2.2.m2.1.1.1.1.1.3">ùêµ</ci></apply></apply><apply id="S5.SS2.p2.2.m2.2.2.2.2.cmml" xref="S5.SS2.p2.2.m2.2.2.2.1"><abs id="S5.SS2.p2.2.m2.2.2.2.2.1.cmml" xref="S5.SS2.p2.2.m2.2.2.2.1.2"></abs><apply id="S5.SS2.p2.2.m2.2.2.2.1.1.cmml" xref="S5.SS2.p2.2.m2.2.2.2.1.1"><union id="S5.SS2.p2.2.m2.2.2.2.1.1.1.cmml" xref="S5.SS2.p2.2.m2.2.2.2.1.1.1"></union><ci id="S5.SS2.p2.2.m2.2.2.2.1.1.2.cmml" xref="S5.SS2.p2.2.m2.2.2.2.1.1.2">ùê¥</ci><ci id="S5.SS2.p2.2.m2.2.2.2.1.1.3.cmml" xref="S5.SS2.p2.2.m2.2.2.2.1.1.3">ùêµ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p2.2.m2.2c">\frac{|A\cap B|}{|A\cup B|}</annotation></semantics></math><span id="S5.SS2.p2.2.6" class="ltx_text" style="font-size:90%;">. We compute overlaps between each pair of captions, drawing an edge between two captions if the overlap is above some minimum overlap threshold, selected on the validation sample.</span></p>
</div>
<div id="S5.SS2.p3" class="ltx_para">
<p id="S5.SS2.p3.2" class="ltx_p"><span id="S5.SS2.p3.2.1" class="ltx_text" style="font-size:90%;">To compute Jaccard similarity, we use 2-grams and a 10% overlap threshold. Both the </span><math id="S5.SS2.p3.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS2.p3.1.m1.1a"><mi mathsize="90%" id="S5.SS2.p3.1.m1.1.1" xref="S5.SS2.p3.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.1.m1.1b"><ci id="S5.SS2.p3.1.m1.1.1.cmml" xref="S5.SS2.p3.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.1.m1.1c">n</annotation></semantics></math><span id="S5.SS2.p3.2.2" class="ltx_text" style="font-size:90%;"> for </span><math id="S5.SS2.p3.2.m2.1" class="ltx_Math" alttext="n" display="inline"><semantics id="S5.SS2.p3.2.m2.1a"><mi mathsize="90%" id="S5.SS2.p3.2.m2.1.1" xref="S5.SS2.p3.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S5.SS2.p3.2.m2.1b"><ci id="S5.SS2.p3.2.m2.1.1.cmml" xref="S5.SS2.p3.2.m2.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p3.2.m2.1c">n</annotation></semantics></math><span id="S5.SS2.p3.2.3" class="ltx_text" style="font-size:90%;">-grams and the similarity threshold were tuned on the same validation sample used to choose the single linkage clustering threshold. As is standard, text is uncased and stripped of punctuation. The text used for all experiments was spell-checked. Jaccard similarity achieves an ARI of 40.3.
The purely self-supervised language-image model achieves an ARI of 43.0 (pooled embeddings), beating the ruled based method without requiring any annotation.</span></p>
</div>
<div id="S5.SS2.p4" class="ltx_para">
<p id="S5.SS2.p4.1" class="ltx_p"><span id="S5.SS2.p4.1.1" class="ltx_text" style="font-size:90%;">Ablations are reported in the supplementary materials.</span></p>
</div>
</section>
</section>
<section id="S6" class="ltx_section">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>

<div id="S6.p1" class="ltx_para">
<p id="S6.p1.1" class="ltx_p"><span id="S6.p1.1.1" class="ltx_text" style="font-size:90%;">This study demonstrates that multimodal methods can significantly improve record linkage, while remaining highly scalable. Compared to methods that predominate in the record linkage literature, deep multimodal approaches can leverage the power of transfer learning, facilitating their application to highly diverse settings with modest annotation requirements.
Purely self-supervised multimodal methods can also outperform widely used string matching methods.
While our focus here is on image-text linking, it would be straightforward to extend the </span><span id="S6.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="S6.p1.1.3" class="ltx_text" style="font-size:90%;"> framework to include audio, which is potentially central for datasets where noise enters through misspellings of entity names. Overall deep multimodal record linkage offers potentially substantial gains in a variety of settings where traditional string matching methods fall short of the required accuracy, including a variety of low resource settings, broadening the diversity and improving the quality of data for downstream applications.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
<section id="Ax1" class="ltx_appendix ltx_centering">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">Supplementary Materials</h2>

</section>
</section>
<section id="A1" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Methods</h2>

<section id="A1.SS1" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Japanese Multimodal Models</h3>

<div id="A1.SS1.p1" class="ltx_para">
<p id="A1.SS1.p1.1" class="ltx_p"><span id="A1.SS1.p1.1.1" class="ltx_text" style="font-size:90%;">The Japanese multimodal models were initialized with a Japanese CLIP checkpoint </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS1.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="A1.SS1.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS1.p1.1.4" class="ltx_text" style="font-size:90%;">. Japanese CLIP was trained with the standard CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS1.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="A1.SS1.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS1.p1.1.7" class="ltx_text" style="font-size:90%;"> loss but used a BERT-based text encoder and the vision transformer was initialized by weights from the AugReg ViT-B/16 model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS1.p1.1.8.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib54" title="" class="ltx_ref">54</a><span id="A1.SS1.p1.1.9.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS1.p1.1.10" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<section id="A1.SS1.SSSx1" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">Synthetic Data</h4>

<div id="A1.SS1.SSSx1.p1" class="ltx_para">
<p id="A1.SS1.SSSx1.p1.1" class="ltx_p"><span id="A1.SS1.SSSx1.p1.1.1" class="ltx_text" style="font-size:90%;">Both language-image pretraining and the supervised training of </span><span id="A1.SS1.SSSx1.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="A1.SS1.SSSx1.p1.1.3" class="ltx_text" style="font-size:90%;"> employed synthetic data. To create synthetic data, we rendered a list of common Japanese words as images using different fonts (a type of augmentation), applied image augmentations, and fed the resulting images to OCR. For the same word, this produced varying views of the word‚Äôs image due to different augmentations, as well as different views of the word‚Äôs text due to varying OCR errors induced by the augmentations.</span></p>
</div>
<div id="A1.SS1.SSSx1.p2" class="ltx_para">
<p id="A1.SS1.SSSx1.p2.1" class="ltx_p"><span id="A1.SS1.SSSx1.p2.1.1" class="ltx_text" style="font-size:90%;">We randomly sample one image-text pair per label (word) and use this subset for language-image pretraining. For training </span><span id="A1.SS1.SSSx1.p2.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="A1.SS1.SSSx1.p2.1.3" class="ltx_text" style="font-size:90%;">, we train on the full synthetic dataset and then fine-tune on our labelled data.</span></p>
</div>
</section>
<section id="A1.SS1.SSSx2" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">Other Training Details</h4>

<div id="A1.SS1.SSSx2.p1" class="ltx_para">
<p id="A1.SS1.SSSx2.p1.1" class="ltx_p"><span id="A1.SS1.SSSx2.p1.1.1" class="ltx_text" style="font-size:90%;">Text crops are thin vertically or horizontally oriented rectangles, with highly diverse aspect ratios, whereas vision encoders are almost always trained on square images. Center cropping would discard important information and resizing often badly morphs the image, given that the underlying aspect ratios are far from a square. To preserve the aspect ratio, we pad the rectangular crop with the median value of the border pixel such that the text region is always centered.</span></p>
</div>
<div id="A1.SS1.SSSx2.p2" class="ltx_para">
<p id="A1.SS1.SSSx2.p2.1" class="ltx_p"><span id="A1.SS1.SSSx2.p2.1.1" class="ltx_text" style="font-size:90%;">For language-image pretraining, the standard CLIP loss was used to align the image and text encoders </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS1.SSSx2.p2.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="A1.SS1.SSSx2.p2.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS1.SSSx2.p2.1.4" class="ltx_text" style="font-size:90%;">. Supervised Contrastive loss </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS1.SSSx2.p2.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib30" title="" class="ltx_ref">30</a><span id="A1.SS1.SSSx2.p2.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS1.SSSx2.p2.1.7" class="ltx_text" style="font-size:90%;"> was used for all supervised training. We used the AdamW optimizer for all model training along with a Cosine Annealing with Warm Restarts learning rate (LR) scheduler where the maximum LR was specified for each run and the minimum LR was set to 0. 10 steps were chosen for the first restart with a restart factor of 2 that doubles the time to restart after every restart. An epoch involved sampling </span><math id="A1.SS1.SSSx2.p2.1.m1.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A1.SS1.SSSx2.p2.1.m1.1a"><mi mathsize="90%" id="A1.SS1.SSSx2.p2.1.m1.1.1" xref="A1.SS1.SSSx2.p2.1.m1.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx2.p2.1.m1.1b"><ci id="A1.SS1.SSSx2.p2.1.m1.1.1.cmml" xref="A1.SS1.SSSx2.p2.1.m1.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx2.p2.1.m1.1c">m</annotation></semantics></math><span id="A1.SS1.SSSx2.p2.1.8" class="ltx_text" style="font-size:90%;"> views (image-text pair) of each label in the dataset and going through each of them once. It took a 24 hours to perform language-image pretraining, 10 hours to perform supervised training using synthetic data and 40 minutes to train on the labelled data - a total training time of 34.6 hours to train </span><span id="A1.SS1.SSSx2.p2.1.9" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="A1.SS1.SSSx2.p2.1.10" class="ltx_text" style="font-size:90%;"> on a single A6000 GPU card.
Hyperparameters and additional details about model training are listed in Table </span><a href="#A2.T1" title="Table S-1 ‚Ä£ Appendix B Additional Results ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">S-1</span></a><span id="A1.SS1.SSSx2.p2.1.11" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A1.SS1.SSSx2.p3" class="ltx_para">
<p id="A1.SS1.SSSx2.p3.1" class="ltx_p"><span id="A1.SS1.SSSx2.p3.1.1" class="ltx_text" style="font-size:90%;">At inference time, we used </span><span id="A1.SS1.SSSx2.p3.1.2" class="ltx_text ltx_font_italic" style="font-size:90%;">IndexIPFlat</span><span id="A1.SS1.SSSx2.p3.1.3" class="ltx_text" style="font-size:90%;"> from FAISS </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS1.SSSx2.p3.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib26" title="" class="ltx_ref">26</a><span id="A1.SS1.SSSx2.p3.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS1.SSSx2.p3.1.6" class="ltx_text" style="font-size:90%;"> to find the nearest neighbor on L2-normalized embeddings.</span></p>
</div>
</section>
<section id="A1.SS1.SSSx3" class="ltx_subsubsection">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">Hard Negative Mining</h4>

<div id="A1.SS1.SSSx3.p1" class="ltx_para">
<p id="A1.SS1.SSSx3.p1.1" class="ltx_p"><span id="A1.SS1.SSSx3.p1.1.1" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="A1.SS1.SSSx3.p1.1.2" class="ltx_text" style="font-size:90%;"> was trained on a single A6000 GPU card, which could fit a batch size </span><math id="A1.SS1.SSSx3.p1.1.m1.1" class="ltx_Math" alttext="B" display="inline"><semantics id="A1.SS1.SSSx3.p1.1.m1.1a"><mi mathsize="90%" id="A1.SS1.SSSx3.p1.1.m1.1.1" xref="A1.SS1.SSSx3.p1.1.m1.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p1.1.m1.1b"><ci id="A1.SS1.SSSx3.p1.1.m1.1.1.cmml" xref="A1.SS1.SSSx3.p1.1.m1.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p1.1.m1.1c">B</annotation></semantics></math><span id="A1.SS1.SSSx3.p1.1.3" class="ltx_text" style="font-size:90%;"> of 153 image-text pairs. This compares to a batch size of 32,768 used to train the original CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS1.SSSx3.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="A1.SS1.SSSx3.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS1.SSSx3.p1.1.6" class="ltx_text" style="font-size:90%;"> on 256 V100 GPUs. Offline hard negative mining was used to achieve sufficient in-batch negatives with the small batch sizes that can fit into compute setups that are realistic for diverse downstream users.</span></p>
</div>
<div id="A1.SS1.SSSx3.p2" class="ltx_para">
<p id="A1.SS1.SSSx3.p2.14" class="ltx_p"><span id="A1.SS1.SSSx3.p2.14.1" class="ltx_text" style="font-size:90%;">Define the data as a triple </span><math id="A1.SS1.SSSx3.p2.1.m1.3" class="ltx_Math" alttext="(x_{n},t_{n},y_{n})" display="inline"><semantics id="A1.SS1.SSSx3.p2.1.m1.3a"><mrow id="A1.SS1.SSSx3.p2.1.m1.3.3.3" xref="A1.SS1.SSSx3.p2.1.m1.3.3.4.cmml"><mo maxsize="90%" minsize="90%" id="A1.SS1.SSSx3.p2.1.m1.3.3.3.4" xref="A1.SS1.SSSx3.p2.1.m1.3.3.4.cmml">(</mo><msub id="A1.SS1.SSSx3.p2.1.m1.1.1.1.1" xref="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.2" xref="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.2.cmml">x</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.3" xref="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.3.cmml">n</mi></msub><mo mathsize="90%" id="A1.SS1.SSSx3.p2.1.m1.3.3.3.5" xref="A1.SS1.SSSx3.p2.1.m1.3.3.4.cmml">,</mo><msub id="A1.SS1.SSSx3.p2.1.m1.2.2.2.2" xref="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.2" xref="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.2.cmml">t</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.3" xref="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.3.cmml">n</mi></msub><mo mathsize="90%" id="A1.SS1.SSSx3.p2.1.m1.3.3.3.6" xref="A1.SS1.SSSx3.p2.1.m1.3.3.4.cmml">,</mo><msub id="A1.SS1.SSSx3.p2.1.m1.3.3.3.3" xref="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.2" xref="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.2.cmml">y</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.3" xref="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.3.cmml">n</mi></msub><mo maxsize="90%" minsize="90%" id="A1.SS1.SSSx3.p2.1.m1.3.3.3.7" xref="A1.SS1.SSSx3.p2.1.m1.3.3.4.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.1.m1.3b"><vector id="A1.SS1.SSSx3.p2.1.m1.3.3.4.cmml" xref="A1.SS1.SSSx3.p2.1.m1.3.3.3"><apply id="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.1.m1.1.1.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.1.m1.1.1.1.1">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.2">ùë•</ci><ci id="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.1.m1.1.1.1.1.3">ùëõ</ci></apply><apply id="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.cmml" xref="A1.SS1.SSSx3.p2.1.m1.2.2.2.2"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.1.cmml" xref="A1.SS1.SSSx3.p2.1.m1.2.2.2.2">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.2.cmml" xref="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.2">ùë°</ci><ci id="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.3.cmml" xref="A1.SS1.SSSx3.p2.1.m1.2.2.2.2.3">ùëõ</ci></apply><apply id="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.cmml" xref="A1.SS1.SSSx3.p2.1.m1.3.3.3.3"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.1.cmml" xref="A1.SS1.SSSx3.p2.1.m1.3.3.3.3">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.2.cmml" xref="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.2">ùë¶</ci><ci id="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.3.cmml" xref="A1.SS1.SSSx3.p2.1.m1.3.3.3.3.3">ùëõ</ci></apply></vector></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.1.m1.3c">(x_{n},t_{n},y_{n})</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.2" class="ltx_text" style="font-size:90%;">,
where </span><math id="A1.SS1.SSSx3.p2.2.m2.1" class="ltx_Math" alttext="x_{n}\in\mathcal{X}" display="inline"><semantics id="A1.SS1.SSSx3.p2.2.m2.1a"><mrow id="A1.SS1.SSSx3.p2.2.m2.1.1" xref="A1.SS1.SSSx3.p2.2.m2.1.1.cmml"><msub id="A1.SS1.SSSx3.p2.2.m2.1.1.2" xref="A1.SS1.SSSx3.p2.2.m2.1.1.2.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.2.m2.1.1.2.2" xref="A1.SS1.SSSx3.p2.2.m2.1.1.2.2.cmml">x</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.2.m2.1.1.2.3" xref="A1.SS1.SSSx3.p2.2.m2.1.1.2.3.cmml">n</mi></msub><mo mathsize="90%" id="A1.SS1.SSSx3.p2.2.m2.1.1.1" xref="A1.SS1.SSSx3.p2.2.m2.1.1.1.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="A1.SS1.SSSx3.p2.2.m2.1.1.3" xref="A1.SS1.SSSx3.p2.2.m2.1.1.3.cmml">ùí≥</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.2.m2.1b"><apply id="A1.SS1.SSSx3.p2.2.m2.1.1.cmml" xref="A1.SS1.SSSx3.p2.2.m2.1.1"><in id="A1.SS1.SSSx3.p2.2.m2.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.2.m2.1.1.1"></in><apply id="A1.SS1.SSSx3.p2.2.m2.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.2.m2.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.2.m2.1.1.2.1.cmml" xref="A1.SS1.SSSx3.p2.2.m2.1.1.2">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.2.m2.1.1.2.2.cmml" xref="A1.SS1.SSSx3.p2.2.m2.1.1.2.2">ùë•</ci><ci id="A1.SS1.SSSx3.p2.2.m2.1.1.2.3.cmml" xref="A1.SS1.SSSx3.p2.2.m2.1.1.2.3">ùëõ</ci></apply><ci id="A1.SS1.SSSx3.p2.2.m2.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.2.m2.1.1.3">ùí≥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.2.m2.1c">x_{n}\in\mathcal{X}</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.3" class="ltx_text" style="font-size:90%;"> is the image, </span><math id="A1.SS1.SSSx3.p2.3.m3.1" class="ltx_Math" alttext="t_{n}\in\mathcal{T}" display="inline"><semantics id="A1.SS1.SSSx3.p2.3.m3.1a"><mrow id="A1.SS1.SSSx3.p2.3.m3.1.1" xref="A1.SS1.SSSx3.p2.3.m3.1.1.cmml"><msub id="A1.SS1.SSSx3.p2.3.m3.1.1.2" xref="A1.SS1.SSSx3.p2.3.m3.1.1.2.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.3.m3.1.1.2.2" xref="A1.SS1.SSSx3.p2.3.m3.1.1.2.2.cmml">t</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.3.m3.1.1.2.3" xref="A1.SS1.SSSx3.p2.3.m3.1.1.2.3.cmml">n</mi></msub><mo mathsize="90%" id="A1.SS1.SSSx3.p2.3.m3.1.1.1" xref="A1.SS1.SSSx3.p2.3.m3.1.1.1.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="A1.SS1.SSSx3.p2.3.m3.1.1.3" xref="A1.SS1.SSSx3.p2.3.m3.1.1.3.cmml">ùíØ</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.3.m3.1b"><apply id="A1.SS1.SSSx3.p2.3.m3.1.1.cmml" xref="A1.SS1.SSSx3.p2.3.m3.1.1"><in id="A1.SS1.SSSx3.p2.3.m3.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.3.m3.1.1.1"></in><apply id="A1.SS1.SSSx3.p2.3.m3.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.3.m3.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.3.m3.1.1.2.1.cmml" xref="A1.SS1.SSSx3.p2.3.m3.1.1.2">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.3.m3.1.1.2.2.cmml" xref="A1.SS1.SSSx3.p2.3.m3.1.1.2.2">ùë°</ci><ci id="A1.SS1.SSSx3.p2.3.m3.1.1.2.3.cmml" xref="A1.SS1.SSSx3.p2.3.m3.1.1.2.3">ùëõ</ci></apply><ci id="A1.SS1.SSSx3.p2.3.m3.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.3.m3.1.1.3">ùíØ</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.3.m3.1c">t_{n}\in\mathcal{T}</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.4" class="ltx_text" style="font-size:90%;"> is the text, and </span><math id="A1.SS1.SSSx3.p2.4.m4.1" class="ltx_Math" alttext="y_{n}\in\mathcal{Y}" display="inline"><semantics id="A1.SS1.SSSx3.p2.4.m4.1a"><mrow id="A1.SS1.SSSx3.p2.4.m4.1.1" xref="A1.SS1.SSSx3.p2.4.m4.1.1.cmml"><msub id="A1.SS1.SSSx3.p2.4.m4.1.1.2" xref="A1.SS1.SSSx3.p2.4.m4.1.1.2.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.4.m4.1.1.2.2" xref="A1.SS1.SSSx3.p2.4.m4.1.1.2.2.cmml">y</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.4.m4.1.1.2.3" xref="A1.SS1.SSSx3.p2.4.m4.1.1.2.3.cmml">n</mi></msub><mo mathsize="90%" id="A1.SS1.SSSx3.p2.4.m4.1.1.1" xref="A1.SS1.SSSx3.p2.4.m4.1.1.1.cmml">‚àà</mo><mi class="ltx_font_mathcaligraphic" mathsize="90%" id="A1.SS1.SSSx3.p2.4.m4.1.1.3" xref="A1.SS1.SSSx3.p2.4.m4.1.1.3.cmml">ùí¥</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.4.m4.1b"><apply id="A1.SS1.SSSx3.p2.4.m4.1.1.cmml" xref="A1.SS1.SSSx3.p2.4.m4.1.1"><in id="A1.SS1.SSSx3.p2.4.m4.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.4.m4.1.1.1"></in><apply id="A1.SS1.SSSx3.p2.4.m4.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.4.m4.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.4.m4.1.1.2.1.cmml" xref="A1.SS1.SSSx3.p2.4.m4.1.1.2">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.4.m4.1.1.2.2.cmml" xref="A1.SS1.SSSx3.p2.4.m4.1.1.2.2">ùë¶</ci><ci id="A1.SS1.SSSx3.p2.4.m4.1.1.2.3.cmml" xref="A1.SS1.SSSx3.p2.4.m4.1.1.2.3">ùëõ</ci></apply><ci id="A1.SS1.SSSx3.p2.4.m4.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.4.m4.1.1.3">ùí¥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.4.m4.1c">y_{n}\in\mathcal{Y}</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.5" class="ltx_text" style="font-size:90%;"> is an associated label (class). Let </span><math id="A1.SS1.SSSx3.p2.5.m5.1" class="ltx_Math" alttext="D" display="inline"><semantics id="A1.SS1.SSSx3.p2.5.m5.1a"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.5.m5.1.1" xref="A1.SS1.SSSx3.p2.5.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.5.m5.1b"><ci id="A1.SS1.SSSx3.p2.5.m5.1.1.cmml" xref="A1.SS1.SSSx3.p2.5.m5.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.5.m5.1c">D</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.6" class="ltx_text" style="font-size:90%;"> be the set of all triples.
For supervised pertaining using synthetic data, we randomly sampled one image-text pair per label in </span><math id="A1.SS1.SSSx3.p2.6.m6.1" class="ltx_Math" alttext="D" display="inline"><semantics id="A1.SS1.SSSx3.p2.6.m6.1a"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.6.m6.1.1" xref="A1.SS1.SSSx3.p2.6.m6.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.6.m6.1b"><ci id="A1.SS1.SSSx3.p2.6.m6.1.1.cmml" xref="A1.SS1.SSSx3.p2.6.m6.1.1">ùê∑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.6.m6.1c">D</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.7" class="ltx_text" style="font-size:90%;"> to form </span><math id="A1.SS1.SSSx3.p2.7.m7.1" class="ltx_Math" alttext="D^{\prime}\subset D" display="inline"><semantics id="A1.SS1.SSSx3.p2.7.m7.1a"><mrow id="A1.SS1.SSSx3.p2.7.m7.1.1" xref="A1.SS1.SSSx3.p2.7.m7.1.1.cmml"><msup id="A1.SS1.SSSx3.p2.7.m7.1.1.2" xref="A1.SS1.SSSx3.p2.7.m7.1.1.2.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.7.m7.1.1.2.2" xref="A1.SS1.SSSx3.p2.7.m7.1.1.2.2.cmml">D</mi><mo mathsize="90%" id="A1.SS1.SSSx3.p2.7.m7.1.1.2.3" xref="A1.SS1.SSSx3.p2.7.m7.1.1.2.3.cmml">‚Ä≤</mo></msup><mo mathsize="90%" id="A1.SS1.SSSx3.p2.7.m7.1.1.1" xref="A1.SS1.SSSx3.p2.7.m7.1.1.1.cmml">‚äÇ</mo><mi mathsize="90%" id="A1.SS1.SSSx3.p2.7.m7.1.1.3" xref="A1.SS1.SSSx3.p2.7.m7.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.7.m7.1b"><apply id="A1.SS1.SSSx3.p2.7.m7.1.1.cmml" xref="A1.SS1.SSSx3.p2.7.m7.1.1"><subset id="A1.SS1.SSSx3.p2.7.m7.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.7.m7.1.1.1"></subset><apply id="A1.SS1.SSSx3.p2.7.m7.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.7.m7.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.7.m7.1.1.2.1.cmml" xref="A1.SS1.SSSx3.p2.7.m7.1.1.2">superscript</csymbol><ci id="A1.SS1.SSSx3.p2.7.m7.1.1.2.2.cmml" xref="A1.SS1.SSSx3.p2.7.m7.1.1.2.2">ùê∑</ci><ci id="A1.SS1.SSSx3.p2.7.m7.1.1.2.3.cmml" xref="A1.SS1.SSSx3.p2.7.m7.1.1.2.3">‚Ä≤</ci></apply><ci id="A1.SS1.SSSx3.p2.7.m7.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.7.m7.1.1.3">ùê∑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.7.m7.1c">D^{\prime}\subset D</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.8" class="ltx_text" style="font-size:90%;">. For each image-text pair </span><math id="A1.SS1.SSSx3.p2.8.m8.2" class="ltx_Math" alttext="(x_{a},t_{a})" display="inline"><semantics id="A1.SS1.SSSx3.p2.8.m8.2a"><mrow id="A1.SS1.SSSx3.p2.8.m8.2.2.2" xref="A1.SS1.SSSx3.p2.8.m8.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="A1.SS1.SSSx3.p2.8.m8.2.2.2.3" xref="A1.SS1.SSSx3.p2.8.m8.2.2.3.cmml">(</mo><msub id="A1.SS1.SSSx3.p2.8.m8.1.1.1.1" xref="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.2" xref="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.2.cmml">x</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.3" xref="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.3.cmml">a</mi></msub><mo mathsize="90%" id="A1.SS1.SSSx3.p2.8.m8.2.2.2.4" xref="A1.SS1.SSSx3.p2.8.m8.2.2.3.cmml">,</mo><msub id="A1.SS1.SSSx3.p2.8.m8.2.2.2.2" xref="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.2" xref="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.2.cmml">t</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.3" xref="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.3.cmml">a</mi></msub><mo maxsize="90%" minsize="90%" id="A1.SS1.SSSx3.p2.8.m8.2.2.2.5" xref="A1.SS1.SSSx3.p2.8.m8.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.8.m8.2b"><interval closure="open" id="A1.SS1.SSSx3.p2.8.m8.2.2.3.cmml" xref="A1.SS1.SSSx3.p2.8.m8.2.2.2"><apply id="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.8.m8.1.1.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.8.m8.1.1.1.1">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.2">ùë•</ci><ci id="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.8.m8.1.1.1.1.3">ùëé</ci></apply><apply id="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.cmml" xref="A1.SS1.SSSx3.p2.8.m8.2.2.2.2"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.1.cmml" xref="A1.SS1.SSSx3.p2.8.m8.2.2.2.2">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.2.cmml" xref="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.2">ùë°</ci><ci id="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.3.cmml" xref="A1.SS1.SSSx3.p2.8.m8.2.2.2.2.3">ùëé</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.8.m8.2c">(x_{a},t_{a})</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.9" class="ltx_text" style="font-size:90%;"> in </span><math id="A1.SS1.SSSx3.p2.9.m9.1" class="ltx_Math" alttext="D^{\prime}" display="inline"><semantics id="A1.SS1.SSSx3.p2.9.m9.1a"><msup id="A1.SS1.SSSx3.p2.9.m9.1.1" xref="A1.SS1.SSSx3.p2.9.m9.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.9.m9.1.1.2" xref="A1.SS1.SSSx3.p2.9.m9.1.1.2.cmml">D</mi><mo mathsize="90%" id="A1.SS1.SSSx3.p2.9.m9.1.1.3" xref="A1.SS1.SSSx3.p2.9.m9.1.1.3.cmml">‚Ä≤</mo></msup><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.9.m9.1b"><apply id="A1.SS1.SSSx3.p2.9.m9.1.1.cmml" xref="A1.SS1.SSSx3.p2.9.m9.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.9.m9.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.9.m9.1.1">superscript</csymbol><ci id="A1.SS1.SSSx3.p2.9.m9.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.9.m9.1.1.2">ùê∑</ci><ci id="A1.SS1.SSSx3.p2.9.m9.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.9.m9.1.1.3">‚Ä≤</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.9.m9.1c">D^{\prime}</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.10" class="ltx_text" style="font-size:90%;">, we use the domain-adapted CLIP model to find its </span><math id="A1.SS1.SSSx3.p2.10.m10.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A1.SS1.SSSx3.p2.10.m10.1a"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.10.m10.1.1" xref="A1.SS1.SSSx3.p2.10.m10.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.10.m10.1b"><ci id="A1.SS1.SSSx3.p2.10.m10.1.1.cmml" xref="A1.SS1.SSSx3.p2.10.m10.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.10.m10.1c">k</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.11" class="ltx_text" style="font-size:90%;"> nearest neighbor pairs </span><math id="A1.SS1.SSSx3.p2.11.m11.2" class="ltx_Math" alttext="(x_{k},t_{k})" display="inline"><semantics id="A1.SS1.SSSx3.p2.11.m11.2a"><mrow id="A1.SS1.SSSx3.p2.11.m11.2.2.2" xref="A1.SS1.SSSx3.p2.11.m11.2.2.3.cmml"><mo maxsize="90%" minsize="90%" id="A1.SS1.SSSx3.p2.11.m11.2.2.2.3" xref="A1.SS1.SSSx3.p2.11.m11.2.2.3.cmml">(</mo><msub id="A1.SS1.SSSx3.p2.11.m11.1.1.1.1" xref="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.2" xref="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.2.cmml">x</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.3" xref="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.3.cmml">k</mi></msub><mo mathsize="90%" id="A1.SS1.SSSx3.p2.11.m11.2.2.2.4" xref="A1.SS1.SSSx3.p2.11.m11.2.2.3.cmml">,</mo><msub id="A1.SS1.SSSx3.p2.11.m11.2.2.2.2" xref="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.2" xref="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.2.cmml">t</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.3" xref="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.3.cmml">k</mi></msub><mo maxsize="90%" minsize="90%" id="A1.SS1.SSSx3.p2.11.m11.2.2.2.5" xref="A1.SS1.SSSx3.p2.11.m11.2.2.3.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.11.m11.2b"><interval closure="open" id="A1.SS1.SSSx3.p2.11.m11.2.2.3.cmml" xref="A1.SS1.SSSx3.p2.11.m11.2.2.2"><apply id="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.11.m11.1.1.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.11.m11.1.1.1.1">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.2">ùë•</ci><ci id="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.11.m11.1.1.1.1.3">ùëò</ci></apply><apply id="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.cmml" xref="A1.SS1.SSSx3.p2.11.m11.2.2.2.2"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.1.cmml" xref="A1.SS1.SSSx3.p2.11.m11.2.2.2.2">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.2.cmml" xref="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.2">ùë°</ci><ci id="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.3.cmml" xref="A1.SS1.SSSx3.p2.11.m11.2.2.2.2.3">ùëò</ci></apply></interval></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.11.m11.2c">(x_{k},t_{k})</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.12" class="ltx_text" style="font-size:90%;"> (including itself). This gives us the </span><math id="A1.SS1.SSSx3.p2.12.m12.1" class="ltx_Math" alttext="k-1" display="inline"><semantics id="A1.SS1.SSSx3.p2.12.m12.1a"><mrow id="A1.SS1.SSSx3.p2.12.m12.1.1" xref="A1.SS1.SSSx3.p2.12.m12.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.12.m12.1.1.2" xref="A1.SS1.SSSx3.p2.12.m12.1.1.2.cmml">k</mi><mo mathsize="90%" id="A1.SS1.SSSx3.p2.12.m12.1.1.1" xref="A1.SS1.SSSx3.p2.12.m12.1.1.1.cmml">‚àí</mo><mn mathsize="90%" id="A1.SS1.SSSx3.p2.12.m12.1.1.3" xref="A1.SS1.SSSx3.p2.12.m12.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.12.m12.1b"><apply id="A1.SS1.SSSx3.p2.12.m12.1.1.cmml" xref="A1.SS1.SSSx3.p2.12.m12.1.1"><minus id="A1.SS1.SSSx3.p2.12.m12.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.12.m12.1.1.1"></minus><ci id="A1.SS1.SSSx3.p2.12.m12.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.12.m12.1.1.2">ùëò</ci><cn type="integer" id="A1.SS1.SSSx3.p2.12.m12.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.12.m12.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.12.m12.1c">k-1</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.13" class="ltx_text" style="font-size:90%;"> nearest neighbours for each label </span><math id="A1.SS1.SSSx3.p2.13.m13.1" class="ltx_Math" alttext="y_{a}" display="inline"><semantics id="A1.SS1.SSSx3.p2.13.m13.1a"><msub id="A1.SS1.SSSx3.p2.13.m13.1.1" xref="A1.SS1.SSSx3.p2.13.m13.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.13.m13.1.1.2" xref="A1.SS1.SSSx3.p2.13.m13.1.1.2.cmml">y</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p2.13.m13.1.1.3" xref="A1.SS1.SSSx3.p2.13.m13.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.13.m13.1b"><apply id="A1.SS1.SSSx3.p2.13.m13.1.1.cmml" xref="A1.SS1.SSSx3.p2.13.m13.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.13.m13.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.13.m13.1.1">subscript</csymbol><ci id="A1.SS1.SSSx3.p2.13.m13.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.13.m13.1.1.2">ùë¶</ci><ci id="A1.SS1.SSSx3.p2.13.m13.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.13.m13.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.13.m13.1c">y_{a}</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.14" class="ltx_text" style="font-size:90%;"> in </span><math id="A1.SS1.SSSx3.p2.14.m14.1" class="ltx_Math" alttext="D^{\prime}\subset D" display="inline"><semantics id="A1.SS1.SSSx3.p2.14.m14.1a"><mrow id="A1.SS1.SSSx3.p2.14.m14.1.1" xref="A1.SS1.SSSx3.p2.14.m14.1.1.cmml"><msup id="A1.SS1.SSSx3.p2.14.m14.1.1.2" xref="A1.SS1.SSSx3.p2.14.m14.1.1.2.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p2.14.m14.1.1.2.2" xref="A1.SS1.SSSx3.p2.14.m14.1.1.2.2.cmml">D</mi><mo mathsize="90%" id="A1.SS1.SSSx3.p2.14.m14.1.1.2.3" xref="A1.SS1.SSSx3.p2.14.m14.1.1.2.3.cmml">‚Ä≤</mo></msup><mo mathsize="90%" id="A1.SS1.SSSx3.p2.14.m14.1.1.1" xref="A1.SS1.SSSx3.p2.14.m14.1.1.1.cmml">‚äÇ</mo><mi mathsize="90%" id="A1.SS1.SSSx3.p2.14.m14.1.1.3" xref="A1.SS1.SSSx3.p2.14.m14.1.1.3.cmml">D</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p2.14.m14.1b"><apply id="A1.SS1.SSSx3.p2.14.m14.1.1.cmml" xref="A1.SS1.SSSx3.p2.14.m14.1.1"><subset id="A1.SS1.SSSx3.p2.14.m14.1.1.1.cmml" xref="A1.SS1.SSSx3.p2.14.m14.1.1.1"></subset><apply id="A1.SS1.SSSx3.p2.14.m14.1.1.2.cmml" xref="A1.SS1.SSSx3.p2.14.m14.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p2.14.m14.1.1.2.1.cmml" xref="A1.SS1.SSSx3.p2.14.m14.1.1.2">superscript</csymbol><ci id="A1.SS1.SSSx3.p2.14.m14.1.1.2.2.cmml" xref="A1.SS1.SSSx3.p2.14.m14.1.1.2.2">ùê∑</ci><ci id="A1.SS1.SSSx3.p2.14.m14.1.1.2.3.cmml" xref="A1.SS1.SSSx3.p2.14.m14.1.1.2.3">‚Ä≤</ci></apply><ci id="A1.SS1.SSSx3.p2.14.m14.1.1.3.cmml" xref="A1.SS1.SSSx3.p2.14.m14.1.1.3">ùê∑</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p2.14.m14.1c">D^{\prime}\subset D</annotation></semantics></math><span id="A1.SS1.SSSx3.p2.14.15" class="ltx_text" style="font-size:90%;"> .</span></p>
</div>
<div id="A1.SS1.SSSx3.p3" class="ltx_para">
<p id="A1.SS1.SSSx3.p3.11" class="ltx_p"><span id="A1.SS1.SSSx3.p3.11.1" class="ltx_text" style="font-size:90%;">The anchor label </span><math id="A1.SS1.SSSx3.p3.1.m1.1" class="ltx_Math" alttext="y_{a}" display="inline"><semantics id="A1.SS1.SSSx3.p3.1.m1.1a"><msub id="A1.SS1.SSSx3.p3.1.m1.1.1" xref="A1.SS1.SSSx3.p3.1.m1.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.1.m1.1.1.2" xref="A1.SS1.SSSx3.p3.1.m1.1.1.2.cmml">y</mi><mi mathsize="90%" id="A1.SS1.SSSx3.p3.1.m1.1.1.3" xref="A1.SS1.SSSx3.p3.1.m1.1.1.3.cmml">a</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.1.m1.1b"><apply id="A1.SS1.SSSx3.p3.1.m1.1.1.cmml" xref="A1.SS1.SSSx3.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS1.SSSx3.p3.1.m1.1.1.1.cmml" xref="A1.SS1.SSSx3.p3.1.m1.1.1">subscript</csymbol><ci id="A1.SS1.SSSx3.p3.1.m1.1.1.2.cmml" xref="A1.SS1.SSSx3.p3.1.m1.1.1.2">ùë¶</ci><ci id="A1.SS1.SSSx3.p3.1.m1.1.1.3.cmml" xref="A1.SS1.SSSx3.p3.1.m1.1.1.3">ùëé</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.1.m1.1c">y_{a}</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.2" class="ltx_text" style="font-size:90%;"> and its </span><math id="A1.SS1.SSSx3.p3.2.m2.1" class="ltx_Math" alttext="k-1" display="inline"><semantics id="A1.SS1.SSSx3.p3.2.m2.1a"><mrow id="A1.SS1.SSSx3.p3.2.m2.1.1" xref="A1.SS1.SSSx3.p3.2.m2.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.2.m2.1.1.2" xref="A1.SS1.SSSx3.p3.2.m2.1.1.2.cmml">k</mi><mo mathsize="90%" id="A1.SS1.SSSx3.p3.2.m2.1.1.1" xref="A1.SS1.SSSx3.p3.2.m2.1.1.1.cmml">‚àí</mo><mn mathsize="90%" id="A1.SS1.SSSx3.p3.2.m2.1.1.3" xref="A1.SS1.SSSx3.p3.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.2.m2.1b"><apply id="A1.SS1.SSSx3.p3.2.m2.1.1.cmml" xref="A1.SS1.SSSx3.p3.2.m2.1.1"><minus id="A1.SS1.SSSx3.p3.2.m2.1.1.1.cmml" xref="A1.SS1.SSSx3.p3.2.m2.1.1.1"></minus><ci id="A1.SS1.SSSx3.p3.2.m2.1.1.2.cmml" xref="A1.SS1.SSSx3.p3.2.m2.1.1.2">ùëò</ci><cn type="integer" id="A1.SS1.SSSx3.p3.2.m2.1.1.3.cmml" xref="A1.SS1.SSSx3.p3.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.2.m2.1c">k-1</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.3" class="ltx_text" style="font-size:90%;"> neighbors form a ‚Äùhard-negative‚Äù set. In a batch, we sample </span><math id="A1.SS1.SSSx3.p3.3.m3.1" class="ltx_Math" alttext="m=3" display="inline"><semantics id="A1.SS1.SSSx3.p3.3.m3.1a"><mrow id="A1.SS1.SSSx3.p3.3.m3.1.1" xref="A1.SS1.SSSx3.p3.3.m3.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.3.m3.1.1.2" xref="A1.SS1.SSSx3.p3.3.m3.1.1.2.cmml">m</mi><mo mathsize="90%" id="A1.SS1.SSSx3.p3.3.m3.1.1.1" xref="A1.SS1.SSSx3.p3.3.m3.1.1.1.cmml">=</mo><mn mathsize="90%" id="A1.SS1.SSSx3.p3.3.m3.1.1.3" xref="A1.SS1.SSSx3.p3.3.m3.1.1.3.cmml">3</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.3.m3.1b"><apply id="A1.SS1.SSSx3.p3.3.m3.1.1.cmml" xref="A1.SS1.SSSx3.p3.3.m3.1.1"><eq id="A1.SS1.SSSx3.p3.3.m3.1.1.1.cmml" xref="A1.SS1.SSSx3.p3.3.m3.1.1.1"></eq><ci id="A1.SS1.SSSx3.p3.3.m3.1.1.2.cmml" xref="A1.SS1.SSSx3.p3.3.m3.1.1.2">ùëö</ci><cn type="integer" id="A1.SS1.SSSx3.p3.3.m3.1.1.3.cmml" xref="A1.SS1.SSSx3.p3.3.m3.1.1.3">3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.3.m3.1c">m=3</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.4" class="ltx_text" style="font-size:90%;"> views of image-text pairs with replacement. A batch-size divisible by </span><math id="A1.SS1.SSSx3.p3.4.m4.1" class="ltx_Math" alttext="k*m" display="inline"><semantics id="A1.SS1.SSSx3.p3.4.m4.1a"><mrow id="A1.SS1.SSSx3.p3.4.m4.1.1" xref="A1.SS1.SSSx3.p3.4.m4.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.4.m4.1.1.2" xref="A1.SS1.SSSx3.p3.4.m4.1.1.2.cmml">k</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A1.SS1.SSSx3.p3.4.m4.1.1.1" xref="A1.SS1.SSSx3.p3.4.m4.1.1.1.cmml">‚àó</mo><mi mathsize="90%" id="A1.SS1.SSSx3.p3.4.m4.1.1.3" xref="A1.SS1.SSSx3.p3.4.m4.1.1.3.cmml">m</mi></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.4.m4.1b"><apply id="A1.SS1.SSSx3.p3.4.m4.1.1.cmml" xref="A1.SS1.SSSx3.p3.4.m4.1.1"><times id="A1.SS1.SSSx3.p3.4.m4.1.1.1.cmml" xref="A1.SS1.SSSx3.p3.4.m4.1.1.1"></times><ci id="A1.SS1.SSSx3.p3.4.m4.1.1.2.cmml" xref="A1.SS1.SSSx3.p3.4.m4.1.1.2">ùëò</ci><ci id="A1.SS1.SSSx3.p3.4.m4.1.1.3.cmml" xref="A1.SS1.SSSx3.p3.4.m4.1.1.3">ùëö</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.4.m4.1c">k*m</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.5" class="ltx_text" style="font-size:90%;"> can fit </span><math id="A1.SS1.SSSx3.p3.5.m5.1" class="ltx_Math" alttext="\dfrac{B}{k*m}" display="inline"><semantics id="A1.SS1.SSSx3.p3.5.m5.1a"><mstyle displaystyle="true" id="A1.SS1.SSSx3.p3.5.m5.1.1" xref="A1.SS1.SSSx3.p3.5.m5.1.1.cmml"><mfrac id="A1.SS1.SSSx3.p3.5.m5.1.1a" xref="A1.SS1.SSSx3.p3.5.m5.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.5.m5.1.1.2" xref="A1.SS1.SSSx3.p3.5.m5.1.1.2.cmml">B</mi><mrow id="A1.SS1.SSSx3.p3.5.m5.1.1.3" xref="A1.SS1.SSSx3.p3.5.m5.1.1.3.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.5.m5.1.1.3.2" xref="A1.SS1.SSSx3.p3.5.m5.1.1.3.2.cmml">k</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A1.SS1.SSSx3.p3.5.m5.1.1.3.1" xref="A1.SS1.SSSx3.p3.5.m5.1.1.3.1.cmml">‚àó</mo><mi mathsize="90%" id="A1.SS1.SSSx3.p3.5.m5.1.1.3.3" xref="A1.SS1.SSSx3.p3.5.m5.1.1.3.3.cmml">m</mi></mrow></mfrac></mstyle><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.5.m5.1b"><apply id="A1.SS1.SSSx3.p3.5.m5.1.1.cmml" xref="A1.SS1.SSSx3.p3.5.m5.1.1"><divide id="A1.SS1.SSSx3.p3.5.m5.1.1.1.cmml" xref="A1.SS1.SSSx3.p3.5.m5.1.1"></divide><ci id="A1.SS1.SSSx3.p3.5.m5.1.1.2.cmml" xref="A1.SS1.SSSx3.p3.5.m5.1.1.2">ùêµ</ci><apply id="A1.SS1.SSSx3.p3.5.m5.1.1.3.cmml" xref="A1.SS1.SSSx3.p3.5.m5.1.1.3"><times id="A1.SS1.SSSx3.p3.5.m5.1.1.3.1.cmml" xref="A1.SS1.SSSx3.p3.5.m5.1.1.3.1"></times><ci id="A1.SS1.SSSx3.p3.5.m5.1.1.3.2.cmml" xref="A1.SS1.SSSx3.p3.5.m5.1.1.3.2">ùëò</ci><ci id="A1.SS1.SSSx3.p3.5.m5.1.1.3.3.cmml" xref="A1.SS1.SSSx3.p3.5.m5.1.1.3.3">ùëö</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.5.m5.1c">\dfrac{B}{k*m}</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.6" class="ltx_text" style="font-size:90%;"> unique classes, each with their own </span><math id="A1.SS1.SSSx3.p3.6.m6.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A1.SS1.SSSx3.p3.6.m6.1a"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.6.m6.1.1" xref="A1.SS1.SSSx3.p3.6.m6.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.6.m6.1b"><ci id="A1.SS1.SSSx3.p3.6.m6.1.1.cmml" xref="A1.SS1.SSSx3.p3.6.m6.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.6.m6.1c">m</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.7" class="ltx_text" style="font-size:90%;"> views and the </span><math id="A1.SS1.SSSx3.p3.7.m7.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A1.SS1.SSSx3.p3.7.m7.1a"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.7.m7.1.1" xref="A1.SS1.SSSx3.p3.7.m7.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.7.m7.1b"><ci id="A1.SS1.SSSx3.p3.7.m7.1.1.cmml" xref="A1.SS1.SSSx3.p3.7.m7.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.7.m7.1c">m</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.8" class="ltx_text" style="font-size:90%;"> views of all </span><math id="A1.SS1.SSSx3.p3.8.m8.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A1.SS1.SSSx3.p3.8.m8.1a"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.8.m8.1.1" xref="A1.SS1.SSSx3.p3.8.m8.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.8.m8.1b"><ci id="A1.SS1.SSSx3.p3.8.m8.1.1.cmml" xref="A1.SS1.SSSx3.p3.8.m8.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.8.m8.1c">k</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.9" class="ltx_text" style="font-size:90%;"> neighbors.
We shuffle the hard-negative sets and partition them into groups of </span><math id="A1.SS1.SSSx3.p3.9.m9.1" class="ltx_Math" alttext="\dfrac{B}{k*m}" display="inline"><semantics id="A1.SS1.SSSx3.p3.9.m9.1a"><mstyle displaystyle="true" id="A1.SS1.SSSx3.p3.9.m9.1.1" xref="A1.SS1.SSSx3.p3.9.m9.1.1.cmml"><mfrac id="A1.SS1.SSSx3.p3.9.m9.1.1a" xref="A1.SS1.SSSx3.p3.9.m9.1.1.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.9.m9.1.1.2" xref="A1.SS1.SSSx3.p3.9.m9.1.1.2.cmml">B</mi><mrow id="A1.SS1.SSSx3.p3.9.m9.1.1.3" xref="A1.SS1.SSSx3.p3.9.m9.1.1.3.cmml"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.9.m9.1.1.3.2" xref="A1.SS1.SSSx3.p3.9.m9.1.1.3.2.cmml">k</mi><mo lspace="0.222em" mathsize="90%" rspace="0.222em" id="A1.SS1.SSSx3.p3.9.m9.1.1.3.1" xref="A1.SS1.SSSx3.p3.9.m9.1.1.3.1.cmml">‚àó</mo><mi mathsize="90%" id="A1.SS1.SSSx3.p3.9.m9.1.1.3.3" xref="A1.SS1.SSSx3.p3.9.m9.1.1.3.3.cmml">m</mi></mrow></mfrac></mstyle><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.9.m9.1b"><apply id="A1.SS1.SSSx3.p3.9.m9.1.1.cmml" xref="A1.SS1.SSSx3.p3.9.m9.1.1"><divide id="A1.SS1.SSSx3.p3.9.m9.1.1.1.cmml" xref="A1.SS1.SSSx3.p3.9.m9.1.1"></divide><ci id="A1.SS1.SSSx3.p3.9.m9.1.1.2.cmml" xref="A1.SS1.SSSx3.p3.9.m9.1.1.2">ùêµ</ci><apply id="A1.SS1.SSSx3.p3.9.m9.1.1.3.cmml" xref="A1.SS1.SSSx3.p3.9.m9.1.1.3"><times id="A1.SS1.SSSx3.p3.9.m9.1.1.3.1.cmml" xref="A1.SS1.SSSx3.p3.9.m9.1.1.3.1"></times><ci id="A1.SS1.SSSx3.p3.9.m9.1.1.3.2.cmml" xref="A1.SS1.SSSx3.p3.9.m9.1.1.3.2">ùëò</ci><ci id="A1.SS1.SSSx3.p3.9.m9.1.1.3.3.cmml" xref="A1.SS1.SSSx3.p3.9.m9.1.1.3.3">ùëö</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.9.m9.1c">\dfrac{B}{k*m}</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.10" class="ltx_text" style="font-size:90%;"> such that each group can constitute a batch. Each constituent class has </span><math id="A1.SS1.SSSx3.p3.10.m10.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A1.SS1.SSSx3.p3.10.m10.1a"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.10.m10.1.1" xref="A1.SS1.SSSx3.p3.10.m10.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.10.m10.1b"><ci id="A1.SS1.SSSx3.p3.10.m10.1.1.cmml" xref="A1.SS1.SSSx3.p3.10.m10.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.10.m10.1c">k</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.11" class="ltx_text" style="font-size:90%;"> neighbors and </span><math id="A1.SS1.SSSx3.p3.11.m11.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A1.SS1.SSSx3.p3.11.m11.1a"><mi mathsize="90%" id="A1.SS1.SSSx3.p3.11.m11.1.1" xref="A1.SS1.SSSx3.p3.11.m11.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.SSSx3.p3.11.m11.1b"><ci id="A1.SS1.SSSx3.p3.11.m11.1.1.cmml" xref="A1.SS1.SSSx3.p3.11.m11.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.SSSx3.p3.11.m11.1c">m</annotation></semantics></math><span id="A1.SS1.SSSx3.p3.11.12" class="ltx_text" style="font-size:90%;"> views within the minibatch.
For the next step - fine-tuning with labeled data - we follow the same approach but with the best model checkpoint from synthetic pretraining.</span></p>
</div>
</section>
</section>
<section id="A1.SS2" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>English Multimodal Models</h3>

<div id="A1.SS2.p1" class="ltx_para">
<p id="A1.SS2.p1.1" class="ltx_p"><span id="A1.SS2.p1.1.1" class="ltx_text" style="font-size:90%;">Our English </span><span id="A1.SS2.p1.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="A1.SS2.p1.1.3" class="ltx_text" style="font-size:90%;"> uses the official OpenAI CLIP model </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS2.p1.1.4.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="A1.SS2.p1.1.5.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS2.p1.1.6" class="ltx_text" style="font-size:90%;">, which has a ViT-B/32 Transformer architecture for the image encoder and a masked self-attention Transformer as the text encoder.
To train </span><span id="A1.SS2.p1.1.7" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="A1.SS2.p1.1.8" class="ltx_text" style="font-size:90%;"> for this application, we used the standard image processor, which resized on the shortest edge, center cropped, and normalized. Language-image pretraining took 13 hours and the supervised training on labeled image-text pairs took 18 hours on a single NVIDIA GeForce RTX 3090. Thus, it took a total of 21 hours to train english multimodal clippings.
Hyperparameters and other training details are listed in Table </span><a href="#A2.T1" title="Table S-1 ‚Ä£ Appendix B Additional Results ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">S-1</span></a><span id="A1.SS2.p1.1.9" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A1.SS2.p2" class="ltx_para">
<p id="A1.SS2.p2.1" class="ltx_p"><span id="A1.SS2.p2.1.1" class="ltx_text" style="font-size:90%;">To cluster the embeddings, we use Single Linkage Clustering. The distance threshold was tuned on the validation set jointly with the weight put on the image/text embedding to create the pooled embedding.</span></p>
</div>
</section>
<section id="A1.SS3" class="ltx_subsection">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">A.3 </span>Vision Transformer</h3>

<div id="A1.SS3.p1" class="ltx_para">
<p id="A1.SS3.p1.1" class="ltx_p"><span id="A1.SS3.p1.1.1" class="ltx_text" style="font-size:90%;">We initialized the weights of the Vision Transformer from the DINO-pretrained checkpoint for ViT/B16 </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A1.SS3.p1.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib12" title="" class="ltx_ref">12</a><span id="A1.SS3.p1.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A1.SS3.p1.1.4" class="ltx_text" style="font-size:90%;">.
Hyperparameters and other training details are listed in Table </span><a href="#A2.T1" title="Table S-1 ‚Ä£ Appendix B Additional Results ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">S-1</span></a><span id="A1.SS3.p1.1.5" class="ltx_text" style="font-size:90%;">.</span></p>
</div>
<div id="A1.SS3.p2" class="ltx_para">
<p id="A1.SS3.p2.1" class="ltx_p"><span id="A1.SS3.p2.1.1" class="ltx_text" style="font-size:90%;">As for </span><span id="A1.SS3.p2.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="A1.SS3.p2.1.3" class="ltx_text" style="font-size:90%;">, ViT training employed synthetic data. The same pipeline as above was used to generate synthetically noised images. For each word in a list of Japanese words, the text was rendered using different fonts and augmented to create synthetically noised views.</span></p>
</div>
<div id="A1.SS3.p3" class="ltx_para">
<p id="A1.SS3.p3.4" class="ltx_p"><span id="A1.SS3.p3.4.1" class="ltx_text" style="font-size:90%;">Offline hard-negative mining was used to train the ViT. The approach is similar to that described above. We used a pretrained checkpoint to find </span><math id="A1.SS3.p3.1.m1.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A1.SS3.p3.1.m1.1a"><mi mathsize="90%" id="A1.SS3.p3.1.m1.1.1" xref="A1.SS3.p3.1.m1.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.p3.1.m1.1b"><ci id="A1.SS3.p3.1.m1.1.1.cmml" xref="A1.SS3.p3.1.m1.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p3.1.m1.1c">k</annotation></semantics></math><span id="A1.SS3.p3.4.2" class="ltx_text" style="font-size:90%;"> nearest neighbors for each class. This was used to create a batch containing </span><math id="A1.SS3.p3.2.m2.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A1.SS3.p3.2.m2.1a"><mi mathsize="90%" id="A1.SS3.p3.2.m2.1.1" xref="A1.SS3.p3.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.p3.2.m2.1b"><ci id="A1.SS3.p3.2.m2.1.1.cmml" xref="A1.SS3.p3.2.m2.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p3.2.m2.1c">m</annotation></semantics></math><span id="A1.SS3.p3.4.3" class="ltx_text" style="font-size:90%;"> views of that class, along with </span><math id="A1.SS3.p3.3.m3.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A1.SS3.p3.3.m3.1a"><mi mathsize="90%" id="A1.SS3.p3.3.m3.1.1" xref="A1.SS3.p3.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A1.SS3.p3.3.m3.1b"><ci id="A1.SS3.p3.3.m3.1.1.cmml" xref="A1.SS3.p3.3.m3.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p3.3.m3.1c">m</annotation></semantics></math><span id="A1.SS3.p3.4.4" class="ltx_text" style="font-size:90%;"> views of </span><math id="A1.SS3.p3.4.m4.1" class="ltx_Math" alttext="\dfrac{B}{m}-1" display="inline"><semantics id="A1.SS3.p3.4.m4.1a"><mrow id="A1.SS3.p3.4.m4.1.1" xref="A1.SS3.p3.4.m4.1.1.cmml"><mstyle displaystyle="true" id="A1.SS3.p3.4.m4.1.1.2" xref="A1.SS3.p3.4.m4.1.1.2.cmml"><mfrac id="A1.SS3.p3.4.m4.1.1.2a" xref="A1.SS3.p3.4.m4.1.1.2.cmml"><mi mathsize="90%" id="A1.SS3.p3.4.m4.1.1.2.2" xref="A1.SS3.p3.4.m4.1.1.2.2.cmml">B</mi><mi mathsize="90%" id="A1.SS3.p3.4.m4.1.1.2.3" xref="A1.SS3.p3.4.m4.1.1.2.3.cmml">m</mi></mfrac></mstyle><mo mathsize="90%" id="A1.SS3.p3.4.m4.1.1.1" xref="A1.SS3.p3.4.m4.1.1.1.cmml">‚àí</mo><mn mathsize="90%" id="A1.SS3.p3.4.m4.1.1.3" xref="A1.SS3.p3.4.m4.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A1.SS3.p3.4.m4.1b"><apply id="A1.SS3.p3.4.m4.1.1.cmml" xref="A1.SS3.p3.4.m4.1.1"><minus id="A1.SS3.p3.4.m4.1.1.1.cmml" xref="A1.SS3.p3.4.m4.1.1.1"></minus><apply id="A1.SS3.p3.4.m4.1.1.2.cmml" xref="A1.SS3.p3.4.m4.1.1.2"><divide id="A1.SS3.p3.4.m4.1.1.2.1.cmml" xref="A1.SS3.p3.4.m4.1.1.2"></divide><ci id="A1.SS3.p3.4.m4.1.1.2.2.cmml" xref="A1.SS3.p3.4.m4.1.1.2.2">ùêµ</ci><ci id="A1.SS3.p3.4.m4.1.1.2.3.cmml" xref="A1.SS3.p3.4.m4.1.1.2.3">ùëö</ci></apply><cn type="integer" id="A1.SS3.p3.4.m4.1.1.3.cmml" xref="A1.SS3.p3.4.m4.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS3.p3.4.m4.1c">\dfrac{B}{m}-1</annotation></semantics></math><span id="A1.SS3.p3.4.5" class="ltx_text" style="font-size:90%;"> other classes. When using hard negatives, we substituted ‚Äùk-1‚Äù of these other classes with the nearest neighbor classes of the anchor.</span></p>
</div>
</section>
</section>
<section id="A2" class="ltx_appendix">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Additional Results</h2>

<div id="A2.p1" class="ltx_para">
<p id="A2.p1.1" class="ltx_p"><span id="A2.p1.1.1" class="ltx_text" style="font-size:90%;">Table </span><a href="#A2.T2" title="Table S-2 ‚Ä£ Appendix B Additional Results ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">S-2</span></a><span id="A2.p1.1.2" class="ltx_text" style="font-size:90%;"> examines the contribution of different elements of the </span><span id="A2.p1.1.3" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="A2.p1.1.4" class="ltx_text" style="font-size:90%;"> training recipe.
Panel A considers the performance of Japanese CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p1.1.5.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib40" title="" class="ltx_ref">40</a><span id="A2.p1.1.6.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p1.1.7" class="ltx_text" style="font-size:90%;"> off-the-shelf. Using the vision encoder alone, every instance is mispredicted. The off-the-shelf text encoder, while better than the vision encoder, is outperformed by traditional string matching methods.</span></p>
</div>
<div id="A2.p2" class="ltx_para">
<p id="A2.p2.1" class="ltx_p"><span id="A2.p2.1.1" class="ltx_text" style="font-size:90%;">Panel B examines the performance of </span><span id="A2.p2.1.2" class="ltx_text ltx_font_typewriter" style="font-size:90%;">CLIPPINGS</span><span id="A2.p2.1.3" class="ltx_text" style="font-size:90%;"> when only supervised training is used, discarding the self-supervised training on image-OCR pairs. Performance declines significantly relative to when self-supervised language-image pre-training is employed - with accuracy similar to that of traditional string matching methods - illustrating the importance of first aligning the vision and text encoders for the document image-OCR domain.</span></p>
</div>
<div id="A2.p3" class="ltx_para">
<p id="A2.p3.1" class="ltx_p"><span id="A2.p3.1.1" class="ltx_text" style="font-size:90%;">Table </span><a href="#A2.T3" title="Table S-3 ‚Ä£ Appendix B Additional Results ‚Ä£ Linking Representations with Multimodal Contrastive Learning" class="ltx_ref" style="font-size:90%;"><span class="ltx_text ltx_ref_tag">S-3</span></a><span id="A2.p3.1.2" class="ltx_text" style="font-size:90%;"> examines the inclusion of instances without a match in the firm directory, for example, a range of government agencies. While performance declines somewhat relative to the results reported in the main text, the relative comparisons between models hold.</span></p>
</div>
<div id="A2.p4" class="ltx_para">
<p id="A2.p4.1" class="ltx_p"><span id="A2.p4.1.1" class="ltx_text" style="font-size:90%;">We also consider a similar set of ablations for detecting noisy duplicates in newspaper data.
Panel A shows that off-the-shelf CLIP </span><cite class="ltx_cite ltx_citemacro_cite"><span id="A2.p4.1.2.1" class="ltx_text" style="font-size:90%;">[</span><a href="#bib.bib1" title="" class="ltx_ref">1</a><span id="A2.p4.1.3.2" class="ltx_text" style="font-size:90%;">]</span></cite><span id="A2.p4.1.4" class="ltx_text" style="font-size:90%;"> (Panel A) does better in this case than in the record linkage case, though performance is still relatively poor and significantly worse than our self-supervised model. The model with supervised training only (Panel B) is again outperformed by the baseline that combines self-supervised language-image training with supervised training.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A2.T1" class="ltx_table">
<table id="A2.T1.19" class="ltx_tabular ltx_centering ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T1.19.1.1" class="ltx_tr">
<td id="A2.T1.19.1.1.1" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.1.1" class="ltx_text ltx_font_bold" style="font-size:90%;">Model</span></td>
<td id="A2.T1.19.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.2.1" class="ltx_text ltx_font_bold" style="font-size:90%;">lr</span></td>
<td id="A2.T1.19.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">B</span></td>
<td id="A2.T1.19.1.1.4" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.4.1" class="ltx_text ltx_font_bold" style="font-size:90%;">w_decay</span></td>
<td id="A2.T1.19.1.1.5" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.5.1" class="ltx_text ltx_font_bold" style="font-size:90%;">temp</span></td>
<td id="A2.T1.19.1.1.6" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.6.1" class="ltx_text ltx_font_bold" style="font-size:90%;">im_wt</span></td>
<td id="A2.T1.19.1.1.7" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.7.1" class="ltx_text ltx_font_bold" style="font-size:90%;">m</span></td>
<td id="A2.T1.19.1.1.8" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.8.1" class="ltx_text ltx_font_bold" style="font-size:90%;">k</span></td>
<td id="A2.T1.19.1.1.9" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.9.1" class="ltx_text ltx_font_bold" style="font-size:90%;">epochs</span></td>
<td id="A2.T1.19.1.1.10" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T1.19.1.1.10.1" class="ltx_text ltx_font_bold" style="font-size:90%;">nm_thresh</span></td>
</tr>
<tr id="A2.T1.19.2.2" class="ltx_tr">
<td id="A2.T1.19.2.2.1" class="ltx_td ltx_align_center ltx_border_t" colspan="10"><span id="A2.T1.19.2.2.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Panel A: Record Linkage Models</span></td>
</tr>
<tr id="A2.T1.19.3.3" class="ltx_tr">
<td id="A2.T1.19.3.3.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A2.T1.19.3.3.1.1" class="ltx_text" style="font-size:90%;">Language-image Pretraining</span></td>
<td id="A2.T1.19.3.3.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.3.3.2.1" class="ltx_text" style="font-size:90%;">5e-5</span></td>
<td id="A2.T1.19.3.3.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.3.3.3.1" class="ltx_text" style="font-size:90%;">153</span></td>
<td id="A2.T1.19.3.3.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.3.3.4.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="A2.T1.19.3.3.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.3.3.5.1" class="ltx_text" style="font-size:90%;">0.048</span></td>
<td id="A2.T1.19.3.3.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.3.3.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="A2.T1.19.3.3.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.3.3.7.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="A2.T1.19.3.3.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.3.3.8.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="A2.T1.19.3.3.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.3.3.9.1" class="ltx_text" style="font-size:90%;">40</span></td>
<td id="A2.T1.19.3.3.10" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.3.3.10.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="A2.T1.19.4.4" class="ltx_tr">
<td id="A2.T1.19.4.4.1" class="ltx_td ltx_align_left"><span id="A2.T1.19.4.4.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Supervised models</span></td>
<td id="A2.T1.19.4.4.2" class="ltx_td"></td>
<td id="A2.T1.19.4.4.3" class="ltx_td"></td>
<td id="A2.T1.19.4.4.4" class="ltx_td"></td>
<td id="A2.T1.19.4.4.5" class="ltx_td"></td>
<td id="A2.T1.19.4.4.6" class="ltx_td"></td>
<td id="A2.T1.19.4.4.7" class="ltx_td"></td>
<td id="A2.T1.19.4.4.8" class="ltx_td"></td>
<td id="A2.T1.19.4.4.9" class="ltx_td"></td>
<td id="A2.T1.19.4.4.10" class="ltx_td"></td>
</tr>
<tr id="A2.T1.19.5.5" class="ltx_tr">
<td id="A2.T1.19.5.5.1" class="ltx_td ltx_align_left"><span id="A2.T1.19.5.5.1.1" class="ltx_text" style="font-size:90%;">ViT (synthetic)</span></td>
<td id="A2.T1.19.5.5.2" class="ltx_td ltx_align_center"><span id="A2.T1.19.5.5.2.1" class="ltx_text" style="font-size:90%;">5.8e-5</span></td>
<td id="A2.T1.19.5.5.3" class="ltx_td ltx_align_center"><span id="A2.T1.19.5.5.3.1" class="ltx_text" style="font-size:90%;">256</span></td>
<td id="A2.T1.19.5.5.4" class="ltx_td ltx_align_center"><span id="A2.T1.19.5.5.4.1" class="ltx_text" style="font-size:90%;">0.0398</span></td>
<td id="A2.T1.19.5.5.5" class="ltx_td ltx_align_center"><span id="A2.T1.19.5.5.5.1" class="ltx_text" style="font-size:90%;">0.048</span></td>
<td id="A2.T1.19.5.5.6" class="ltx_td ltx_align_center"><span id="A2.T1.19.5.5.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="A2.T1.19.5.5.7" class="ltx_td ltx_align_center"><span id="A2.T1.19.5.5.7.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="A2.T1.19.5.5.8" class="ltx_td ltx_align_center"><span id="A2.T1.19.5.5.8.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="A2.T1.19.5.5.9" class="ltx_td ltx_align_center"><span id="A2.T1.19.5.5.9.1" class="ltx_text" style="font-size:90%;">5</span></td>
<td id="A2.T1.19.5.5.10" class="ltx_td ltx_align_center"><span id="A2.T1.19.5.5.10.1" class="ltx_text" style="font-size:90%;">-</span></td>
</tr>
<tr id="A2.T1.19.6.6" class="ltx_tr">
<td id="A2.T1.19.6.6.1" class="ltx_td ltx_align_left"><span id="A2.T1.19.6.6.1.1" class="ltx_text" style="font-size:90%;">ViT (labelled)</span></td>
<td id="A2.T1.19.6.6.2" class="ltx_td ltx_align_center"><span id="A2.T1.19.6.6.2.1" class="ltx_text" style="font-size:90%;">2e-6</span></td>
<td id="A2.T1.19.6.6.3" class="ltx_td ltx_align_center"><span id="A2.T1.19.6.6.3.1" class="ltx_text" style="font-size:90%;">252</span></td>
<td id="A2.T1.19.6.6.4" class="ltx_td ltx_align_center"><span id="A2.T1.19.6.6.4.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="A2.T1.19.6.6.5" class="ltx_td ltx_align_center"><span id="A2.T1.19.6.6.5.1" class="ltx_text" style="font-size:90%;">0.09</span></td>
<td id="A2.T1.19.6.6.6" class="ltx_td ltx_align_center"><span id="A2.T1.19.6.6.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="A2.T1.19.6.6.7" class="ltx_td ltx_align_center"><span id="A2.T1.19.6.6.7.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.6.6.8" class="ltx_td ltx_align_center"><span id="A2.T1.19.6.6.8.1" class="ltx_text" style="font-size:90%;">8</span></td>
<td id="A2.T1.19.6.6.9" class="ltx_td ltx_align_center"><span id="A2.T1.19.6.6.9.1" class="ltx_text" style="font-size:90%;">10</span></td>
<td id="A2.T1.19.6.6.10" class="ltx_td ltx_align_center"><span id="A2.T1.19.6.6.10.1" class="ltx_text" style="font-size:90%;">0.88</span></td>
</tr>
<tr id="A2.T1.19.7.7" class="ltx_tr">
<td id="A2.T1.19.7.7.1" class="ltx_td ltx_align_left"><span id="A2.T1.19.7.7.1.1" class="ltx_text" style="font-size:90%;">Sup. Lang.-only (synthetic)</span></td>
<td id="A2.T1.19.7.7.2" class="ltx_td ltx_align_center"><span id="A2.T1.19.7.7.2.1" class="ltx_text" style="font-size:90%;">5e-6</span></td>
<td id="A2.T1.19.7.7.3" class="ltx_td ltx_align_center"><span id="A2.T1.19.7.7.3.1" class="ltx_text" style="font-size:90%;">153</span></td>
<td id="A2.T1.19.7.7.4" class="ltx_td ltx_align_center"><span id="A2.T1.19.7.7.4.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="A2.T1.19.7.7.5" class="ltx_td ltx_align_center"><span id="A2.T1.19.7.7.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="A2.T1.19.7.7.6" class="ltx_td ltx_align_center"><span id="A2.T1.19.7.7.6.1" class="ltx_text" style="font-size:90%;">0</span></td>
<td id="A2.T1.19.7.7.7" class="ltx_td ltx_align_center"><span id="A2.T1.19.7.7.7.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.7.7.8" class="ltx_td ltx_align_center"><span id="A2.T1.19.7.7.8.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.7.7.9" class="ltx_td ltx_align_center"><span id="A2.T1.19.7.7.9.1" class="ltx_text" style="font-size:90%;">30</span></td>
<td id="A2.T1.19.7.7.10" class="ltx_td ltx_align_center"><span id="A2.T1.19.7.7.10.1" class="ltx_text" style="font-size:90%;">0.85, 0.85</span></td>
</tr>
<tr id="A2.T1.19.8.8" class="ltx_tr">
<td id="A2.T1.19.8.8.1" class="ltx_td ltx_align_left"><span id="A2.T1.19.8.8.1.1" class="ltx_text" style="font-size:90%;">Sup. Image-only (synthetic)</span></td>
<td id="A2.T1.19.8.8.2" class="ltx_td ltx_align_center"><span id="A2.T1.19.8.8.2.1" class="ltx_text" style="font-size:90%;">5e-6</span></td>
<td id="A2.T1.19.8.8.3" class="ltx_td ltx_align_center"><span id="A2.T1.19.8.8.3.1" class="ltx_text" style="font-size:90%;">153</span></td>
<td id="A2.T1.19.8.8.4" class="ltx_td ltx_align_center"><span id="A2.T1.19.8.8.4.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="A2.T1.19.8.8.5" class="ltx_td ltx_align_center"><span id="A2.T1.19.8.8.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="A2.T1.19.8.8.6" class="ltx_td ltx_align_center"><span id="A2.T1.19.8.8.6.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="A2.T1.19.8.8.7" class="ltx_td ltx_align_center"><span id="A2.T1.19.8.8.7.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.8.8.8" class="ltx_td ltx_align_center"><span id="A2.T1.19.8.8.8.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.8.8.9" class="ltx_td ltx_align_center"><span id="A2.T1.19.8.8.9.1" class="ltx_text" style="font-size:90%;">30</span></td>
<td id="A2.T1.19.8.8.10" class="ltx_td ltx_align_center"><span id="A2.T1.19.8.8.10.1" class="ltx_text" style="font-size:90%;">0.76</span></td>
</tr>
<tr id="A2.T1.19.9.9" class="ltx_tr">
<td id="A2.T1.19.9.9.1" class="ltx_td ltx_align_left"><span id="A2.T1.19.9.9.1.1" class="ltx_text" style="font-size:90%;">Sup. Mean-pool (synthetic)</span></td>
<td id="A2.T1.19.9.9.2" class="ltx_td ltx_align_center"><span id="A2.T1.19.9.9.2.1" class="ltx_text" style="font-size:90%;">5e-6</span></td>
<td id="A2.T1.19.9.9.3" class="ltx_td ltx_align_center"><span id="A2.T1.19.9.9.3.1" class="ltx_text" style="font-size:90%;">153</span></td>
<td id="A2.T1.19.9.9.4" class="ltx_td ltx_align_center"><span id="A2.T1.19.9.9.4.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="A2.T1.19.9.9.5" class="ltx_td ltx_align_center"><span id="A2.T1.19.9.9.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="A2.T1.19.9.9.6" class="ltx_td ltx_align_center"><span id="A2.T1.19.9.9.6.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="A2.T1.19.9.9.7" class="ltx_td ltx_align_center"><span id="A2.T1.19.9.9.7.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.9.9.8" class="ltx_td ltx_align_center"><span id="A2.T1.19.9.9.8.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.9.9.9" class="ltx_td ltx_align_center"><span id="A2.T1.19.9.9.9.1" class="ltx_text" style="font-size:90%;">30</span></td>
<td id="A2.T1.19.9.9.10" class="ltx_td ltx_align_center"><span id="A2.T1.19.9.9.10.1" class="ltx_text" style="font-size:90%;">0.81, 0.80</span></td>
</tr>
<tr id="A2.T1.19.10.10" class="ltx_tr">
<td id="A2.T1.19.10.10.1" class="ltx_td ltx_align_left"><span id="A2.T1.19.10.10.1.1" class="ltx_text" style="font-size:90%;">Sup. Lang-only (labelled)</span></td>
<td id="A2.T1.19.10.10.2" class="ltx_td ltx_align_center"><span id="A2.T1.19.10.10.2.1" class="ltx_text" style="font-size:90%;">5e-6</span></td>
<td id="A2.T1.19.10.10.3" class="ltx_td ltx_align_center"><span id="A2.T1.19.10.10.3.1" class="ltx_text" style="font-size:90%;">153</span></td>
<td id="A2.T1.19.10.10.4" class="ltx_td ltx_align_center"><span id="A2.T1.19.10.10.4.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="A2.T1.19.10.10.5" class="ltx_td ltx_align_center"><span id="A2.T1.19.10.10.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="A2.T1.19.10.10.6" class="ltx_td ltx_align_center"><span id="A2.T1.19.10.10.6.1" class="ltx_text" style="font-size:90%;">0</span></td>
<td id="A2.T1.19.10.10.7" class="ltx_td ltx_align_center"><span id="A2.T1.19.10.10.7.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.10.10.8" class="ltx_td ltx_align_center"><span id="A2.T1.19.10.10.8.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.10.10.9" class="ltx_td ltx_align_center"><span id="A2.T1.19.10.10.9.1" class="ltx_text" style="font-size:90%;">30</span></td>
<td id="A2.T1.19.10.10.10" class="ltx_td ltx_align_center"><span id="A2.T1.19.10.10.10.1" class="ltx_text" style="font-size:90%;">0.84,0.82</span></td>
</tr>
<tr id="A2.T1.19.11.11" class="ltx_tr">
<td id="A2.T1.19.11.11.1" class="ltx_td ltx_align_left"><span id="A2.T1.19.11.11.1.1" class="ltx_text" style="font-size:90%;">Sup. Image-only (labelled)</span></td>
<td id="A2.T1.19.11.11.2" class="ltx_td ltx_align_center"><span id="A2.T1.19.11.11.2.1" class="ltx_text" style="font-size:90%;">5e-6</span></td>
<td id="A2.T1.19.11.11.3" class="ltx_td ltx_align_center"><span id="A2.T1.19.11.11.3.1" class="ltx_text" style="font-size:90%;">153</span></td>
<td id="A2.T1.19.11.11.4" class="ltx_td ltx_align_center"><span id="A2.T1.19.11.11.4.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="A2.T1.19.11.11.5" class="ltx_td ltx_align_center"><span id="A2.T1.19.11.11.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="A2.T1.19.11.11.6" class="ltx_td ltx_align_center"><span id="A2.T1.19.11.11.6.1" class="ltx_text" style="font-size:90%;">1</span></td>
<td id="A2.T1.19.11.11.7" class="ltx_td ltx_align_center"><span id="A2.T1.19.11.11.7.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.11.11.8" class="ltx_td ltx_align_center"><span id="A2.T1.19.11.11.8.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.11.11.9" class="ltx_td ltx_align_center"><span id="A2.T1.19.11.11.9.1" class="ltx_text" style="font-size:90%;">30</span></td>
<td id="A2.T1.19.11.11.10" class="ltx_td ltx_align_center"><span id="A2.T1.19.11.11.10.1" class="ltx_text" style="font-size:90%;">0.79</span></td>
</tr>
<tr id="A2.T1.19.12.12" class="ltx_tr">
<td id="A2.T1.19.12.12.1" class="ltx_td ltx_align_left"><span id="A2.T1.19.12.12.1.1" class="ltx_text" style="font-size:90%;">Sup. Mean-pooling (labelled)</span></td>
<td id="A2.T1.19.12.12.2" class="ltx_td ltx_align_center"><span id="A2.T1.19.12.12.2.1" class="ltx_text" style="font-size:90%;">5e-6</span></td>
<td id="A2.T1.19.12.12.3" class="ltx_td ltx_align_center"><span id="A2.T1.19.12.12.3.1" class="ltx_text" style="font-size:90%;">153</span></td>
<td id="A2.T1.19.12.12.4" class="ltx_td ltx_align_center"><span id="A2.T1.19.12.12.4.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="A2.T1.19.12.12.5" class="ltx_td ltx_align_center"><span id="A2.T1.19.12.12.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="A2.T1.19.12.12.6" class="ltx_td ltx_align_center"><span id="A2.T1.19.12.12.6.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="A2.T1.19.12.12.7" class="ltx_td ltx_align_center"><span id="A2.T1.19.12.12.7.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.12.12.8" class="ltx_td ltx_align_center"><span id="A2.T1.19.12.12.8.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.12.12.9" class="ltx_td ltx_align_center"><span id="A2.T1.19.12.12.9.1" class="ltx_text" style="font-size:90%;">30</span></td>
<td id="A2.T1.19.12.12.10" class="ltx_td ltx_align_center"><span id="A2.T1.19.12.12.10.1" class="ltx_text" style="font-size:90%;">0.82,0.82</span></td>
</tr>
<tr id="A2.T1.19.13.13" class="ltx_tr">
<td id="A2.T1.19.13.13.1" class="ltx_td ltx_align_center ltx_border_t" colspan="10"><span id="A2.T1.19.13.13.1.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Panel B: Newspaper Image-Caption Models</span></td>
</tr>
<tr id="A2.T1.19.14.14" class="ltx_tr">
<td id="A2.T1.19.14.14.1" class="ltx_td ltx_align_left ltx_border_t"><span id="A2.T1.19.14.14.1.1" class="ltx_text" style="font-size:90%;">Language-image pretraining</span></td>
<td id="A2.T1.19.14.14.2" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.14.14.2.1" class="ltx_text" style="font-size:90%;">5e-6</span></td>
<td id="A2.T1.19.14.14.3" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.14.14.3.1" class="ltx_text" style="font-size:90%;">153</span></td>
<td id="A2.T1.19.14.14.4" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.14.14.4.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="A2.T1.19.14.14.5" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.14.14.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="A2.T1.19.14.14.6" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.14.14.6.1" class="ltx_text" style="font-size:90%;">-</span></td>
<td id="A2.T1.19.14.14.7" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.14.14.7.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.14.14.8" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.14.14.8.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.14.14.9" class="ltx_td ltx_align_center ltx_border_t"><span id="A2.T1.19.14.14.9.1" class="ltx_text" style="font-size:90%;">9</span></td>
<td id="A2.T1.19.14.14.10" class="ltx_td ltx_border_t"></td>
</tr>
<tr id="A2.T1.19.15.15" class="ltx_tr">
<td id="A2.T1.19.15.15.1" class="ltx_td ltx_align_left ltx_border_bb"><span id="A2.T1.19.15.15.1.1" class="ltx_text" style="font-size:90%;">Sup. mean pooling (labelled)</span></td>
<td id="A2.T1.19.15.15.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T1.19.15.15.2.1" class="ltx_text" style="font-size:90%;">5e-9</span></td>
<td id="A2.T1.19.15.15.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T1.19.15.15.3.1" class="ltx_text" style="font-size:90%;">153</span></td>
<td id="A2.T1.19.15.15.4" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T1.19.15.15.4.1" class="ltx_text" style="font-size:90%;">0.001</span></td>
<td id="A2.T1.19.15.15.5" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T1.19.15.15.5.1" class="ltx_text" style="font-size:90%;">0.1</span></td>
<td id="A2.T1.19.15.15.6" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T1.19.15.15.6.1" class="ltx_text" style="font-size:90%;">0.5</span></td>
<td id="A2.T1.19.15.15.7" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T1.19.15.15.7.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.15.15.8" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T1.19.15.15.8.1" class="ltx_text" style="font-size:90%;">3</span></td>
<td id="A2.T1.19.15.15.9" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T1.19.15.15.9.1" class="ltx_text" style="font-size:90%;">228</span></td>
<td id="A2.T1.19.15.15.10" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T1.19.15.15.10.1" class="ltx_text" style="font-size:90%;">0.227</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table S-1: </span>Training Hyperparameters: <math id="A2.T1.10.m1.1" class="ltx_Math" alttext="lr" display="inline"><semantics id="A2.T1.10.m1.1b"><mrow id="A2.T1.10.m1.1.1" xref="A2.T1.10.m1.1.1.cmml"><mi id="A2.T1.10.m1.1.1.2" xref="A2.T1.10.m1.1.1.2.cmml">l</mi><mo lspace="0em" rspace="0em" id="A2.T1.10.m1.1.1.1" xref="A2.T1.10.m1.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.10.m1.1.1.3" xref="A2.T1.10.m1.1.1.3.cmml">r</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T1.10.m1.1c"><apply id="A2.T1.10.m1.1.1.cmml" xref="A2.T1.10.m1.1.1"><times id="A2.T1.10.m1.1.1.1.cmml" xref="A2.T1.10.m1.1.1.1"></times><ci id="A2.T1.10.m1.1.1.2.cmml" xref="A2.T1.10.m1.1.1.2">ùëô</ci><ci id="A2.T1.10.m1.1.1.3.cmml" xref="A2.T1.10.m1.1.1.3">ùëü</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.10.m1.1d">lr</annotation></semantics></math> is the maximum learning rate, <math id="A2.T1.11.m2.1" class="ltx_Math" alttext="B" display="inline"><semantics id="A2.T1.11.m2.1b"><mi id="A2.T1.11.m2.1.1" xref="A2.T1.11.m2.1.1.cmml">B</mi><annotation-xml encoding="MathML-Content" id="A2.T1.11.m2.1c"><ci id="A2.T1.11.m2.1.1.cmml" xref="A2.T1.11.m2.1.1">ùêµ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.11.m2.1d">B</annotation></semantics></math> is the batch size, <math id="A2.T1.12.m3.1" class="ltx_Math" alttext="w\_decay" display="inline"><semantics id="A2.T1.12.m3.1b"><mrow id="A2.T1.12.m3.1.1" xref="A2.T1.12.m3.1.1.cmml"><mi id="A2.T1.12.m3.1.1.2" xref="A2.T1.12.m3.1.1.2.cmml">w</mi><mo lspace="0em" rspace="0em" id="A2.T1.12.m3.1.1.1" xref="A2.T1.12.m3.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="A2.T1.12.m3.1.1.3" xref="A2.T1.12.m3.1.1.3.cmml">_</mi><mo lspace="0em" rspace="0em" id="A2.T1.12.m3.1.1.1b" xref="A2.T1.12.m3.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.12.m3.1.1.4" xref="A2.T1.12.m3.1.1.4.cmml">d</mi><mo lspace="0em" rspace="0em" id="A2.T1.12.m3.1.1.1c" xref="A2.T1.12.m3.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.12.m3.1.1.5" xref="A2.T1.12.m3.1.1.5.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.T1.12.m3.1.1.1d" xref="A2.T1.12.m3.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.12.m3.1.1.6" xref="A2.T1.12.m3.1.1.6.cmml">c</mi><mo lspace="0em" rspace="0em" id="A2.T1.12.m3.1.1.1e" xref="A2.T1.12.m3.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.12.m3.1.1.7" xref="A2.T1.12.m3.1.1.7.cmml">a</mi><mo lspace="0em" rspace="0em" id="A2.T1.12.m3.1.1.1f" xref="A2.T1.12.m3.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.12.m3.1.1.8" xref="A2.T1.12.m3.1.1.8.cmml">y</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T1.12.m3.1c"><apply id="A2.T1.12.m3.1.1.cmml" xref="A2.T1.12.m3.1.1"><times id="A2.T1.12.m3.1.1.1.cmml" xref="A2.T1.12.m3.1.1.1"></times><ci id="A2.T1.12.m3.1.1.2.cmml" xref="A2.T1.12.m3.1.1.2">ùë§</ci><ci id="A2.T1.12.m3.1.1.3.cmml" xref="A2.T1.12.m3.1.1.3">_</ci><ci id="A2.T1.12.m3.1.1.4.cmml" xref="A2.T1.12.m3.1.1.4">ùëë</ci><ci id="A2.T1.12.m3.1.1.5.cmml" xref="A2.T1.12.m3.1.1.5">ùëí</ci><ci id="A2.T1.12.m3.1.1.6.cmml" xref="A2.T1.12.m3.1.1.6">ùëê</ci><ci id="A2.T1.12.m3.1.1.7.cmml" xref="A2.T1.12.m3.1.1.7">ùëé</ci><ci id="A2.T1.12.m3.1.1.8.cmml" xref="A2.T1.12.m3.1.1.8">ùë¶</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.12.m3.1d">w\_decay</annotation></semantics></math> is the AdamW weight decay, <math id="A2.T1.13.m4.1" class="ltx_Math" alttext="im\_wt" display="inline"><semantics id="A2.T1.13.m4.1b"><mrow id="A2.T1.13.m4.1.1" xref="A2.T1.13.m4.1.1.cmml"><mi id="A2.T1.13.m4.1.1.2" xref="A2.T1.13.m4.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.T1.13.m4.1.1.1" xref="A2.T1.13.m4.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.13.m4.1.1.3" xref="A2.T1.13.m4.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="A2.T1.13.m4.1.1.1b" xref="A2.T1.13.m4.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="A2.T1.13.m4.1.1.4" xref="A2.T1.13.m4.1.1.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="A2.T1.13.m4.1.1.1c" xref="A2.T1.13.m4.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.13.m4.1.1.5" xref="A2.T1.13.m4.1.1.5.cmml">w</mi><mo lspace="0em" rspace="0em" id="A2.T1.13.m4.1.1.1d" xref="A2.T1.13.m4.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.13.m4.1.1.6" xref="A2.T1.13.m4.1.1.6.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T1.13.m4.1c"><apply id="A2.T1.13.m4.1.1.cmml" xref="A2.T1.13.m4.1.1"><times id="A2.T1.13.m4.1.1.1.cmml" xref="A2.T1.13.m4.1.1.1"></times><ci id="A2.T1.13.m4.1.1.2.cmml" xref="A2.T1.13.m4.1.1.2">ùëñ</ci><ci id="A2.T1.13.m4.1.1.3.cmml" xref="A2.T1.13.m4.1.1.3">ùëö</ci><ci id="A2.T1.13.m4.1.1.4.cmml" xref="A2.T1.13.m4.1.1.4">_</ci><ci id="A2.T1.13.m4.1.1.5.cmml" xref="A2.T1.13.m4.1.1.5">ùë§</ci><ci id="A2.T1.13.m4.1.1.6.cmml" xref="A2.T1.13.m4.1.1.6">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.13.m4.1d">im\_wt</annotation></semantics></math> is the weight of the image embedding in the pooled embedding, <math id="A2.T1.14.m5.1" class="ltx_Math" alttext="m" display="inline"><semantics id="A2.T1.14.m5.1b"><mi id="A2.T1.14.m5.1.1" xref="A2.T1.14.m5.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A2.T1.14.m5.1c"><ci id="A2.T1.14.m5.1.1.cmml" xref="A2.T1.14.m5.1.1">ùëö</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.14.m5.1d">m</annotation></semantics></math> is the number of views sampled in each epoch, <math id="A2.T1.15.m6.1" class="ltx_Math" alttext="k" display="inline"><semantics id="A2.T1.15.m6.1b"><mi id="A2.T1.15.m6.1.1" xref="A2.T1.15.m6.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="A2.T1.15.m6.1c"><ci id="A2.T1.15.m6.1.1.cmml" xref="A2.T1.15.m6.1.1">ùëò</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.15.m6.1d">k</annotation></semantics></math> is the number of nearest neighbours in a hard-negative set, and epochs is the number of epochs. For Panel A, <math id="A2.T1.16.m7.1" class="ltx_Math" alttext="nm\_threshold" display="inline"><semantics id="A2.T1.16.m7.1b"><mrow id="A2.T1.16.m7.1.1" xref="A2.T1.16.m7.1.1.cmml"><mi id="A2.T1.16.m7.1.1.2" xref="A2.T1.16.m7.1.1.2.cmml">n</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.3" xref="A2.T1.16.m7.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1b" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="A2.T1.16.m7.1.1.4" xref="A2.T1.16.m7.1.1.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1c" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.5" xref="A2.T1.16.m7.1.1.5.cmml">t</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1d" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.6" xref="A2.T1.16.m7.1.1.6.cmml">h</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1e" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.7" xref="A2.T1.16.m7.1.1.7.cmml">r</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1f" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.8" xref="A2.T1.16.m7.1.1.8.cmml">e</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1g" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.9" xref="A2.T1.16.m7.1.1.9.cmml">s</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1h" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.10" xref="A2.T1.16.m7.1.1.10.cmml">h</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1i" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.11" xref="A2.T1.16.m7.1.1.11.cmml">o</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1j" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.12" xref="A2.T1.16.m7.1.1.12.cmml">l</mi><mo lspace="0em" rspace="0em" id="A2.T1.16.m7.1.1.1k" xref="A2.T1.16.m7.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.16.m7.1.1.13" xref="A2.T1.16.m7.1.1.13.cmml">d</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T1.16.m7.1c"><apply id="A2.T1.16.m7.1.1.cmml" xref="A2.T1.16.m7.1.1"><times id="A2.T1.16.m7.1.1.1.cmml" xref="A2.T1.16.m7.1.1.1"></times><ci id="A2.T1.16.m7.1.1.2.cmml" xref="A2.T1.16.m7.1.1.2">ùëõ</ci><ci id="A2.T1.16.m7.1.1.3.cmml" xref="A2.T1.16.m7.1.1.3">ùëö</ci><ci id="A2.T1.16.m7.1.1.4.cmml" xref="A2.T1.16.m7.1.1.4">_</ci><ci id="A2.T1.16.m7.1.1.5.cmml" xref="A2.T1.16.m7.1.1.5">ùë°</ci><ci id="A2.T1.16.m7.1.1.6.cmml" xref="A2.T1.16.m7.1.1.6">‚Ñé</ci><ci id="A2.T1.16.m7.1.1.7.cmml" xref="A2.T1.16.m7.1.1.7">ùëü</ci><ci id="A2.T1.16.m7.1.1.8.cmml" xref="A2.T1.16.m7.1.1.8">ùëí</ci><ci id="A2.T1.16.m7.1.1.9.cmml" xref="A2.T1.16.m7.1.1.9">ùë†</ci><ci id="A2.T1.16.m7.1.1.10.cmml" xref="A2.T1.16.m7.1.1.10">‚Ñé</ci><ci id="A2.T1.16.m7.1.1.11.cmml" xref="A2.T1.16.m7.1.1.11">ùëú</ci><ci id="A2.T1.16.m7.1.1.12.cmml" xref="A2.T1.16.m7.1.1.12">ùëô</ci><ci id="A2.T1.16.m7.1.1.13.cmml" xref="A2.T1.16.m7.1.1.13">ùëë</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.16.m7.1d">nm\_threshold</annotation></semantics></math> is a tuple with two tuned similarity thresholds (for noisy and clean OCR respectively) under which a retrieved neighbor is considered to not match with any of the target images. For Panel B, it corresponds to the <span id="A2.T1.33.1" class="ltx_text ltx_font_italic">distance</span> threshold used in single linkage clustering. Note that for Panel B, <math id="A2.T1.17.m8.1" class="ltx_Math" alttext="im\_wt" display="inline"><semantics id="A2.T1.17.m8.1b"><mrow id="A2.T1.17.m8.1.1" xref="A2.T1.17.m8.1.1.cmml"><mi id="A2.T1.17.m8.1.1.2" xref="A2.T1.17.m8.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.T1.17.m8.1.1.1" xref="A2.T1.17.m8.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.17.m8.1.1.3" xref="A2.T1.17.m8.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="A2.T1.17.m8.1.1.1b" xref="A2.T1.17.m8.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="A2.T1.17.m8.1.1.4" xref="A2.T1.17.m8.1.1.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="A2.T1.17.m8.1.1.1c" xref="A2.T1.17.m8.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.17.m8.1.1.5" xref="A2.T1.17.m8.1.1.5.cmml">w</mi><mo lspace="0em" rspace="0em" id="A2.T1.17.m8.1.1.1d" xref="A2.T1.17.m8.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.17.m8.1.1.6" xref="A2.T1.17.m8.1.1.6.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T1.17.m8.1c"><apply id="A2.T1.17.m8.1.1.cmml" xref="A2.T1.17.m8.1.1"><times id="A2.T1.17.m8.1.1.1.cmml" xref="A2.T1.17.m8.1.1.1"></times><ci id="A2.T1.17.m8.1.1.2.cmml" xref="A2.T1.17.m8.1.1.2">ùëñ</ci><ci id="A2.T1.17.m8.1.1.3.cmml" xref="A2.T1.17.m8.1.1.3">ùëö</ci><ci id="A2.T1.17.m8.1.1.4.cmml" xref="A2.T1.17.m8.1.1.4">_</ci><ci id="A2.T1.17.m8.1.1.5.cmml" xref="A2.T1.17.m8.1.1.5">ùë§</ci><ci id="A2.T1.17.m8.1.1.6.cmml" xref="A2.T1.17.m8.1.1.6">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.17.m8.1d">im\_wt</annotation></semantics></math> at inference time does not have to be the same as the <math id="A2.T1.18.m9.1" class="ltx_Math" alttext="im\_wt" display="inline"><semantics id="A2.T1.18.m9.1b"><mrow id="A2.T1.18.m9.1.1" xref="A2.T1.18.m9.1.1.cmml"><mi id="A2.T1.18.m9.1.1.2" xref="A2.T1.18.m9.1.1.2.cmml">i</mi><mo lspace="0em" rspace="0em" id="A2.T1.18.m9.1.1.1" xref="A2.T1.18.m9.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.18.m9.1.1.3" xref="A2.T1.18.m9.1.1.3.cmml">m</mi><mo lspace="0em" rspace="0em" id="A2.T1.18.m9.1.1.1b" xref="A2.T1.18.m9.1.1.1.cmml">‚Äã</mo><mi mathvariant="normal" id="A2.T1.18.m9.1.1.4" xref="A2.T1.18.m9.1.1.4.cmml">_</mi><mo lspace="0em" rspace="0em" id="A2.T1.18.m9.1.1.1c" xref="A2.T1.18.m9.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.18.m9.1.1.5" xref="A2.T1.18.m9.1.1.5.cmml">w</mi><mo lspace="0em" rspace="0em" id="A2.T1.18.m9.1.1.1d" xref="A2.T1.18.m9.1.1.1.cmml">‚Äã</mo><mi id="A2.T1.18.m9.1.1.6" xref="A2.T1.18.m9.1.1.6.cmml">t</mi></mrow><annotation-xml encoding="MathML-Content" id="A2.T1.18.m9.1c"><apply id="A2.T1.18.m9.1.1.cmml" xref="A2.T1.18.m9.1.1"><times id="A2.T1.18.m9.1.1.1.cmml" xref="A2.T1.18.m9.1.1.1"></times><ci id="A2.T1.18.m9.1.1.2.cmml" xref="A2.T1.18.m9.1.1.2">ùëñ</ci><ci id="A2.T1.18.m9.1.1.3.cmml" xref="A2.T1.18.m9.1.1.3">ùëö</ci><ci id="A2.T1.18.m9.1.1.4.cmml" xref="A2.T1.18.m9.1.1.4">_</ci><ci id="A2.T1.18.m9.1.1.5.cmml" xref="A2.T1.18.m9.1.1.5">ùë§</ci><ci id="A2.T1.18.m9.1.1.6.cmml" xref="A2.T1.18.m9.1.1.6">ùë°</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.T1.18.m9.1d">im\_wt</annotation></semantics></math> specified during multimodal training.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A2.T2" class="ltx_table">
<table id="A2.T2.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T2.1.1.1" class="ltx_tr">
<th id="A2.T2.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="A2.T2.1.1.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T2.1.1.1.2.1" class="ltx_text" style="font-size:90%;">Noisy OCR</span></td>
<td id="A2.T2.1.1.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T2.1.1.1.3.1" class="ltx_text" style="font-size:90%;">Clean OCR</span></td>
</tr>
<tr id="A2.T2.1.2.2" class="ltx_tr">
<th id="A2.T2.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="3"><span id="A2.T2.1.2.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel A: Zero-shot Japanese-CLIP</span></th>
</tr>
<tr id="A2.T2.1.3.3" class="ltx_tr">
<th id="A2.T2.1.3.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T2.1.3.3.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="A2.T2.1.3.3.2" class="ltx_td ltx_align_center"><span id="A2.T2.1.3.3.2.1" class="ltx_text" style="font-size:90%;">0.000</span></td>
<td id="A2.T2.1.3.3.3" class="ltx_td ltx_align_center"><span id="A2.T2.1.3.3.3.1" class="ltx_text" style="font-size:90%;">0.000</span></td>
</tr>
<tr id="A2.T2.1.4.4" class="ltx_tr">
<th id="A2.T2.1.4.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T2.1.4.4.1.1" class="ltx_text" style="font-size:90%;">Language Linking</span></th>
<td id="A2.T2.1.4.4.2" class="ltx_td ltx_align_center"><span id="A2.T2.1.4.4.2.1" class="ltx_text" style="font-size:90%;">0.639</span></td>
<td id="A2.T2.1.4.4.3" class="ltx_td ltx_align_center"><span id="A2.T2.1.4.4.3.1" class="ltx_text" style="font-size:90%;">0.626</span></td>
</tr>
<tr id="A2.T2.1.5.5" class="ltx_tr">
<th id="A2.T2.1.5.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T2.1.5.5.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="A2.T2.1.5.5.2" class="ltx_td ltx_align_center"><span id="A2.T2.1.5.5.2.1" class="ltx_text" style="font-size:90%;">0.491</span></td>
<td id="A2.T2.1.5.5.3" class="ltx_td ltx_align_center"><span id="A2.T2.1.5.5.3.1" class="ltx_text" style="font-size:90%;">0.433</span></td>
</tr>
<tr id="A2.T2.1.6.6" class="ltx_tr">
<th id="A2.T2.1.6.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="A2.T2.1.6.6.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel B: Only Supervised Training</span></th>
</tr>
<tr id="A2.T2.1.7.7" class="ltx_tr">
<th id="A2.T2.1.7.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="A2.T2.1.7.7.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="A2.T2.1.7.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T2.1.7.7.2.1" class="ltx_text" style="font-size:90%;">0.676</span></td>
<td id="A2.T2.1.7.7.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T2.1.7.7.3.1" class="ltx_text" style="font-size:90%;">0.731</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table S-2: </span>Record Linkage Ablations: This table reports accuracy for linking Japanese firms from supply chain records to a large firm directory. <span id="A2.T2.8.1" class="ltx_text ltx_font_italic">Noisy OCR</span> uses off-the-shelf Google Cloud Vision for OCR, and <span id="A2.T2.9.2" class="ltx_text ltx_font_italic">Clean OCR</span> uses an accurate custom-trained OCR. Panel A uses Japanese CLIP off-the-shelf, and Panel B uses only supervised training, without further self-supervised pre-training.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A2.T3" class="ltx_table">
<table id="A2.T3.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<tbody class="ltx_tbody">
<tr id="A2.T3.1.2.1" class="ltx_tr">
<th id="A2.T3.1.2.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<td id="A2.T3.1.2.1.2" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T3.1.2.1.2.1" class="ltx_text" style="font-size:90%;">Noisy OCR</span></td>
<td id="A2.T3.1.2.1.3" class="ltx_td ltx_align_center ltx_border_tt"><span id="A2.T3.1.2.1.3.1" class="ltx_text" style="font-size:90%;">Clean OCR</span></td>
</tr>
<tr id="A2.T3.1.3.2" class="ltx_tr">
<th id="A2.T3.1.3.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="3"><span id="A2.T3.1.3.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel A: String-Matching</span></th>
</tr>
<tr id="A2.T3.1.4.3" class="ltx_tr">
<th id="A2.T3.1.4.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T3.1.4.3.1.1" class="ltx_text" style="font-size:90%;">Levenshtein distance</span></th>
<td id="A2.T3.1.4.3.2" class="ltx_td ltx_align_center"><span id="A2.T3.1.4.3.2.1" class="ltx_text" style="font-size:90%;">0.605</span></td>
<td id="A2.T3.1.4.3.3" class="ltx_td ltx_align_center"><span id="A2.T3.1.4.3.3.1" class="ltx_text" style="font-size:90%;">0.625</span></td>
</tr>
<tr id="A2.T3.1.1" class="ltx_tr">
<th id="A2.T3.1.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row">
<span id="A2.T3.1.1.1.1" class="ltx_text" style="font-size:90%;">Stroke </span><math id="A2.T3.1.1.1.m1.1" class="ltx_Math" alttext="n" display="inline"><semantics id="A2.T3.1.1.1.m1.1a"><mi mathsize="90%" id="A2.T3.1.1.1.m1.1.1" xref="A2.T3.1.1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A2.T3.1.1.1.m1.1b"><ci id="A2.T3.1.1.1.m1.1.1.cmml" xref="A2.T3.1.1.1.m1.1.1">ùëõ</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T3.1.1.1.m1.1c">n</annotation></semantics></math><span id="A2.T3.1.1.1.2" class="ltx_text" style="font-size:90%;">-gram similarity</span>
</th>
<td id="A2.T3.1.1.2" class="ltx_td ltx_align_center"><span id="A2.T3.1.1.2.1" class="ltx_text" style="font-size:90%;">0.625</span></td>
<td id="A2.T3.1.1.3" class="ltx_td ltx_align_center"><span id="A2.T3.1.1.3.1" class="ltx_text" style="font-size:90%;">0.650</span></td>
</tr>
<tr id="A2.T3.1.5.4" class="ltx_tr">
<th id="A2.T3.1.5.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="A2.T3.1.5.4.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel B: Language-Image Self-Supervised Training</span></th>
</tr>
<tr id="A2.T3.1.6.5" class="ltx_tr">
<th id="A2.T3.1.6.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T3.1.6.5.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="A2.T3.1.6.5.2" class="ltx_td ltx_align_center"><span id="A2.T3.1.6.5.2.1" class="ltx_text" style="font-size:90%;">0.693</span></td>
<td id="A2.T3.1.6.5.3" class="ltx_td ltx_align_center"><span id="A2.T3.1.6.5.3.1" class="ltx_text" style="font-size:90%;">0.693</span></td>
</tr>
<tr id="A2.T3.1.7.6" class="ltx_tr">
<th id="A2.T3.1.7.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T3.1.7.6.1.1" class="ltx_text" style="font-size:90%;">Language Linking</span></th>
<td id="A2.T3.1.7.6.2" class="ltx_td ltx_align_center"><span id="A2.T3.1.7.6.2.1" class="ltx_text" style="font-size:90%;">0.680</span></td>
<td id="A2.T3.1.7.6.3" class="ltx_td ltx_align_center"><span id="A2.T3.1.7.6.3.1" class="ltx_text" style="font-size:90%;">0.741</span></td>
</tr>
<tr id="A2.T3.1.8.7" class="ltx_tr">
<th id="A2.T3.1.8.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T3.1.8.7.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="A2.T3.1.8.7.2" class="ltx_td ltx_align_center"><span id="A2.T3.1.8.7.2.1" class="ltx_text" style="font-size:90%;">0.770</span></td>
<td id="A2.T3.1.8.7.3" class="ltx_td ltx_align_center"><span id="A2.T3.1.8.7.3.1" class="ltx_text" style="font-size:90%;">0.770</span></td>
</tr>
<tr id="A2.T3.1.9.8" class="ltx_tr">
<th id="A2.T3.1.9.8.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="A2.T3.1.9.8.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel C: Supervised Training on Linked Data</span></th>
</tr>
<tr id="A2.T3.1.10.9" class="ltx_tr">
<th id="A2.T3.1.10.9.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="A2.T3.1.10.9.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">with Vision Pre-training</span></th>
</tr>
<tr id="A2.T3.1.11.10" class="ltx_tr">
<th id="A2.T3.1.11.10.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T3.1.11.10.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="A2.T3.1.11.10.2" class="ltx_td ltx_align_center"><span id="A2.T3.1.11.10.2.1" class="ltx_text" style="font-size:90%;">0.819</span></td>
<td id="A2.T3.1.11.10.3" class="ltx_td ltx_align_center"><span id="A2.T3.1.11.10.3.1" class="ltx_text" style="font-size:90%;">0.819</span></td>
</tr>
<tr id="A2.T3.1.12.11" class="ltx_tr">
<th id="A2.T3.1.12.11.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="A2.T3.1.12.11.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel D: Supervised Training on Linked Data</span></th>
</tr>
<tr id="A2.T3.1.13.12" class="ltx_tr">
<th id="A2.T3.1.13.12.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="3"><span id="A2.T3.1.13.12.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">with Language-Image Pre-training</span></th>
</tr>
<tr id="A2.T3.1.14.13" class="ltx_tr">
<th id="A2.T3.1.14.13.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T3.1.14.13.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="A2.T3.1.14.13.2" class="ltx_td ltx_align_center"><span id="A2.T3.1.14.13.2.1" class="ltx_text" style="font-size:90%;">0.829</span></td>
<td id="A2.T3.1.14.13.3" class="ltx_td ltx_align_center"><span id="A2.T3.1.14.13.3.1" class="ltx_text" style="font-size:90%;">0.829</span></td>
</tr>
<tr id="A2.T3.1.15.14" class="ltx_tr">
<th id="A2.T3.1.15.14.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T3.1.15.14.1.1" class="ltx_text" style="font-size:90%;">Language Linking</span></th>
<td id="A2.T3.1.15.14.2" class="ltx_td ltx_align_center"><span id="A2.T3.1.15.14.2.1" class="ltx_text" style="font-size:90%;">0.757</span></td>
<td id="A2.T3.1.15.14.3" class="ltx_td ltx_align_center"><span id="A2.T3.1.15.14.3.1" class="ltx_text" style="font-size:90%;">0.825</span></td>
</tr>
<tr id="A2.T3.1.16.15" class="ltx_tr">
<th id="A2.T3.1.16.15.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="A2.T3.1.16.15.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="A2.T3.1.16.15.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T3.1.16.15.2.1" class="ltx_text" style="font-size:90%;">0.845</span></td>
<td id="A2.T3.1.16.15.3" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T3.1.16.15.3.1" class="ltx_text ltx_font_bold" style="font-size:90%;">0.871</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table S-3: </span>Including instances without a match: This table reports accuracy on the test set using a variety of different methods for linking Japanese firms from supply chain records to a large firm directory. <span id="A2.T3.8.1" class="ltx_text ltx_font_italic">Noisy OCR</span> uses off-the-shelf Google Cloud Vision for OCR, and <span id="A2.T3.9.2" class="ltx_text ltx_font_italic">Clean OCR</span> uses an accurate custom-trained OCR.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
<figure id="A2.T4" class="ltx_table">
<table id="A2.T4.1" class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle">
<thead class="ltx_thead">
<tr id="A2.T4.1.1.1" class="ltx_tr">
<th id="A2.T4.1.1.1.1" class="ltx_td ltx_th ltx_th_row ltx_border_tt"></th>
<th id="A2.T4.1.1.1.2" class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt"><span id="A2.T4.1.1.1.2.1" class="ltx_text" style="font-size:90%;">ARI</span></th>
</tr>
<tr id="A2.T4.1.2.2" class="ltx_tr">
<th id="A2.T4.1.2.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" colspan="2"><span id="A2.T4.1.2.2.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel A: Zero-shot CLIP</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr id="A2.T4.1.3.1" class="ltx_tr">
<th id="A2.T4.1.3.1.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T4.1.3.1.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="A2.T4.1.3.1.2" class="ltx_td ltx_align_center"><span id="A2.T4.1.3.1.2.1" class="ltx_text" style="font-size:90%;">0.182</span></td>
</tr>
<tr id="A2.T4.1.4.2" class="ltx_tr">
<th id="A2.T4.1.4.2.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T4.1.4.2.1.1" class="ltx_text" style="font-size:90%;">Language Linking</span></th>
<td id="A2.T4.1.4.2.2" class="ltx_td ltx_align_center"><span id="A2.T4.1.4.2.2.1" class="ltx_text" style="font-size:90%;">0.291</span></td>
</tr>
<tr id="A2.T4.1.5.3" class="ltx_tr">
<th id="A2.T4.1.5.3.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T4.1.5.3.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="A2.T4.1.5.3.2" class="ltx_td ltx_align_center"><span id="A2.T4.1.5.3.2.1" class="ltx_text" style="font-size:90%;">0.314</span></td>
</tr>
<tr id="A2.T4.1.6.4" class="ltx_tr">
<th id="A2.T4.1.6.4.1" class="ltx_td ltx_align_left ltx_th ltx_th_row" colspan="2"><span id="A2.T4.1.6.4.1.1" class="ltx_text ltx_font_bold ltx_font_italic" style="font-size:90%;">Panel B: Only Supervised Training</span></th>
</tr>
<tr id="A2.T4.1.7.5" class="ltx_tr">
<th id="A2.T4.1.7.5.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T4.1.7.5.1.1" class="ltx_text" style="font-size:90%;">Visual Linking</span></th>
<td id="A2.T4.1.7.5.2" class="ltx_td ltx_align_center"><span id="A2.T4.1.7.5.2.1" class="ltx_text" style="font-size:90%;">0.200</span></td>
</tr>
<tr id="A2.T4.1.8.6" class="ltx_tr">
<th id="A2.T4.1.8.6.1" class="ltx_td ltx_align_left ltx_th ltx_th_row"><span id="A2.T4.1.8.6.1.1" class="ltx_text" style="font-size:90%;">Language Linking</span></th>
<td id="A2.T4.1.8.6.2" class="ltx_td ltx_align_center"><span id="A2.T4.1.8.6.2.1" class="ltx_text" style="font-size:90%;">0.303</span></td>
</tr>
<tr id="A2.T4.1.9.7" class="ltx_tr">
<th id="A2.T4.1.9.7.1" class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb"><span id="A2.T4.1.9.7.1.1" class="ltx_text" style="font-size:90%;">Multimodal Linking</span></th>
<td id="A2.T4.1.9.7.2" class="ltx_td ltx_align_center ltx_border_bb"><span id="A2.T4.1.9.7.2.1" class="ltx_text" style="font-size:90%;">0.559</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table S-4: </span>Noisy Duplicate Ablations: This table reports the adjusted rand index for detecting noisy duplicated image-caption pairs in a historical newspaper corpus, using CLIP off-the-shelf (Panel A) and using only supervised training, with no language-image pre-training (Panel B).</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section id="bib" class="ltx_bibliography">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>

<ul class="ltx_biblist">
<li id="bib.bib1" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span id="bib.bib1.1.1" class="ltx_text" style="font-size:90%;">
Alec , Jong¬†Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini
Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.2.1" class="ltx_text" style="font-size:90%;">Learning transferable visual models from natural language
supervision.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib1.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib1.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">International conference on machine learning</span><span id="bib.bib1.5.3" class="ltx_text" style="font-size:90%;">, pages
8748‚Äì8763. PMLR, 2021.
</span>
</span>
</li>
<li id="bib.bib2" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span id="bib.bib2.1.1" class="ltx_text" style="font-size:90%;">
Ran Abramitzky, Leah Boustan, Katherine Eriksson, James Feigenbaum, and
Santiago P√©rez.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.2.1" class="ltx_text" style="font-size:90%;">Automated linking of historical data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib2.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Economic Literature</span><span id="bib.bib2.4.2" class="ltx_text" style="font-size:90%;">, 59(3):865‚Äì918, 2021.
</span>
</span>
</li>
<li id="bib.bib3" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span id="bib.bib3.1.1" class="ltx_text" style="font-size:90%;">
Daron Acemoglu, Ufuk Akcigit, and William Kerr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.2.1" class="ltx_text" style="font-size:90%;">Networks and the macroeconomy: An empirical exploration.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib3.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nber macroeconomics annual</span><span id="bib.bib3.4.2" class="ltx_text" style="font-size:90%;">, 30(1):273‚Äì335, 2016.
</span>
</span>
</li>
<li id="bib.bib4" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span id="bib.bib4.1.1" class="ltx_text" style="font-size:90%;">
Daron Acemoglu, Vasco¬†M Carvalho, Asuman Ozdaglar, and Alireza Tahbaz-Salehi.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.2.1" class="ltx_text" style="font-size:90%;">The network origins of aggregate fluctuations.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib4.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Econometrica</span><span id="bib.bib4.4.2" class="ltx_text" style="font-size:90%;">, 80(5):1977‚Äì2016, 2012.
</span>
</span>
</li>
<li id="bib.bib5" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span id="bib.bib5.1.1" class="ltx_text" style="font-size:90%;">
Srikar Appalaraju, Bhavan Jasani, Bhargava¬†Urala Kota, Yusheng Xie, and R
Manmatha.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.2.1" class="ltx_text" style="font-size:90%;">Docformer: End-to-end transformer for document understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib5.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib5.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF international conference on
computer vision</span><span id="bib.bib5.5.3" class="ltx_text" style="font-size:90%;">, pages 993‚Äì1003, 2021.
</span>
</span>
</li>
<li id="bib.bib6" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span id="bib.bib6.1.1" class="ltx_text" style="font-size:90%;">
Martha¬†J Bailey, Connor Cole, Morgan Henderson, and Catherine Massey.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.2.1" class="ltx_text" style="font-size:90%;">How well do automated linking methods perform? lessons from us
historical data.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib6.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of Economic Literature</span><span id="bib.bib6.4.2" class="ltx_text" style="font-size:90%;">, 58(4):997‚Äì1044, 2020.
</span>
</span>
</li>
<li id="bib.bib7" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span id="bib.bib7.1.1" class="ltx_text" style="font-size:90%;">
Dominick Bartelme and Yuriy Gorodnichenko.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.2.1" class="ltx_text" style="font-size:90%;">Linkages and economic development.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib7.3.1" class="ltx_text" style="font-size:90%;">Technical report, National Bureau of Economic Research, 2015.
</span>
</span>
</li>
<li id="bib.bib8" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span id="bib.bib8.1.1" class="ltx_text" style="font-size:90%;">
Olivier Binette and Rebecca¬†C Steorts.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.2.1" class="ltx_text" style="font-size:90%;">(almost) all of entity resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib8.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Science Advances</span><span id="bib.bib8.4.2" class="ltx_text" style="font-size:90%;">, 8(12):eabi8021, 2022.
</span>
</span>
</li>
<li id="bib.bib9" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span id="bib.bib9.1.1" class="ltx_text" style="font-size:90%;">
Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag,
Florian Tram√®r, Borja Balle, Daphne Ippolito, and Eric Wallace.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.2.1" class="ltx_text" style="font-size:90%;">Extracting training data from diffusion models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib9.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2301.13188</span><span id="bib.bib9.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib10" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span id="bib.bib10.1.1" class="ltx_text" style="font-size:90%;">
Nicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian
Tramer, and Chiyuan Zhang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.2.1" class="ltx_text" style="font-size:90%;">Quantifying memorization across neural language models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib10.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2202.07646</span><span id="bib.bib10.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib11" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span id="bib.bib11.1.1" class="ltx_text" style="font-size:90%;">
Jacob Carlson, Tom Bryan, and Melissa Dell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.2.1" class="ltx_text" style="font-size:90%;">Efficient ocr for building a diverse digital history.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib11.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv e-prints arxiv:2304.02737</span><span id="bib.bib11.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib12" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span id="bib.bib12.1.1" class="ltx_text" style="font-size:90%;">
Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal,
Piotr Bojanowski, and Armand Joulin.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.2.1" class="ltx_text" style="font-size:90%;">Emerging properties in self-supervised vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib12.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2104.14294</span><span id="bib.bib12.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib13" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span id="bib.bib13.1.1" class="ltx_text" style="font-size:90%;">
Arun Chandrasekhar and Randall Lewis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.2.1" class="ltx_text" style="font-size:90%;">Econometrics of sampled networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib13.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Unpublished manuscript, MIT.[422]</span><span id="bib.bib13.4.2" class="ltx_text" style="font-size:90%;">, 2011.
</span>
</span>
</li>
<li id="bib.bib14" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span id="bib.bib14.1.1" class="ltx_text" style="font-size:90%;">
Ryan Cordell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.2.1" class="ltx_text" style="font-size:90%;">Reprinting, circulation, and the network author in antebellum
newspapers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib14.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">American Literary History</span><span id="bib.bib14.4.2" class="ltx_text" style="font-size:90%;">, 27(3):417‚Äì445, 2015.
</span>
</span>
</li>
<li id="bib.bib15" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span id="bib.bib15.1.1" class="ltx_text" style="font-size:90%;">
Nicola De¬†Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.2.1" class="ltx_text" style="font-size:90%;">Autoregressive entity retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib15.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.00904</span><span id="bib.bib15.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib16" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span id="bib.bib16.1.1" class="ltx_text" style="font-size:90%;">
Yongkun Du, Zhineng Chen, Caiyan Jia, Xiaoting Yin, Tianlun Zheng, Chenxia Li,
Yuning Du, and Yu-Gang Jiang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.2.1" class="ltx_text" style="font-size:90%;">Svtr: Scene text recognition with a single visual model.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib16.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.00159</span><span id="bib.bib16.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib17" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span id="bib.bib17.1.1" class="ltx_text" style="font-size:90%;">
Glenn Ellison, Edward¬†L Glaeser, and William¬†R Kerr.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.2.1" class="ltx_text" style="font-size:90%;">What causes industry agglomeration? evidence from coagglomeration
patterns.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib17.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">American Economic Review</span><span id="bib.bib17.4.2" class="ltx_text" style="font-size:90%;">, 100(3):1195‚Äì1213, 2010.
</span>
</span>
</li>
<li id="bib.bib18" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span id="bib.bib18.1.1" class="ltx_text" style="font-size:90%;">
He Guo, Xiameng Qin, Jiaming Liu, Junyu Han, Jingtuo Liu, and Errui Ding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.2.1" class="ltx_text" style="font-size:90%;">Eaten: Entity-aware attention for single shot visual text extraction.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib18.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">2019 International Conference on Document Analysis and
Recognition (ICDAR)</span><span id="bib.bib18.4.2" class="ltx_text" style="font-size:90%;">, pages 254‚Äì259, 2019.
</span>
</span>
</li>
<li id="bib.bib19" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span id="bib.bib19.1.1" class="ltx_text" style="font-size:90%;">
Kaiming He, Georgia Gkioxari, Piotr Doll√°r, and Ross Girshick.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.2.1" class="ltx_text" style="font-size:90%;">Mask r-cnn.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib19.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib19.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE international conference on computer
vision</span><span id="bib.bib19.5.3" class="ltx_text" style="font-size:90%;">, pages 2961‚Äì2969, 2017.
</span>
</span>
</li>
<li id="bib.bib20" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span id="bib.bib20.1.1" class="ltx_text" style="font-size:90%;">
Albert¬†O Hirschman.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The strategy of economic development.</span><span id="bib.bib20.3.2" class="ltx_text" style="font-size:90%;">
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib20.4.1" class="ltx_text" style="font-size:90%;">Yale Univ. Press, New Haven, Conn, 1958.
</span>
</span>
</li>
<li id="bib.bib21" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span id="bib.bib21.1.1" class="ltx_text" style="font-size:90%;">
Benjamin Hsu and Graham Horwood.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.2.1" class="ltx_text" style="font-size:90%;">Contrastive representation learning for cross-document coreference
resolution of events and entities.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib21.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2205.11438</span><span id="bib.bib21.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib22" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span id="bib.bib22.1.1" class="ltx_text" style="font-size:90%;">
Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma,
Tengchao Lv, Lei Cui, Owais¬†Khan Mohammed, Qiang Liu, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.2.1" class="ltx_text" style="font-size:90%;">Language is not all you need: Aligning perception with language
models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib22.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2302.14045</span><span id="bib.bib22.4.2" class="ltx_text" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li id="bib.bib23" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span id="bib.bib23.1.1" class="ltx_text" style="font-size:90%;">
Matthew¬†A Jaro.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.2.1" class="ltx_text" style="font-size:90%;">Advances in record-linkage methodology as applied to matching the
1985 census of tampa, florida.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib23.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Journal of the American Statistical Association</span><span id="bib.bib23.4.2" class="ltx_text" style="font-size:90%;">,
84(406):414‚Äì420, 1989.
</span>
</span>
</li>
<li id="bib.bib24" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span id="bib.bib24.1.1" class="ltx_text" style="font-size:90%;">
Abhinav Java, Shripad Deshmukh, Milan Aggarwal, Surgan Jandial, Mausoom Sarkar,
and Balaji Krishnamurthy.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.2.1" class="ltx_text" style="font-size:90%;">One-shot doc snippet detection: Powering search in document beyond
text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib24.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib24.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Winter Conference on Applications
of Computer Vision</span><span id="bib.bib24.5.3" class="ltx_text" style="font-size:90%;">, pages 5437‚Äì5446, 2023.
</span>
</span>
</li>
<li id="bib.bib25" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span id="bib.bib25.1.1" class="ltx_text" style="font-size:90%;">
Jinji K≈çÃÑshinjo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Nihon shokuinrokj</span><span id="bib.bib25.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib25.4.1" class="ltx_text" style="font-size:90%;">Jinji K≈çÃÑshinjo, 1954.
</span>
</span>
</li>
<li id="bib.bib26" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span id="bib.bib26.1.1" class="ltx_text" style="font-size:90%;">
Jeff Johnson, Matthijs Douze, and Herv√© J√©gou.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.2.1" class="ltx_text" style="font-size:90%;">Billion-scale similarity search with gpus.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib26.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">IEEE Transactions on Big Data</span><span id="bib.bib26.4.2" class="ltx_text" style="font-size:90%;">, 7(3):535‚Äì547, 2019.
</span>
</span>
</li>
<li id="bib.bib27" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span id="bib.bib27.1.1" class="ltx_text" style="font-size:90%;">
Nikhil Kandpal, Eric Wallace, and Colin Raffel.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.2.1" class="ltx_text" style="font-size:90%;">Deduplicating training data mitigates privacy risks in language
models.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib27.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2202.06539</span><span id="bib.bib27.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib28" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span id="bib.bib28.1.1" class="ltx_text" style="font-size:90%;">
Vladimir Karpukhin, Barlas Oƒüuz, Sewon Min, Patrick Lewis, Ledell Wu,
Sergey Edunov, Danqi Chen, and Wen-tau Yih.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.2.1" class="ltx_text" style="font-size:90%;">Dense passage retrieval for open-domain question answering.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib28.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2004.04906</span><span id="bib.bib28.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib29" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span id="bib.bib29.1.1" class="ltx_text" style="font-size:90%;">
kfcd.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib29.2.1" class="ltx_text" style="font-size:90%;">chaizi.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/kfcd/chaizi" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/kfcd/chaizi</a><span id="bib.bib29.3.1" class="ltx_text" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li id="bib.bib30" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span id="bib.bib30.1.1" class="ltx_text" style="font-size:90%;">
Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip
Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.2.1" class="ltx_text" style="font-size:90%;">Supervised contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib30.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2004.11362</span><span id="bib.bib30.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib31" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span id="bib.bib31.1.1" class="ltx_text" style="font-size:90%;">
Douwe Kiela, Suvrat Bhooshan, Hamed Firooz, Ethan Perez, and Davide Testuggine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.2.1" class="ltx_text" style="font-size:90%;">Supervised multimodal bitransformers for classifying images and text.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib31.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1909.02950</span><span id="bib.bib31.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib32" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span id="bib.bib32.1.1" class="ltx_text" style="font-size:90%;">
Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh,
Pratik Ringshia, and Davide Testuggine.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.2.1" class="ltx_text" style="font-size:90%;">The hateful memes challenge: Detecting hate speech in multimodal
memes.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib32.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span id="bib.bib32.4.2" class="ltx_text" style="font-size:90%;">,
33:2611‚Äì2624, 2020.
</span>
</span>
</li>
<li id="bib.bib33" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span id="bib.bib33.1.1" class="ltx_text" style="font-size:90%;">
Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong
Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.2.1" class="ltx_text" style="font-size:90%;">Ocr-free document understanding transformer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib33.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib33.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Computer Vision‚ÄìECCV 2022: 17th European Conference, Tel
Aviv, Israel, October 23‚Äì27, 2022, Proceedings, Part XXVIII</span><span id="bib.bib33.5.3" class="ltx_text" style="font-size:90%;">, pages
498‚Äì517. Springer, 2022.
</span>
</span>
</li>
<li id="bib.bib34" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span id="bib.bib34.1.1" class="ltx_text" style="font-size:90%;">
Nathan Lane.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.2.1" class="ltx_text" style="font-size:90%;">Manufacturing revolutions: Industrial policy and industrialization in
south korea.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib34.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Available at SSRN 3890311</span><span id="bib.bib34.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib35" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span id="bib.bib35.1.1" class="ltx_text" style="font-size:90%;">
Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck,
Chris Callison-Burch, and Nicholas Carlini.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.2.1" class="ltx_text" style="font-size:90%;">Deduplicating training data makes language models better.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib35.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2107.06499</span><span id="bib.bib35.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib36" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span id="bib.bib36.1.1" class="ltx_text" style="font-size:90%;">
Megan Leszczynski, Daniel¬†Y Fu, Mayee¬†F Chen, and Christopher R√©.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.2.1" class="ltx_text" style="font-size:90%;">Tabi: Type-aware bi-encoders for open-domain entity retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib36.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2204.08173</span><span id="bib.bib36.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib37" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span id="bib.bib37.1.1" class="ltx_text" style="font-size:90%;">
Vladimir¬†I Levenshtein et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.2.1" class="ltx_text" style="font-size:90%;">Binary codes capable of correcting deletions, insertions, and
reversals.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib37.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib37.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Soviet physics doklady</span><span id="bib.bib37.5.3" class="ltx_text" style="font-size:90%;">, volume¬†10, pages 707‚Äì710. Soviet
Union, 1966.
</span>
</span>
</li>
<li id="bib.bib38" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span id="bib.bib38.1.1" class="ltx_text" style="font-size:90%;">
Peizhao Li, Jiuxiang Gu, Jason Kuen, Vlad¬†I Morariu, Handong Zhao, Rajiv Jain,
Varun Manjunatha, and Hongfu Liu.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.2.1" class="ltx_text" style="font-size:90%;">Selfdoc: Self-supervised document representation learning.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib38.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib38.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib38.5.3" class="ltx_text" style="font-size:90%;">, pages 5652‚Äì5660, 2021.
</span>
</span>
</li>
<li id="bib.bib39" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span id="bib.bib39.1.1" class="ltx_text" style="font-size:90%;">
Huidong Liu, Shaoyuan Xu, Jinmiao Fu, Yang Liu, Ning Xie, Chien-Chih Wang,
Bryan Wang, and Yi Sun.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.2.1" class="ltx_text" style="font-size:90%;">Cma-clip: Cross-modality attention clip for image-text
classification.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib39.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2112.03562</span><span id="bib.bib39.4.2" class="ltx_text" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li id="bib.bib40" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span id="bib.bib40.1.1" class="ltx_text" style="font-size:90%;">
Kei¬†Sawada Makoto¬†Sheen, Tenu¬†Cho.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.2.1" class="ltx_text" style="font-size:90%;">nihongo niokeru gengo gaz≈ç jizen gakush≈´ moderu no k≈çchiku to
k≈çkai.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib40.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib40.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">The 25th Meeting on Image Recognition and Understanding</span><span id="bib.bib40.5.3" class="ltx_text" style="font-size:90%;">, 7
2022.
</span>
</span>
</li>
<li id="bib.bib41" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span id="bib.bib41.1.1" class="ltx_text" style="font-size:90%;">
Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.2.1" class="ltx_text" style="font-size:90%;">Docvqa: A dataset for vqa on document images.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib41.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF winter conference on applications of
computer vision</span><span id="bib.bib41.4.2" class="ltx_text" style="font-size:90%;">, pages 2200‚Äì2209, 2021.
</span>
</span>
</li>
<li id="bib.bib42" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span id="bib.bib42.1.1" class="ltx_text" style="font-size:90%;">
Gunnar Myrdal and Paul Sitohang.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.2.1" class="ltx_text" style="font-size:90%;">Economic theory and under-developed regions.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib42.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Regional Studies</span><span id="bib.bib42.4.2" class="ltx_text" style="font-size:90%;">, 1957.
</span>
</span>
</li>
<li id="bib.bib43" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span id="bib.bib43.1.1" class="ltx_text" style="font-size:90%;">
Paul Novosad.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib43.2.1" class="ltx_text" style="font-size:90%;">Masala merge.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/paulnov/masala-merge" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/paulnov/masala-merge</a><span id="bib.bib43.3.1" class="ltx_text" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li id="bib.bib44" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span id="bib.bib44.1.1" class="ltx_text" style="font-size:90%;">
Naoaki Okazaki and Jun‚Äôichi Tsujii.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.2.1" class="ltx_text" style="font-size:90%;">Simple and efficient algorithm for approximate dictionary matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib44.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib44.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 23rd International Conference on
Computational Linguistics (Coling 2010)</span><span id="bib.bib44.5.3" class="ltx_text" style="font-size:90%;">, pages 851‚Äì859, 2010.
</span>
</span>
</li>
<li id="bib.bib45" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span id="bib.bib45.1.1" class="ltx_text" style="font-size:90%;">
George Papadakis, Dimitrios Skoutas, Emmanouil Thanos, and Themis Palpanas.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.2.1" class="ltx_text" style="font-size:90%;">A survey of blocking and filtering techniques for entity resolution.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib45.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1905.06167</span><span id="bib.bib45.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib46" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span id="bib.bib46.1.1" class="ltx_text" style="font-size:90%;">
Seunghyun Park, Seung Shin, Bado Lee, Junyeop Lee, Jaeheung Surh, Minjoon Seo,
and Hwalsuk Lee.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.2.1" class="ltx_text" style="font-size:90%;">Cord: a consolidated receipt dataset for post-ocr parsing.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib46.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Workshop on Document Intelligence at NeurIPS 2019</span><span id="bib.bib46.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib47" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span id="bib.bib47.1.1" class="ltx_text" style="font-size:90%;">
Poul¬†N√∏rregaard Rasmussen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Studies in inter-sectoral relations</span><span id="bib.bib47.3.2" class="ltx_text" style="font-size:90%;">, volume¬†15.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib47.4.1" class="ltx_text" style="font-size:90%;">E. Harck, 1956.
</span>
</span>
</li>
<li id="bib.bib48" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span id="bib.bib48.1.1" class="ltx_text" style="font-size:90%;">
Nils Reimers and Iryna Gurevych.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.2.1" class="ltx_text" style="font-size:90%;">Sentence-bert: Sentence embeddings using siamese bert-networks.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib48.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1908.10084</span><span id="bib.bib48.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib49" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span id="bib.bib49.1.1" class="ltx_text" style="font-size:90%;">
Phillip Rust, Jonas¬†F Lotz, Emanuele Bugliarello, Elizabeth Salesky, Miryam de
Lhoneux, and Desmond Elliott.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.2.1" class="ltx_text" style="font-size:90%;">Language modelling with pixels.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib49.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2207.06991</span><span id="bib.bib49.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib50" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span id="bib.bib50.1.1" class="ltx_text" style="font-size:90%;">
Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles¬†Germain Lee, Jacob
Carlson, and Weining Li.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.2.1" class="ltx_text" style="font-size:90%;">Layoutparser: A unified toolkit for deep learning based document
image analysis.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib50.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">International Conference on Document Analysis and Recognition</span><span id="bib.bib50.4.2" class="ltx_text" style="font-size:90%;">,
12821, 2021.
</span>
</span>
</li>
<li id="bib.bib51" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span id="bib.bib51.1.1" class="ltx_text" style="font-size:90%;">
Jeffrey¬†M Silbert.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.2.1" class="ltx_text" style="font-size:90%;">The world‚Äôs first computerized criminal-justice information-sharing
system-the new york state identification and intelligence system (nysiis).
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib51.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Criminology</span><span id="bib.bib51.4.2" class="ltx_text" style="font-size:90%;">, 8:107, 1970.
</span>
</span>
</li>
<li id="bib.bib52" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span id="bib.bib52.1.1" class="ltx_text" style="font-size:90%;">
Emily Silcock, Luca D‚ÄôAmico-Wong, Jinglin Yang, and Melissa Dell.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.2.1" class="ltx_text" style="font-size:90%;">Noise-robust de-duplication at scale.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib52.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2210.04261</span><span id="bib.bib52.4.2" class="ltx_text" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li id="bib.bib53" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span id="bib.bib53.1.1" class="ltx_text" style="font-size:90%;">
David¬†A Smith, Ryan Cordell, and Abby Mullen.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.2.1" class="ltx_text" style="font-size:90%;">Computational methods for uncovering reprinted texts in antebellum
newspapers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib53.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">American Literary History</span><span id="bib.bib53.4.2" class="ltx_text" style="font-size:90%;">, 27(3):E1‚ÄìE15, 2015.
</span>
</span>
</li>
<li id="bib.bib54" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span id="bib.bib54.1.1" class="ltx_text" style="font-size:90%;">
Andreas Steiner, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob
Uszkoreit, and Lucas Beyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.2.1" class="ltx_text" style="font-size:90%;">How to train your vit? data, augmentation, and regularization in
vision transformers.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib54.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">CoRR</span><span id="bib.bib54.4.2" class="ltx_text" style="font-size:90%;">, abs/2106.10270, 2021.
</span>
</span>
</li>
<li id="bib.bib55" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span id="bib.bib55.1.1" class="ltx_text" style="font-size:90%;">
Rebecca¬†C Steorts, Samuel¬†L Ventura, Mauricio Sadinle, and Stephen¬†E Fienberg.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.2.1" class="ltx_text" style="font-size:90%;">A comparison of blocking methods for record linkage.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib55.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib55.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Privacy in Statistical Databases: UNESCO Chair in Data
Privacy, International Conference, PSD 2014, Ibiza, Spain, September 17-19,
2014. Proceedings</span><span id="bib.bib55.5.3" class="ltx_text" style="font-size:90%;">, pages 253‚Äì268. Springer, 2014.
</span>
</span>
</li>
<li id="bib.bib56" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span id="bib.bib56.1.1" class="ltx_text" style="font-size:90%;">
Teikoku K≈çshinjo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.2.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Teikoku Gink≈ç Kaisha Y≈çroku</span><span id="bib.bib56.3.2" class="ltx_text" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib56.4.1" class="ltx_text" style="font-size:90%;">Teikoku K≈çshinjo, 1957.
</span>
</span>
</li>
<li id="bib.bib57" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span id="bib.bib57.1.1" class="ltx_text" style="font-size:90%;">
M.P.J. van der Loo.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.2.1" class="ltx_text" style="font-size:90%;">The stringdist package for approximate string matching.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib57.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">The R Journal</span><span id="bib.bib57.4.2" class="ltx_text" style="font-size:90%;">, 6:111‚Äì122, 2014.
</span>
</span>
</li>
<li id="bib.bib58" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span id="bib.bib58.1.1" class="ltx_text" style="font-size:90%;">
Samuel¬†L Ventura, Rebecca Nugent, and Erica¬†RH Fuchs.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.2.1" class="ltx_text" style="font-size:90%;">Seeing the non-stars:(some) sources of bias in past disambiguation
approaches and a new public tool leveraging labeled records.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib58.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">Research Policy</span><span id="bib.bib58.4.2" class="ltx_text" style="font-size:90%;">, 44(9):1672‚Äì1701, 2015.
</span>
</span>
</li>
<li id="bib.bib59" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span id="bib.bib59.1.1" class="ltx_text" style="font-size:90%;">
Aleksi Vesanto, Filip Ginter, Hannu Salmi, Asko Nivala, and Tapio Salakoski.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.2.1" class="ltx_text" style="font-size:90%;">A system for identifying and exploring text repetition in large
historical document corpora.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib59.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib59.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the 21st Nordic Conference on Computational
Linguistics</span><span id="bib.bib59.5.3" class="ltx_text" style="font-size:90%;">, pages 330‚Äì333, 2017.
</span>
</span>
</li>
<li id="bib.bib60" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span id="bib.bib60.1.1" class="ltx_text" style="font-size:90%;">
William¬†E Winkler.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib60.2.1" class="ltx_text" style="font-size:90%;">String comparator metrics and enhanced decision rules in the
fellegi-sunter model of record linkage., 1990.
</span>
</span>
</li>
<li id="bib.bib61" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span id="bib.bib61.1.1" class="ltx_text" style="font-size:90%;">
Ledell Wu, Fabio Petroni, Martin Josifoski, Sebastian Riedel, and Luke
Zettlemoyer.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.2.1" class="ltx_text" style="font-size:90%;">Scalable zero-shot entity linking with dense entity retrieval.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib61.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:1911.03814</span><span id="bib.bib61.4.2" class="ltx_text" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li id="bib.bib62" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span id="bib.bib62.1.1" class="ltx_text" style="font-size:90%;">
Yang Xu, Yiheng Xu, Tengchao Lv, Lei Cui, Furu Wei, Guoxin Wang, Yijuan Lu,
Dinei Florencio, Cha Zhang, Wanxiang Che, et¬†al.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.2.1" class="ltx_text" style="font-size:90%;">Layoutlmv2: Multi-modal pre-training for visually-rich document
understanding.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib62.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2012.14740</span><span id="bib.bib62.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib63" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span id="bib.bib63.1.1" class="ltx_text" style="font-size:90%;">
Ikuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.2.1" class="ltx_text" style="font-size:90%;">Luke: Deep contextualized entity representations with entity-aware
self-attention.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib63.3.1" class="ltx_text ltx_font_italic" style="font-size:90%;">arXiv preprint arXiv:2010.01057</span><span id="bib.bib63.4.2" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li id="bib.bib64" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span id="bib.bib64.1.1" class="ltx_text" style="font-size:90%;">
Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and
Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.2.1" class="ltx_text" style="font-size:90%;">Unified contrastive learning in image-text-label space.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib64.3.1" class="ltx_text" style="font-size:90%;">In </span><span id="bib.bib64.4.2" class="ltx_text ltx_font_italic" style="font-size:90%;">Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition</span><span id="bib.bib64.5.3" class="ltx_text" style="font-size:90%;">, pages 19163‚Äì19173, 2022.
</span>
</span>
</li>
<li id="bib.bib65" class="ltx_bibitem">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span id="bib.bib65.1.1" class="ltx_text" style="font-size:90%;">
znwang25.
</span>
</span>
<span class="ltx_bibblock"><span id="bib.bib65.2.1" class="ltx_text" style="font-size:90%;">fuzzychinese.
</span>
</span>
<span class="ltx_bibblock"><a target="_blank" href="https://github.com/znwang25/fuzzychinese" title="" class="ltx_ref ltx_url ltx_font_typewriter" style="font-size:90%;">https://github.com/znwang25/fuzzychinese</a><span id="bib.bib65.3.1" class="ltx_text" style="font-size:90%;">, 2020.
</span>
</span>
</li>
</ul>
</section>
</article>
</div>
<div class="ar5iv-footer"><a href="/html/2304.03463" class="ar5iv-nav-button ar5iv-nav-button-prev">‚óÑ</a>
    <a class="ar5iv-home-button" href="/"><img height="40" alt="ar5iv homepage" src="/assets/ar5iv.png"></a>
    <a href="/feeling_lucky" class="ar5iv-text-button">Feeling<br>lucky?</a>
    <a href="/log/2304.03464" class="ar5iv-text-button ar5iv-severity-error">Conversion<br>report</a>
    <a class="ar5iv-text-button" target="_blank" href="https://github.com/dginev/ar5iv/issues/new?template=improve-article--arxiv-id-.md&title=Improve+article+2304.03464">Report<br>an issue</a>
    <a href="https://arxiv.org/abs/2304.03464" class="ar5iv-text-button arxiv-ui-theme">View&nbsp;original<br>on&nbsp;arXiv</a><a href="/html/2304.03465" class="ar5iv-nav-button ar5iv-nav-button-next">‚ñ∫</a>
</div><footer class="ltx_page_footer">
<a class="ar5iv-toggle-color-scheme" href="javascript:toggleColorScheme()" title="Toggle ar5iv color scheme"><span class="color-scheme-icon"></span></a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/license" target="_blank">Copyright</a>
<a class="ar5iv-footer-button" href="https://arxiv.org/help/policies/privacy_policy" target="_blank">Privacy Policy</a>

<div class="ltx_page_logo">Generated  on Thu Feb 29 15:52:22 2024 by <a target="_blank" href="http://dlmf.nist.gov/LaTeXML/" class="ltx_LaTeXML_logo"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg==" alt="Mascot Sammy"></a>
</div></footer>
</div>

    <script>
      var canMathML = typeof(MathMLElement) == "function";
      if (!canMathML) {
        var body = document.querySelector("body");
        body.firstElementChild.setAttribute('style', 'opacity: 0;');
        var loading = document.createElement("div");
        loading.setAttribute("id", "mathjax-loading-spinner");
        var message = document.createElement("div");
        message.setAttribute("id", "mathjax-loading-message");
        message.innerText = "Typesetting Equations...";
        body.prepend(loading);
        body.prepend(message);

        var el = document.createElement("script");
        el.src = "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js";
        document.querySelector("head").appendChild(el);

        window.MathJax = {
          startup: {
            pageReady: () => {
              return MathJax.startup.defaultPageReady().then(() => {
                body.removeChild(loading);
                body.removeChild(message);
                body.firstElementChild.removeAttribute('style');
              }); } } };
      }
    </script>
    <script>
    // Auxiliary function, building the preview feature when
    // an inline citation is clicked
    function clicked_cite(e) {
      e.preventDefault();
      let cite = this.closest('.ltx_cite');
      let next = cite.nextSibling;
      if (next && next.nodeType == Node.ELEMENT_NODE && next.getAttribute('class') == "ar5iv-bibitem-preview") {
        next.remove();
        return; }
      // Before adding a preview modal,
      // cleanup older previews, in case they're still open
      document.querySelectorAll('span.ar5iv-bibitem-preview').forEach(function(node) {
        node.remove();
      })

      // Create the preview
      preview = document.createElement('span');
      preview.setAttribute('class','ar5iv-bibitem-preview');
      let target = document.getElementById(this.getAttribute('href').slice(1));
      target.childNodes.forEach(function (child) {
        preview.append(child.cloneNode(true));
      });
      let close_x = document.createElement('button');
      close_x.setAttribute("aria-label","Close modal for bibliography item preview");
      close_x.textContent = "√ó";
      close_x.setAttribute('class', 'ar5iv-button-close-preview');
      close_x.setAttribute('onclick','this.parentNode.remove()');
      preview.append(close_x);
      preview.querySelectorAll('.ltx_tag_bibitem').forEach(function(node) {
        node.remove();
      });
      cite.parentNode.insertBefore(preview, cite.nextSibling);
      return;
    }
    // Global Document initialization:
    // - assign the preview feature to all inline citation links
    document.querySelectorAll(".ltx_cite .ltx_ref").forEach(function (link) {
      link.addEventListener("click", clicked_cite);
    });
    </script>
    </body>
</html>
