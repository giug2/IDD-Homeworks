<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995.</title>
<!--Generated on Fri Sep 27 12:16:28 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Cultural Heritage Digitization 3D Scene Segmentation Scene Understanding" lang="en" name="keywords"/>
<base href="/html/2409.19039v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S1" title="In Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S2" title="In Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S3" title="In Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S3.SS0.SSS0.Px1" title="In 3 Methodology ‣ Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_title">Model Training and Optimization.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S3.SS0.SSS0.Px2" title="In 3 Methodology ‣ Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_title">2D Mask Rendering.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S3.SS0.SSS0.Px3" title="In 3 Methodology ‣ Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_title">3D Extraction.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S4" title="In Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Early Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S5" title="In Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>Pattern Analysis and Computer Vision (PAVIS)
<br class="ltx_break"/>Istituto Italiano di Tecnologia (IIT),
Genoa, Italy
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id1.1" style="font-size:90%;">{name.surname}@iit.it
<br class="ltx_break"/></span></span></span></span>
<h1 class="ltx_title ltx_title_document">Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation <span class="ltx_note ltx_role_thanks" id="id1.id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">thanks: </span>This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995.</span></span></span>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mahtab Dahaghin<span class="ltx_ERROR undefined" id="id2.1.id1">\orcidlink</span>0009-0001-5559-4884
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Myrna Castillo <span class="ltx_ERROR undefined" id="id3.1.id1">\orcidlink</span>0000-0001-6814-261X
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Kourosh Riahidehkordi<span class="ltx_ERROR undefined" id="id4.1.id1">\orcidlink</span>0009-0003-2780-9942
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Matteo Toso<span class="ltx_ERROR undefined" id="id5.1.id1">\orcidlink</span>0000-0002-8990-7156
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Alessio Del Bue<span class="ltx_ERROR undefined" id="id6.1.id1">\orcidlink</span>0000-0002-2262-4872
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id7.id1">The creation of digital replicas of physical objects has valuable applications for the preservation and dissemination of tangible cultural heritage. However, existing methods are often slow, expensive, and require expert knowledge. We propose a pipeline to generate a 3D replica of a scene using only RGB images (<em class="ltx_emph ltx_font_italic" id="id7.id1.1">e.g</em>.<span class="ltx_text" id="id7.id1.2"></span> photos of a museum) and then extract a model for each item of interest (<em class="ltx_emph ltx_font_italic" id="id7.id1.3">e.g</em>.<span class="ltx_text" id="id7.id1.4"></span> pieces in the exhibit). We do this by leveraging the advancements in novel view synthesis and Gaussian Splatting, modified to enable efficient 3D segmentation. This approach does not need manual annotation, and the visual inputs can be captured using a standard smartphone, making it both affordable and easy to deploy. We provide an overview of the method and baseline evaluation of the accuracy of object segmentation. The code is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mahtaabdn.github.io/gaussian_heritage.github.io/" title="">https://mahtaabdn.github.io/gaussian_heritage.github.io/</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>Cultural Heritage Digitization 3D Scene Segmentation Scene Understanding
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Computer Vision has many interesting applications in the diffusion and preservation of Cultural Heritage (CH), including the digitization of art. Creating virtual replicas enables designing virtual museum tours <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib2" title="">2</a>]</cite>; creating catalogs for archival purposes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib4" title="">4</a>]</cite>; or aiding art restoration by reassembling digital copies before handling the potentially delicate remains <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib18" title="">18</a>]</cite>.
However, current art digitization methods are slow, expensive, and often rely on expert technicians and specialized equipment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib1" title="">1</a>]</cite>, limiting the widespread adoption of these techniques. To address these limitations, we propose a simpler framework, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_tag">1</span></a>. Using only RGB images captured by a standard smartphone, we leverage advancements in novel view synthesis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib21" title="">21</a>]</cite> to generate a photo-realistic model of the scene that also provides segmentation of the CH items of interest.
We then use open-vocabulary object detector Grounding DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib14" title="">14</a>]</cite> to identify CH items in arbitrary views and extract the corresponding components from the model. This approach is a significant step towards making CH digitization also accessible to institutions with limited resources.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">The main contributions of our work are:
<em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">i)</em> a pipeline to generate instance-aware 3D Gaussian Splatting models, enabling segmentation of the model into its components;
and <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">ii)</em> an automated reconstruction and segmentation process, packaged in an easily deployable Docker container.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="213" id="S1.F1.g1" src="extracted/5884468/Images/Figure1.png" width="509"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Given a set of images, we a) use a web interface to upload them to a local server, where they are processed to generate b) 2D instance segmentation masks and c) a sparse 3D model. Using these, we train d) a model that captures appearance and 3D segmentation of the scene, from which we can e) extract a model for each object. </span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Classical approaches model 3D scenes as point-clouds <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib20" title="">20</a>]</cite> or 3D meshes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib22" title="">22</a>]</cite>, but recent models like NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib16" title="">16</a>]</cite> use a continuous volumetric function that maps position and viewing direction to density and color, learned from a limited number of images with known poses. This enables the generation of photo-realistic images from any point of view. Alternatively, 3D Gaussian Splatting (3DGS) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib8" title="">8</a>]</cite>, replaces the continuous representation with a discrete number of 3D Gaussians with different sizes, orientations, and view-dependent colors, achieving both competitive training times and real-time rendering at higher image resolution.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">These models can be combined with the advancements in foundation models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib10" title="">10</a>]</cite>, that give access to accurate, open-vocabulary segmentation masks, to embed the segmentation masks generated by LLMs into NeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib25" title="">25</a>]</cite> and 3DGS models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib6" title="">6</a>]</cite>. This is done using well-established strategies to embed 2D masks into 3D models, <em class="ltx_emph ltx_font_italic" id="S2.p2.1.1">e.g</em>.<span class="ltx_text" id="S2.p2.1.2"></span>, by “lifting” 2D masks by back-projection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib7" title="">7</a>]</cite> or using contrastive learning between 2D and 3D features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib19" title="">19</a>]</cite>. Building on these works, we automatically generate SAM masks for each input image, using a contrastive loss to embed such information into a 3DGS model. Unlike existing methods, however, we complement the contrastive loss with other forms of supervision, such as regularization based on the spatial distance between the Gaussians; and instead of learning the visual and segmentation information separately, we jointly optimize all information stored in the 3D Gaussians.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Methodology</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our pipeline for scene modeling and segmentation is composed of three key stages: <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">i)</em> training the model, <em class="ltx_emph ltx_font_italic" id="S3.p1.1.2">ii)</em> rendering 2D masks, and <em class="ltx_emph ltx_font_italic" id="S3.p1.1.3">iii)</em> 3D extraction.</p>
</div>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h3 class="ltx_title ltx_title_paragraph">Model Training and Optimization.</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.1">Our method represents the scenes as clouds of 3D Gaussians that model geometry, appearance, and instance segmentation, using a modified 3DGS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib8" title="">8</a>]</cite> model. Each Gaussian is augmented with a 16-dimensional feature vector (<math alttext="\mathbb{R}^{16}" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.1a"><msup id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">ℝ</mi><mn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">16</mn></msup><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">superscript</csymbol><ci id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.2">ℝ</ci><cn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.1c">\mathbb{R}^{16}</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p1.1.m1.1d">blackboard_R start_POSTSUPERSCRIPT 16 end_POSTSUPERSCRIPT</annotation></semantics></math>) to encode segmentation information, extrapolated from 2D segmentation masks generated by ViT-H SAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib10" title="">10</a>]</cite>. We simultaneously optimize the segmentation feature vector and the standard 3DGS parameters using a combination of three losses:</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p2">
<ol class="ltx_enumerate" id="S3.I1">
<li class="ltx_item" id="S3.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S3.I1.i1.p1">
<p class="ltx_p" id="S3.I1.i1.p1.1">The standard 3DGS loss to optimize geometry and appearance (<math alttext="\mathcal{L}_{rendering}" class="ltx_Math" display="inline" id="S3.I1.i1.p1.1.m1.1"><semantics id="S3.I1.i1.p1.1.m1.1a"><msub id="S3.I1.i1.p1.1.m1.1.1" xref="S3.I1.i1.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i1.p1.1.m1.1.1.2" xref="S3.I1.i1.p1.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.I1.i1.p1.1.m1.1.1.3" xref="S3.I1.i1.p1.1.m1.1.1.3.cmml"><mi id="S3.I1.i1.p1.1.m1.1.1.3.2" xref="S3.I1.i1.p1.1.m1.1.1.3.2.cmml">r</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3.1" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.3" xref="S3.I1.i1.p1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3.1a" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.4" xref="S3.I1.i1.p1.1.m1.1.1.3.4.cmml">n</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3.1b" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.5" xref="S3.I1.i1.p1.1.m1.1.1.3.5.cmml">d</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3.1c" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.6" xref="S3.I1.i1.p1.1.m1.1.1.3.6.cmml">e</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3.1d" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.7" xref="S3.I1.i1.p1.1.m1.1.1.3.7.cmml">r</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3.1e" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.8" xref="S3.I1.i1.p1.1.m1.1.1.3.8.cmml">i</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3.1f" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.9" xref="S3.I1.i1.p1.1.m1.1.1.3.9.cmml">n</mi><mo id="S3.I1.i1.p1.1.m1.1.1.3.1g" xref="S3.I1.i1.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i1.p1.1.m1.1.1.3.10" xref="S3.I1.i1.p1.1.m1.1.1.3.10.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i1.p1.1.m1.1b"><apply id="S3.I1.i1.p1.1.m1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i1.p1.1.m1.1.1.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i1.p1.1.m1.1.1.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.2">ℒ</ci><apply id="S3.I1.i1.p1.1.m1.1.1.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3"><times id="S3.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.1"></times><ci id="S3.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.2">𝑟</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.4.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.4">𝑛</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.5.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.5">𝑑</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.6.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.6">𝑒</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.7.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.7">𝑟</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.8.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.8">𝑖</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.9.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.9">𝑛</ci><ci id="S3.I1.i1.p1.1.m1.1.1.3.10.cmml" xref="S3.I1.i1.p1.1.m1.1.1.3.10">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i1.p1.1.m1.1c">\mathcal{L}_{rendering}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i1.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_n italic_d italic_e italic_r italic_i italic_n italic_g end_POSTSUBSCRIPT</annotation></semantics></math>).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S3.I1.i2.p1">
<p class="ltx_p" id="S3.I1.i2.p1.1">A contrastive clustering loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib24" title="">24</a>]</cite> to align rendered features with 2D segmentation masks (<math alttext="\mathcal{L}_{CC}" class="ltx_Math" display="inline" id="S3.I1.i2.p1.1.m1.1"><semantics id="S3.I1.i2.p1.1.m1.1a"><msub id="S3.I1.i2.p1.1.m1.1.1" xref="S3.I1.i2.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i2.p1.1.m1.1.1.2" xref="S3.I1.i2.p1.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.I1.i2.p1.1.m1.1.1.3" xref="S3.I1.i2.p1.1.m1.1.1.3.cmml"><mi id="S3.I1.i2.p1.1.m1.1.1.3.2" xref="S3.I1.i2.p1.1.m1.1.1.3.2.cmml">C</mi><mo id="S3.I1.i2.p1.1.m1.1.1.3.1" xref="S3.I1.i2.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i2.p1.1.m1.1.1.3.3" xref="S3.I1.i2.p1.1.m1.1.1.3.3.cmml">C</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i2.p1.1.m1.1b"><apply id="S3.I1.i2.p1.1.m1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i2.p1.1.m1.1.1.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i2.p1.1.m1.1.1.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.2">ℒ</ci><apply id="S3.I1.i2.p1.1.m1.1.1.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3"><times id="S3.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.1"></times><ci id="S3.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.2">𝐶</ci><ci id="S3.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i2.p1.1.m1.1.1.3.3">𝐶</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i2.p1.1.m1.1c">\mathcal{L}_{CC}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i2.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_C italic_C end_POSTSUBSCRIPT</annotation></semantics></math>).</p>
</div>
</li>
<li class="ltx_item" id="S3.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S3.I1.i3.p1">
<p class="ltx_p" id="S3.I1.i3.p1.1">A spatial-similarity regularization to enforce spatial continuity of feature vectors, encouraging adjacent 3D Gaussians to have consistent segmentation features while discouraging distant ones from having similar features (<math alttext="\mathcal{L}_{reg}" class="ltx_Math" display="inline" id="S3.I1.i3.p1.1.m1.1"><semantics id="S3.I1.i3.p1.1.m1.1a"><msub id="S3.I1.i3.p1.1.m1.1.1" xref="S3.I1.i3.p1.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.I1.i3.p1.1.m1.1.1.2" xref="S3.I1.i3.p1.1.m1.1.1.2.cmml">ℒ</mi><mrow id="S3.I1.i3.p1.1.m1.1.1.3" xref="S3.I1.i3.p1.1.m1.1.1.3.cmml"><mi id="S3.I1.i3.p1.1.m1.1.1.3.2" xref="S3.I1.i3.p1.1.m1.1.1.3.2.cmml">r</mi><mo id="S3.I1.i3.p1.1.m1.1.1.3.1" xref="S3.I1.i3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.1.m1.1.1.3.3" xref="S3.I1.i3.p1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.I1.i3.p1.1.m1.1.1.3.1a" xref="S3.I1.i3.p1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.I1.i3.p1.1.m1.1.1.3.4" xref="S3.I1.i3.p1.1.m1.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.I1.i3.p1.1.m1.1b"><apply id="S3.I1.i3.p1.1.m1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.I1.i3.p1.1.m1.1.1.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.I1.i3.p1.1.m1.1.1.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.2">ℒ</ci><apply id="S3.I1.i3.p1.1.m1.1.1.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3"><times id="S3.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3.1"></times><ci id="S3.I1.i3.p1.1.m1.1.1.3.2.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3.2">𝑟</ci><ci id="S3.I1.i3.p1.1.m1.1.1.3.3.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.I1.i3.p1.1.m1.1.1.3.4.cmml" xref="S3.I1.i3.p1.1.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.I1.i3.p1.1.m1.1c">\mathcal{L}_{reg}</annotation><annotation encoding="application/x-llamapun" id="S3.I1.i3.p1.1.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT</annotation></semantics></math>).</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p3.1">The total loss is composed of rendering, clustering, and regularization terms:</p>
</div>
<div class="ltx_para" id="S3.SS0.SSS0.Px1.p4">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}=\mathcal{L}_{rendering}+\lambda_{clustering}\mathcal{L}_{CC}+%
\mathcal{L}_{reg}," class="ltx_Math" display="block" id="S3.E1.m1.1"><semantics id="S3.E1.m1.1a"><mrow id="S3.E1.m1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mrow id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.2.cmml">ℒ</mi><mo id="S3.E1.m1.1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.1.cmml">=</mo><mrow id="S3.E1.m1.1.1.1.1.3" xref="S3.E1.m1.1.1.1.1.3.cmml"><msub id="S3.E1.m1.1.1.1.1.3.2" xref="S3.E1.m1.1.1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.2.2" xref="S3.E1.m1.1.1.1.1.3.2.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.1.1.3.2.3" xref="S3.E1.m1.1.1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.2.3.2" xref="S3.E1.m1.1.1.1.1.3.2.3.2.cmml">r</mi><mo id="S3.E1.m1.1.1.1.1.3.2.3.1" xref="S3.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.2.3.3" xref="S3.E1.m1.1.1.1.1.3.2.3.3.cmml">e</mi><mo id="S3.E1.m1.1.1.1.1.3.2.3.1a" xref="S3.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.2.3.4" xref="S3.E1.m1.1.1.1.1.3.2.3.4.cmml">n</mi><mo id="S3.E1.m1.1.1.1.1.3.2.3.1b" xref="S3.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.2.3.5" xref="S3.E1.m1.1.1.1.1.3.2.3.5.cmml">d</mi><mo id="S3.E1.m1.1.1.1.1.3.2.3.1c" xref="S3.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.2.3.6" xref="S3.E1.m1.1.1.1.1.3.2.3.6.cmml">e</mi><mo id="S3.E1.m1.1.1.1.1.3.2.3.1d" xref="S3.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.2.3.7" xref="S3.E1.m1.1.1.1.1.3.2.3.7.cmml">r</mi><mo id="S3.E1.m1.1.1.1.1.3.2.3.1e" xref="S3.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.2.3.8" xref="S3.E1.m1.1.1.1.1.3.2.3.8.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.3.2.3.1f" xref="S3.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.2.3.9" xref="S3.E1.m1.1.1.1.1.3.2.3.9.cmml">n</mi><mo id="S3.E1.m1.1.1.1.1.3.2.3.1g" xref="S3.E1.m1.1.1.1.1.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.2.3.10" xref="S3.E1.m1.1.1.1.1.3.2.3.10.cmml">g</mi></mrow></msub><mo id="S3.E1.m1.1.1.1.1.3.1" xref="S3.E1.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="S3.E1.m1.1.1.1.1.3.3" xref="S3.E1.m1.1.1.1.1.3.3.cmml"><msub id="S3.E1.m1.1.1.1.1.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2.2" xref="S3.E1.m1.1.1.1.1.3.3.2.2.cmml">λ</mi><mrow id="S3.E1.m1.1.1.1.1.3.3.2.3" xref="S3.E1.m1.1.1.1.1.3.3.2.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.2" xref="S3.E1.m1.1.1.1.1.3.3.2.3.2.cmml">c</mi><mo id="S3.E1.m1.1.1.1.1.3.3.2.3.1" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.3" xref="S3.E1.m1.1.1.1.1.3.3.2.3.3.cmml">l</mi><mo id="S3.E1.m1.1.1.1.1.3.3.2.3.1a" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.4" xref="S3.E1.m1.1.1.1.1.3.3.2.3.4.cmml">u</mi><mo id="S3.E1.m1.1.1.1.1.3.3.2.3.1b" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.5" xref="S3.E1.m1.1.1.1.1.3.3.2.3.5.cmml">s</mi><mo id="S3.E1.m1.1.1.1.1.3.3.2.3.1c" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.6" xref="S3.E1.m1.1.1.1.1.3.3.2.3.6.cmml">t</mi><mo id="S3.E1.m1.1.1.1.1.3.3.2.3.1d" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.7" xref="S3.E1.m1.1.1.1.1.3.3.2.3.7.cmml">e</mi><mo id="S3.E1.m1.1.1.1.1.3.3.2.3.1e" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.8" xref="S3.E1.m1.1.1.1.1.3.3.2.3.8.cmml">r</mi><mo id="S3.E1.m1.1.1.1.1.3.3.2.3.1f" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.9" xref="S3.E1.m1.1.1.1.1.3.3.2.3.9.cmml">i</mi><mo id="S3.E1.m1.1.1.1.1.3.3.2.3.1g" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.10" xref="S3.E1.m1.1.1.1.1.3.3.2.3.10.cmml">n</mi><mo id="S3.E1.m1.1.1.1.1.3.3.2.3.1h" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.2.3.11" xref="S3.E1.m1.1.1.1.1.3.3.2.3.11.cmml">g</mi></mrow></msub><mo id="S3.E1.m1.1.1.1.1.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><msub id="S3.E1.m1.1.1.1.1.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.3.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.1.1.3.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.3.3.3.2" xref="S3.E1.m1.1.1.1.1.3.3.3.3.2.cmml">C</mi><mo id="S3.E1.m1.1.1.1.1.3.3.3.3.1" xref="S3.E1.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.3.3.3.3" xref="S3.E1.m1.1.1.1.1.3.3.3.3.3.cmml">C</mi></mrow></msub></mrow><mo id="S3.E1.m1.1.1.1.1.3.1a" xref="S3.E1.m1.1.1.1.1.3.1.cmml">+</mo><msub id="S3.E1.m1.1.1.1.1.3.4" xref="S3.E1.m1.1.1.1.1.3.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1.3.4.2" xref="S3.E1.m1.1.1.1.1.3.4.2.cmml">ℒ</mi><mrow id="S3.E1.m1.1.1.1.1.3.4.3" xref="S3.E1.m1.1.1.1.1.3.4.3.cmml"><mi id="S3.E1.m1.1.1.1.1.3.4.3.2" xref="S3.E1.m1.1.1.1.1.3.4.3.2.cmml">r</mi><mo id="S3.E1.m1.1.1.1.1.3.4.3.1" xref="S3.E1.m1.1.1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.4.3.3" xref="S3.E1.m1.1.1.1.1.3.4.3.3.cmml">e</mi><mo id="S3.E1.m1.1.1.1.1.3.4.3.1a" xref="S3.E1.m1.1.1.1.1.3.4.3.1.cmml">⁢</mo><mi id="S3.E1.m1.1.1.1.1.3.4.3.4" xref="S3.E1.m1.1.1.1.1.3.4.3.4.cmml">g</mi></mrow></msub></mrow></mrow><mo id="S3.E1.m1.1.1.1.2" xref="S3.E1.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.1b"><apply id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1"><eq id="S3.E1.m1.1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1.1"></eq><ci id="S3.E1.m1.1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.1.2">ℒ</ci><apply id="S3.E1.m1.1.1.1.1.3.cmml" xref="S3.E1.m1.1.1.1.1.3"><plus id="S3.E1.m1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.1"></plus><apply id="S3.E1.m1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.2">ℒ</ci><apply id="S3.E1.m1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3"><times id="S3.E1.m1.1.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.1"></times><ci id="S3.E1.m1.1.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.2">𝑟</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.4">𝑛</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.5.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.5">𝑑</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.6.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.6">𝑒</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.7.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.7">𝑟</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.8.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.8">𝑖</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.9.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.9">𝑛</ci><ci id="S3.E1.m1.1.1.1.1.3.2.3.10.cmml" xref="S3.E1.m1.1.1.1.1.3.2.3.10">𝑔</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3"><times id="S3.E1.m1.1.1.1.1.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.1"></times><apply id="S3.E1.m1.1.1.1.1.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.2.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.2.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.2">𝜆</ci><apply id="S3.E1.m1.1.1.1.1.3.3.2.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3"><times id="S3.E1.m1.1.1.1.1.3.3.2.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.1"></times><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.2">𝑐</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.3">𝑙</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.4">𝑢</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.5.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.5">𝑠</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.6.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.6">𝑡</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.7.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.7">𝑒</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.8.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.8">𝑟</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.9.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.9">𝑖</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.10.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.10">𝑛</ci><ci id="S3.E1.m1.1.1.1.1.3.3.2.3.11.cmml" xref="S3.E1.m1.1.1.1.1.3.3.2.3.11">𝑔</ci></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.2">ℒ</ci><apply id="S3.E1.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3"><times id="S3.E1.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3.1"></times><ci id="S3.E1.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3.2">𝐶</ci><ci id="S3.E1.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.3.3.3.3">𝐶</ci></apply></apply></apply><apply id="S3.E1.m1.1.1.1.1.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.4"><csymbol cd="ambiguous" id="S3.E1.m1.1.1.1.1.3.4.1.cmml" xref="S3.E1.m1.1.1.1.1.3.4">subscript</csymbol><ci id="S3.E1.m1.1.1.1.1.3.4.2.cmml" xref="S3.E1.m1.1.1.1.1.3.4.2">ℒ</ci><apply id="S3.E1.m1.1.1.1.1.3.4.3.cmml" xref="S3.E1.m1.1.1.1.1.3.4.3"><times id="S3.E1.m1.1.1.1.1.3.4.3.1.cmml" xref="S3.E1.m1.1.1.1.1.3.4.3.1"></times><ci id="S3.E1.m1.1.1.1.1.3.4.3.2.cmml" xref="S3.E1.m1.1.1.1.1.3.4.3.2">𝑟</ci><ci id="S3.E1.m1.1.1.1.1.3.4.3.3.cmml" xref="S3.E1.m1.1.1.1.1.3.4.3.3">𝑒</ci><ci id="S3.E1.m1.1.1.1.1.3.4.3.4.cmml" xref="S3.E1.m1.1.1.1.1.3.4.3.4">𝑔</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.1c">\mathcal{L}=\mathcal{L}_{rendering}+\lambda_{clustering}\mathcal{L}_{CC}+%
\mathcal{L}_{reg},</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.1d">caligraphic_L = caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_n italic_d italic_e italic_r italic_i italic_n italic_g end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT italic_c italic_l italic_u italic_s italic_t italic_e italic_r italic_i italic_n italic_g end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_C italic_C end_POSTSUBSCRIPT + caligraphic_L start_POSTSUBSCRIPT italic_r italic_e italic_g end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px2">
<h3 class="ltx_title ltx_title_paragraph">2D Mask Rendering.</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px2.p1.2">The trained model can be used to render both semantic and instance 2D segmentation masks. To obtain <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.2.1">instance segmentation</em>, we select an arbitrary viewpoint and splat on it the segmentation features. This results in a feature vector for each pixel, and we can assign to the same instance all pixels whose feature vectors have a cosine similarity below a given threshold <math alttext="t" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S3.SS0.SSS0.Px2.p1.1.m1.1a"><mi id="S3.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.1.m1.1b"><ci id="S3.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.1.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px2.p1.1.m1.1d">italic_t</annotation></semantics></math>, which we empirically set at <math alttext="t=0.7" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S3.SS0.SSS0.Px2.p1.2.m2.1a"><mrow id="S3.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml">t</mi><mo id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml">=</mo><mn id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml">0.7</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px2.p1.2.m2.1b"><apply id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1"><eq id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.1"></eq><ci id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.2">𝑡</ci><cn id="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3.cmml" type="float" xref="S3.SS0.SSS0.Px2.p1.2.m2.1.1.3">0.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px2.p1.2.m2.1c">t=0.7</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px2.p1.2.m2.1d">italic_t = 0.7</annotation></semantics></math>.
The <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.2.2">semantic segmentation</em> is instead obtained using text prompts. Given <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.2.3">a)</em> a text prompt related to an object in the scene, we <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.2.4">b)</em> randomly sample various viewpoints and select in each a bounding box using Grounding DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib14" title="">14</a>]</cite>. Then <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.2.5">c)</em> we use SAM to refine the bounding boxes into 2D masks and <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px2.p1.2.6">d)</em> select the best fitting instance segmentation mask.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px3">
<h3 class="ltx_title ltx_title_paragraph">3D Extraction.</h3>
<div class="ltx_para" id="S3.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px3.p1.1">To achieve 3D segmentation, we render an initial view and use GroundingDINO and SAM to generate a mask for the target object based on a user-provided text prompt. We extract a feature prompt from this mask and perform similarity-based segmentation of 3D Gaussians using cosine similarity. To improve coherence, we apply a convex hull algorithm <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib23" title="">23</a>]</cite> on the masked points. We then modify the original model by extracting the subset of Gaussians corresponding to the refined 3D mask. Finally, we save the modified Gaussian model and render the scene from multiple viewpoints. ambra marascio</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Early Results</h2>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.4.1.1" style="font-size:129%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.5.2" style="font-size:129%;">Comparison of semantic segmentation on the LERF-Mask and 3D-OVS datasets. We report average mIoU and mBIoU for each dataset (higher is better). </span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T1.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.6.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.6.1.1.1"><span class="ltx_text" id="S4.T1.6.1.1.1.1" style="font-size:70%;">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.1.1.2"><span class="ltx_text" id="S4.T1.6.1.1.2.1" style="font-size:70%;">Metric</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.1.1.3">
<span class="ltx_text" id="S4.T1.6.1.1.3.1" style="font-size:70%;">LeRF </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.6.1.1.3.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib9" title="">9</a><span class="ltx_text" id="S4.T1.6.1.1.3.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.1.1.4">
<span class="ltx_text" id="S4.T1.6.1.1.4.1" style="font-size:70%;">Gaussian Grouping </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.6.1.1.4.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib23" title="">23</a><span class="ltx_text" id="S4.T1.6.1.1.4.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.1.1.5">
<span class="ltx_text" id="S4.T1.6.1.1.5.1" style="font-size:70%;">LangSplat </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T1.6.1.1.5.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib17" title="">17</a><span class="ltx_text" id="S4.T1.6.1.1.5.3.2" style="font-size:70%;">]</span></cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.1.1.6"><span class="ltx_text" id="S4.T1.6.1.1.6.1" style="font-size:70%;">Ours</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.2.2">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.6.2.2.1" rowspan="2"><span class="ltx_text" id="S4.T1.6.2.2.1.1" style="font-size:70%;">LeRF-Mask</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.2.2.2"><span class="ltx_text" id="S4.T1.6.2.2.2.1" style="font-size:70%;">mIoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.2.2.3"><span class="ltx_text" id="S4.T1.6.2.2.3.1" style="font-size:70%;">37.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.2.2.4"><span class="ltx_text" id="S4.T1.6.2.2.4.1" style="font-size:70%;">72.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.2.2.5"><span class="ltx_text" id="S4.T1.6.2.2.5.1" style="font-size:70%;">44.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.2.2.6"><span class="ltx_text ltx_font_bold" id="S4.T1.6.2.2.6.1" style="font-size:70%;">80.3</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.3.3">
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.3.3.1"><span class="ltx_text" id="S4.T1.6.3.3.1.1" style="font-size:70%;">mBIoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.3.3.2"><span class="ltx_text" id="S4.T1.6.3.3.2.1" style="font-size:70%;">29.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.3.3.3"><span class="ltx_text" id="S4.T1.6.3.3.3.1" style="font-size:70%;">67.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.3.3.4"><span class="ltx_text" id="S4.T1.6.3.3.4.1" style="font-size:70%;">39.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.3.3.5"><span class="ltx_text ltx_font_bold" id="S4.T1.6.3.3.5.1" style="font-size:70%;">76.9</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.4.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.6.4.4.1" rowspan="2"><span class="ltx_text" id="S4.T1.6.4.4.1.1" style="font-size:70%;">3D-OVS</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.4.4.2"><span class="ltx_text" id="S4.T1.6.4.4.2.1" style="font-size:70%;">mIoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.4.4.3"><span class="ltx_text" id="S4.T1.6.4.4.3.1" style="font-size:70%;">54.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.4.4.4"><span class="ltx_text" id="S4.T1.6.4.4.4.1" style="font-size:70%;">82.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.4.4.5"><span class="ltx_text" id="S4.T1.6.4.4.5.1" style="font-size:70%;">67.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.6.4.4.6"><span class="ltx_text ltx_font_bold" id="S4.T1.6.4.4.6.1" style="font-size:70%;">87.5</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.6.5.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.6.5.5.1"><span class="ltx_text" id="S4.T1.6.5.5.1.1" style="font-size:70%;">mBIoU</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.6.5.5.2"><span class="ltx_text" id="S4.T1.6.5.5.2.1" style="font-size:70%;">n.a.</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.6.5.5.3"><span class="ltx_text" id="S4.T1.6.5.5.3.1" style="font-size:70%;">78.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.6.5.5.4"><span class="ltx_text" id="S4.T1.6.5.5.4.1" style="font-size:70%;">59.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.6.5.5.5"><span class="ltx_text ltx_font_bold" id="S4.T1.6.5.5.5.1" style="font-size:70%;">81.1</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">To support the proposed pipeline and methodology, we first explore the accuracy of the segmentation field learned by our method. We provide an evaluation benchmark by testing the method on two datasets commonly used in 3D scene segmentation with 3DGS: LERF-Mask <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib9" title="">9</a>]</cite> and 3D-OVS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib13" title="">13</a>]</cite>. We compare our results against the segmentation accuracy of LeRF <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib9" title="">9</a>]</cite>, Gaussian Grouping <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib23" title="">23</a>]</cite> and LangSplat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib17" title="">17</a>]</cite>. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S4.T1" title="Table 1 ‣ 4 Early Results ‣ Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_tag">1</span></a>, we report the average performance in terms of mean intersection over union (mIoU) and mean boundary intersection over union (mBIoU), showing how our model provides more accurate segmentations.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#S4.F2" title="Figure 2 ‣ 4 Early Results ‣ Gaussian Heritage: 3D Digitization of Cultural Heritage with Integrated Object Segmentation This project has received funding from the European Union’s Horizon research and innovation programme under grant agreement No 101079116 and No 101079995."><span class="ltx_text ltx_ref_tag">2</span></a> then provides qualitative results to showcase how our method can extract accurate models for individual objects in the scene. Using as input 150 images from the “Family” scene of the Tanks and Temples dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib11" title="">11</a>]</cite>, we train a model of the full scene and extract the Gaussians corresponding to two text prompts.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="183" id="S4.F2.g1" src="extracted/5884468/Images/fig2final.png" width="479"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S4.F2.3.2" style="font-size:90%;">Sample view extracted from the “Family” scene of the Tanks and Temples dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.19039v1#bib.bib11" title="">11</a>]</cite>, using the labels a) “man statue” and b) “mother and baby statue”.
</span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We have deployed a pipeline that can extract accurate 3D models of any object in a scene using only RGB images as input. Future work will focus on evaluating the model in 3D segmentation tasks, on-site testing in museums, and other possible target beneficiaries of this approach. The code for the pipeline is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://mahtaabdn.github.io/gaussian_heritage.github.io/" title="">https://mahtaabdn.github.io/gaussian_heritage.github.io/</a>.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Cultarm3d. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.smarttech3d.com/micron3d-color" title="">https://www.smarttech3d.com/micron3d-color</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Louvre online tours. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.louvre.fr/en/online-tours" title="">https://www.louvre.fr/en/online-tours</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Versus digital. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://verus.digital/" title="">https://verus.digital/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Beyond2022: The public record office of ireland at the four courts. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://noho.ie/beyond-2022-reconstruction/" title="">https://noho.ie/beyond-2022-reconstruction/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Caron, M., Touvron, H., Misra, I., Jégou, H., Mairal, J., Bojanowski, P., Joulin, A.: Emerging properties in self-supervised vision transformers. In: ICCV (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Cen, J., Fang, J., Yang, C., Xie, L., Zhang, X., Shen, W., Tian, Q.: Segment Any 3D Gaussians. arXiv preprint arXiv:2312.00860 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Genova, K., Yin, X., Kundu, A., Pantofaru, C., Cole, F., Sud, A., Brewington, B., Shucker, B., Funkhouser, T.A.: Learning 3d semantic segmentation with only 2d image supervision. 3DV (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Kerbl, B., Kopanas, G., Leimkühler, T., Drettakis, G.: 3D Gaussian Splatting for Real-Time Radiance Field Rendering. ACM Transactions on Graphics (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Kerr, J., Kim, C.M., Goldberg, K., Kanazawa, A., Tancik, M.: LERF: Language Embedded Radiance Fields. In: ICCV (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A.C., Lo, W.Y., Dollár, P., Girshick, R.: Segment Anything. arXiv:2304.02643 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Knapitsch, A., Park, J., Zhou, Q.Y., Koltun, V.: Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics <span class="ltx_text ltx_font_bold" id="bib.bib11.1.1">36</span>(4) (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Li, B., Weinberger, K.Q., Belongie, S., Koltun, V., Ranftl, R.: Language-driven Semantic Segmentation. In: ICLR (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Liu, K., Zhan, F., Zhang, J., Xu, M., Yu, Y., El Saddik, A., Lu, S.: Weakly Supervised 3D Open-vocabulary Segmentation. In: NeurIPS (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Liu, S., Zeng, Z., Ren, T., Li, F., Zhang, H., Yang, J., Li, C., Yang, J., Su, H., Zhu, J., et al.: Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Liu, Y., Fan, Q., Zhang, S., Dong, H., Funkhouser, T.A., Yi, L.: Contrastive multimodal fusion with tupleinfonce. ICCV (2021)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng, R.: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In: ECCV (2020)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Qin, M., Li, W., Zhou, J., Wang, H., Pfister, H.: LangSplat: 3D Language Gaussian Splatting (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
research, E.U.H.., innovation programme: Repair project. <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.repairproject.eu/" title="">https://www.repairproject.eu/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Sautier, C., Puy, G., Gidaris, S., Boulch, A., Bursuc, A., Marlet, R.: Image-to-Lidar Self-Supervised Distillation for Autonomous Driving Data. CVPR (2022)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Schönberger, J.L., Frahm, J.M.: Structure-from-motion revisited. In: Conference on Computer Vision and Pattern Recognition (CVPR) (2016)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Silva, M.C., Dahaghin, M., Toso, M., Bue, A.D.: Contrastive gaussian clustering: Weakly supervised 3d scene segmentation (2024), <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.12784" title="">https://arxiv.org/abs/2404.12784</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., Jiang, Y.G.: Pixel2mesh: Generating 3d mesh models from single rgb images. In: Proceedings of the European Conference on Computer Vision (ECCV) (September 2018)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Ye, M., Danelljan, M., Yu, F., Ke, L.: Gaussian Grouping: Segment and Edit Anything in 3D Scenes. arXiv preprint arXiv:2312.00732 (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ying, H., Yin, Y., Zhang, J., Wang, F., Yu, T., Huang, R., Fang, L.: Omniseg3d: Omniversal 3d segmentation via hierarchical contrastive learning (2023)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Zhi, S., Laidlow, T., Leutenegger, S., Davison, A.J.: In-place scene labelling and understanding with implicit scene representation. In: ICCV (2021)

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri Sep 27 12:16:28 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
