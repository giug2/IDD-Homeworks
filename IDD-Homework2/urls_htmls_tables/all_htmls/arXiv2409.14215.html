<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>@Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology</title>
<!--Generated on Sat Sep 21 18:28:17 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2409.14215v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S1" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S2" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S2.SS1" title="In 2 Related Work ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Assistive Technologies for the Blind</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S2.SS2" title="In 2 Related Work ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Generalist Vision-Language Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S2.SS3" title="In 2 Related Work ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Benchmarks for Vision-Language Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">@Bench</span>: Assistive Technology Benchmark</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3.SS1" title="In 3 @Bench: Assistive Technology Benchmark ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>User-centered Study</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3.SS2" title="In 3 @Bench: Assistive Technology Benchmark ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Assistive Tasks</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3.SS3" title="In 3 @Bench: Assistive Technology Benchmark ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Efficiency-Performance Trade-off</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S4" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span><span class="ltx_text ltx_font_smallcaps">@Model</span>: Assistive Technology Model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.SS1" title="In 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Comparison with Existing Generalist Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.SS2" title="In 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Comparison with Specialized SoTA Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.SS3" title="In 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Multi-task Training <span class="ltx_text ltx_font_italic">v.s.</span> Single-task Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.SS4" title="In 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Efficiency-Performance Trade-off</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.SS5" title="In 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.5 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S6" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S7" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A1" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Model Architecture</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A1.SS1" title="In Appendix A Model Architecture ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Formulation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A1.SS2" title="In Appendix A Model Architecture ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Tasks</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A2" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Loss Functions</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A2.SS1" title="In Appendix B Loss Functions ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Pixel-level Output Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A2.SS2" title="In Appendix B Loss Functions ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Token-level Output Loss</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A2.SS3" title="In Appendix B Loss Functions ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Multi-task Training Loss</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A3" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A3.SS1" title="In Appendix C Implementation Details ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.1 </span>Multi-task Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A3.SS2" title="In Appendix C Implementation Details ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.2 </span>Single-task Training</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A3.SS3" title="In Appendix C Implementation Details ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C.3 </span>Character-based Tokenizer with Limited Vocabulary for OCR</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A4" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>User Study</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A4.SS1" title="In Appendix D User Study ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.1 </span>Comments on Generalist Assistance Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A4.SS2" title="In Appendix D User Study ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D.2 </span>More Comments</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A5" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>More Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A5.SS1" title="In Appendix E More Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E.1 </span>Complementariness in Multitasking</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A6" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>More Visualization</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A6.SS1" title="In Appendix F More Visualization ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F.1 </span>Visualization on Test Datasets</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A6.SS2" title="In Appendix F More Visualization ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F.2 </span>Zero Shot</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A7" title="In @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>More Discussion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id13.id1">@Bench</span>: Benchmarking Vision-Language Models for Human-centered Assistive Technology</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">
Xin Jiang<sup class="ltx_sup" id="id14.11.id1"><span class="ltx_text ltx_font_italic" id="id14.11.id1.1">1,3,∗</span></sup>,
  Junwei Zheng<sup class="ltx_sup" id="id15.12.id2"><span class="ltx_text ltx_font_italic" id="id15.12.id2.1">1,∗</span></sup>,
  Ruiping Liu<sup class="ltx_sup" id="id16.13.id3">1</sup>,
  Jiahang Li<sup class="ltx_sup" id="id17.14.id4">2</sup>,
  Jiaming Zhang<sup class="ltx_sup" id="id18.15.id5"><span class="ltx_text ltx_font_italic" id="id18.15.id5.1">1,†</span></sup>,
<br class="ltx_break"/>  Sven Matthiesen<sup class="ltx_sup" id="id19.16.id6">2</sup>,
  Rainer Stiefelhagen<sup class="ltx_sup" id="id20.17.id7">1</sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id21.18.id8">1</sup>CV:HCI, Karlsruhe Institute of Technology,
  <sup class="ltx_sup" id="id22.19.id9">2</sup>IPEK, Karlsruhe Institute of Technology,
  <sup class="ltx_sup" id="id23.20.id10">3</sup>Li Auto Inc
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id24.id1">As Vision-Language Models (VLMs) advance, Assistive Technologies (ATs) for helping People with Visual Impairments (PVIs) are evolving into generalists, capable of performing multiple tasks simultaneously. However, benchmarking VLMs for ATs remains under-explored in the literature.
To bridge this gap, we first create a novel <span class="ltx_text ltx_font_bold" id="id24.id1.1">AT benchmark (<span class="ltx_text ltx_font_smallcaps" id="id24.id1.1.1">@Bench</span>)</span>. Guided by a pre-design user study with PVIs, our benchmark includes the five most crucial vision-language tasks: <em class="ltx_emph ltx_font_italic" id="id24.id1.2">Panoptic Segmentation</em>, <em class="ltx_emph ltx_font_italic" id="id24.id1.3">Depth Estimation</em>, <em class="ltx_emph ltx_font_italic" id="id24.id1.4">Optical Character Recognition (OCR)</em>, <em class="ltx_emph ltx_font_italic" id="id24.id1.5">Image Captioning</em>, and <em class="ltx_emph ltx_font_italic" id="id24.id1.6">Visual Question Answering (VQA)</em>. Additionally, we propose a novel <span class="ltx_text ltx_font_bold" id="id24.id1.7">AT model (<span class="ltx_text ltx_font_smallcaps" id="id24.id1.7.1">@Model</span>)</span> that addresses all tasks simultaneously and can be expanded to more assistive functions for helping PVIs. Our framework exhibits outstanding performance across tasks by integrating multi-modal information, and it offers PVIs a more comprehensive scene understanding. Extensive experiments prove the effectiveness and generalizability of our framework.</p>
</div>
<div class="ltx_logical-block" id="id12">
<div class="ltx_para" id="id12.p1">
<img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="id11.g1" src="x1.png" width="951"/>
</div>
<figure class="ltx_figure ltx_align_center" id="S0.F1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S0.F1.5.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S0.F1.6.2" style="font-size:90%;">
<span class="ltx_text ltx_font_bold" id="S0.F1.6.2.1">Overview of our Assistive Technology Model (<span class="ltx_text ltx_font_smallcaps" id="S0.F1.6.2.1.1" style="color:#C55A11;">@Model</span>) and Benchmark (<span class="ltx_text ltx_font_smallcaps" id="S0.F1.6.2.1.2" style="color:#C55A11;">@Bench</span>).</span>
<span class="ltx_text ltx_font_smallcaps" id="S0.F1.6.2.2">@Model</span> can perform vision-language tasks all at once, including: Panoptic Segmentation, Depth Estimation, Image Captioning, Optical Character Recognition and Visual Question Answering. All tasks of <span class="ltx_text ltx_font_smallcaps" id="S0.F1.6.2.3">@Bench</span> are selected by People with Visual Impairments (PVIs) to evaluate VLMs for AT.</span></figcaption>
</figure>
</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup class="ltx_sup" id="footnotex1.1">∗</sup>Equal contribution.</span></span></span><span class="ltx_note ltx_role_footnotetext" id="footnotex2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span><sup class="ltx_sup" id="footnotex2.1"><span class="ltx_text ltx_font_italic" id="footnotex2.1.1">†</span></sup>Corresponding author (e-mail: <span class="ltx_text ltx_font_typewriter" id="footnotex2.2">jiaming.zhang@kit.edu</span>).</span></span></span><span class="ltx_note ltx_role_footnotetext" id="footnotex3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>All codes will be made publicly available at <a class="ltx_ref ltx_href" href="https://junweizheng93.github.io/publications/ATBench/ATBench.html" title="">ATBench</a>.</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Assistive Technologies (ATs) for People with Visual Impairments (PVIs) have witnessed significant advancements in recent years where computer vision and natural language processing play an important role. Previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib46" title="">46</a>]</cite> focus on providing PVIs with specific and limited functionalities, such as navigation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib1" title="">1</a>]</cite>, obstacle avoidance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib46" title="">46</a>]</cite>, image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib11" title="">11</a>]</cite>, <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">etc.</span>
However, existing methods face challenges in efficiently processing multiple tasks simultaneously.
Specifically, existing approaches struggle to accurately interpret complex scenes, which is essential to meet the needs of PVIs. Additionally, they often provide less contextually relevant information for scene description.
Recently, the generalist Vision-Language Models (VLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib24" title="">24</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib51" title="">51</a>]</cite> have been proposed and shown great potential in multi-tasking and revolutionizing the next-generation assistive systems.
Regarding Vision-Language (VL) tasks, these models benefit from the dividend in both computer vision and natural language processing domains, facilitating collaboration between visual and language tasks for a more comprehensive understanding of the surrounding environment.
Nonetheless, benchmarking VLMs for ATs is still under-explored.
The previous Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) benchmarks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib48" title="">48</a>]</cite> have limitation in two aspects. Firstly, while the benchmarks focus on language-specific and cross-modal tasks, they seldom address pure vision-specific tasks, which are crucial in the context of visual impairments. Secondly, these benchmarks tend to prioritize general applicability over the specific needs of individuals with visual impairments.
Therefore, we raise the research question: <span class="ltx_text ltx_font_bold ltx_font_italic" id="S1.p1.1.2">Are VLMs ready for empowering assistive technology for helping People with Visual Impairments (PVIs)?</span></p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To answer this question,
we propose a new VL-based AT benchmark (<span class="ltx_text ltx_font_smallcaps" id="S1.p2.1.1">@Bench</span>) as a platform for evaluating VLMs for visually impaired assistance.
To involve the target group in shaping the benchmark, we conduct a number of questionnaires via a human-in-the-loop process with seven participants who are blind or have low vision, so as to understand the practical demands of PVIs. Based on the feedback of the user study, we introduce five tasks ranked by blind users according to the level of interest, frequency of usage, and level of importance. As presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S0.F1" title="Figure 1 ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">1</span></a>, the most assistance-related tasks include: <em class="ltx_emph ltx_font_italic" id="S1.p2.1.2">Panoptic Segmentation (PS)</em>, <em class="ltx_emph ltx_font_italic" id="S1.p2.1.3">Depth Estimation (DE)</em>, <em class="ltx_emph ltx_font_italic" id="S1.p2.1.4">Optical Character Recognition (OCR)</em>, <em class="ltx_emph ltx_font_italic" id="S1.p2.1.5">Image Captioning (IC)</em>, and <em class="ltx_emph ltx_font_italic" id="S1.p2.1.6">Visual Question Answering (VQA)</em>.
Beyond the range of tasks, performance and efficiency stand as crucial considerations in AT for PVIs. Therefore, we introduce an evaluation framework for assessing the trade-off between efficiency and performance in generalist VLMs.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">With this benchmark in place, we propose a novel AT model (<span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.1">@Model</span>) that use <span class="ltx_text ltx_font_italic" id="S1.p3.1.2">task-specific prompt</span> to combine these 5 uni-modal or cross-modal tasks and realize the paradigm of multi-task training. Thanks to this, <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.3">@Model</span> can use one suit of parameters to implement multiple tasks. It is crucial to significantly reduce the number of parameters, and it will be possible to deploy one model and one suit of weights on the portable device for PVIs.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To summarize, we present the following contributions:</p>
</div>
<div class="ltx_para" id="S1.p5">
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S1.I1.ix1.p1">
<p class="ltx_p" id="S1.I1.ix1.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.ix1.p1.1.1">Human-in-the-loop User Study.</span> As a part of PVIs-specific design, it is necessary to investigate the needs of the target group. We conduct a participatory user study for the sake of understanding the most related tasks, enabling a user-driven design.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S1.I1.ix2.p1">
<p class="ltx_p" id="S1.I1.ix2.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.ix2.p1.1.1">Vision-Language Benchmark for Assistive Technology.</span> We release a new benchmark with five representative VL tasks close to the daily life of PVIs. Other complex functionalities can be derived from these tasks, such as obstacle avoidance. We further evaluate the efficiency-performance trade-off on <span class="ltx_text ltx_font_smallcaps" id="S1.I1.ix2.p1.1.2">@Bench</span>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S1.I1.ix3.p1">
<p class="ltx_p" id="S1.I1.ix3.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.ix3.p1.1.1">Generalist Vision-Language Model for Assistive Technology.</span> We propose a new end-to-end baseline <span class="ltx_text ltx_font_smallcaps" id="S1.I1.ix3.p1.1.2">@Model</span> for addressing multiple tasks in <span class="ltx_text ltx_font_smallcaps" id="S1.I1.ix3.p1.1.3">@Bench</span> all at once. The model achieves competitive performance compared with state-of-the-art methods.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="S1.I1.ix4.p1">
<p class="ltx_p" id="S1.I1.ix4.p1.1"><span class="ltx_text ltx_font_bold" id="S1.I1.ix4.p1.1.1">One Suit of Weights for All Tasks.</span> Benefiting from multi-task training, our model is capable of concurrently executing all tasks with a unified set of weights, resulting in a significant reduction in the number of parameters and computational cost.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Assistive Technologies for the Blind</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">A common goal in ATs is to develop artificial intelligent systems via vision-language algorithms to help PVIs.
VizWiz, introduced by Bigham <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">et al.</span> in 2010 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib3" title="">3</a>]</cite>, presents the first multi-task datasets and artificial intelligence challenges originating from PVIs. These include over 10 tasks, such as VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib10" title="">10</a>]</cite>, image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib11" title="">11</a>]</cite>, object detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib33" title="">33</a>]</cite> and object classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib2" title="">2</a>]</cite>, <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">etc.</span>
It covers various scenes in the daily life of PVIs and provides valuable data for the research of ATs.
Other task-specific datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib47" title="">47</a>]</cite> focus on the recognition of obstacles and tactile paving, further contributing to this field.
Based on the PVIs-oriented and general datasets, most work used visual model to address the daily challenges encountered by PVIs.
For example, previous works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib9" title="">9</a>]</cite> employ visual tasks such as detection, segmentation and depth estimation to accomplish avoidance, navigation, privacy protection, <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.3">etc.</span>
Some other works have focused on language-modal or cross-modal tasks, such as OCR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib30" title="">30</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib25" title="">25</a>]</cite>, image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib11" title="">11</a>]</cite>, and VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib10" title="">10</a>]</cite>. Compared to these existing methods, we conduct a human-in-the-loop study to design a unified multi-modal benchmark for evaluating VLMs for ATs.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Generalist Vision-Language Models</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Generalist VLMs have witnessed remarkable advancements, driven by breakthroughs in deep learning techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib51" title="">51</a>]</cite>.
Developing a generalist model for multiple tasks poses unique challenges due to the heterogeneous inputs and outputs, including RGB images, depth maps,
binary masks, bounding boxes, language, etc.
Previous methods MetaLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib12" title="">12</a>]</cite> and PaLI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib5" title="">5</a>]</cite> use language models as general-purpose interfaces to various foundation models.
GLIPv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib45" title="">45</a>]</cite> unifies both localization and VL understanding tasks as grounded vision-language tasks.
OFA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib42" title="">42</a>]</cite> and Unified-IO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib24" title="">24</a>]</cite> introduce a Seq2Seq framework for the unification of I/O, architectures, tasks, and modalities.
X-Decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib51" title="">51</a>]</cite> can predict pixel-level segmentation and language tokens through a generalized decoding model.
Uni-Perceiver v2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib22" title="">22</a>]</cite> formulates different tasks as a unified maximum likelihood estimation problem without any task-specific fine-tuning. However, these methods are either unable to perform multi-task training, or they focus too much on language-modal tasks or cross-modal tasks, while ignoring vision-modal tasks that are important for PVIs, such as segmentation and depth estimation. Therefore, we propose a new method that can comprehensively consider and balance multiple assistance-related uni-modal and cross-modal tasks, and can use one suit of weights for all tasks.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Benchmarks for Vision-Language Models</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">To evaluate vision-language systems, Zhou <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib50" title="">50</a>]</cite> propose a multi-task multi-dimension benchmark for Vision Language Pretraining (VLP) models. Su <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib36" title="">36</a>]</cite> introduce the GEM benchmark, a multi-modal benchmark that focuses on both image-language tasks and video-language tasks. Recently, many LVLMs benchmarks  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib48" title="">48</a>]</cite> have emerged to more comprehensively evaluate the fine-grained capabilities of models.
However, a vision-language benchmark for ATs and PVIs is lacking in the literature. To bridge this gap, we introduce <span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p1.1.3">@Bench</span> , a benchmark that includes realistic multi-modal tasks closely relevant to the daily lives of PVIs.
Therefore, <span class="ltx_text ltx_font_smallcaps" id="S2.SS3.p1.1.4">@Bench</span> is designed to serve as a fundamental benchmark for evaluating VLMs in the realm of ATs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">@Bench</span>: Assistive Technology Benchmark</h2>
<figure class="ltx_table" id="S3.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T1.2" style="width:433.6pt;height:125.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-93.8pt,27.2pt) scale(0.69800992506304,0.69800992506304) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T1.2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T1.2.1.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.1.1" rowspan="2" style="padding:1pt 8.0pt;"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.1.2" rowspan="2" style="padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.1.1.2.1">Function</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T1.2.1.1.1.3" rowspan="2" style="padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.1.1.3.1">Related Task</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.1.4" style="padding:1pt 8.0pt;">Level of Interest</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.1.5" style="padding:1pt 8.0pt;">Frequency of Usage</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.1.6" style="padding:1pt 8.0pt;">Importance</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T1.2.1.1.1.7" rowspan="2" style="padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.1.1.7.1">Total</span></th>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.2.1.2.2.1" style="padding:1pt 8.0pt;">1 (low)– 5 (high)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.2.1.2.2.2" style="padding:1pt 8.0pt;">1 (low) – 5 (high)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S3.T1.2.1.2.2.3" style="padding:1pt 8.0pt;">1 (low) – 5 (high)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.2.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.1.3.1.1" style="padding:1pt 8.0pt;">1</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T1.2.1.3.1.2" style="padding:1pt 8.0pt;">Obstacle Avoidance</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.2.1.3.1.3" style="padding:1pt 8.0pt;">Panoptic Segmentation</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.1.3.1.4" style="padding:1pt 8.0pt;">3.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.1.3.1.5" style="padding:1pt 8.0pt;">2.71</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.1.3.1.6" style="padding:1pt 8.0pt;">2.43</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.2.1.3.1.7" style="background-color:#B7E4C7;padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.3.1.7.1" style="background-color:#B7E4C7;">08.14</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.4.2">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.4.2.1" style="padding:1pt 8.0pt;">2</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.4.2.2" style="padding:1pt 8.0pt;">Indoor Distance Estimation</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.2.1.4.2.3" style="padding:1pt 8.0pt;">Depth Estimation</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.4.2.4" style="padding:1pt 8.0pt;">3.14</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.4.2.5" style="padding:1pt 8.0pt;">2.43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.4.2.6" style="padding:1pt 8.0pt;">2.43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.4.2.7" style="background-color:#D8F3DC;padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.4.2.7.1" style="background-color:#D8F3DC;">08.00</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.5.3">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.5.3.1" style="padding:1pt 8.0pt;">3</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.5.3.2" style="padding:1pt 8.0pt;">Object Recognition</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.2.1.5.3.3" style="padding:1pt 8.0pt;">Panoptic Segmentation</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.5.3.4" style="padding:1pt 8.0pt;">3.86</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.5.3.5" style="padding:1pt 8.0pt;">3.71</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.5.3.6" style="padding:1pt 8.0pt;">4.00</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.5.3.7" style="background-color:#40916C;padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.5.3.7.1" style="background-color:#40916C;">11.57</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.6.4">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.6.4.1" style="padding:1pt 8.0pt;">4</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.6.4.2" style="padding:1pt 8.0pt;">Object Location</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.2.1.6.4.3" style="padding:1pt 8.0pt;">Panoptic Segmentation</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.6.4.4" style="padding:1pt 8.0pt;">3.29</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.6.4.5" style="padding:1pt 8.0pt;">3.14</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.6.4.6" style="padding:1pt 8.0pt;">3.29</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.6.4.7" style="background-color:#74C69D;padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.6.4.7.1" style="background-color:#74C69D;">09.72</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.7.5">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.7.5.1" style="padding:1pt 8.0pt;">5</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.7.5.2" style="padding:1pt 8.0pt;">Text Recognition</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.2.1.7.5.3" style="padding:1pt 8.0pt;">OCR</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.7.5.4" style="padding:1pt 8.0pt;">4.57</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.7.5.5" style="padding:1pt 8.0pt;">4.43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.7.5.6" style="padding:1pt 8.0pt;">4.43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.7.5.7" style="background-color:#40916C;padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.7.5.7.1" style="background-color:#40916C;">13.43</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.8.6">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.8.6.1" style="padding:1pt 8.0pt;">6</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.8.6.2" style="padding:1pt 8.0pt;">Surroundings Understanding</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.2.1.8.6.3" style="padding:1pt 8.0pt;">Image Captioning</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.8.6.4" style="padding:1pt 8.0pt;">3.43</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.8.6.5" style="padding:1pt 8.0pt;">2.86</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.8.6.6" style="padding:1pt 8.0pt;">2.71</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.8.6.7" style="background-color:#52B788;padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.8.6.7.1" style="background-color:#52B788;">09.00</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.9.7">
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.9.7.1" style="padding:1pt 8.0pt;">7</td>
<td class="ltx_td ltx_align_left" id="S3.T1.2.1.9.7.2" style="padding:1pt 8.0pt;">Scene Recognition</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.2.1.9.7.3" style="padding:1pt 8.0pt;">Scene Recognition</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.9.7.4" style="padding:1pt 8.0pt;">2.14</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.9.7.5" style="padding:1pt 8.0pt;">1.71</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.9.7.6" style="padding:1pt 8.0pt;">1.86</td>
<td class="ltx_td ltx_align_center" id="S3.T1.2.1.9.7.7" style="background-color:#DFDFDF;padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.9.7.7.1" style="background-color:#DFDFDF;">05.71</span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.1.10.8">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.1.10.8.1" style="padding:1pt 8.0pt;">8</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T1.2.1.10.8.2" style="padding:1pt 8.0pt;">Visual Q&amp;A</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T1.2.1.10.8.3" style="padding:1pt 8.0pt;">Visual Question Answering</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.1.10.8.4" style="padding:1pt 8.0pt;">3.57</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.1.10.8.5" style="padding:1pt 8.0pt;">3.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.1.10.8.6" style="padding:1pt 8.0pt;">3.43</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.2.1.10.8.7" style="background-color:#52B788;padding:1pt 8.0pt;"><span class="ltx_text" id="S3.T1.2.1.10.8.7.1" style="background-color:#52B788;">10.43</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T1.4.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text ltx_font_bold" id="S3.T1.5.2" style="font-size:90%;">Quantitative result of the user study.<span class="ltx_text ltx_font_medium" id="S3.T1.5.2.1"> Potential functions can be achieved via related tasks, which are listed in the questionnaires for the user study. Note: all scores are averages across 7 participants.</span></span></figcaption>
</figure>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.1">@Bench</span> is a pioneering multi-modal benchmark tailored specifically for the domain of Assistive Technology (AT). The primary target of <span class="ltx_text ltx_font_smallcaps" id="S3.p1.1.2">@Bench</span> is to establish a comprehensive and standardized evaluation platform for vision-language models in the context of helping PVIs.</p>
</div>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">In this section, we introduce the detail of the user study (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3.SS1" title="3.1 User-centered Study ‣ 3 @Bench: Assistive Technology Benchmark ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">3.1</span></a>), which helps us identify important tasks that are closely related to PVIs. We then provide an overview (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3.SS2" title="3.2 Assistive Tasks ‣ 3 @Bench: Assistive Technology Benchmark ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">3.2</span></a>) of the tasks, datasets and metrics encompassed within the <span class="ltx_text ltx_font_smallcaps" id="S3.p2.1.1">@Bench</span> framework. Subsequently, we introduce the vital dimension for assessing the performance of VLMs for ATs: efficiency-performance trade-off (Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3.SS3" title="3.3 Efficiency-Performance Trade-off ‣ 3 @Bench: Assistive Technology Benchmark ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">3.3</span></a>).</p>
</div>
<figure class="ltx_table" id="S3.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S3.T2.1" style="width:433.6pt;height:170.5pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(50.7pt,-19.9pt) scale(1.30520856283676,1.30520856283676) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S3.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="S3.T2.1.1.2.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">Task</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_tt" id="S3.T2.1.1.2.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Dataset</th>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.2.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">#Train</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.2.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">#Val</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T2.1.1.2.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">#Test</td>
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.2.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Metric</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S3.T2.1.1.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">PS</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">ADE20K</th>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.3.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">25,574</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.3.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">2,000</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.1.1.3.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">2,000</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.1.3.2.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">PQ</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.4.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">DE</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.4.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">NYU v2</th>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.4.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">24,230</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.4.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">654</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.4.3.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">654</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.4.3.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">RMSE</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.5.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">OCR</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.5.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">MJ, ST, 6 OCR</th>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.5.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">15,895,356</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.5.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">7,507</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.5.4.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">7,507</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.5.4.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.6.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">IC</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.6.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">VizWiz_Cap</th>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.6.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">23,431</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.6.5.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">7,750</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.6.5.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">8,000</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.6.5.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">BLEU-1/CIDEr</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S3.T2.1.1.7.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">VQA</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.7.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">VizWiz_VQA</th>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.7.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">20,523</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.7.6.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">4,319</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.1.1.7.6.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">8,000</td>
<td class="ltx_td ltx_align_left" id="S3.T2.1.1.7.6.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Accuracy</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S3.T2.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><math alttext="\sum" class="ltx_Math" display="inline" id="S3.T2.1.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.m1.1.1.cmml">∑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><sum id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1"></sum></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\sum</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.1.m1.1d">∑</annotation></semantics></math></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">@<span class="ltx_text ltx_font_smallcaps" id="S3.T2.1.1.1.2.1">Bench</span>
</th>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S3.T2.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">15,989,114</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S3.T2.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">22230</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S3.T2.1.1.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">26161</td>
<td class="ltx_td ltx_border_bb ltx_border_t" id="S3.T2.1.1.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;"></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S3.T2.6.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text ltx_font_bold" id="S3.T2.7.2" style="font-size:90%;">Statistic of pre-selected tasks and datasets in <span class="ltx_text ltx_font_smallcaps" id="S3.T2.7.2.1">@Bench</span><span class="ltx_text ltx_font_medium" id="S3.T2.7.2.2">. Note that some datasets do not have a <span class="ltx_text ltx_font_italic" id="S3.T2.7.2.2.1">test</span> subset, so use <span class="ltx_text ltx_font_italic" id="S3.T2.7.2.2.2">val</span> subset for evaluation. The 6 OCR datasets are IC13, IC15, IIIT5K, SVT, SVTP and CUTE.</span></span></figcaption>
</figure>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>User-centered Study</h3>
<div class="ltx_para ltx_noindent" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p1.1.1">Organization.</span> To build the AT benchmark, we conducted a pre-study with 2 accessibility experts to develop a reasonable questionnaire. Based on their suggestions and multiple discussions, we further organized a user-centered study with 7 participants who are blind or have low vision. The goal was to identify which vision-language tasks are beneficial for the target group and meet their requirements.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">Questionnaire.</span>
To enhance the rationale of our benchmark design, we conducted a questionnaire session with the participants. In this questionnaire, we presented 8 functions relevant to the daily lives of PVIs in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3.T1" title="Table 1 ‣ 3 @Bench: Assistive Technology Benchmark ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">1</span></a>: (1) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">obstacle avoidance</span>, (2) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">indoor distance estimation</span>, (3) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.4">object recognition</span>, (4) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.5">object location</span>, (5) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.6">text recognition</span>, (6) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.7">surroundings understanding</span>, (7) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.8">scene recognition</span> and (8) <span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.9">visual Q&amp;A for surroundings</span>. At the begining of the user study, we first explained the specific concepts of different functions and related usage scenarios to the participants. Then, they were asked to rate each function based on 3 criteria: (1) level of interest, (2) frequency of usage, and (3) importance in their daily life from 1 (lowest) to 5 (highest). In addition, each function has and can be performed by using a corresponding uni-modal or cross-modal task.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">Quantitative Result.</span> Each participant rated each function from 1 to 5 based on these three criteria. After collecting the scores of each participant, we averaged the scores of the 7 participants to represent the score of the function, and finally used the total score of the 3 criteria as the score of each function. And we use the total score as a basis to select relevant functions and tasks.
The ratings were then aggregated and summarized, as illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3.T1" title="Table 1 ‣ 3 @Bench: Assistive Technology Benchmark ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">1</span></a>.
Functions 5 and 3 have the highest scores. PVIs think text recognition and object recognition are the most important in their daily life. Function 7 has the lowest score and has a large gap with other functions. At the same time, functions 1, 2, 4, 6, and 8 have similar scores. Therefore, we did not consider the tasks corresponding to function 7 in the benchmark, and retained the tasks related to all the remaining functions. According to the study, the selected functions are: panoptic segmentation, depth estimation, OCR, image captioning and VQA.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Assistive Tasks</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Guided by the user study, <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p1.1.1">@Bench</span> contains 5 tasks that are extremely relevant to the daily lives of PVIs.
We give an overview of all tasks and the corresponding datasets in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S3.T2" title="Table 2 ‣ 3 @Bench: Assistive Technology Benchmark ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">2</span></a>, and describe the details as follows:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Panoptic Segmentation.</span> It is the task that combines both semantic segmentation and instance segmentation, aiming to simultaneously recognize all object instances in an image and segment them by category, helping blind people perceive the surroundings more accurately. We opt to ADE20K  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib49" title="">49</a>]</cite>, which contains more than 27K images spanning 365 different scenes with totally 150 semantic categories, including indoor, outdoor, urban, rural, <span class="ltx_text ltx_font_italic" id="S3.SS2.p2.1.2">etc.</span>, basically covering almost all daily life scenes and common objects of PVIs.
And we use Panoptic Quality (PQ)  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib20" title="">20</a>]</cite> to measure the performance.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Depth Estimation.</span> It is the task of measuring the distance of each pixel relative to the user’s camera. According to user study, the majority of PVIs spend most of their time indoors. Therefore, we choose NYU v2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib27" title="">27</a>]</cite>, which includes indoor scenes. We evaluate with the RMSE metric.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Optical Character Recognition.</span> It is the conversion of images of typed, handwritten or printed text into machine-encoded text. We know text recognition is the most important function for the PVIs from user study, so we select two widely-used, large synthetic datasets for training: MJSynth (MJ) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib14" title="">14</a>]</cite> and SynthText (ST) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib14" title="">14</a>]</cite>. We evaluate recognition accuracy on 6 datasets, covering various text scenarios: ICDAR 2013 (IC13) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib17" title="">17</a>]</cite>, ICDAR 2015 (IC15) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib16" title="">16</a>]</cite>, IIIT5K-Words (IIIT5K) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib26" title="">26</a>]</cite>, Street View Text (SVT) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib41" title="">41</a>]</cite>, Street View Text-Perspective (SVTP) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib29" title="">29</a>]</cite>, and CUTE80 (CUTE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib34" title="">34</a>]</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Image Captioning.</span>
It is a challenging task that involves generating human-like and coherent natural language descriptions for images. The task is to comprehend visual content and express in natural language that is descriptive and contextually relevant, which allows PVIs to have an overall understanding of their surroundings.
To better align with the daily experiences of PVIs, we opted for the VizWiz_Cap <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib11" title="">11</a>]</cite>, collected from the perspective of PVIs. We use BLEU-1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib28" title="">28</a>]</cite> and CIDEr <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib38" title="">38</a>]</cite> as evaluation metrics.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p6.1.1">Visual Question Answering.</span> It requires the model to take as input an image and a free-form, open-ended, natural language question. It produces or selects a natural language answer as output <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib50" title="">50</a>]</cite>. In our work, we found that PVIs expressed significant interest in this task. By posing questions, they can experience and comprehend the unseen world around them, providing them with a novel and enriching experience. We select VizWiz_VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib10" title="">10</a>]</cite> and use the publicly released VizWiz_VQA evaluation scripts in <span class="ltx_text ltx_font_smallcaps" id="S3.SS2.p6.1.2">@Bench</span>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Efficiency-Performance Trade-off</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">In designing models for assistive systems, an optimal balance between efficiency and performance is essential.
Normally, the performance of VLMs can be easily measured by task-specific metrics. For efficiency, there are few common choices: the number of parameters, FLOPs and inference time. To compare with previous methods, we evaluate efficiency through the number of parameters.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">In sum, <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p2.1.1">@Bench</span> is designed specifically for multi-modal, multi-task scenarios and tailored to assistive systems for PVIs. All tasks are closely tied to the needs of PVIs community. <span class="ltx_text ltx_font_smallcaps" id="S3.SS3.p2.1.2">@Bench</span> not only prioritizes performance but also places emphasis on efficiency-performance trade-off. We aspire for this benchmark to serve as a cornerstone for researchers within PVIs assistance community, encouraging exploration of multi-modal models’ applications in ATs.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">@Model</span>: Assistive Technology Model</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Based on X-Decoder, we propose <span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.1">@Model</span>, the first generalist model to support all these assistance-related vision-language tasks. As show in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S4.F2" title="Figure 2 ‣ 4 @Model: Assistive Technology Model ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">2</span></a>, the overall model is built on top of a image encoder for extracting image features, two text encoders that share parameters for extracting text features, and a transformer decoder with generic latent queries and textual queries.
<span class="ltx_text ltx_font_smallcaps" id="S4.p1.1.2">@Model</span> has two types of output: (1) pixel-level output for dense prediction, such as panoptic segmentation and dense estimation and (2) token-level output for a diverse set of language-related vision tasks, such VQA, image captioning and OCR.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="394" id="S4.F2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F2.3.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F2.4.2" style="font-size:90%;">Overall architecture of <span class="ltx_text ltx_font_smallcaps" id="S4.F2.4.2.1">@Model</span><span class="ltx_text ltx_font_medium" id="S4.F2.4.2.2">. We propose task-based prompts to unify inputs and perform different tasks all at once.</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="303" id="S4.F3.g1" src="x3.png" width="476"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F3.4.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F3.5.2" style="font-size:90%;">Paradigms of multi-task methods.<span class="ltx_text ltx_font_medium" id="S4.F3.5.2.1"> Our <span class="ltx_text ltx_font_smallcaps" id="S4.F3.5.2.1.1">@Model</span> incorporates task-specific prompts that effectively unify tasks all at once and with almost no additional parameters.
</span></span></figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Unified Multi-task Architecture.</span>
X-Decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib51" title="">51</a>]</cite> includes only two tasks in <span class="ltx_text ltx_font_smallcaps" id="S4.p2.1.2">@Bench</span>, panoptic segmentation and image captioning. To include token-level output like VQA or OCR, X-Decoder requires a specific head for each task. This paradigm (in Fig <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S4.F3" title="Figure 3 ‣ 4 @Model: Assistive Technology Model ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">3</span></a>) will make the structure of the entire model very bloated, which is a major drawback for portable assistance systems. In contrast,
we use task-specific prompt to build a unified input paradigm “image + prompt” as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S4.F3" title="Figure 3 ‣ 4 @Model: Assistive Technology Model ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">3</span></a>. Compared with multi-head output, the benefits are three-fold: (1) Unifying input forms for different tasks. For example, it can unify the inputs of VQA (image and question text) and segmentation (image) in the manner of “image + prompt”. (2) Enabling the model to distinguish different tasks, extract corresponding features in early phase. (3) Reducing the number of parameters.
We design a corresponding prompt shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S0.F1" title="Figure 1 ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">1</span></a> for each task. For VQA, we use its own questions directly as the prompt. In Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.SS5" title="5.5 Ablation Study ‣ 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">5.5</span></a>, we analyze the performance and efficiency advantages of such a design.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Character-based Tokenizer with Limited Vocabulary for OCR.</span>
In the text encoders of <span class="ltx_text ltx_font_smallcaps" id="S4.p3.1.2">@Model</span>, we use pretrained CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib31" title="">31</a>]</cite> subword-based tokenizer with a vocabulary containing approximately 50,000 subwords as default.
But during language-related tasks training, we need to consider a problem, there is a mismatch between dataset’s vocabulary space and model’s prediction vocabulary space.
For example, the dataset of captioning contains a rich vocabulary that is comparable to the vocabulary that can be predicted by the model. But for OCR, we have some observations: (1) In the English OCR datasets in <span class="ltx_text ltx_font_smallcaps" id="S4.p3.1.3">@Bench</span>, an image usually contains only one pure text. (2) The texts basically use 26 English letters and 10 numbers. (3) If each text is divided by a single character, the length is basically less than 15. When we use the default subword-based tokenizer and complete vocabulary, we found that the training effect is unsatisfactory. Subword-based tokenizer with a large vocabulary that can provide semantic information is not effective enough for OCR. The model only needs to recognize the text but not the representations of the text. A character-based tokenizer can bring a much smaller vocabulary and relieve this mismatch. More details locate in the supplementary material. The respective experiment is presented in Sec. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.SS5" title="5.5 Ablation Study ‣ 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">5.5</span></a>.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="327" id="S4.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.4.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text ltx_font_bold" id="S4.F4.5.2" style="font-size:90%;">Examples of multi-task training results on 5 tasks.<span class="ltx_text ltx_font_medium" id="S4.F4.5.2.1"> Given one image as input our <span class="ltx_text ltx_font_smallcaps" id="S4.F4.5.2.1.1">@Model</span> can output all predictions.
</span></span></figcaption>
</figure>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experiments</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">This section provides experimental results and analyses to demonstrate the effectiveness of our proposed model. Implementation details are in the supplementary material.</p>
</div>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T3.19" style="width:433.6pt;height:197.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(19.6pt,-8.9pt) scale(1.09927003696428,1.09927003696428) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.19.19">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.19.19.20.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.19.19.20.1.1" rowspan="3" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.20.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.19.19.20.1.2" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.19.19.20.1.2.1"><em class="ltx_emph ltx_font_italic" id="S5.T3.19.19.20.1.2.1.1">PS</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.19.19.20.1.3" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.19.19.20.1.3.1"><em class="ltx_emph ltx_font_italic" id="S5.T3.19.19.20.1.3.1.1">DE</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.19.19.20.1.4" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.19.19.20.1.4.1"><em class="ltx_emph ltx_font_italic" id="S5.T3.19.19.20.1.4.1.1">OCR</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T3.19.19.20.1.5" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.19.19.20.1.5.1"><em class="ltx_emph ltx_font_italic" id="S5.T3.19.19.20.1.5.1.1">IC</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T3.19.19.20.1.6" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T3.19.19.20.1.6.1"><em class="ltx_emph ltx_font_italic" id="S5.T3.19.19.20.1.6.1.1">VQA</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T3.19.19.20.1.7" rowspan="3" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.20.1.7.1">#Params</span></th>
</tr>
<tr class="ltx_tr" id="S5.T3.19.19.21.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.19.19.21.2.1" style="padding:2.5pt 2.0pt;">ADE-150</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.19.19.21.2.2" style="padding:2.5pt 2.0pt;">NYU-V2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.19.19.21.2.3" style="padding:2.5pt 2.0pt;">6 Datasets avg</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="2" id="S5.T3.19.19.21.2.4" style="padding:2.5pt 2.0pt;">VizWiz_Cap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.19.19.21.2.5" style="padding:2.5pt 2.0pt;">VizWiz_VQA</th>
</tr>
<tr class="ltx_tr" id="S5.T3.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.3.3.3.4" style="padding:2.5pt 2.0pt;">PQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.1.1.1.1" style="padding:2.5pt 2.0pt;">RMSE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T3.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.2.2.2.2" style="padding:2.5pt 2.0pt;">Acc(<math alttext="\%" class="ltx_Math" display="inline" id="S5.T3.2.2.2.2.m1.1"><semantics id="S5.T3.2.2.2.2.m1.1a"><mo id="S5.T3.2.2.2.2.m1.1.1" xref="S5.T3.2.2.2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T3.2.2.2.2.m1.1.1.cmml" xref="S5.T3.2.2.2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.2.m1.1d">%</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T3.3.3.3.5" style="padding:2.5pt 2.0pt;">B@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.3.3.3.6" style="padding:2.5pt 2.0pt;">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T3.3.3.3.3" style="padding:2.5pt 2.0pt;">Acc(<math alttext="\%" class="ltx_Math" display="inline" id="S5.T3.3.3.3.3.m1.1"><semantics id="S5.T3.3.3.3.3.m1.1a"><mo id="S5.T3.3.3.3.3.m1.1.1" xref="S5.T3.3.3.3.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S5.T3.3.3.3.3.m1.1.1.cmml" xref="S5.T3.3.3.3.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.3.m1.1d">%</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T3.4.4.4.1" style="padding:2.5pt 2.0pt;">Unified-IO (S)<sup class="ltx_sup" id="S5.T3.4.4.4.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.4.4.4.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.6.4" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.6.5" style="padding:2.5pt 2.0pt;">0.649</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.6.6" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.5.5.5.2" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.5.5.5.2.m1.1"><semantics id="S5.T3.5.5.5.2.m1.1a"><mo id="S5.T3.5.5.5.2.m1.1.1" xref="S5.T3.5.5.5.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.5.5.5.2.m1.1b"><ci id="S5.T3.5.5.5.2.m1.1.1.cmml" xref="S5.T3.5.5.5.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.5.5.5.2.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.5.5.5.2.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.6.3" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.6.6.6.3.m1.1"><semantics id="S5.T3.6.6.6.3.m1.1a"><mo id="S5.T3.6.6.6.3.m1.1.1" xref="S5.T3.6.6.6.3.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.6.6.3.m1.1b"><ci id="S5.T3.6.6.6.3.m1.1.1.cmml" xref="S5.T3.6.6.6.3.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.6.6.3.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.6.6.3.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T3.6.6.6.7" style="padding:2.5pt 2.0pt;">42.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T3.6.6.6.8" style="padding:2.5pt 2.0pt;">71M</td>
</tr>
<tr class="ltx_tr" id="S5.T3.9.9.9">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.7.7.7.1" style="padding:2.5pt 2.0pt;">Unified-IO (B)<sup class="ltx_sup" id="S5.T3.7.7.7.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.7.7.7.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.9.9.9.4" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.9.9.9.5" style="padding:2.5pt 2.0pt;">0.469</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.9.9.9.6" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="S5.T3.8.8.8.2" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.8.8.8.2.m1.1"><semantics id="S5.T3.8.8.8.2.m1.1a"><mo id="S5.T3.8.8.8.2.m1.1.1" xref="S5.T3.8.8.8.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.8.8.8.2.m1.1b"><ci id="S5.T3.8.8.8.2.m1.1.1.cmml" xref="S5.T3.8.8.8.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.8.8.8.2.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.8.8.8.2.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.9.9.9.3" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.9.9.9.3.m1.1"><semantics id="S5.T3.9.9.9.3.m1.1a"><mo id="S5.T3.9.9.9.3.m1.1.1" xref="S5.T3.9.9.9.3.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.9.9.9.3.m1.1b"><ci id="S5.T3.9.9.9.3.m1.1.1.cmml" xref="S5.T3.9.9.9.3.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.9.9.9.3.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.9.9.9.3.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.9.9.9.7" style="padding:2.5pt 2.0pt;">45.8</td>
<td class="ltx_td ltx_align_center" id="S5.T3.9.9.9.8" style="padding:2.5pt 2.0pt;">241M</td>
</tr>
<tr class="ltx_tr" id="S5.T3.12.12.12">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.10.10.10.1" style="padding:2.5pt 2.0pt;">Unified-IO (L)<sup class="ltx_sup" id="S5.T3.10.10.10.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.10.10.10.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.12.12.12.4" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.12.12.12.5" style="padding:2.5pt 2.0pt;">0.402</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.12.12.12.6" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="S5.T3.11.11.11.2" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.11.11.11.2.m1.1"><semantics id="S5.T3.11.11.11.2.m1.1a"><mo id="S5.T3.11.11.11.2.m1.1.1" xref="S5.T3.11.11.11.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.11.11.11.2.m1.1b"><ci id="S5.T3.11.11.11.2.m1.1.1.cmml" xref="S5.T3.11.11.11.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.11.11.11.2.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.11.11.11.2.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.12.12.12.3" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.12.12.12.3.m1.1"><semantics id="S5.T3.12.12.12.3.m1.1a"><mo id="S5.T3.12.12.12.3.m1.1.1" xref="S5.T3.12.12.12.3.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.12.12.12.3.m1.1b"><ci id="S5.T3.12.12.12.3.m1.1.1.cmml" xref="S5.T3.12.12.12.3.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.12.12.12.3.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.12.12.12.3.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.12.12.12.7" style="padding:2.5pt 2.0pt;">47.7</td>
<td class="ltx_td ltx_align_center" id="S5.T3.12.12.12.8" style="padding:2.5pt 2.0pt;">776M</td>
</tr>
<tr class="ltx_tr" id="S5.T3.15.15.15">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.13.13.13.1" style="padding:2.5pt 2.0pt;">X-Decoder (T)<sup class="ltx_sup" id="S5.T3.13.13.13.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.13.13.13.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.15.15.15.4" style="padding:2.5pt 2.0pt;">41.6</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.15.15.15.5" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.15.15.15.6" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="S5.T3.14.14.14.2" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.14.14.14.2.m1.1"><semantics id="S5.T3.14.14.14.2.m1.1a"><mo id="S5.T3.14.14.14.2.m1.1.1" xref="S5.T3.14.14.14.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.14.14.14.2.m1.1b"><ci id="S5.T3.14.14.14.2.m1.1.1.cmml" xref="S5.T3.14.14.14.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.14.14.14.2.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.14.14.14.2.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.15.15.15.3" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.15.15.15.3.m1.1"><semantics id="S5.T3.15.15.15.3.m1.1a"><mo id="S5.T3.15.15.15.3.m1.1.1" xref="S5.T3.15.15.15.3.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.15.15.15.3.m1.1b"><ci id="S5.T3.15.15.15.3.m1.1.1.cmml" xref="S5.T3.15.15.15.3.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.15.15.15.3.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.15.15.15.3.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.15.15.15.7" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="S5.T3.15.15.15.8" style="padding:2.5pt 2.0pt;">164M</td>
</tr>
<tr class="ltx_tr" id="S5.T3.17.17.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.16.16.16.1" style="padding:2.5pt 2.0pt;">GIT<sup class="ltx_sup" id="S5.T3.16.16.16.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.16.16.16.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.17.17.17.3" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.17.17.17.4" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.17.17.17.5" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="S5.T3.17.17.17.2" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.17.17.17.2.m1.1"><semantics id="S5.T3.17.17.17.2.m1.1a"><mo id="S5.T3.17.17.17.2.m1.1.1" xref="S5.T3.17.17.17.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.17.17.17.2.m1.1b"><ci id="S5.T3.17.17.17.2.m1.1.1.cmml" xref="S5.T3.17.17.17.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.17.17.17.2.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.17.17.17.2.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.17.17.17.6" style="padding:2.5pt 2.0pt;">113.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.17.17.17.7" style="padding:2.5pt 2.0pt;">68.0</td>
<td class="ltx_td ltx_align_center" id="S5.T3.17.17.17.8" style="padding:2.5pt 2.0pt;">0.7B</td>
</tr>
<tr class="ltx_tr" id="S5.T3.19.19.19">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T3.18.18.18.1" style="padding:2.5pt 2.0pt;">PaLI<sup class="ltx_sup" id="S5.T3.18.18.18.1.1"><span class="ltx_text ltx_font_italic" id="S5.T3.18.18.18.1.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.19.19.19.3" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.19.19.19.4" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.19.19.19.5" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="S5.T3.19.19.19.2" style="padding:2.5pt 2.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.19.19.19.2.m1.1"><semantics id="S5.T3.19.19.19.2.m1.1a"><mo id="S5.T3.19.19.19.2.m1.1.1" xref="S5.T3.19.19.19.2.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.19.19.19.2.m1.1b"><ci id="S5.T3.19.19.19.2.m1.1.1.cmml" xref="S5.T3.19.19.19.2.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.19.19.19.2.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.19.19.19.2.m1.1d">⋆</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.19.19.19.6" style="padding:2.5pt 2.0pt;">117.2</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T3.19.19.19.7" style="padding:2.5pt 2.0pt;">67.5</td>
<td class="ltx_td ltx_align_center" id="S5.T3.19.19.19.8" style="padding:2.5pt 2.0pt;">3.0B</td>
</tr>
<tr class="ltx_tr" id="S5.T3.19.19.22.1" style="background-color:#ECECEC;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.19.19.22.1.1" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.22.1.1.1" style="background-color:#ECECEC;">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.19.19.22.1.2" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.22.1.2.1" style="background-color:#ECECEC;">38.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.19.19.22.1.3" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.22.1.3.1" style="background-color:#ECECEC;">0.425</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.19.19.22.1.4" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.22.1.4.1" style="background-color:#ECECEC;">80.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.19.19.22.1.5" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.22.1.5.1" style="background-color:#ECECEC;">61.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.19.19.22.1.6" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.22.1.6.1" style="background-color:#ECECEC;">52.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T3.19.19.22.1.7" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.22.1.7.1" style="background-color:#ECECEC;">53.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T3.19.19.22.1.8" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T3.19.19.22.1.8.1" style="background-color:#ECECEC;">62M</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.26.3.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T3.23.2" style="font-size:90%;">Comparison of multi-task training <span class="ltx_text ltx_font_smallcaps" id="S5.T3.23.2.3">@Model</span> and other generalist models.<span class="ltx_text ltx_font_medium" id="S5.T3.23.2.2"> We report the multi-task training results without any pre-training and task-specific fine-tuning. Note: GIT and PaLI are LVLMs. “<math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.22.1.1.m1.1"><semantics id="S5.T3.22.1.1.m1.1b"><mo id="S5.T3.22.1.1.m1.1.1" xref="S5.T3.22.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.22.1.1.m1.1c"><ci id="S5.T3.22.1.1.m1.1.1.cmml" xref="S5.T3.22.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.22.1.1.m1.1d">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.22.1.1.m1.1e">⋆</annotation></semantics></math>” denotes the model has the capability for the task but does not have number reported. “–” means the model does not have the ability for the specific task. “<math alttext="{{\dagger}}" class="ltx_Math" display="inline" id="S5.T3.23.2.2.m2.1"><semantics id="S5.T3.23.2.2.m2.1b"><mo id="S5.T3.23.2.2.m2.1.1" xref="S5.T3.23.2.2.m2.1.1.cmml">†</mo><annotation-xml encoding="MathML-Content" id="S5.T3.23.2.2.m2.1c"><ci id="S5.T3.23.2.2.m2.1.1.cmml" xref="S5.T3.23.2.2.m2.1.1">†</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.23.2.2.m2.1d">{{\dagger}}</annotation><annotation encoding="application/x-llamapun" id="S5.T3.23.2.2.m2.1e">†</annotation></semantics></math>” means the model uses pre-trained weights for training. (B@1 = BLEU-1)</span></span></figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Comparison with Existing Generalist Models</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.2">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.T3" title="Table 3 ‣ 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">3</span></a>, we compare the performances of <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.2.1">@Model</span> and some other generalist models on the tasks in <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.2.2">@Bench</span>.
<span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.2.3">@Model</span> is the first generalist model to support assistance-related vision-language tasks and can achieve superior results. However, (1) most generalist models only include a few specific tasks and do not include all tasks in <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.2.4">@Bench</span>, especially OCR task. And (2) most generalist models only report fine-tuned results and few methods report their results on assistance-related datasets. Therefore, we only compare a few related methods.
Since generalist models aim to process different tasks with shared architecture and parameters, some generalist models only report results with task-specific fine-tuning, <span class="ltx_text ltx_font_italic" id="S5.SS1.p1.2.5">e.g.</span>, X-Decoder, GIT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib40" title="">40</a>]</cite> and PaLI, this fine-tuning will lose the general modeling ability, so we report the numbers without any task-specific adaptation in the manner of multi-task training.
Nonetheless, with similar number of parameters, <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.2.6">@Model</span> can outperform Unified-IO (S) in depth estimation and VQA by <math alttext="0.224" class="ltx_Math" display="inline" id="S5.SS1.p1.1.m1.1"><semantics id="S5.SS1.p1.1.m1.1a"><mn id="S5.SS1.p1.1.m1.1.1" xref="S5.SS1.p1.1.m1.1.1.cmml">0.224</mn><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.1.m1.1b"><cn id="S5.SS1.p1.1.m1.1.1.cmml" type="float" xref="S5.SS1.p1.1.m1.1.1">0.224</cn></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.1.m1.1c">0.224</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.1.m1.1d">0.224</annotation></semantics></math> and <math alttext="11.3\%" class="ltx_Math" display="inline" id="S5.SS1.p1.2.m2.1"><semantics id="S5.SS1.p1.2.m2.1a"><mrow id="S5.SS1.p1.2.m2.1.1" xref="S5.SS1.p1.2.m2.1.1.cmml"><mn id="S5.SS1.p1.2.m2.1.1.2" xref="S5.SS1.p1.2.m2.1.1.2.cmml">11.3</mn><mo id="S5.SS1.p1.2.m2.1.1.1" xref="S5.SS1.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS1.p1.2.m2.1b"><apply id="S5.SS1.p1.2.m2.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS1.p1.2.m2.1.1.1.cmml" xref="S5.SS1.p1.2.m2.1.1.1">percent</csymbol><cn id="S5.SS1.p1.2.m2.1.1.2.cmml" type="float" xref="S5.SS1.p1.2.m2.1.1.2">11.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.p1.2.m2.1c">11.3\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.p1.2.m2.1d">11.3 %</annotation></semantics></math>, respectively, and even better than the Unified-IO (B). On other tasks, since many methods are pre-trained, fine-tuned, and have a larger number of parameters, there is still a certain gap between <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.p1.2.7">@Model</span> and these methods.
The visualization of some multi-task training results is presented in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S4.F4" title="Figure 4 ‣ 4 @Model: Assistive Technology Model ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">4</span></a>.</p>
</div>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T4.11" style="width:433.6pt;height:68.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-182.5pt,28.8pt) scale(0.542978018249423,0.542978018249423) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.11.11">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T4.11.11.12.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.11.11.12.1.1" rowspan="3" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.12.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" id="S5.T4.11.11.12.1.2" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.11.11.12.1.2.1"><em class="ltx_emph ltx_font_italic" id="S5.T4.11.11.12.1.2.1.1">PS</em></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.11.11.12.1.3" rowspan="3" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.12.1.3.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" id="S5.T4.11.11.12.1.4" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.11.11.12.1.4.1"><em class="ltx_emph ltx_font_italic" id="S5.T4.11.11.12.1.4.1.1">DE</em></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.11.11.12.1.5" rowspan="3" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.12.1.5.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" colspan="7" id="S5.T4.11.11.12.1.6" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.11.11.12.1.6.1"><em class="ltx_emph ltx_font_italic" id="S5.T4.11.11.12.1.6.1.1">OCR</em></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.11.11.12.1.7" rowspan="3" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.12.1.7.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_t" colspan="2" id="S5.T4.11.11.12.1.8" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.11.11.12.1.8.1"><em class="ltx_emph ltx_font_italic" id="S5.T4.11.11.12.1.8.1.1">IC</em></span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T4.11.11.12.1.9" rowspan="3" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.12.1.9.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T4.11.11.12.1.10" style="padding:2.5pt 2.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T4.11.11.12.1.10.1"><em class="ltx_emph ltx_font_italic" id="S5.T4.11.11.12.1.10.1.1">VQA</em></span></th>
</tr>
<tr class="ltx_tr" id="S5.T4.11.11.13.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" id="S5.T4.11.11.13.2.1" style="padding:2.5pt 2.0pt;">ADE-150</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" id="S5.T4.11.11.13.2.2" style="padding:2.5pt 2.0pt;">NYU-V2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.11.11.13.2.3" style="padding:2.5pt 2.0pt;">IC13</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.11.11.13.2.4" style="padding:2.5pt 2.0pt;">IC15</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.11.11.13.2.5" style="padding:2.5pt 2.0pt;">SVT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.11.11.13.2.6" style="padding:2.5pt 2.0pt;">IIIT5K</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.11.11.13.2.7" style="padding:2.5pt 2.0pt;">SVTP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.11.11.13.2.8" style="padding:2.5pt 2.0pt;">CUTE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" id="S5.T4.11.11.13.2.9" style="padding:2.5pt 2.0pt;">avg</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" colspan="2" id="S5.T4.11.11.13.2.10" style="padding:2.5pt 2.0pt;">VizWiz_Cap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.11.11.13.2.11" style="padding:2.5pt 2.0pt;">VizWiz_VQA</th>
</tr>
<tr class="ltx_tr" id="S5.T4.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" id="S5.T4.3.3.3.4" style="padding:2.5pt 2.0pt;">PQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" id="S5.T4.1.1.1.1" style="padding:2.5pt 2.0pt;">RMSE<math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T4.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" colspan="7" id="S5.T4.2.2.2.2" style="padding:2.5pt 2.0pt;">Acc (<math alttext="\%" class="ltx_Math" display="inline" id="S5.T4.2.2.2.2.m1.1"><semantics id="S5.T4.2.2.2.2.m1.1a"><mo id="S5.T4.2.2.2.2.m1.1.1" xref="S5.T4.2.2.2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T4.2.2.2.2.m1.1.1.cmml" xref="S5.T4.2.2.2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.2.m1.1d">%</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.3.3.5" style="padding:2.5pt 2.0pt;">B@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr" id="S5.T4.3.3.3.6" style="padding:2.5pt 2.0pt;">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T4.3.3.3.3" style="padding:2.5pt 2.0pt;">Acc (<math alttext="\%" class="ltx_Math" display="inline" id="S5.T4.3.3.3.3.m1.1"><semantics id="S5.T4.3.3.3.3.m1.1a"><mo id="S5.T4.3.3.3.3.m1.1.1" xref="S5.T4.3.3.3.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T4.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S5.T4.3.3.3.3.m1.1.1.cmml" xref="S5.T4.3.3.3.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.3.3.3.3.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T4.3.3.3.3.m1.1d">%</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T4.6.6.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.4.4.4.1" style="padding:2.5pt 2.0pt;">MaskFormer<sup class="ltx_sup" id="S5.T4.4.4.4.1.1"><span class="ltx_text ltx_font_italic" id="S5.T4.4.4.4.1.1.1">†</span></sup> (45M)</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S5.T4.6.6.6.4" style="padding:2.5pt 2.0pt;">34.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.5.5.5.2" style="padding:2.5pt 2.0pt;">GLP<sup class="ltx_sup" id="S5.T4.5.5.5.2.1"><span class="ltx_text ltx_font_italic" id="S5.T4.5.5.5.2.1.1">†</span></sup> (62M)</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S5.T4.6.6.6.5" style="padding:2.5pt 2.0pt;">0.344</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.6.6.6.6" style="padding:2.5pt 2.0pt;">ASTER</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.7" style="padding:2.5pt 2.0pt;">91.8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.8" style="padding:2.5pt 2.0pt;">76.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.9" style="padding:2.5pt 2.0pt;">89.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.10" style="padding:2.5pt 2.0pt;">93.4</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.11" style="padding:2.5pt 2.0pt;">78.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.12" style="padding:2.5pt 2.0pt;">79.5</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S5.T4.6.6.6.13" style="padding:2.5pt 2.0pt;">86.7</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.6.6.6.3" style="padding:2.5pt 2.0pt;">VizWiz_Cap <sup class="ltx_sup" id="S5.T4.6.6.6.3.1"><span class="ltx_text ltx_font_italic" id="S5.T4.6.6.6.3.1.1">†</span></sup>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.14" style="padding:2.5pt 2.0pt;">62.1</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S5.T4.6.6.6.15" style="padding:2.5pt 2.0pt;">48.2</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S5.T4.6.6.6.16" style="padding:2.5pt 2.0pt;">VizWiz_VQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T4.6.6.6.17" style="padding:2.5pt 2.0pt;">47.5</td>
</tr>
<tr class="ltx_tr" id="S5.T4.8.8.8">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.7.7.7.1" style="padding:2.5pt 2.0pt;">Mask2Former<sup class="ltx_sup" id="S5.T4.7.7.7.1.1"><span class="ltx_text ltx_font_italic" id="S5.T4.7.7.7.1.1.1">†</span></sup> (44M)</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T4.8.8.8.3" style="padding:2.5pt 2.0pt;">39.7</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.8.8.8.2" style="padding:2.5pt 2.0pt;">DPT*<sup class="ltx_sup" id="S5.T4.8.8.8.2.1"><span class="ltx_text ltx_font_italic" id="S5.T4.8.8.8.2.1.1">†</span></sup> (123M)</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T4.8.8.8.4" style="padding:2.5pt 2.0pt;">0.357</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.8.8.8.5" style="padding:2.5pt 2.0pt;">SEED</td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.8.6" style="padding:2.5pt 2.0pt;">92.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.8.7" style="padding:2.5pt 2.0pt;">80.0</td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.8.8" style="padding:2.5pt 2.0pt;">89.6</td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.8.9" style="padding:2.5pt 2.0pt;">93.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.8.10" style="padding:2.5pt 2.0pt;">81.4</td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.8.11" style="padding:2.5pt 2.0pt;">83.6</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T4.8.8.8.12" style="padding:2.5pt 2.0pt;">88.3</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.8.8.8.13" style="padding:2.5pt 2.0pt;">AoANet</td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.8.14" style="padding:2.5pt 2.0pt;">65.9</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T4.8.8.8.15" style="padding:2.5pt 2.0pt;">59.7</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.8.8.8.16" style="padding:2.5pt 2.0pt;">S-VQA</td>
<td class="ltx_td ltx_align_center" id="S5.T4.8.8.8.17" style="padding:2.5pt 2.0pt;">51.6</td>
</tr>
<tr class="ltx_tr" id="S5.T4.11.11.11">
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.9.9.9.1" style="padding:2.5pt 2.0pt;">kMaX-DeepLab<sup class="ltx_sup" id="S5.T4.9.9.9.1.1"><span class="ltx_text ltx_font_italic" id="S5.T4.9.9.9.1.1.1">†</span></sup> (57M)</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T4.11.11.11.4" style="padding:2.5pt 2.0pt;">41.5</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.10.10.10.2" style="padding:2.5pt 2.0pt;">BTS<sup class="ltx_sup" id="S5.T4.10.10.10.2.1"><span class="ltx_text ltx_font_italic" id="S5.T4.10.10.10.2.1.1">†</span></sup> (47M)</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T4.11.11.11.5" style="padding:2.5pt 2.0pt;">0.392</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.11.11.11.3" style="padding:2.5pt 2.0pt;">MaskOCR<sup class="ltx_sup" id="S5.T4.11.11.11.3.1"><span class="ltx_text ltx_font_italic" id="S5.T4.11.11.11.3.1.1">†</span></sup> (97M)</td>
<td class="ltx_td ltx_align_center" id="S5.T4.11.11.11.6" style="padding:2.5pt 2.0pt;">98.1</td>
<td class="ltx_td ltx_align_center" id="S5.T4.11.11.11.7" style="padding:2.5pt 2.0pt;">87.3</td>
<td class="ltx_td ltx_align_center" id="S5.T4.11.11.11.8" style="padding:2.5pt 2.0pt;">94.7</td>
<td class="ltx_td ltx_align_center" id="S5.T4.11.11.11.9" style="padding:2.5pt 2.0pt;">95.8</td>
<td class="ltx_td ltx_align_center" id="S5.T4.11.11.11.10" style="padding:2.5pt 2.0pt;">89.9</td>
<td class="ltx_td ltx_align_center" id="S5.T4.11.11.11.11" style="padding:2.5pt 2.0pt;">89.2</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T4.11.11.11.12" style="padding:2.5pt 2.0pt;">93.1</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.11.11.11.13" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="S5.T4.11.11.11.14" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S5.T4.11.11.11.15" style="padding:2.5pt 2.0pt;">–</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S5.T4.11.11.11.16" style="padding:2.5pt 2.0pt;">CS-VQA</td>
<td class="ltx_td ltx_align_center" id="S5.T4.11.11.11.17" style="padding:2.5pt 2.0pt;">53.2</td>
</tr>
<tr class="ltx_tr" id="S5.T4.11.11.14.1" style="background-color:#ECECEC;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.11.11.14.1.1" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.1.1" style="background-color:#ECECEC;">Ours (62M)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t" id="S5.T4.11.11.14.1.2" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.2.1" style="background-color:#ECECEC;">39.2</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.11.11.14.1.3" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.3.1" style="background-color:#ECECEC;">Ours (62M)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t" id="S5.T4.11.11.14.1.4" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.4.1" style="background-color:#ECECEC;">0.386</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.11.11.14.1.5" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.5.1" style="background-color:#ECECEC;">Ours (62M)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.11.11.14.1.6" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.6.1" style="background-color:#ECECEC;">97.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.11.11.14.1.7" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.7.1" style="background-color:#ECECEC;">84.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.11.11.14.1.8" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.8.1" style="background-color:#ECECEC;">92.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.11.11.14.1.9" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.9.1" style="background-color:#ECECEC;">90.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.11.11.14.1.10" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.10.1" style="background-color:#ECECEC;">88.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.11.11.14.1.11" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.11.1" style="background-color:#ECECEC;">93.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t" id="S5.T4.11.11.14.1.12" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.12.1" style="background-color:#ECECEC;">90.0</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.11.11.14.1.13" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.13.1" style="background-color:#ECECEC;">Ours (62M)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.11.11.14.1.14" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.14.1" style="background-color:#ECECEC;">60.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_rr ltx_border_t" id="S5.T4.11.11.14.1.15" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.15.1" style="background-color:#ECECEC;">45.1</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S5.T4.11.11.14.1.16" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.16.1" style="background-color:#ECECEC;">Ours (62M)</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T4.11.11.14.1.17" style="padding:2.5pt 2.0pt;"><span class="ltx_text" id="S5.T4.11.11.14.1.17.1" style="background-color:#ECECEC;">49.1</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T4.14.1.1" style="font-size:90%;">Table 4</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T4.15.2" style="font-size:90%;">Comparison of single-task training <span class="ltx_text ltx_font_smallcaps" id="S5.T4.15.2.1">@Model</span> and specialized SoTA models.<span class="ltx_text ltx_font_medium" id="S5.T4.15.2.2"> Note: “model (#params)” donates the number of parameters of the model. DPT* is trained with an extra dataset.
</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>Comparison with Specialized SoTA Models</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.3">Due to the scarcity of existing VLMs capable of encompassing all five tasks of our <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.3.1">@Bench</span>, to comprehensively showcase the effectiveness of our model, we compare our multi-task model with previous single-task SoTA models in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.T4" title="Table 4 ‣ 5.1 Comparison with Existing Generalist Models ‣ 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">4</span></a>. The representative works for each task are: MaskFormer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib7" title="">7</a>]</cite>, Mask2Former <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib6" title="">6</a>]</cite> and kMaX-DeepLab <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib44" title="">44</a>]</cite> for panoptic segmentation; BTS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib21" title="">21</a>]</cite>, DPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib32" title="">32</a>]</cite> and GLP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib19" title="">19</a>]</cite> for depth estimation; ASTER <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib35" title="">35</a>]</cite>, SEED <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib30" title="">30</a>]</cite> and MaskOCR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib25" title="">25</a>]</cite> for OCR; VizWiz_Cap, AoANet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib13" title="">13</a>]</cite> for captioning, VizWiz_VQA, S-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib18" title="">18</a>]</cite> and CS-VQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib15" title="">15</a>]</cite> for VQA.
Note that all these specialized methods are tailored for their respective specific tasks, while <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.3.2">@Model</span> is proposed to cover all these tasks.
<span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.3.3">@Model</span> can achieve, and in some cases, even surpass the single-task SoTA model on multiple tasks.
For example, <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.3.4">@Model</span> outperforms the MaskFormer (<math alttext="+4.5\%" class="ltx_Math" display="inline" id="S5.SS2.p1.1.m1.1"><semantics id="S5.SS2.p1.1.m1.1a"><mrow id="S5.SS2.p1.1.m1.1.1" xref="S5.SS2.p1.1.m1.1.1.cmml"><mo id="S5.SS2.p1.1.m1.1.1a" xref="S5.SS2.p1.1.m1.1.1.cmml">+</mo><mrow id="S5.SS2.p1.1.m1.1.1.2" xref="S5.SS2.p1.1.m1.1.1.2.cmml"><mn id="S5.SS2.p1.1.m1.1.1.2.2" xref="S5.SS2.p1.1.m1.1.1.2.2.cmml">4.5</mn><mo id="S5.SS2.p1.1.m1.1.1.2.1" xref="S5.SS2.p1.1.m1.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.1.m1.1b"><apply id="S5.SS2.p1.1.m1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"><plus id="S5.SS2.p1.1.m1.1.1.1.cmml" xref="S5.SS2.p1.1.m1.1.1"></plus><apply id="S5.SS2.p1.1.m1.1.1.2.cmml" xref="S5.SS2.p1.1.m1.1.1.2"><csymbol cd="latexml" id="S5.SS2.p1.1.m1.1.1.2.1.cmml" xref="S5.SS2.p1.1.m1.1.1.2.1">percent</csymbol><cn id="S5.SS2.p1.1.m1.1.1.2.2.cmml" type="float" xref="S5.SS2.p1.1.m1.1.1.2.2">4.5</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.1.m1.1c">+4.5\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.1.m1.1d">+ 4.5 %</annotation></semantics></math>) and gets comparable results with pre-trained models, such as Mask2Former and kMaX-DeepLab. Even for highly competitive OCR tasks, <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.3.5">@Model</span> can outperform non-pretrained SoTA models, ASTER and SEED, by <math alttext="3.3\%" class="ltx_Math" display="inline" id="S5.SS2.p1.2.m2.1"><semantics id="S5.SS2.p1.2.m2.1a"><mrow id="S5.SS2.p1.2.m2.1.1" xref="S5.SS2.p1.2.m2.1.1.cmml"><mn id="S5.SS2.p1.2.m2.1.1.2" xref="S5.SS2.p1.2.m2.1.1.2.cmml">3.3</mn><mo id="S5.SS2.p1.2.m2.1.1.1" xref="S5.SS2.p1.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.2.m2.1b"><apply id="S5.SS2.p1.2.m2.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1"><csymbol cd="latexml" id="S5.SS2.p1.2.m2.1.1.1.cmml" xref="S5.SS2.p1.2.m2.1.1.1">percent</csymbol><cn id="S5.SS2.p1.2.m2.1.1.2.cmml" type="float" xref="S5.SS2.p1.2.m2.1.1.2">3.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.2.m2.1c">3.3\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.2.m2.1d">3.3 %</annotation></semantics></math> and <math alttext="1.7\%" class="ltx_Math" display="inline" id="S5.SS2.p1.3.m3.1"><semantics id="S5.SS2.p1.3.m3.1a"><mrow id="S5.SS2.p1.3.m3.1.1" xref="S5.SS2.p1.3.m3.1.1.cmml"><mn id="S5.SS2.p1.3.m3.1.1.2" xref="S5.SS2.p1.3.m3.1.1.2.cmml">1.7</mn><mo id="S5.SS2.p1.3.m3.1.1.1" xref="S5.SS2.p1.3.m3.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS2.p1.3.m3.1b"><apply id="S5.SS2.p1.3.m3.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1"><csymbol cd="latexml" id="S5.SS2.p1.3.m3.1.1.1.cmml" xref="S5.SS2.p1.3.m3.1.1.1">percent</csymbol><cn id="S5.SS2.p1.3.m3.1.1.2.cmml" type="float" xref="S5.SS2.p1.3.m3.1.1.2">1.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS2.p1.3.m3.1c">1.7\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS2.p1.3.m3.1d">1.7 %</annotation></semantics></math>, respectively.
These results show that our <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.p1.3.6">@Model</span> significantly bridges the performance gap between generalist models and strong baselines.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3 </span>Multi-task Training <span class="ltx_text ltx_font_italic" id="S5.SS3.1.1">v.s.</span> Single-task Training</h3>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="412" id="S5.F5.g1" src="extracted/5870251/images/radar_plot.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S5.F5.3.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text ltx_font_bold" id="S5.F5.4.2" style="font-size:90%;">Single-task and multi-task training performance<span class="ltx_text ltx_font_medium" id="S5.F5.4.2.1"> (relative) against the specialized SoTA models on different tasks</span></span></figcaption>
</figure>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.F5" title="Figure 5 ‣ 5.3 Multi-task Training v.s. Single-task Training ‣ 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">5</span></a>, <span class="ltx_text ltx_font_smallcaps" id="S5.SS3.p1.1.1">@Model</span> performs less effectively in segmentation, depth estimation, and OCR under multi-task training, while demonstrating superior performance in captioning and VQA. We analyze that due to the huge OCR datasets, it is difficult to balance various tasks during multi-task training, resulting in a decline in OCR performance. For captioning and VQA, these two tasks are related to the scene understanding, they can promote each other during joint training. Furthermore, panoptic segmentation can increase the scene perception ability. See Appendix. for more analysis.</p>
</div>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T5.3" style="width:433.6pt;height:69.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-119.5pt,19.2pt) scale(0.644680511017219,0.644680511017219) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.3.3">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T5.3.3.4.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.4.1.1" rowspan="3" style="padding:2.5pt 4.0pt;"><span class="ltx_text" id="S5.T5.3.3.4.1.1.1">Method</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.3.4.1.2" rowspan="3" style="padding:2.5pt 4.0pt;"><span class="ltx_text" id="S5.T5.3.3.4.1.2.1">Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.3.4.1.3" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.3.4.1.3.1"><em class="ltx_emph ltx_font_italic" id="S5.T5.3.3.4.1.3.1.1">PS</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.3.4.1.4" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.3.4.1.4.1"><em class="ltx_emph ltx_font_italic" id="S5.T5.3.3.4.1.4.1.1">DE</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="7" id="S5.T5.3.3.4.1.5" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.3.4.1.5.1"><em class="ltx_emph ltx_font_italic" id="S5.T5.3.3.4.1.5.1.1">OCR</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T5.3.3.4.1.6" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.3.4.1.6.1"><em class="ltx_emph ltx_font_italic" id="S5.T5.3.3.4.1.6.1.1">IC</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S5.T5.3.3.4.1.7" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.3.3.4.1.7.1"><em class="ltx_emph ltx_font_italic" id="S5.T5.3.3.4.1.7.1.1">VQA</em></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T5.3.3.4.1.8" rowspan="3" style="padding:2.5pt 4.0pt;"><span class="ltx_text" id="S5.T5.3.3.4.1.8.1">#Params</span></th>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.5.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.3.3.5.2.1" style="padding:2.5pt 4.0pt;">ADE-150</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.3.3.5.2.2" style="padding:2.5pt 4.0pt;">NYU-V2</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.3.5.2.3" style="padding:2.5pt 4.0pt;">IC13</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.3.5.2.4" style="padding:2.5pt 4.0pt;">IC15</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.3.5.2.5" style="padding:2.5pt 4.0pt;">SVT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.3.5.2.6" style="padding:2.5pt 4.0pt;">IIIT5K</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.3.5.2.7" style="padding:2.5pt 4.0pt;">SVTP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.3.5.2.8" style="padding:2.5pt 4.0pt;">CUTE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.3.3.5.2.9" style="padding:2.5pt 4.0pt;">avg</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="2" id="S5.T5.3.3.5.2.10" style="padding:2.5pt 4.0pt;">VizWiz_Cap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.3.3.5.2.11" style="padding:2.5pt 4.0pt;">VizWiz_VQA</th>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.3.3.3.4" style="padding:2.5pt 4.0pt;">PQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.1.1.1.1" style="padding:2.5pt 4.0pt;">RMSE <math alttext="\downarrow" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.m1.1.1" stretchy="false" xref="S5.T5.1.1.1.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.1.m1.1d">↓</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" colspan="7" id="S5.T5.2.2.2.2" style="padding:2.5pt 4.0pt;">Acc (<math alttext="\%" class="ltx_Math" display="inline" id="S5.T5.2.2.2.2.m1.1"><semantics id="S5.T5.2.2.2.2.m1.1a"><mo id="S5.T5.2.2.2.2.m1.1.1" xref="S5.T5.2.2.2.2.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.2.m1.1b"><csymbol cd="latexml" id="S5.T5.2.2.2.2.m1.1.1.cmml" xref="S5.T5.2.2.2.2.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.2.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.2.2.m1.1d">%</annotation></semantics></math>)</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T5.3.3.3.5" style="padding:2.5pt 4.0pt;">B@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.3.3.3.6" style="padding:2.5pt 4.0pt;">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T5.3.3.3.3" style="padding:2.5pt 4.0pt;">Acc (<math alttext="\%" class="ltx_Math" display="inline" id="S5.T5.3.3.3.3.m1.1"><semantics id="S5.T5.3.3.3.3.m1.1a"><mo id="S5.T5.3.3.3.3.m1.1.1" xref="S5.T5.3.3.3.3.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.3.m1.1b"><csymbol cd="latexml" id="S5.T5.3.3.3.3.m1.1.1.cmml" xref="S5.T5.3.3.3.3.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.3.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.3.3.3.m1.1d">%</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T5.3.3.6.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.3.3.6.1.1" style="padding:2.5pt 4.0pt;">X-Decoder (our impl.)</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.6.1.2" style="padding:2.5pt 4.0pt;">original</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.6.1.3" style="padding:2.5pt 4.0pt;">37.7</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.6.1.4" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.6.1.5" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.6.1.6" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.6.1.7" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.6.1.8" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.6.1.9" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.6.1.10" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.6.1.11" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.6.1.12" style="padding:2.5pt 4.0pt;">57.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.6.1.13" style="padding:2.5pt 4.0pt;">46.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S5.T5.3.3.6.1.14" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T5.3.3.6.1.15" style="padding:2.5pt 4.0pt;">62M</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.7.2">
<td class="ltx_td ltx_align_left" id="S5.T5.3.3.7.2.1" style="padding:2.5pt 4.0pt;">X-Decoder (our impl.)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.2.2" style="padding:2.5pt 4.0pt;">multi-head</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.2.3" style="padding:2.5pt 4.0pt;">38.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.2.4" style="padding:2.5pt 4.0pt;">0.432</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.2.5" style="padding:2.5pt 4.0pt;">89.6</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.2.6" style="padding:2.5pt 4.0pt;">68.3</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.2.7" style="padding:2.5pt 4.0pt;">80.5</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.2.8" style="padding:2.5pt 4.0pt;">84.4</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.2.9" style="padding:2.5pt 4.0pt;">73.0</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.2.10" style="padding:2.5pt 4.0pt;">77.1</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.2.11" style="padding:2.5pt 4.0pt;">79.4</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.2.12" style="padding:2.5pt 4.0pt;">59.5</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.2.13" style="padding:2.5pt 4.0pt;">50.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T5.3.3.7.2.14" style="padding:2.5pt 4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="S5.T5.3.3.7.2.15" style="padding:2.5pt 4.0pt;">63M</td>
</tr>
<tr class="ltx_tr" id="S5.T5.3.3.8.3" style="background-color:#ECECEC;">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_t" id="S5.T5.3.3.8.3.1" style="padding:2.5pt 4.0pt;"><span class="ltx_text" id="S5.T5.3.3.8.3.1.1" style="background-color:#ECECEC;">Ours</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.3.3.8.3.2" style="padding:2.5pt 4.0pt;"><span class="ltx_text" id="S5.T5.3.3.8.3.2.1" style="background-color:#ECECEC;">task-prompt</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.3.3.8.3.3" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.3.1" style="background-color:#ECECEC;">38.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.3.3.8.3.4" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.4.1" style="background-color:#ECECEC;">0.425</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.3.3.8.3.5" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.5.1" style="background-color:#ECECEC;">90.2</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.3.3.8.3.6" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.6.1" style="background-color:#ECECEC;">68.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.3.3.8.3.7" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.7.1" style="background-color:#ECECEC;">81.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.3.3.8.3.8" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.8.1" style="background-color:#ECECEC;">84.9</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.3.3.8.3.9" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.9.1" style="background-color:#ECECEC;">73.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.3.3.8.3.10" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.10.1" style="background-color:#ECECEC;">77.8</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.3.3.8.3.11" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.11.1" style="background-color:#ECECEC;">80.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.3.3.8.3.12" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.12.1" style="background-color:#ECECEC;">61.0</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.3.3.8.3.13" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.13.1" style="background-color:#ECECEC;">52.5</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S5.T5.3.3.8.3.14" style="padding:2.5pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T5.3.3.8.3.14.1" style="background-color:#ECECEC;">53.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_t" id="S5.T5.3.3.8.3.15" style="padding:2.5pt 4.0pt;"><span class="ltx_text" id="S5.T5.3.3.8.3.15.1" style="background-color:#ECECEC;">62M</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T5.8.1.1" style="font-size:90%;">Table 5</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T5.9.2" style="font-size:90%;">Ablation study of task-specific prompt<span class="ltx_text ltx_font_medium" id="S5.T5.9.2.1"> in <span class="ltx_text ltx_font_smallcaps" id="S5.T5.9.2.1.1">@Model</span>. “our impl.” means our implement based on the original paper, “multi-head” means we add multiple output heads (a 3-layer MLP for each task) to the original model to achieve different tasks on <span class="ltx_text ltx_font_smallcaps" id="S5.T5.9.2.1.2">@Bench</span>.</span></span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4 </span>Efficiency-Performance Trade-off</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1"><span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.1">@Model</span> exhibits a favorable balance between efficiency and performance. As demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.T3" title="Table 3 ‣ 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">3</span></a>, <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.2">@Model</span> and the small version of Unified-IO have a similar number of parameters, yet <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.3">@Model</span> outperforms Unified-IO (S) in all tasks they have in common, and surpass the basic version of Unified-IO. In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.T4" title="Table 4 ‣ 5.1 Comparison with Existing Generalist Models ‣ 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">4</span></a>, with almost the same number of parameters, the performance of <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.4">@Model</span> is better than many single-task SoTA models without pre-training. This further demonstrates that <span class="ltx_text ltx_font_smallcaps" id="S5.SS4.p1.1.5">@Model</span> can achieve the SoTA level on multiple tasks with fewer parameters, which is crucial for ATs with limited computational capacity.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.5 </span>Ablation Study</h3>
<div class="ltx_para" id="S5.SS5.p1">
<p class="ltx_p" id="S5.SS5.p1.1">To dive deep into the model design and to investigate the effect of the proposed method, we conduct ablation studies of the task-specific prompt, the tokenizer and vocabularies.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p2">
<p class="ltx_p" id="S5.SS5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS5.p2.1.1">From Task-specific Prompt to Unified Input.</span>
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.T5" title="Table 5 ‣ 5.3 Multi-task Training v.s. Single-task Training ‣ 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">5</span></a>, the “image + prompt” training paradigm is demonstrated to be more concise and compatible with a wider range of tasks, outperforming multi-output heads training across all tasks. While there isn’t a significant difference in the number of parameters between the two approaches, one potential advantage of prompt-based training is its ability to use prompts to differentiate between various tasks during training earlier. This enables the model to extract distinct features for each task, leading to varied and improved results. During multi-output head training, models extract mixed features, and only the final output head is used to decode corresponding features for different tasks.
Besides, advanced output heads should be added when making the models with a multi-output head paradigm compatible with more tasks.
Therefore, for generalist models, the prompt-based training paradigm is more promising.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS5.p3">
<p class="ltx_p" id="S5.SS5.p3.2"><span class="ltx_text ltx_font_bold" id="S5.SS5.p3.2.1">Different Tokenizers and Vocabularies for OCR.</span>
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#S5.T6" title="Table 6 ‣ 5.5 Ablation Study ‣ 5 Experiments ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">6</span></a>, utilizing a character-based tokenizer can lead to a performance improvement of <math alttext="4.3\%" class="ltx_Math" display="inline" id="S5.SS5.p3.1.m1.1"><semantics id="S5.SS5.p3.1.m1.1a"><mrow id="S5.SS5.p3.1.m1.1.1" xref="S5.SS5.p3.1.m1.1.1.cmml"><mn id="S5.SS5.p3.1.m1.1.1.2" xref="S5.SS5.p3.1.m1.1.1.2.cmml">4.3</mn><mo id="S5.SS5.p3.1.m1.1.1.1" xref="S5.SS5.p3.1.m1.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.1.m1.1b"><apply id="S5.SS5.p3.1.m1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1"><csymbol cd="latexml" id="S5.SS5.p3.1.m1.1.1.1.cmml" xref="S5.SS5.p3.1.m1.1.1.1">percent</csymbol><cn id="S5.SS5.p3.1.m1.1.1.2.cmml" type="float" xref="S5.SS5.p3.1.m1.1.1.2">4.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.1.m1.1c">4.3\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p3.1.m1.1d">4.3 %</annotation></semantics></math>, and incorporating a limited vocabulary can further enhance performance by <math alttext="3.3\%" class="ltx_Math" display="inline" id="S5.SS5.p3.2.m2.1"><semantics id="S5.SS5.p3.2.m2.1a"><mrow id="S5.SS5.p3.2.m2.1.1" xref="S5.SS5.p3.2.m2.1.1.cmml"><mn id="S5.SS5.p3.2.m2.1.1.2" xref="S5.SS5.p3.2.m2.1.1.2.cmml">3.3</mn><mo id="S5.SS5.p3.2.m2.1.1.1" xref="S5.SS5.p3.2.m2.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S5.SS5.p3.2.m2.1b"><apply id="S5.SS5.p3.2.m2.1.1.cmml" xref="S5.SS5.p3.2.m2.1.1"><csymbol cd="latexml" id="S5.SS5.p3.2.m2.1.1.1.cmml" xref="S5.SS5.p3.2.m2.1.1.1">percent</csymbol><cn id="S5.SS5.p3.2.m2.1.1.2.cmml" type="float" xref="S5.SS5.p3.2.m2.1.1.2">3.3</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS5.p3.2.m2.1c">3.3\%</annotation><annotation encoding="application/x-llamapun" id="S5.SS5.p3.2.m2.1d">3.3 %</annotation></semantics></math>.
OCR solely recognizes image information, while captioning and VQA require processing and logical reasoning of image information to obtain reasonable answers. Therefore, when integrating tasks at different granularities, such as “relatively simple” OCR and “more complex” captioning into one model, it is suggested to carefully select different tokenizers and vocabularies to achieve the best performance on each task.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T6.1" style="width:216.8pt;height:63.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-44.2pt,13.0pt) scale(0.710462964009289,0.710462964009289) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T6.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T6.1.1.2.1.1" style="padding:2pt 2.0pt;">Tokenizer</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="S5.T6.1.1.2.1.2" style="padding:2pt 2.0pt;">Vocabulary</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.2.1.3" style="padding:2pt 2.0pt;">IC13</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.2.1.4" style="padding:2pt 2.0pt;">IC15</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.2.1.5" style="padding:2pt 2.0pt;">SVT</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.2.1.6" style="padding:2pt 2.0pt;">IIIT5K</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.2.1.7" style="padding:2pt 2.0pt;">SVTP</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.2.1.8" style="padding:2pt 2.0pt;">CUTE</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T6.1.1.2.1.9" style="padding:2pt 2.0pt;">avg</th>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.1.1.2" style="padding:2pt 2.0pt;">sub</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T6.1.1.1.3" style="padding:2pt 2.0pt;">ch</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="S5.T6.1.1.1.4" style="padding:2pt 2.0pt;">complete</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="S5.T6.1.1.1.5" style="padding:2pt 2.0pt;">limited</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="7" id="S5.T6.1.1.1.1" style="padding:2pt 2.0pt;">Acc (<math alttext="\%" class="ltx_Math" display="inline" id="S5.T6.1.1.1.1.m1.1"><semantics id="S5.T6.1.1.1.1.m1.1a"><mo id="S5.T6.1.1.1.1.m1.1.1" xref="S5.T6.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="S5.T6.1.1.1.1.m1.1b"><csymbol cd="latexml" id="S5.T6.1.1.1.1.m1.1.1.cmml" xref="S5.T6.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.1.1.1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="S5.T6.1.1.1.1.m1.1d">%</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T6.1.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.1.1" style="padding:2pt 2.0pt;">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T6.1.1.3.1.2" style="padding:2pt 2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.1.3" style="padding:2pt 2.0pt;">✓</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S5.T6.1.1.3.1.4" style="padding:2pt 2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.1.5" style="padding:2pt 2.0pt;">95.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.1.6" style="padding:2pt 2.0pt;">73.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.1.7" style="padding:2pt 2.0pt;">90.1</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.1.8" style="padding:2pt 2.0pt;">82.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.1.9" style="padding:2pt 2.0pt;">84.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.1.10" style="padding:2pt 2.0pt;">81.6</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.1.1.3.1.11" style="padding:2pt 2.0pt;">82.4</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.4.2">
<td class="ltx_td" id="S5.T6.1.1.4.2.1" style="padding:2pt 2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S5.T6.1.1.4.2.2" style="padding:2pt 2.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.2.3" style="padding:2pt 2.0pt;">✓</td>
<td class="ltx_td ltx_border_r" id="S5.T6.1.1.4.2.4" style="padding:2pt 2.0pt;"></td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.2.5" style="padding:2pt 2.0pt;">95.8</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.2.6" style="padding:2pt 2.0pt;">80.0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.2.7" style="padding:2pt 2.0pt;">90.0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.2.8" style="padding:2pt 2.0pt;">87.5</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.2.9" style="padding:2pt 2.0pt;">85.0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.2.10" style="padding:2pt 2.0pt;">90.0</td>
<td class="ltx_td ltx_align_center" id="S5.T6.1.1.4.2.11" style="padding:2pt 2.0pt;">86.7</td>
</tr>
<tr class="ltx_tr" id="S5.T6.1.1.5.3">
<td class="ltx_td ltx_border_b" id="S5.T6.1.1.5.3.1" style="padding:2pt 2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.1.1.5.3.2" style="padding:2pt 2.0pt;">✓</td>
<td class="ltx_td ltx_border_b" id="S5.T6.1.1.5.3.3" style="padding:2pt 2.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="S5.T6.1.1.5.3.4" style="padding:2pt 2.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.1.5.3.5" style="padding:2pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.5.3.5.1">97.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.1.5.3.6" style="padding:2pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.5.3.6.1">84.4</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.1.5.3.7" style="padding:2pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.5.3.7.1">92.6</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.1.5.3.8" style="padding:2pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.5.3.8.1">90.3</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.1.5.3.9" style="padding:2pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.5.3.9.1">88.7</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.1.5.3.10" style="padding:2pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.5.3.10.1">93.1</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S5.T6.1.1.5.3.11" style="padding:2pt 2.0pt;"><span class="ltx_text ltx_font_bold" id="S5.T6.1.1.5.3.11.1">90.0</span></td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T6.4.1.1" style="font-size:90%;">Table 6</span>: </span><span class="ltx_text ltx_font_bold" id="S5.T6.5.2" style="font-size:90%;">Ablation study of tokenizer and vocabulary<span class="ltx_text ltx_font_medium" id="S5.T6.5.2.1"> for task text recognition. “sub” and “ch” denote subword-based and character-based, respectively.</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="S6.p1">
<p class="ltx_p" id="S6.p1.1"><span class="ltx_text ltx_font_bold" id="S6.p1.1.1">Qualitative Analysis of User Study.</span>
Guided by the human-in-the-loop user study, we create the benchmark <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.2">@Bench</span>. Score potential functions from multiple perspectives to ensure the selected functions and tasks are reasonable and important to PVIs. The five tasks in <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.3">@Bench</span> are closely related to these functions and are one of the ways to achieve these functions. Furthermore, based on these tasks implemented by <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.4">@Model</span>, we are prepared to carry out further assistive function development in the future, such as indoor obstacle avoidance, text recognition, common objects detection, initial understanding of unfamiliar scenes, <span class="ltx_text ltx_font_italic" id="S6.p1.1.5">etc</span>.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Additionally, we analyze the comments given by the blind user and list a few. About OCR, <span class="ltx_text ltx_font_italic" id="S6.p2.1.1">“Usability by completely blind people would be very important to me. Existing systems of this type have difficulty selecting the right target. Text recognition on a document in front of me or on packaging in my hand is also very possible with a smartphone. However, if the function is able to read door signs, hanging posters or street signs that are not within direct reach, that would be an extremely useful function.”</span>. According to the scores in the questionnaire and comments, PVIs attach great importance to text recognition, especially in some special scenarios. Therefore, unlike other generalist models that ignore OCR task, <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.2">@Bench</span> contains OCR task and introduce multiple OCR datasets, covering various text scenarios. About object recognition, <span class="ltx_text ltx_font_italic" id="S6.p2.1.3">“This is a function I would definitely expect!”, “It would be important, on the one hand, to have a high level of reliability of recognition and, on the other hand, to be able to determine, even for completely blind people, that the correct object is being recognized.”</span>. By introducing multi-category ADE20K and combining panoptic segmentation, <span class="ltx_text ltx_font_smallcaps" id="S6.p2.1.4">@Model</span> can recognize a large number of stuffs and things, compared with detection, semantic segmentation. At the same time, the more categories of objects are recognized, the accuracy of recognition will increase and the recognition will be more reliable. In sum, these positive comments and great suggestions inspire our work and provide guidance for future work of exploring VLMs for assistive technology.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.p3">
<p class="ltx_p" id="S6.p3.1"><span class="ltx_text ltx_font_bold" id="S6.p3.1.1">Future Directions.</span>
The extensive quantitative and qualitative results have demonstrated the strong effectiveness and efficiency of our <span class="ltx_text ltx_font_smallcaps" id="S6.p3.1.2">@Model</span> for a variety of assistance-related tasks at different granularities. Upon the current, we see two directions worth future explorations: (1) <span class="ltx_text ltx_font_italic" id="S6.p3.1.3">Pre-training</span>. Currently,
we did not perform pre-training and
<span class="ltx_text ltx_font_smallcaps" id="S6.p3.1.4">@Model</span> can reach a level close to the pre-trained SoTA. We believe that after pre-training, <span class="ltx_text ltx_font_smallcaps" id="S6.p3.1.5">@Model</span> can achieve higher performance. (2) <span class="ltx_text ltx_font_italic" id="S6.p3.1.6">Functions development and deployment</span>. Going back to the user study, we came up with the idea for work precisely because we understood the difficulties that PVIs encounter in daily life. Existing assistive systems generally can only implement one or a few functions.
A future work is to implement a multi-functional assistive system based on <span class="ltx_text ltx_font_smallcaps" id="S6.p3.1.7">@Model</span>.
More discussions locate in the supplementary material.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Conclusion</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">In this work, we introduce <span class="ltx_text ltx_font_smallcaps" id="S7.p1.1.1">@Bench</span>, a multi-modal, multi-task, multi-dimension benchmark for the evaluation of generalist VLMs that can empower assistive technology and help PVIs. Based on the human-in-the-loop user study with the target group, our <span class="ltx_text ltx_font_smallcaps" id="S7.p1.1.2">@Bench</span> not only considers 5 practical tasks closely related to the daily lives of PVIs, but also takes into account the efficiency guideline for VLMs.
Furthermore, we present a unified and multi-task <span class="ltx_text ltx_font_smallcaps" id="S7.p1.1.3">@Model</span> to address the multiple vision-language tasks. Thanks to the unified task-specific prompt design, our model can use one suit of parameters to address all 5 tasks and achieve competitive results. Extensive experiments and qualitative analysis prove the effectiveness of the proposed <span class="ltx_text ltx_font_smallcaps" id="S7.p1.1.4">@Bench</span> and <span class="ltx_text ltx_font_smallcaps" id="S7.p1.1.5">@Model</span> in helping PVIs.
We hope this work can provide inspiration for the design of next-generation assistive systems for helping PVIs.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<br class="ltx_break"/>
<p class="ltx_p" id="S7.p2.1"><span class="ltx_text ltx_font_bold" id="S7.p2.1.1">Acknowledgement.</span>
This work was supported in part by the Ministry of Science, Research and the Arts of Baden-Wurttemberg (MWK) through the Cooperative Graduate School Accessibility through AI-based Assistive Technology (KATE) under Grant BW6-03, in part by the Federal Ministry of Education and Research (BMBF) through a fellowship within the IFI program of the German Academic Exchange Service (DAAD), and in part by Future Mobility Grants from InnovationCampus Future Mobility (ICM).
We thank HoreKA@KIT, HAICORE@KIT, and bwHPC supercomputer partitions.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
Aitor Aladren, Gonzalo López-Nicolás, Luis Puig, and Josechu J Guerrero.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">Navigation assistance for the visually impaired using rgb-d sensor with range expansion.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.3.1" style="font-size:90%;">IEEE Systems Journal</span><span class="ltx_text" id="bib.bib1.4.2" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
Reza Akbarian Bafghi and Danna Gurari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">A new dataset based on images taken by blind people for testing the robustness of image classification models trained for imagenet categories.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib2.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib2.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
Jeffrey P Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samual White, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Vizwiz: nearly real-time answers to visual questions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib3.4.2" style="font-size:90%;">UIST</span><span class="ltx_text" id="bib.bib3.5.3" style="font-size:90%;">, 2010.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">End-to-end object detection with transformers.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib4.4.2" style="font-size:90%;">European conference on computer vision</span><span class="ltx_text" id="bib.bib4.5.3" style="font-size:90%;">, pages 213–229. Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Pali: A jointly-scaled multilingual language-image model.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.3.1" style="font-size:90%;">arXiv preprint arXiv:2209.06794</span><span class="ltx_text" id="bib.bib5.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">Masked-attention mask transformer for universal image segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib6.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib6.5.3" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
Bowen Cheng, Alex Schwing, and Alexander Kirillov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Per-pixel classification is not all you need for semantic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.3.1" style="font-size:90%;">NeurIPS</span><span class="ltx_text" id="bib.bib7.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
David Eigen, Christian Puhrsch, and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">Depth map prediction from a single image using a multi-scale deep network.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib8.3.1" style="font-size:90%;">Advances in neural information processing systems</span><span class="ltx_text" id="bib.bib8.4.2" style="font-size:90%;">, 27, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
Danna Gurari, Qing Li, Chi Lin, Yinan Zhao, Anhong Guo, Abigale Stangl, and Jeffrey P Bigham.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Vizwiz-priv: A dataset for recognizing the presence and purpose of private visual information in images taken by blind people.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib9.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib9.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
Danna Gurari, Qing Li, Abigale J Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P Bigham.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Vizwiz grand challenge: Answering visual questions from blind people.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib10.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib10.5.3" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
Danna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">Captioning images taken by people who are blind.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib11.4.2" style="font-size:90%;">ECCV</span><span class="ltx_text" id="bib.bib11.5.3" style="font-size:90%;">. Springer, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Language models are general-purpose interfaces.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.3.1" style="font-size:90%;">arXiv preprint arXiv:2206.06336</span><span class="ltx_text" id="bib.bib12.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Lun Huang, Wenmin Wang, Jie Chen, and Xiao-Yong Wei.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Attention on attention for image captioning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib13.4.2" style="font-size:90%;">ICCV</span><span class="ltx_text" id="bib.bib13.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Synthetic data and artificial neural networks for natural scene text recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.3.1" style="font-size:90%;">arXiv preprint arXiv:1406.2227</span><span class="ltx_text" id="bib.bib14.4.2" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
Rachana Jayaram, Shreya Maheshwari, Hemanth C, Sathvik N Jois, and Dr. Mamatha H.R.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Cross-attention with self-attention for vizwiz vqa.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib15.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib15.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Icdar 2015 competition on robust reading.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib16.4.2" style="font-size:90%;">ICDAR</span><span class="ltx_text" id="bib.bib16.5.3" style="font-size:90%;">. IEEE, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">Icdar 2013 robust reading competition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib17.4.2" style="font-size:90%;">2013 12th international conference on document analysis and recognition</span><span class="ltx_text" id="bib.bib17.5.3" style="font-size:90%;">. IEEE, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
Vahid Kazemi and Ali Elqursh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">Show, ask, attend, and answer: A strong baseline for visual question answering.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.3.1" style="font-size:90%;">arXiv preprint arXiv:1704.03162</span><span class="ltx_text" id="bib.bib18.4.2" style="font-size:90%;">, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
Doyeon Kim, Woonghyun Ka, Pyungwhan Ahn, Donggyu Joo, Sehwan Chun, and Junmo Kim.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">Global-local path networks for monocular depth estimation with vertical cutdepth.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.3.1" style="font-size:90%;">arXiv preprint arXiv:2201.07436</span><span class="ltx_text" id="bib.bib19.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Dollár.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">Panoptic segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib20.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib20.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
Jin Han Lee, Myung-Kyu Han, Dong Wook Ko, and Il Hong Suh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">From big to small: Multi-scale local planar guidance for monocular depth estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.3.1" style="font-size:90%;">arXiv preprint arXiv:1907.10326</span><span class="ltx_text" id="bib.bib21.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
Hao Li, Jinguo Zhu, Xiaohu Jiang, Xizhou Zhu, Hongsheng Li, Chun Yuan, Xiaohua Wang, Yu Qiao, Xiaogang Wang, Wenhai Wang, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">Uni-perceiver v2: A generalist model for large-scale vision and vision-language tasks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib22.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib22.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">Mmbench: Is your multi-modal model an all-around player?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.3.1" style="font-size:90%;">arXiv preprint arXiv:2307.06281</span><span class="ltx_text" id="bib.bib23.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Unified-io: A unified model for vision, language, and multi-modal tasks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.3.1" style="font-size:90%;">arXiv preprint arXiv:2206.08916</span><span class="ltx_text" id="bib.bib24.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
Pengyuan Lyu, Chengquan Zhang, Shanshan Liu, Meina Qiao, Yangliu Xu, Liang Wu, Kun Yao, Junyu Han, Errui Ding, and Jingdong Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">Maskocr: text recognition with masked encoder-decoder pretraining.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.3.1" style="font-size:90%;">arXiv preprint arXiv:2206.00311</span><span class="ltx_text" id="bib.bib25.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
Anand Mishra, Karteek Alahari, and CV Jawahar.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Scene text recognition using higher order language priors.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib26.4.2" style="font-size:90%;">BMVC</span><span class="ltx_text" id="bib.bib26.5.3" style="font-size:90%;">. BMVA, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Indoor segmentation and support inference from rgbd images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib27.4.2" style="font-size:90%;">ECCV</span><span class="ltx_text" id="bib.bib27.5.3" style="font-size:90%;">, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Bleu: a method for automatic evaluation of machine translation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib28.4.2" style="font-size:90%;">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</span><span class="ltx_text" id="bib.bib28.5.3" style="font-size:90%;">, 2002.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
Trung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, and Chew Lim Tan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Recognizing text with perspective distortion in natural scenes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib29.4.2" style="font-size:90%;">ICCV</span><span class="ltx_text" id="bib.bib29.5.3" style="font-size:90%;">, 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
Zhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weiping Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Seed: Semantics enhanced encoder-decoder framework for scene text recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib30.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib30.5.3" style="font-size:90%;">, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Learning transferable visual models from natural language supervision.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib31.4.2" style="font-size:90%;">ICML</span><span class="ltx_text" id="bib.bib31.5.3" style="font-size:90%;">. PMLR, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Vision transformers for dense prediction.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib32.4.2" style="font-size:90%;">ICCV</span><span class="ltx_text" id="bib.bib32.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
Jarek Reynolds, Chandra Kanth Nagesh, and Danna Gurari.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">Salient object detection for images taken by people with vision impairments.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib33.3.1" style="font-size:90%;">arXiv preprint arXiv:2301.05323</span><span class="ltx_text" id="bib.bib33.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
Anhar Risnumawan, Palaiahankote Shivakumara, Chee Seng Chan, and Chew Lim Tan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">A robust arbitrary text detection system for natural scene images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.3.1" style="font-size:90%;">Expert Systems with Applications</span><span class="ltx_text" id="bib.bib34.4.2" style="font-size:90%;">, 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
Baoguang Shi, Mingkun Yang, Xinggang Wang, Pengyuan Lyu, Cong Yao, and Xiang Bai.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Aster: An attentional scene text recognizer with flexible rectification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.3.1" style="font-size:90%;">TPAMI</span><span class="ltx_text" id="bib.bib35.4.2" style="font-size:90%;">, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
Lin Su, Nan Duan, Edward Cui, Lei Ji, Chenfei Wu, Huaishao Luo, Yongfei Liu, Ming Zhong, Taroon Bharti, and Arun Sacheti.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Gem: A general evaluation benchmark for multimodal tasks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.3.1" style="font-size:90%;">arXiv preprint arXiv:2106.09889</span><span class="ltx_text" id="bib.bib36.4.2" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
Wu Tang, De-er Liu, Xiaoli Zhao, Zenghui Chen, and Chen Zhao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">A dataset for the recognition of obstacles on blind sidewalk.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">Universal Access in the Information Society</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">Cider: Consensus-based image description evaluation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib38.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib38.5.3" style="font-size:90%;">, 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
Hsueh-Cheng Wang, Robert K Katzschmann, Santani Teng, Brandon Araki, Laura Giarré, and Daniela Rus.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">Enabling independent navigation for visually impaired people through a wearable vision-based feedback system.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib39.4.2" style="font-size:90%;">ICRA</span><span class="ltx_text" id="bib.bib39.5.3" style="font-size:90%;">. IEEE, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Git: A generative image-to-text transformer for vision and language.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.3.1" style="font-size:90%;">arXiv preprint arXiv:2205.14100</span><span class="ltx_text" id="bib.bib40.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
Kai Wang, Boris Babenko, and Serge Belongie.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">End-to-end scene text recognition.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib41.4.2" style="font-size:90%;">ICCV</span><span class="ltx_text" id="bib.bib41.5.3" style="font-size:90%;">. IEEE, 2011.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib42.4.2" style="font-size:90%;">ICML</span><span class="ltx_text" id="bib.bib42.5.3" style="font-size:90%;">. PMLR, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
Jianwei Yang, Chunyuan Li, Xiyang Dai, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">Focal modulation networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.3.1" style="font-size:90%;">Advances in Neural Information Processing Systems</span><span class="ltx_text" id="bib.bib43.4.2" style="font-size:90%;">, 35:4203–4217, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
Qihang Yu, Huiyu Wang, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">k-means mask transformer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib44.4.2" style="font-size:90%;">ECCV</span><span class="ltx_text" id="bib.bib44.5.3" style="font-size:90%;">. Springer, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">Glipv2: Unifying localization and vision-language understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib45.3.1" style="font-size:90%;">NeurIPS</span><span class="ltx_text" id="bib.bib45.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
Jiaming Zhang, Kailun Yang, Angela Constantinescu, Kunyu Peng, Karin Müller, and Rainer Stiefelhagen.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.2.1" style="font-size:90%;">Trans4trans: Efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib46.4.2" style="font-size:90%;">ICCV</span><span class="ltx_text" id="bib.bib46.5.3" style="font-size:90%;">, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
Xingli Zhang, Lei Liang, Shenglu Zhao, and Zhihui Wang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">Grfb-unet: A new multi-scale attention network with group receptive field block for tactile paving segmentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib47.3.1" style="font-size:90%;">Expert Systems with Applications</span><span class="ltx_text" id="bib.bib47.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
Wanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">Agieval: A human-centric benchmark for evaluating foundation models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib48.3.1" style="font-size:90%;">arXiv preprint arXiv:2304.06364</span><span class="ltx_text" id="bib.bib48.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">Semantic understanding of scenes through the ade20k dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib49.3.1" style="font-size:90%;">International Journal of Computer Vision</span><span class="ltx_text" id="bib.bib49.4.2" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.1.1" style="font-size:90%;">
Wangchunshu Zhou, Yan Zeng, Shizhe Diao, and Xinsong Zhang.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.2.1" style="font-size:90%;">Vlue: A multi-task benchmark for evaluating vision-language models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib50.3.1" style="font-size:90%;">arXiv preprint arXiv:2205.15237</span><span class="ltx_text" id="bib.bib50.4.2" style="font-size:90%;">, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.1.1" style="font-size:90%;">
Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.2.1" style="font-size:90%;">Generalized decoding for pixel, image, and language.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib51.4.2" style="font-size:90%;">CVPR</span><span class="ltx_text" id="bib.bib51.5.3" style="font-size:90%;">, 2023.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Model Architecture</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.2">The overall architecture of <span class="ltx_text ltx_font_smallcaps" id="A1.p1.2.1">@Model</span> is a generic encoder-decoder design as shown in main paper. We follow X-Decoder <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib51" title="">51</a>]</cite> to adapt Focal-T <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib43" title="">43</a>]</cite> as image encoder <math alttext="\mathbf{Enc_{I}}" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><msub id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml"><mi id="A1.p1.1.m1.1.1.2" xref="A1.p1.1.m1.1.1.2.cmml">𝐄𝐧𝐜</mi><mi id="A1.p1.1.m1.1.1.3" xref="A1.p1.1.m1.1.1.3.cmml">𝐈</mi></msub><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><apply id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.p1.1.m1.1.1.1.cmml" xref="A1.p1.1.m1.1.1">subscript</csymbol><ci id="A1.p1.1.m1.1.1.2.cmml" xref="A1.p1.1.m1.1.1.2">𝐄𝐧𝐜</ci><ci id="A1.p1.1.m1.1.1.3.cmml" xref="A1.p1.1.m1.1.1.3">𝐈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">\mathbf{Enc_{I}}</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">bold_Enc start_POSTSUBSCRIPT bold_I end_POSTSUBSCRIPT</annotation></semantics></math> and use a number of transformer layers as text encoder <math alttext="\mathbf{Enc_{T}}" class="ltx_Math" display="inline" id="A1.p1.2.m2.1"><semantics id="A1.p1.2.m2.1a"><msub id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml"><mi id="A1.p1.2.m2.1.1.2" xref="A1.p1.2.m2.1.1.2.cmml">𝐄𝐧𝐜</mi><mi id="A1.p1.2.m2.1.1.3" xref="A1.p1.2.m2.1.1.3.cmml">𝐓</mi></msub><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><apply id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A1.p1.2.m2.1.1.1.cmml" xref="A1.p1.2.m2.1.1">subscript</csymbol><ci id="A1.p1.2.m2.1.1.2.cmml" xref="A1.p1.2.m2.1.1.2">𝐄𝐧𝐜</ci><ci id="A1.p1.2.m2.1.1.3.cmml" xref="A1.p1.2.m2.1.1.3">𝐓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">\mathbf{Enc_{T}}</annotation><annotation encoding="application/x-llamapun" id="A1.p1.2.m2.1d">bold_Enc start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT</annotation></semantics></math>. Decoder is a common Transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib51" title="">51</a>]</cite> decoder structure with self- and cross-attention layers.</p>
</div>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1 </span>Formulation</h3>
<div class="ltx_para" id="A1.SS1.p1">
<p class="ltx_p" id="A1.SS1.p1.3">First, we use image encoder <math alttext="\mathbf{Enc_{I}}" class="ltx_Math" display="inline" id="A1.SS1.p1.1.m1.1"><semantics id="A1.SS1.p1.1.m1.1a"><msub id="A1.SS1.p1.1.m1.1.1" xref="A1.SS1.p1.1.m1.1.1.cmml"><mi id="A1.SS1.p1.1.m1.1.1.2" xref="A1.SS1.p1.1.m1.1.1.2.cmml">𝐄𝐧𝐜</mi><mi id="A1.SS1.p1.1.m1.1.1.3" xref="A1.SS1.p1.1.m1.1.1.3.cmml">𝐈</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.1.m1.1b"><apply id="A1.SS1.p1.1.m1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.1.m1.1.1.1.cmml" xref="A1.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="A1.SS1.p1.1.m1.1.1.2.cmml" xref="A1.SS1.p1.1.m1.1.1.2">𝐄𝐧𝐜</ci><ci id="A1.SS1.p1.1.m1.1.1.3.cmml" xref="A1.SS1.p1.1.m1.1.1.3">𝐈</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.1.m1.1c">\mathbf{Enc_{I}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.1.m1.1d">bold_Enc start_POSTSUBSCRIPT bold_I end_POSTSUBSCRIPT</annotation></semantics></math> to extract multi-scale features <math alttext="\mathbf{Z}" class="ltx_Math" display="inline" id="A1.SS1.p1.2.m2.1"><semantics id="A1.SS1.p1.2.m2.1a"><mi id="A1.SS1.p1.2.m2.1.1" xref="A1.SS1.p1.2.m2.1.1.cmml">𝐙</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.2.m2.1b"><ci id="A1.SS1.p1.2.m2.1.1.cmml" xref="A1.SS1.p1.2.m2.1.1">𝐙</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.2.m2.1c">\mathbf{Z}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.2.m2.1d">bold_Z</annotation></semantics></math> from input image <math alttext="\mathbf{I}{\in}\mathcal{R}^{H\times W\times 3}" class="ltx_Math" display="inline" id="A1.SS1.p1.3.m3.1"><semantics id="A1.SS1.p1.3.m3.1a"><mrow id="A1.SS1.p1.3.m3.1.1" xref="A1.SS1.p1.3.m3.1.1.cmml"><mi id="A1.SS1.p1.3.m3.1.1.2" xref="A1.SS1.p1.3.m3.1.1.2.cmml">𝐈</mi><mo id="A1.SS1.p1.3.m3.1.1.1" xref="A1.SS1.p1.3.m3.1.1.1.cmml">∈</mo><msup id="A1.SS1.p1.3.m3.1.1.3" xref="A1.SS1.p1.3.m3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A1.SS1.p1.3.m3.1.1.3.2" xref="A1.SS1.p1.3.m3.1.1.3.2.cmml">ℛ</mi><mrow id="A1.SS1.p1.3.m3.1.1.3.3" xref="A1.SS1.p1.3.m3.1.1.3.3.cmml"><mi id="A1.SS1.p1.3.m3.1.1.3.3.2" xref="A1.SS1.p1.3.m3.1.1.3.3.2.cmml">H</mi><mo id="A1.SS1.p1.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.3.m3.1.1.3.3.1.cmml">×</mo><mi id="A1.SS1.p1.3.m3.1.1.3.3.3" xref="A1.SS1.p1.3.m3.1.1.3.3.3.cmml">W</mi><mo id="A1.SS1.p1.3.m3.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.3.m3.1.1.3.3.1.cmml">×</mo><mn id="A1.SS1.p1.3.m3.1.1.3.3.4" xref="A1.SS1.p1.3.m3.1.1.3.3.4.cmml">3</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.3.m3.1b"><apply id="A1.SS1.p1.3.m3.1.1.cmml" xref="A1.SS1.p1.3.m3.1.1"><in id="A1.SS1.p1.3.m3.1.1.1.cmml" xref="A1.SS1.p1.3.m3.1.1.1"></in><ci id="A1.SS1.p1.3.m3.1.1.2.cmml" xref="A1.SS1.p1.3.m3.1.1.2">𝐈</ci><apply id="A1.SS1.p1.3.m3.1.1.3.cmml" xref="A1.SS1.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="A1.SS1.p1.3.m3.1.1.3.1.cmml" xref="A1.SS1.p1.3.m3.1.1.3">superscript</csymbol><ci id="A1.SS1.p1.3.m3.1.1.3.2.cmml" xref="A1.SS1.p1.3.m3.1.1.3.2">ℛ</ci><apply id="A1.SS1.p1.3.m3.1.1.3.3.cmml" xref="A1.SS1.p1.3.m3.1.1.3.3"><times id="A1.SS1.p1.3.m3.1.1.3.3.1.cmml" xref="A1.SS1.p1.3.m3.1.1.3.3.1"></times><ci id="A1.SS1.p1.3.m3.1.1.3.3.2.cmml" xref="A1.SS1.p1.3.m3.1.1.3.3.2">𝐻</ci><ci id="A1.SS1.p1.3.m3.1.1.3.3.3.cmml" xref="A1.SS1.p1.3.m3.1.1.3.3.3">𝑊</ci><cn id="A1.SS1.p1.3.m3.1.1.3.3.4.cmml" type="integer" xref="A1.SS1.p1.3.m3.1.1.3.3.4">3</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.3.m3.1c">\mathbf{I}{\in}\mathcal{R}^{H\times W\times 3}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.3.m3.1d">bold_I ∈ caligraphic_R start_POSTSUPERSCRIPT italic_H × italic_W × 3 end_POSTSUPERSCRIPT</annotation></semantics></math>:</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{Z}=\mathbf{Enc_{I}(I)}=\left\langle{\mathbf{z}_{l}}\right\rangle_{l=1}%
^{L}" class="ltx_Math" display="block" id="A1.E1.m1.2"><semantics id="A1.E1.m1.2a"><mrow id="A1.E1.m1.2.2" xref="A1.E1.m1.2.2.cmml"><mi id="A1.E1.m1.2.2.3" xref="A1.E1.m1.2.2.3.cmml">𝐙</mi><mo id="A1.E1.m1.2.2.4" xref="A1.E1.m1.2.2.4.cmml">=</mo><mrow id="A1.E1.m1.2.2.5" xref="A1.E1.m1.2.2.5.cmml"><msub id="A1.E1.m1.2.2.5.2" xref="A1.E1.m1.2.2.5.2.cmml"><mi id="A1.E1.m1.2.2.5.2.2" xref="A1.E1.m1.2.2.5.2.2.cmml">𝐄𝐧𝐜</mi><mi id="A1.E1.m1.2.2.5.2.3" xref="A1.E1.m1.2.2.5.2.3.cmml">𝐈</mi></msub><mo id="A1.E1.m1.2.2.5.1" xref="A1.E1.m1.2.2.5.1.cmml">⁢</mo><mrow id="A1.E1.m1.2.2.5.3.2" xref="A1.E1.m1.2.2.5.cmml"><mo id="A1.E1.m1.2.2.5.3.2.1" stretchy="false" xref="A1.E1.m1.2.2.5.cmml">(</mo><mi id="A1.E1.m1.1.1" xref="A1.E1.m1.1.1.cmml">𝐈</mi><mo id="A1.E1.m1.2.2.5.3.2.2" stretchy="false" xref="A1.E1.m1.2.2.5.cmml">)</mo></mrow></mrow><mo id="A1.E1.m1.2.2.6" xref="A1.E1.m1.2.2.6.cmml">=</mo><msubsup id="A1.E1.m1.2.2.1" xref="A1.E1.m1.2.2.1.cmml"><mrow id="A1.E1.m1.2.2.1.1.1.1" xref="A1.E1.m1.2.2.1.1.1.2.cmml"><mo id="A1.E1.m1.2.2.1.1.1.1.2" xref="A1.E1.m1.2.2.1.1.1.2.1.cmml">⟨</mo><msub id="A1.E1.m1.2.2.1.1.1.1.1" xref="A1.E1.m1.2.2.1.1.1.1.1.cmml"><mi id="A1.E1.m1.2.2.1.1.1.1.1.2" xref="A1.E1.m1.2.2.1.1.1.1.1.2.cmml">𝐳</mi><mi id="A1.E1.m1.2.2.1.1.1.1.1.3" xref="A1.E1.m1.2.2.1.1.1.1.1.3.cmml">l</mi></msub><mo id="A1.E1.m1.2.2.1.1.1.1.3" xref="A1.E1.m1.2.2.1.1.1.2.1.cmml">⟩</mo></mrow><mrow id="A1.E1.m1.2.2.1.1.3" xref="A1.E1.m1.2.2.1.1.3.cmml"><mi id="A1.E1.m1.2.2.1.1.3.2" xref="A1.E1.m1.2.2.1.1.3.2.cmml">l</mi><mo id="A1.E1.m1.2.2.1.1.3.1" xref="A1.E1.m1.2.2.1.1.3.1.cmml">=</mo><mn id="A1.E1.m1.2.2.1.1.3.3" xref="A1.E1.m1.2.2.1.1.3.3.cmml">1</mn></mrow><mi id="A1.E1.m1.2.2.1.3" xref="A1.E1.m1.2.2.1.3.cmml">L</mi></msubsup></mrow><annotation-xml encoding="MathML-Content" id="A1.E1.m1.2b"><apply id="A1.E1.m1.2.2.cmml" xref="A1.E1.m1.2.2"><and id="A1.E1.m1.2.2a.cmml" xref="A1.E1.m1.2.2"></and><apply id="A1.E1.m1.2.2b.cmml" xref="A1.E1.m1.2.2"><eq id="A1.E1.m1.2.2.4.cmml" xref="A1.E1.m1.2.2.4"></eq><ci id="A1.E1.m1.2.2.3.cmml" xref="A1.E1.m1.2.2.3">𝐙</ci><apply id="A1.E1.m1.2.2.5.cmml" xref="A1.E1.m1.2.2.5"><times id="A1.E1.m1.2.2.5.1.cmml" xref="A1.E1.m1.2.2.5.1"></times><apply id="A1.E1.m1.2.2.5.2.cmml" xref="A1.E1.m1.2.2.5.2"><csymbol cd="ambiguous" id="A1.E1.m1.2.2.5.2.1.cmml" xref="A1.E1.m1.2.2.5.2">subscript</csymbol><ci id="A1.E1.m1.2.2.5.2.2.cmml" xref="A1.E1.m1.2.2.5.2.2">𝐄𝐧𝐜</ci><ci id="A1.E1.m1.2.2.5.2.3.cmml" xref="A1.E1.m1.2.2.5.2.3">𝐈</ci></apply><ci id="A1.E1.m1.1.1.cmml" xref="A1.E1.m1.1.1">𝐈</ci></apply></apply><apply id="A1.E1.m1.2.2c.cmml" xref="A1.E1.m1.2.2"><eq id="A1.E1.m1.2.2.6.cmml" xref="A1.E1.m1.2.2.6"></eq><share href="https://arxiv.org/html/2409.14215v1#A1.E1.m1.2.2.5.cmml" id="A1.E1.m1.2.2d.cmml" xref="A1.E1.m1.2.2"></share><apply id="A1.E1.m1.2.2.1.cmml" xref="A1.E1.m1.2.2.1"><csymbol cd="ambiguous" id="A1.E1.m1.2.2.1.2.cmml" xref="A1.E1.m1.2.2.1">superscript</csymbol><apply id="A1.E1.m1.2.2.1.1.cmml" xref="A1.E1.m1.2.2.1"><csymbol cd="ambiguous" id="A1.E1.m1.2.2.1.1.2.cmml" xref="A1.E1.m1.2.2.1">subscript</csymbol><apply id="A1.E1.m1.2.2.1.1.1.2.cmml" xref="A1.E1.m1.2.2.1.1.1.1"><csymbol cd="latexml" id="A1.E1.m1.2.2.1.1.1.2.1.cmml" xref="A1.E1.m1.2.2.1.1.1.1.2">delimited-⟨⟩</csymbol><apply id="A1.E1.m1.2.2.1.1.1.1.1.cmml" xref="A1.E1.m1.2.2.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.E1.m1.2.2.1.1.1.1.1.1.cmml" xref="A1.E1.m1.2.2.1.1.1.1.1">subscript</csymbol><ci id="A1.E1.m1.2.2.1.1.1.1.1.2.cmml" xref="A1.E1.m1.2.2.1.1.1.1.1.2">𝐳</ci><ci id="A1.E1.m1.2.2.1.1.1.1.1.3.cmml" xref="A1.E1.m1.2.2.1.1.1.1.1.3">𝑙</ci></apply></apply><apply id="A1.E1.m1.2.2.1.1.3.cmml" xref="A1.E1.m1.2.2.1.1.3"><eq id="A1.E1.m1.2.2.1.1.3.1.cmml" xref="A1.E1.m1.2.2.1.1.3.1"></eq><ci id="A1.E1.m1.2.2.1.1.3.2.cmml" xref="A1.E1.m1.2.2.1.1.3.2">𝑙</ci><cn id="A1.E1.m1.2.2.1.1.3.3.cmml" type="integer" xref="A1.E1.m1.2.2.1.1.3.3">1</cn></apply></apply><ci id="A1.E1.m1.2.2.1.3.cmml" xref="A1.E1.m1.2.2.1.3">𝐿</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.E1.m1.2c">\mathbf{Z}=\mathbf{Enc_{I}(I)}=\left\langle{\mathbf{z}_{l}}\right\rangle_{l=1}%
^{L}</annotation><annotation encoding="application/x-llamapun" id="A1.E1.m1.2d">bold_Z = bold_Enc start_POSTSUBSCRIPT bold_I end_POSTSUBSCRIPT ( bold_I ) = ⟨ bold_z start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ⟩ start_POSTSUBSCRIPT italic_l = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A1.SS1.p1.13">where <math alttext="\mathbf{z}_{l}\in\mathcal{R}^{H_{l}\times W_{l}\times d}" class="ltx_Math" display="inline" id="A1.SS1.p1.4.m1.1"><semantics id="A1.SS1.p1.4.m1.1a"><mrow id="A1.SS1.p1.4.m1.1.1" xref="A1.SS1.p1.4.m1.1.1.cmml"><msub id="A1.SS1.p1.4.m1.1.1.2" xref="A1.SS1.p1.4.m1.1.1.2.cmml"><mi id="A1.SS1.p1.4.m1.1.1.2.2" xref="A1.SS1.p1.4.m1.1.1.2.2.cmml">𝐳</mi><mi id="A1.SS1.p1.4.m1.1.1.2.3" xref="A1.SS1.p1.4.m1.1.1.2.3.cmml">l</mi></msub><mo id="A1.SS1.p1.4.m1.1.1.1" xref="A1.SS1.p1.4.m1.1.1.1.cmml">∈</mo><msup id="A1.SS1.p1.4.m1.1.1.3" xref="A1.SS1.p1.4.m1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A1.SS1.p1.4.m1.1.1.3.2" xref="A1.SS1.p1.4.m1.1.1.3.2.cmml">ℛ</mi><mrow id="A1.SS1.p1.4.m1.1.1.3.3" xref="A1.SS1.p1.4.m1.1.1.3.3.cmml"><msub id="A1.SS1.p1.4.m1.1.1.3.3.2" xref="A1.SS1.p1.4.m1.1.1.3.3.2.cmml"><mi id="A1.SS1.p1.4.m1.1.1.3.3.2.2" xref="A1.SS1.p1.4.m1.1.1.3.3.2.2.cmml">H</mi><mi id="A1.SS1.p1.4.m1.1.1.3.3.2.3" xref="A1.SS1.p1.4.m1.1.1.3.3.2.3.cmml">l</mi></msub><mo id="A1.SS1.p1.4.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.4.m1.1.1.3.3.1.cmml">×</mo><msub id="A1.SS1.p1.4.m1.1.1.3.3.3" xref="A1.SS1.p1.4.m1.1.1.3.3.3.cmml"><mi id="A1.SS1.p1.4.m1.1.1.3.3.3.2" xref="A1.SS1.p1.4.m1.1.1.3.3.3.2.cmml">W</mi><mi id="A1.SS1.p1.4.m1.1.1.3.3.3.3" xref="A1.SS1.p1.4.m1.1.1.3.3.3.3.cmml">l</mi></msub><mo id="A1.SS1.p1.4.m1.1.1.3.3.1a" lspace="0.222em" rspace="0.222em" xref="A1.SS1.p1.4.m1.1.1.3.3.1.cmml">×</mo><mi id="A1.SS1.p1.4.m1.1.1.3.3.4" xref="A1.SS1.p1.4.m1.1.1.3.3.4.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.4.m1.1b"><apply id="A1.SS1.p1.4.m1.1.1.cmml" xref="A1.SS1.p1.4.m1.1.1"><in id="A1.SS1.p1.4.m1.1.1.1.cmml" xref="A1.SS1.p1.4.m1.1.1.1"></in><apply id="A1.SS1.p1.4.m1.1.1.2.cmml" xref="A1.SS1.p1.4.m1.1.1.2"><csymbol cd="ambiguous" id="A1.SS1.p1.4.m1.1.1.2.1.cmml" xref="A1.SS1.p1.4.m1.1.1.2">subscript</csymbol><ci id="A1.SS1.p1.4.m1.1.1.2.2.cmml" xref="A1.SS1.p1.4.m1.1.1.2.2">𝐳</ci><ci id="A1.SS1.p1.4.m1.1.1.2.3.cmml" xref="A1.SS1.p1.4.m1.1.1.2.3">𝑙</ci></apply><apply id="A1.SS1.p1.4.m1.1.1.3.cmml" xref="A1.SS1.p1.4.m1.1.1.3"><csymbol cd="ambiguous" id="A1.SS1.p1.4.m1.1.1.3.1.cmml" xref="A1.SS1.p1.4.m1.1.1.3">superscript</csymbol><ci id="A1.SS1.p1.4.m1.1.1.3.2.cmml" xref="A1.SS1.p1.4.m1.1.1.3.2">ℛ</ci><apply id="A1.SS1.p1.4.m1.1.1.3.3.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3"><times id="A1.SS1.p1.4.m1.1.1.3.3.1.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.1"></times><apply id="A1.SS1.p1.4.m1.1.1.3.3.2.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.2"><csymbol cd="ambiguous" id="A1.SS1.p1.4.m1.1.1.3.3.2.1.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.2">subscript</csymbol><ci id="A1.SS1.p1.4.m1.1.1.3.3.2.2.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.2.2">𝐻</ci><ci id="A1.SS1.p1.4.m1.1.1.3.3.2.3.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.2.3">𝑙</ci></apply><apply id="A1.SS1.p1.4.m1.1.1.3.3.3.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="A1.SS1.p1.4.m1.1.1.3.3.3.1.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.3">subscript</csymbol><ci id="A1.SS1.p1.4.m1.1.1.3.3.3.2.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.3.2">𝑊</ci><ci id="A1.SS1.p1.4.m1.1.1.3.3.3.3.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.3.3">𝑙</ci></apply><ci id="A1.SS1.p1.4.m1.1.1.3.3.4.cmml" xref="A1.SS1.p1.4.m1.1.1.3.3.4">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.4.m1.1c">\mathbf{z}_{l}\in\mathcal{R}^{H_{l}\times W_{l}\times d}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.4.m1.1d">bold_z start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT ∈ caligraphic_R start_POSTSUPERSCRIPT italic_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT × italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\{H_{l},W_{l}\}" class="ltx_Math" display="inline" id="A1.SS1.p1.5.m2.2"><semantics id="A1.SS1.p1.5.m2.2a"><mrow id="A1.SS1.p1.5.m2.2.2.2" xref="A1.SS1.p1.5.m2.2.2.3.cmml"><mo id="A1.SS1.p1.5.m2.2.2.2.3" stretchy="false" xref="A1.SS1.p1.5.m2.2.2.3.cmml">{</mo><msub id="A1.SS1.p1.5.m2.1.1.1.1" xref="A1.SS1.p1.5.m2.1.1.1.1.cmml"><mi id="A1.SS1.p1.5.m2.1.1.1.1.2" xref="A1.SS1.p1.5.m2.1.1.1.1.2.cmml">H</mi><mi id="A1.SS1.p1.5.m2.1.1.1.1.3" xref="A1.SS1.p1.5.m2.1.1.1.1.3.cmml">l</mi></msub><mo id="A1.SS1.p1.5.m2.2.2.2.4" xref="A1.SS1.p1.5.m2.2.2.3.cmml">,</mo><msub id="A1.SS1.p1.5.m2.2.2.2.2" xref="A1.SS1.p1.5.m2.2.2.2.2.cmml"><mi id="A1.SS1.p1.5.m2.2.2.2.2.2" xref="A1.SS1.p1.5.m2.2.2.2.2.2.cmml">W</mi><mi id="A1.SS1.p1.5.m2.2.2.2.2.3" xref="A1.SS1.p1.5.m2.2.2.2.2.3.cmml">l</mi></msub><mo id="A1.SS1.p1.5.m2.2.2.2.5" stretchy="false" xref="A1.SS1.p1.5.m2.2.2.3.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.5.m2.2b"><set id="A1.SS1.p1.5.m2.2.2.3.cmml" xref="A1.SS1.p1.5.m2.2.2.2"><apply id="A1.SS1.p1.5.m2.1.1.1.1.cmml" xref="A1.SS1.p1.5.m2.1.1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.5.m2.1.1.1.1.1.cmml" xref="A1.SS1.p1.5.m2.1.1.1.1">subscript</csymbol><ci id="A1.SS1.p1.5.m2.1.1.1.1.2.cmml" xref="A1.SS1.p1.5.m2.1.1.1.1.2">𝐻</ci><ci id="A1.SS1.p1.5.m2.1.1.1.1.3.cmml" xref="A1.SS1.p1.5.m2.1.1.1.1.3">𝑙</ci></apply><apply id="A1.SS1.p1.5.m2.2.2.2.2.cmml" xref="A1.SS1.p1.5.m2.2.2.2.2"><csymbol cd="ambiguous" id="A1.SS1.p1.5.m2.2.2.2.2.1.cmml" xref="A1.SS1.p1.5.m2.2.2.2.2">subscript</csymbol><ci id="A1.SS1.p1.5.m2.2.2.2.2.2.cmml" xref="A1.SS1.p1.5.m2.2.2.2.2.2">𝑊</ci><ci id="A1.SS1.p1.5.m2.2.2.2.2.3.cmml" xref="A1.SS1.p1.5.m2.2.2.2.2.3">𝑙</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.5.m2.2c">\{H_{l},W_{l}\}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.5.m2.2d">{ italic_H start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT , italic_W start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT }</annotation></semantics></math>
is the size of feature map at level <math alttext="l" class="ltx_Math" display="inline" id="A1.SS1.p1.6.m3.1"><semantics id="A1.SS1.p1.6.m3.1a"><mi id="A1.SS1.p1.6.m3.1.1" xref="A1.SS1.p1.6.m3.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.6.m3.1b"><ci id="A1.SS1.p1.6.m3.1.1.cmml" xref="A1.SS1.p1.6.m3.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.6.m3.1c">l</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.6.m3.1d">italic_l</annotation></semantics></math> and <math alttext="d" class="ltx_Math" display="inline" id="A1.SS1.p1.7.m4.1"><semantics id="A1.SS1.p1.7.m4.1a"><mi id="A1.SS1.p1.7.m4.1.1" xref="A1.SS1.p1.7.m4.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.7.m4.1b"><ci id="A1.SS1.p1.7.m4.1.1.cmml" xref="A1.SS1.p1.7.m4.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.7.m4.1c">d</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.7.m4.1d">italic_d</annotation></semantics></math> is the feature dimension. Then, we use the text encoder <math alttext="\mathbf{Enc_{T}}" class="ltx_Math" display="inline" id="A1.SS1.p1.8.m5.1"><semantics id="A1.SS1.p1.8.m5.1a"><msub id="A1.SS1.p1.8.m5.1.1" xref="A1.SS1.p1.8.m5.1.1.cmml"><mi id="A1.SS1.p1.8.m5.1.1.2" xref="A1.SS1.p1.8.m5.1.1.2.cmml">𝐄𝐧𝐜</mi><mi id="A1.SS1.p1.8.m5.1.1.3" xref="A1.SS1.p1.8.m5.1.1.3.cmml">𝐓</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.8.m5.1b"><apply id="A1.SS1.p1.8.m5.1.1.cmml" xref="A1.SS1.p1.8.m5.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.8.m5.1.1.1.cmml" xref="A1.SS1.p1.8.m5.1.1">subscript</csymbol><ci id="A1.SS1.p1.8.m5.1.1.2.cmml" xref="A1.SS1.p1.8.m5.1.1.2">𝐄𝐧𝐜</ci><ci id="A1.SS1.p1.8.m5.1.1.3.cmml" xref="A1.SS1.p1.8.m5.1.1.3">𝐓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.8.m5.1c">\mathbf{Enc_{T}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.8.m5.1d">bold_Enc start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT</annotation></semantics></math> to encode a task-specific prompt into <math alttext="\textbf{P}=\left\langle{p_{1}},\cdot\cdot\cdot,{p_{n}}\right\rangle" class="ltx_Math" display="inline" id="A1.SS1.p1.9.m6.3"><semantics id="A1.SS1.p1.9.m6.3a"><mrow id="A1.SS1.p1.9.m6.3.3" xref="A1.SS1.p1.9.m6.3.3.cmml"><mtext class="ltx_mathvariant_bold" id="A1.SS1.p1.9.m6.3.3.4" xref="A1.SS1.p1.9.m6.3.3.4a.cmml">P</mtext><mo id="A1.SS1.p1.9.m6.3.3.3" xref="A1.SS1.p1.9.m6.3.3.3.cmml">=</mo><mrow id="A1.SS1.p1.9.m6.3.3.2.2" xref="A1.SS1.p1.9.m6.3.3.2.3.cmml"><mo id="A1.SS1.p1.9.m6.3.3.2.2.3" xref="A1.SS1.p1.9.m6.3.3.2.3.cmml">⟨</mo><msub id="A1.SS1.p1.9.m6.2.2.1.1.1" xref="A1.SS1.p1.9.m6.2.2.1.1.1.cmml"><mi id="A1.SS1.p1.9.m6.2.2.1.1.1.2" xref="A1.SS1.p1.9.m6.2.2.1.1.1.2.cmml">p</mi><mn id="A1.SS1.p1.9.m6.2.2.1.1.1.3" xref="A1.SS1.p1.9.m6.2.2.1.1.1.3.cmml">1</mn></msub><mo id="A1.SS1.p1.9.m6.3.3.2.2.4" xref="A1.SS1.p1.9.m6.3.3.2.3.cmml">,</mo><mi id="A1.SS1.p1.9.m6.1.1" mathvariant="normal" xref="A1.SS1.p1.9.m6.1.1.cmml">⋯</mi><mo id="A1.SS1.p1.9.m6.3.3.2.2.5" xref="A1.SS1.p1.9.m6.3.3.2.3.cmml">,</mo><msub id="A1.SS1.p1.9.m6.3.3.2.2.2" xref="A1.SS1.p1.9.m6.3.3.2.2.2.cmml"><mi id="A1.SS1.p1.9.m6.3.3.2.2.2.2" xref="A1.SS1.p1.9.m6.3.3.2.2.2.2.cmml">p</mi><mi id="A1.SS1.p1.9.m6.3.3.2.2.2.3" xref="A1.SS1.p1.9.m6.3.3.2.2.2.3.cmml">n</mi></msub><mo id="A1.SS1.p1.9.m6.3.3.2.2.6" xref="A1.SS1.p1.9.m6.3.3.2.3.cmml">⟩</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.9.m6.3b"><apply id="A1.SS1.p1.9.m6.3.3.cmml" xref="A1.SS1.p1.9.m6.3.3"><eq id="A1.SS1.p1.9.m6.3.3.3.cmml" xref="A1.SS1.p1.9.m6.3.3.3"></eq><ci id="A1.SS1.p1.9.m6.3.3.4a.cmml" xref="A1.SS1.p1.9.m6.3.3.4"><mtext class="ltx_mathvariant_bold" id="A1.SS1.p1.9.m6.3.3.4.cmml" xref="A1.SS1.p1.9.m6.3.3.4">P</mtext></ci><list id="A1.SS1.p1.9.m6.3.3.2.3.cmml" xref="A1.SS1.p1.9.m6.3.3.2.2"><apply id="A1.SS1.p1.9.m6.2.2.1.1.1.cmml" xref="A1.SS1.p1.9.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.9.m6.2.2.1.1.1.1.cmml" xref="A1.SS1.p1.9.m6.2.2.1.1.1">subscript</csymbol><ci id="A1.SS1.p1.9.m6.2.2.1.1.1.2.cmml" xref="A1.SS1.p1.9.m6.2.2.1.1.1.2">𝑝</ci><cn id="A1.SS1.p1.9.m6.2.2.1.1.1.3.cmml" type="integer" xref="A1.SS1.p1.9.m6.2.2.1.1.1.3">1</cn></apply><ci id="A1.SS1.p1.9.m6.1.1.cmml" xref="A1.SS1.p1.9.m6.1.1">⋯</ci><apply id="A1.SS1.p1.9.m6.3.3.2.2.2.cmml" xref="A1.SS1.p1.9.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="A1.SS1.p1.9.m6.3.3.2.2.2.1.cmml" xref="A1.SS1.p1.9.m6.3.3.2.2.2">subscript</csymbol><ci id="A1.SS1.p1.9.m6.3.3.2.2.2.2.cmml" xref="A1.SS1.p1.9.m6.3.3.2.2.2.2">𝑝</ci><ci id="A1.SS1.p1.9.m6.3.3.2.2.2.3.cmml" xref="A1.SS1.p1.9.m6.3.3.2.2.2.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.9.m6.3c">\textbf{P}=\left\langle{p_{1}},\cdot\cdot\cdot,{p_{n}}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.9.m6.3d">P = ⟨ italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , ⋯ , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ⟩</annotation></semantics></math> of length <math alttext="n" class="ltx_Math" display="inline" id="A1.SS1.p1.10.m7.1"><semantics id="A1.SS1.p1.10.m7.1a"><mi id="A1.SS1.p1.10.m7.1.1" xref="A1.SS1.p1.10.m7.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.10.m7.1b"><ci id="A1.SS1.p1.10.m7.1.1.cmml" xref="A1.SS1.p1.10.m7.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.10.m7.1c">n</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.10.m7.1d">italic_n</annotation></semantics></math>. Afterwards, we use the same text encoder <math alttext="\mathbf{Enc_{T}}" class="ltx_Math" display="inline" id="A1.SS1.p1.11.m8.1"><semantics id="A1.SS1.p1.11.m8.1a"><msub id="A1.SS1.p1.11.m8.1.1" xref="A1.SS1.p1.11.m8.1.1.cmml"><mi id="A1.SS1.p1.11.m8.1.1.2" xref="A1.SS1.p1.11.m8.1.1.2.cmml">𝐄𝐧𝐜</mi><mi id="A1.SS1.p1.11.m8.1.1.3" xref="A1.SS1.p1.11.m8.1.1.3.cmml">𝐓</mi></msub><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.11.m8.1b"><apply id="A1.SS1.p1.11.m8.1.1.cmml" xref="A1.SS1.p1.11.m8.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.11.m8.1.1.1.cmml" xref="A1.SS1.p1.11.m8.1.1">subscript</csymbol><ci id="A1.SS1.p1.11.m8.1.1.2.cmml" xref="A1.SS1.p1.11.m8.1.1.2">𝐄𝐧𝐜</ci><ci id="A1.SS1.p1.11.m8.1.1.3.cmml" xref="A1.SS1.p1.11.m8.1.1.3">𝐓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.11.m8.1c">\mathbf{Enc_{T}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.11.m8.1d">bold_Enc start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT</annotation></semantics></math> to encode a textual label into <math alttext="\mathbf{Q^{t}}=\left\langle{q_{1}}^{t},\cdot\cdot\cdot,{q_{n}}^{t}\right\rangle" class="ltx_Math" display="inline" id="A1.SS1.p1.12.m9.3"><semantics id="A1.SS1.p1.12.m9.3a"><mrow id="A1.SS1.p1.12.m9.3.3" xref="A1.SS1.p1.12.m9.3.3.cmml"><msup id="A1.SS1.p1.12.m9.3.3.4" xref="A1.SS1.p1.12.m9.3.3.4.cmml"><mi id="A1.SS1.p1.12.m9.3.3.4.2" xref="A1.SS1.p1.12.m9.3.3.4.2.cmml">𝐐</mi><mi id="A1.SS1.p1.12.m9.3.3.4.3" xref="A1.SS1.p1.12.m9.3.3.4.3.cmml">𝐭</mi></msup><mo id="A1.SS1.p1.12.m9.3.3.3" xref="A1.SS1.p1.12.m9.3.3.3.cmml">=</mo><mrow id="A1.SS1.p1.12.m9.3.3.2.2" xref="A1.SS1.p1.12.m9.3.3.2.3.cmml"><mo id="A1.SS1.p1.12.m9.3.3.2.2.3" xref="A1.SS1.p1.12.m9.3.3.2.3.cmml">⟨</mo><mmultiscripts id="A1.SS1.p1.12.m9.2.2.1.1.1" xref="A1.SS1.p1.12.m9.2.2.1.1.1.cmml"><mi id="A1.SS1.p1.12.m9.2.2.1.1.1.2.2" xref="A1.SS1.p1.12.m9.2.2.1.1.1.2.2.cmml">q</mi><mn id="A1.SS1.p1.12.m9.2.2.1.1.1.2.3" xref="A1.SS1.p1.12.m9.2.2.1.1.1.2.3.cmml">1</mn><mrow id="A1.SS1.p1.12.m9.2.2.1.1.1a" xref="A1.SS1.p1.12.m9.2.2.1.1.1.cmml"></mrow><mrow id="A1.SS1.p1.12.m9.2.2.1.1.1b" xref="A1.SS1.p1.12.m9.2.2.1.1.1.cmml"></mrow><mi id="A1.SS1.p1.12.m9.2.2.1.1.1.3" xref="A1.SS1.p1.12.m9.2.2.1.1.1.3.cmml">t</mi></mmultiscripts><mo id="A1.SS1.p1.12.m9.3.3.2.2.4" xref="A1.SS1.p1.12.m9.3.3.2.3.cmml">,</mo><mi id="A1.SS1.p1.12.m9.1.1" mathvariant="normal" xref="A1.SS1.p1.12.m9.1.1.cmml">⋯</mi><mo id="A1.SS1.p1.12.m9.3.3.2.2.5" xref="A1.SS1.p1.12.m9.3.3.2.3.cmml">,</mo><mmultiscripts id="A1.SS1.p1.12.m9.3.3.2.2.2" xref="A1.SS1.p1.12.m9.3.3.2.2.2.cmml"><mi id="A1.SS1.p1.12.m9.3.3.2.2.2.2.2" xref="A1.SS1.p1.12.m9.3.3.2.2.2.2.2.cmml">q</mi><mi id="A1.SS1.p1.12.m9.3.3.2.2.2.2.3" xref="A1.SS1.p1.12.m9.3.3.2.2.2.2.3.cmml">n</mi><mrow id="A1.SS1.p1.12.m9.3.3.2.2.2a" xref="A1.SS1.p1.12.m9.3.3.2.2.2.cmml"></mrow><mrow id="A1.SS1.p1.12.m9.3.3.2.2.2b" xref="A1.SS1.p1.12.m9.3.3.2.2.2.cmml"></mrow><mi id="A1.SS1.p1.12.m9.3.3.2.2.2.3" xref="A1.SS1.p1.12.m9.3.3.2.2.2.3.cmml">t</mi></mmultiscripts><mo id="A1.SS1.p1.12.m9.3.3.2.2.6" xref="A1.SS1.p1.12.m9.3.3.2.3.cmml">⟩</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.12.m9.3b"><apply id="A1.SS1.p1.12.m9.3.3.cmml" xref="A1.SS1.p1.12.m9.3.3"><eq id="A1.SS1.p1.12.m9.3.3.3.cmml" xref="A1.SS1.p1.12.m9.3.3.3"></eq><apply id="A1.SS1.p1.12.m9.3.3.4.cmml" xref="A1.SS1.p1.12.m9.3.3.4"><csymbol cd="ambiguous" id="A1.SS1.p1.12.m9.3.3.4.1.cmml" xref="A1.SS1.p1.12.m9.3.3.4">superscript</csymbol><ci id="A1.SS1.p1.12.m9.3.3.4.2.cmml" xref="A1.SS1.p1.12.m9.3.3.4.2">𝐐</ci><ci id="A1.SS1.p1.12.m9.3.3.4.3.cmml" xref="A1.SS1.p1.12.m9.3.3.4.3">𝐭</ci></apply><list id="A1.SS1.p1.12.m9.3.3.2.3.cmml" xref="A1.SS1.p1.12.m9.3.3.2.2"><apply id="A1.SS1.p1.12.m9.2.2.1.1.1.cmml" xref="A1.SS1.p1.12.m9.2.2.1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.12.m9.2.2.1.1.1.1.cmml" xref="A1.SS1.p1.12.m9.2.2.1.1.1">superscript</csymbol><apply id="A1.SS1.p1.12.m9.2.2.1.1.1.2.cmml" xref="A1.SS1.p1.12.m9.2.2.1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.12.m9.2.2.1.1.1.2.1.cmml" xref="A1.SS1.p1.12.m9.2.2.1.1.1">subscript</csymbol><ci id="A1.SS1.p1.12.m9.2.2.1.1.1.2.2.cmml" xref="A1.SS1.p1.12.m9.2.2.1.1.1.2.2">𝑞</ci><cn id="A1.SS1.p1.12.m9.2.2.1.1.1.2.3.cmml" type="integer" xref="A1.SS1.p1.12.m9.2.2.1.1.1.2.3">1</cn></apply><ci id="A1.SS1.p1.12.m9.2.2.1.1.1.3.cmml" xref="A1.SS1.p1.12.m9.2.2.1.1.1.3">𝑡</ci></apply><ci id="A1.SS1.p1.12.m9.1.1.cmml" xref="A1.SS1.p1.12.m9.1.1">⋯</ci><apply id="A1.SS1.p1.12.m9.3.3.2.2.2.cmml" xref="A1.SS1.p1.12.m9.3.3.2.2.2"><csymbol cd="ambiguous" id="A1.SS1.p1.12.m9.3.3.2.2.2.1.cmml" xref="A1.SS1.p1.12.m9.3.3.2.2.2">superscript</csymbol><apply id="A1.SS1.p1.12.m9.3.3.2.2.2.2.cmml" xref="A1.SS1.p1.12.m9.3.3.2.2.2"><csymbol cd="ambiguous" id="A1.SS1.p1.12.m9.3.3.2.2.2.2.1.cmml" xref="A1.SS1.p1.12.m9.3.3.2.2.2">subscript</csymbol><ci id="A1.SS1.p1.12.m9.3.3.2.2.2.2.2.cmml" xref="A1.SS1.p1.12.m9.3.3.2.2.2.2.2">𝑞</ci><ci id="A1.SS1.p1.12.m9.3.3.2.2.2.2.3.cmml" xref="A1.SS1.p1.12.m9.3.3.2.2.2.2.3">𝑛</ci></apply><ci id="A1.SS1.p1.12.m9.3.3.2.2.2.3.cmml" xref="A1.SS1.p1.12.m9.3.3.2.2.2.3">𝑡</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.12.m9.3c">\mathbf{Q^{t}}=\left\langle{q_{1}}^{t},\cdot\cdot\cdot,{q_{n}}^{t}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.12.m9.3d">bold_Q start_POSTSUPERSCRIPT bold_t end_POSTSUPERSCRIPT = ⟨ italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT , ⋯ , italic_q start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_t end_POSTSUPERSCRIPT ⟩</annotation></semantics></math> and create a latent queries <math alttext="\mathbf{Q^{l}}=\left\langle{q_{1}}^{l},\cdot\cdot\cdot,{q_{m}}^{l}\right\rangle" class="ltx_Math" display="inline" id="A1.SS1.p1.13.m10.3"><semantics id="A1.SS1.p1.13.m10.3a"><mrow id="A1.SS1.p1.13.m10.3.3" xref="A1.SS1.p1.13.m10.3.3.cmml"><msup id="A1.SS1.p1.13.m10.3.3.4" xref="A1.SS1.p1.13.m10.3.3.4.cmml"><mi id="A1.SS1.p1.13.m10.3.3.4.2" xref="A1.SS1.p1.13.m10.3.3.4.2.cmml">𝐐</mi><mi id="A1.SS1.p1.13.m10.3.3.4.3" xref="A1.SS1.p1.13.m10.3.3.4.3.cmml">𝐥</mi></msup><mo id="A1.SS1.p1.13.m10.3.3.3" xref="A1.SS1.p1.13.m10.3.3.3.cmml">=</mo><mrow id="A1.SS1.p1.13.m10.3.3.2.2" xref="A1.SS1.p1.13.m10.3.3.2.3.cmml"><mo id="A1.SS1.p1.13.m10.3.3.2.2.3" xref="A1.SS1.p1.13.m10.3.3.2.3.cmml">⟨</mo><mmultiscripts id="A1.SS1.p1.13.m10.2.2.1.1.1" xref="A1.SS1.p1.13.m10.2.2.1.1.1.cmml"><mi id="A1.SS1.p1.13.m10.2.2.1.1.1.2.2" xref="A1.SS1.p1.13.m10.2.2.1.1.1.2.2.cmml">q</mi><mn id="A1.SS1.p1.13.m10.2.2.1.1.1.2.3" xref="A1.SS1.p1.13.m10.2.2.1.1.1.2.3.cmml">1</mn><mrow id="A1.SS1.p1.13.m10.2.2.1.1.1a" xref="A1.SS1.p1.13.m10.2.2.1.1.1.cmml"></mrow><mrow id="A1.SS1.p1.13.m10.2.2.1.1.1b" xref="A1.SS1.p1.13.m10.2.2.1.1.1.cmml"></mrow><mi id="A1.SS1.p1.13.m10.2.2.1.1.1.3" xref="A1.SS1.p1.13.m10.2.2.1.1.1.3.cmml">l</mi></mmultiscripts><mo id="A1.SS1.p1.13.m10.3.3.2.2.4" xref="A1.SS1.p1.13.m10.3.3.2.3.cmml">,</mo><mi id="A1.SS1.p1.13.m10.1.1" mathvariant="normal" xref="A1.SS1.p1.13.m10.1.1.cmml">⋯</mi><mo id="A1.SS1.p1.13.m10.3.3.2.2.5" xref="A1.SS1.p1.13.m10.3.3.2.3.cmml">,</mo><mmultiscripts id="A1.SS1.p1.13.m10.3.3.2.2.2" xref="A1.SS1.p1.13.m10.3.3.2.2.2.cmml"><mi id="A1.SS1.p1.13.m10.3.3.2.2.2.2.2" xref="A1.SS1.p1.13.m10.3.3.2.2.2.2.2.cmml">q</mi><mi id="A1.SS1.p1.13.m10.3.3.2.2.2.2.3" xref="A1.SS1.p1.13.m10.3.3.2.2.2.2.3.cmml">m</mi><mrow id="A1.SS1.p1.13.m10.3.3.2.2.2a" xref="A1.SS1.p1.13.m10.3.3.2.2.2.cmml"></mrow><mrow id="A1.SS1.p1.13.m10.3.3.2.2.2b" xref="A1.SS1.p1.13.m10.3.3.2.2.2.cmml"></mrow><mi id="A1.SS1.p1.13.m10.3.3.2.2.2.3" xref="A1.SS1.p1.13.m10.3.3.2.2.2.3.cmml">l</mi></mmultiscripts><mo id="A1.SS1.p1.13.m10.3.3.2.2.6" xref="A1.SS1.p1.13.m10.3.3.2.3.cmml">⟩</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.13.m10.3b"><apply id="A1.SS1.p1.13.m10.3.3.cmml" xref="A1.SS1.p1.13.m10.3.3"><eq id="A1.SS1.p1.13.m10.3.3.3.cmml" xref="A1.SS1.p1.13.m10.3.3.3"></eq><apply id="A1.SS1.p1.13.m10.3.3.4.cmml" xref="A1.SS1.p1.13.m10.3.3.4"><csymbol cd="ambiguous" id="A1.SS1.p1.13.m10.3.3.4.1.cmml" xref="A1.SS1.p1.13.m10.3.3.4">superscript</csymbol><ci id="A1.SS1.p1.13.m10.3.3.4.2.cmml" xref="A1.SS1.p1.13.m10.3.3.4.2">𝐐</ci><ci id="A1.SS1.p1.13.m10.3.3.4.3.cmml" xref="A1.SS1.p1.13.m10.3.3.4.3">𝐥</ci></apply><list id="A1.SS1.p1.13.m10.3.3.2.3.cmml" xref="A1.SS1.p1.13.m10.3.3.2.2"><apply id="A1.SS1.p1.13.m10.2.2.1.1.1.cmml" xref="A1.SS1.p1.13.m10.2.2.1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.13.m10.2.2.1.1.1.1.cmml" xref="A1.SS1.p1.13.m10.2.2.1.1.1">superscript</csymbol><apply id="A1.SS1.p1.13.m10.2.2.1.1.1.2.cmml" xref="A1.SS1.p1.13.m10.2.2.1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.13.m10.2.2.1.1.1.2.1.cmml" xref="A1.SS1.p1.13.m10.2.2.1.1.1">subscript</csymbol><ci id="A1.SS1.p1.13.m10.2.2.1.1.1.2.2.cmml" xref="A1.SS1.p1.13.m10.2.2.1.1.1.2.2">𝑞</ci><cn id="A1.SS1.p1.13.m10.2.2.1.1.1.2.3.cmml" type="integer" xref="A1.SS1.p1.13.m10.2.2.1.1.1.2.3">1</cn></apply><ci id="A1.SS1.p1.13.m10.2.2.1.1.1.3.cmml" xref="A1.SS1.p1.13.m10.2.2.1.1.1.3">𝑙</ci></apply><ci id="A1.SS1.p1.13.m10.1.1.cmml" xref="A1.SS1.p1.13.m10.1.1">⋯</ci><apply id="A1.SS1.p1.13.m10.3.3.2.2.2.cmml" xref="A1.SS1.p1.13.m10.3.3.2.2.2"><csymbol cd="ambiguous" id="A1.SS1.p1.13.m10.3.3.2.2.2.1.cmml" xref="A1.SS1.p1.13.m10.3.3.2.2.2">superscript</csymbol><apply id="A1.SS1.p1.13.m10.3.3.2.2.2.2.cmml" xref="A1.SS1.p1.13.m10.3.3.2.2.2"><csymbol cd="ambiguous" id="A1.SS1.p1.13.m10.3.3.2.2.2.2.1.cmml" xref="A1.SS1.p1.13.m10.3.3.2.2.2">subscript</csymbol><ci id="A1.SS1.p1.13.m10.3.3.2.2.2.2.2.cmml" xref="A1.SS1.p1.13.m10.3.3.2.2.2.2.2">𝑞</ci><ci id="A1.SS1.p1.13.m10.3.3.2.2.2.2.3.cmml" xref="A1.SS1.p1.13.m10.3.3.2.2.2.2.3">𝑚</ci></apply><ci id="A1.SS1.p1.13.m10.3.3.2.2.2.3.cmml" xref="A1.SS1.p1.13.m10.3.3.2.2.2.3">𝑙</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.13.m10.3c">\mathbf{Q^{l}}=\left\langle{q_{1}}^{l},\cdot\cdot\cdot,{q_{m}}^{l}\right\rangle</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.13.m10.3d">bold_Q start_POSTSUPERSCRIPT bold_l end_POSTSUPERSCRIPT = ⟨ italic_q start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT , ⋯ , italic_q start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_l end_POSTSUPERSCRIPT ⟩</annotation></semantics></math> as inputs of decoder. All these features are fed into <span class="ltx_text ltx_font_smallcaps" id="A1.SS1.p1.13.1">@Model</span> to predict the outputs:</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\left\langle\mathbf{O^{p}},\mathbf{O^{s}}\right\rangle=\mathbf{@Model}(\left%
\langle\mathbf{P},\mathbf{Z}\right\rangle;\left\langle\mathbf{Q^{l}},\mathbf{Q%
^{t}}\right\rangle)," class="ltx_Math" display="block" id="A1.E2.m1.3"><semantics id="A1.E2.m1.3a"><mrow id="A1.E2.m1.3.3.1" xref="A1.E2.m1.3.3.1.1.cmml"><mrow id="A1.E2.m1.3.3.1.1" xref="A1.E2.m1.3.3.1.1.cmml"><mrow id="A1.E2.m1.3.3.1.1.2.2" xref="A1.E2.m1.3.3.1.1.2.3.cmml"><mo id="A1.E2.m1.3.3.1.1.2.2.3" xref="A1.E2.m1.3.3.1.1.2.3.cmml">⟨</mo><msup id="A1.E2.m1.3.3.1.1.1.1.1" xref="A1.E2.m1.3.3.1.1.1.1.1.cmml"><mi id="A1.E2.m1.3.3.1.1.1.1.1.2" xref="A1.E2.m1.3.3.1.1.1.1.1.2.cmml">𝐎</mi><mi id="A1.E2.m1.3.3.1.1.1.1.1.3" xref="A1.E2.m1.3.3.1.1.1.1.1.3.cmml">𝐩</mi></msup><mo id="A1.E2.m1.3.3.1.1.2.2.4" xref="A1.E2.m1.3.3.1.1.2.3.cmml">,</mo><msup id="A1.E2.m1.3.3.1.1.2.2.2" xref="A1.E2.m1.3.3.1.1.2.2.2.cmml"><mi id="A1.E2.m1.3.3.1.1.2.2.2.2" xref="A1.E2.m1.3.3.1.1.2.2.2.2.cmml">𝐎</mi><mi id="A1.E2.m1.3.3.1.1.2.2.2.3" xref="A1.E2.m1.3.3.1.1.2.2.2.3.cmml">𝐬</mi></msup><mo id="A1.E2.m1.3.3.1.1.2.2.5" xref="A1.E2.m1.3.3.1.1.2.3.cmml">⟩</mo></mrow><mo id="A1.E2.m1.3.3.1.1.5" xref="A1.E2.m1.3.3.1.1.5.cmml">=</mo><mrow id="A1.E2.m1.3.3.1.1.4" xref="A1.E2.m1.3.3.1.1.4.cmml"><mi id="A1.E2.m1.3.3.1.1.4.4" mathvariant="normal" xref="A1.E2.m1.3.3.1.1.4.4.cmml">@</mi><mo id="A1.E2.m1.3.3.1.1.4.3" xref="A1.E2.m1.3.3.1.1.4.3.cmml">⁢</mo><mi id="A1.E2.m1.3.3.1.1.4.5" xref="A1.E2.m1.3.3.1.1.4.5.cmml">𝐌𝐨𝐝𝐞𝐥</mi><mo id="A1.E2.m1.3.3.1.1.4.3a" xref="A1.E2.m1.3.3.1.1.4.3.cmml">⁢</mo><mrow id="A1.E2.m1.3.3.1.1.4.2.2" xref="A1.E2.m1.3.3.1.1.4.2.3.cmml"><mo id="A1.E2.m1.3.3.1.1.4.2.2.3" stretchy="false" xref="A1.E2.m1.3.3.1.1.4.2.3.cmml">(</mo><mrow id="A1.E2.m1.3.3.1.1.3.1.1.1.2" xref="A1.E2.m1.3.3.1.1.3.1.1.1.1.cmml"><mo id="A1.E2.m1.3.3.1.1.3.1.1.1.2.1" xref="A1.E2.m1.3.3.1.1.3.1.1.1.1.cmml">⟨</mo><mi id="A1.E2.m1.1.1" xref="A1.E2.m1.1.1.cmml">𝐏</mi><mo id="A1.E2.m1.3.3.1.1.3.1.1.1.2.2" xref="A1.E2.m1.3.3.1.1.3.1.1.1.1.cmml">,</mo><mi id="A1.E2.m1.2.2" xref="A1.E2.m1.2.2.cmml">𝐙</mi><mo id="A1.E2.m1.3.3.1.1.3.1.1.1.2.3" xref="A1.E2.m1.3.3.1.1.3.1.1.1.1.cmml">⟩</mo></mrow><mo id="A1.E2.m1.3.3.1.1.4.2.2.4" xref="A1.E2.m1.3.3.1.1.4.2.3.cmml">;</mo><mrow id="A1.E2.m1.3.3.1.1.4.2.2.2.2" xref="A1.E2.m1.3.3.1.1.4.2.2.2.3.cmml"><mo id="A1.E2.m1.3.3.1.1.4.2.2.2.2.3" xref="A1.E2.m1.3.3.1.1.4.2.2.2.3.cmml">⟨</mo><msup id="A1.E2.m1.3.3.1.1.4.2.2.2.1.1" xref="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.cmml"><mi id="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.2" xref="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.2.cmml">𝐐</mi><mi id="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.3" xref="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.3.cmml">𝐥</mi></msup><mo id="A1.E2.m1.3.3.1.1.4.2.2.2.2.4" xref="A1.E2.m1.3.3.1.1.4.2.2.2.3.cmml">,</mo><msup id="A1.E2.m1.3.3.1.1.4.2.2.2.2.2" xref="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.cmml"><mi id="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.2" xref="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.2.cmml">𝐐</mi><mi id="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.3" xref="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.3.cmml">𝐭</mi></msup><mo id="A1.E2.m1.3.3.1.1.4.2.2.2.2.5" xref="A1.E2.m1.3.3.1.1.4.2.2.2.3.cmml">⟩</mo></mrow><mo id="A1.E2.m1.3.3.1.1.4.2.2.5" stretchy="false" xref="A1.E2.m1.3.3.1.1.4.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="A1.E2.m1.3.3.1.2" xref="A1.E2.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.E2.m1.3b"><apply id="A1.E2.m1.3.3.1.1.cmml" xref="A1.E2.m1.3.3.1"><eq id="A1.E2.m1.3.3.1.1.5.cmml" xref="A1.E2.m1.3.3.1.1.5"></eq><list id="A1.E2.m1.3.3.1.1.2.3.cmml" xref="A1.E2.m1.3.3.1.1.2.2"><apply id="A1.E2.m1.3.3.1.1.1.1.1.cmml" xref="A1.E2.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="A1.E2.m1.3.3.1.1.1.1.1.1.cmml" xref="A1.E2.m1.3.3.1.1.1.1.1">superscript</csymbol><ci id="A1.E2.m1.3.3.1.1.1.1.1.2.cmml" xref="A1.E2.m1.3.3.1.1.1.1.1.2">𝐎</ci><ci id="A1.E2.m1.3.3.1.1.1.1.1.3.cmml" xref="A1.E2.m1.3.3.1.1.1.1.1.3">𝐩</ci></apply><apply id="A1.E2.m1.3.3.1.1.2.2.2.cmml" xref="A1.E2.m1.3.3.1.1.2.2.2"><csymbol cd="ambiguous" id="A1.E2.m1.3.3.1.1.2.2.2.1.cmml" xref="A1.E2.m1.3.3.1.1.2.2.2">superscript</csymbol><ci id="A1.E2.m1.3.3.1.1.2.2.2.2.cmml" xref="A1.E2.m1.3.3.1.1.2.2.2.2">𝐎</ci><ci id="A1.E2.m1.3.3.1.1.2.2.2.3.cmml" xref="A1.E2.m1.3.3.1.1.2.2.2.3">𝐬</ci></apply></list><apply id="A1.E2.m1.3.3.1.1.4.cmml" xref="A1.E2.m1.3.3.1.1.4"><times id="A1.E2.m1.3.3.1.1.4.3.cmml" xref="A1.E2.m1.3.3.1.1.4.3"></times><ci id="A1.E2.m1.3.3.1.1.4.4.cmml" xref="A1.E2.m1.3.3.1.1.4.4">@</ci><ci id="A1.E2.m1.3.3.1.1.4.5.cmml" xref="A1.E2.m1.3.3.1.1.4.5">𝐌𝐨𝐝𝐞𝐥</ci><list id="A1.E2.m1.3.3.1.1.4.2.3.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2"><list id="A1.E2.m1.3.3.1.1.3.1.1.1.1.cmml" xref="A1.E2.m1.3.3.1.1.3.1.1.1.2"><ci id="A1.E2.m1.1.1.cmml" xref="A1.E2.m1.1.1">𝐏</ci><ci id="A1.E2.m1.2.2.cmml" xref="A1.E2.m1.2.2">𝐙</ci></list><list id="A1.E2.m1.3.3.1.1.4.2.2.2.3.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2.2.2"><apply id="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2.2.1.1"><csymbol cd="ambiguous" id="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.1.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2.2.1.1">superscript</csymbol><ci id="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.2.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.2">𝐐</ci><ci id="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.3.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2.2.1.1.3">𝐥</ci></apply><apply id="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2.2.2.2"><csymbol cd="ambiguous" id="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.1.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2.2.2.2">superscript</csymbol><ci id="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.2.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.2">𝐐</ci><ci id="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.3.cmml" xref="A1.E2.m1.3.3.1.1.4.2.2.2.2.2.3">𝐭</ci></apply></list></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.E2.m1.3c">\left\langle\mathbf{O^{p}},\mathbf{O^{s}}\right\rangle=\mathbf{@Model}(\left%
\langle\mathbf{P},\mathbf{Z}\right\rangle;\left\langle\mathbf{Q^{l}},\mathbf{Q%
^{t}}\right\rangle),</annotation><annotation encoding="application/x-llamapun" id="A1.E2.m1.3d">⟨ bold_O start_POSTSUPERSCRIPT bold_p end_POSTSUPERSCRIPT , bold_O start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT ⟩ = @ bold_Model ( ⟨ bold_P , bold_Z ⟩ ; ⟨ bold_Q start_POSTSUPERSCRIPT bold_l end_POSTSUPERSCRIPT , bold_Q start_POSTSUPERSCRIPT bold_t end_POSTSUPERSCRIPT ⟩ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A1.SS1.p1.15">where <math alttext="\mathbf{O^{p}}" class="ltx_Math" display="inline" id="A1.SS1.p1.14.m1.1"><semantics id="A1.SS1.p1.14.m1.1a"><msup id="A1.SS1.p1.14.m1.1.1" xref="A1.SS1.p1.14.m1.1.1.cmml"><mi id="A1.SS1.p1.14.m1.1.1.2" xref="A1.SS1.p1.14.m1.1.1.2.cmml">𝐎</mi><mi id="A1.SS1.p1.14.m1.1.1.3" xref="A1.SS1.p1.14.m1.1.1.3.cmml">𝐩</mi></msup><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.14.m1.1b"><apply id="A1.SS1.p1.14.m1.1.1.cmml" xref="A1.SS1.p1.14.m1.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.14.m1.1.1.1.cmml" xref="A1.SS1.p1.14.m1.1.1">superscript</csymbol><ci id="A1.SS1.p1.14.m1.1.1.2.cmml" xref="A1.SS1.p1.14.m1.1.1.2">𝐎</ci><ci id="A1.SS1.p1.14.m1.1.1.3.cmml" xref="A1.SS1.p1.14.m1.1.1.3">𝐩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.14.m1.1c">\mathbf{O^{p}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.14.m1.1d">bold_O start_POSTSUPERSCRIPT bold_p end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{O^{s}}" class="ltx_Math" display="inline" id="A1.SS1.p1.15.m2.1"><semantics id="A1.SS1.p1.15.m2.1a"><msup id="A1.SS1.p1.15.m2.1.1" xref="A1.SS1.p1.15.m2.1.1.cmml"><mi id="A1.SS1.p1.15.m2.1.1.2" xref="A1.SS1.p1.15.m2.1.1.2.cmml">𝐎</mi><mi id="A1.SS1.p1.15.m2.1.1.3" xref="A1.SS1.p1.15.m2.1.1.3.cmml">𝐬</mi></msup><annotation-xml encoding="MathML-Content" id="A1.SS1.p1.15.m2.1b"><apply id="A1.SS1.p1.15.m2.1.1.cmml" xref="A1.SS1.p1.15.m2.1.1"><csymbol cd="ambiguous" id="A1.SS1.p1.15.m2.1.1.1.cmml" xref="A1.SS1.p1.15.m2.1.1">superscript</csymbol><ci id="A1.SS1.p1.15.m2.1.1.2.cmml" xref="A1.SS1.p1.15.m2.1.1.2">𝐎</ci><ci id="A1.SS1.p1.15.m2.1.1.3.cmml" xref="A1.SS1.p1.15.m2.1.1.3">𝐬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS1.p1.15.m2.1c">\mathbf{O^{s}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS1.p1.15.m2.1d">bold_O start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT</annotation></semantics></math> are the pixel-level outputs and token-level
semantic outputs, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2 </span>Tasks</h3>
<div class="ltx_para" id="A1.SS2.p1">
<p class="ltx_p" id="A1.SS2.p1.1">Based on the aforementioned designs, <span class="ltx_text ltx_font_smallcaps" id="A1.SS2.p1.1.1">@Model</span> can be effectively employed to integrate various vision and vision-language tasks by utilizing different input combinations.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p2">
<p class="ltx_p" id="A1.SS2.p2.3"><span class="ltx_text ltx_font_bold" id="A1.SS2.p2.3.1">Pixel-level Output Tasks. </span>
For these tasks, such as panoptic segmentation and depth estimation, there is no textual
label as input for decoder:</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{O^{p}}=\mathbf{@Model}(\left\langle\mathbf{P},\mathbf{Z}\right\rangle;%
\mathbf{Q^{l}})," class="ltx_Math" display="block" id="A1.E3.m1.3"><semantics id="A1.E3.m1.3a"><mrow id="A1.E3.m1.3.3.1" xref="A1.E3.m1.3.3.1.1.cmml"><mrow id="A1.E3.m1.3.3.1.1" xref="A1.E3.m1.3.3.1.1.cmml"><msup id="A1.E3.m1.3.3.1.1.4" xref="A1.E3.m1.3.3.1.1.4.cmml"><mi id="A1.E3.m1.3.3.1.1.4.2" xref="A1.E3.m1.3.3.1.1.4.2.cmml">𝐎</mi><mi id="A1.E3.m1.3.3.1.1.4.3" xref="A1.E3.m1.3.3.1.1.4.3.cmml">𝐩</mi></msup><mo id="A1.E3.m1.3.3.1.1.3" xref="A1.E3.m1.3.3.1.1.3.cmml">=</mo><mrow id="A1.E3.m1.3.3.1.1.2" xref="A1.E3.m1.3.3.1.1.2.cmml"><mi id="A1.E3.m1.3.3.1.1.2.4" mathvariant="normal" xref="A1.E3.m1.3.3.1.1.2.4.cmml">@</mi><mo id="A1.E3.m1.3.3.1.1.2.3" xref="A1.E3.m1.3.3.1.1.2.3.cmml">⁢</mo><mi id="A1.E3.m1.3.3.1.1.2.5" xref="A1.E3.m1.3.3.1.1.2.5.cmml">𝐌𝐨𝐝𝐞𝐥</mi><mo id="A1.E3.m1.3.3.1.1.2.3a" xref="A1.E3.m1.3.3.1.1.2.3.cmml">⁢</mo><mrow id="A1.E3.m1.3.3.1.1.2.2.2" xref="A1.E3.m1.3.3.1.1.2.2.3.cmml"><mo id="A1.E3.m1.3.3.1.1.2.2.2.3" stretchy="false" xref="A1.E3.m1.3.3.1.1.2.2.3.cmml">(</mo><mrow id="A1.E3.m1.3.3.1.1.1.1.1.1.2" xref="A1.E3.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="A1.E3.m1.3.3.1.1.1.1.1.1.2.1" xref="A1.E3.m1.3.3.1.1.1.1.1.1.1.cmml">⟨</mo><mi id="A1.E3.m1.1.1" xref="A1.E3.m1.1.1.cmml">𝐏</mi><mo id="A1.E3.m1.3.3.1.1.1.1.1.1.2.2" xref="A1.E3.m1.3.3.1.1.1.1.1.1.1.cmml">,</mo><mi id="A1.E3.m1.2.2" xref="A1.E3.m1.2.2.cmml">𝐙</mi><mo id="A1.E3.m1.3.3.1.1.1.1.1.1.2.3" xref="A1.E3.m1.3.3.1.1.1.1.1.1.1.cmml">⟩</mo></mrow><mo id="A1.E3.m1.3.3.1.1.2.2.2.4" xref="A1.E3.m1.3.3.1.1.2.2.3.cmml">;</mo><msup id="A1.E3.m1.3.3.1.1.2.2.2.2" xref="A1.E3.m1.3.3.1.1.2.2.2.2.cmml"><mi id="A1.E3.m1.3.3.1.1.2.2.2.2.2" xref="A1.E3.m1.3.3.1.1.2.2.2.2.2.cmml">𝐐</mi><mi id="A1.E3.m1.3.3.1.1.2.2.2.2.3" xref="A1.E3.m1.3.3.1.1.2.2.2.2.3.cmml">𝐥</mi></msup><mo id="A1.E3.m1.3.3.1.1.2.2.2.5" stretchy="false" xref="A1.E3.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="A1.E3.m1.3.3.1.2" xref="A1.E3.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.E3.m1.3b"><apply id="A1.E3.m1.3.3.1.1.cmml" xref="A1.E3.m1.3.3.1"><eq id="A1.E3.m1.3.3.1.1.3.cmml" xref="A1.E3.m1.3.3.1.1.3"></eq><apply id="A1.E3.m1.3.3.1.1.4.cmml" xref="A1.E3.m1.3.3.1.1.4"><csymbol cd="ambiguous" id="A1.E3.m1.3.3.1.1.4.1.cmml" xref="A1.E3.m1.3.3.1.1.4">superscript</csymbol><ci id="A1.E3.m1.3.3.1.1.4.2.cmml" xref="A1.E3.m1.3.3.1.1.4.2">𝐎</ci><ci id="A1.E3.m1.3.3.1.1.4.3.cmml" xref="A1.E3.m1.3.3.1.1.4.3">𝐩</ci></apply><apply id="A1.E3.m1.3.3.1.1.2.cmml" xref="A1.E3.m1.3.3.1.1.2"><times id="A1.E3.m1.3.3.1.1.2.3.cmml" xref="A1.E3.m1.3.3.1.1.2.3"></times><ci id="A1.E3.m1.3.3.1.1.2.4.cmml" xref="A1.E3.m1.3.3.1.1.2.4">@</ci><ci id="A1.E3.m1.3.3.1.1.2.5.cmml" xref="A1.E3.m1.3.3.1.1.2.5">𝐌𝐨𝐝𝐞𝐥</ci><list id="A1.E3.m1.3.3.1.1.2.2.3.cmml" xref="A1.E3.m1.3.3.1.1.2.2.2"><list id="A1.E3.m1.3.3.1.1.1.1.1.1.1.cmml" xref="A1.E3.m1.3.3.1.1.1.1.1.1.2"><ci id="A1.E3.m1.1.1.cmml" xref="A1.E3.m1.1.1">𝐏</ci><ci id="A1.E3.m1.2.2.cmml" xref="A1.E3.m1.2.2">𝐙</ci></list><apply id="A1.E3.m1.3.3.1.1.2.2.2.2.cmml" xref="A1.E3.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="A1.E3.m1.3.3.1.1.2.2.2.2.1.cmml" xref="A1.E3.m1.3.3.1.1.2.2.2.2">superscript</csymbol><ci id="A1.E3.m1.3.3.1.1.2.2.2.2.2.cmml" xref="A1.E3.m1.3.3.1.1.2.2.2.2.2">𝐐</ci><ci id="A1.E3.m1.3.3.1.1.2.2.2.2.3.cmml" xref="A1.E3.m1.3.3.1.1.2.2.2.2.3">𝐥</ci></apply></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.E3.m1.3c">\mathbf{O^{p}}=\mathbf{@Model}(\left\langle\mathbf{P},\mathbf{Z}\right\rangle;%
\mathbf{Q^{l}}),</annotation><annotation encoding="application/x-llamapun" id="A1.E3.m1.3d">bold_O start_POSTSUPERSCRIPT bold_p end_POSTSUPERSCRIPT = @ bold_Model ( ⟨ bold_P , bold_Z ⟩ ; bold_Q start_POSTSUPERSCRIPT bold_l end_POSTSUPERSCRIPT ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A1.SS2.p2.2">where <math alttext="\mathbf{O^{p}}" class="ltx_Math" display="inline" id="A1.SS2.p2.1.m1.1"><semantics id="A1.SS2.p2.1.m1.1a"><msup id="A1.SS2.p2.1.m1.1.1" xref="A1.SS2.p2.1.m1.1.1.cmml"><mi id="A1.SS2.p2.1.m1.1.1.2" xref="A1.SS2.p2.1.m1.1.1.2.cmml">𝐎</mi><mi id="A1.SS2.p2.1.m1.1.1.3" xref="A1.SS2.p2.1.m1.1.1.3.cmml">𝐩</mi></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.1.m1.1b"><apply id="A1.SS2.p2.1.m1.1.1.cmml" xref="A1.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS2.p2.1.m1.1.1.1.cmml" xref="A1.SS2.p2.1.m1.1.1">superscript</csymbol><ci id="A1.SS2.p2.1.m1.1.1.2.cmml" xref="A1.SS2.p2.1.m1.1.1.2">𝐎</ci><ci id="A1.SS2.p2.1.m1.1.1.3.cmml" xref="A1.SS2.p2.1.m1.1.1.3">𝐩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.1.m1.1c">\mathbf{O^{p}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.1.m1.1d">bold_O start_POSTSUPERSCRIPT bold_p end_POSTSUPERSCRIPT</annotation></semantics></math> has the same size of <math alttext="\mathbf{Q^{l}}" class="ltx_Math" display="inline" id="A1.SS2.p2.2.m2.1"><semantics id="A1.SS2.p2.2.m2.1a"><msup id="A1.SS2.p2.2.m2.1.1" xref="A1.SS2.p2.2.m2.1.1.cmml"><mi id="A1.SS2.p2.2.m2.1.1.2" xref="A1.SS2.p2.2.m2.1.1.2.cmml">𝐐</mi><mi id="A1.SS2.p2.2.m2.1.1.3" xref="A1.SS2.p2.2.m2.1.1.3.cmml">𝐥</mi></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p2.2.m2.1b"><apply id="A1.SS2.p2.2.m2.1.1.cmml" xref="A1.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS2.p2.2.m2.1.1.1.cmml" xref="A1.SS2.p2.2.m2.1.1">superscript</csymbol><ci id="A1.SS2.p2.2.m2.1.1.2.cmml" xref="A1.SS2.p2.2.m2.1.1.2">𝐐</ci><ci id="A1.SS2.p2.2.m2.1.1.3.cmml" xref="A1.SS2.p2.2.m2.1.1.3">𝐥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p2.2.m2.1c">\mathbf{Q^{l}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p2.2.m2.1d">bold_Q start_POSTSUPERSCRIPT bold_l end_POSTSUPERSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A1.SS2.p3">
<p class="ltx_p" id="A1.SS2.p3.3"><span class="ltx_text ltx_font_bold" id="A1.SS2.p3.3.1">Token-level Output Tasks.</span>
For OCR, captioning and VQA, they require both latent and text queries as inputs. Hence, Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A1.E2" title="Equation 2 ‣ A.1 Formulation ‣ Appendix A Model Architecture ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">2</span></a>) is adapted to:</p>
<table class="ltx_equation ltx_eqn_table" id="A1.E4">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathbf{O^{s}}=\mathbf{@Model}(\left\langle\mathbf{P},\mathbf{Z}\right\rangle;%
\left\langle\mathbf{Q^{l}},\mathbf{Q^{t}}\right\rangle)," class="ltx_Math" display="block" id="A1.E4.m1.3"><semantics id="A1.E4.m1.3a"><mrow id="A1.E4.m1.3.3.1" xref="A1.E4.m1.3.3.1.1.cmml"><mrow id="A1.E4.m1.3.3.1.1" xref="A1.E4.m1.3.3.1.1.cmml"><msup id="A1.E4.m1.3.3.1.1.4" xref="A1.E4.m1.3.3.1.1.4.cmml"><mi id="A1.E4.m1.3.3.1.1.4.2" xref="A1.E4.m1.3.3.1.1.4.2.cmml">𝐎</mi><mi id="A1.E4.m1.3.3.1.1.4.3" xref="A1.E4.m1.3.3.1.1.4.3.cmml">𝐬</mi></msup><mo id="A1.E4.m1.3.3.1.1.3" xref="A1.E4.m1.3.3.1.1.3.cmml">=</mo><mrow id="A1.E4.m1.3.3.1.1.2" xref="A1.E4.m1.3.3.1.1.2.cmml"><mi id="A1.E4.m1.3.3.1.1.2.4" mathvariant="normal" xref="A1.E4.m1.3.3.1.1.2.4.cmml">@</mi><mo id="A1.E4.m1.3.3.1.1.2.3" xref="A1.E4.m1.3.3.1.1.2.3.cmml">⁢</mo><mi id="A1.E4.m1.3.3.1.1.2.5" xref="A1.E4.m1.3.3.1.1.2.5.cmml">𝐌𝐨𝐝𝐞𝐥</mi><mo id="A1.E4.m1.3.3.1.1.2.3a" xref="A1.E4.m1.3.3.1.1.2.3.cmml">⁢</mo><mrow id="A1.E4.m1.3.3.1.1.2.2.2" xref="A1.E4.m1.3.3.1.1.2.2.3.cmml"><mo id="A1.E4.m1.3.3.1.1.2.2.2.3" stretchy="false" xref="A1.E4.m1.3.3.1.1.2.2.3.cmml">(</mo><mrow id="A1.E4.m1.3.3.1.1.1.1.1.1.2" xref="A1.E4.m1.3.3.1.1.1.1.1.1.1.cmml"><mo id="A1.E4.m1.3.3.1.1.1.1.1.1.2.1" xref="A1.E4.m1.3.3.1.1.1.1.1.1.1.cmml">⟨</mo><mi id="A1.E4.m1.1.1" xref="A1.E4.m1.1.1.cmml">𝐏</mi><mo id="A1.E4.m1.3.3.1.1.1.1.1.1.2.2" xref="A1.E4.m1.3.3.1.1.1.1.1.1.1.cmml">,</mo><mi id="A1.E4.m1.2.2" xref="A1.E4.m1.2.2.cmml">𝐙</mi><mo id="A1.E4.m1.3.3.1.1.1.1.1.1.2.3" xref="A1.E4.m1.3.3.1.1.1.1.1.1.1.cmml">⟩</mo></mrow><mo id="A1.E4.m1.3.3.1.1.2.2.2.4" xref="A1.E4.m1.3.3.1.1.2.2.3.cmml">;</mo><mrow id="A1.E4.m1.3.3.1.1.2.2.2.2.2" xref="A1.E4.m1.3.3.1.1.2.2.2.2.3.cmml"><mo id="A1.E4.m1.3.3.1.1.2.2.2.2.2.3" xref="A1.E4.m1.3.3.1.1.2.2.2.2.3.cmml">⟨</mo><msup id="A1.E4.m1.3.3.1.1.2.2.2.2.1.1" xref="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.cmml"><mi id="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.2" xref="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.2.cmml">𝐐</mi><mi id="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.3" xref="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.3.cmml">𝐥</mi></msup><mo id="A1.E4.m1.3.3.1.1.2.2.2.2.2.4" xref="A1.E4.m1.3.3.1.1.2.2.2.2.3.cmml">,</mo><msup id="A1.E4.m1.3.3.1.1.2.2.2.2.2.2" xref="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.cmml"><mi id="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.2" xref="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.2.cmml">𝐐</mi><mi id="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.3" xref="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.3.cmml">𝐭</mi></msup><mo id="A1.E4.m1.3.3.1.1.2.2.2.2.2.5" xref="A1.E4.m1.3.3.1.1.2.2.2.2.3.cmml">⟩</mo></mrow><mo id="A1.E4.m1.3.3.1.1.2.2.2.5" stretchy="false" xref="A1.E4.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow></mrow><mo id="A1.E4.m1.3.3.1.2" xref="A1.E4.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A1.E4.m1.3b"><apply id="A1.E4.m1.3.3.1.1.cmml" xref="A1.E4.m1.3.3.1"><eq id="A1.E4.m1.3.3.1.1.3.cmml" xref="A1.E4.m1.3.3.1.1.3"></eq><apply id="A1.E4.m1.3.3.1.1.4.cmml" xref="A1.E4.m1.3.3.1.1.4"><csymbol cd="ambiguous" id="A1.E4.m1.3.3.1.1.4.1.cmml" xref="A1.E4.m1.3.3.1.1.4">superscript</csymbol><ci id="A1.E4.m1.3.3.1.1.4.2.cmml" xref="A1.E4.m1.3.3.1.1.4.2">𝐎</ci><ci id="A1.E4.m1.3.3.1.1.4.3.cmml" xref="A1.E4.m1.3.3.1.1.4.3">𝐬</ci></apply><apply id="A1.E4.m1.3.3.1.1.2.cmml" xref="A1.E4.m1.3.3.1.1.2"><times id="A1.E4.m1.3.3.1.1.2.3.cmml" xref="A1.E4.m1.3.3.1.1.2.3"></times><ci id="A1.E4.m1.3.3.1.1.2.4.cmml" xref="A1.E4.m1.3.3.1.1.2.4">@</ci><ci id="A1.E4.m1.3.3.1.1.2.5.cmml" xref="A1.E4.m1.3.3.1.1.2.5">𝐌𝐨𝐝𝐞𝐥</ci><list id="A1.E4.m1.3.3.1.1.2.2.3.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2"><list id="A1.E4.m1.3.3.1.1.1.1.1.1.1.cmml" xref="A1.E4.m1.3.3.1.1.1.1.1.1.2"><ci id="A1.E4.m1.1.1.cmml" xref="A1.E4.m1.1.1">𝐏</ci><ci id="A1.E4.m1.2.2.cmml" xref="A1.E4.m1.2.2">𝐙</ci></list><list id="A1.E4.m1.3.3.1.1.2.2.2.2.3.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2.2.2"><apply id="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2.2.1.1"><csymbol cd="ambiguous" id="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.1.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2.2.1.1">superscript</csymbol><ci id="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.2.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.2">𝐐</ci><ci id="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.3.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2.2.1.1.3">𝐥</ci></apply><apply id="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.1.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2.2.2.2">superscript</csymbol><ci id="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.2.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.2">𝐐</ci><ci id="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.3.cmml" xref="A1.E4.m1.3.3.1.1.2.2.2.2.2.2.3">𝐭</ci></apply></list></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.E4.m1.3c">\mathbf{O^{s}}=\mathbf{@Model}(\left\langle\mathbf{P},\mathbf{Z}\right\rangle;%
\left\langle\mathbf{Q^{l}},\mathbf{Q^{t}}\right\rangle),</annotation><annotation encoding="application/x-llamapun" id="A1.E4.m1.3d">bold_O start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT = @ bold_Model ( ⟨ bold_P , bold_Z ⟩ ; ⟨ bold_Q start_POSTSUPERSCRIPT bold_l end_POSTSUPERSCRIPT , bold_Q start_POSTSUPERSCRIPT bold_t end_POSTSUPERSCRIPT ⟩ ) ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A1.SS2.p3.2">where <math alttext="\mathbf{O^{s}}" class="ltx_Math" display="inline" id="A1.SS2.p3.1.m1.1"><semantics id="A1.SS2.p3.1.m1.1a"><msup id="A1.SS2.p3.1.m1.1.1" xref="A1.SS2.p3.1.m1.1.1.cmml"><mi id="A1.SS2.p3.1.m1.1.1.2" xref="A1.SS2.p3.1.m1.1.1.2.cmml">𝐎</mi><mi id="A1.SS2.p3.1.m1.1.1.3" xref="A1.SS2.p3.1.m1.1.1.3.cmml">𝐬</mi></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.1.m1.1b"><apply id="A1.SS2.p3.1.m1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A1.SS2.p3.1.m1.1.1.1.cmml" xref="A1.SS2.p3.1.m1.1.1">superscript</csymbol><ci id="A1.SS2.p3.1.m1.1.1.2.cmml" xref="A1.SS2.p3.1.m1.1.1.2">𝐎</ci><ci id="A1.SS2.p3.1.m1.1.1.3.cmml" xref="A1.SS2.p3.1.m1.1.1.3">𝐬</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.1.m1.1c">\mathbf{O^{s}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p3.1.m1.1d">bold_O start_POSTSUPERSCRIPT bold_s end_POSTSUPERSCRIPT</annotation></semantics></math> correspondingly has equal size of <math alttext="\mathbf{Q^{t}}" class="ltx_Math" display="inline" id="A1.SS2.p3.2.m2.1"><semantics id="A1.SS2.p3.2.m2.1a"><msup id="A1.SS2.p3.2.m2.1.1" xref="A1.SS2.p3.2.m2.1.1.cmml"><mi id="A1.SS2.p3.2.m2.1.1.2" xref="A1.SS2.p3.2.m2.1.1.2.cmml">𝐐</mi><mi id="A1.SS2.p3.2.m2.1.1.3" xref="A1.SS2.p3.2.m2.1.1.3.cmml">𝐭</mi></msup><annotation-xml encoding="MathML-Content" id="A1.SS2.p3.2.m2.1b"><apply id="A1.SS2.p3.2.m2.1.1.cmml" xref="A1.SS2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="A1.SS2.p3.2.m2.1.1.1.cmml" xref="A1.SS2.p3.2.m2.1.1">superscript</csymbol><ci id="A1.SS2.p3.2.m2.1.1.2.cmml" xref="A1.SS2.p3.2.m2.1.1.2">𝐐</ci><ci id="A1.SS2.p3.2.m2.1.1.3.cmml" xref="A1.SS2.p3.2.m2.1.1.3">𝐭</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.SS2.p3.2.m2.1c">\mathbf{Q^{t}}</annotation><annotation encoding="application/x-llamapun" id="A1.SS2.p3.2.m2.1d">bold_Q start_POSTSUPERSCRIPT bold_t end_POSTSUPERSCRIPT</annotation></semantics></math>, and no pixel-level output are predicted. All predictions follow an auto-regressive strategy.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Loss Functions</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Pixel-level Output Loss</h3>
<div class="ltx_para ltx_noindent" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.11"><span class="ltx_text ltx_font_bold" id="A2.SS1.p1.11.1">Segmentation Loss.</span> There are two losses on the segmentation corresponding to two tasks. For mask classification, we use text encoder <math alttext="\mathbf{Enc_{T}}" class="ltx_Math" display="inline" id="A2.SS1.p1.1.m1.1"><semantics id="A2.SS1.p1.1.m1.1a"><msub id="A2.SS1.p1.1.m1.1.1" xref="A2.SS1.p1.1.m1.1.1.cmml"><mi id="A2.SS1.p1.1.m1.1.1.2" xref="A2.SS1.p1.1.m1.1.1.2.cmml">𝐄𝐧𝐜</mi><mi id="A2.SS1.p1.1.m1.1.1.3" xref="A2.SS1.p1.1.m1.1.1.3.cmml">𝐓</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.1.m1.1b"><apply id="A2.SS1.p1.1.m1.1.1.cmml" xref="A2.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.1.m1.1.1.1.cmml" xref="A2.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="A2.SS1.p1.1.m1.1.1.2.cmml" xref="A2.SS1.p1.1.m1.1.1.2">𝐄𝐧𝐜</ci><ci id="A2.SS1.p1.1.m1.1.1.3.cmml" xref="A2.SS1.p1.1.m1.1.1.3">𝐓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.1.m1.1c">\mathbf{Enc_{T}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.1.m1.1d">bold_Enc start_POSTSUBSCRIPT bold_T end_POSTSUBSCRIPT</annotation></semantics></math> to encode all <math alttext="N" class="ltx_Math" display="inline" id="A2.SS1.p1.2.m2.1"><semantics id="A2.SS1.p1.2.m2.1a"><mi id="A2.SS1.p1.2.m2.1.1" xref="A2.SS1.p1.2.m2.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.2.m2.1b"><ci id="A2.SS1.p1.2.m2.1.1.cmml" xref="A2.SS1.p1.2.m2.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.2.m2.1c">N</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.2.m2.1d">italic_N</annotation></semantics></math> class names including “background” into <math alttext="N" class="ltx_Math" display="inline" id="A2.SS1.p1.3.m3.1"><semantics id="A2.SS1.p1.3.m3.1a"><mi id="A2.SS1.p1.3.m3.1.1" xref="A2.SS1.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.3.m3.1b"><ci id="A2.SS1.p1.3.m3.1.1.cmml" xref="A2.SS1.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.3.m3.1d">italic_N</annotation></semantics></math> text embeddings <math alttext="\mathbf{E}_{cls}{\in}\mathcal{R}^{N\times C}" class="ltx_Math" display="inline" id="A2.SS1.p1.4.m4.1"><semantics id="A2.SS1.p1.4.m4.1a"><mrow id="A2.SS1.p1.4.m4.1.1" xref="A2.SS1.p1.4.m4.1.1.cmml"><msub id="A2.SS1.p1.4.m4.1.1.2" xref="A2.SS1.p1.4.m4.1.1.2.cmml"><mi id="A2.SS1.p1.4.m4.1.1.2.2" xref="A2.SS1.p1.4.m4.1.1.2.2.cmml">𝐄</mi><mrow id="A2.SS1.p1.4.m4.1.1.2.3" xref="A2.SS1.p1.4.m4.1.1.2.3.cmml"><mi id="A2.SS1.p1.4.m4.1.1.2.3.2" xref="A2.SS1.p1.4.m4.1.1.2.3.2.cmml">c</mi><mo id="A2.SS1.p1.4.m4.1.1.2.3.1" xref="A2.SS1.p1.4.m4.1.1.2.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.4.m4.1.1.2.3.3" xref="A2.SS1.p1.4.m4.1.1.2.3.3.cmml">l</mi><mo id="A2.SS1.p1.4.m4.1.1.2.3.1a" xref="A2.SS1.p1.4.m4.1.1.2.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.4.m4.1.1.2.3.4" xref="A2.SS1.p1.4.m4.1.1.2.3.4.cmml">s</mi></mrow></msub><mo id="A2.SS1.p1.4.m4.1.1.1" xref="A2.SS1.p1.4.m4.1.1.1.cmml">∈</mo><msup id="A2.SS1.p1.4.m4.1.1.3" xref="A2.SS1.p1.4.m4.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p1.4.m4.1.1.3.2" xref="A2.SS1.p1.4.m4.1.1.3.2.cmml">ℛ</mi><mrow id="A2.SS1.p1.4.m4.1.1.3.3" xref="A2.SS1.p1.4.m4.1.1.3.3.cmml"><mi id="A2.SS1.p1.4.m4.1.1.3.3.2" xref="A2.SS1.p1.4.m4.1.1.3.3.2.cmml">N</mi><mo id="A2.SS1.p1.4.m4.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A2.SS1.p1.4.m4.1.1.3.3.1.cmml">×</mo><mi id="A2.SS1.p1.4.m4.1.1.3.3.3" xref="A2.SS1.p1.4.m4.1.1.3.3.3.cmml">C</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.4.m4.1b"><apply id="A2.SS1.p1.4.m4.1.1.cmml" xref="A2.SS1.p1.4.m4.1.1"><in id="A2.SS1.p1.4.m4.1.1.1.cmml" xref="A2.SS1.p1.4.m4.1.1.1"></in><apply id="A2.SS1.p1.4.m4.1.1.2.cmml" xref="A2.SS1.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p1.4.m4.1.1.2.1.cmml" xref="A2.SS1.p1.4.m4.1.1.2">subscript</csymbol><ci id="A2.SS1.p1.4.m4.1.1.2.2.cmml" xref="A2.SS1.p1.4.m4.1.1.2.2">𝐄</ci><apply id="A2.SS1.p1.4.m4.1.1.2.3.cmml" xref="A2.SS1.p1.4.m4.1.1.2.3"><times id="A2.SS1.p1.4.m4.1.1.2.3.1.cmml" xref="A2.SS1.p1.4.m4.1.1.2.3.1"></times><ci id="A2.SS1.p1.4.m4.1.1.2.3.2.cmml" xref="A2.SS1.p1.4.m4.1.1.2.3.2">𝑐</ci><ci id="A2.SS1.p1.4.m4.1.1.2.3.3.cmml" xref="A2.SS1.p1.4.m4.1.1.2.3.3">𝑙</ci><ci id="A2.SS1.p1.4.m4.1.1.2.3.4.cmml" xref="A2.SS1.p1.4.m4.1.1.2.3.4">𝑠</ci></apply></apply><apply id="A2.SS1.p1.4.m4.1.1.3.cmml" xref="A2.SS1.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="A2.SS1.p1.4.m4.1.1.3.1.cmml" xref="A2.SS1.p1.4.m4.1.1.3">superscript</csymbol><ci id="A2.SS1.p1.4.m4.1.1.3.2.cmml" xref="A2.SS1.p1.4.m4.1.1.3.2">ℛ</ci><apply id="A2.SS1.p1.4.m4.1.1.3.3.cmml" xref="A2.SS1.p1.4.m4.1.1.3.3"><times id="A2.SS1.p1.4.m4.1.1.3.3.1.cmml" xref="A2.SS1.p1.4.m4.1.1.3.3.1"></times><ci id="A2.SS1.p1.4.m4.1.1.3.3.2.cmml" xref="A2.SS1.p1.4.m4.1.1.3.3.2">𝑁</ci><ci id="A2.SS1.p1.4.m4.1.1.3.3.3.cmml" xref="A2.SS1.p1.4.m4.1.1.3.3.3">𝐶</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.4.m4.1c">\mathbf{E}_{cls}{\in}\mathcal{R}^{N\times C}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.4.m4.1d">bold_E start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT ∈ caligraphic_R start_POSTSUPERSCRIPT italic_N × italic_C end_POSTSUPERSCRIPT</annotation></semantics></math> and take it to represent the concept. Afterward, we take the first (<math alttext="m-1" class="ltx_Math" display="inline" id="A2.SS1.p1.5.m5.1"><semantics id="A2.SS1.p1.5.m5.1a"><mrow id="A2.SS1.p1.5.m5.1.1" xref="A2.SS1.p1.5.m5.1.1.cmml"><mi id="A2.SS1.p1.5.m5.1.1.2" xref="A2.SS1.p1.5.m5.1.1.2.cmml">m</mi><mo id="A2.SS1.p1.5.m5.1.1.1" xref="A2.SS1.p1.5.m5.1.1.1.cmml">−</mo><mn id="A2.SS1.p1.5.m5.1.1.3" xref="A2.SS1.p1.5.m5.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.5.m5.1b"><apply id="A2.SS1.p1.5.m5.1.1.cmml" xref="A2.SS1.p1.5.m5.1.1"><minus id="A2.SS1.p1.5.m5.1.1.1.cmml" xref="A2.SS1.p1.5.m5.1.1.1"></minus><ci id="A2.SS1.p1.5.m5.1.1.2.cmml" xref="A2.SS1.p1.5.m5.1.1.2">𝑚</ci><cn id="A2.SS1.p1.5.m5.1.1.3.cmml" type="integer" xref="A2.SS1.p1.5.m5.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.5.m5.1c">m-1</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.5.m5.1d">italic_m - 1</annotation></semantics></math>) latent queries and compute the dot-product between these outputs and concept embeddings to obtain an affinity matrix <math alttext="\mathbf{S}_{cls}{\in}\mathcal{R}^{(m-1){\times}N}" class="ltx_Math" display="inline" id="A2.SS1.p1.6.m6.1"><semantics id="A2.SS1.p1.6.m6.1a"><mrow id="A2.SS1.p1.6.m6.1.2" xref="A2.SS1.p1.6.m6.1.2.cmml"><msub id="A2.SS1.p1.6.m6.1.2.2" xref="A2.SS1.p1.6.m6.1.2.2.cmml"><mi id="A2.SS1.p1.6.m6.1.2.2.2" xref="A2.SS1.p1.6.m6.1.2.2.2.cmml">𝐒</mi><mrow id="A2.SS1.p1.6.m6.1.2.2.3" xref="A2.SS1.p1.6.m6.1.2.2.3.cmml"><mi id="A2.SS1.p1.6.m6.1.2.2.3.2" xref="A2.SS1.p1.6.m6.1.2.2.3.2.cmml">c</mi><mo id="A2.SS1.p1.6.m6.1.2.2.3.1" xref="A2.SS1.p1.6.m6.1.2.2.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.6.m6.1.2.2.3.3" xref="A2.SS1.p1.6.m6.1.2.2.3.3.cmml">l</mi><mo id="A2.SS1.p1.6.m6.1.2.2.3.1a" xref="A2.SS1.p1.6.m6.1.2.2.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.6.m6.1.2.2.3.4" xref="A2.SS1.p1.6.m6.1.2.2.3.4.cmml">s</mi></mrow></msub><mo id="A2.SS1.p1.6.m6.1.2.1" xref="A2.SS1.p1.6.m6.1.2.1.cmml">∈</mo><msup id="A2.SS1.p1.6.m6.1.2.3" xref="A2.SS1.p1.6.m6.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p1.6.m6.1.2.3.2" xref="A2.SS1.p1.6.m6.1.2.3.2.cmml">ℛ</mi><mrow id="A2.SS1.p1.6.m6.1.1.1" xref="A2.SS1.p1.6.m6.1.1.1.cmml"><mrow id="A2.SS1.p1.6.m6.1.1.1.1.1" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.cmml"><mo id="A2.SS1.p1.6.m6.1.1.1.1.1.2" stretchy="false" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.SS1.p1.6.m6.1.1.1.1.1.1" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.cmml"><mi id="A2.SS1.p1.6.m6.1.1.1.1.1.1.2" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.2.cmml">m</mi><mo id="A2.SS1.p1.6.m6.1.1.1.1.1.1.1" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.1.cmml">−</mo><mn id="A2.SS1.p1.6.m6.1.1.1.1.1.1.3" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.3.cmml">1</mn></mrow><mo id="A2.SS1.p1.6.m6.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="A2.SS1.p1.6.m6.1.1.1.2" rspace="0.222em" xref="A2.SS1.p1.6.m6.1.1.1.2.cmml">×</mo><mi id="A2.SS1.p1.6.m6.1.1.1.3" xref="A2.SS1.p1.6.m6.1.1.1.3.cmml">N</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.6.m6.1b"><apply id="A2.SS1.p1.6.m6.1.2.cmml" xref="A2.SS1.p1.6.m6.1.2"><in id="A2.SS1.p1.6.m6.1.2.1.cmml" xref="A2.SS1.p1.6.m6.1.2.1"></in><apply id="A2.SS1.p1.6.m6.1.2.2.cmml" xref="A2.SS1.p1.6.m6.1.2.2"><csymbol cd="ambiguous" id="A2.SS1.p1.6.m6.1.2.2.1.cmml" xref="A2.SS1.p1.6.m6.1.2.2">subscript</csymbol><ci id="A2.SS1.p1.6.m6.1.2.2.2.cmml" xref="A2.SS1.p1.6.m6.1.2.2.2">𝐒</ci><apply id="A2.SS1.p1.6.m6.1.2.2.3.cmml" xref="A2.SS1.p1.6.m6.1.2.2.3"><times id="A2.SS1.p1.6.m6.1.2.2.3.1.cmml" xref="A2.SS1.p1.6.m6.1.2.2.3.1"></times><ci id="A2.SS1.p1.6.m6.1.2.2.3.2.cmml" xref="A2.SS1.p1.6.m6.1.2.2.3.2">𝑐</ci><ci id="A2.SS1.p1.6.m6.1.2.2.3.3.cmml" xref="A2.SS1.p1.6.m6.1.2.2.3.3">𝑙</ci><ci id="A2.SS1.p1.6.m6.1.2.2.3.4.cmml" xref="A2.SS1.p1.6.m6.1.2.2.3.4">𝑠</ci></apply></apply><apply id="A2.SS1.p1.6.m6.1.2.3.cmml" xref="A2.SS1.p1.6.m6.1.2.3"><csymbol cd="ambiguous" id="A2.SS1.p1.6.m6.1.2.3.1.cmml" xref="A2.SS1.p1.6.m6.1.2.3">superscript</csymbol><ci id="A2.SS1.p1.6.m6.1.2.3.2.cmml" xref="A2.SS1.p1.6.m6.1.2.3.2">ℛ</ci><apply id="A2.SS1.p1.6.m6.1.1.1.cmml" xref="A2.SS1.p1.6.m6.1.1.1"><times id="A2.SS1.p1.6.m6.1.1.1.2.cmml" xref="A2.SS1.p1.6.m6.1.1.1.2"></times><apply id="A2.SS1.p1.6.m6.1.1.1.1.1.1.cmml" xref="A2.SS1.p1.6.m6.1.1.1.1.1"><minus id="A2.SS1.p1.6.m6.1.1.1.1.1.1.1.cmml" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.1"></minus><ci id="A2.SS1.p1.6.m6.1.1.1.1.1.1.2.cmml" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.2">𝑚</ci><cn id="A2.SS1.p1.6.m6.1.1.1.1.1.1.3.cmml" type="integer" xref="A2.SS1.p1.6.m6.1.1.1.1.1.1.3">1</cn></apply><ci id="A2.SS1.p1.6.m6.1.1.1.3.cmml" xref="A2.SS1.p1.6.m6.1.1.1.3">𝑁</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.6.m6.1c">\mathbf{S}_{cls}{\in}\mathcal{R}^{(m-1){\times}N}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.6.m6.1d">bold_S start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT ∈ caligraphic_R start_POSTSUPERSCRIPT ( italic_m - 1 ) × italic_N end_POSTSUPERSCRIPT</annotation></semantics></math> and compute <math alttext="\mathcal{L}_{cls}{=}\mathbf{CE}(\mathbf{S}_{cls},\mathbf{y}_{cls})" class="ltx_Math" display="inline" id="A2.SS1.p1.7.m7.2"><semantics id="A2.SS1.p1.7.m7.2a"><mrow id="A2.SS1.p1.7.m7.2.2" xref="A2.SS1.p1.7.m7.2.2.cmml"><msub id="A2.SS1.p1.7.m7.2.2.4" xref="A2.SS1.p1.7.m7.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p1.7.m7.2.2.4.2" xref="A2.SS1.p1.7.m7.2.2.4.2.cmml">ℒ</mi><mrow id="A2.SS1.p1.7.m7.2.2.4.3" xref="A2.SS1.p1.7.m7.2.2.4.3.cmml"><mi id="A2.SS1.p1.7.m7.2.2.4.3.2" xref="A2.SS1.p1.7.m7.2.2.4.3.2.cmml">c</mi><mo id="A2.SS1.p1.7.m7.2.2.4.3.1" xref="A2.SS1.p1.7.m7.2.2.4.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.7.m7.2.2.4.3.3" xref="A2.SS1.p1.7.m7.2.2.4.3.3.cmml">l</mi><mo id="A2.SS1.p1.7.m7.2.2.4.3.1a" xref="A2.SS1.p1.7.m7.2.2.4.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.7.m7.2.2.4.3.4" xref="A2.SS1.p1.7.m7.2.2.4.3.4.cmml">s</mi></mrow></msub><mo id="A2.SS1.p1.7.m7.2.2.3" xref="A2.SS1.p1.7.m7.2.2.3.cmml">=</mo><mrow id="A2.SS1.p1.7.m7.2.2.2" xref="A2.SS1.p1.7.m7.2.2.2.cmml"><mi id="A2.SS1.p1.7.m7.2.2.2.4" xref="A2.SS1.p1.7.m7.2.2.2.4.cmml">𝐂𝐄</mi><mo id="A2.SS1.p1.7.m7.2.2.2.3" xref="A2.SS1.p1.7.m7.2.2.2.3.cmml">⁢</mo><mrow id="A2.SS1.p1.7.m7.2.2.2.2.2" xref="A2.SS1.p1.7.m7.2.2.2.2.3.cmml"><mo id="A2.SS1.p1.7.m7.2.2.2.2.2.3" stretchy="false" xref="A2.SS1.p1.7.m7.2.2.2.2.3.cmml">(</mo><msub id="A2.SS1.p1.7.m7.1.1.1.1.1.1" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.cmml"><mi id="A2.SS1.p1.7.m7.1.1.1.1.1.1.2" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.2.cmml">𝐒</mi><mrow id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.cmml"><mi id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.2" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.2.cmml">c</mi><mo id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.1" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.3" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.3.cmml">l</mi><mo id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.1a" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.4" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.4.cmml">s</mi></mrow></msub><mo id="A2.SS1.p1.7.m7.2.2.2.2.2.4" xref="A2.SS1.p1.7.m7.2.2.2.2.3.cmml">,</mo><msub id="A2.SS1.p1.7.m7.2.2.2.2.2.2" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.cmml"><mi id="A2.SS1.p1.7.m7.2.2.2.2.2.2.2" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.2.cmml">𝐲</mi><mrow id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.cmml"><mi id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.2" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.2.cmml">c</mi><mo id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.1" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.3" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.3.cmml">l</mi><mo id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.1a" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.4" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.4.cmml">s</mi></mrow></msub><mo id="A2.SS1.p1.7.m7.2.2.2.2.2.5" stretchy="false" xref="A2.SS1.p1.7.m7.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.7.m7.2b"><apply id="A2.SS1.p1.7.m7.2.2.cmml" xref="A2.SS1.p1.7.m7.2.2"><eq id="A2.SS1.p1.7.m7.2.2.3.cmml" xref="A2.SS1.p1.7.m7.2.2.3"></eq><apply id="A2.SS1.p1.7.m7.2.2.4.cmml" xref="A2.SS1.p1.7.m7.2.2.4"><csymbol cd="ambiguous" id="A2.SS1.p1.7.m7.2.2.4.1.cmml" xref="A2.SS1.p1.7.m7.2.2.4">subscript</csymbol><ci id="A2.SS1.p1.7.m7.2.2.4.2.cmml" xref="A2.SS1.p1.7.m7.2.2.4.2">ℒ</ci><apply id="A2.SS1.p1.7.m7.2.2.4.3.cmml" xref="A2.SS1.p1.7.m7.2.2.4.3"><times id="A2.SS1.p1.7.m7.2.2.4.3.1.cmml" xref="A2.SS1.p1.7.m7.2.2.4.3.1"></times><ci id="A2.SS1.p1.7.m7.2.2.4.3.2.cmml" xref="A2.SS1.p1.7.m7.2.2.4.3.2">𝑐</ci><ci id="A2.SS1.p1.7.m7.2.2.4.3.3.cmml" xref="A2.SS1.p1.7.m7.2.2.4.3.3">𝑙</ci><ci id="A2.SS1.p1.7.m7.2.2.4.3.4.cmml" xref="A2.SS1.p1.7.m7.2.2.4.3.4">𝑠</ci></apply></apply><apply id="A2.SS1.p1.7.m7.2.2.2.cmml" xref="A2.SS1.p1.7.m7.2.2.2"><times id="A2.SS1.p1.7.m7.2.2.2.3.cmml" xref="A2.SS1.p1.7.m7.2.2.2.3"></times><ci id="A2.SS1.p1.7.m7.2.2.2.4.cmml" xref="A2.SS1.p1.7.m7.2.2.2.4">𝐂𝐄</ci><interval closure="open" id="A2.SS1.p1.7.m7.2.2.2.2.3.cmml" xref="A2.SS1.p1.7.m7.2.2.2.2.2"><apply id="A2.SS1.p1.7.m7.1.1.1.1.1.1.cmml" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.7.m7.1.1.1.1.1.1.1.cmml" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1">subscript</csymbol><ci id="A2.SS1.p1.7.m7.1.1.1.1.1.1.2.cmml" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.2">𝐒</ci><apply id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.cmml" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3"><times id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.1.cmml" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.1"></times><ci id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.2.cmml" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.2">𝑐</ci><ci id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.3.cmml" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.3">𝑙</ci><ci id="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.4.cmml" xref="A2.SS1.p1.7.m7.1.1.1.1.1.1.3.4">𝑠</ci></apply></apply><apply id="A2.SS1.p1.7.m7.2.2.2.2.2.2.cmml" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2"><csymbol cd="ambiguous" id="A2.SS1.p1.7.m7.2.2.2.2.2.2.1.cmml" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2">subscript</csymbol><ci id="A2.SS1.p1.7.m7.2.2.2.2.2.2.2.cmml" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.2">𝐲</ci><apply id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.cmml" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3"><times id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.1.cmml" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.1"></times><ci id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.2.cmml" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.2">𝑐</ci><ci id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.3.cmml" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.3">𝑙</ci><ci id="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.4.cmml" xref="A2.SS1.p1.7.m7.2.2.2.2.2.2.3.4">𝑠</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.7.m7.2c">\mathcal{L}_{cls}{=}\mathbf{CE}(\mathbf{S}_{cls},\mathbf{y}_{cls})</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.7.m7.2d">caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT = bold_CE ( bold_S start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT )</annotation></semantics></math>, with the ground-truth class <math alttext="\mathbf{y}_{cls}" class="ltx_Math" display="inline" id="A2.SS1.p1.8.m8.1"><semantics id="A2.SS1.p1.8.m8.1a"><msub id="A2.SS1.p1.8.m8.1.1" xref="A2.SS1.p1.8.m8.1.1.cmml"><mi id="A2.SS1.p1.8.m8.1.1.2" xref="A2.SS1.p1.8.m8.1.1.2.cmml">𝐲</mi><mrow id="A2.SS1.p1.8.m8.1.1.3" xref="A2.SS1.p1.8.m8.1.1.3.cmml"><mi id="A2.SS1.p1.8.m8.1.1.3.2" xref="A2.SS1.p1.8.m8.1.1.3.2.cmml">c</mi><mo id="A2.SS1.p1.8.m8.1.1.3.1" xref="A2.SS1.p1.8.m8.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.8.m8.1.1.3.3" xref="A2.SS1.p1.8.m8.1.1.3.3.cmml">l</mi><mo id="A2.SS1.p1.8.m8.1.1.3.1a" xref="A2.SS1.p1.8.m8.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.8.m8.1.1.3.4" xref="A2.SS1.p1.8.m8.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.8.m8.1b"><apply id="A2.SS1.p1.8.m8.1.1.cmml" xref="A2.SS1.p1.8.m8.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.8.m8.1.1.1.cmml" xref="A2.SS1.p1.8.m8.1.1">subscript</csymbol><ci id="A2.SS1.p1.8.m8.1.1.2.cmml" xref="A2.SS1.p1.8.m8.1.1.2">𝐲</ci><apply id="A2.SS1.p1.8.m8.1.1.3.cmml" xref="A2.SS1.p1.8.m8.1.1.3"><times id="A2.SS1.p1.8.m8.1.1.3.1.cmml" xref="A2.SS1.p1.8.m8.1.1.3.1"></times><ci id="A2.SS1.p1.8.m8.1.1.3.2.cmml" xref="A2.SS1.p1.8.m8.1.1.3.2">𝑐</ci><ci id="A2.SS1.p1.8.m8.1.1.3.3.cmml" xref="A2.SS1.p1.8.m8.1.1.3.3">𝑙</ci><ci id="A2.SS1.p1.8.m8.1.1.3.4.cmml" xref="A2.SS1.p1.8.m8.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.8.m8.1c">\mathbf{y}_{cls}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.8.m8.1d">bold_y start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT</annotation></semantics></math>. For mask prediction, we use Hungarian matching <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib6" title="">6</a>]</cite> to find the matched entries of first (<math alttext="m-1" class="ltx_Math" display="inline" id="A2.SS1.p1.9.m9.1"><semantics id="A2.SS1.p1.9.m9.1a"><mrow id="A2.SS1.p1.9.m9.1.1" xref="A2.SS1.p1.9.m9.1.1.cmml"><mi id="A2.SS1.p1.9.m9.1.1.2" xref="A2.SS1.p1.9.m9.1.1.2.cmml">m</mi><mo id="A2.SS1.p1.9.m9.1.1.1" xref="A2.SS1.p1.9.m9.1.1.1.cmml">−</mo><mn id="A2.SS1.p1.9.m9.1.1.3" xref="A2.SS1.p1.9.m9.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.9.m9.1b"><apply id="A2.SS1.p1.9.m9.1.1.cmml" xref="A2.SS1.p1.9.m9.1.1"><minus id="A2.SS1.p1.9.m9.1.1.1.cmml" xref="A2.SS1.p1.9.m9.1.1.1"></minus><ci id="A2.SS1.p1.9.m9.1.1.2.cmml" xref="A2.SS1.p1.9.m9.1.1.2">𝑚</ci><cn id="A2.SS1.p1.9.m9.1.1.3.cmml" type="integer" xref="A2.SS1.p1.9.m9.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.9.m9.1c">m-1</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.9.m9.1d">italic_m - 1</annotation></semantics></math>) outputs to ground-truth annotations. Afterward, we to use binary cross-entropy loss <math alttext="\mathcal{L}_{bce}" class="ltx_Math" display="inline" id="A2.SS1.p1.10.m10.1"><semantics id="A2.SS1.p1.10.m10.1a"><msub id="A2.SS1.p1.10.m10.1.1" xref="A2.SS1.p1.10.m10.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p1.10.m10.1.1.2" xref="A2.SS1.p1.10.m10.1.1.2.cmml">ℒ</mi><mrow id="A2.SS1.p1.10.m10.1.1.3" xref="A2.SS1.p1.10.m10.1.1.3.cmml"><mi id="A2.SS1.p1.10.m10.1.1.3.2" xref="A2.SS1.p1.10.m10.1.1.3.2.cmml">b</mi><mo id="A2.SS1.p1.10.m10.1.1.3.1" xref="A2.SS1.p1.10.m10.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.10.m10.1.1.3.3" xref="A2.SS1.p1.10.m10.1.1.3.3.cmml">c</mi><mo id="A2.SS1.p1.10.m10.1.1.3.1a" xref="A2.SS1.p1.10.m10.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.10.m10.1.1.3.4" xref="A2.SS1.p1.10.m10.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.10.m10.1b"><apply id="A2.SS1.p1.10.m10.1.1.cmml" xref="A2.SS1.p1.10.m10.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.10.m10.1.1.1.cmml" xref="A2.SS1.p1.10.m10.1.1">subscript</csymbol><ci id="A2.SS1.p1.10.m10.1.1.2.cmml" xref="A2.SS1.p1.10.m10.1.1.2">ℒ</ci><apply id="A2.SS1.p1.10.m10.1.1.3.cmml" xref="A2.SS1.p1.10.m10.1.1.3"><times id="A2.SS1.p1.10.m10.1.1.3.1.cmml" xref="A2.SS1.p1.10.m10.1.1.3.1"></times><ci id="A2.SS1.p1.10.m10.1.1.3.2.cmml" xref="A2.SS1.p1.10.m10.1.1.3.2">𝑏</ci><ci id="A2.SS1.p1.10.m10.1.1.3.3.cmml" xref="A2.SS1.p1.10.m10.1.1.3.3">𝑐</ci><ci id="A2.SS1.p1.10.m10.1.1.3.4.cmml" xref="A2.SS1.p1.10.m10.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.10.m10.1c">\mathcal{L}_{bce}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.10.m10.1d">caligraphic_L start_POSTSUBSCRIPT italic_b italic_c italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and dice loss <math alttext="\mathcal{L}_{dice}" class="ltx_Math" display="inline" id="A2.SS1.p1.11.m11.1"><semantics id="A2.SS1.p1.11.m11.1a"><msub id="A2.SS1.p1.11.m11.1.1" xref="A2.SS1.p1.11.m11.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS1.p1.11.m11.1.1.2" xref="A2.SS1.p1.11.m11.1.1.2.cmml">ℒ</mi><mrow id="A2.SS1.p1.11.m11.1.1.3" xref="A2.SS1.p1.11.m11.1.1.3.cmml"><mi id="A2.SS1.p1.11.m11.1.1.3.2" xref="A2.SS1.p1.11.m11.1.1.3.2.cmml">d</mi><mo id="A2.SS1.p1.11.m11.1.1.3.1" xref="A2.SS1.p1.11.m11.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.11.m11.1.1.3.3" xref="A2.SS1.p1.11.m11.1.1.3.3.cmml">i</mi><mo id="A2.SS1.p1.11.m11.1.1.3.1a" xref="A2.SS1.p1.11.m11.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.11.m11.1.1.3.4" xref="A2.SS1.p1.11.m11.1.1.3.4.cmml">c</mi><mo id="A2.SS1.p1.11.m11.1.1.3.1b" xref="A2.SS1.p1.11.m11.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.11.m11.1.1.3.5" xref="A2.SS1.p1.11.m11.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.11.m11.1b"><apply id="A2.SS1.p1.11.m11.1.1.cmml" xref="A2.SS1.p1.11.m11.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.11.m11.1.1.1.cmml" xref="A2.SS1.p1.11.m11.1.1">subscript</csymbol><ci id="A2.SS1.p1.11.m11.1.1.2.cmml" xref="A2.SS1.p1.11.m11.1.1.2">ℒ</ci><apply id="A2.SS1.p1.11.m11.1.1.3.cmml" xref="A2.SS1.p1.11.m11.1.1.3"><times id="A2.SS1.p1.11.m11.1.1.3.1.cmml" xref="A2.SS1.p1.11.m11.1.1.3.1"></times><ci id="A2.SS1.p1.11.m11.1.1.3.2.cmml" xref="A2.SS1.p1.11.m11.1.1.3.2">𝑑</ci><ci id="A2.SS1.p1.11.m11.1.1.3.3.cmml" xref="A2.SS1.p1.11.m11.1.1.3.3">𝑖</ci><ci id="A2.SS1.p1.11.m11.1.1.3.4.cmml" xref="A2.SS1.p1.11.m11.1.1.3.4">𝑐</ci><ci id="A2.SS1.p1.11.m11.1.1.3.5.cmml" xref="A2.SS1.p1.11.m11.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.11.m11.1c">\mathcal{L}_{dice}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.11.m11.1d">caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_c italic_e end_POSTSUBSCRIPT</annotation></semantics></math> to compute the loss. Thus, the overall training loss function of panoptic segmentation is:</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E5">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{ps}=\lambda_{cls}\mathcal{L}_{cls}+\lambda_{bce}\mathcal{L}_{bce}%
+\lambda_{dice}\mathcal{L}_{dice}," class="ltx_Math" display="block" id="A2.E5.m1.1"><semantics id="A2.E5.m1.1a"><mrow id="A2.E5.m1.1.1.1" xref="A2.E5.m1.1.1.1.1.cmml"><mrow id="A2.E5.m1.1.1.1.1" xref="A2.E5.m1.1.1.1.1.cmml"><msub id="A2.E5.m1.1.1.1.1.2" xref="A2.E5.m1.1.1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.E5.m1.1.1.1.1.2.2" xref="A2.E5.m1.1.1.1.1.2.2.cmml">ℒ</mi><mrow id="A2.E5.m1.1.1.1.1.2.3" xref="A2.E5.m1.1.1.1.1.2.3.cmml"><mi id="A2.E5.m1.1.1.1.1.2.3.2" xref="A2.E5.m1.1.1.1.1.2.3.2.cmml">p</mi><mo id="A2.E5.m1.1.1.1.1.2.3.1" xref="A2.E5.m1.1.1.1.1.2.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.2.3.3" xref="A2.E5.m1.1.1.1.1.2.3.3.cmml">s</mi></mrow></msub><mo id="A2.E5.m1.1.1.1.1.1" xref="A2.E5.m1.1.1.1.1.1.cmml">=</mo><mrow id="A2.E5.m1.1.1.1.1.3" xref="A2.E5.m1.1.1.1.1.3.cmml"><mrow id="A2.E5.m1.1.1.1.1.3.2" xref="A2.E5.m1.1.1.1.1.3.2.cmml"><msub id="A2.E5.m1.1.1.1.1.3.2.2" xref="A2.E5.m1.1.1.1.1.3.2.2.cmml"><mi id="A2.E5.m1.1.1.1.1.3.2.2.2" xref="A2.E5.m1.1.1.1.1.3.2.2.2.cmml">λ</mi><mrow id="A2.E5.m1.1.1.1.1.3.2.2.3" xref="A2.E5.m1.1.1.1.1.3.2.2.3.cmml"><mi id="A2.E5.m1.1.1.1.1.3.2.2.3.2" xref="A2.E5.m1.1.1.1.1.3.2.2.3.2.cmml">c</mi><mo id="A2.E5.m1.1.1.1.1.3.2.2.3.1" xref="A2.E5.m1.1.1.1.1.3.2.2.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.2.2.3.3" xref="A2.E5.m1.1.1.1.1.3.2.2.3.3.cmml">l</mi><mo id="A2.E5.m1.1.1.1.1.3.2.2.3.1a" xref="A2.E5.m1.1.1.1.1.3.2.2.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.2.2.3.4" xref="A2.E5.m1.1.1.1.1.3.2.2.3.4.cmml">s</mi></mrow></msub><mo id="A2.E5.m1.1.1.1.1.3.2.1" xref="A2.E5.m1.1.1.1.1.3.2.1.cmml">⁢</mo><msub id="A2.E5.m1.1.1.1.1.3.2.3" xref="A2.E5.m1.1.1.1.1.3.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.E5.m1.1.1.1.1.3.2.3.2" xref="A2.E5.m1.1.1.1.1.3.2.3.2.cmml">ℒ</mi><mrow id="A2.E5.m1.1.1.1.1.3.2.3.3" xref="A2.E5.m1.1.1.1.1.3.2.3.3.cmml"><mi id="A2.E5.m1.1.1.1.1.3.2.3.3.2" xref="A2.E5.m1.1.1.1.1.3.2.3.3.2.cmml">c</mi><mo id="A2.E5.m1.1.1.1.1.3.2.3.3.1" xref="A2.E5.m1.1.1.1.1.3.2.3.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.2.3.3.3" xref="A2.E5.m1.1.1.1.1.3.2.3.3.3.cmml">l</mi><mo id="A2.E5.m1.1.1.1.1.3.2.3.3.1a" xref="A2.E5.m1.1.1.1.1.3.2.3.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.2.3.3.4" xref="A2.E5.m1.1.1.1.1.3.2.3.3.4.cmml">s</mi></mrow></msub></mrow><mo id="A2.E5.m1.1.1.1.1.3.1" xref="A2.E5.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="A2.E5.m1.1.1.1.1.3.3" xref="A2.E5.m1.1.1.1.1.3.3.cmml"><msub id="A2.E5.m1.1.1.1.1.3.3.2" xref="A2.E5.m1.1.1.1.1.3.3.2.cmml"><mi id="A2.E5.m1.1.1.1.1.3.3.2.2" xref="A2.E5.m1.1.1.1.1.3.3.2.2.cmml">λ</mi><mrow id="A2.E5.m1.1.1.1.1.3.3.2.3" xref="A2.E5.m1.1.1.1.1.3.3.2.3.cmml"><mi id="A2.E5.m1.1.1.1.1.3.3.2.3.2" xref="A2.E5.m1.1.1.1.1.3.3.2.3.2.cmml">b</mi><mo id="A2.E5.m1.1.1.1.1.3.3.2.3.1" xref="A2.E5.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.3.2.3.3" xref="A2.E5.m1.1.1.1.1.3.3.2.3.3.cmml">c</mi><mo id="A2.E5.m1.1.1.1.1.3.3.2.3.1a" xref="A2.E5.m1.1.1.1.1.3.3.2.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.3.2.3.4" xref="A2.E5.m1.1.1.1.1.3.3.2.3.4.cmml">e</mi></mrow></msub><mo id="A2.E5.m1.1.1.1.1.3.3.1" xref="A2.E5.m1.1.1.1.1.3.3.1.cmml">⁢</mo><msub id="A2.E5.m1.1.1.1.1.3.3.3" xref="A2.E5.m1.1.1.1.1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.E5.m1.1.1.1.1.3.3.3.2" xref="A2.E5.m1.1.1.1.1.3.3.3.2.cmml">ℒ</mi><mrow id="A2.E5.m1.1.1.1.1.3.3.3.3" xref="A2.E5.m1.1.1.1.1.3.3.3.3.cmml"><mi id="A2.E5.m1.1.1.1.1.3.3.3.3.2" xref="A2.E5.m1.1.1.1.1.3.3.3.3.2.cmml">b</mi><mo id="A2.E5.m1.1.1.1.1.3.3.3.3.1" xref="A2.E5.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.3.3.3.3" xref="A2.E5.m1.1.1.1.1.3.3.3.3.3.cmml">c</mi><mo id="A2.E5.m1.1.1.1.1.3.3.3.3.1a" xref="A2.E5.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.3.3.3.4" xref="A2.E5.m1.1.1.1.1.3.3.3.3.4.cmml">e</mi></mrow></msub></mrow><mo id="A2.E5.m1.1.1.1.1.3.1a" xref="A2.E5.m1.1.1.1.1.3.1.cmml">+</mo><mrow id="A2.E5.m1.1.1.1.1.3.4" xref="A2.E5.m1.1.1.1.1.3.4.cmml"><msub id="A2.E5.m1.1.1.1.1.3.4.2" xref="A2.E5.m1.1.1.1.1.3.4.2.cmml"><mi id="A2.E5.m1.1.1.1.1.3.4.2.2" xref="A2.E5.m1.1.1.1.1.3.4.2.2.cmml">λ</mi><mrow id="A2.E5.m1.1.1.1.1.3.4.2.3" xref="A2.E5.m1.1.1.1.1.3.4.2.3.cmml"><mi id="A2.E5.m1.1.1.1.1.3.4.2.3.2" xref="A2.E5.m1.1.1.1.1.3.4.2.3.2.cmml">d</mi><mo id="A2.E5.m1.1.1.1.1.3.4.2.3.1" xref="A2.E5.m1.1.1.1.1.3.4.2.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.4.2.3.3" xref="A2.E5.m1.1.1.1.1.3.4.2.3.3.cmml">i</mi><mo id="A2.E5.m1.1.1.1.1.3.4.2.3.1a" xref="A2.E5.m1.1.1.1.1.3.4.2.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.4.2.3.4" xref="A2.E5.m1.1.1.1.1.3.4.2.3.4.cmml">c</mi><mo id="A2.E5.m1.1.1.1.1.3.4.2.3.1b" xref="A2.E5.m1.1.1.1.1.3.4.2.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.4.2.3.5" xref="A2.E5.m1.1.1.1.1.3.4.2.3.5.cmml">e</mi></mrow></msub><mo id="A2.E5.m1.1.1.1.1.3.4.1" xref="A2.E5.m1.1.1.1.1.3.4.1.cmml">⁢</mo><msub id="A2.E5.m1.1.1.1.1.3.4.3" xref="A2.E5.m1.1.1.1.1.3.4.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.E5.m1.1.1.1.1.3.4.3.2" xref="A2.E5.m1.1.1.1.1.3.4.3.2.cmml">ℒ</mi><mrow id="A2.E5.m1.1.1.1.1.3.4.3.3" xref="A2.E5.m1.1.1.1.1.3.4.3.3.cmml"><mi id="A2.E5.m1.1.1.1.1.3.4.3.3.2" xref="A2.E5.m1.1.1.1.1.3.4.3.3.2.cmml">d</mi><mo id="A2.E5.m1.1.1.1.1.3.4.3.3.1" xref="A2.E5.m1.1.1.1.1.3.4.3.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.4.3.3.3" xref="A2.E5.m1.1.1.1.1.3.4.3.3.3.cmml">i</mi><mo id="A2.E5.m1.1.1.1.1.3.4.3.3.1a" xref="A2.E5.m1.1.1.1.1.3.4.3.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.4.3.3.4" xref="A2.E5.m1.1.1.1.1.3.4.3.3.4.cmml">c</mi><mo id="A2.E5.m1.1.1.1.1.3.4.3.3.1b" xref="A2.E5.m1.1.1.1.1.3.4.3.3.1.cmml">⁢</mo><mi id="A2.E5.m1.1.1.1.1.3.4.3.3.5" xref="A2.E5.m1.1.1.1.1.3.4.3.3.5.cmml">e</mi></mrow></msub></mrow></mrow></mrow><mo id="A2.E5.m1.1.1.1.2" xref="A2.E5.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E5.m1.1b"><apply id="A2.E5.m1.1.1.1.1.cmml" xref="A2.E5.m1.1.1.1"><eq id="A2.E5.m1.1.1.1.1.1.cmml" xref="A2.E5.m1.1.1.1.1.1"></eq><apply id="A2.E5.m1.1.1.1.1.2.cmml" xref="A2.E5.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="A2.E5.m1.1.1.1.1.2.1.cmml" xref="A2.E5.m1.1.1.1.1.2">subscript</csymbol><ci id="A2.E5.m1.1.1.1.1.2.2.cmml" xref="A2.E5.m1.1.1.1.1.2.2">ℒ</ci><apply id="A2.E5.m1.1.1.1.1.2.3.cmml" xref="A2.E5.m1.1.1.1.1.2.3"><times id="A2.E5.m1.1.1.1.1.2.3.1.cmml" xref="A2.E5.m1.1.1.1.1.2.3.1"></times><ci id="A2.E5.m1.1.1.1.1.2.3.2.cmml" xref="A2.E5.m1.1.1.1.1.2.3.2">𝑝</ci><ci id="A2.E5.m1.1.1.1.1.2.3.3.cmml" xref="A2.E5.m1.1.1.1.1.2.3.3">𝑠</ci></apply></apply><apply id="A2.E5.m1.1.1.1.1.3.cmml" xref="A2.E5.m1.1.1.1.1.3"><plus id="A2.E5.m1.1.1.1.1.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.1"></plus><apply id="A2.E5.m1.1.1.1.1.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.2"><times id="A2.E5.m1.1.1.1.1.3.2.1.cmml" xref="A2.E5.m1.1.1.1.1.3.2.1"></times><apply id="A2.E5.m1.1.1.1.1.3.2.2.cmml" xref="A2.E5.m1.1.1.1.1.3.2.2"><csymbol cd="ambiguous" id="A2.E5.m1.1.1.1.1.3.2.2.1.cmml" xref="A2.E5.m1.1.1.1.1.3.2.2">subscript</csymbol><ci id="A2.E5.m1.1.1.1.1.3.2.2.2.cmml" xref="A2.E5.m1.1.1.1.1.3.2.2.2">𝜆</ci><apply id="A2.E5.m1.1.1.1.1.3.2.2.3.cmml" xref="A2.E5.m1.1.1.1.1.3.2.2.3"><times id="A2.E5.m1.1.1.1.1.3.2.2.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.2.2.3.1"></times><ci id="A2.E5.m1.1.1.1.1.3.2.2.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.2.2.3.2">𝑐</ci><ci id="A2.E5.m1.1.1.1.1.3.2.2.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.2.2.3.3">𝑙</ci><ci id="A2.E5.m1.1.1.1.1.3.2.2.3.4.cmml" xref="A2.E5.m1.1.1.1.1.3.2.2.3.4">𝑠</ci></apply></apply><apply id="A2.E5.m1.1.1.1.1.3.2.3.cmml" xref="A2.E5.m1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="A2.E5.m1.1.1.1.1.3.2.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.2.3">subscript</csymbol><ci id="A2.E5.m1.1.1.1.1.3.2.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.2.3.2">ℒ</ci><apply id="A2.E5.m1.1.1.1.1.3.2.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.2.3.3"><times id="A2.E5.m1.1.1.1.1.3.2.3.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.2.3.3.1"></times><ci id="A2.E5.m1.1.1.1.1.3.2.3.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.2.3.3.2">𝑐</ci><ci id="A2.E5.m1.1.1.1.1.3.2.3.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.2.3.3.3">𝑙</ci><ci id="A2.E5.m1.1.1.1.1.3.2.3.3.4.cmml" xref="A2.E5.m1.1.1.1.1.3.2.3.3.4">𝑠</ci></apply></apply></apply><apply id="A2.E5.m1.1.1.1.1.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.3"><times id="A2.E5.m1.1.1.1.1.3.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.3.1"></times><apply id="A2.E5.m1.1.1.1.1.3.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="A2.E5.m1.1.1.1.1.3.3.2.1.cmml" xref="A2.E5.m1.1.1.1.1.3.3.2">subscript</csymbol><ci id="A2.E5.m1.1.1.1.1.3.3.2.2.cmml" xref="A2.E5.m1.1.1.1.1.3.3.2.2">𝜆</ci><apply id="A2.E5.m1.1.1.1.1.3.3.2.3.cmml" xref="A2.E5.m1.1.1.1.1.3.3.2.3"><times id="A2.E5.m1.1.1.1.1.3.3.2.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.3.2.3.1"></times><ci id="A2.E5.m1.1.1.1.1.3.3.2.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.3.2.3.2">𝑏</ci><ci id="A2.E5.m1.1.1.1.1.3.3.2.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.3.2.3.3">𝑐</ci><ci id="A2.E5.m1.1.1.1.1.3.3.2.3.4.cmml" xref="A2.E5.m1.1.1.1.1.3.3.2.3.4">𝑒</ci></apply></apply><apply id="A2.E5.m1.1.1.1.1.3.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="A2.E5.m1.1.1.1.1.3.3.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="A2.E5.m1.1.1.1.1.3.3.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.3.3.2">ℒ</ci><apply id="A2.E5.m1.1.1.1.1.3.3.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.3.3.3"><times id="A2.E5.m1.1.1.1.1.3.3.3.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.3.3.3.1"></times><ci id="A2.E5.m1.1.1.1.1.3.3.3.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.3.3.3.2">𝑏</ci><ci id="A2.E5.m1.1.1.1.1.3.3.3.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.3.3.3.3">𝑐</ci><ci id="A2.E5.m1.1.1.1.1.3.3.3.3.4.cmml" xref="A2.E5.m1.1.1.1.1.3.3.3.3.4">𝑒</ci></apply></apply></apply><apply id="A2.E5.m1.1.1.1.1.3.4.cmml" xref="A2.E5.m1.1.1.1.1.3.4"><times id="A2.E5.m1.1.1.1.1.3.4.1.cmml" xref="A2.E5.m1.1.1.1.1.3.4.1"></times><apply id="A2.E5.m1.1.1.1.1.3.4.2.cmml" xref="A2.E5.m1.1.1.1.1.3.4.2"><csymbol cd="ambiguous" id="A2.E5.m1.1.1.1.1.3.4.2.1.cmml" xref="A2.E5.m1.1.1.1.1.3.4.2">subscript</csymbol><ci id="A2.E5.m1.1.1.1.1.3.4.2.2.cmml" xref="A2.E5.m1.1.1.1.1.3.4.2.2">𝜆</ci><apply id="A2.E5.m1.1.1.1.1.3.4.2.3.cmml" xref="A2.E5.m1.1.1.1.1.3.4.2.3"><times id="A2.E5.m1.1.1.1.1.3.4.2.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.4.2.3.1"></times><ci id="A2.E5.m1.1.1.1.1.3.4.2.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.4.2.3.2">𝑑</ci><ci id="A2.E5.m1.1.1.1.1.3.4.2.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.4.2.3.3">𝑖</ci><ci id="A2.E5.m1.1.1.1.1.3.4.2.3.4.cmml" xref="A2.E5.m1.1.1.1.1.3.4.2.3.4">𝑐</ci><ci id="A2.E5.m1.1.1.1.1.3.4.2.3.5.cmml" xref="A2.E5.m1.1.1.1.1.3.4.2.3.5">𝑒</ci></apply></apply><apply id="A2.E5.m1.1.1.1.1.3.4.3.cmml" xref="A2.E5.m1.1.1.1.1.3.4.3"><csymbol cd="ambiguous" id="A2.E5.m1.1.1.1.1.3.4.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.4.3">subscript</csymbol><ci id="A2.E5.m1.1.1.1.1.3.4.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.4.3.2">ℒ</ci><apply id="A2.E5.m1.1.1.1.1.3.4.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.4.3.3"><times id="A2.E5.m1.1.1.1.1.3.4.3.3.1.cmml" xref="A2.E5.m1.1.1.1.1.3.4.3.3.1"></times><ci id="A2.E5.m1.1.1.1.1.3.4.3.3.2.cmml" xref="A2.E5.m1.1.1.1.1.3.4.3.3.2">𝑑</ci><ci id="A2.E5.m1.1.1.1.1.3.4.3.3.3.cmml" xref="A2.E5.m1.1.1.1.1.3.4.3.3.3">𝑖</ci><ci id="A2.E5.m1.1.1.1.1.3.4.3.3.4.cmml" xref="A2.E5.m1.1.1.1.1.3.4.3.3.4">𝑐</ci><ci id="A2.E5.m1.1.1.1.1.3.4.3.3.5.cmml" xref="A2.E5.m1.1.1.1.1.3.4.3.3.5">𝑒</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E5.m1.1c">\mathcal{L}_{ps}=\lambda_{cls}\mathcal{L}_{cls}+\lambda_{bce}\mathcal{L}_{bce}%
+\lambda_{dice}\mathcal{L}_{dice},</annotation><annotation encoding="application/x-llamapun" id="A2.E5.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_p italic_s end_POSTSUBSCRIPT = italic_λ start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT italic_b italic_c italic_e end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_b italic_c italic_e end_POSTSUBSCRIPT + italic_λ start_POSTSUBSCRIPT italic_d italic_i italic_c italic_e end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_d italic_i italic_c italic_e end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(5)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.SS1.p1.14">where <math alttext="\lambda_{cls}" class="ltx_Math" display="inline" id="A2.SS1.p1.12.m1.1"><semantics id="A2.SS1.p1.12.m1.1a"><msub id="A2.SS1.p1.12.m1.1.1" xref="A2.SS1.p1.12.m1.1.1.cmml"><mi id="A2.SS1.p1.12.m1.1.1.2" xref="A2.SS1.p1.12.m1.1.1.2.cmml">λ</mi><mrow id="A2.SS1.p1.12.m1.1.1.3" xref="A2.SS1.p1.12.m1.1.1.3.cmml"><mi id="A2.SS1.p1.12.m1.1.1.3.2" xref="A2.SS1.p1.12.m1.1.1.3.2.cmml">c</mi><mo id="A2.SS1.p1.12.m1.1.1.3.1" xref="A2.SS1.p1.12.m1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.12.m1.1.1.3.3" xref="A2.SS1.p1.12.m1.1.1.3.3.cmml">l</mi><mo id="A2.SS1.p1.12.m1.1.1.3.1a" xref="A2.SS1.p1.12.m1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.12.m1.1.1.3.4" xref="A2.SS1.p1.12.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.12.m1.1b"><apply id="A2.SS1.p1.12.m1.1.1.cmml" xref="A2.SS1.p1.12.m1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.12.m1.1.1.1.cmml" xref="A2.SS1.p1.12.m1.1.1">subscript</csymbol><ci id="A2.SS1.p1.12.m1.1.1.2.cmml" xref="A2.SS1.p1.12.m1.1.1.2">𝜆</ci><apply id="A2.SS1.p1.12.m1.1.1.3.cmml" xref="A2.SS1.p1.12.m1.1.1.3"><times id="A2.SS1.p1.12.m1.1.1.3.1.cmml" xref="A2.SS1.p1.12.m1.1.1.3.1"></times><ci id="A2.SS1.p1.12.m1.1.1.3.2.cmml" xref="A2.SS1.p1.12.m1.1.1.3.2">𝑐</ci><ci id="A2.SS1.p1.12.m1.1.1.3.3.cmml" xref="A2.SS1.p1.12.m1.1.1.3.3">𝑙</ci><ci id="A2.SS1.p1.12.m1.1.1.3.4.cmml" xref="A2.SS1.p1.12.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.12.m1.1c">\lambda_{cls}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.12.m1.1d">italic_λ start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="\lambda_{bce}" class="ltx_Math" display="inline" id="A2.SS1.p1.13.m2.1"><semantics id="A2.SS1.p1.13.m2.1a"><msub id="A2.SS1.p1.13.m2.1.1" xref="A2.SS1.p1.13.m2.1.1.cmml"><mi id="A2.SS1.p1.13.m2.1.1.2" xref="A2.SS1.p1.13.m2.1.1.2.cmml">λ</mi><mrow id="A2.SS1.p1.13.m2.1.1.3" xref="A2.SS1.p1.13.m2.1.1.3.cmml"><mi id="A2.SS1.p1.13.m2.1.1.3.2" xref="A2.SS1.p1.13.m2.1.1.3.2.cmml">b</mi><mo id="A2.SS1.p1.13.m2.1.1.3.1" xref="A2.SS1.p1.13.m2.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.13.m2.1.1.3.3" xref="A2.SS1.p1.13.m2.1.1.3.3.cmml">c</mi><mo id="A2.SS1.p1.13.m2.1.1.3.1a" xref="A2.SS1.p1.13.m2.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.13.m2.1.1.3.4" xref="A2.SS1.p1.13.m2.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.13.m2.1b"><apply id="A2.SS1.p1.13.m2.1.1.cmml" xref="A2.SS1.p1.13.m2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.13.m2.1.1.1.cmml" xref="A2.SS1.p1.13.m2.1.1">subscript</csymbol><ci id="A2.SS1.p1.13.m2.1.1.2.cmml" xref="A2.SS1.p1.13.m2.1.1.2">𝜆</ci><apply id="A2.SS1.p1.13.m2.1.1.3.cmml" xref="A2.SS1.p1.13.m2.1.1.3"><times id="A2.SS1.p1.13.m2.1.1.3.1.cmml" xref="A2.SS1.p1.13.m2.1.1.3.1"></times><ci id="A2.SS1.p1.13.m2.1.1.3.2.cmml" xref="A2.SS1.p1.13.m2.1.1.3.2">𝑏</ci><ci id="A2.SS1.p1.13.m2.1.1.3.3.cmml" xref="A2.SS1.p1.13.m2.1.1.3.3">𝑐</ci><ci id="A2.SS1.p1.13.m2.1.1.3.4.cmml" xref="A2.SS1.p1.13.m2.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.13.m2.1c">\lambda_{bce}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.13.m2.1d">italic_λ start_POSTSUBSCRIPT italic_b italic_c italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\lambda_{dice}" class="ltx_Math" display="inline" id="A2.SS1.p1.14.m3.1"><semantics id="A2.SS1.p1.14.m3.1a"><msub id="A2.SS1.p1.14.m3.1.1" xref="A2.SS1.p1.14.m3.1.1.cmml"><mi id="A2.SS1.p1.14.m3.1.1.2" xref="A2.SS1.p1.14.m3.1.1.2.cmml">λ</mi><mrow id="A2.SS1.p1.14.m3.1.1.3" xref="A2.SS1.p1.14.m3.1.1.3.cmml"><mi id="A2.SS1.p1.14.m3.1.1.3.2" xref="A2.SS1.p1.14.m3.1.1.3.2.cmml">d</mi><mo id="A2.SS1.p1.14.m3.1.1.3.1" xref="A2.SS1.p1.14.m3.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.14.m3.1.1.3.3" xref="A2.SS1.p1.14.m3.1.1.3.3.cmml">i</mi><mo id="A2.SS1.p1.14.m3.1.1.3.1a" xref="A2.SS1.p1.14.m3.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.14.m3.1.1.3.4" xref="A2.SS1.p1.14.m3.1.1.3.4.cmml">c</mi><mo id="A2.SS1.p1.14.m3.1.1.3.1b" xref="A2.SS1.p1.14.m3.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p1.14.m3.1.1.3.5" xref="A2.SS1.p1.14.m3.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p1.14.m3.1b"><apply id="A2.SS1.p1.14.m3.1.1.cmml" xref="A2.SS1.p1.14.m3.1.1"><csymbol cd="ambiguous" id="A2.SS1.p1.14.m3.1.1.1.cmml" xref="A2.SS1.p1.14.m3.1.1">subscript</csymbol><ci id="A2.SS1.p1.14.m3.1.1.2.cmml" xref="A2.SS1.p1.14.m3.1.1.2">𝜆</ci><apply id="A2.SS1.p1.14.m3.1.1.3.cmml" xref="A2.SS1.p1.14.m3.1.1.3"><times id="A2.SS1.p1.14.m3.1.1.3.1.cmml" xref="A2.SS1.p1.14.m3.1.1.3.1"></times><ci id="A2.SS1.p1.14.m3.1.1.3.2.cmml" xref="A2.SS1.p1.14.m3.1.1.3.2">𝑑</ci><ci id="A2.SS1.p1.14.m3.1.1.3.3.cmml" xref="A2.SS1.p1.14.m3.1.1.3.3">𝑖</ci><ci id="A2.SS1.p1.14.m3.1.1.3.4.cmml" xref="A2.SS1.p1.14.m3.1.1.3.4">𝑐</ci><ci id="A2.SS1.p1.14.m3.1.1.3.5.cmml" xref="A2.SS1.p1.14.m3.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p1.14.m3.1c">\lambda_{dice}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p1.14.m3.1d">italic_λ start_POSTSUBSCRIPT italic_d italic_i italic_c italic_e end_POSTSUBSCRIPT</annotation></semantics></math> are coefficient weights to control different losses</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.5"><span class="ltx_text ltx_font_bold" id="A2.SS1.p2.5.1">Depth Estimation Loss.</span> Given the prediction <math alttext="\mathbf{O^{p}}" class="ltx_Math" display="inline" id="A2.SS1.p2.1.m1.1"><semantics id="A2.SS1.p2.1.m1.1a"><msup id="A2.SS1.p2.1.m1.1.1" xref="A2.SS1.p2.1.m1.1.1.cmml"><mi id="A2.SS1.p2.1.m1.1.1.2" xref="A2.SS1.p2.1.m1.1.1.2.cmml">𝐎</mi><mi id="A2.SS1.p2.1.m1.1.1.3" xref="A2.SS1.p2.1.m1.1.1.3.cmml">𝐩</mi></msup><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.1.m1.1b"><apply id="A2.SS1.p2.1.m1.1.1.cmml" xref="A2.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.1.1.1.cmml" xref="A2.SS1.p2.1.m1.1.1">superscript</csymbol><ci id="A2.SS1.p2.1.m1.1.1.2.cmml" xref="A2.SS1.p2.1.m1.1.1.2">𝐎</ci><ci id="A2.SS1.p2.1.m1.1.1.3.cmml" xref="A2.SS1.p2.1.m1.1.1.3">𝐩</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.1.m1.1c">\mathbf{O^{p}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.1.m1.1d">bold_O start_POSTSUPERSCRIPT bold_p end_POSTSUPERSCRIPT</annotation></semantics></math> derived from <math alttext="m" class="ltx_Math" display="inline" id="A2.SS1.p2.2.m2.1"><semantics id="A2.SS1.p2.2.m2.1a"><mi id="A2.SS1.p2.2.m2.1.1" xref="A2.SS1.p2.2.m2.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.2.m2.1b"><ci id="A2.SS1.p2.2.m2.1.1.cmml" xref="A2.SS1.p2.2.m2.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.2.m2.1c">m</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.2.m2.1d">italic_m</annotation></semantics></math> latent queries, we use the last (<math alttext="m" class="ltx_Math" display="inline" id="A2.SS1.p2.3.m3.1"><semantics id="A2.SS1.p2.3.m3.1a"><mi id="A2.SS1.p2.3.m3.1.1" xref="A2.SS1.p2.3.m3.1.1.cmml">m</mi><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.3.m3.1b"><ci id="A2.SS1.p2.3.m3.1.1.cmml" xref="A2.SS1.p2.3.m3.1.1">𝑚</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.3.m3.1c">m</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.3.m3.1d">italic_m</annotation></semantics></math>-th) latent querie to make depth prediction. In order to calculate the distance between predicted output <math alttext="\mathbf{\widehat{Y}}_{de}" class="ltx_Math" display="inline" id="A2.SS1.p2.4.m4.1"><semantics id="A2.SS1.p2.4.m4.1a"><msub id="A2.SS1.p2.4.m4.1.1" xref="A2.SS1.p2.4.m4.1.1.cmml"><mover accent="true" id="A2.SS1.p2.4.m4.1.1.2" xref="A2.SS1.p2.4.m4.1.1.2.cmml"><mi id="A2.SS1.p2.4.m4.1.1.2.2" xref="A2.SS1.p2.4.m4.1.1.2.2.cmml">𝐘</mi><mo id="A2.SS1.p2.4.m4.1.1.2.1" xref="A2.SS1.p2.4.m4.1.1.2.1.cmml">^</mo></mover><mrow id="A2.SS1.p2.4.m4.1.1.3" xref="A2.SS1.p2.4.m4.1.1.3.cmml"><mi id="A2.SS1.p2.4.m4.1.1.3.2" xref="A2.SS1.p2.4.m4.1.1.3.2.cmml">d</mi><mo id="A2.SS1.p2.4.m4.1.1.3.1" xref="A2.SS1.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p2.4.m4.1.1.3.3" xref="A2.SS1.p2.4.m4.1.1.3.3.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.4.m4.1b"><apply id="A2.SS1.p2.4.m4.1.1.cmml" xref="A2.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.4.m4.1.1.1.cmml" xref="A2.SS1.p2.4.m4.1.1">subscript</csymbol><apply id="A2.SS1.p2.4.m4.1.1.2.cmml" xref="A2.SS1.p2.4.m4.1.1.2"><ci id="A2.SS1.p2.4.m4.1.1.2.1.cmml" xref="A2.SS1.p2.4.m4.1.1.2.1">^</ci><ci id="A2.SS1.p2.4.m4.1.1.2.2.cmml" xref="A2.SS1.p2.4.m4.1.1.2.2">𝐘</ci></apply><apply id="A2.SS1.p2.4.m4.1.1.3.cmml" xref="A2.SS1.p2.4.m4.1.1.3"><times id="A2.SS1.p2.4.m4.1.1.3.1.cmml" xref="A2.SS1.p2.4.m4.1.1.3.1"></times><ci id="A2.SS1.p2.4.m4.1.1.3.2.cmml" xref="A2.SS1.p2.4.m4.1.1.3.2">𝑑</ci><ci id="A2.SS1.p2.4.m4.1.1.3.3.cmml" xref="A2.SS1.p2.4.m4.1.1.3.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.4.m4.1c">\mathbf{\widehat{Y}}_{de}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.4.m4.1d">over^ start_ARG bold_Y end_ARG start_POSTSUBSCRIPT italic_d italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and ground truth <math alttext="\mathbf{{Y}}_{de}" class="ltx_Math" display="inline" id="A2.SS1.p2.5.m5.1"><semantics id="A2.SS1.p2.5.m5.1a"><msub id="A2.SS1.p2.5.m5.1.1" xref="A2.SS1.p2.5.m5.1.1.cmml"><mi id="A2.SS1.p2.5.m5.1.1.2" xref="A2.SS1.p2.5.m5.1.1.2.cmml">𝐘</mi><mrow id="A2.SS1.p2.5.m5.1.1.3" xref="A2.SS1.p2.5.m5.1.1.3.cmml"><mi id="A2.SS1.p2.5.m5.1.1.3.2" xref="A2.SS1.p2.5.m5.1.1.3.2.cmml">d</mi><mo id="A2.SS1.p2.5.m5.1.1.3.1" xref="A2.SS1.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p2.5.m5.1.1.3.3" xref="A2.SS1.p2.5.m5.1.1.3.3.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.5.m5.1b"><apply id="A2.SS1.p2.5.m5.1.1.cmml" xref="A2.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.5.m5.1.1.1.cmml" xref="A2.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="A2.SS1.p2.5.m5.1.1.2.cmml" xref="A2.SS1.p2.5.m5.1.1.2">𝐘</ci><apply id="A2.SS1.p2.5.m5.1.1.3.cmml" xref="A2.SS1.p2.5.m5.1.1.3"><times id="A2.SS1.p2.5.m5.1.1.3.1.cmml" xref="A2.SS1.p2.5.m5.1.1.3.1"></times><ci id="A2.SS1.p2.5.m5.1.1.3.2.cmml" xref="A2.SS1.p2.5.m5.1.1.3.2">𝑑</ci><ci id="A2.SS1.p2.5.m5.1.1.3.3.cmml" xref="A2.SS1.p2.5.m5.1.1.3.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.5.m5.1c">\mathbf{{Y}}_{de}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.5.m5.1d">bold_Y start_POSTSUBSCRIPT italic_d italic_e end_POSTSUBSCRIPT</annotation></semantics></math>, we use scale-invariant log scale loss <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#bib.bib19" title="">19</a>]</cite>. The equation of training loss is as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E6">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{de}=\frac{1}{n}\sum_{i}{d_{i}}^{2}-\frac{1}{2}(\frac{1}{n}\sum_{i%
}{d_{i}})^{2}," class="ltx_Math" display="block" id="A2.E6.m1.1"><semantics id="A2.E6.m1.1a"><mrow id="A2.E6.m1.1.1.1" xref="A2.E6.m1.1.1.1.1.cmml"><mrow id="A2.E6.m1.1.1.1.1" xref="A2.E6.m1.1.1.1.1.cmml"><msub id="A2.E6.m1.1.1.1.1.3" xref="A2.E6.m1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.E6.m1.1.1.1.1.3.2" xref="A2.E6.m1.1.1.1.1.3.2.cmml">ℒ</mi><mrow id="A2.E6.m1.1.1.1.1.3.3" xref="A2.E6.m1.1.1.1.1.3.3.cmml"><mi id="A2.E6.m1.1.1.1.1.3.3.2" xref="A2.E6.m1.1.1.1.1.3.3.2.cmml">d</mi><mo id="A2.E6.m1.1.1.1.1.3.3.1" xref="A2.E6.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="A2.E6.m1.1.1.1.1.3.3.3" xref="A2.E6.m1.1.1.1.1.3.3.3.cmml">e</mi></mrow></msub><mo id="A2.E6.m1.1.1.1.1.2" xref="A2.E6.m1.1.1.1.1.2.cmml">=</mo><mrow id="A2.E6.m1.1.1.1.1.1" xref="A2.E6.m1.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.1.1.1.1.1.3" xref="A2.E6.m1.1.1.1.1.1.3.cmml"><mfrac id="A2.E6.m1.1.1.1.1.1.3.2" xref="A2.E6.m1.1.1.1.1.1.3.2.cmml"><mn id="A2.E6.m1.1.1.1.1.1.3.2.2" xref="A2.E6.m1.1.1.1.1.1.3.2.2.cmml">1</mn><mi id="A2.E6.m1.1.1.1.1.1.3.2.3" xref="A2.E6.m1.1.1.1.1.1.3.2.3.cmml">n</mi></mfrac><mo id="A2.E6.m1.1.1.1.1.1.3.1" xref="A2.E6.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mrow id="A2.E6.m1.1.1.1.1.1.3.3" xref="A2.E6.m1.1.1.1.1.1.3.3.cmml"><munder id="A2.E6.m1.1.1.1.1.1.3.3.1" xref="A2.E6.m1.1.1.1.1.1.3.3.1.cmml"><mo id="A2.E6.m1.1.1.1.1.1.3.3.1.2" movablelimits="false" xref="A2.E6.m1.1.1.1.1.1.3.3.1.2.cmml">∑</mo><mi id="A2.E6.m1.1.1.1.1.1.3.3.1.3" xref="A2.E6.m1.1.1.1.1.1.3.3.1.3.cmml">i</mi></munder><mmultiscripts id="A2.E6.m1.1.1.1.1.1.3.3.2" xref="A2.E6.m1.1.1.1.1.1.3.3.2.cmml"><mi id="A2.E6.m1.1.1.1.1.1.3.3.2.2.2" xref="A2.E6.m1.1.1.1.1.1.3.3.2.2.2.cmml">d</mi><mi id="A2.E6.m1.1.1.1.1.1.3.3.2.2.3" xref="A2.E6.m1.1.1.1.1.1.3.3.2.2.3.cmml">i</mi><mrow id="A2.E6.m1.1.1.1.1.1.3.3.2a" xref="A2.E6.m1.1.1.1.1.1.3.3.2.cmml"></mrow><mrow id="A2.E6.m1.1.1.1.1.1.3.3.2b" xref="A2.E6.m1.1.1.1.1.1.3.3.2.cmml"></mrow><mn id="A2.E6.m1.1.1.1.1.1.3.3.2.3" xref="A2.E6.m1.1.1.1.1.1.3.3.2.3.cmml">2</mn></mmultiscripts></mrow></mrow><mo id="A2.E6.m1.1.1.1.1.1.2" xref="A2.E6.m1.1.1.1.1.1.2.cmml">−</mo><mrow id="A2.E6.m1.1.1.1.1.1.1" xref="A2.E6.m1.1.1.1.1.1.1.cmml"><mfrac id="A2.E6.m1.1.1.1.1.1.1.3" xref="A2.E6.m1.1.1.1.1.1.1.3.cmml"><mn id="A2.E6.m1.1.1.1.1.1.1.3.2" xref="A2.E6.m1.1.1.1.1.1.1.3.2.cmml">1</mn><mn id="A2.E6.m1.1.1.1.1.1.1.3.3" xref="A2.E6.m1.1.1.1.1.1.1.3.3.cmml">2</mn></mfrac><mo id="A2.E6.m1.1.1.1.1.1.1.2" xref="A2.E6.m1.1.1.1.1.1.1.2.cmml">⁢</mo><msup id="A2.E6.m1.1.1.1.1.1.1.1" xref="A2.E6.m1.1.1.1.1.1.1.1.cmml"><mrow id="A2.E6.m1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="A2.E6.m1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mfrac id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.cmml"><mn id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">1</mn><mi id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">n</mi></mfrac><mo id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.1" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml">⁢</mo><mrow id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.cmml"><munder id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml"><mo id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.2" movablelimits="false" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.2.cmml">∑</mo><mi id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.3" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.3.cmml">i</mi></munder><msub id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml"><mi id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">d</mi><mi id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml">i</mi></msub></mrow></mrow><mo id="A2.E6.m1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mn id="A2.E6.m1.1.1.1.1.1.1.1.3" xref="A2.E6.m1.1.1.1.1.1.1.1.3.cmml">2</mn></msup></mrow></mrow></mrow><mo id="A2.E6.m1.1.1.1.2" xref="A2.E6.m1.1.1.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E6.m1.1b"><apply id="A2.E6.m1.1.1.1.1.cmml" xref="A2.E6.m1.1.1.1"><eq id="A2.E6.m1.1.1.1.1.2.cmml" xref="A2.E6.m1.1.1.1.1.2"></eq><apply id="A2.E6.m1.1.1.1.1.3.cmml" xref="A2.E6.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="A2.E6.m1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.1.1.1.1.3">subscript</csymbol><ci id="A2.E6.m1.1.1.1.1.3.2.cmml" xref="A2.E6.m1.1.1.1.1.3.2">ℒ</ci><apply id="A2.E6.m1.1.1.1.1.3.3.cmml" xref="A2.E6.m1.1.1.1.1.3.3"><times id="A2.E6.m1.1.1.1.1.3.3.1.cmml" xref="A2.E6.m1.1.1.1.1.3.3.1"></times><ci id="A2.E6.m1.1.1.1.1.3.3.2.cmml" xref="A2.E6.m1.1.1.1.1.3.3.2">𝑑</ci><ci id="A2.E6.m1.1.1.1.1.3.3.3.cmml" xref="A2.E6.m1.1.1.1.1.3.3.3">𝑒</ci></apply></apply><apply id="A2.E6.m1.1.1.1.1.1.cmml" xref="A2.E6.m1.1.1.1.1.1"><minus id="A2.E6.m1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.1.1.1.1.1.2"></minus><apply id="A2.E6.m1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.1.1.1.1.1.3"><times id="A2.E6.m1.1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.1.1.1.1.1.3.1"></times><apply id="A2.E6.m1.1.1.1.1.1.3.2.cmml" xref="A2.E6.m1.1.1.1.1.1.3.2"><divide id="A2.E6.m1.1.1.1.1.1.3.2.1.cmml" xref="A2.E6.m1.1.1.1.1.1.3.2"></divide><cn id="A2.E6.m1.1.1.1.1.1.3.2.2.cmml" type="integer" xref="A2.E6.m1.1.1.1.1.1.3.2.2">1</cn><ci id="A2.E6.m1.1.1.1.1.1.3.2.3.cmml" xref="A2.E6.m1.1.1.1.1.1.3.2.3">𝑛</ci></apply><apply id="A2.E6.m1.1.1.1.1.1.3.3.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3"><apply id="A2.E6.m1.1.1.1.1.1.3.3.1.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.1"><csymbol cd="ambiguous" id="A2.E6.m1.1.1.1.1.1.3.3.1.1.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.1">subscript</csymbol><sum id="A2.E6.m1.1.1.1.1.1.3.3.1.2.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.1.2"></sum><ci id="A2.E6.m1.1.1.1.1.1.3.3.1.3.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.1.3">𝑖</ci></apply><apply id="A2.E6.m1.1.1.1.1.1.3.3.2.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="A2.E6.m1.1.1.1.1.1.3.3.2.1.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.2">superscript</csymbol><apply id="A2.E6.m1.1.1.1.1.1.3.3.2.2.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.2"><csymbol cd="ambiguous" id="A2.E6.m1.1.1.1.1.1.3.3.2.2.1.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.2">subscript</csymbol><ci id="A2.E6.m1.1.1.1.1.1.3.3.2.2.2.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.2.2.2">𝑑</ci><ci id="A2.E6.m1.1.1.1.1.1.3.3.2.2.3.cmml" xref="A2.E6.m1.1.1.1.1.1.3.3.2.2.3">𝑖</ci></apply><cn id="A2.E6.m1.1.1.1.1.1.3.3.2.3.cmml" type="integer" xref="A2.E6.m1.1.1.1.1.1.3.3.2.3">2</cn></apply></apply></apply><apply id="A2.E6.m1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.1.1.1.1.1.1"><times id="A2.E6.m1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.1.1.1.1.1.1.2"></times><apply id="A2.E6.m1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.1.1.1.1.1.1.3"><divide id="A2.E6.m1.1.1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.1.1.1.1.1.1.3"></divide><cn id="A2.E6.m1.1.1.1.1.1.1.3.2.cmml" type="integer" xref="A2.E6.m1.1.1.1.1.1.1.3.2">1</cn><cn id="A2.E6.m1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="A2.E6.m1.1.1.1.1.1.1.3.3">2</cn></apply><apply id="A2.E6.m1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.E6.m1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1">superscript</csymbol><apply id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1"><times id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.1"></times><apply id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2"><divide id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2"></divide><cn id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" type="integer" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.2">1</cn><ci id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.2.3">𝑛</ci></apply><apply id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3"><apply id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1"><csymbol cd="ambiguous" id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.1.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1">subscript</csymbol><sum id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.2.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.2"></sum><ci id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.3.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.1.3">𝑖</ci></apply><apply id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2">subscript</csymbol><ci id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.2">𝑑</ci><ci id="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="A2.E6.m1.1.1.1.1.1.1.1.1.1.1.3.2.3">𝑖</ci></apply></apply></apply><cn id="A2.E6.m1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="A2.E6.m1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E6.m1.1c">\mathcal{L}_{de}=\frac{1}{n}\sum_{i}{d_{i}}^{2}-\frac{1}{2}(\frac{1}{n}\sum_{i%
}{d_{i}})^{2},</annotation><annotation encoding="application/x-llamapun" id="A2.E6.m1.1d">caligraphic_L start_POSTSUBSCRIPT italic_d italic_e end_POSTSUBSCRIPT = divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT - divide start_ARG 1 end_ARG start_ARG 2 end_ARG ( divide start_ARG 1 end_ARG start_ARG italic_n end_ARG ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(6)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.SS1.p2.11">where <math alttext="d_{i}=log(y_{i})-log(\widehat{y_{i}})" class="ltx_Math" display="inline" id="A2.SS1.p2.6.m1.2"><semantics id="A2.SS1.p2.6.m1.2a"><mrow id="A2.SS1.p2.6.m1.2.2" xref="A2.SS1.p2.6.m1.2.2.cmml"><msub id="A2.SS1.p2.6.m1.2.2.3" xref="A2.SS1.p2.6.m1.2.2.3.cmml"><mi id="A2.SS1.p2.6.m1.2.2.3.2" xref="A2.SS1.p2.6.m1.2.2.3.2.cmml">d</mi><mi id="A2.SS1.p2.6.m1.2.2.3.3" xref="A2.SS1.p2.6.m1.2.2.3.3.cmml">i</mi></msub><mo id="A2.SS1.p2.6.m1.2.2.2" xref="A2.SS1.p2.6.m1.2.2.2.cmml">=</mo><mrow id="A2.SS1.p2.6.m1.2.2.1" xref="A2.SS1.p2.6.m1.2.2.1.cmml"><mrow id="A2.SS1.p2.6.m1.2.2.1.1" xref="A2.SS1.p2.6.m1.2.2.1.1.cmml"><mi id="A2.SS1.p2.6.m1.2.2.1.1.3" xref="A2.SS1.p2.6.m1.2.2.1.1.3.cmml">l</mi><mo id="A2.SS1.p2.6.m1.2.2.1.1.2" xref="A2.SS1.p2.6.m1.2.2.1.1.2.cmml">⁢</mo><mi id="A2.SS1.p2.6.m1.2.2.1.1.4" xref="A2.SS1.p2.6.m1.2.2.1.1.4.cmml">o</mi><mo id="A2.SS1.p2.6.m1.2.2.1.1.2a" xref="A2.SS1.p2.6.m1.2.2.1.1.2.cmml">⁢</mo><mi id="A2.SS1.p2.6.m1.2.2.1.1.5" xref="A2.SS1.p2.6.m1.2.2.1.1.5.cmml">g</mi><mo id="A2.SS1.p2.6.m1.2.2.1.1.2b" xref="A2.SS1.p2.6.m1.2.2.1.1.2.cmml">⁢</mo><mrow id="A2.SS1.p2.6.m1.2.2.1.1.1.1" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.cmml"><mo id="A2.SS1.p2.6.m1.2.2.1.1.1.1.2" stretchy="false" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.cmml">(</mo><msub id="A2.SS1.p2.6.m1.2.2.1.1.1.1.1" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.cmml"><mi id="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.2" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.2.cmml">y</mi><mi id="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.3" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.3.cmml">i</mi></msub><mo id="A2.SS1.p2.6.m1.2.2.1.1.1.1.3" stretchy="false" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="A2.SS1.p2.6.m1.2.2.1.2" xref="A2.SS1.p2.6.m1.2.2.1.2.cmml">−</mo><mrow id="A2.SS1.p2.6.m1.2.2.1.3" xref="A2.SS1.p2.6.m1.2.2.1.3.cmml"><mi id="A2.SS1.p2.6.m1.2.2.1.3.2" xref="A2.SS1.p2.6.m1.2.2.1.3.2.cmml">l</mi><mo id="A2.SS1.p2.6.m1.2.2.1.3.1" xref="A2.SS1.p2.6.m1.2.2.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p2.6.m1.2.2.1.3.3" xref="A2.SS1.p2.6.m1.2.2.1.3.3.cmml">o</mi><mo id="A2.SS1.p2.6.m1.2.2.1.3.1a" xref="A2.SS1.p2.6.m1.2.2.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p2.6.m1.2.2.1.3.4" xref="A2.SS1.p2.6.m1.2.2.1.3.4.cmml">g</mi><mo id="A2.SS1.p2.6.m1.2.2.1.3.1b" xref="A2.SS1.p2.6.m1.2.2.1.3.1.cmml">⁢</mo><mrow id="A2.SS1.p2.6.m1.2.2.1.3.5.2" xref="A2.SS1.p2.6.m1.1.1.cmml"><mo id="A2.SS1.p2.6.m1.2.2.1.3.5.2.1" stretchy="false" xref="A2.SS1.p2.6.m1.1.1.cmml">(</mo><mover accent="true" id="A2.SS1.p2.6.m1.1.1" xref="A2.SS1.p2.6.m1.1.1.cmml"><msub id="A2.SS1.p2.6.m1.1.1.2" xref="A2.SS1.p2.6.m1.1.1.2.cmml"><mi id="A2.SS1.p2.6.m1.1.1.2.2" xref="A2.SS1.p2.6.m1.1.1.2.2.cmml">y</mi><mi id="A2.SS1.p2.6.m1.1.1.2.3" xref="A2.SS1.p2.6.m1.1.1.2.3.cmml">i</mi></msub><mo id="A2.SS1.p2.6.m1.1.1.1" xref="A2.SS1.p2.6.m1.1.1.1.cmml">^</mo></mover><mo id="A2.SS1.p2.6.m1.2.2.1.3.5.2.2" stretchy="false" xref="A2.SS1.p2.6.m1.1.1.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.6.m1.2b"><apply id="A2.SS1.p2.6.m1.2.2.cmml" xref="A2.SS1.p2.6.m1.2.2"><eq id="A2.SS1.p2.6.m1.2.2.2.cmml" xref="A2.SS1.p2.6.m1.2.2.2"></eq><apply id="A2.SS1.p2.6.m1.2.2.3.cmml" xref="A2.SS1.p2.6.m1.2.2.3"><csymbol cd="ambiguous" id="A2.SS1.p2.6.m1.2.2.3.1.cmml" xref="A2.SS1.p2.6.m1.2.2.3">subscript</csymbol><ci id="A2.SS1.p2.6.m1.2.2.3.2.cmml" xref="A2.SS1.p2.6.m1.2.2.3.2">𝑑</ci><ci id="A2.SS1.p2.6.m1.2.2.3.3.cmml" xref="A2.SS1.p2.6.m1.2.2.3.3">𝑖</ci></apply><apply id="A2.SS1.p2.6.m1.2.2.1.cmml" xref="A2.SS1.p2.6.m1.2.2.1"><minus id="A2.SS1.p2.6.m1.2.2.1.2.cmml" xref="A2.SS1.p2.6.m1.2.2.1.2"></minus><apply id="A2.SS1.p2.6.m1.2.2.1.1.cmml" xref="A2.SS1.p2.6.m1.2.2.1.1"><times id="A2.SS1.p2.6.m1.2.2.1.1.2.cmml" xref="A2.SS1.p2.6.m1.2.2.1.1.2"></times><ci id="A2.SS1.p2.6.m1.2.2.1.1.3.cmml" xref="A2.SS1.p2.6.m1.2.2.1.1.3">𝑙</ci><ci id="A2.SS1.p2.6.m1.2.2.1.1.4.cmml" xref="A2.SS1.p2.6.m1.2.2.1.1.4">𝑜</ci><ci id="A2.SS1.p2.6.m1.2.2.1.1.5.cmml" xref="A2.SS1.p2.6.m1.2.2.1.1.5">𝑔</ci><apply id="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.cmml" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.1.cmml" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1">subscript</csymbol><ci id="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.2.cmml" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.2">𝑦</ci><ci id="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.3.cmml" xref="A2.SS1.p2.6.m1.2.2.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="A2.SS1.p2.6.m1.2.2.1.3.cmml" xref="A2.SS1.p2.6.m1.2.2.1.3"><times id="A2.SS1.p2.6.m1.2.2.1.3.1.cmml" xref="A2.SS1.p2.6.m1.2.2.1.3.1"></times><ci id="A2.SS1.p2.6.m1.2.2.1.3.2.cmml" xref="A2.SS1.p2.6.m1.2.2.1.3.2">𝑙</ci><ci id="A2.SS1.p2.6.m1.2.2.1.3.3.cmml" xref="A2.SS1.p2.6.m1.2.2.1.3.3">𝑜</ci><ci id="A2.SS1.p2.6.m1.2.2.1.3.4.cmml" xref="A2.SS1.p2.6.m1.2.2.1.3.4">𝑔</ci><apply id="A2.SS1.p2.6.m1.1.1.cmml" xref="A2.SS1.p2.6.m1.2.2.1.3.5.2"><ci id="A2.SS1.p2.6.m1.1.1.1.cmml" xref="A2.SS1.p2.6.m1.1.1.1">^</ci><apply id="A2.SS1.p2.6.m1.1.1.2.cmml" xref="A2.SS1.p2.6.m1.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p2.6.m1.1.1.2.1.cmml" xref="A2.SS1.p2.6.m1.1.1.2">subscript</csymbol><ci id="A2.SS1.p2.6.m1.1.1.2.2.cmml" xref="A2.SS1.p2.6.m1.1.1.2.2">𝑦</ci><ci id="A2.SS1.p2.6.m1.1.1.2.3.cmml" xref="A2.SS1.p2.6.m1.1.1.2.3">𝑖</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.6.m1.2c">d_{i}=log(y_{i})-log(\widehat{y_{i}})</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.6.m1.2d">italic_d start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = italic_l italic_o italic_g ( italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) - italic_l italic_o italic_g ( over^ start_ARG italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG )</annotation></semantics></math>, <math alttext="y_{i}" class="ltx_Math" display="inline" id="A2.SS1.p2.7.m2.1"><semantics id="A2.SS1.p2.7.m2.1a"><msub id="A2.SS1.p2.7.m2.1.1" xref="A2.SS1.p2.7.m2.1.1.cmml"><mi id="A2.SS1.p2.7.m2.1.1.2" xref="A2.SS1.p2.7.m2.1.1.2.cmml">y</mi><mi id="A2.SS1.p2.7.m2.1.1.3" xref="A2.SS1.p2.7.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.7.m2.1b"><apply id="A2.SS1.p2.7.m2.1.1.cmml" xref="A2.SS1.p2.7.m2.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.7.m2.1.1.1.cmml" xref="A2.SS1.p2.7.m2.1.1">subscript</csymbol><ci id="A2.SS1.p2.7.m2.1.1.2.cmml" xref="A2.SS1.p2.7.m2.1.1.2">𝑦</ci><ci id="A2.SS1.p2.7.m2.1.1.3.cmml" xref="A2.SS1.p2.7.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.7.m2.1c">y_{i}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.7.m2.1d">italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\widehat{y_{i}}" class="ltx_Math" display="inline" id="A2.SS1.p2.8.m3.1"><semantics id="A2.SS1.p2.8.m3.1a"><mover accent="true" id="A2.SS1.p2.8.m3.1.1" xref="A2.SS1.p2.8.m3.1.1.cmml"><msub id="A2.SS1.p2.8.m3.1.1.2" xref="A2.SS1.p2.8.m3.1.1.2.cmml"><mi id="A2.SS1.p2.8.m3.1.1.2.2" xref="A2.SS1.p2.8.m3.1.1.2.2.cmml">y</mi><mi id="A2.SS1.p2.8.m3.1.1.2.3" xref="A2.SS1.p2.8.m3.1.1.2.3.cmml">i</mi></msub><mo id="A2.SS1.p2.8.m3.1.1.1" xref="A2.SS1.p2.8.m3.1.1.1.cmml">^</mo></mover><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.8.m3.1b"><apply id="A2.SS1.p2.8.m3.1.1.cmml" xref="A2.SS1.p2.8.m3.1.1"><ci id="A2.SS1.p2.8.m3.1.1.1.cmml" xref="A2.SS1.p2.8.m3.1.1.1">^</ci><apply id="A2.SS1.p2.8.m3.1.1.2.cmml" xref="A2.SS1.p2.8.m3.1.1.2"><csymbol cd="ambiguous" id="A2.SS1.p2.8.m3.1.1.2.1.cmml" xref="A2.SS1.p2.8.m3.1.1.2">subscript</csymbol><ci id="A2.SS1.p2.8.m3.1.1.2.2.cmml" xref="A2.SS1.p2.8.m3.1.1.2.2">𝑦</ci><ci id="A2.SS1.p2.8.m3.1.1.2.3.cmml" xref="A2.SS1.p2.8.m3.1.1.2.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.8.m3.1c">\widehat{y_{i}}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.8.m3.1d">over^ start_ARG italic_y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_ARG</annotation></semantics></math> are <math alttext="i" class="ltx_Math" display="inline" id="A2.SS1.p2.9.m4.1"><semantics id="A2.SS1.p2.9.m4.1a"><mi id="A2.SS1.p2.9.m4.1.1" xref="A2.SS1.p2.9.m4.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.9.m4.1b"><ci id="A2.SS1.p2.9.m4.1.1.cmml" xref="A2.SS1.p2.9.m4.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.9.m4.1c">i</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.9.m4.1d">italic_i</annotation></semantics></math>th pixel-value of <math alttext="\mathbf{{Y}}_{de}" class="ltx_Math" display="inline" id="A2.SS1.p2.10.m5.1"><semantics id="A2.SS1.p2.10.m5.1a"><msub id="A2.SS1.p2.10.m5.1.1" xref="A2.SS1.p2.10.m5.1.1.cmml"><mi id="A2.SS1.p2.10.m5.1.1.2" xref="A2.SS1.p2.10.m5.1.1.2.cmml">𝐘</mi><mrow id="A2.SS1.p2.10.m5.1.1.3" xref="A2.SS1.p2.10.m5.1.1.3.cmml"><mi id="A2.SS1.p2.10.m5.1.1.3.2" xref="A2.SS1.p2.10.m5.1.1.3.2.cmml">d</mi><mo id="A2.SS1.p2.10.m5.1.1.3.1" xref="A2.SS1.p2.10.m5.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p2.10.m5.1.1.3.3" xref="A2.SS1.p2.10.m5.1.1.3.3.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.10.m5.1b"><apply id="A2.SS1.p2.10.m5.1.1.cmml" xref="A2.SS1.p2.10.m5.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.10.m5.1.1.1.cmml" xref="A2.SS1.p2.10.m5.1.1">subscript</csymbol><ci id="A2.SS1.p2.10.m5.1.1.2.cmml" xref="A2.SS1.p2.10.m5.1.1.2">𝐘</ci><apply id="A2.SS1.p2.10.m5.1.1.3.cmml" xref="A2.SS1.p2.10.m5.1.1.3"><times id="A2.SS1.p2.10.m5.1.1.3.1.cmml" xref="A2.SS1.p2.10.m5.1.1.3.1"></times><ci id="A2.SS1.p2.10.m5.1.1.3.2.cmml" xref="A2.SS1.p2.10.m5.1.1.3.2">𝑑</ci><ci id="A2.SS1.p2.10.m5.1.1.3.3.cmml" xref="A2.SS1.p2.10.m5.1.1.3.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.10.m5.1c">\mathbf{{Y}}_{de}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.10.m5.1d">bold_Y start_POSTSUBSCRIPT italic_d italic_e end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathbf{\widehat{Y}}_{de}" class="ltx_Math" display="inline" id="A2.SS1.p2.11.m6.1"><semantics id="A2.SS1.p2.11.m6.1a"><msub id="A2.SS1.p2.11.m6.1.1" xref="A2.SS1.p2.11.m6.1.1.cmml"><mover accent="true" id="A2.SS1.p2.11.m6.1.1.2" xref="A2.SS1.p2.11.m6.1.1.2.cmml"><mi id="A2.SS1.p2.11.m6.1.1.2.2" xref="A2.SS1.p2.11.m6.1.1.2.2.cmml">𝐘</mi><mo id="A2.SS1.p2.11.m6.1.1.2.1" xref="A2.SS1.p2.11.m6.1.1.2.1.cmml">^</mo></mover><mrow id="A2.SS1.p2.11.m6.1.1.3" xref="A2.SS1.p2.11.m6.1.1.3.cmml"><mi id="A2.SS1.p2.11.m6.1.1.3.2" xref="A2.SS1.p2.11.m6.1.1.3.2.cmml">d</mi><mo id="A2.SS1.p2.11.m6.1.1.3.1" xref="A2.SS1.p2.11.m6.1.1.3.1.cmml">⁢</mo><mi id="A2.SS1.p2.11.m6.1.1.3.3" xref="A2.SS1.p2.11.m6.1.1.3.3.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.11.m6.1b"><apply id="A2.SS1.p2.11.m6.1.1.cmml" xref="A2.SS1.p2.11.m6.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.11.m6.1.1.1.cmml" xref="A2.SS1.p2.11.m6.1.1">subscript</csymbol><apply id="A2.SS1.p2.11.m6.1.1.2.cmml" xref="A2.SS1.p2.11.m6.1.1.2"><ci id="A2.SS1.p2.11.m6.1.1.2.1.cmml" xref="A2.SS1.p2.11.m6.1.1.2.1">^</ci><ci id="A2.SS1.p2.11.m6.1.1.2.2.cmml" xref="A2.SS1.p2.11.m6.1.1.2.2">𝐘</ci></apply><apply id="A2.SS1.p2.11.m6.1.1.3.cmml" xref="A2.SS1.p2.11.m6.1.1.3"><times id="A2.SS1.p2.11.m6.1.1.3.1.cmml" xref="A2.SS1.p2.11.m6.1.1.3.1"></times><ci id="A2.SS1.p2.11.m6.1.1.3.2.cmml" xref="A2.SS1.p2.11.m6.1.1.3.2">𝑑</ci><ci id="A2.SS1.p2.11.m6.1.1.3.3.cmml" xref="A2.SS1.p2.11.m6.1.1.3.3">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.11.m6.1c">\mathbf{\widehat{Y}}_{de}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.11.m6.1d">over^ start_ARG bold_Y end_ARG start_POSTSUBSCRIPT italic_d italic_e end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Token-level Output Loss</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.5">For token-level tasks, we begin by extracting embeddings for all tokens in the vocabulary, which has a size of <math alttext="V" class="ltx_Math" display="inline" id="A2.SS2.p1.1.m1.1"><semantics id="A2.SS2.p1.1.m1.1a"><mi id="A2.SS2.p1.1.m1.1.1" xref="A2.SS2.p1.1.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.1.m1.1b"><ci id="A2.SS2.p1.1.m1.1.1.cmml" xref="A2.SS2.p1.1.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.1.m1.1c">V</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.1.m1.1d">italic_V</annotation></semantics></math>, from the text encoder. Using the last <math alttext="n" class="ltx_Math" display="inline" id="A2.SS2.p1.2.m2.1"><semantics id="A2.SS2.p1.2.m2.1a"><mi id="A2.SS2.p1.2.m2.1.1" xref="A2.SS2.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.2.m2.1b"><ci id="A2.SS2.p1.2.m2.1.1.cmml" xref="A2.SS2.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.2.m2.1d">italic_n</annotation></semantics></math> semantic token-level outputs from <span class="ltx_text ltx_font_smallcaps" id="A2.SS2.p1.5.1">@Model</span>, we calculate the dot product with all token embeddings to generate an affinity matrix <math alttext="\mathbf{S}_{token}{\in}\mathcal{R}^{n\times V}" class="ltx_Math" display="inline" id="A2.SS2.p1.3.m3.1"><semantics id="A2.SS2.p1.3.m3.1a"><mrow id="A2.SS2.p1.3.m3.1.1" xref="A2.SS2.p1.3.m3.1.1.cmml"><msub id="A2.SS2.p1.3.m3.1.1.2" xref="A2.SS2.p1.3.m3.1.1.2.cmml"><mi id="A2.SS2.p1.3.m3.1.1.2.2" xref="A2.SS2.p1.3.m3.1.1.2.2.cmml">𝐒</mi><mrow id="A2.SS2.p1.3.m3.1.1.2.3" xref="A2.SS2.p1.3.m3.1.1.2.3.cmml"><mi id="A2.SS2.p1.3.m3.1.1.2.3.2" xref="A2.SS2.p1.3.m3.1.1.2.3.2.cmml">t</mi><mo id="A2.SS2.p1.3.m3.1.1.2.3.1" xref="A2.SS2.p1.3.m3.1.1.2.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.3.m3.1.1.2.3.3" xref="A2.SS2.p1.3.m3.1.1.2.3.3.cmml">o</mi><mo id="A2.SS2.p1.3.m3.1.1.2.3.1a" xref="A2.SS2.p1.3.m3.1.1.2.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.3.m3.1.1.2.3.4" xref="A2.SS2.p1.3.m3.1.1.2.3.4.cmml">k</mi><mo id="A2.SS2.p1.3.m3.1.1.2.3.1b" xref="A2.SS2.p1.3.m3.1.1.2.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.3.m3.1.1.2.3.5" xref="A2.SS2.p1.3.m3.1.1.2.3.5.cmml">e</mi><mo id="A2.SS2.p1.3.m3.1.1.2.3.1c" xref="A2.SS2.p1.3.m3.1.1.2.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.3.m3.1.1.2.3.6" xref="A2.SS2.p1.3.m3.1.1.2.3.6.cmml">n</mi></mrow></msub><mo id="A2.SS2.p1.3.m3.1.1.1" xref="A2.SS2.p1.3.m3.1.1.1.cmml">∈</mo><msup id="A2.SS2.p1.3.m3.1.1.3" xref="A2.SS2.p1.3.m3.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS2.p1.3.m3.1.1.3.2" xref="A2.SS2.p1.3.m3.1.1.3.2.cmml">ℛ</mi><mrow id="A2.SS2.p1.3.m3.1.1.3.3" xref="A2.SS2.p1.3.m3.1.1.3.3.cmml"><mi id="A2.SS2.p1.3.m3.1.1.3.3.2" xref="A2.SS2.p1.3.m3.1.1.3.3.2.cmml">n</mi><mo id="A2.SS2.p1.3.m3.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="A2.SS2.p1.3.m3.1.1.3.3.1.cmml">×</mo><mi id="A2.SS2.p1.3.m3.1.1.3.3.3" xref="A2.SS2.p1.3.m3.1.1.3.3.3.cmml">V</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.3.m3.1b"><apply id="A2.SS2.p1.3.m3.1.1.cmml" xref="A2.SS2.p1.3.m3.1.1"><in id="A2.SS2.p1.3.m3.1.1.1.cmml" xref="A2.SS2.p1.3.m3.1.1.1"></in><apply id="A2.SS2.p1.3.m3.1.1.2.cmml" xref="A2.SS2.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="A2.SS2.p1.3.m3.1.1.2.1.cmml" xref="A2.SS2.p1.3.m3.1.1.2">subscript</csymbol><ci id="A2.SS2.p1.3.m3.1.1.2.2.cmml" xref="A2.SS2.p1.3.m3.1.1.2.2">𝐒</ci><apply id="A2.SS2.p1.3.m3.1.1.2.3.cmml" xref="A2.SS2.p1.3.m3.1.1.2.3"><times id="A2.SS2.p1.3.m3.1.1.2.3.1.cmml" xref="A2.SS2.p1.3.m3.1.1.2.3.1"></times><ci id="A2.SS2.p1.3.m3.1.1.2.3.2.cmml" xref="A2.SS2.p1.3.m3.1.1.2.3.2">𝑡</ci><ci id="A2.SS2.p1.3.m3.1.1.2.3.3.cmml" xref="A2.SS2.p1.3.m3.1.1.2.3.3">𝑜</ci><ci id="A2.SS2.p1.3.m3.1.1.2.3.4.cmml" xref="A2.SS2.p1.3.m3.1.1.2.3.4">𝑘</ci><ci id="A2.SS2.p1.3.m3.1.1.2.3.5.cmml" xref="A2.SS2.p1.3.m3.1.1.2.3.5">𝑒</ci><ci id="A2.SS2.p1.3.m3.1.1.2.3.6.cmml" xref="A2.SS2.p1.3.m3.1.1.2.3.6">𝑛</ci></apply></apply><apply id="A2.SS2.p1.3.m3.1.1.3.cmml" xref="A2.SS2.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="A2.SS2.p1.3.m3.1.1.3.1.cmml" xref="A2.SS2.p1.3.m3.1.1.3">superscript</csymbol><ci id="A2.SS2.p1.3.m3.1.1.3.2.cmml" xref="A2.SS2.p1.3.m3.1.1.3.2">ℛ</ci><apply id="A2.SS2.p1.3.m3.1.1.3.3.cmml" xref="A2.SS2.p1.3.m3.1.1.3.3"><times id="A2.SS2.p1.3.m3.1.1.3.3.1.cmml" xref="A2.SS2.p1.3.m3.1.1.3.3.1"></times><ci id="A2.SS2.p1.3.m3.1.1.3.3.2.cmml" xref="A2.SS2.p1.3.m3.1.1.3.3.2">𝑛</ci><ci id="A2.SS2.p1.3.m3.1.1.3.3.3.cmml" xref="A2.SS2.p1.3.m3.1.1.3.3.3">𝑉</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.3.m3.1c">\mathbf{S}_{token}{\in}\mathcal{R}^{n\times V}</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.3.m3.1d">bold_S start_POSTSUBSCRIPT italic_t italic_o italic_k italic_e italic_n end_POSTSUBSCRIPT ∈ caligraphic_R start_POSTSUPERSCRIPT italic_n × italic_V end_POSTSUPERSCRIPT</annotation></semantics></math>. Subsequently, we compute the cross-entropy loss <math alttext="\mathcal{L}_{token}{=}\mathbf{CE}(\mathbf{S}_{token},\mathbf{y}_{token})" class="ltx_Math" display="inline" id="A2.SS2.p1.4.m4.2"><semantics id="A2.SS2.p1.4.m4.2a"><mrow id="A2.SS2.p1.4.m4.2.2" xref="A2.SS2.p1.4.m4.2.2.cmml"><msub id="A2.SS2.p1.4.m4.2.2.4" xref="A2.SS2.p1.4.m4.2.2.4.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS2.p1.4.m4.2.2.4.2" xref="A2.SS2.p1.4.m4.2.2.4.2.cmml">ℒ</mi><mrow id="A2.SS2.p1.4.m4.2.2.4.3" xref="A2.SS2.p1.4.m4.2.2.4.3.cmml"><mi id="A2.SS2.p1.4.m4.2.2.4.3.2" xref="A2.SS2.p1.4.m4.2.2.4.3.2.cmml">t</mi><mo id="A2.SS2.p1.4.m4.2.2.4.3.1" xref="A2.SS2.p1.4.m4.2.2.4.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.2.2.4.3.3" xref="A2.SS2.p1.4.m4.2.2.4.3.3.cmml">o</mi><mo id="A2.SS2.p1.4.m4.2.2.4.3.1a" xref="A2.SS2.p1.4.m4.2.2.4.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.2.2.4.3.4" xref="A2.SS2.p1.4.m4.2.2.4.3.4.cmml">k</mi><mo id="A2.SS2.p1.4.m4.2.2.4.3.1b" xref="A2.SS2.p1.4.m4.2.2.4.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.2.2.4.3.5" xref="A2.SS2.p1.4.m4.2.2.4.3.5.cmml">e</mi><mo id="A2.SS2.p1.4.m4.2.2.4.3.1c" xref="A2.SS2.p1.4.m4.2.2.4.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.2.2.4.3.6" xref="A2.SS2.p1.4.m4.2.2.4.3.6.cmml">n</mi></mrow></msub><mo id="A2.SS2.p1.4.m4.2.2.3" xref="A2.SS2.p1.4.m4.2.2.3.cmml">=</mo><mrow id="A2.SS2.p1.4.m4.2.2.2" xref="A2.SS2.p1.4.m4.2.2.2.cmml"><mi id="A2.SS2.p1.4.m4.2.2.2.4" xref="A2.SS2.p1.4.m4.2.2.2.4.cmml">𝐂𝐄</mi><mo id="A2.SS2.p1.4.m4.2.2.2.3" xref="A2.SS2.p1.4.m4.2.2.2.3.cmml">⁢</mo><mrow id="A2.SS2.p1.4.m4.2.2.2.2.2" xref="A2.SS2.p1.4.m4.2.2.2.2.3.cmml"><mo id="A2.SS2.p1.4.m4.2.2.2.2.2.3" stretchy="false" xref="A2.SS2.p1.4.m4.2.2.2.2.3.cmml">(</mo><msub id="A2.SS2.p1.4.m4.1.1.1.1.1.1" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.cmml"><mi id="A2.SS2.p1.4.m4.1.1.1.1.1.1.2" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.2.cmml">𝐒</mi><mrow id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.cmml"><mi id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.2" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.2.cmml">t</mi><mo id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.3" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.3.cmml">o</mi><mo id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1a" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.4" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.4.cmml">k</mi><mo id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1b" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.5" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.5.cmml">e</mi><mo id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1c" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.6" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.6.cmml">n</mi></mrow></msub><mo id="A2.SS2.p1.4.m4.2.2.2.2.2.4" xref="A2.SS2.p1.4.m4.2.2.2.2.3.cmml">,</mo><msub id="A2.SS2.p1.4.m4.2.2.2.2.2.2" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.cmml"><mi id="A2.SS2.p1.4.m4.2.2.2.2.2.2.2" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.2.cmml">𝐲</mi><mrow id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.cmml"><mi id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.2" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.2.cmml">t</mi><mo id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.3" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.3.cmml">o</mi><mo id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1a" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.4" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.4.cmml">k</mi><mo id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1b" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.5" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.5.cmml">e</mi><mo id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1c" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.6" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.6.cmml">n</mi></mrow></msub><mo id="A2.SS2.p1.4.m4.2.2.2.2.2.5" stretchy="false" xref="A2.SS2.p1.4.m4.2.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.4.m4.2b"><apply id="A2.SS2.p1.4.m4.2.2.cmml" xref="A2.SS2.p1.4.m4.2.2"><eq id="A2.SS2.p1.4.m4.2.2.3.cmml" xref="A2.SS2.p1.4.m4.2.2.3"></eq><apply id="A2.SS2.p1.4.m4.2.2.4.cmml" xref="A2.SS2.p1.4.m4.2.2.4"><csymbol cd="ambiguous" id="A2.SS2.p1.4.m4.2.2.4.1.cmml" xref="A2.SS2.p1.4.m4.2.2.4">subscript</csymbol><ci id="A2.SS2.p1.4.m4.2.2.4.2.cmml" xref="A2.SS2.p1.4.m4.2.2.4.2">ℒ</ci><apply id="A2.SS2.p1.4.m4.2.2.4.3.cmml" xref="A2.SS2.p1.4.m4.2.2.4.3"><times id="A2.SS2.p1.4.m4.2.2.4.3.1.cmml" xref="A2.SS2.p1.4.m4.2.2.4.3.1"></times><ci id="A2.SS2.p1.4.m4.2.2.4.3.2.cmml" xref="A2.SS2.p1.4.m4.2.2.4.3.2">𝑡</ci><ci id="A2.SS2.p1.4.m4.2.2.4.3.3.cmml" xref="A2.SS2.p1.4.m4.2.2.4.3.3">𝑜</ci><ci id="A2.SS2.p1.4.m4.2.2.4.3.4.cmml" xref="A2.SS2.p1.4.m4.2.2.4.3.4">𝑘</ci><ci id="A2.SS2.p1.4.m4.2.2.4.3.5.cmml" xref="A2.SS2.p1.4.m4.2.2.4.3.5">𝑒</ci><ci id="A2.SS2.p1.4.m4.2.2.4.3.6.cmml" xref="A2.SS2.p1.4.m4.2.2.4.3.6">𝑛</ci></apply></apply><apply id="A2.SS2.p1.4.m4.2.2.2.cmml" xref="A2.SS2.p1.4.m4.2.2.2"><times id="A2.SS2.p1.4.m4.2.2.2.3.cmml" xref="A2.SS2.p1.4.m4.2.2.2.3"></times><ci id="A2.SS2.p1.4.m4.2.2.2.4.cmml" xref="A2.SS2.p1.4.m4.2.2.2.4">𝐂𝐄</ci><interval closure="open" id="A2.SS2.p1.4.m4.2.2.2.2.3.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2"><apply id="A2.SS2.p1.4.m4.1.1.1.1.1.1.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1"><csymbol cd="ambiguous" id="A2.SS2.p1.4.m4.1.1.1.1.1.1.1.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1">subscript</csymbol><ci id="A2.SS2.p1.4.m4.1.1.1.1.1.1.2.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.2">𝐒</ci><apply id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3"><times id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.1"></times><ci id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.2.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.2">𝑡</ci><ci id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.3.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.3">𝑜</ci><ci id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.4.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.4">𝑘</ci><ci id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.5.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.5">𝑒</ci><ci id="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.6.cmml" xref="A2.SS2.p1.4.m4.1.1.1.1.1.1.3.6">𝑛</ci></apply></apply><apply id="A2.SS2.p1.4.m4.2.2.2.2.2.2.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2"><csymbol cd="ambiguous" id="A2.SS2.p1.4.m4.2.2.2.2.2.2.1.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2">subscript</csymbol><ci id="A2.SS2.p1.4.m4.2.2.2.2.2.2.2.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.2">𝐲</ci><apply id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3"><times id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.1"></times><ci id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.2.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.2">𝑡</ci><ci id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.3.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.3">𝑜</ci><ci id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.4.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.4">𝑘</ci><ci id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.5.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.5">𝑒</ci><ci id="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.6.cmml" xref="A2.SS2.p1.4.m4.2.2.2.2.2.2.3.6">𝑛</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.4.m4.2c">\mathcal{L}_{token}{=}\mathbf{CE}(\mathbf{S}_{token},\mathbf{y}_{token})</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.4.m4.2d">caligraphic_L start_POSTSUBSCRIPT italic_t italic_o italic_k italic_e italic_n end_POSTSUBSCRIPT = bold_CE ( bold_S start_POSTSUBSCRIPT italic_t italic_o italic_k italic_e italic_n end_POSTSUBSCRIPT , bold_y start_POSTSUBSCRIPT italic_t italic_o italic_k italic_e italic_n end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="\mathbf{y}_{token}" class="ltx_Math" display="inline" id="A2.SS2.p1.5.m5.1"><semantics id="A2.SS2.p1.5.m5.1a"><msub id="A2.SS2.p1.5.m5.1.1" xref="A2.SS2.p1.5.m5.1.1.cmml"><mi id="A2.SS2.p1.5.m5.1.1.2" xref="A2.SS2.p1.5.m5.1.1.2.cmml">𝐲</mi><mrow id="A2.SS2.p1.5.m5.1.1.3" xref="A2.SS2.p1.5.m5.1.1.3.cmml"><mi id="A2.SS2.p1.5.m5.1.1.3.2" xref="A2.SS2.p1.5.m5.1.1.3.2.cmml">t</mi><mo id="A2.SS2.p1.5.m5.1.1.3.1" xref="A2.SS2.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.5.m5.1.1.3.3" xref="A2.SS2.p1.5.m5.1.1.3.3.cmml">o</mi><mo id="A2.SS2.p1.5.m5.1.1.3.1a" xref="A2.SS2.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.5.m5.1.1.3.4" xref="A2.SS2.p1.5.m5.1.1.3.4.cmml">k</mi><mo id="A2.SS2.p1.5.m5.1.1.3.1b" xref="A2.SS2.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.5.m5.1.1.3.5" xref="A2.SS2.p1.5.m5.1.1.3.5.cmml">e</mi><mo id="A2.SS2.p1.5.m5.1.1.3.1c" xref="A2.SS2.p1.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A2.SS2.p1.5.m5.1.1.3.6" xref="A2.SS2.p1.5.m5.1.1.3.6.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.5.m5.1b"><apply id="A2.SS2.p1.5.m5.1.1.cmml" xref="A2.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A2.SS2.p1.5.m5.1.1.1.cmml" xref="A2.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="A2.SS2.p1.5.m5.1.1.2.cmml" xref="A2.SS2.p1.5.m5.1.1.2">𝐲</ci><apply id="A2.SS2.p1.5.m5.1.1.3.cmml" xref="A2.SS2.p1.5.m5.1.1.3"><times id="A2.SS2.p1.5.m5.1.1.3.1.cmml" xref="A2.SS2.p1.5.m5.1.1.3.1"></times><ci id="A2.SS2.p1.5.m5.1.1.3.2.cmml" xref="A2.SS2.p1.5.m5.1.1.3.2">𝑡</ci><ci id="A2.SS2.p1.5.m5.1.1.3.3.cmml" xref="A2.SS2.p1.5.m5.1.1.3.3">𝑜</ci><ci id="A2.SS2.p1.5.m5.1.1.3.4.cmml" xref="A2.SS2.p1.5.m5.1.1.3.4">𝑘</ci><ci id="A2.SS2.p1.5.m5.1.1.3.5.cmml" xref="A2.SS2.p1.5.m5.1.1.3.5">𝑒</ci><ci id="A2.SS2.p1.5.m5.1.1.3.6.cmml" xref="A2.SS2.p1.5.m5.1.1.3.6">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.5.m5.1c">\mathbf{y}_{token}</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.5.m5.1d">bold_y start_POSTSUBSCRIPT italic_t italic_o italic_k italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math> represents the ground-truth next-token id.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Multi-task Training Loss</h3>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.4">During multi-task training, we calculate losses on the top decoder layers for each task to guide the model to converge faster in the early training stage and accelerate the overall training process.
The overall training loss function is:</p>
<table class="ltx_equation ltx_eqn_table" id="A2.E7">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\sum_{task\in\{ps,de,ocr,ic,vqa\}}\sum_{i=1}^{nl_{task}}\lambda_{task}\mathcal%
{L}_{task}," class="ltx_Math" display="block" id="A2.E7.m1.6"><semantics id="A2.E7.m1.6a"><mrow id="A2.E7.m1.6.6.1" xref="A2.E7.m1.6.6.1.1.cmml"><mrow id="A2.E7.m1.6.6.1.1" xref="A2.E7.m1.6.6.1.1.cmml"><munder id="A2.E7.m1.6.6.1.1.1" xref="A2.E7.m1.6.6.1.1.1.cmml"><mo id="A2.E7.m1.6.6.1.1.1.2" movablelimits="false" xref="A2.E7.m1.6.6.1.1.1.2.cmml">∑</mo><mrow id="A2.E7.m1.5.5.5" xref="A2.E7.m1.5.5.5.cmml"><mrow id="A2.E7.m1.5.5.5.7" xref="A2.E7.m1.5.5.5.7.cmml"><mi id="A2.E7.m1.5.5.5.7.2" xref="A2.E7.m1.5.5.5.7.2.cmml">t</mi><mo id="A2.E7.m1.5.5.5.7.1" xref="A2.E7.m1.5.5.5.7.1.cmml">⁢</mo><mi id="A2.E7.m1.5.5.5.7.3" xref="A2.E7.m1.5.5.5.7.3.cmml">a</mi><mo id="A2.E7.m1.5.5.5.7.1a" xref="A2.E7.m1.5.5.5.7.1.cmml">⁢</mo><mi id="A2.E7.m1.5.5.5.7.4" xref="A2.E7.m1.5.5.5.7.4.cmml">s</mi><mo id="A2.E7.m1.5.5.5.7.1b" xref="A2.E7.m1.5.5.5.7.1.cmml">⁢</mo><mi id="A2.E7.m1.5.5.5.7.5" xref="A2.E7.m1.5.5.5.7.5.cmml">k</mi></mrow><mo id="A2.E7.m1.5.5.5.6" xref="A2.E7.m1.5.5.5.6.cmml">∈</mo><mrow id="A2.E7.m1.5.5.5.5.5" xref="A2.E7.m1.5.5.5.5.6.cmml"><mo id="A2.E7.m1.5.5.5.5.5.6" stretchy="false" xref="A2.E7.m1.5.5.5.5.6.cmml">{</mo><mrow id="A2.E7.m1.1.1.1.1.1.1" xref="A2.E7.m1.1.1.1.1.1.1.cmml"><mi id="A2.E7.m1.1.1.1.1.1.1.2" xref="A2.E7.m1.1.1.1.1.1.1.2.cmml">p</mi><mo id="A2.E7.m1.1.1.1.1.1.1.1" xref="A2.E7.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="A2.E7.m1.1.1.1.1.1.1.3" xref="A2.E7.m1.1.1.1.1.1.1.3.cmml">s</mi></mrow><mo id="A2.E7.m1.5.5.5.5.5.7" xref="A2.E7.m1.5.5.5.5.6.cmml">,</mo><mrow id="A2.E7.m1.2.2.2.2.2.2" xref="A2.E7.m1.2.2.2.2.2.2.cmml"><mi id="A2.E7.m1.2.2.2.2.2.2.2" xref="A2.E7.m1.2.2.2.2.2.2.2.cmml">d</mi><mo id="A2.E7.m1.2.2.2.2.2.2.1" xref="A2.E7.m1.2.2.2.2.2.2.1.cmml">⁢</mo><mi id="A2.E7.m1.2.2.2.2.2.2.3" xref="A2.E7.m1.2.2.2.2.2.2.3.cmml">e</mi></mrow><mo id="A2.E7.m1.5.5.5.5.5.8" xref="A2.E7.m1.5.5.5.5.6.cmml">,</mo><mrow id="A2.E7.m1.3.3.3.3.3.3" xref="A2.E7.m1.3.3.3.3.3.3.cmml"><mi id="A2.E7.m1.3.3.3.3.3.3.2" xref="A2.E7.m1.3.3.3.3.3.3.2.cmml">o</mi><mo id="A2.E7.m1.3.3.3.3.3.3.1" xref="A2.E7.m1.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="A2.E7.m1.3.3.3.3.3.3.3" xref="A2.E7.m1.3.3.3.3.3.3.3.cmml">c</mi><mo id="A2.E7.m1.3.3.3.3.3.3.1a" xref="A2.E7.m1.3.3.3.3.3.3.1.cmml">⁢</mo><mi id="A2.E7.m1.3.3.3.3.3.3.4" xref="A2.E7.m1.3.3.3.3.3.3.4.cmml">r</mi></mrow><mo id="A2.E7.m1.5.5.5.5.5.9" xref="A2.E7.m1.5.5.5.5.6.cmml">,</mo><mrow id="A2.E7.m1.4.4.4.4.4.4" xref="A2.E7.m1.4.4.4.4.4.4.cmml"><mi id="A2.E7.m1.4.4.4.4.4.4.2" xref="A2.E7.m1.4.4.4.4.4.4.2.cmml">i</mi><mo id="A2.E7.m1.4.4.4.4.4.4.1" xref="A2.E7.m1.4.4.4.4.4.4.1.cmml">⁢</mo><mi id="A2.E7.m1.4.4.4.4.4.4.3" xref="A2.E7.m1.4.4.4.4.4.4.3.cmml">c</mi></mrow><mo id="A2.E7.m1.5.5.5.5.5.10" xref="A2.E7.m1.5.5.5.5.6.cmml">,</mo><mrow id="A2.E7.m1.5.5.5.5.5.5" xref="A2.E7.m1.5.5.5.5.5.5.cmml"><mi id="A2.E7.m1.5.5.5.5.5.5.2" xref="A2.E7.m1.5.5.5.5.5.5.2.cmml">v</mi><mo id="A2.E7.m1.5.5.5.5.5.5.1" xref="A2.E7.m1.5.5.5.5.5.5.1.cmml">⁢</mo><mi id="A2.E7.m1.5.5.5.5.5.5.3" xref="A2.E7.m1.5.5.5.5.5.5.3.cmml">q</mi><mo id="A2.E7.m1.5.5.5.5.5.5.1a" xref="A2.E7.m1.5.5.5.5.5.5.1.cmml">⁢</mo><mi id="A2.E7.m1.5.5.5.5.5.5.4" xref="A2.E7.m1.5.5.5.5.5.5.4.cmml">a</mi></mrow><mo id="A2.E7.m1.5.5.5.5.5.11" stretchy="false" xref="A2.E7.m1.5.5.5.5.6.cmml">}</mo></mrow></mrow></munder><mrow id="A2.E7.m1.6.6.1.1.2" xref="A2.E7.m1.6.6.1.1.2.cmml"><munderover id="A2.E7.m1.6.6.1.1.2.1" xref="A2.E7.m1.6.6.1.1.2.1.cmml"><mo id="A2.E7.m1.6.6.1.1.2.1.2.2" lspace="0.167em" movablelimits="false" xref="A2.E7.m1.6.6.1.1.2.1.2.2.cmml">∑</mo><mrow id="A2.E7.m1.6.6.1.1.2.1.2.3" xref="A2.E7.m1.6.6.1.1.2.1.2.3.cmml"><mi id="A2.E7.m1.6.6.1.1.2.1.2.3.2" xref="A2.E7.m1.6.6.1.1.2.1.2.3.2.cmml">i</mi><mo id="A2.E7.m1.6.6.1.1.2.1.2.3.1" xref="A2.E7.m1.6.6.1.1.2.1.2.3.1.cmml">=</mo><mn id="A2.E7.m1.6.6.1.1.2.1.2.3.3" xref="A2.E7.m1.6.6.1.1.2.1.2.3.3.cmml">1</mn></mrow><mrow id="A2.E7.m1.6.6.1.1.2.1.3" xref="A2.E7.m1.6.6.1.1.2.1.3.cmml"><mi id="A2.E7.m1.6.6.1.1.2.1.3.2" xref="A2.E7.m1.6.6.1.1.2.1.3.2.cmml">n</mi><mo id="A2.E7.m1.6.6.1.1.2.1.3.1" xref="A2.E7.m1.6.6.1.1.2.1.3.1.cmml">⁢</mo><msub id="A2.E7.m1.6.6.1.1.2.1.3.3" xref="A2.E7.m1.6.6.1.1.2.1.3.3.cmml"><mi id="A2.E7.m1.6.6.1.1.2.1.3.3.2" xref="A2.E7.m1.6.6.1.1.2.1.3.3.2.cmml">l</mi><mrow id="A2.E7.m1.6.6.1.1.2.1.3.3.3" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.cmml"><mi id="A2.E7.m1.6.6.1.1.2.1.3.3.3.2" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.2.cmml">t</mi><mo id="A2.E7.m1.6.6.1.1.2.1.3.3.3.1" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.1.cmml">⁢</mo><mi id="A2.E7.m1.6.6.1.1.2.1.3.3.3.3" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.3.cmml">a</mi><mo id="A2.E7.m1.6.6.1.1.2.1.3.3.3.1a" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.1.cmml">⁢</mo><mi id="A2.E7.m1.6.6.1.1.2.1.3.3.3.4" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.4.cmml">s</mi><mo id="A2.E7.m1.6.6.1.1.2.1.3.3.3.1b" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.1.cmml">⁢</mo><mi id="A2.E7.m1.6.6.1.1.2.1.3.3.3.5" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.5.cmml">k</mi></mrow></msub></mrow></munderover><mrow id="A2.E7.m1.6.6.1.1.2.2" xref="A2.E7.m1.6.6.1.1.2.2.cmml"><msub id="A2.E7.m1.6.6.1.1.2.2.2" xref="A2.E7.m1.6.6.1.1.2.2.2.cmml"><mi id="A2.E7.m1.6.6.1.1.2.2.2.2" xref="A2.E7.m1.6.6.1.1.2.2.2.2.cmml">λ</mi><mrow id="A2.E7.m1.6.6.1.1.2.2.2.3" xref="A2.E7.m1.6.6.1.1.2.2.2.3.cmml"><mi id="A2.E7.m1.6.6.1.1.2.2.2.3.2" xref="A2.E7.m1.6.6.1.1.2.2.2.3.2.cmml">t</mi><mo id="A2.E7.m1.6.6.1.1.2.2.2.3.1" xref="A2.E7.m1.6.6.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="A2.E7.m1.6.6.1.1.2.2.2.3.3" xref="A2.E7.m1.6.6.1.1.2.2.2.3.3.cmml">a</mi><mo id="A2.E7.m1.6.6.1.1.2.2.2.3.1a" xref="A2.E7.m1.6.6.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="A2.E7.m1.6.6.1.1.2.2.2.3.4" xref="A2.E7.m1.6.6.1.1.2.2.2.3.4.cmml">s</mi><mo id="A2.E7.m1.6.6.1.1.2.2.2.3.1b" xref="A2.E7.m1.6.6.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="A2.E7.m1.6.6.1.1.2.2.2.3.5" xref="A2.E7.m1.6.6.1.1.2.2.2.3.5.cmml">k</mi></mrow></msub><mo id="A2.E7.m1.6.6.1.1.2.2.1" xref="A2.E7.m1.6.6.1.1.2.2.1.cmml">⁢</mo><msub id="A2.E7.m1.6.6.1.1.2.2.3" xref="A2.E7.m1.6.6.1.1.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.E7.m1.6.6.1.1.2.2.3.2" xref="A2.E7.m1.6.6.1.1.2.2.3.2.cmml">ℒ</mi><mrow id="A2.E7.m1.6.6.1.1.2.2.3.3" xref="A2.E7.m1.6.6.1.1.2.2.3.3.cmml"><mi id="A2.E7.m1.6.6.1.1.2.2.3.3.2" xref="A2.E7.m1.6.6.1.1.2.2.3.3.2.cmml">t</mi><mo id="A2.E7.m1.6.6.1.1.2.2.3.3.1" xref="A2.E7.m1.6.6.1.1.2.2.3.3.1.cmml">⁢</mo><mi id="A2.E7.m1.6.6.1.1.2.2.3.3.3" xref="A2.E7.m1.6.6.1.1.2.2.3.3.3.cmml">a</mi><mo id="A2.E7.m1.6.6.1.1.2.2.3.3.1a" xref="A2.E7.m1.6.6.1.1.2.2.3.3.1.cmml">⁢</mo><mi id="A2.E7.m1.6.6.1.1.2.2.3.3.4" xref="A2.E7.m1.6.6.1.1.2.2.3.3.4.cmml">s</mi><mo id="A2.E7.m1.6.6.1.1.2.2.3.3.1b" xref="A2.E7.m1.6.6.1.1.2.2.3.3.1.cmml">⁢</mo><mi id="A2.E7.m1.6.6.1.1.2.2.3.3.5" xref="A2.E7.m1.6.6.1.1.2.2.3.3.5.cmml">k</mi></mrow></msub></mrow></mrow></mrow><mo id="A2.E7.m1.6.6.1.2" xref="A2.E7.m1.6.6.1.1.cmml">,</mo></mrow><annotation-xml encoding="MathML-Content" id="A2.E7.m1.6b"><apply id="A2.E7.m1.6.6.1.1.cmml" xref="A2.E7.m1.6.6.1"><apply id="A2.E7.m1.6.6.1.1.1.cmml" xref="A2.E7.m1.6.6.1.1.1"><csymbol cd="ambiguous" id="A2.E7.m1.6.6.1.1.1.1.cmml" xref="A2.E7.m1.6.6.1.1.1">subscript</csymbol><sum id="A2.E7.m1.6.6.1.1.1.2.cmml" xref="A2.E7.m1.6.6.1.1.1.2"></sum><apply id="A2.E7.m1.5.5.5.cmml" xref="A2.E7.m1.5.5.5"><in id="A2.E7.m1.5.5.5.6.cmml" xref="A2.E7.m1.5.5.5.6"></in><apply id="A2.E7.m1.5.5.5.7.cmml" xref="A2.E7.m1.5.5.5.7"><times id="A2.E7.m1.5.5.5.7.1.cmml" xref="A2.E7.m1.5.5.5.7.1"></times><ci id="A2.E7.m1.5.5.5.7.2.cmml" xref="A2.E7.m1.5.5.5.7.2">𝑡</ci><ci id="A2.E7.m1.5.5.5.7.3.cmml" xref="A2.E7.m1.5.5.5.7.3">𝑎</ci><ci id="A2.E7.m1.5.5.5.7.4.cmml" xref="A2.E7.m1.5.5.5.7.4">𝑠</ci><ci id="A2.E7.m1.5.5.5.7.5.cmml" xref="A2.E7.m1.5.5.5.7.5">𝑘</ci></apply><set id="A2.E7.m1.5.5.5.5.6.cmml" xref="A2.E7.m1.5.5.5.5.5"><apply id="A2.E7.m1.1.1.1.1.1.1.cmml" xref="A2.E7.m1.1.1.1.1.1.1"><times id="A2.E7.m1.1.1.1.1.1.1.1.cmml" xref="A2.E7.m1.1.1.1.1.1.1.1"></times><ci id="A2.E7.m1.1.1.1.1.1.1.2.cmml" xref="A2.E7.m1.1.1.1.1.1.1.2">𝑝</ci><ci id="A2.E7.m1.1.1.1.1.1.1.3.cmml" xref="A2.E7.m1.1.1.1.1.1.1.3">𝑠</ci></apply><apply id="A2.E7.m1.2.2.2.2.2.2.cmml" xref="A2.E7.m1.2.2.2.2.2.2"><times id="A2.E7.m1.2.2.2.2.2.2.1.cmml" xref="A2.E7.m1.2.2.2.2.2.2.1"></times><ci id="A2.E7.m1.2.2.2.2.2.2.2.cmml" xref="A2.E7.m1.2.2.2.2.2.2.2">𝑑</ci><ci id="A2.E7.m1.2.2.2.2.2.2.3.cmml" xref="A2.E7.m1.2.2.2.2.2.2.3">𝑒</ci></apply><apply id="A2.E7.m1.3.3.3.3.3.3.cmml" xref="A2.E7.m1.3.3.3.3.3.3"><times id="A2.E7.m1.3.3.3.3.3.3.1.cmml" xref="A2.E7.m1.3.3.3.3.3.3.1"></times><ci id="A2.E7.m1.3.3.3.3.3.3.2.cmml" xref="A2.E7.m1.3.3.3.3.3.3.2">𝑜</ci><ci id="A2.E7.m1.3.3.3.3.3.3.3.cmml" xref="A2.E7.m1.3.3.3.3.3.3.3">𝑐</ci><ci id="A2.E7.m1.3.3.3.3.3.3.4.cmml" xref="A2.E7.m1.3.3.3.3.3.3.4">𝑟</ci></apply><apply id="A2.E7.m1.4.4.4.4.4.4.cmml" xref="A2.E7.m1.4.4.4.4.4.4"><times id="A2.E7.m1.4.4.4.4.4.4.1.cmml" xref="A2.E7.m1.4.4.4.4.4.4.1"></times><ci id="A2.E7.m1.4.4.4.4.4.4.2.cmml" xref="A2.E7.m1.4.4.4.4.4.4.2">𝑖</ci><ci id="A2.E7.m1.4.4.4.4.4.4.3.cmml" xref="A2.E7.m1.4.4.4.4.4.4.3">𝑐</ci></apply><apply id="A2.E7.m1.5.5.5.5.5.5.cmml" xref="A2.E7.m1.5.5.5.5.5.5"><times id="A2.E7.m1.5.5.5.5.5.5.1.cmml" xref="A2.E7.m1.5.5.5.5.5.5.1"></times><ci id="A2.E7.m1.5.5.5.5.5.5.2.cmml" xref="A2.E7.m1.5.5.5.5.5.5.2">𝑣</ci><ci id="A2.E7.m1.5.5.5.5.5.5.3.cmml" xref="A2.E7.m1.5.5.5.5.5.5.3">𝑞</ci><ci id="A2.E7.m1.5.5.5.5.5.5.4.cmml" xref="A2.E7.m1.5.5.5.5.5.5.4">𝑎</ci></apply></set></apply></apply><apply id="A2.E7.m1.6.6.1.1.2.cmml" xref="A2.E7.m1.6.6.1.1.2"><apply id="A2.E7.m1.6.6.1.1.2.1.cmml" xref="A2.E7.m1.6.6.1.1.2.1"><csymbol cd="ambiguous" id="A2.E7.m1.6.6.1.1.2.1.1.cmml" xref="A2.E7.m1.6.6.1.1.2.1">superscript</csymbol><apply id="A2.E7.m1.6.6.1.1.2.1.2.cmml" xref="A2.E7.m1.6.6.1.1.2.1"><csymbol cd="ambiguous" id="A2.E7.m1.6.6.1.1.2.1.2.1.cmml" xref="A2.E7.m1.6.6.1.1.2.1">subscript</csymbol><sum id="A2.E7.m1.6.6.1.1.2.1.2.2.cmml" xref="A2.E7.m1.6.6.1.1.2.1.2.2"></sum><apply id="A2.E7.m1.6.6.1.1.2.1.2.3.cmml" xref="A2.E7.m1.6.6.1.1.2.1.2.3"><eq id="A2.E7.m1.6.6.1.1.2.1.2.3.1.cmml" xref="A2.E7.m1.6.6.1.1.2.1.2.3.1"></eq><ci id="A2.E7.m1.6.6.1.1.2.1.2.3.2.cmml" xref="A2.E7.m1.6.6.1.1.2.1.2.3.2">𝑖</ci><cn id="A2.E7.m1.6.6.1.1.2.1.2.3.3.cmml" type="integer" xref="A2.E7.m1.6.6.1.1.2.1.2.3.3">1</cn></apply></apply><apply id="A2.E7.m1.6.6.1.1.2.1.3.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3"><times id="A2.E7.m1.6.6.1.1.2.1.3.1.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.1"></times><ci id="A2.E7.m1.6.6.1.1.2.1.3.2.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.2">𝑛</ci><apply id="A2.E7.m1.6.6.1.1.2.1.3.3.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.3"><csymbol cd="ambiguous" id="A2.E7.m1.6.6.1.1.2.1.3.3.1.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.3">subscript</csymbol><ci id="A2.E7.m1.6.6.1.1.2.1.3.3.2.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.3.2">𝑙</ci><apply id="A2.E7.m1.6.6.1.1.2.1.3.3.3.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3"><times id="A2.E7.m1.6.6.1.1.2.1.3.3.3.1.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.1"></times><ci id="A2.E7.m1.6.6.1.1.2.1.3.3.3.2.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.2">𝑡</ci><ci id="A2.E7.m1.6.6.1.1.2.1.3.3.3.3.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.3">𝑎</ci><ci id="A2.E7.m1.6.6.1.1.2.1.3.3.3.4.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.4">𝑠</ci><ci id="A2.E7.m1.6.6.1.1.2.1.3.3.3.5.cmml" xref="A2.E7.m1.6.6.1.1.2.1.3.3.3.5">𝑘</ci></apply></apply></apply></apply><apply id="A2.E7.m1.6.6.1.1.2.2.cmml" xref="A2.E7.m1.6.6.1.1.2.2"><times id="A2.E7.m1.6.6.1.1.2.2.1.cmml" xref="A2.E7.m1.6.6.1.1.2.2.1"></times><apply id="A2.E7.m1.6.6.1.1.2.2.2.cmml" xref="A2.E7.m1.6.6.1.1.2.2.2"><csymbol cd="ambiguous" id="A2.E7.m1.6.6.1.1.2.2.2.1.cmml" xref="A2.E7.m1.6.6.1.1.2.2.2">subscript</csymbol><ci id="A2.E7.m1.6.6.1.1.2.2.2.2.cmml" xref="A2.E7.m1.6.6.1.1.2.2.2.2">𝜆</ci><apply id="A2.E7.m1.6.6.1.1.2.2.2.3.cmml" xref="A2.E7.m1.6.6.1.1.2.2.2.3"><times id="A2.E7.m1.6.6.1.1.2.2.2.3.1.cmml" xref="A2.E7.m1.6.6.1.1.2.2.2.3.1"></times><ci id="A2.E7.m1.6.6.1.1.2.2.2.3.2.cmml" xref="A2.E7.m1.6.6.1.1.2.2.2.3.2">𝑡</ci><ci id="A2.E7.m1.6.6.1.1.2.2.2.3.3.cmml" xref="A2.E7.m1.6.6.1.1.2.2.2.3.3">𝑎</ci><ci id="A2.E7.m1.6.6.1.1.2.2.2.3.4.cmml" xref="A2.E7.m1.6.6.1.1.2.2.2.3.4">𝑠</ci><ci id="A2.E7.m1.6.6.1.1.2.2.2.3.5.cmml" xref="A2.E7.m1.6.6.1.1.2.2.2.3.5">𝑘</ci></apply></apply><apply id="A2.E7.m1.6.6.1.1.2.2.3.cmml" xref="A2.E7.m1.6.6.1.1.2.2.3"><csymbol cd="ambiguous" id="A2.E7.m1.6.6.1.1.2.2.3.1.cmml" xref="A2.E7.m1.6.6.1.1.2.2.3">subscript</csymbol><ci id="A2.E7.m1.6.6.1.1.2.2.3.2.cmml" xref="A2.E7.m1.6.6.1.1.2.2.3.2">ℒ</ci><apply id="A2.E7.m1.6.6.1.1.2.2.3.3.cmml" xref="A2.E7.m1.6.6.1.1.2.2.3.3"><times id="A2.E7.m1.6.6.1.1.2.2.3.3.1.cmml" xref="A2.E7.m1.6.6.1.1.2.2.3.3.1"></times><ci id="A2.E7.m1.6.6.1.1.2.2.3.3.2.cmml" xref="A2.E7.m1.6.6.1.1.2.2.3.3.2">𝑡</ci><ci id="A2.E7.m1.6.6.1.1.2.2.3.3.3.cmml" xref="A2.E7.m1.6.6.1.1.2.2.3.3.3">𝑎</ci><ci id="A2.E7.m1.6.6.1.1.2.2.3.3.4.cmml" xref="A2.E7.m1.6.6.1.1.2.2.3.3.4">𝑠</ci><ci id="A2.E7.m1.6.6.1.1.2.2.3.3.5.cmml" xref="A2.E7.m1.6.6.1.1.2.2.3.3.5">𝑘</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.E7.m1.6c">\sum_{task\in\{ps,de,ocr,ic,vqa\}}\sum_{i=1}^{nl_{task}}\lambda_{task}\mathcal%
{L}_{task},</annotation><annotation encoding="application/x-llamapun" id="A2.E7.m1.6d">∑ start_POSTSUBSCRIPT italic_t italic_a italic_s italic_k ∈ { italic_p italic_s , italic_d italic_e , italic_o italic_c italic_r , italic_i italic_c , italic_v italic_q italic_a } end_POSTSUBSCRIPT ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n italic_l start_POSTSUBSCRIPT italic_t italic_a italic_s italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT italic_λ start_POSTSUBSCRIPT italic_t italic_a italic_s italic_k end_POSTSUBSCRIPT caligraphic_L start_POSTSUBSCRIPT italic_t italic_a italic_s italic_k end_POSTSUBSCRIPT ,</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(7)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="A2.SS3.p1.3">where <math alttext="nl_{task}" class="ltx_Math" display="inline" id="A2.SS3.p1.1.m1.1"><semantics id="A2.SS3.p1.1.m1.1a"><mrow id="A2.SS3.p1.1.m1.1.1" xref="A2.SS3.p1.1.m1.1.1.cmml"><mi id="A2.SS3.p1.1.m1.1.1.2" xref="A2.SS3.p1.1.m1.1.1.2.cmml">n</mi><mo id="A2.SS3.p1.1.m1.1.1.1" xref="A2.SS3.p1.1.m1.1.1.1.cmml">⁢</mo><msub id="A2.SS3.p1.1.m1.1.1.3" xref="A2.SS3.p1.1.m1.1.1.3.cmml"><mi id="A2.SS3.p1.1.m1.1.1.3.2" xref="A2.SS3.p1.1.m1.1.1.3.2.cmml">l</mi><mrow id="A2.SS3.p1.1.m1.1.1.3.3" xref="A2.SS3.p1.1.m1.1.1.3.3.cmml"><mi id="A2.SS3.p1.1.m1.1.1.3.3.2" xref="A2.SS3.p1.1.m1.1.1.3.3.2.cmml">t</mi><mo id="A2.SS3.p1.1.m1.1.1.3.3.1" xref="A2.SS3.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="A2.SS3.p1.1.m1.1.1.3.3.3" xref="A2.SS3.p1.1.m1.1.1.3.3.3.cmml">a</mi><mo id="A2.SS3.p1.1.m1.1.1.3.3.1a" xref="A2.SS3.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="A2.SS3.p1.1.m1.1.1.3.3.4" xref="A2.SS3.p1.1.m1.1.1.3.3.4.cmml">s</mi><mo id="A2.SS3.p1.1.m1.1.1.3.3.1b" xref="A2.SS3.p1.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="A2.SS3.p1.1.m1.1.1.3.3.5" xref="A2.SS3.p1.1.m1.1.1.3.3.5.cmml">k</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="A2.SS3.p1.1.m1.1b"><apply id="A2.SS3.p1.1.m1.1.1.cmml" xref="A2.SS3.p1.1.m1.1.1"><times id="A2.SS3.p1.1.m1.1.1.1.cmml" xref="A2.SS3.p1.1.m1.1.1.1"></times><ci id="A2.SS3.p1.1.m1.1.1.2.cmml" xref="A2.SS3.p1.1.m1.1.1.2">𝑛</ci><apply id="A2.SS3.p1.1.m1.1.1.3.cmml" xref="A2.SS3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="A2.SS3.p1.1.m1.1.1.3.1.cmml" xref="A2.SS3.p1.1.m1.1.1.3">subscript</csymbol><ci id="A2.SS3.p1.1.m1.1.1.3.2.cmml" xref="A2.SS3.p1.1.m1.1.1.3.2">𝑙</ci><apply id="A2.SS3.p1.1.m1.1.1.3.3.cmml" xref="A2.SS3.p1.1.m1.1.1.3.3"><times id="A2.SS3.p1.1.m1.1.1.3.3.1.cmml" xref="A2.SS3.p1.1.m1.1.1.3.3.1"></times><ci id="A2.SS3.p1.1.m1.1.1.3.3.2.cmml" xref="A2.SS3.p1.1.m1.1.1.3.3.2">𝑡</ci><ci id="A2.SS3.p1.1.m1.1.1.3.3.3.cmml" xref="A2.SS3.p1.1.m1.1.1.3.3.3">𝑎</ci><ci id="A2.SS3.p1.1.m1.1.1.3.3.4.cmml" xref="A2.SS3.p1.1.m1.1.1.3.3.4">𝑠</ci><ci id="A2.SS3.p1.1.m1.1.1.3.3.5.cmml" xref="A2.SS3.p1.1.m1.1.1.3.3.5">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p1.1.m1.1c">nl_{task}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p1.1.m1.1d">italic_n italic_l start_POSTSUBSCRIPT italic_t italic_a italic_s italic_k end_POSTSUBSCRIPT</annotation></semantics></math> represents the number of decoder layers that need to calculate the loss for different task, <math alttext="\lambda_{task}" class="ltx_Math" display="inline" id="A2.SS3.p1.2.m2.1"><semantics id="A2.SS3.p1.2.m2.1a"><msub id="A2.SS3.p1.2.m2.1.1" xref="A2.SS3.p1.2.m2.1.1.cmml"><mi id="A2.SS3.p1.2.m2.1.1.2" xref="A2.SS3.p1.2.m2.1.1.2.cmml">λ</mi><mrow id="A2.SS3.p1.2.m2.1.1.3" xref="A2.SS3.p1.2.m2.1.1.3.cmml"><mi id="A2.SS3.p1.2.m2.1.1.3.2" xref="A2.SS3.p1.2.m2.1.1.3.2.cmml">t</mi><mo id="A2.SS3.p1.2.m2.1.1.3.1" xref="A2.SS3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="A2.SS3.p1.2.m2.1.1.3.3" xref="A2.SS3.p1.2.m2.1.1.3.3.cmml">a</mi><mo id="A2.SS3.p1.2.m2.1.1.3.1a" xref="A2.SS3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="A2.SS3.p1.2.m2.1.1.3.4" xref="A2.SS3.p1.2.m2.1.1.3.4.cmml">s</mi><mo id="A2.SS3.p1.2.m2.1.1.3.1b" xref="A2.SS3.p1.2.m2.1.1.3.1.cmml">⁢</mo><mi id="A2.SS3.p1.2.m2.1.1.3.5" xref="A2.SS3.p1.2.m2.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p1.2.m2.1b"><apply id="A2.SS3.p1.2.m2.1.1.cmml" xref="A2.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A2.SS3.p1.2.m2.1.1.1.cmml" xref="A2.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="A2.SS3.p1.2.m2.1.1.2.cmml" xref="A2.SS3.p1.2.m2.1.1.2">𝜆</ci><apply id="A2.SS3.p1.2.m2.1.1.3.cmml" xref="A2.SS3.p1.2.m2.1.1.3"><times id="A2.SS3.p1.2.m2.1.1.3.1.cmml" xref="A2.SS3.p1.2.m2.1.1.3.1"></times><ci id="A2.SS3.p1.2.m2.1.1.3.2.cmml" xref="A2.SS3.p1.2.m2.1.1.3.2">𝑡</ci><ci id="A2.SS3.p1.2.m2.1.1.3.3.cmml" xref="A2.SS3.p1.2.m2.1.1.3.3">𝑎</ci><ci id="A2.SS3.p1.2.m2.1.1.3.4.cmml" xref="A2.SS3.p1.2.m2.1.1.3.4">𝑠</ci><ci id="A2.SS3.p1.2.m2.1.1.3.5.cmml" xref="A2.SS3.p1.2.m2.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p1.2.m2.1c">\lambda_{task}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p1.2.m2.1d">italic_λ start_POSTSUBSCRIPT italic_t italic_a italic_s italic_k end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="\mathcal{L}_{task}" class="ltx_Math" display="inline" id="A2.SS3.p1.3.m3.1"><semantics id="A2.SS3.p1.3.m3.1a"><msub id="A2.SS3.p1.3.m3.1.1" xref="A2.SS3.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="A2.SS3.p1.3.m3.1.1.2" xref="A2.SS3.p1.3.m3.1.1.2.cmml">ℒ</mi><mrow id="A2.SS3.p1.3.m3.1.1.3" xref="A2.SS3.p1.3.m3.1.1.3.cmml"><mi id="A2.SS3.p1.3.m3.1.1.3.2" xref="A2.SS3.p1.3.m3.1.1.3.2.cmml">t</mi><mo id="A2.SS3.p1.3.m3.1.1.3.1" xref="A2.SS3.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="A2.SS3.p1.3.m3.1.1.3.3" xref="A2.SS3.p1.3.m3.1.1.3.3.cmml">a</mi><mo id="A2.SS3.p1.3.m3.1.1.3.1a" xref="A2.SS3.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="A2.SS3.p1.3.m3.1.1.3.4" xref="A2.SS3.p1.3.m3.1.1.3.4.cmml">s</mi><mo id="A2.SS3.p1.3.m3.1.1.3.1b" xref="A2.SS3.p1.3.m3.1.1.3.1.cmml">⁢</mo><mi id="A2.SS3.p1.3.m3.1.1.3.5" xref="A2.SS3.p1.3.m3.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A2.SS3.p1.3.m3.1b"><apply id="A2.SS3.p1.3.m3.1.1.cmml" xref="A2.SS3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A2.SS3.p1.3.m3.1.1.1.cmml" xref="A2.SS3.p1.3.m3.1.1">subscript</csymbol><ci id="A2.SS3.p1.3.m3.1.1.2.cmml" xref="A2.SS3.p1.3.m3.1.1.2">ℒ</ci><apply id="A2.SS3.p1.3.m3.1.1.3.cmml" xref="A2.SS3.p1.3.m3.1.1.3"><times id="A2.SS3.p1.3.m3.1.1.3.1.cmml" xref="A2.SS3.p1.3.m3.1.1.3.1"></times><ci id="A2.SS3.p1.3.m3.1.1.3.2.cmml" xref="A2.SS3.p1.3.m3.1.1.3.2">𝑡</ci><ci id="A2.SS3.p1.3.m3.1.1.3.3.cmml" xref="A2.SS3.p1.3.m3.1.1.3.3">𝑎</ci><ci id="A2.SS3.p1.3.m3.1.1.3.4.cmml" xref="A2.SS3.p1.3.m3.1.1.3.4">𝑠</ci><ci id="A2.SS3.p1.3.m3.1.1.3.5.cmml" xref="A2.SS3.p1.3.m3.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS3.p1.3.m3.1c">\mathcal{L}_{task}</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p1.3.m3.1d">caligraphic_L start_POSTSUBSCRIPT italic_t italic_a italic_s italic_k end_POSTSUBSCRIPT</annotation></semantics></math> are loss weights and losses for different task, respectively.</p>
</div>
<figure class="ltx_figure" id="A2.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="879" id="A2.F6.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F6.3.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="A2.F6.4.2" style="font-size:90%;">Comparison between subword-based tokenizer and character-based word tokenizer in our proposed <span class="ltx_text ltx_font_smallcaps" id="A2.F6.4.2.1">@Model</span></span></figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Implementation Details</h2>
<section class="ltx_subsection" id="A3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.1 </span>Multi-task Training</h3>
<div class="ltx_para ltx_noindent" id="A3.SS1.p1">
<p class="ltx_p" id="A3.SS1.p1.1"><span class="ltx_text ltx_font_bold" id="A3.SS1.p1.1.1">Training Setting.</span> Since the number of images in OCR training dataset is much larger than datasets for the other tasks, we define OCR training dataset as the major dataset for multi-task training. It means that the total number of iterations is calculated based on the number of images in the OCR dataset. The batch sizes for panoptic segmentation, depth estimation, OCR, captioning, and VQA are 4, 4, 768, 8, and 4, respectively, to accommodate datasets of different sizes for various tasks. The model is trained with 15 epochs based on OCR datasets on 4 A100 (40G). The AdamW optimizer is used with the initial learning rate 1<sup class="ltx_sup" id="A3.SS1.p1.1.2"><span class="ltx_text ltx_font_italic" id="A3.SS1.p1.1.2.1">e</span></sup>-5. A step-wise scheduler is used to decay the learning rate by 0.1 on the fraction [0.6, 0.8] of training steps.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS1.p2">
<p class="ltx_p" id="A3.SS1.p2.5"><span class="ltx_text ltx_font_bold" id="A3.SS1.p2.5.1">Hyperparameter Choice.</span> In <span class="ltx_text ltx_font_smallcaps" id="A3.SS1.p2.5.2">@Model</span>, the decoder has 7 decoder layers. Due to segmentation and OCR are the top two scored tasks (from user study), and the OCR datasets are very large, the amount of data for each task is unbalanced, we set <math alttext="nl_{task}" class="ltx_Math" display="inline" id="A3.SS1.p2.1.m1.1"><semantics id="A3.SS1.p2.1.m1.1a"><mrow id="A3.SS1.p2.1.m1.1.1" xref="A3.SS1.p2.1.m1.1.1.cmml"><mi id="A3.SS1.p2.1.m1.1.1.2" xref="A3.SS1.p2.1.m1.1.1.2.cmml">n</mi><mo id="A3.SS1.p2.1.m1.1.1.1" xref="A3.SS1.p2.1.m1.1.1.1.cmml">⁢</mo><msub id="A3.SS1.p2.1.m1.1.1.3" xref="A3.SS1.p2.1.m1.1.1.3.cmml"><mi id="A3.SS1.p2.1.m1.1.1.3.2" xref="A3.SS1.p2.1.m1.1.1.3.2.cmml">l</mi><mrow id="A3.SS1.p2.1.m1.1.1.3.3" xref="A3.SS1.p2.1.m1.1.1.3.3.cmml"><mi id="A3.SS1.p2.1.m1.1.1.3.3.2" xref="A3.SS1.p2.1.m1.1.1.3.3.2.cmml">t</mi><mo id="A3.SS1.p2.1.m1.1.1.3.3.1" xref="A3.SS1.p2.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.1.m1.1.1.3.3.3" xref="A3.SS1.p2.1.m1.1.1.3.3.3.cmml">a</mi><mo id="A3.SS1.p2.1.m1.1.1.3.3.1a" xref="A3.SS1.p2.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.1.m1.1.1.3.3.4" xref="A3.SS1.p2.1.m1.1.1.3.3.4.cmml">s</mi><mo id="A3.SS1.p2.1.m1.1.1.3.3.1b" xref="A3.SS1.p2.1.m1.1.1.3.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.1.m1.1.1.3.3.5" xref="A3.SS1.p2.1.m1.1.1.3.3.5.cmml">k</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.1.m1.1b"><apply id="A3.SS1.p2.1.m1.1.1.cmml" xref="A3.SS1.p2.1.m1.1.1"><times id="A3.SS1.p2.1.m1.1.1.1.cmml" xref="A3.SS1.p2.1.m1.1.1.1"></times><ci id="A3.SS1.p2.1.m1.1.1.2.cmml" xref="A3.SS1.p2.1.m1.1.1.2">𝑛</ci><apply id="A3.SS1.p2.1.m1.1.1.3.cmml" xref="A3.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="A3.SS1.p2.1.m1.1.1.3.1.cmml" xref="A3.SS1.p2.1.m1.1.1.3">subscript</csymbol><ci id="A3.SS1.p2.1.m1.1.1.3.2.cmml" xref="A3.SS1.p2.1.m1.1.1.3.2">𝑙</ci><apply id="A3.SS1.p2.1.m1.1.1.3.3.cmml" xref="A3.SS1.p2.1.m1.1.1.3.3"><times id="A3.SS1.p2.1.m1.1.1.3.3.1.cmml" xref="A3.SS1.p2.1.m1.1.1.3.3.1"></times><ci id="A3.SS1.p2.1.m1.1.1.3.3.2.cmml" xref="A3.SS1.p2.1.m1.1.1.3.3.2">𝑡</ci><ci id="A3.SS1.p2.1.m1.1.1.3.3.3.cmml" xref="A3.SS1.p2.1.m1.1.1.3.3.3">𝑎</ci><ci id="A3.SS1.p2.1.m1.1.1.3.3.4.cmml" xref="A3.SS1.p2.1.m1.1.1.3.3.4">𝑠</ci><ci id="A3.SS1.p2.1.m1.1.1.3.3.5.cmml" xref="A3.SS1.p2.1.m1.1.1.3.3.5">𝑘</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.1.m1.1c">nl_{task}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.1.m1.1d">italic_n italic_l start_POSTSUBSCRIPT italic_t italic_a italic_s italic_k end_POSTSUBSCRIPT</annotation></semantics></math> as 6, 3, 6, 3 and 3 for panoptic segmentation, depth estimation, OCR, captioning and VQA, respectively, to allow the model to focus more on segmentation and OCR during training. We set loss weights <math alttext="\lambda_{task}" class="ltx_Math" display="inline" id="A3.SS1.p2.2.m2.1"><semantics id="A3.SS1.p2.2.m2.1a"><msub id="A3.SS1.p2.2.m2.1.1" xref="A3.SS1.p2.2.m2.1.1.cmml"><mi id="A3.SS1.p2.2.m2.1.1.2" xref="A3.SS1.p2.2.m2.1.1.2.cmml">λ</mi><mrow id="A3.SS1.p2.2.m2.1.1.3" xref="A3.SS1.p2.2.m2.1.1.3.cmml"><mi id="A3.SS1.p2.2.m2.1.1.3.2" xref="A3.SS1.p2.2.m2.1.1.3.2.cmml">t</mi><mo id="A3.SS1.p2.2.m2.1.1.3.1" xref="A3.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.2.m2.1.1.3.3" xref="A3.SS1.p2.2.m2.1.1.3.3.cmml">a</mi><mo id="A3.SS1.p2.2.m2.1.1.3.1a" xref="A3.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.2.m2.1.1.3.4" xref="A3.SS1.p2.2.m2.1.1.3.4.cmml">s</mi><mo id="A3.SS1.p2.2.m2.1.1.3.1b" xref="A3.SS1.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.2.m2.1.1.3.5" xref="A3.SS1.p2.2.m2.1.1.3.5.cmml">k</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.2.m2.1b"><apply id="A3.SS1.p2.2.m2.1.1.cmml" xref="A3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="A3.SS1.p2.2.m2.1.1.1.cmml" xref="A3.SS1.p2.2.m2.1.1">subscript</csymbol><ci id="A3.SS1.p2.2.m2.1.1.2.cmml" xref="A3.SS1.p2.2.m2.1.1.2">𝜆</ci><apply id="A3.SS1.p2.2.m2.1.1.3.cmml" xref="A3.SS1.p2.2.m2.1.1.3"><times id="A3.SS1.p2.2.m2.1.1.3.1.cmml" xref="A3.SS1.p2.2.m2.1.1.3.1"></times><ci id="A3.SS1.p2.2.m2.1.1.3.2.cmml" xref="A3.SS1.p2.2.m2.1.1.3.2">𝑡</ci><ci id="A3.SS1.p2.2.m2.1.1.3.3.cmml" xref="A3.SS1.p2.2.m2.1.1.3.3">𝑎</ci><ci id="A3.SS1.p2.2.m2.1.1.3.4.cmml" xref="A3.SS1.p2.2.m2.1.1.3.4">𝑠</ci><ci id="A3.SS1.p2.2.m2.1.1.3.5.cmml" xref="A3.SS1.p2.2.m2.1.1.3.5">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.2.m2.1c">\lambda_{task}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.2.m2.1d">italic_λ start_POSTSUBSCRIPT italic_t italic_a italic_s italic_k end_POSTSUBSCRIPT</annotation></semantics></math> as 1, 10, 10, 2 and 2, respectively. Because the loss values of OCR and depth estimation in the later stage of training are very small, in order to minimize significant differences in the loss magnitude for each task as much as possible, we have made such a setting. And in Eq. (<a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A2.E5" title="Equation 5 ‣ B.1 Pixel-level Output Loss ‣ Appendix B Loss Functions ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">5</span></a>), we set <math alttext="\lambda_{cls}" class="ltx_Math" display="inline" id="A3.SS1.p2.3.m3.1"><semantics id="A3.SS1.p2.3.m3.1a"><msub id="A3.SS1.p2.3.m3.1.1" xref="A3.SS1.p2.3.m3.1.1.cmml"><mi id="A3.SS1.p2.3.m3.1.1.2" xref="A3.SS1.p2.3.m3.1.1.2.cmml">λ</mi><mrow id="A3.SS1.p2.3.m3.1.1.3" xref="A3.SS1.p2.3.m3.1.1.3.cmml"><mi id="A3.SS1.p2.3.m3.1.1.3.2" xref="A3.SS1.p2.3.m3.1.1.3.2.cmml">c</mi><mo id="A3.SS1.p2.3.m3.1.1.3.1" xref="A3.SS1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.3.m3.1.1.3.3" xref="A3.SS1.p2.3.m3.1.1.3.3.cmml">l</mi><mo id="A3.SS1.p2.3.m3.1.1.3.1a" xref="A3.SS1.p2.3.m3.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.3.m3.1.1.3.4" xref="A3.SS1.p2.3.m3.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.3.m3.1b"><apply id="A3.SS1.p2.3.m3.1.1.cmml" xref="A3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="A3.SS1.p2.3.m3.1.1.1.cmml" xref="A3.SS1.p2.3.m3.1.1">subscript</csymbol><ci id="A3.SS1.p2.3.m3.1.1.2.cmml" xref="A3.SS1.p2.3.m3.1.1.2">𝜆</ci><apply id="A3.SS1.p2.3.m3.1.1.3.cmml" xref="A3.SS1.p2.3.m3.1.1.3"><times id="A3.SS1.p2.3.m3.1.1.3.1.cmml" xref="A3.SS1.p2.3.m3.1.1.3.1"></times><ci id="A3.SS1.p2.3.m3.1.1.3.2.cmml" xref="A3.SS1.p2.3.m3.1.1.3.2">𝑐</ci><ci id="A3.SS1.p2.3.m3.1.1.3.3.cmml" xref="A3.SS1.p2.3.m3.1.1.3.3">𝑙</ci><ci id="A3.SS1.p2.3.m3.1.1.3.4.cmml" xref="A3.SS1.p2.3.m3.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.3.m3.1c">\lambda_{cls}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.3.m3.1d">italic_λ start_POSTSUBSCRIPT italic_c italic_l italic_s end_POSTSUBSCRIPT</annotation></semantics></math> = 2, <math alttext="\lambda_{bce}" class="ltx_Math" display="inline" id="A3.SS1.p2.4.m4.1"><semantics id="A3.SS1.p2.4.m4.1a"><msub id="A3.SS1.p2.4.m4.1.1" xref="A3.SS1.p2.4.m4.1.1.cmml"><mi id="A3.SS1.p2.4.m4.1.1.2" xref="A3.SS1.p2.4.m4.1.1.2.cmml">λ</mi><mrow id="A3.SS1.p2.4.m4.1.1.3" xref="A3.SS1.p2.4.m4.1.1.3.cmml"><mi id="A3.SS1.p2.4.m4.1.1.3.2" xref="A3.SS1.p2.4.m4.1.1.3.2.cmml">b</mi><mo id="A3.SS1.p2.4.m4.1.1.3.1" xref="A3.SS1.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.4.m4.1.1.3.3" xref="A3.SS1.p2.4.m4.1.1.3.3.cmml">c</mi><mo id="A3.SS1.p2.4.m4.1.1.3.1a" xref="A3.SS1.p2.4.m4.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.4.m4.1.1.3.4" xref="A3.SS1.p2.4.m4.1.1.3.4.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.4.m4.1b"><apply id="A3.SS1.p2.4.m4.1.1.cmml" xref="A3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="A3.SS1.p2.4.m4.1.1.1.cmml" xref="A3.SS1.p2.4.m4.1.1">subscript</csymbol><ci id="A3.SS1.p2.4.m4.1.1.2.cmml" xref="A3.SS1.p2.4.m4.1.1.2">𝜆</ci><apply id="A3.SS1.p2.4.m4.1.1.3.cmml" xref="A3.SS1.p2.4.m4.1.1.3"><times id="A3.SS1.p2.4.m4.1.1.3.1.cmml" xref="A3.SS1.p2.4.m4.1.1.3.1"></times><ci id="A3.SS1.p2.4.m4.1.1.3.2.cmml" xref="A3.SS1.p2.4.m4.1.1.3.2">𝑏</ci><ci id="A3.SS1.p2.4.m4.1.1.3.3.cmml" xref="A3.SS1.p2.4.m4.1.1.3.3">𝑐</ci><ci id="A3.SS1.p2.4.m4.1.1.3.4.cmml" xref="A3.SS1.p2.4.m4.1.1.3.4">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.4.m4.1c">\lambda_{bce}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.4.m4.1d">italic_λ start_POSTSUBSCRIPT italic_b italic_c italic_e end_POSTSUBSCRIPT</annotation></semantics></math> = 5 and <math alttext="\lambda_{dice}" class="ltx_Math" display="inline" id="A3.SS1.p2.5.m5.1"><semantics id="A3.SS1.p2.5.m5.1a"><msub id="A3.SS1.p2.5.m5.1.1" xref="A3.SS1.p2.5.m5.1.1.cmml"><mi id="A3.SS1.p2.5.m5.1.1.2" xref="A3.SS1.p2.5.m5.1.1.2.cmml">λ</mi><mrow id="A3.SS1.p2.5.m5.1.1.3" xref="A3.SS1.p2.5.m5.1.1.3.cmml"><mi id="A3.SS1.p2.5.m5.1.1.3.2" xref="A3.SS1.p2.5.m5.1.1.3.2.cmml">d</mi><mo id="A3.SS1.p2.5.m5.1.1.3.1" xref="A3.SS1.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.5.m5.1.1.3.3" xref="A3.SS1.p2.5.m5.1.1.3.3.cmml">i</mi><mo id="A3.SS1.p2.5.m5.1.1.3.1a" xref="A3.SS1.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.5.m5.1.1.3.4" xref="A3.SS1.p2.5.m5.1.1.3.4.cmml">c</mi><mo id="A3.SS1.p2.5.m5.1.1.3.1b" xref="A3.SS1.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="A3.SS1.p2.5.m5.1.1.3.5" xref="A3.SS1.p2.5.m5.1.1.3.5.cmml">e</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="A3.SS1.p2.5.m5.1b"><apply id="A3.SS1.p2.5.m5.1.1.cmml" xref="A3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="A3.SS1.p2.5.m5.1.1.1.cmml" xref="A3.SS1.p2.5.m5.1.1">subscript</csymbol><ci id="A3.SS1.p2.5.m5.1.1.2.cmml" xref="A3.SS1.p2.5.m5.1.1.2">𝜆</ci><apply id="A3.SS1.p2.5.m5.1.1.3.cmml" xref="A3.SS1.p2.5.m5.1.1.3"><times id="A3.SS1.p2.5.m5.1.1.3.1.cmml" xref="A3.SS1.p2.5.m5.1.1.3.1"></times><ci id="A3.SS1.p2.5.m5.1.1.3.2.cmml" xref="A3.SS1.p2.5.m5.1.1.3.2">𝑑</ci><ci id="A3.SS1.p2.5.m5.1.1.3.3.cmml" xref="A3.SS1.p2.5.m5.1.1.3.3">𝑖</ci><ci id="A3.SS1.p2.5.m5.1.1.3.4.cmml" xref="A3.SS1.p2.5.m5.1.1.3.4">𝑐</ci><ci id="A3.SS1.p2.5.m5.1.1.3.5.cmml" xref="A3.SS1.p2.5.m5.1.1.3.5">𝑒</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A3.SS1.p2.5.m5.1c">\lambda_{dice}</annotation><annotation encoding="application/x-llamapun" id="A3.SS1.p2.5.m5.1d">italic_λ start_POSTSUBSCRIPT italic_d italic_i italic_c italic_e end_POSTSUBSCRIPT</annotation></semantics></math> = 5 as default, following the settings of X-Decoder.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.2 </span>Single-task Training</h3>
<div class="ltx_para" id="A3.SS2.p1">
<p class="ltx_p" id="A3.SS2.p1.1">All tasks are trained with AdamW as the optimizer on 4 1080Ti (11G), except OCR. The initial learning rate is 1<sup class="ltx_sup" id="A3.SS2.p1.1.1"><span class="ltx_text ltx_font_italic" id="A3.SS2.p1.1.1.1">e</span></sup>-5 and reduced by 10 times after 60% and 80%.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p2">
<p class="ltx_p" id="A3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p2.1.1">Panoptic Segmentation.</span>
We train the model for 50 epochs. We set the image resolution to 640<math alttext="{\times}" class="ltx_Math" display="inline" id="A3.SS2.p2.1.m1.1"><semantics id="A3.SS2.p2.1.m1.1a"><mo id="A3.SS2.p2.1.m1.1.1" xref="A3.SS2.p2.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p2.1.m1.1b"><times id="A3.SS2.p2.1.m1.1.1.cmml" xref="A3.SS2.p2.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p2.1.m1.1c">{\times}</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p2.1.m1.1d">×</annotation></semantics></math>640 and the batch size to 4.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p3">
<p class="ltx_p" id="A3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p3.1.1">Depth Estimation.</span>
We train the model for 50 epochs. We set the image resolution to 480<math alttext="{\times}" class="ltx_Math" display="inline" id="A3.SS2.p3.1.m1.1"><semantics id="A3.SS2.p3.1.m1.1a"><mo id="A3.SS2.p3.1.m1.1.1" xref="A3.SS2.p3.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p3.1.m1.1b"><times id="A3.SS2.p3.1.m1.1.1.cmml" xref="A3.SS2.p3.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p3.1.m1.1c">{\times}</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p3.1.m1.1d">×</annotation></semantics></math>640 and the batch size to 16. Note that this task is very unstable and requires careful hyperparameter tuning. If you encounter training errors, you can increase the batch size, reduce the learning rate or training with single precision (FP32).</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p4">
<p class="ltx_p" id="A3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p4.1.1">Optical Character Recognition.</span>
We train the model for 10 epochs on 4 A100 (40G). We set the image resolution to 64<math alttext="{\times}" class="ltx_Math" display="inline" id="A3.SS2.p4.1.m1.1"><semantics id="A3.SS2.p4.1.m1.1a"><mo id="A3.SS2.p4.1.m1.1.1" xref="A3.SS2.p4.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p4.1.m1.1b"><times id="A3.SS2.p4.1.m1.1.1.cmml" xref="A3.SS2.p4.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p4.1.m1.1c">{\times}</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p4.1.m1.1d">×</annotation></semantics></math>200 and the batch size to 1024.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p5">
<p class="ltx_p" id="A3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p5.1.1">Image Captioning.</span>
We train the model for 50 epochs. We set the image resolution to 480<math alttext="{\times}" class="ltx_Math" display="inline" id="A3.SS2.p5.1.m1.1"><semantics id="A3.SS2.p5.1.m1.1a"><mo id="A3.SS2.p5.1.m1.1.1" xref="A3.SS2.p5.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p5.1.m1.1b"><times id="A3.SS2.p5.1.m1.1.1.cmml" xref="A3.SS2.p5.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p5.1.m1.1c">{\times}</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p5.1.m1.1d">×</annotation></semantics></math>640 and the batch size to 16. We use all captions for training and do not use beam search and CIDEr optimization.</p>
</div>
<div class="ltx_para ltx_noindent" id="A3.SS2.p6">
<p class="ltx_p" id="A3.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="A3.SS2.p6.1.1">Visual Question Answering.</span>
We train the model for 30 epochs. We set the image resolution to 480<math alttext="{\times}" class="ltx_Math" display="inline" id="A3.SS2.p6.1.m1.1"><semantics id="A3.SS2.p6.1.m1.1a"><mo id="A3.SS2.p6.1.m1.1.1" xref="A3.SS2.p6.1.m1.1.1.cmml">×</mo><annotation-xml encoding="MathML-Content" id="A3.SS2.p6.1.m1.1b"><times id="A3.SS2.p6.1.m1.1.1.cmml" xref="A3.SS2.p6.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="A3.SS2.p6.1.m1.1c">{\times}</annotation><annotation encoding="application/x-llamapun" id="A3.SS2.p6.1.m1.1d">×</annotation></semantics></math>640 and the batch size to 16.</p>
</div>
</section>
<section class="ltx_subsection" id="A3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">C.3 </span>Character-based Tokenizer with Limited Vocabulary for OCR</h3>
<div class="ltx_para" id="A3.SS3.p1">
<p class="ltx_p" id="A3.SS3.p1.1">In our main paper, we observed that subword-based tokenizer with complete vocabulary hurts the performance of OCR task. In Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A2.F6" title="Figure 6 ‣ B.3 Multi-task Training Loss ‣ Appendix B Loss Functions ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">6</span></a>, we show how to use character-based tokenizer and much smaller limited vocabulary to perform OCR task. Using a character-based word tokenizer to divide the text that needs to be recognized into characters one by one, model only needs to predict token from the limited vocabulary space, and do not need to select candidate subword from the complete vocabulary. This reduces the prediction space and improves the accuracy of prediction.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>User Study</h2>
<section class="ltx_subsection" id="A4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.1 </span>Comments on Generalist Assistance Systems</h3>
<div class="ltx_para" id="A4.SS1.p1">
<p class="ltx_p" id="A4.SS1.p1.1">By conducting the questionnaire survey, we communicated with visually impaired individuals to comprehend the functionalities they expect a generalist assistive system should possess. We got some thoughts like: <span class="ltx_text ltx_font_italic" id="A4.SS1.p1.1.1">“It should find the door, look for stairs in an open area, read the house/room number, read signs/plates, describe the environment, warn me of obstacles, and can navigate the corridors with a floor plan.”</span>. Some participants also described specific usage scenarios, <span class="ltx_text ltx_font_italic" id="A4.SS1.p1.1.2">“I would use navigation and obstacle detection systems outside. It should warn me of obstacles or describe something I’m about to encounter. For example, if I’m navigating outside and there’s a road ahead, then it should say if it has a roundabout or an intersection. Or, if there is a railroad crossing, announce something similar. It would be cool if there was an all around view. The system says, the front of you is street and the back is a building, left is bike racks, etc. If there is a name of the store, read it out. The most important thing is to have a general navigation ability based on this all around view. If I then say navigate to the store (name of the store) recognized by this system, then it should navigate me there.”</span>. Based on these thoughts and comments, essential functions identified by People with Visual Impairments (PVIs) that a generalist assistive system should include are:</p>
<ul class="ltx_itemize" id="A4.I1">
<li class="ltx_item" id="A4.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="A4.I1.ix1.p1">
<p class="ltx_p" id="A4.I1.ix1.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.ix1.p1.1.1">Navigation and Obstacle Avoidance.</span> A critical component is a navigation system integrated with obstacle detection capabilities. PVIs desire a system that allow for interactive navigation, where users can request directions to specific locations identified by the system.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="A4.I1.ix2.p1">
<p class="ltx_p" id="A4.I1.ix2.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.ix2.p1.1.1">Text Recognition and Environmental Description.</span> The ability to recognize and verbally relay textual information is also important. This includes identifying and reading door labels, room numbers, and signs. Furthermore, recognizing the names of stores, significant landmarks or other text contributes to better environmental understanding and orientation.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="A4.I1.ix3.p1">
<p class="ltx_p" id="A4.I1.ix3.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.ix3.p1.1.1">Comprehensive Scene Interpretation.</span> PVIs expressed a desire for a system that provides a holistic view of their surroundings. This “all-around vision” function should describe streets, buildings, and other elements in the vicinity.</p>
</div>
</li>
<li class="ltx_item" id="A4.I1.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(4)</span>
<div class="ltx_para" id="A4.I1.ix4.p1">
<p class="ltx_p" id="A4.I1.ix4.p1.1"><span class="ltx_text ltx_font_bold" id="A4.I1.ix4.p1.1.1">Integration of Text-to-Speech Technology.</span> Incorporating text-to-speech technology for dynamic interaction is also valuable.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsection" id="A4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">D.2 </span>More Comments</h3>
<div class="ltx_para ltx_noindent" id="A4.SS2.p1">
<p class="ltx_p" id="A4.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="A4.SS2.p1.1.1">Navigation.</span> The majority of participants (5 P: 5 participants) prioritize outdoor navigation, noting its greater complexity and risk. They highlight that outdoor environments pose larger obstacles, longer and more complex routes, and a higher likelihood of getting lost compared to indoor scenarios. One participant emphasized, <span class="ltx_text ltx_font_italic" id="A4.SS2.p1.1.2">“Outdoor navigation is much more important. Indoors, the reach of a cane is much more likely to adequately capture the surroundings. The distances are shorter and the density of people is higher.”</span>. Another added, <span class="ltx_text ltx_font_italic" id="A4.SS2.p1.1.3">“Definitely outdoors. If I have to go into a building I don’t know, it will probably only be for once. It’s not worth learning a way to do that.”</span>. The unpredictability of outdoor spaces, such as traffic, was also mentioned as a significant factor. Conversely, a minority (2 P) believes that indoor navigation is more important. They mention the challenges of navigating within large unfamiliar buildings, locating specific rooms, stairs, elevators, or exits, walking across a large open-area, and walking in rooms with highly differentiated structures, such as restrooms. Importantly, they spend most of their time indoors.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS2.p2">
<p class="ltx_p" id="A4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="A4.SS2.p2.1.1">Text Recognition.</span> Today, PVIs mainly use screen readers to recognize digital texts and usually use smartphones, or smartphones Apps to read non-digital texts. However, they find non-digital text reading is difficult and cumbersome, like <span class="ltx_text ltx_font_italic" id="A4.SS2.p2.1.2">“Everything I receive on paper in the post annoys me. I use apps like Seeing AI and Be My Eyes or the iPhone’s magnifying glass to read non-digital texts, but using a smart glass to read these text directly would be better.”</span>. They also pointed out that it is also important for them to read signs to find the right floor or hallway and read door numbers to enter the right room.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS2.p3">
<p class="ltx_p" id="A4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="A4.SS2.p3.1.1">Other Functions.</span> About depth estimation, <span class="ltx_text ltx_font_italic" id="A4.SS2.p3.1.2">“This function helps one develop a mental map of an environment. You get the proportions well.”</span>. About object location, <span class="ltx_text ltx_font_italic" id="A4.SS2.p3.1.3">“In my personal environment I am always very sure where all the things I am looking for are. However, locating a true one in a larger shelf section of 3-4 meters would be very useful. A function that detects objects that don’t belong in that space would also be very helpful to check a room for overlooked clutter. The glasses could use a reference photo of the tidy room and then report any anomalies, such as dirty dishes on the table or socks on the floor.”</span> and <span class="ltx_text ltx_font_italic" id="A4.SS2.p3.1.4">“If I only need it if I can’t find something in my apartment, it could make the search easier, but I would need it pretty rarely.”</span>. About surroundings understanding, <span class="ltx_text ltx_font_italic" id="A4.SS2.p3.1.5">“It would be important to me that the description be highly efficient. The short form is always first”</span> and <span class="ltx_text ltx_font_italic" id="A4.SS2.p3.1.6">“Most of the time we are not interested in the scene because it is too much information for us. But descriptions of photos, environments, etc. are very exciting. ChatGPT is really great.”</span>. About scene recognition, <span class="ltx_text ltx_font_italic" id="A4.SS2.p3.1.7">“Perhaps interesting for recognize different scenarios, but a correspondingly efficient description of the image would serve the same purpose. I can’t imagine a situation where I would need room detection. I usually know which room I’m going to or being led into.”</span>. About visual Q&amp;A, <span class="ltx_text ltx_font_italic" id="A4.SS2.p3.1.8">“This function would make it possible to expand a short initial description of an image dynamically and according to your own needs. That would improve the overall function enormously.”</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.SS2.p4">
<p class="ltx_p" id="A4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="A4.SS2.p4.1.1">Interaction.</span> If there were such a general system, PVIs prefer interacting with system through discrete button presses or subtle gestures (6 P), rather than voice commands (1 P) for privacy reasons, when inputting instructions. For receiving system feedback, they show a preference for auditory feedback (for general purpose) and vibrations (for special purpose such as obstacle avoidance).</p>
</div>
<div class="ltx_para" id="A4.SS2.p5">
<p class="ltx_p" id="A4.SS2.p5.1">Based on these comments and ideas, it becomes evident that for PVIs, navigation and quick, direct recognition of non-digital text are the two most critical functionalities. Meanwhile, the multifaceted nature of navigation encompasses functions like environmental comprehension, obstacle avoidance, path planning, voice guidance and <span class="ltx_text ltx_font_italic" id="A4.SS2.p5.1.1">etc.</span> These insights serve as valuable guidance for our work. Furthermore, the analysis of participants’ relevant feedback has provided us with an initial understanding of creating a universal assistive system.</p>
</div>
<figure class="ltx_figure" id="A4.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="820" id="A4.F7.g1" src="x6.png" width="951"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="A4.F7.3.2" style="font-size:90%;">Examples to show the promotion of vision for vision-language and complementariness between different vision-language tasks.</span></figcaption>
</figure>
<figure class="ltx_table" id="A4.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A4.T7.1" style="width:433.6pt;height:165.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="transform:translate(28.5pt,-10.9pt) scale(1.15162636914013,1.15162636914013) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A4.T7.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T7.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="3" id="A4.T7.1.1.2.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Task</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A4.T7.1.1.2.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">ADE-150</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" colspan="2" id="A4.T7.1.1.2.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">VizWiz_Cap</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" colspan="5" id="A4.T7.1.1.2.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">VizWiz_VQA</th>
</tr>
<tr class="ltx_tr" id="A4.T7.1.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T7.1.1.1.2" style="padding-left:4.0pt;padding-right:4.0pt;">PS</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T7.1.1.1.3" style="padding-left:4.0pt;padding-right:4.0pt;">IC</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A4.T7.1.1.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">VQA</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A4.T7.1.1.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">PQ</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T7.1.1.1.6" style="padding-left:4.0pt;padding-right:4.0pt;">B@1</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r" id="A4.T7.1.1.1.7" style="padding-left:4.0pt;padding-right:4.0pt;">CIDEr</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T7.1.1.1.8" style="padding-left:4.0pt;padding-right:4.0pt;">Other</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T7.1.1.1.9" style="padding-left:4.0pt;padding-right:4.0pt;">Unans</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T7.1.1.1.10" style="padding-left:4.0pt;padding-right:4.0pt;">Yes/No</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T7.1.1.1.11" style="padding-left:4.0pt;padding-right:4.0pt;">Number</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" id="A4.T7.1.1.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">Acc(<math alttext="\%" class="ltx_Math" display="inline" id="A4.T7.1.1.1.1.m1.1"><semantics id="A4.T7.1.1.1.1.m1.1a"><mo id="A4.T7.1.1.1.1.m1.1.1" xref="A4.T7.1.1.1.1.m1.1.1.cmml">%</mo><annotation-xml encoding="MathML-Content" id="A4.T7.1.1.1.1.m1.1b"><csymbol cd="latexml" id="A4.T7.1.1.1.1.m1.1.1.cmml" xref="A4.T7.1.1.1.1.m1.1.1">percent</csymbol></annotation-xml><annotation encoding="application/x-tex" id="A4.T7.1.1.1.1.m1.1c">\%</annotation><annotation encoding="application/x-llamapun" id="A4.T7.1.1.1.1.m1.1d">%</annotation></semantics></math>)</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T7.1.1.3.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T7.1.1.3.1.1" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_border_t" id="A4.T7.1.1.3.1.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="A4.T7.1.1.3.1.3" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A4.T7.1.1.3.1.4" style="padding-left:4.0pt;padding-right:4.0pt;">39.2</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T7.1.1.3.1.5" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A4.T7.1.1.3.1.6" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T7.1.1.3.1.7" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T7.1.1.3.1.8" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T7.1.1.3.1.9" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T7.1.1.3.1.10" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T7.1.1.3.1.11" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
</tr>
<tr class="ltx_tr" id="A4.T7.1.1.4.2">
<td class="ltx_td" id="A4.T7.1.1.4.2.1" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.4.2.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_border_r" id="A4.T7.1.1.4.2.3" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.4.2.4" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.4.2.5" style="padding-left:4.0pt;padding-right:4.0pt;">60.0</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.4.2.6" style="padding-left:4.0pt;padding-right:4.0pt;">45.1</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.4.2.7" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.4.2.8" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.4.2.9" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.4.2.10" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.4.2.11" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
</tr>
<tr class="ltx_tr" id="A4.T7.1.1.5.3">
<td class="ltx_td" id="A4.T7.1.1.5.3.1" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td" id="A4.T7.1.1.5.3.2" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.5.3.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.5.3.4" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.5.3.5" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.5.3.6" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.5.3.7" style="padding-left:4.0pt;padding-right:4.0pt;">30.5</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.5.3.8" style="padding-left:4.0pt;padding-right:4.0pt;">92.1</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.5.3.9" style="padding-left:4.0pt;padding-right:4.0pt;">70.1</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.5.3.10" style="padding-left:4.0pt;padding-right:4.0pt;">13.7</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.5.3.11" style="padding-left:4.0pt;padding-right:4.0pt;">49.1</td>
</tr>
<tr class="ltx_tr" id="A4.T7.1.1.6.4">
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.6.4.1" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.6.4.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_border_r" id="A4.T7.1.1.6.4.3" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.6.4.4" style="padding-left:4.0pt;padding-right:4.0pt;">37.7</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.6.4.5" style="padding-left:4.0pt;padding-right:4.0pt;">57.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.6.4.6" style="padding-left:4.0pt;padding-right:4.0pt;">46.8</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.6.4.7" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.6.4.8" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.6.4.9" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.6.4.10" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.6.4.11" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
</tr>
<tr class="ltx_tr" id="A4.T7.1.1.7.5">
<td class="ltx_td" id="A4.T7.1.1.7.5.1" style="padding-left:4.0pt;padding-right:4.0pt;"></td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.7.5.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.7.5.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.7.5.4" style="padding-left:4.0pt;padding-right:4.0pt;">–</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.7.5.5" style="padding-left:4.0pt;padding-right:4.0pt;">59.8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A4.T7.1.1.7.5.6" style="padding-left:4.0pt;padding-right:4.0pt;">46.3</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.7.5.7" style="padding-left:4.0pt;padding-right:4.0pt;">32.2</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.7.5.8" style="padding-left:4.0pt;padding-right:4.0pt;">86.5</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.7.5.9" style="padding-left:4.0pt;padding-right:4.0pt;">73.4</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.7.5.10" style="padding-left:4.0pt;padding-right:4.0pt;">16.4</td>
<td class="ltx_td ltx_align_center" id="A4.T7.1.1.7.5.11" style="padding-left:4.0pt;padding-right:4.0pt;">48.8</td>
</tr>
<tr class="ltx_tr" id="A4.T7.1.1.8.6">
<td class="ltx_td ltx_align_center ltx_border_b" id="A4.T7.1.1.8.6.1" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A4.T7.1.1.8.6.2" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A4.T7.1.1.8.6.3" style="padding-left:4.0pt;padding-right:4.0pt;">✓</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A4.T7.1.1.8.6.4" style="padding-left:4.0pt;padding-right:4.0pt;">38.5</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A4.T7.1.1.8.6.5" style="padding-left:4.0pt;padding-right:4.0pt;">61.0</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="A4.T7.1.1.8.6.6" style="padding-left:4.0pt;padding-right:4.0pt;">52.5</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A4.T7.1.1.8.6.7" style="padding-left:4.0pt;padding-right:4.0pt;">39.4</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A4.T7.1.1.8.6.8" style="padding-left:4.0pt;padding-right:4.0pt;">88.2</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A4.T7.1.1.8.6.9" style="padding-left:4.0pt;padding-right:4.0pt;">70.1</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A4.T7.1.1.8.6.10" style="padding-left:4.0pt;padding-right:4.0pt;">10.8</td>
<td class="ltx_td ltx_align_center ltx_border_b" id="A4.T7.1.1.8.6.11" style="padding-left:4.0pt;padding-right:4.0pt;">53.7</td>
</tr>
</tbody>
</table>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="A4.T7.4.1.1" style="font-size:90%;">Table 7</span>: </span><span class="ltx_text ltx_font_bold" id="A4.T7.5.2" style="font-size:90%;">Comparison of results of mixed training for different tasks.<span class="ltx_text ltx_font_medium" id="A4.T7.5.2.1"> Note:“Other”, “Unanswerable”, “Yes/No”, “Number” are 4 different answer types for VQA. (PS = panoptic segmentation, IC = image captioning).</span></span></figcaption>
</figure>
<figure class="ltx_figure" id="A4.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="976" id="A4.F8.g1" src="x7.png" width="951"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="A4.F8.3.2" style="font-size:90%;">Examples on different test datasets. These images cover a diversity of visual domains and concepts in the daily life of PVIs.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A4.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="454" id="A4.F9.g1" src="x8.png" width="951"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A4.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="A4.F9.3.2" style="font-size:90%;">Examples on real-world scenes. These images were randomly collected by using mobile phone.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>More Experiments</h2>
<section class="ltx_subsection" id="A5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">E.1 </span>Complementariness in Multitasking</h3>
<div class="ltx_para" id="A5.SS1.p1">
<p class="ltx_p" id="A5.SS1.p1.4">As shown in the experiments section, our <span class="ltx_text ltx_font_smallcaps" id="A5.SS1.p1.4.1">@Model</span> exhibits a strong performance in captioning and VQA under multi-task training. Here, we further study the role of segmentation objectives in vision-language (VL) understanding, as well as the role of different vision-language understanding tasks on each other. To investigate, we mix different tasks for training.
In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A4.T7" title="Table 7 ‣ D.2 More Comments ‣ Appendix D User Study ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">7</span></a>, for captioning, when jointly trained with VQA or PS, or all tasks, CIDEr improved by <math alttext="1.2" class="ltx_Math" display="inline" id="A5.SS1.p1.1.m1.1"><semantics id="A5.SS1.p1.1.m1.1a"><mn id="A5.SS1.p1.1.m1.1.1" xref="A5.SS1.p1.1.m1.1.1.cmml">1.2</mn><annotation-xml encoding="MathML-Content" id="A5.SS1.p1.1.m1.1b"><cn id="A5.SS1.p1.1.m1.1.1.cmml" type="float" xref="A5.SS1.p1.1.m1.1.1">1.2</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.SS1.p1.1.m1.1c">1.2</annotation><annotation encoding="application/x-llamapun" id="A5.SS1.p1.1.m1.1d">1.2</annotation></semantics></math>, <math alttext="1.7" class="ltx_Math" display="inline" id="A5.SS1.p1.2.m2.1"><semantics id="A5.SS1.p1.2.m2.1a"><mn id="A5.SS1.p1.2.m2.1.1" xref="A5.SS1.p1.2.m2.1.1.cmml">1.7</mn><annotation-xml encoding="MathML-Content" id="A5.SS1.p1.2.m2.1b"><cn id="A5.SS1.p1.2.m2.1.1.cmml" type="float" xref="A5.SS1.p1.2.m2.1.1">1.7</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.SS1.p1.2.m2.1c">1.7</annotation><annotation encoding="application/x-llamapun" id="A5.SS1.p1.2.m2.1d">1.7</annotation></semantics></math> and <math alttext="7.4" class="ltx_Math" display="inline" id="A5.SS1.p1.3.m3.1"><semantics id="A5.SS1.p1.3.m3.1a"><mn id="A5.SS1.p1.3.m3.1.1" xref="A5.SS1.p1.3.m3.1.1.cmml">7.4</mn><annotation-xml encoding="MathML-Content" id="A5.SS1.p1.3.m3.1b"><cn id="A5.SS1.p1.3.m3.1.1.cmml" type="float" xref="A5.SS1.p1.3.m3.1.1">7.4</cn></annotation-xml><annotation encoding="application/x-tex" id="A5.SS1.p1.3.m3.1c">7.4</annotation><annotation encoding="application/x-llamapun" id="A5.SS1.p1.3.m3.1d">7.4</annotation></semantics></math> respectively. For VQA, we report 5 numbers for better analysis, namely the accuracy for 4 Q&amp;A types: <span class="ltx_text ltx_font_italic" id="A5.SS1.p1.4.2">other</span>, <span class="ltx_text ltx_font_italic" id="A5.SS1.p1.4.3">unanswerable</span>, <span class="ltx_text ltx_font_italic" id="A5.SS1.p1.4.4">yes/no</span>, <span class="ltx_text ltx_font_italic" id="A5.SS1.p1.4.5">number</span>, and the overall accuracy. From the comparison of these numbers, when training VQA alone, the model tends to predict “unanswerable” to improve the accuracy. Because in the dataset, the <span class="ltx_text ltx_font_italic" id="A5.SS1.p1.4.6">unanswerable</span> type of Q&amp;A is the most common. For other types of Q&amp;A, the accuracy is relatively lower because a deeper or more granular understanding of the semantic information of image is required to predict the correct answers. After joint training with captioning, the accuracy of <span class="ltx_text ltx_font_italic" id="A5.SS1.p1.4.7">unanswerable</span> type Q&amp;A decreased, and the accuracy of other types increased. The model does not just return “unanswerable” blindly but understands more semantic information of the image and then make predictions. When all tasks are trained together, the accuracy of <span class="ltx_text ltx_font_italic" id="A5.SS1.p1.4.8">other</span> type Q&amp;A is greatly improved (<math alttext="+8.9\%" class="ltx_Math" display="inline" id="A5.SS1.p1.4.m4.1"><semantics id="A5.SS1.p1.4.m4.1a"><mrow id="A5.SS1.p1.4.m4.1.1" xref="A5.SS1.p1.4.m4.1.1.cmml"><mo id="A5.SS1.p1.4.m4.1.1a" xref="A5.SS1.p1.4.m4.1.1.cmml">+</mo><mrow id="A5.SS1.p1.4.m4.1.1.2" xref="A5.SS1.p1.4.m4.1.1.2.cmml"><mn id="A5.SS1.p1.4.m4.1.1.2.2" xref="A5.SS1.p1.4.m4.1.1.2.2.cmml">8.9</mn><mo id="A5.SS1.p1.4.m4.1.1.2.1" xref="A5.SS1.p1.4.m4.1.1.2.1.cmml">%</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A5.SS1.p1.4.m4.1b"><apply id="A5.SS1.p1.4.m4.1.1.cmml" xref="A5.SS1.p1.4.m4.1.1"><plus id="A5.SS1.p1.4.m4.1.1.1.cmml" xref="A5.SS1.p1.4.m4.1.1"></plus><apply id="A5.SS1.p1.4.m4.1.1.2.cmml" xref="A5.SS1.p1.4.m4.1.1.2"><csymbol cd="latexml" id="A5.SS1.p1.4.m4.1.1.2.1.cmml" xref="A5.SS1.p1.4.m4.1.1.2.1">percent</csymbol><cn id="A5.SS1.p1.4.m4.1.1.2.2.cmml" type="float" xref="A5.SS1.p1.4.m4.1.1.2.2">8.9</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A5.SS1.p1.4.m4.1c">+8.9\%</annotation><annotation encoding="application/x-llamapun" id="A5.SS1.p1.4.m4.1d">+ 8.9 %</annotation></semantics></math>). We analyze that it is because the question of this type of Q&amp;A is usually “<span class="ltx_text ltx_font_italic" id="A5.SS1.p1.4.9">what is this?</span>”, and the segmentation task naturally has a very good assisting effect in answering this question. Segmentation data can help models to learn more fine-grained visual understanding and consequently benefit vision-language tasks. We also give some examples to show these improvements in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A4.F7" title="Figure 7 ‣ D.2 More Comments ‣ Appendix D User Study ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">7</span></a>. Along with our findings in the main paper, we conclude that segmentation has clear benefits to VL learning and different VL tasks are complementary to each other.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>More Visualization</h2>
<section class="ltx_subsection" id="A6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.1 </span>Visualization on Test Datasets</h3>
<div class="ltx_para" id="A6.SS1.p1">
<p class="ltx_p" id="A6.SS1.p1.1">We present a comprehensive visualization of our model’s performance on the test datasets in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A4.F8" title="Figure 8 ‣ D.2 More Comments ‣ Appendix D User Study ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">8</span></a>. For segmentation, we show some results in outdoor scene, indoor scene, multi-person scene, especially the open-area mentioned by the PVIs. For OCR, various types of text recognition results can show the robustness and generalization of <span class="ltx_text ltx_font_smallcaps" id="A6.SS1.p1.1.1">@Model</span>. For other task, <span class="ltx_text ltx_font_smallcaps" id="A6.SS1.p1.1.2">@Model</span> can also perform well.</p>
</div>
</section>
<section class="ltx_subsection" id="A6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">F.2 </span>Zero Shot</h3>
<div class="ltx_para" id="A6.SS2.p1">
<p class="ltx_p" id="A6.SS2.p1.1">Finally, we apply the 5 tasks in a zero-shot manner to show the generalization ability of <span class="ltx_text ltx_font_smallcaps" id="A6.SS2.p1.1.1">@Model</span>. <span class="ltx_text ltx_font_smallcaps" id="A6.SS2.p1.1.2">@Model</span> performs well on three tasks: segmentation, depth estimation, and OCR, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A4.F9" title="Figure 9 ‣ D.2 More Comments ‣ Appendix D User Study ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">9</span></a>. However, for open-ended tasks, captioning and VQA , the performance on out-of-dataset data can sometimes be less satisfactory (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.14215v1#A4.F9" title="Figure 9 ‣ D.2 More Comments ‣ Appendix D User Study ‣ @Bench: Benchmarking Vision-Language Models for Human-centered Assistive Technology"><span class="ltx_text ltx_ref_tag">9</span></a> (B)). Therefore, it may be necessary to perform large-scale pre-training to enhance the model’s capability for handling these tasks well in zero-shot.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>More Discussion</h2>
<div class="ltx_para" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">This section discusses the limitations and future work of this work for more insights on the research in this track.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p2">
<p class="ltx_p" id="A7.p2.1"><span class="ltx_text ltx_font_bold" id="A7.p2.1.1">Pre-training.</span>
In the main paper, we did not perform pre-training. This has a certain impact on the capability of zero shot, especially for open-ended tasks. In the future, we plan to conduct pre-training on large-scale corpora to enhance the model’s zero-shot capability. Additionally, we use a unified language encoder to encode text in <span class="ltx_text ltx_font_smallcaps" id="A7.p2.1.2">@Model</span>. Pre-training can enrich the vocabulary size, thereby improving the model’s ability to open-vocabulary segmentation. The importance of this open-vocabulary capability for practical applications is self-evident, especially for blind users. As mentioned by blind users in user study, they require systems with high object recognition accuracy. When the model has seen a greater variety of objects and can distinguish between them, the recognition accuracy also increases. Additionally, this open-vocabulary capability allows the model to handle previously unseen objects. In sum, after pre-training, the model can better handle the diversity, complexity and unpredictability of usage scenarios.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p3">
<p class="ltx_p" id="A7.p3.1"><span class="ltx_text ltx_font_bold" id="A7.p3.1.1">Multi-task Training.</span>
As shown in the main submission, <span class="ltx_text ltx_font_smallcaps" id="A7.p3.1.2">@Model</span> performs well on the OCR task during single-task training, but there is a certain gap in performance during multi-task training. Our analysis suggests that the OCR dataset is too large, and the model does not balance multiple tasks during training. When dealing with multi-task training with extremely imbalanced dataset sizes, it is not enough to merely adjust loss weights differently. In the future, we may try more optimization methods for multi-task learning to ensure performance without greatly increasing the training time.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p4">
<p class="ltx_p" id="A7.p4.1"><span class="ltx_text ltx_font_bold" id="A7.p4.1.1">Functions Development and Model Deployment.</span>
In our user study, we have identified several potential and crucial functions that received unanimous agreement from participants. Furthermore, it’s important to note that <span class="ltx_text ltx_font_smallcaps" id="A7.p4.1.2">@Model</span> is not limited to these five tasks alone; it can be extended to more uni-modal or multi-modal tasks to provide more functionalities. Our future research direction will focus on building a PVIs-Centred generalist assistive system, leveraging <span class="ltx_text ltx_font_smallcaps" id="A7.p4.1.3">@Bench</span> and <span class="ltx_text ltx_font_smallcaps" id="A7.p4.1.4">@Model</span> as cornerstones, to develop a wide range of practical functions and services.
As for model deployment, although <span class="ltx_text ltx_font_smallcaps" id="A7.p4.1.5">@Model</span> achieves high performance on multiple datasets, since the model is based on Transformer, its costs are larger than the non-Transformer models.
Additionally, though <span class="ltx_text ltx_font_smallcaps" id="A7.p4.1.6">@Model</span> only has 62M parameters, it is still difficult to deploy such a model in the portable device used by PVIs. Therefore, in our future work we will discover how to extract or compress <span class="ltx_text ltx_font_smallcaps" id="A7.p4.1.7">@Model</span> into an efficient light-weight model.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 21 18:28:17 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
