<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering</title>
<!--Generated on Thu Sep 12 17:48:30 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="personal memory,  contextual augmentation,  diary study,  multimodal question answering,  RAG" lang="en" name="keywords"/>
<base href="/html/2409.08250v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S1" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>INTRODUCTION</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S2" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S2.SS1" title="In 2. Related Work ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Personal Memory Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S2.SS2" title="In 2. Related Work ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Multimodal Question Answering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S2.SS3" title="In 2. Related Work ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Applications Utilizing Contextual Information</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>DIARY STUDY</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS1" title="In 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Method</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS2" title="In 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Participants</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS3" title="In 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Data Summary and analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS3.SSS0.Px1" title="In 3.3. Data Summary and analysis ‣ 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">Direct content queries</span>:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS3.SSS0.Px2" title="In 3.3. Data Summary and analysis ‣ 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">Contextual filters</span>:</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS3.SSS0.Px3" title="In 3.3. Data Summary and analysis ‣ 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">Hybrid queries</span>:</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS4" title="In 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Analysis</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S4" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>TAXONOMY OF CONTEXTUAL INFORMATION</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S4.SS1" title="In 4. TAXONOMY OF CONTEXTUAL INFORMATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Atomic Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S4.SS2" title="In 4. TAXONOMY OF CONTEXTUAL INFORMATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Composite Context</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S4.SS3" title="In 4. TAXONOMY OF CONTEXTUAL INFORMATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span>Semantic Knowledge</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>OMNIQUERY: AUGMENTING CAPTURED MEMORIES</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.SS1" title="In 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Step 1: Structuring Individual Captured Memories</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.SS1.SSS0.Px1" title="In 5.1. Step 1: Structuring Individual Captured Memories ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title">Processing content</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.SS1.SSS0.Px2" title="In 5.1. Step 1: Structuring Individual Captured Memories ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title">Annotating atomic contexts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.SS1.SSS1" title="In 5.1. Step 1: Structuring Individual Captured Memories ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1.1 </span>Indexing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.SS2" title="In 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>Step 2: Identifying Composite Context</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.SS2.SSS0.Px1" title="In 5.2. Step 2: Identifying Composite Context ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title">Explicitly mentioned contexts.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.SS3" title="In 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.3 </span>Step 3: Inferring Semantic Knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.SS4" title="In 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.4 </span>Implementation Details</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S6" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>OMNIQUERY: QUESTION-ANSWERING SYSTEM</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S6.SS1" title="In 6. OMNIQUERY: QUESTION-ANSWERING SYSTEM ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>Query Augmentation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S6.SS2" title="In 6. OMNIQUERY: QUESTION-ANSWERING SYSTEM ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Retrieving Relevant Augmented Memories</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S6.SS2.SSS0.Px1" title="In 6.2. Retrieving Relevant Augmented Memories ‣ 6. OMNIQUERY: QUESTION-ANSWERING SYSTEM ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">Declarative query</span> <math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math> Semantic knowledge &amp; Processed content</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S6.SS2.SSS0.Px2" title="In 6.2. Retrieving Relevant Augmented Memories ‣ 6. OMNIQUERY: QUESTION-ANSWERING SYSTEM ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">Decomposed atomic contexts</span> <math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math> Annotated atomic contexts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S6.SS2.SSS0.Px3" title="In 6.2. Retrieving Relevant Augmented Memories ‣ 6. OMNIQUERY: QUESTION-ANSWERING SYSTEM ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_bold">Decomposed composite contexts</span> <math alttext="\rightarrow" class="ltx_Math" display="inline"><semantics><mo stretchy="false">→</mo><annotation-xml encoding="MathML-Content"><ci>→</ci></annotation-xml><annotation encoding="application/x-tex">\rightarrow</annotation><annotation encoding="application/x-llamapun">→</annotation></semantics></math> Identified composite contexts</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S6.SS3" title="In 6. OMNIQUERY: QUESTION-ANSWERING SYSTEM ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Answer Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S7" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>RESULTS</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S7.SS1" title="In 7. RESULTS ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span>Experiment Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S7.SS2" title="In 7. RESULTS ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span>Example Walkthrough</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>USER EVALUATION</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS1" title="In 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span>Participants</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS2" title="In 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span>Apparatus</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS3" title="In 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span>Procedure</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS3.SSS0.Px1" title="In 8.3. Procedure ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title">System setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS3.SSS0.Px2" title="In 8.3. Procedure ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title">Data preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS3.SSS0.Px3" title="In 8.3. Procedure ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title">Main session</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS4" title="In 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.4 </span>Comparison Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS5" title="In 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.5 </span>Quantitative Result</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS6" title="In 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6 </span>Qualitative Feedback and Findings</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS6.SSS1" title="In 8.6. Qualitative Feedback and Findings ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6.1 </span>Comparing with Existing Tools</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS6.SSS2" title="In 8.6. Qualitative Feedback and Findings ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6.2 </span>Reaction to Answers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS6.SSS3" title="In 8.6. Qualitative Feedback and Findings ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.6.3 </span>Iterative Editing of Questions</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S9" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span>DISCUSSION</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S9.SS1" title="In 9. DISCUSSION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1 </span>From Chat Interface to Multimodal Interactions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S9.SS2" title="In 9. DISCUSSION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2 </span>Enriching Memory Data and Visual Intelligence</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S9.SS3" title="In 9. DISCUSSION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.3 </span>Preserving Privacy</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S10" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span>CONCLUSION</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A1" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Prompts for LLMs</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A1.SS1" title="In Appendix A Prompts for LLMs ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.1 </span>Identifying Composite Contexts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A1.SS2" title="In Appendix A Prompts for LLMs ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.2 </span>Inferring Semantic Knowledge</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A1.SS3" title="In Appendix A Prompts for LLMs ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A.3 </span>Generating Answers Based on Retrieved Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A2" title="In OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Baseline Implementation</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiahao Nick Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">UCLA</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Los Angeles</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:ljhnick@ucla.edu">ljhnick@ucla.edu</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhuohao (Jerry) Zhang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">University of Washington</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Seattle</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:zhuohao@uw.edu">zhuohao@uw.edu</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jiaju Ma
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Stanford University</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Palo Alto</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">USA</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jiajuma@stanford.edu">jiajuma@stanford.edu</a>
</span></span></span>
</div>
<div class="ltx_dates">(2018; 20 February 2007; 12 March 2009; 5 June 2009)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id10.id1">People often capture memories through photos, screenshots, and videos. While existing AI-based tools enable querying this data using natural language, they mostly only support retrieving individual pieces of information like certain objects in photos, and struggle with answering more complex queries that involve interpreting interconnected memories like event sequences.
We conducted a one-month diary study to collect realistic user queries and generated a taxonomy of necessary contextual information for integrating with captured memories.
We then introduce OmniQuery, a novel system that is able to answer complex personal memory-related questions that require extracting and inferring contextual information.
OmniQuery augments single captured memories through integrating scattered contextual information from multiple interconnected memories,
retrieves relevant memories, and uses a large language model (LLM) to comprehensive answers.
In human evaluations, we show the effectiveness of OmniQuery with an accuracy of 71.5% and it
outperformed a conventional RAG system, winning or tying in 74.5% of the time.
</p>
</div>
<div class="ltx_keywords">personal memory, contextual augmentation, diary study, multimodal question answering, RAG
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2018</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_doi" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">doi: </span>XXXXXXX.XXXXXXX</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_isbn" id="id4"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">isbn: </span>978-1-4503-XXXX-X/18/06</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id5"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Human computer interaction (HCI)</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id6"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing Natural language interfaces</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id7"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Human-centered computing User studies</span></span></span>
<figure class="ltx_figure ltx_teaserfigure" id="S0.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="373" id="S0.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>
OmniQuery enables answering natural language personal questions (a) on individuals’ captured memories such as captured photos, saved screenshots and recorded videos (b). This is achieved by augmenting the captured memories through identifying and integrating scattered contextual information from multiple interconnected memories (c). Using the augmented contextual information, OmniQuery retrieves relevant memories, and leverages an LLM to generate a comprehensive answer and the reference memories (d).</figcaption>
</figure>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>INTRODUCTION</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">People often record their everyday life by taking photos, screenshots, and videos, whether for
saving important information, documenting special occasions, or simply capturing a funny moment <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib37" title="">2024b</a>)</cite>.
These captured instances, referred to as <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">captured memories</span>, collectively represent subsets of an individual’s <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">episodic memories</span> <cite class="ltx_cite ltx_citemacro_citep">(Tulving, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib57" title="">2002</a>)</cite>, a type of long-term memory that contains both specific past experiences and associated contextual details.
These episodic memories are essential for answering memory-related personal questions like, ”<span class="ltx_text ltx_font_italic" id="S1.p1.1.3">What social events did I attend during CHI 2024?</span>” (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S0.F1" title="Figure 1 ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>a),
which can help users reflect on past experiences and make informed decisions in daily tasks.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">However, these raw captured memories by themselves are insufficient to answer personal questions, as they lack contextual details that are typically implicit and scattered across multiple pieces of data.
For examples, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S0.F1" title="Figure 1 ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>b,
memories of attending parties during CHI 2024 are not explicitly annotated as occurring during the event.
Answering such personal questions requires extracting and integrating contextual information not typically contained within a single captured instance.
For example, by integrating multiple memories that mention “CHI 2024” in their content and extracting their metadata, it is possible to determine when the users attended the conference and connect related social events memories from that period to CHI 2024 (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S0.F1" title="Figure 1 ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>c), enabling the answer of the query (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S0.F1" title="Figure 1 ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a>d).</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Advancements in AI have enable question answering (QA) on long documents <cite class="ltx_cite ltx_citemacro_citep">(Beltagy et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib5" title="">2020</a>; Wang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib59" title="">2024</a>)</cite>, knowledge graph <cite class="ltx_cite ltx_citemacro_citep">(Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib29" title="">2019a</a>; Yasunaga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib63" title="">2021a</a>)</cite>, multimodal databases <cite class="ltx_cite ltx_citemacro_citep">(Talmor et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib55" title="">2021</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib14" title="">2022</a>)</cite>, egocentric videos <cite class="ltx_cite ltx_citemacro_citep">(Mangalam et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib44" title="">2023</a>; Hou et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib28" title="">2023</a>)</cite>.
These methods typically rely on data-driven approaches to train powerful models for the target task.
However, the private nature of captured memories makes it difficult to curate large datasets, posing challenges for training models specifically for QA on personal captured memories.
Recent LLM-based work has adopted retrieval augmented generation (RAG) workflow to handle external databases without specific training <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib35" title="">2021</a>)</cite>.
However, such methods depend on explicit connections between queries and relevant external data <cite class="ltx_cite ltx_citemacro_citep">(Edge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib17" title="">2024</a>)</cite>.
In contrast, captured memories are often unstructured and lack contextual annotations, making it difficult to establish explicit links between queries and interconnected memories.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">To achieve this, we propose OmniQuery, a novel approach designed to robustly and comprehensively answer users’ queries on their captured memories.
OmniQuery has two key components:
<span class="ltx_text ltx_font_italic" id="S1.p4.1.1">(i)</span> a question-agnostic pipeline to augment captured memories with contextual information extracted from other related memories to produce <span class="ltx_text ltx_font_italic" id="S1.p4.1.2">context-augmented</span> memories,
and <span class="ltx_text ltx_font_italic" id="S1.p4.1.3">(ii)</span> a natural language question answering system that retrieves these processed memories and generates comprehensive answers with referenced captured memories as evidence.
The processing pipeline in <span class="ltx_text ltx_font_italic" id="S1.p4.1.4">(i)</span> is informed by the taxonomy of contextual information that we generated from a one-month diary study with 29 participants.
Specifically, we collected and analyzed 299 user queries, identifying the types of contextual information that would be most useful for augmenting capture memories.
We identified three types of personal questions (direct content queries, contextual filters, and hybrid queries) and generated a taxonomy of three major categories (atomic context, composite context, and semantic knowledge)
which
then guide the design of memory augmentation pipeline to enhance memory retrieval comprehensiveness.
For <span class="ltx_text ltx_font_italic" id="S1.p4.1.5">(ii)</span>, OmniQuery employs a retrieval-augmented architecture: given a user input query, it
(1) augments the query via a rewriting strategy, and
(2) retrieves related memories from the augmented data and uses an LLM to generate the final answer, along with the reference memory instances.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To evaluate OmniQuery, we conducted a user evaluation with 10 participants against a generic RAG-based baseline.
The participants tested queries both logged during the diary study and generated during the evaluation session on a subset of their own captured memories.
For each tested query, participants rated the user perceived correctness and completeness of the answers generated by both systems in a blinded manner.
The results show that OmniQuery effectively answers different types of queries on users’ personal memories, outperforming the baseline with higher accuracy (71.5%, exceeding the baseline by 27.6%) and winning or tying 74.5% of the time in direct comparisons, which are based on user-perceived accuracy and completeness.
</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">In summary, we contribute:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">A taxonomy of contextual information for augmenting captured memories, consisting of three major categories. The taxonomy was derived from authentic queries collected in a one-month diary study with 29 participants.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">A pipeline of augmenting captured memories that leverages temporal-based reasoning to extract and infer missing contextual information from other related captured memories.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">The design and implementation of an end-to-end system that comprehensively answers user queries.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">A user evaluation of OmniQuery against a baseline system, showing OmniQuery’s effectiveness with 71.5% accuracy and outperforming the baseline (winning or tying 74.5% of the time).</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Related Work</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">OmniQuery is inspired by and related to prior work in the areas on personal memory augmentation, multimodal question answering and applications that utilize contextual information.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>Personal Memory Augmentation</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">A large body of work in human-computer interaction (HCI) has been focused on augmenting users’ memory.
This includes developing reminder tools for elderlies or people with memory impairments <cite class="ltx_cite ltx_citemacro_citep">(Caprani et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib9" title="">2006</a>; Jamieson et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib32" title="">2014</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib33" title="">2017</a>; Shin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib53" title="">2021</a>)</cite>, providing proactive support in daily tasks <cite class="ltx_cite ltx_citemacro_citep">(Zulfikar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib67" title="">2024</a>; Chan, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib12" title="">2022</a>)</cite>, or manipulating users’ memory focus in extended reality <cite class="ltx_cite ltx_citemacro_citep">(Bonnail et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib6" title="">2023</a>)</cite>.
These work typically focus on the “capturing” stage of the memory augmentation, where researchers develop wearable devices that continuously capture data using designated sensors, which record various modalities such as videos <cite class="ltx_cite ltx_citemacro_citep">(Hodges et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib25" title="">2006</a>; Dubourg et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib16" title="">2016</a>; Mann, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib45" title="">1996</a>)</cite>, audios <cite class="ltx_cite ltx_citemacro_citep">(Hayes et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib24" title="">2004</a>; Vemuri et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib58" title="">2004</a>)</cite>, or bio-signals <cite class="ltx_cite ltx_citemacro_citep">(Chan, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib11" title="">2020</a>)</cite>, to augment the memory database.
For example, recent work such as Memoro developed a wearable, audio-based device that continuously records users’ conversations and enables memory suggestions in real-time, either through explicit queries or query-less contextual cues <cite class="ltx_cite ltx_citemacro_citep">(Zulfikar et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib67" title="">2024</a>)</cite>.
Differently, OmniQuery focuses on the “post-capturing” stage, utilizing already-existing memory data (e.g., photos and videos users have already captured). It addresses challenges in processing, annotating, and augmenting captured memories with contextual information.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">Prior work in natural language processing (NLP), computer vision (CV), and information retrieval (IR) has studies methods of augmenting people memory.
Perhaps the most related is QA on egocentric videos, which are also a form of personal data.
Representative tasks include episodic memory retrieval <cite class="ltx_cite ltx_citemacro_citep">(Gurrin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib23" title="">2014</a>; Grauman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib22" title="">2021</a>; Engel et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib18" title="">2023</a>)</cite>, where the system, given a long egocentric video and a query, localizes the answer within the video.
However, these datasets differ from the captured data targeted by OmniQuery. The main challenge in egocentric videos is filtering through large, often noisy data, using data-driven approaches to train models for feature extraction.
In contrast, captured memories represent a smaller, intentionally collected dataset, where the challenge lies in integrating scattered contextual information across multiple implicitly related memories.
Therefore, OmniQuery employs a taxonomy-based method to augment existing data without the need for specific model training, improving QA performance.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Multimodal Question Answering</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Over time, natural language QA research has shifted to more complex settings, including QA across different modalities (e.g., images <cite class="ltx_cite ltx_citemacro_citep">(Agrawal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib3" title="">2015</a>; Goyal et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib21" title="">2016</a>)</cite>, videos <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib61" title="">2003</a>; Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib65" title="">2023b</a>; Maaz et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib42" title="">2023</a>)</cite>, tables <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib66" title="">2023a</a>)</cite> or knowledge graph <cite class="ltx_cite ltx_citemacro_citep">(Yasunaga et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib64" title="">2021b</a>; Huang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib30" title="">2019b</a>)</cite>), QA on large datasets <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib35" title="">2021</a>; Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib13" title="">2017</a>)</cite> and tasks that require multihop reasoning <cite class="ltx_cite ltx_citemacro_citep">(Yang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib62" title="">2018</a>; Mavi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib46" title="">2024</a>)</cite>.
Recent advancements in large language models (LLMs) and multimodal foundation models (e.g., <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib39" title="">2024b</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib38" title="">2023a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib40" title="">2023b</a>)</cite>) have have enabled improved reasoning and answer generation over large, multimodal datasets.
This is similar to OmniQuery’s use case as answering personal questions requires handling large amounts of captured memories and performing complex reasoning.
Prior work has used retrieval-augmented generation (RAG) workflow <cite class="ltx_cite ltx_citemacro_citep">(Lewis et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib35" title="">2021</a>)</cite>, which retrieve relevant information from external datasets based on a query and then generate output using the retrieved results.
For example, MuRAG leverages RAG to answer open questions via retrieving related information from databases of images and text <cite class="ltx_cite ltx_citemacro_citep">(Chen et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib14" title="">2022</a>)</cite>.
VideoAgent leverages structured memories processed from long videos to accomplish video understanding tasks <cite class="ltx_cite ltx_citemacro_citep">(Fan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib19" title="">2024</a>)</cite>.
However, these methods rely on datasets already rich in context (e.g., Wikipedia<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>https://www.wikipedia.org/</span></span></span>)
and improvements are often achieved by designing new retrieval workflows (e.g., Self-RAG <cite class="ltx_cite ltx_citemacro_citep">(Asai et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib4" title="">2023</a>)</cite> and tree-based retrieval <cite class="ltx_cite ltx_citemacro_citep">(Sarthi et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib51" title="">2024</a>)</cite>)
or query augmentation <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib10" title="">2024</a>)</cite>.
More recently, GraphRAG introduced a data augmentation approach that generates a knowledge graph from extensive raw data to tackle tasks requiring higher-level understanding, such as query-focused summarization <cite class="ltx_cite ltx_citemacro_citep">(Edge et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib17" title="">2024</a>)</cite>.
While GraphRAG leverages data-driven methods to identify themes and communities within graph nodes, OmniQuery extends this concept by adopting a taxonomy-based augmentation approach, informed by insights from a diary study, to enhance retrieval on personal captured memories.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3. </span>Applications Utilizing Contextual Information</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Contextual information has long been important in HCI research from early mixed-initiative systems <cite class="ltx_cite ltx_citemacro_citep">(Horvitz, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib27" title="">1999</a>)</cite> to recent agentic workflow <cite class="ltx_cite ltx_citemacro_citep">(Jaber et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib31" title="">2024</a>)</cite>.
Over the past few years, there has been a surge in the usage of AI and LLMs in the HCI community,
which enables extracting contextual information from processing raw multimodal information.
For example, Li <span class="ltx_text ltx_font_italic" id="S2.SS3.p1.1.1">et al.</span> studied how visually impaired people cook and emphasized the importance of conveying contextual information to users through multimodal models <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib36" title="">2024a</a>)</cite>.
Additionally, Human I/O leverages egocentric perceptions of users and detect situational impairments through reasoning on the multimodal sensing data <cite class="ltx_cite ltx_citemacro_citep">(Liu et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib41" title="">2024a</a>)</cite>.
GazePointAR develops a context-aware voice assistant to disambiguate users’ intent when interacting with real-world information <cite class="ltx_cite ltx_citemacro_citep">(Lee et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib34" title="">2024</a>)</cite>.
OmniActions categorizes digital follow-up actions on real-world information and provides proactive action prediction based on perceived context <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib37" title="">2024b</a>)</cite>.
Specifically, these system utilized off-the-shelf multimodal models to process raw sensory data, and leverages the reasoning capabilities of LLMs to infer the semantic context.
OmniQuery builds on this approach by applying these AI techniques to extract and integrate semantic context scattered across various unstructured, raw captured memories.
This augmentation enhances users’ memory databases, enabling them to answer personal questions about their memories through natural language queries.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>DIARY STUDY</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">While single captured memory often lacks essential contextual information,
OmniQuery proposes to augment such memories by extracting and inferring semantic context from other explicitly or implicitly related memories.
To understand how to effectively augment captured memories, we need to answer the following research question:
</p>
<ul class="ltx_itemize" id="S3.I1">
<li class="ltx_item" id="S3.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"><span class="ltx_text ltx_font_bold" id="S3.I1.ix1.1.1.1">RQ</span>:</span>
<div class="ltx_para" id="S3.I1.ix1.p1">
<p class="ltx_p" id="S3.I1.ix1.p1.1">What contextual information is essential to integrate with captured memory instances to ensure accurate retrieval in response to user queries?</p>
</div>
</li>
</ul>
<p class="ltx_p" id="S3.p1.2">This question is important as “context” is a broad term, and thus the focus should be on categorizing and identifying the most effective contextual information that enables accurate and meaningful responses to the types of queries users generate when reflecting on past experiences.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Method</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To answer the research question above, we conducted a diary study, a methodology that enables participants to log data whenever need arose <cite class="ltx_cite ltx_citemacro_citep">(Sohn et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib54" title="">2008</a>)</cite>.
Specifically, we adopted the <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.1">snippet-based technique</span> proposed by Brandt <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.1.2">et al.</span> <cite class="ltx_cite ltx_citemacro_citep">(Brandt et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib7" title="">2007</a>)</cite>.
We asked participants to log queries on their past memories only when they had real intent under a genuine context,
rather than brainstorming potential questions they might ask to retrieve specific past memories.
This approach enabled us to collect authentic and spontaneous queries that users have in real-world scenarios.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">We collect the data including:
<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.1">(i)</span> the queries participants would use to retrieve or ask about their past memories,
<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.2">(ii)</span> the reasons and contexts prompting these queries (e.g., wanting to show a past experience while chatting with a friend) and
<span class="ltx_text ltx_font_italic" id="S3.SS1.p2.1.3">(iii)</span> (optional) whether they were able to retrieve the corresponding memories from their album, and if so, how they did it (e.g., by scrolling through the photo album).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Participants</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Thirty-two participants (i.e., 14 male, 17 female, one binary) were initially recruited through an online RSVP form distributed via the X platform<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>https://x.com/</span></span></span>. Participants come from North America and Asia.
Eleven participants reported using Android devices, while the remainder used iOS devices in their daily life.
Additionally, 16 participants reported actively logging their daily lives, 13 regularly logged important events and memorable experiences, 2 logged only essential information, and one seldom logged their lives.
While participants were compensated based on their participation ($50 for full participation), they were not required to log a specific number queries each day or over the entire study period.
This approach was intentional, as we did not want to pressure them into generating queries artificially.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Data Summary and analysis</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">During the diary study, one participant opted out during the first week, and two participants did not log any queries throughout the entire study. Of the remaining participants, seven stopped logging after the first week. The rest remained active until the end of the study. As a result, we collected a total of 299 queries. On average, each participant contributed 10.27 queries, with a standard deviation of 6.09, and the highest number of queries contributed by a single participant was 25.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">From the collected queries, we identified three major types of query: (1) direct content queries (75 queries), (2) context-based filters (28 queries), and (3) hybrid queries (191 queries).
The rest five queries fell outside these categories as participants were attempting non memory-related tasks, such as “<span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.1">Mark yesterday pictures as favorites</span>”.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px1.1.1">Direct content queries</span>:</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">These queries aim to get direct answers that can be retrieved by searching for memories via description (e.g., “<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p1.1.1">skateboarding in a tie-dye shirt</span>”) or rely on information explicitly contained within a single captured instance (e.g., “<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px1.p1.1.2">What is my driver’s license number?</span>”).
Typically, this type of query <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px1.p1.1.3">does not</span> require additional context not contained in a single captured memory.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px2.1.1">Contextual filters</span>:</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">These queries focus on retrieving memories based on specific contexts, such as time, location, or event.
For example, a query like “<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px2.p1.1.1">All the photos in Hawaii</span>” might only require filtering based on metadata like location.
However, for more complex queries such as “<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px2.p1.1.2">All the photos from my graduation ceremony</span>”,
it <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px2.p1.1.3">does</span> require a deeper synthesis of multiple interconnected memories to reconstruct the context surrounding the event.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px3.1.1">Hybrid queries</span>:</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">These queries are more complex, combining both direct content queries and contextual filters.
For example, a participant asked “<span class="ltx_text ltx_font_italic" id="S3.SS3.SSS0.Px3.p1.1.1">Which meat did I order the last time I came to this Japanese BBQ restaurant?</span>”
Answering such a query typically requires a <span class="ltx_text ltx_font_bold" id="S3.SS3.SSS0.Px3.p1.1.2">multi-hop</span> process: (1) filter all captured memories under the specific context (e.g., dining in this Japanese restaurant); (2) analyze the filtered data to generate the final result.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Analysis</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Inspired by the psychological memory theory <cite class="ltx_cite ltx_citemacro_citep">(Tulving, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib57" title="">2002</a>)</cite>,
our data summary indicates that 74.4% of the queries (contextual filters + hybrid queries) require more than just querying the direct content. The complexity in these queries require integration of contextual information in captured memories for accurate processing and filtering. Therefore, we take a step further to build a taxonomy of contextual information in user queries to inform the design of OmniQuery.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">To identify this essential contextual information, two researchers on the team independently analyzed the logged queries, and coded, filtered, and categorized the types of context required to filter captured memories and better answer the queries. Their results were compared, and discrepancies in categorizations, hierarchy, naming, and granularity were discussed and resolved.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>TAXONOMY OF CONTEXTUAL INFORMATION</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">In this section, we present the taxonomy built from analyzing user queries. We identified three key types of contextual information that can be integrated with captured memories.
These include (1) atomic context, (2) composite context and (3) semantic knowledge.</p>
</div>
<figure class="ltx_figure" id="S4.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="527" id="S4.F2.g1" src="x2.png" width="832"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>Number of appearance of each types of context (atomic and composite) in the logged queries. Note that a query may contain multiple types of categories, such as “<span class="ltx_text ltx_font_italic" id="S4.F2.2.1">What <span class="ltx_text" id="S4.F2.2.1.1" style="background-color:#000000;">boba tea</span> did I drink <span class="ltx_text" id="S4.F2.2.1.2" style="background-color:#000000;">last week</span>?</span>”</figcaption>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1. </span>Atomic Context</h3>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Categorization and examples of atomic and composite context </figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.T1.1">{tblr}</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.T1.2">row3-9=4mm,row11=4mm,column1=22mm,c,column2=65mm,c,column3=60mm,c,colspec=—Q[r]—Q[l]—Q[l]—,rowspec=—Q[m]—Q[m]—Q[m]—Q[m]—Q[m]—Q[m]—Q[m]—Q[m]—Q[m]—Q[m]—Q[m]—
<span class="ltx_text ltx_font_bold" id="S4.T1.2.1">Category</span> &amp; <span class="ltx_ERROR undefined" id="S4.T1.2.2">\SetCell</span>[c=1]c <span class="ltx_text ltx_font_bold" id="S4.T1.2.3">Definition</span> <span class="ltx_text ltx_font_bold" id="S4.T1.2.4">Exemplar queries</span> <span class="ltx_text" id="S4.T1.2.5" style="background-color:#000000;">   </span> refers to contextual cues 
<br class="ltx_break"/><span class="ltx_ERROR undefined" id="S4.T1.2.6">\SetCell</span>[c=3]c,m <span class="ltx_text ltx_font_bold" id="S4.T1.2.7">Atomic context</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S4.T1.2.8">Temporal info</span>  Specific time period or particular time of the day  “<span class="ltx_text ltx_font_italic" id="S4.T1.2.9">What boba tea did I drink <span class="ltx_text" id="S4.T1.2.9.1" style="background-color:#000000;">last week</span>?</span>” 
<br class="ltx_break"/>“<span class="ltx_text ltx_font_italic" id="S4.T1.2.10">what is my routine <span class="ltx_text" id="S4.T1.2.10.1" style="background-color:#000000;">in the morning</span>?</span>” 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S4.T1.2.11">Geographical info</span>  Location data such as city names or venue details  “<span class="ltx_text ltx_font_italic" id="S4.T1.2.12">How many churches did I visit <span class="ltx_text" id="S4.T1.2.12.1" style="background-color:#000000;">in Barcelona</span>?</span>” 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S4.T1.2.13">People</span>  Individuals present in the captured memories  “<span class="ltx_text ltx_font_italic" id="S4.T1.2.14">find the photo of <span class="ltx_text" id="S4.T1.2.14.1" style="background-color:#000000;">me and my grandpa</span> last year.</span>” 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S4.T1.2.15">Visual elements</span>  Other directly sensible elements, including animals, physical objects, or specific visual features 
“<span class="ltx_text ltx_font_italic" id="S4.T1.2.16">My photo with <span class="ltx_text" id="S4.T1.2.16.1" style="background-color:#000000;">short hair</span> last year.</span>” 
<br class="ltx_break"/>“<span class="ltx_text ltx_font_italic" id="S4.T1.2.17">Photo of <span class="ltx_text" id="S4.T1.2.17.1" style="background-color:#000000;">my dog</span> when he was a puppy.</span>”
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S4.T1.2.18">Environment</span>  Inferred environment based on the content  “<span class="ltx_text ltx_font_italic" id="S4.T1.2.19" style="background-color:#000000;">Gym</span><span class="ltx_text ltx_font_italic" id="S4.T1.2.20"> selfies from last year.</span>” 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S4.T1.2.21">Activities</span>  Actions or activities inferred from the content  “<span class="ltx_text ltx_font_italic" id="S4.T1.2.22">How many <span class="ltx_text" id="S4.T1.2.22.1" style="background-color:#000000;">cardio session</span> did I complete last month?</span>” 
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="S4.T1.2.23">Emotion</span>  Subjective emotion or emotional cues  “<span class="ltx_text ltx_font_italic" id="S4.T1.2.24">My <span class="ltx_text" id="S4.T1.2.24.1" style="background-color:#000000;">happiest moment</span> last year</span>” 
<br class="ltx_break"/></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><span class="ltx_ERROR ltx_centering ltx_figure_panel undefined" id="S4.T1.3">\SetCell</span></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S4.T1.4">[c=3]c,m <span class="ltx_text ltx_font_bold" id="S4.T1.4.1">Composite context</span>
<br class="ltx_break"/>-  Combination of multiple <span class="ltx_text ltx_font_bold" id="S4.T1.4.2">atomic contexts</span>  “<span class="ltx_text ltx_font_italic" id="S4.T1.4.3">Who did I ski with in <span class="ltx_text" id="S4.T1.4.3.1" style="background-color:#000000;">the lab retreat</span> last year</span>” 
<br class="ltx_break"/></p>
</div>
</div>
</figure>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Atomic context refers to contextual information typically obtainable from a single captured memory.
This includes data directly from metadata, sensed from visual and auditory content, or inferred from the content itself.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S4.T1" title="Table 1 ‣ 4.1. Atomic Context ‣ 4. TAXONOMY OF CONTEXTUAL INFORMATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a> shows the seven types of atomic contexts categorized from the queries.
Among them, temporal information and geographical info can be directly obtained from the memory media’s metadata.
People and visual elements typically require machine learning models or facial recognition for detection. Environment, activity, and emotion are more implicit and require reasoning based on the content (e.g., a photo of a menu may suggest the person is in a restaurant).
The number of appearance of each category is shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S4.F2" title="Figure 2 ‣ 4. TAXONOMY OF CONTEXTUAL INFORMATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2. </span>Composite Context</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Composite context is how people often remember and refer to past experiences with a single phrase, such as ”<span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.1">Who did I ski with in the lab retreat last year?</span>”
These contexts can range from significant events like a wedding or a conference trip to smaller incidents like hanging out with a friend or a day trip to Seattle.
Specifically, composite context is defined as <span class="ltx_text ltx_font_bold" id="S4.SS2.p1.1.2">a combination of multiple atomic contexts</span>.
For example, the composite context “lab retreat” encompasses atomic contexts including “Feburary, 2024” (temporal), “Lake Tahoe, California” (geographical) and “hanging out with labmates” (activity).</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">While atomic context is typically available within a single captured memory, composite context requires integrating multiple memories to understand the connection between them.
Since an individual’s captured memories are linear on the timeline, memories related to a specific event tend to cluster closely together.
We leveraged this <span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">temporal proximity</span> to identify and extract various composite contexts from the raw captured memories. For a detailed discussion of this approach, please refer to Section <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.SS2" title="5.2. Step 2: Identifying Composite Context ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">5.2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3. </span>Semantic Knowledge</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In psychology theories, semantic knowledge refers to the general world knowledge that humans accumulate over time <cite class="ltx_cite ltx_citemacro_citep">(Tulving, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib57" title="">2002</a>; McRae and Jones, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib47" title="">2013</a>)</cite>, distinct from episodic memories, which are tied to specific experiences and events.
Similarly, we can generate semantic knowledge from a user’s captured memories, providing broader insights of the user’s past experiences.
For example, patterns like “Jason has a habit of going to the gym 3-4 times a week” can be inferred from multiple captured memories.
Such patterns are helpful in answering queries that not necessarily require specific knowledge such as “<span class="ltx_text ltx_font_italic" id="S4.SS3.p1.1.1">How often do I go to the gym in April?</span>”</p>
</div>
<figure class="ltx_figure" id="S4.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="308" id="S4.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>Augmenting captured memories involves three steps: (1) structuring memories by processing content and annotating with atomic contexts; (2) identifying composite context through sliding windows; (3) inferring semantic knowledge from the structured memories and identified contexts.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>OMNIQUERY: AUGMENTING CAPTURED MEMORIES</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Informed by the generated taxonomy, OmniQuery employs a <span class="ltx_text ltx_font_bold" id="S5.p1.1.1">query-agnostic</span> preprocessing pipeline to augment existing captured memories.
The pipeline extracts scattered contextual information from interconnected captured memories, synthesizes it, and augment each memory with the enhanced context.
Specifically, the augmentation pipeline involves three steps (as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S4.F3" title="Figure 3 ‣ 4.3. Semantic Knowledge ‣ 4. TAXONOMY OF CONTEXTUAL INFORMATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a>):
(1) structuring individual captured memories via processing their content and annotating with atomic contexts,
(2) identifying and synthesizing composite contexts from multiple captured memories using sliding windows, and
(3) inferring semantic knowledge from multiple captured memories and the identified composite contexts.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1. </span>Step 1: Structuring Individual Captured Memories</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">Raw captured memories are often unstructured and lack contextual annotation <cite class="ltx_cite ltx_citemacro_citep">(Sedlakova et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib52" title="">2023</a>)</cite>.
In this step, OmniQuery structures each captured memory, making it easier to analyze and extract information.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F4" title="Figure 4 ‣ Annotating atomic contexts ‣ 5.1. Step 1: Structuring Individual Captured Memories ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a> shows an example of structuring an single captured memory, which involves two key parts:
(1) processing and understanding the content of the memory and (2) annotating the memory with atomic contexts.</p>
</div>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Processing content</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">Content of a captured memory includes the overall description of the memory as caption, visible text, and transcribed speech transcribed speech (for videos, not shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F4" title="Figure 4 ‣ Annotating atomic contexts ‣ 5.1. Step 1: Structuring Individual Captured Memories ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a>).
Specifically, OmniQuery leverages multimodal models to process and describe the captured memory as the caption, performs optical character recognition (OCR) to recognize visible text, and uses audio-to-text models to transcribe speech.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Annotating atomic contexts</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">With the content processed, OmniQuery annotates each captured memory with each type of atomic context.
As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F4" title="Figure 4 ‣ Annotating atomic contexts ‣ 5.1. Step 1: Structuring Individual Captured Memories ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">4</span></a>b, OmniQuery extracts the temporal and geographical information from the metadata and uses multimodal models to detect people and other visual elements.
Then OmniQuery synthesizes the processed information and infers the environment and activities.
For example, based on a photo of a sign displaying conference Wi-Fi details, OmniQuery infers that the user is likely attending a conference (activity) and is at the conference venue (environment).
Note that due to the subjective matter of emotions, which often requires user input, emotion inference is excluded in current implementation.</p>
</div>
<figure class="ltx_figure" id="S5.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="245" id="S5.F4.g1" src="x4.png" width="746"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>An example of structuring an individual captured memory (a photo of the Wifi details of CHI 2024 conference).</figcaption>
</figure>
</section>
<section class="ltx_subsubsection" id="S5.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">5.1.1. </span>Indexing</h4>
<div class="ltx_para" id="S5.SS1.SSS1.p1">
<p class="ltx_p" id="S5.SS1.SSS1.p1.1">After each captured memory is structured, it is indexed and stored in a database.
Additionally, the annotations in textual format are encoded into text embeddings to enable vector-based search during the retrieval process.
In the database,
each data entry represents a data entry corresponding to a captured memory, containing both the original media (e.g., photo, video) and its structured annotations in text and text embeddings.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2. </span>Step 2: Identifying Composite Context</h3>
<figure class="ltx_figure" id="S5.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="460" id="S5.F5.g1" src="x5.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5. </span>An example of using sliding windows to identify composite contexts and infer semantic knowledge: (a) two consecutive sliding windows; (b) composite contexts and semantic knowledge generated in each window; (c) merging results from the two windows.</figcaption>
</figure>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">As captured memories are recorded in a linear manner along a personal timeline, those interconnected through semantic contexts often cluster closely together.
For example, memories related to CHI 2024 are likely to occur during the event itself.
Taking advantage of this <span class="ltx_text ltx_font_bold" id="S5.SS2.p1.1.1">temporal proximity</span>, OmniQuery adopts a sliding window approach to analyze potentially interconnected captured in segments, identifying composite contexts.</p>
</div>
<div class="ltx_para" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F5" title="Figure 5 ‣ 5.2. Step 2: Identifying Composite Context ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>a, a static window size of seven days is used in current implementation.
The inference is performed via an LLM, in which the input is the structured annotations of these memories and the output is the identified composite contexts along with their start and end dates and the associated captured memories (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F5" title="Figure 5 ‣ 5.2. Step 2: Identifying Composite Context ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>b).
To account for cases where composite contexts are split in half, we use a step size (4 days in the current setup) smaller than the window size, allowing for overlap and comprehensive processing.
For longer composite contexts (e.g., lasting more than two weeks), each segment of the context is identified separately within the sliding windows and then merged into a single composite context. Additionally, any duplicated composite contexts caused by the overlap between sliding windows are also merged to avoid redundancy (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F5" title="Figure 5 ‣ 5.2. Step 2: Identifying Composite Context ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>c).</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">Specifically, as opposed to including detailed predefined categories (as with atomic contexts) in the prompt for LLMs, we adopt a few-shot prompting technique <cite class="ltx_cite ltx_citemacro_citep">(Brown et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib8" title="">2020</a>)</cite>, providing examples of composite contexts summarized from the collected questions in the prompt.
For detailed prompt, please refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A1.SS2" title="A.2. Inferring Semantic Knowledge ‣ Appendix A Prompts for LLMs ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">A.2</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Explicitly mentioned contexts.</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">However, some composite contexts are <span class="ltx_text ltx_font_bold" id="S5.SS2.SSS0.Px1.p1.1.1">explicitly</span> mentioned in the captured memories.
For example, a screenshot of a flyer may reference the upcoming ”CHI 2024” event happening next month, or a transcribed conversation might discuss a ”Hawaii trip” that took place the previous year.
We leverage LLMs to differentiate between atomic contexts and composite contexts.
For example, “a workout session” is identified as an activity (atomic context), whereas “CHI 2024” is recognized as a composite context which likely involves multiple interconnected atomic contexts.
Such identified composite contexts are either merged with an existing composite context (e.g., if ”CHI 2024” has already been identified) or added as a new composite context if it is unique.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.3. </span>Step 3: Inferring Semantic Knowledge</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">Different from composite contexts, semantic knowledge focuses on high-level general knowledge rather than specific memory details.
For example, if a chat message screenshot mentions celebrating Jason’s birthday, the inferred semantic knowledge might be ”Jason’s birthday is on [SPECIFIC DATE].” Similarly, analyzing multiple grocery shopping receipts that consistently include lactose-free milk could lead to the inference that the user is possibly lactose intolerant.
Semantic knowledge is inferred in each sliding window, while also taking into account the identified composite contexts to gain higher-level understanding of the user’s past and generalized information (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F5" title="Figure 5 ‣ 5.2. Step 2: Identifying Composite Context ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">5</span></a>b bottom).
The output is a list of inferred declarative semantic knowledge independent from specific memories.
The instructions provided to the model are specifically tailored to guide the inference process toward overarching patterns and trends rather than specific event details.
The detailed prompt for identifying semantic knowledge can be also found in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A1.SS2" title="A.2. Inferring Semantic Knowledge ‣ Appendix A Prompts for LLMs ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">A.2</span></a>.
Each inferred entry of semantic knowledge is either merged with existing entries or added to the knowledge list if new.</p>
</div>
<figure class="ltx_figure" id="S5.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="566" id="S5.F6.g1" src="x6.png" width="789"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6. </span>
The question-answering system consists of: (a) query augmentation by decomposing and inferring contextual information; (b) retrieving memories and semantic knowledge; (c) generating answers with an LLM using referenced memories. Given a query: (d) OmniQuery rewrites and decomposes it; (e) infers contextual information; (f) retrieves relevant memories; and (g) retrieves knowledge from memory and knowledge storage.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S5.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.4. </span>Implementation Details</h3>
<div class="ltx_para" id="S5.SS4.p1">
<p class="ltx_p" id="S5.SS4.p1.1">To deduplicate images as people tend to capture similar content multiple times,
we use CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib50" title="">2021</a>)</cite> to encode images to embeddings and calculate the similarities between images and merge those exceed the similarity of 0.85.
We use the Google Cloud Vision API<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>https://cloud.google.com/vision/docs/ocr</span></span></span> for OCR to detect text in images and OpenAI’s Whisper model<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>https://github.com/openai/whisper</span></span></span> for audio-to-text conversion.
Note that Whipser is known for hallucination when there is no speech in the audio, thus we applied further data cleaning to validate the transcribed result using OpenAI’s <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.1">GPT-4o-mini</span>.
For other visual processing,
We use <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.2">GPT-4o</span> handles multimodal sensing, including identifying people and visual elements in images and generating scene descriptions.
For video processing, as a proof-of-concept, we consider only the first 10 seconds of each video, sampling 10 frames to be analyzed by <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.3">GPT-4o</span> for content understanding.
Text is encoded into embeddings using OpenAI’s <span class="ltx_text ltx_font_typewriter" id="S5.SS4.p1.1.4">text-embedding-3-small</span> model. Currently, we utilize a custom vector database and matrix-based similarity search implemented with NumPy in Python. However, for real-world applications, more advanced vector databases (e.g., Pinecone<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>https://www.pinecone.io/</span></span></span>) would be necessary to handle larger volumes of personal data.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>OMNIQUERY: QUESTION-ANSWERING SYSTEM</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">With Captured memories augmented with contextual information, OmniQuery adopts a RAG architecture for the question answering system.
RAG-based systems are effective in handling large datasets and mitigating hallucination issues by retrieving relevant content and grounding the generated results in this retrieved information. This approach ensures that the output is both relevant and accurate, leveraging specific data rather than relying solely on the model’s internal knowledge.
This approach is chosen because, on average, personal captured memories often exceed 30,000 photos and videos (as reported by participants in our diary study), which exceeds the limit of most foundation models nowadays.
</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F6" title="Figure 6 ‣ 5.3. Step 3: Inferring Semantic Knowledge ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">6</span></a>,
given an input query, OmniQuery first augments the query by disambiguating and decomposing it into specific contextual elements (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F6" title="Figure 6 ‣ 5.3. Step 3: Inferring Semantic Knowledge ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">6</span></a>a).
Then, it retrieves the relevant captured memories from the structured captured memories and the composite contexts, along with related knowledge from the list of semantic knowledge (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F6" title="Figure 6 ‣ 5.3. Step 3: Inferring Semantic Knowledge ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">6</span></a>b).
The retrieved memories and knowledge, along with the augmented query, are then sent to an LLM to generate a comprehensive answer (Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F6" title="Figure 6 ‣ 5.3. Step 3: Inferring Semantic Knowledge ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">6</span></a>c).
The detailed implementation of each step is discussed below.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1. </span>Query Augmentation</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">As mentioned in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS3" title="3.3. Data Summary and analysis ‣ 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">3.3</span></a>, most user queries tend to be hybrid in nature or require contextual information.
This means that directly searching based solely on the content of captured memories often results in an incomplete or insufficient retrieval of relevant memories.
To enhance the retrieval process, OmniQuery adopts the query refinement concept from NLP <cite class="ltx_cite ltx_citemacro_citep">(Chan et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib10" title="">2024</a>)</cite> to augment the queries.
The query augmentation process involves
</p>
<ol class="ltx_enumerate" id="S6.I1">
<li class="ltx_item" id="S6.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(1)</span>
<div class="ltx_para" id="S6.I1.i1.p1">
<p class="ltx_p" id="S6.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i1.p1.1.1">Rewriting the query</span> to declarative format to improve search accuracy of vector-based similarity matching;</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(2)</span>
<div class="ltx_para" id="S6.I1.i2.p1">
<p class="ltx_p" id="S6.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i2.p1.1.1">Decomposing the query</span> to extract necessary contextual filters, such as time, location, or events, which are grounded in the taxonomy. Note that only explicitly mentioned temporal contexts like “… last week” will be recognized temporal filters. Phrase like “… during CHI 2024” are part of a composite context, thus not counted as a temporal filter;</p>
</div>
</li>
<li class="ltx_item" id="S6.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(3)</span>
<div class="ltx_para" id="S6.I1.i3.p1">
<p class="ltx_p" id="S6.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.I1.i3.p1.1.1">Inferring potential related contexts</span> that may not be explicitly mentioned in the query but can enhance the filtering process also grounded in the taxonomy.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1">For example, as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S5.F6" title="Figure 6 ‣ 5.3. Step 3: Inferring Semantic Knowledge ‣ 5. OMNIQUERY: AUGMENTING CAPTURED MEMORIES ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">6</span></a>d-g, the query “<span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.1">What social events did I attend during CHI 2024?</span>” is rewritten into a declarative format of “<span class="ltx_text ltx_font_italic" id="S6.SS1.p2.1.2">The social events I attended during CHI 2024</span>”.
Since “CHI 2024” is explicitly mentioned and identified as a composite context, it is extracted and labeled with the appropriate composite context tag. “Social events” is also extracted and identified as an atomic context (activities).
Additionally, because ”social events” might include various activities like parties, dancing, or casual conversations, and likely involve multiple people present, OmniQuery infers the relevant atomic contexts (people and activities) and annotates them in the corresponding context category.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2. </span>Retrieving Relevant Augmented Memories</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">The decomposed augmented query is used to comprehensively retrieve from the augmented captured memories.
Specifically, the augmented captured memories consist of the structured captured memories (with processed content and annotated atomic contexts), the list of identified composite contexts and the list of semantic knowledge.
OmniQuery uses the decomposed components from the augmented query to perform a multi-source retrieval, pulling related memories from each of these sources. The results are then consolidated into a comprehensive list of relevant memories, which are used to generate an accurate and detailed answer for the user’s query.
</p>
</div>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S6.SS2.SSS0.Px1.2.1">Declarative query</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px1.1.m1.1"><semantics id="S6.SS2.SSS0.Px1.1.m1.1b"><mo id="S6.SS2.SSS0.Px1.1.m1.1.1" stretchy="false" xref="S6.SS2.SSS0.Px1.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS0.Px1.1.m1.1c"><ci id="S6.SS2.SSS0.Px1.1.m1.1.1.cmml" xref="S6.SS2.SSS0.Px1.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS0.Px1.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS0.Px1.1.m1.1e">→</annotation></semantics></math> Semantic knowledge &amp; Processed content</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px1.p1.1">The declarative query is first encoded into text embeddings and used to search both the semantic memories and processed content (caption and visible text) of the captured memories
This initial search step focuses on finding knowledge and memories directly related to the input query, without incorporating additional contextual filters.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S6.SS2.SSS0.Px2.2.1">Decomposed atomic contexts</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px2.1.m1.1"><semantics id="S6.SS2.SSS0.Px2.1.m1.1b"><mo id="S6.SS2.SSS0.Px2.1.m1.1.1" stretchy="false" xref="S6.SS2.SSS0.Px2.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS0.Px2.1.m1.1c"><ci id="S6.SS2.SSS0.Px2.1.m1.1.1.cmml" xref="S6.SS2.SSS0.Px2.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS0.Px2.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS0.Px2.1.m1.1e">→</annotation></semantics></math> Annotated atomic contexts</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px2.p1.1">Each element of the decomposed atomic contexts (both extracted or inferred) is encoded into text embeddings and searched through the corresponding categories in the structured captured memory database.
For example, if the query involves activities like ”party” and ”dancing,” OmniQuery searches for captured memories annotated with similar activities. Any memories that have been annotated with related or similar activities will be retrieved, ensuring that relevant memories are included in the results.
Additionally, <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS0.Px2.p1.1.1">temporal contexts</span> apply a <span class="ltx_text ltx_font_bold" id="S6.SS2.SSS0.Px2.p1.1.2">strict</span> filter, excluding memories outside the specified time frame (e.g., “last month”) from the retrieval process.</p>
</div>
</section>
<section class="ltx_paragraph" id="S6.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">
<span class="ltx_text ltx_font_bold" id="S6.SS2.SSS0.Px3.2.1">Decomposed composite contexts</span> <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S6.SS2.SSS0.Px3.1.m1.1"><semantics id="S6.SS2.SSS0.Px3.1.m1.1b"><mo id="S6.SS2.SSS0.Px3.1.m1.1.1" stretchy="false" xref="S6.SS2.SSS0.Px3.1.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S6.SS2.SSS0.Px3.1.m1.1c"><ci id="S6.SS2.SSS0.Px3.1.m1.1.1.cmml" xref="S6.SS2.SSS0.Px3.1.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.SS2.SSS0.Px3.1.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S6.SS2.SSS0.Px3.1.m1.1e">→</annotation></semantics></math> Identified composite contexts</h5>
<div class="ltx_para" id="S6.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S6.SS2.SSS0.Px3.p1.1">Any composite context decomposed in the augmenting process is also encoded into text embeddings and searched through the list of identified composite contexts.
All captured memories linked to the semantically similar composite contexts are retrieved. This ensures all memories related to the composite contexts are included.
Additionally, OmniQuery leverages an LLM to assess whether a composite context includes temporal constraints. For example, “<span class="ltx_text ltx_font_italic" id="S6.SS2.SSS0.Px3.p1.1.1">… during CHI 2024</span>” implies a strict temporal filter, while <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS0.Px3.p1.1.2">photos related to CHI 2024</span>” does not.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3. </span>Answer Generation</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">The retrieved results is then sent to an LLM to generate the final answer.
Specifically, the input for the LLM consists of:
(1) the augmented query,
(2) the retrieved semantic knowledge from the list,
(3) all the retrieved captured memory entries from the annotated database, including both the memory content and its associated contextual annotations.</p>
</div>
<div class="ltx_para" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1">The model analyzes and reasons which captured memories serve as references for the generated answer.
These reference memories are also included in the final answer presented to the user.
To enhance the reasoning process, OmniQuery leverages chain-of-thought prompting <cite class="ltx_cite ltx_citemacro_citep">(Wei et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib60" title="">2023</a>)</cite>, ensuring the generation is more accurate and contextually rich.
For specific prompts, please refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A1.SS3" title="A.3. Generating Answers Based on Retrieved Results ‣ Appendix A Prompts for LLMs ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">A.3</span></a></p>
</div>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="239" id="S6.F7.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7. </span>Four exemplar results of using OmniQuery to answer hybrid personal memory-related questions.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>RESULTS</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">To demonstrate OmniQuery’s capabilities in answering users’ personal memory-related questions, we showcase four challenging examples of answering hybrid queries as shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S6.F7" title="Figure 7 ‣ 6.3. Answer Generation ‣ 6. OMNIQUERY: QUESTION-ANSWERING SYSTEM ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">7</span></a>.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1. </span>Experiment Setting</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">OmniQuery was tested on the personal data of one of the researchers from the team, consisting of 2,590 images and videos downloaded from the researcher’s smartphone. The data spans from mid-March to mid-August 2024, covering various activities, trips, and events, including a trip to CHI 2024, a summer trip to Hawaii, as well as captured memories of restaurants, fitness logs, and more.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2. </span>Example Walkthrough</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">In <span class="ltx_text ltx_font_bold" id="S7.SS2.p1.1.1">example one</span>, the user asked about the number of cardio sessions completed over the previous two months.
OmniQuery successfully retrieved all relevant memories (photos of the stair master machine taken after each session in April and May) and accurately generated the answer (18 sessions), providing every instance as supporting evidence.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">In <span class="ltx_text ltx_font_bold" id="S7.SS2.p2.1.1">example two</span>, the user sought the name of a Poke restaurant they visited during CHI 2024 in Honolulu.
They remembered taking a photo of the meal but forgot the restaurant’s name. OmniQuery successfully retrieved the relevant memories based on the query and provided the correct answer.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">In <span class="ltx_text ltx_font_bold" id="S7.SS2.p3.1.1">example three</span>, the user asked about the hotel they stayed at during CHI.
Although no explicit memories indicate the stay at the Prince Hotel, OmniQuery inferred the hotel from metadata associated with the photos taken at the venue, such as a nighttime photo of a broken takeout bag and a morning view of the marina.</p>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1">In <span class="ltx_text ltx_font_bold" id="S7.SS2.p4.1.1">example four</span>, the user asked about the length of stay during their “second” visit to Hawaii.
OmniQuery was able to identify the second trip (the first was the CHI trip) and accurately count the number of stay during the visit.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8. </span>USER EVALUATION</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We conducted a self-guided user evaluation to let participants install and use OmniQuery on their local machine with their local album data. This is to protect participants’ personal data including sensitive and personal identifiable information. In this section, we discuss the detailed process and the evaluation result.</p>
</div>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1. </span>Participants</h3>
<div class="ltx_para" id="S8.SS1.p1">
<p class="ltx_p" id="S8.SS1.p1.2">We recruited 10 participants, including seven from our diary study and three additional participants via word-of-mouth. We recruited participants who have basic programming skills so that they were able to install OmniQuery from source on their local machine. They also consented to the whole process, including that their filtered personal data will be processed via an API service. All the 10 participants (4 male, 6 female, age range = 22 to 29, <math alttext="age_{mean}" class="ltx_Math" display="inline" id="S8.SS1.p1.1.m1.1"><semantics id="S8.SS1.p1.1.m1.1a"><mrow id="S8.SS1.p1.1.m1.1.1" xref="S8.SS1.p1.1.m1.1.1.cmml"><mi id="S8.SS1.p1.1.m1.1.1.2" xref="S8.SS1.p1.1.m1.1.1.2.cmml">a</mi><mo id="S8.SS1.p1.1.m1.1.1.1" xref="S8.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><mi id="S8.SS1.p1.1.m1.1.1.3" xref="S8.SS1.p1.1.m1.1.1.3.cmml">g</mi><mo id="S8.SS1.p1.1.m1.1.1.1a" xref="S8.SS1.p1.1.m1.1.1.1.cmml">⁢</mo><msub id="S8.SS1.p1.1.m1.1.1.4" xref="S8.SS1.p1.1.m1.1.1.4.cmml"><mi id="S8.SS1.p1.1.m1.1.1.4.2" xref="S8.SS1.p1.1.m1.1.1.4.2.cmml">e</mi><mrow id="S8.SS1.p1.1.m1.1.1.4.3" xref="S8.SS1.p1.1.m1.1.1.4.3.cmml"><mi id="S8.SS1.p1.1.m1.1.1.4.3.2" xref="S8.SS1.p1.1.m1.1.1.4.3.2.cmml">m</mi><mo id="S8.SS1.p1.1.m1.1.1.4.3.1" xref="S8.SS1.p1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S8.SS1.p1.1.m1.1.1.4.3.3" xref="S8.SS1.p1.1.m1.1.1.4.3.3.cmml">e</mi><mo id="S8.SS1.p1.1.m1.1.1.4.3.1a" xref="S8.SS1.p1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S8.SS1.p1.1.m1.1.1.4.3.4" xref="S8.SS1.p1.1.m1.1.1.4.3.4.cmml">a</mi><mo id="S8.SS1.p1.1.m1.1.1.4.3.1b" xref="S8.SS1.p1.1.m1.1.1.4.3.1.cmml">⁢</mo><mi id="S8.SS1.p1.1.m1.1.1.4.3.5" xref="S8.SS1.p1.1.m1.1.1.4.3.5.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S8.SS1.p1.1.m1.1b"><apply id="S8.SS1.p1.1.m1.1.1.cmml" xref="S8.SS1.p1.1.m1.1.1"><times id="S8.SS1.p1.1.m1.1.1.1.cmml" xref="S8.SS1.p1.1.m1.1.1.1"></times><ci id="S8.SS1.p1.1.m1.1.1.2.cmml" xref="S8.SS1.p1.1.m1.1.1.2">𝑎</ci><ci id="S8.SS1.p1.1.m1.1.1.3.cmml" xref="S8.SS1.p1.1.m1.1.1.3">𝑔</ci><apply id="S8.SS1.p1.1.m1.1.1.4.cmml" xref="S8.SS1.p1.1.m1.1.1.4"><csymbol cd="ambiguous" id="S8.SS1.p1.1.m1.1.1.4.1.cmml" xref="S8.SS1.p1.1.m1.1.1.4">subscript</csymbol><ci id="S8.SS1.p1.1.m1.1.1.4.2.cmml" xref="S8.SS1.p1.1.m1.1.1.4.2">𝑒</ci><apply id="S8.SS1.p1.1.m1.1.1.4.3.cmml" xref="S8.SS1.p1.1.m1.1.1.4.3"><times id="S8.SS1.p1.1.m1.1.1.4.3.1.cmml" xref="S8.SS1.p1.1.m1.1.1.4.3.1"></times><ci id="S8.SS1.p1.1.m1.1.1.4.3.2.cmml" xref="S8.SS1.p1.1.m1.1.1.4.3.2">𝑚</ci><ci id="S8.SS1.p1.1.m1.1.1.4.3.3.cmml" xref="S8.SS1.p1.1.m1.1.1.4.3.3">𝑒</ci><ci id="S8.SS1.p1.1.m1.1.1.4.3.4.cmml" xref="S8.SS1.p1.1.m1.1.1.4.3.4">𝑎</ci><ci id="S8.SS1.p1.1.m1.1.1.4.3.5.cmml" xref="S8.SS1.p1.1.m1.1.1.4.3.5">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p1.1.m1.1c">age_{mean}</annotation><annotation encoding="application/x-llamapun" id="S8.SS1.p1.1.m1.1d">italic_a italic_g italic_e start_POSTSUBSCRIPT italic_m italic_e italic_a italic_n end_POSTSUBSCRIPT</annotation></semantics></math> = 25.3, <math alttext="age_{SD}" class="ltx_Math" display="inline" id="S8.SS1.p1.2.m2.1"><semantics id="S8.SS1.p1.2.m2.1a"><mrow id="S8.SS1.p1.2.m2.1.1" xref="S8.SS1.p1.2.m2.1.1.cmml"><mi id="S8.SS1.p1.2.m2.1.1.2" xref="S8.SS1.p1.2.m2.1.1.2.cmml">a</mi><mo id="S8.SS1.p1.2.m2.1.1.1" xref="S8.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><mi id="S8.SS1.p1.2.m2.1.1.3" xref="S8.SS1.p1.2.m2.1.1.3.cmml">g</mi><mo id="S8.SS1.p1.2.m2.1.1.1a" xref="S8.SS1.p1.2.m2.1.1.1.cmml">⁢</mo><msub id="S8.SS1.p1.2.m2.1.1.4" xref="S8.SS1.p1.2.m2.1.1.4.cmml"><mi id="S8.SS1.p1.2.m2.1.1.4.2" xref="S8.SS1.p1.2.m2.1.1.4.2.cmml">e</mi><mrow id="S8.SS1.p1.2.m2.1.1.4.3" xref="S8.SS1.p1.2.m2.1.1.4.3.cmml"><mi id="S8.SS1.p1.2.m2.1.1.4.3.2" xref="S8.SS1.p1.2.m2.1.1.4.3.2.cmml">S</mi><mo id="S8.SS1.p1.2.m2.1.1.4.3.1" xref="S8.SS1.p1.2.m2.1.1.4.3.1.cmml">⁢</mo><mi id="S8.SS1.p1.2.m2.1.1.4.3.3" xref="S8.SS1.p1.2.m2.1.1.4.3.3.cmml">D</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S8.SS1.p1.2.m2.1b"><apply id="S8.SS1.p1.2.m2.1.1.cmml" xref="S8.SS1.p1.2.m2.1.1"><times id="S8.SS1.p1.2.m2.1.1.1.cmml" xref="S8.SS1.p1.2.m2.1.1.1"></times><ci id="S8.SS1.p1.2.m2.1.1.2.cmml" xref="S8.SS1.p1.2.m2.1.1.2">𝑎</ci><ci id="S8.SS1.p1.2.m2.1.1.3.cmml" xref="S8.SS1.p1.2.m2.1.1.3">𝑔</ci><apply id="S8.SS1.p1.2.m2.1.1.4.cmml" xref="S8.SS1.p1.2.m2.1.1.4"><csymbol cd="ambiguous" id="S8.SS1.p1.2.m2.1.1.4.1.cmml" xref="S8.SS1.p1.2.m2.1.1.4">subscript</csymbol><ci id="S8.SS1.p1.2.m2.1.1.4.2.cmml" xref="S8.SS1.p1.2.m2.1.1.4.2">𝑒</ci><apply id="S8.SS1.p1.2.m2.1.1.4.3.cmml" xref="S8.SS1.p1.2.m2.1.1.4.3"><times id="S8.SS1.p1.2.m2.1.1.4.3.1.cmml" xref="S8.SS1.p1.2.m2.1.1.4.3.1"></times><ci id="S8.SS1.p1.2.m2.1.1.4.3.2.cmml" xref="S8.SS1.p1.2.m2.1.1.4.3.2">𝑆</ci><ci id="S8.SS1.p1.2.m2.1.1.4.3.3.cmml" xref="S8.SS1.p1.2.m2.1.1.4.3.3">𝐷</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S8.SS1.p1.2.m2.1c">age_{SD}</annotation><annotation encoding="application/x-llamapun" id="S8.SS1.p1.2.m2.1d">italic_a italic_g italic_e start_POSTSUBSCRIPT italic_S italic_D end_POSTSUBSCRIPT</annotation></semantics></math> = 2.63) were fluent or native English speaker.
Participants rated their frequency of logging their daily lives as
‘Only record essential information’ (1), ‘Regularly log important events and memorable experiences’ (5) and ‘Actively log my daily life’ (4).
Each participant is compensated with $50 for completing this study.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2. </span>Apparatus</h3>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p" id="S8.SS2.p1.1">Two different systems were implemented in the user evaluation: the OmniQuery pipeline and a baseline system for comparison. The baseline system leverages a conventional RAG architecture, retrieving related memories based solely on vector similarities between the query and the description of the memory.
The same base language model and prompt structure as OmniQuery are then used to generate the answer. For detailed implementation of the baseline, please refer to Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A2" title="Appendix B Baseline Implementation ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para" id="S8.SS2.p2">
<p class="ltx_p" id="S8.SS2.p2.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.F8" title="Figure 8 ‣ 8.2. Apparatus ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">8</span></a>, in the studies, participants were presented with a single text input box similar to search engine input boxes. After they typed in the question, they would see two answers generated by the two systems in a randomized order. Two rating questions on the answer’s accuracy and completeness were then shown under each answer, from a scale of 1-5.</p>
</div>
<figure class="ltx_figure" id="S8.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="386" id="S8.F8.g1" src="x8.png" width="829"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8. </span>User interface used in the user evaluation.</figcaption>
</figure>
<figure class="ltx_table" id="S8.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 2. </span>Quantitative Results of OmniQuery and Baseline, including UPA, UPC, and Accuracy (%)
</figcaption>
<table class="ltx_tabular ltx_align_middle" id="S8.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S8.T2.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.2.1.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_t" colspan="3" id="S8.T2.1.2.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S8.T2.1.2.1.2.1">OmniQuery</span></td>
<td class="ltx_td ltx_align_center ltx_align_middle ltx_border_t" colspan="3" id="S8.T2.1.2.1.3" style="padding-top:1.5pt;padding-bottom:1.5pt;">Baseline</td>
</tr>
<tr class="ltx_tr" id="S8.T2.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T2.1.3.2.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.3.2.1.1">
<span class="ltx_p" id="S8.T2.1.3.2.1.1.1">Metrics</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.3.2.2" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.3.2.2.1">
<span class="ltx_p" id="S8.T2.1.3.2.2.1.1">UPA</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.3.2.3" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.3.2.3.1">
<span class="ltx_p" id="S8.T2.1.3.2.3.1.1">UPC</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.3.2.4" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.3.2.4.1">
<span class="ltx_p" id="S8.T2.1.3.2.4.1.1">ACC</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.3.2.5" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.3.2.5.1">
<span class="ltx_p" id="S8.T2.1.3.2.5.1.1">UPA</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.3.2.6" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.3.2.6.1">
<span class="ltx_p" id="S8.T2.1.3.2.6.1.1">UPC</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.3.2.7" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.3.2.7.1">
<span class="ltx_p" id="S8.T2.1.3.2.7.1.1">ACC</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S8.T2.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.4.3.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.4.3.1.1">
<span class="ltx_p" id="S8.T2.1.4.3.1.1.1">Direct content query (24)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.4.3.2" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.4.3.2.1">
<span class="ltx_p" id="S8.T2.1.4.3.2.1.1">4.42</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.4.3.3" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.4.3.3.1">
<span class="ltx_p" id="S8.T2.1.4.3.3.1.1">4.13</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.4.3.4" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.4.3.4.1">
<span class="ltx_p" id="S8.T2.1.4.3.4.1.1">83.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.4.3.5" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.4.3.5.1">
<span class="ltx_p" id="S8.T2.1.4.3.5.1.1">3.67</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.4.3.6" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.4.3.6.1">
<span class="ltx_p" id="S8.T2.1.4.3.6.1.1">3.46</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.4.3.7" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.4.3.7.1">
<span class="ltx_p" id="S8.T2.1.4.3.7.1.1">62.5</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S8.T2.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T2.1.5.4.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.5.4.1.1">
<span class="ltx_p" id="S8.T2.1.5.4.1.1.1">Contextual filter + Hybrid (113)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T2.1.5.4.2" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.5.4.2.1">
<span class="ltx_p" id="S8.T2.1.5.4.2.1.1">3.89</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T2.1.5.4.3" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.5.4.3.1">
<span class="ltx_p" id="S8.T2.1.5.4.3.1.1">3.85</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T2.1.5.4.4" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.5.4.4.1">
<span class="ltx_p" id="S8.T2.1.5.4.4.1.1">69.0</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T2.1.5.4.5" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.5.4.5.1">
<span class="ltx_p" id="S8.T2.1.5.4.5.1.1">2.93</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T2.1.5.4.6" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.5.4.6.1">
<span class="ltx_p" id="S8.T2.1.5.4.6.1.1">2.83</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T2.1.5.4.7" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.5.4.7.1">
<span class="ltx_p" id="S8.T2.1.5.4.7.1.1">38.9</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S8.T2.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.6.5.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.6.5.1.1">
<span class="ltx_p" id="S8.T2.1.6.5.1.1.1"><span class="ltx_text ltx_font_bold" id="S8.T2.1.6.5.1.1.1.1">All</span> (137)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.6.5.2" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.6.5.2.1">
<span class="ltx_p" id="S8.T2.1.6.5.2.1.1"><span class="ltx_text ltx_font_bold" id="S8.T2.1.6.5.2.1.1.1">3.98</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.6.5.3" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.6.5.3.1">
<span class="ltx_p" id="S8.T2.1.6.5.3.1.1"><span class="ltx_text ltx_font_bold" id="S8.T2.1.6.5.3.1.1.1">3.90</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.6.5.4" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.6.5.4.1">
<span class="ltx_p" id="S8.T2.1.6.5.4.1.1"><span class="ltx_text ltx_font_bold" id="S8.T2.1.6.5.4.1.1.1">71.5</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.6.5.5" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.6.5.5.1">
<span class="ltx_p" id="S8.T2.1.6.5.5.1.1">3.06</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.6.5.6" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.6.5.6.1">
<span class="ltx_p" id="S8.T2.1.6.5.6.1.1">2.94</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T2.1.6.5.7" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T2.1.6.5.7.1">
<span class="ltx_p" id="S8.T2.1.6.5.7.1.1">43.1</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S8.T2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" colspan="5" id="S8.T2.1.1.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text" id="S8.T2.1.1.1.1" style="font-size:80%;">*ACC refers to accuracy, which is considered accurate when UPA <math alttext="\geq" class="ltx_Math" display="inline" id="S8.T2.1.1.1.1.m1.1"><semantics id="S8.T2.1.1.1.1.m1.1a"><mo id="S8.T2.1.1.1.1.m1.1.1" xref="S8.T2.1.1.1.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S8.T2.1.1.1.1.m1.1b"><geq id="S8.T2.1.1.1.1.m1.1.1.cmml" xref="S8.T2.1.1.1.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S8.T2.1.1.1.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S8.T2.1.1.1.1.m1.1d">≥</annotation></semantics></math> 4 (mostly correct).</span></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S8.T2.1.1.2" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_tt" id="S8.T2.1.1.3" style="width:22.8pt;padding-top:1.5pt;padding-bottom:1.5pt;"></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S8.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3. </span>Direct comparison between OmniQuery and baseline
</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S8.T3.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S8.T3.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S8.T3.1.1.1.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;"></th>
<th class="ltx_td ltx_align_center ltx_align_middle ltx_th ltx_th_column ltx_border_t" colspan="4" id="S8.T3.1.1.1.2" style="padding-top:1.5pt;padding-bottom:1.5pt;"><span class="ltx_text ltx_font_bold" id="S8.T3.1.1.1.2.1">Comparison Win Rate (%)</span></th>
</tr>
<tr class="ltx_tr" id="S8.T3.1.2.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_th_row" id="S8.T3.1.2.2.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.2.2.1.1">
<span class="ltx_p" id="S8.T3.1.2.2.1.1.1">Winner</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S8.T3.1.2.2.2" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.2.2.2.1">
<span class="ltx_p" id="S8.T3.1.2.2.2.1.1"><span class="ltx_text ltx_font_bold" id="S8.T3.1.2.2.2.1.1.1">OmniQuery</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S8.T3.1.2.2.3" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.2.2.3.1">
<span class="ltx_p" id="S8.T3.1.2.2.3.1.1">Baseline</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S8.T3.1.2.2.4" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.2.2.4.1">
<span class="ltx_p" id="S8.T3.1.2.2.4.1.1">Tie</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_column ltx_border_t" id="S8.T3.1.2.2.5" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.2.2.5.1">
<span class="ltx_p" id="S8.T3.1.2.2.5.1.1">Both are bad</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S8.T3.1.3.1">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_t" id="S8.T3.1.3.1.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.3.1.1.1">
<span class="ltx_p" id="S8.T3.1.3.1.1.1.1">Direct content query (24)</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T3.1.3.1.2" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.3.1.2.1">
<span class="ltx_p" id="S8.T3.1.3.1.2.1.1">50.0</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T3.1.3.1.3" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.3.1.3.1">
<span class="ltx_p" id="S8.T3.1.3.1.3.1.1">8.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T3.1.3.1.4" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.3.1.4.1">
<span class="ltx_p" id="S8.T3.1.3.1.4.1.1">33.3</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_t" id="S8.T3.1.3.1.5" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.3.1.5.1">
<span class="ltx_p" id="S8.T3.1.3.1.5.1.1">8.3</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S8.T3.1.4.2">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row" id="S8.T3.1.4.2.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.4.2.1.1">
<span class="ltx_p" id="S8.T3.1.4.2.1.1.1">Contextual filter + Hybrid (113)</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T3.1.4.2.2" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.4.2.2.1">
<span class="ltx_p" id="S8.T3.1.4.2.2.1.1">53.1</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T3.1.4.2.3" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.4.2.3.1">
<span class="ltx_p" id="S8.T3.1.4.2.3.1.1">11.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T3.1.4.2.4" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.4.2.4.1">
<span class="ltx_p" id="S8.T3.1.4.2.4.1.1">19.5</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle" id="S8.T3.1.4.2.5" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.4.2.5.1">
<span class="ltx_p" id="S8.T3.1.4.2.5.1.1">15.9</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S8.T3.1.5.3">
<th class="ltx_td ltx_align_justify ltx_align_middle ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="S8.T3.1.5.3.1" style="width:113.8pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.5.3.1.1">
<span class="ltx_p" id="S8.T3.1.5.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S8.T3.1.5.3.1.1.1.1">All</span> (137)</span>
</span>
</th>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S8.T3.1.5.3.2" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.5.3.2.1">
<span class="ltx_p" id="S8.T3.1.5.3.2.1.1"><span class="ltx_text ltx_font_bold" id="S8.T3.1.5.3.2.1.1.1">52.6</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S8.T3.1.5.3.3" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.5.3.3.1">
<span class="ltx_p" id="S8.T3.1.5.3.3.1.1">10.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S8.T3.1.5.3.4" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.5.3.4.1">
<span class="ltx_p" id="S8.T3.1.5.3.4.1.1">21.9</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_middle ltx_border_bb ltx_border_t" id="S8.T3.1.5.3.5" style="width:51.2pt;padding-top:1.5pt;padding-bottom:1.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S8.T3.1.5.3.5.1">
<span class="ltx_p" id="S8.T3.1.5.3.5.1.1">14.6</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3. </span>Procedure</h3>
<div class="ltx_para" id="S8.SS3.p1">
<p class="ltx_p" id="S8.SS3.p1.1">The user evaluation involved three stages: (1) system setup, (2) data preparation, and (3) the main testing session using user queries.</p>
</div>
<section class="ltx_paragraph" id="S8.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">System setup</h5>
<div class="ltx_para" id="S8.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S8.SS3.SSS0.Px1.p1.1">Participants were given the source code for OmniQuery to install the back-end and a web application on their local machine.
They had the option of an online walkthrough session with the researchers or following the setup instructions on a self-guided manner.</p>
</div>
</section>
<section class="ltx_paragraph" id="S8.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Data preparation</h5>
<div class="ltx_para" id="S8.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S8.SS3.SSS0.Px2.p1.1">Participants were asked to transfer a set of captured memories (photos and videos) from their smartphone’s album to their laptops since OmniQuery was installed on their local laptops. They were also asked to manually review and filter out any content deemed sensitive or preferred to be excluded from the study.
After this filtering process, a total of 1,000 photos and videos were selected for data preprocessing.</p>
</div>
<div class="ltx_para" id="S8.SS3.SSS0.Px2.p2">
<p class="ltx_p" id="S8.SS3.SSS0.Px2.p2.1">Depending on how frequently participants logged their daily lives, the 1,000 selected photos and videos spanned anywhere from <span class="ltx_text ltx_font_italic" id="S8.SS3.SSS0.Px2.p2.1.1">one to four months</span> of past memories.
These captured memories were then fed into OmniQuery for the query-agnostic augmentation process.
The safety of the process data is ensured following the API’s privacy protocol<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>https://openai.com/policies/privacy-policy/</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S8.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_font_bold ltx_title_paragraph">Main session</h5>
<div class="ltx_para" id="S8.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S8.SS3.SSS0.Px3.p1.1">The main session lasts 45 minutes, in which participants tested OmniQuery using two types of questions: (1) questions logged during the diary study and (2) questions they generated during the session.
Participants checked on the diary study questions manually to determine if they could be answered using the filtered set of data on captured memories. Additionally, participants were encouraged to brainstorm and use new questions that were potentially answerable using the filtered data to comprehensively test OmniQuery.</p>
</div>
<div class="ltx_para" id="S8.SS3.SSS0.Px3.p2">
<p class="ltx_p" id="S8.SS3.SSS0.Px3.p2.1">In the question-answering procedure, OmniQuery-generated answers are accompanied with answers generated from the baseline system implemented using RAG.
Each system generated answers anonymously, and participants compared and rated the results for both systems.
The answers and user ratings were recorded for quantitative analysis.
Throughout the process, participants were asked to think aloud <cite class="ltx_cite ltx_citemacro_citep">(Nielsen, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib49" title="">1994</a>)</cite>, and a brief interview was conducted at the end of the session to gather feedback and suggestions.
These results were recorded for qualitative analysis.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S8.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.4. </span>Comparison Metrics</h3>
<div class="ltx_para" id="S8.SS4.p1">
<p class="ltx_p" id="S8.SS4.p1.1">After two answers were presented for a question, participants were asked to rate the two answers.
We used the Chatbot Arena evaluation method <cite class="ltx_cite ltx_citemacro_citep">(Chiang et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib15" title="">2024</a>)</cite>, where participants compared answers from two systems and selected the better one or marked it as a tie.
More specifically, for each question, participants rated the <span class="ltx_text ltx_font_bold" id="S8.SS4.p1.1.1">user perceived accuracy</span> (UPA) and <span class="ltx_text ltx_font_bold" id="S8.SS4.p1.1.2">user perceived completeness</span> (UPC) of the answers from both systems.</p>
</div>
<div class="ltx_para" id="S8.SS4.p2">
<p class="ltx_p" id="S8.SS4.p2.1">The UPA score was rated on a scale from 1 to 5:
<span class="ltx_text ltx_font_bold" id="S8.SS4.p2.1.1">1</span>: Completely wrong or invalid result;
<span class="ltx_text ltx_font_bold" id="S8.SS4.p2.1.2">2</span>: Incorrect, but provides at least some insight that helps answer or further refine the question;
<span class="ltx_text ltx_font_bold" id="S8.SS4.p2.1.3">3</span>: Partially correct, or contains a subset of correct answers (e.g., only listing one meal when asked about all meals eaten last week);
<span class="ltx_text ltx_font_bold" id="S8.SS4.p2.1.4">4</span>: Mostly correct, but missing some minor details (e.g., missing one subway trip when asked how many times I rode the subway);
and <span class="ltx_text ltx_font_bold" id="S8.SS4.p2.1.5">5</span>: Completely correct.
The UPC score, on the other hand, focused on the completeness and credibility of the given answers. In most cases, the questions asked by participants were challenging and needed to be explained and powered by evidence from the captured memory data.
For example, when asked questions which have numeric answers like “how many meals did I have during my last New York trip,” the systems could be right on the numeric answer but wrong at which meals were counted. In those cases, participants examined the answers by looking back at the filtered data, and gave ratings on how they feel about the completeness and credibility of each answer.</p>
</div>
<div class="ltx_para" id="S8.SS4.p3">
<p class="ltx_p" id="S8.SS4.p3.1">We also directly compare the ratings of OmniQuery and the baseline to determine if one can outperform the other.
In the comparison,
if both systems have a UPA of 2 or lower (incorrect), the result is labeled ”both are bad.” If at least one system provides a better-than-”partially correct” answer (<math alttext="\geq" class="ltx_Math" display="inline" id="S8.SS4.p3.1.m1.1"><semantics id="S8.SS4.p3.1.m1.1a"><mo id="S8.SS4.p3.1.m1.1.1" xref="S8.SS4.p3.1.m1.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S8.SS4.p3.1.m1.1b"><geq id="S8.SS4.p3.1.m1.1.1.cmml" xref="S8.SS4.p3.1.m1.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S8.SS4.p3.1.m1.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S8.SS4.p3.1.m1.1d">≥</annotation></semantics></math>3), the system with the higher UPA is considered the winner. In cases where both systems have the same UPA, the one with the higher UPC is the winner. Otherwise, it is a tie.</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.5. </span>Quantitative Result</h3>
<div class="ltx_para" id="S8.SS5.p1">
<p class="ltx_p" id="S8.SS5.p1.1">Participants tested 137 queries in total during the main session.
Among them, 28 were previously logged during the diary study.
We manually labeled each tested query using the categorization and definition mentioned in section <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS3" title="3.3. Data Summary and analysis ‣ 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">3.3</span></a>.
As a result, 24 were categorized as <span class="ltx_text ltx_font_italic" id="S8.SS5.p1.1.1">direct content query</span> while 17 were <span class="ltx_text ltx_font_italic" id="S8.SS5.p1.1.2">contextual filters</span> and 96 were <span class="ltx_text ltx_font_italic" id="S8.SS5.p1.1.3">hybrid queries</span>.
We analyzed the performance metrics of both systems (OmniQuery and baseline) using the scores rated by the participants.
Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.T2" title="Table 2 ‣ 8.2. Apparatus ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.T3" title="Table 3 ‣ 8.2. Apparatus ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a> summarize our results.
In addition to presenting the average UPA and UPC scores, we calculated binary accuracy to evaluate whether the systems provided mostly correct answers. An answer was considered accurate if its UPA score was equal to or greater than 4 (mostly correct) (Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.T2" title="Table 2 ‣ 8.2. Apparatus ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">2</span></a>).
We also present the “comparison result” in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.T3" title="Table 3 ‣ 8.2. Apparatus ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">3</span></a>, which compares the two systems head-to-head on answering personal questions.</p>
</div>
<div class="ltx_para" id="S8.SS5.p2">
<p class="ltx_p" id="S8.SS5.p2.1">The result shows that overall, OmniQuery outperforms the baseline system in both the accuracy and completeness.
Specifically, OmniQuery achieves an accuracy of 71.5%, outperforming the baseline by 28.4%, winning the comparison 52.6% of the time, and tying 21.9% of the time, while in 14.6% of the time, both results are bad.
We also present the results for different categories of queries.
The results indicate that simpler techniques like the baseline handle direct content queries reasonably well (62.5
% accuracy, and winning or tying 41.6% of the time).
While the baseline struggles with more complex queries such as contextual filters or hybrid queries (38.9% accuracy, winning or tying 31.0% of the time), OmniQuery demonstrates it capabilities in effectively handling such queries (69.0% accuracy, winning or tying 72.6% of the time).
</p>
</div>
</section>
<section class="ltx_subsection" id="S8.SS6">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.6. </span>Qualitative Feedback and Findings</h3>
<div class="ltx_para" id="S8.SS6.p1">
<p class="ltx_p" id="S8.SS6.p1.1">Apart from quantitative analysis, we also present qualitative feedback and findings from the think-aloud protocol and the exit interview on the usage of OmniQuery and suggestions of such intelligent question-answering system.</p>
</div>
<section class="ltx_subsubsection" id="S8.SS6.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.6.1. </span>Comparing with Existing Tools</h4>
<div class="ltx_para" id="S8.SS6.SSS1.p1">
<p class="ltx_p" id="S8.SS6.SSS1.p1.1">All participants have tried searching objects in their smartphone albums in their daily lives. They reported that they used the search features mostly to find a specific piece of information, like driver license, SSN number on the card, etc., matching the first type of questions (Direct Content Query) listed in section <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS3" title="3.3. Data Summary and analysis ‣ 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">3.3</span></a>. They would also search for a specific event, like a trip to a specific location, matching the second type of questions (Contextual Filter). However, these searches are usually limited to retrieving a clear and specific object that users were looking for. They could not handle more complex questions like were collected in our diary studies. Plus, some of our participants also anticipated for this to happen because they “know what can be searched and what cannot be searched” from these existing album search tools (P2).</p>
</div>
<div class="ltx_para" id="S8.SS6.SSS1.p2">
<p class="ltx_p" id="S8.SS6.SSS1.p2.1">In the studies, a lot more challenging questions were asked. For <span class="ltx_text ltx_font_bold" id="S8.SS6.SSS1.p2.1.1">Direct Content Queries</span>, it would be challenging to answer when the object is ambiguous or when the users can only describe the object and do not know the exact name of it. For <span class="ltx_text ltx_font_bold" id="S8.SS6.SSS1.p2.1.2">Contextual Filter</span> and <span class="ltx_text ltx_font_bold" id="S8.SS6.SSS1.p2.1.3">Hybrid</span> or even more open-ended and subjective questions, existing searching tools are not comparable to OmniQuery and the baseline at all because tools like iOS album search only return specific photos and videos in a whole without contextual understanding or filtering. In other words, these tools are all not “intelligent enough” to our participants.</p>
</div>
<div class="ltx_para" id="S8.SS6.SSS1.p3">
<p class="ltx_p" id="S8.SS6.SSS1.p3.1">In this subsection, we further summarize the cases that are hard to be accomplished using existing tools. In comparison to the high-level question types provided in section <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S3.SS3" title="3.3. Data Summary and analysis ‣ 3. DIARY STUDY ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">3.3</span></a>, we dived deeper into what these questions in the study were about, and provided detailed examples of these cases.</p>
</div>
<div class="ltx_para" id="S8.SS6.SSS1.p4">
<ul class="ltx_itemize" id="S8.I1">
<li class="ltx_item" id="S8.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i1.p1">
<p class="ltx_p" id="S8.I1.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i1.p1.1.1">Exploratory Search</span>: When users know some characteristics of what they are searching for but cannot specify the exact object. For instance, P1 asked, “What churches did I visit in Barcelona?”</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i2.p1">
<p class="ltx_p" id="S8.I1.i2.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i2.p1.1.1">Look up and Locate</span>: When users know specific references or attributes about an item, such as date, location, or a person in the photo, and want to quickly locate the relevant media. For example, P4 asked, “Can you find the photo of me on a flyer on Instagram?”</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i3.p1">
<p class="ltx_p" id="S8.I1.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i3.p1.1.1">Summarization Tasks</span>: Participants often need answers that summarize their collection of media, rather than finding a single item. For instance, P7 queried, “Which subway stations in New York have art installations?”.</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i4.p1">
<p class="ltx_p" id="S8.I1.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i4.p1.1.1">Comparative Questions</span>: Users sometimes want to compare different sets of media.
For example, P10 asked, “Am I enjoying beach time more or hiking more?”</p>
</div>
</li>
<li class="ltx_item" id="S8.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S8.I1.i5.p1">
<p class="ltx_p" id="S8.I1.i5.p1.1"><span class="ltx_text ltx_font_bold" id="S8.I1.i5.p1.1.1">Open-ended and Subjective Questions</span>: Participants also asked questions that require interpretation or subjective judgment, which were even more challenging for existing tools.
For example, P5 asked, “Given the photos I took, could you analyze what kind of person I am?”</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S8.SS6.SSS1.p5">
<p class="ltx_p" id="S8.SS6.SSS1.p5.1">In the meantime, we want to emphasize again that the comparison between OmniQuery and existing tools is conceptual, given that they serve different purposes and are designed differently in retrieving objects or answering questions. We provide this conceptual comparison to demonstrate the variety of questions OmniQuery can support answering.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS6.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.6.2. </span>Reaction to Answers</h4>
<div class="ltx_para" id="S8.SS6.SSS2.p1">
<p class="ltx_p" id="S8.SS6.SSS2.p1.1">Participants reacted differently to different types of answers. They provided their observations as well as perceived feelings on the answers. Note that since participants weren’t aware of which answer was provided by OmniQuery or the baseline RAG system, we present their feedback on the overall answer structure, given that both OmniQuery and the baseline system provided a similar structure of answer and supporting materials.</p>
</div>
<div class="ltx_para" id="S8.SS6.SSS2.p2">
<p class="ltx_p" id="S8.SS6.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S8.SS6.SSS2.p2.1.1">Dynamics of Detail and Concise Answer.</span> P7 mentioned that they did not always prefer to have a detailed answer composing all related media materials. More reference media would reduce the credibility of the answer to them. P8 also pointed out that if the answer just included all media it could find without a clear and concise connection to the answer, he “might just use iOS album search to get all photos containing [a specific object] and look by myself.”</p>
</div>
<div class="ltx_para" id="S8.SS6.SSS2.p3">
<p class="ltx_p" id="S8.SS6.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S8.SS6.SSS2.p3.1.1">Failure Cases of Answers.</span> We also present cases when participants reacted negatively to the answers.
All participants encountered cases where the answers are inaccurate.
Some were incomplete (e.g., P1 believed that they visited mroe than a few churches on the trip to Barcelona, but answers provided only two of them). Some were presumptive (e.g., P7 asked about recent social events, where the answers gave a piece of memory on a museum visit and explained that visiting museums is “likely with other people.” However, P7 visited the museum alone). Some were making mistakes (e.g., P7 asked for the mostly visited attractions but both system mistakenly answered a museum, which was because P7 took a lot of museum pictures and both systems failed to recognize that they were the same visit.) Some even more challenging questions that caused failure of both systems include questions relate to a specific person. For example, P8 asked about her significant other and P5 asked about their “Korean friend” met in a trip. These cases represent the difficulty of understanding the nuances of personal relationships with personal album data.
</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS6.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.6.3. </span>Iterative Editing of Questions</h4>
<div class="ltx_para" id="S8.SS6.SSS3.p1">
<p class="ltx_p" id="S8.SS6.SSS3.p1.1">Another recurring theme in our study is that six participants mentioned the possibility of iterating on the questions based on the answers. It occurred when participants were uncertain of what answers they were gonna get out of open-ended or subjective answers. They wanted to iterate on the questions based on the given answers, which gave them more understanding on the questions they wanted to ask. For example, P3 asked about how many places they visited during summer, and, based on the answer, realized they were more interested in the number of cities rather than the countries. It highlighted the potential need of a chat interface of personal captured memory data, which posit more challenges in this domain considering the query histories.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9. </span>DISCUSSION</h2>
<div class="ltx_para" id="S9.p1">
<p class="ltx_p" id="S9.p1.1">In this section, we draw on implications from our studies to discuss limitations and propose future work based on what we found.</p>
</div>
<section class="ltx_subsection" id="S9.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1. </span>From Chat Interface to Multimodal Interactions</h3>
<div class="ltx_para" id="S9.SS1.p1">
<p class="ltx_p" id="S9.SS1.p1.1">As a system designed to answer users’ questions on their personal captured memory, OmniQuery is currently designed in an ask-and-react manner for the purpose of evaluating its efficacy in a lab-study settings. In our studies, participants were excited about what OmniQuery was capable of, and gave feedback on having more multimodal interactions rather than a “ChatGPT-like” interface. We recognize the potential of a more interactive OmniQuery in the following ways:</p>
</div>
<div class="ltx_para" id="S9.SS1.p2">
<p class="ltx_p" id="S9.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S9.SS1.p2.1.1">Multimodal Input and Output:</span>
Just like how ChatGPT iterated, OmniQuery could use more multimodal input including but not limited to audio, image, and even videos in the future. Recalling the summarized task cases which can be hard for existing album searching tools to accomplish, a number of these cases can inherently benefit from multimodal inputs. For example, users might want to look up and locate an oddly-shaped cup that they cannot refer to in plain text, or compare all existing dresses in album with a reference color on an image that is hard to describe. Natural language description’s limitation needs to be addressed by introducing different input modalities. Besides input, OmniQuery also has huge potential in helping users relive their memories by visualizing their captured memory data. Users can access their captured memories in an interactive way, inspect and even modify annotations in a transparent way. With that, a “mind palace” style AI assistant becomes more possible.</p>
</div>
<div class="ltx_para" id="S9.SS1.p3">
<p class="ltx_p" id="S9.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S9.SS1.p3.1.1">Error correction:</span>
In our studies, we observed the importance of allowing users to review and access the identified composite contexts and semantic knowledge. Participants expressed the need to correct errors within the system when it retrieved incorrect or irrelevant information. For example, P9 asked about a specific KPop store, and while the system successfully generated relevant memories, it mistakenly included an Instagram screenshot of a Korean TV show. In such cases, participants wanted the ability to mark these irrelevant results as incorrect, suggesting that incorporating error correction mechanisms would enhance the system’s overall performance and accuracy in future interactions.</p>
</div>
<div class="ltx_para" id="S9.SS1.p4">
<p class="ltx_p" id="S9.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S9.SS1.p4.1.1">Follow-up queries:</span>
Another recurring theme in our study is that participants frequently desired the ability to refine their queries or ask follow-up questions. Six out of ten participants mentioned the need to clarify answers or iteratively narrow down their queries. For instance, P4 and P8 emphasized the importance of follow-up questions to hone in on specific information after receiving initial responses. Together with proposed multimodal interactions above, an iterative interaction model, similar to a chatbot workflow, would significantly improve usability and flexibility of OmniQuery.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2. </span>Enriching Memory Data and Visual Intelligence</h3>
<div class="ltx_para" id="S9.SS2.p1">
<p class="ltx_p" id="S9.SS2.p1.1">At present, OmniQuery primarily processes media from a smartphone’s photo album as its main source for captured memories. However, these media alone provide a limited view of a user’s broader personal knowledge. For example, in one of the study’s failure cases, OmniQuery struggled to infer personal relationships from social interactions captured in group photos. To enhance memory augmentation and improve retrieval accuracy, expanding OmniQuery’s data sources and visual intelligence is essential.</p>
</div>
<div class="ltx_para" id="S9.SS2.p2">
<p class="ltx_p" id="S9.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S9.SS2.p2.1.1">Integrating additional data sources:</span>
Personal knowledge extends beyond photo albums and exists across various applications. While our participants’ photo albums included screenshots of emails, calendar events, and chat histories, these represent only a fraction of the broader personal information available in other communication and social interaction apps. Incorporating data from such sources could significantly enhance OmniQuery’s contextual understanding, allowing for more complex queries and richer memory retrieval. However, integrating these additional data sources presents substantial privacy and ethical challenges. While our evaluations were conducted entirely on users’ local machines and did not explore privacy-preserving implementations in detail, existing research efforts, such as those focused on differential privacy and on-device machine learning, offer promising directions for secure and privacy-aware deployment. Additionally, commercial tools like Apple Intelligence’s private cloud computing serve as examples of ongoing progress in protecting user data while enabling advanced memory retrieval capabilities.</p>
</div>
<div class="ltx_para" id="S9.SS2.p3">
<p class="ltx_p" id="S9.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S9.SS2.p3.1.1">Enhancing visual intelligence:</span>
As discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#S8.SS6.SSS2" title="8.6.2. Reaction to Answers ‣ 8.6. Qualitative Feedback and Findings ‣ 8. USER EVALUATION ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">8.6.2</span></a>, questions related to social interactions remain challenging due to the current lack of advanced features like facial recognition for person identification. Future iterations of OmniQuery could integrate such capabilities (with appropriate user consent), enabling the system to track individuals across various memories. This enhancement would support new use cases, such as monitoring social patterns or tracking progress over time, significantly improving the system’s capacity for memory augmentation and retrieval. Additionally, we propose exploring the design and implementation of a comprehensive taxonomy of personal knowledge domains. This would allow users to selectively activate or deactivate specific domains, such as enabling “Social Interactions and Relationships” to infer personal connections while disabling “Personally Identifiable Information” to prevent the system from processing sensitive data like IDs or SSN numbers in photos. This modular approach could enhance both user control and privacy.</p>
</div>
<div class="ltx_para" id="S9.SS2.p4">
<p class="ltx_p" id="S9.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S9.SS2.p4.1.1">Augmenting with future AR technologies:</span>
A limitation of personal memory capture is the potential for missed moments when users either forget or are unable to document an experience. As AR technology advances, OmniQuery’s memory augmentation and retrieval capabilities could be seamlessly integrated into AR systems, allowing for more passive and context-aware memory capture. AR devices could leverage real-time contextual triggers <cite class="ltx_cite ltx_citemacro_citep">(Li et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib37" title="">2024b</a>)</cite> to proactively surface relevant memories or information, offering proactive assistance in pervasive AR environments. This integration would enhance the user experience by making memory retrieval more intuitive and contextually relevant. However, such passive data capture raises even more significant privacy concerns, which will require future research into secure, privacy-preserving implementations to ensure the responsible use of AI in these settings.</p>
</div>
</section>
<section class="ltx_subsection" id="S9.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.3. </span>Preserving Privacy</h3>
<div class="ltx_para" id="S9.SS3.p1">
<p class="ltx_p" id="S9.SS3.p1.1">As was discussed in above subsections, protecting users’ privacy is crucial in developing future personal AI assistants, including but not limited to handling personal data such as media in albums, chat histories or browsing history. Users have limited control over how their data is handled and must rely on service providers’ adherence to privacy protocols. In this subsection, we take a step further to discuss more robust and rigorous measures should be adopted in real-world settings, where the amount of personal data is huge, making approaches like manual filtering in OmniQuery’s evaluation infeasible.</p>
</div>
<div class="ltx_para" id="S9.SS3.p2">
<p class="ltx_p" id="S9.SS3.p2.1">One way is to incorporate more advanced data protection techniques on cloud servers including data anonymization <cite class="ltx_cite ltx_citemacro_citep">(Majeed and Lee, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib43" title="">2021</a>)</cite> and encryption <cite class="ltx_cite ltx_citemacro_citep">(Nadeem and Javed, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib48" title="">2005</a>)</cite>, while preserving the computational capabilities of large models via online computing.
The other approach is leveraging on-device computing, where all data processing occurs locally on the user’s device, ensuring full control over users’ own data.
Recent advances in model compression <cite class="ltx_cite ltx_citemacro_citep">(Hohman et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib26" title="">2023</a>)</cite> have made it possible to run large model on smaller devices like smartphones.
As OmniQuery is designed to be model-agnostic, it is able to work with different model sizes.
While smaller, compressed on-device model may result in reduced performance, future work should focus on developing curated datasets and benchmarks to rigorously evaluate OmniQuery ’s performance across different model sizes (e.g., LLaMAs <cite class="ltx_cite ltx_citemacro_citep">(Touvron et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib56" title="">2023</a>)</cite> and Phi-3 <cite class="ltx_cite ltx_citemacro_citep">(Abdin et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib2" title="">2024</a>)</cite>). This would provide a deeper understanding of how model size impacts both privacy and system effectiveness.
concern is that users do not have direct control over their own data when leveraging powerful AI models via online servers.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10. </span>CONCLUSION</h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">In this paper, we present OmniQuery, which enhances personal question answering on captured memories.
To inform the design of OmniQuery, we ran a one-month diary study to collect and analyze realistic user queries, which were then used to generate a taxonomy of contextual information, including atomic context, composite context and semantic knowledge.
The taxonomy guided the design of an augmentation pipeline of captured memories, which involves structuring individual captured memories, and processing multiple captured memories using sliding windows to identify composite contexts and semantic knowledge.</p>
</div>
<div class="ltx_para" id="S10.p2">
<p class="ltx_p" id="S10.p2.1">We then developed a personal question-answering system, which first augment the user query, then retrieve the related captured memories, and finally using the retrieved memories to generate comprehensive answers using an LLM.
We evaluated OmniQuery against a baseline system with a user evaluation of 10 participants who tested 137 queries in total.
The results show that OmniQuery is effective in answering users’ queries with an accuracy of 71.5%, and winning or tying 74.5% of the time in direct comparison with the baseline.
We also performed qualitative analysis on participants’ reaction and feedback during the evaluation, with the findings demonstrating potential of OmniQuery and providing valuable insights into possible enhancements for future systems alike.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">(1)</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdin et al<span class="ltx_text" id="bib.bib2.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Marah Abdin, Jyoti Aneja, Hany Awadalla, Ahmed Awadallah, Ammar Ahmad Awan, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Jianmin Bao, Harkirat Behl, Alon Benhaim, Misha Bilenko, Johan Bjorck, Sébastien Bubeck, Martin Cai, Qin Cai, Vishrav Chaudhary, Dong Chen, Dongdong Chen, Weizhu Chen, Yen-Chun Chen, Yi-Ling Chen, Hao Cheng, Parul Chopra, Xiyang Dai, Matthew Dixon, Ronen Eldan, Victor Fragoso, Jianfeng Gao, Mei Gao, Min Gao,
Amit Garg, Allie Del Giorno, Abhishek Goswami, Suriya Gunasekar, Emman Haider, Junheng Hao, Russell J. Hewett, Wenxiang Hu, Jamie Huynh, Dan Iter, Sam Ade Jacobs, Mojan Javaheripi, Xin Jin, Nikos Karampatziakis, Piero Kauffmann, Mahoud Khademi, Dongwoo Kim, Young Jin Kim, Lev Kurilenko, James R. Lee, Yin Tat Lee, Yuanzhi Li, Yunsheng Li, Chen Liang, Lars Liden, Xihui Lin, Zeqi Lin, Ce Liu, Liyuan Liu, Mengchen Liu, Weishung Liu, Xiaodong Liu,
Chong Luo, Piyush Madan, Ali Mahmoudzadeh, David Majercak, Matt Mazzola, Caio César Teodoro Mendes, Arindam Mitra, Hardik Modi, Anh Nguyen, Brandon Norick, Barun Patra, Daniel Perez-Becker, Thomas Portet, Reid Pryzant, Heyang Qin, Marko Radmilac, Liliang Ren, Gustavo de Rosa, Corby Rosset, Sambudha Roy, Olatunji Ruwase, Olli Saarikivi, Amin Saied, Adil Salim, Michael Santacroce, Shital Shah, Ning Shang, Hiteshi Sharma, Yelong Shen, Swadheen Shukla, Xia Song,
Masahiro Tanaka, Andrea Tupini, Praneetha Vaddamanu, Chunyu Wang, Guanhua Wang, Lijuan Wang, Shuohang Wang, Xin Wang, Yu Wang, Rachel Ward, Wen Wen, Philipp Witte, Haiping Wu, Xiaoxia Wu, Michael Wyatt, Bin Xiao, Can Xu, Jiahang Xu, Weijian Xu, Jilong Xue, Sonali Yadav, Fan Yang, Jianwei Yang, Yifan Yang, Ziyi Yang, Donghan Yu, Lu Yuan, Chenruidong Zhang, Cyril Zhang, Jianwen Zhang, Li Lyna Zhang, Yi Zhang, Yue Zhang,
Yunan Zhang, and Xiren Zhou. 2024.

</span>
<span class="ltx_bibblock">Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.14219 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.14219" title="">https://arxiv.org/abs/2404.14219</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal et al<span class="ltx_text" id="bib.bib3.2.2.1">.</span> (2015)</span>
<span class="ltx_bibblock">
Aishwarya Agrawal, Jiasen Lu, Stanislaw Antol, Margaret Mitchell, C. Lawrence Zitnick, Devi Parikh, and Dhruv Batra. 2015.

</span>
<span class="ltx_bibblock">VQA: Visual Question Answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.3.1">International Journal of Computer Vision</em> 123 (2015), 4 – 31.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:3180429" title="">https://api.semanticscholar.org/CorpusID:3180429</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Asai et al<span class="ltx_text" id="bib.bib4.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023.

</span>
<span class="ltx_bibblock">Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2310.11511 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2310.11511" title="">https://arxiv.org/abs/2310.11511</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Beltagy et al<span class="ltx_text" id="bib.bib5.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Iz Beltagy, Matthew E Peters, and Arman Cohan. 2020.

</span>
<span class="ltx_bibblock">Longformer: The long-document transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.3.1">arXiv preprint arXiv:2004.05150</em> (2020).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bonnail et al<span class="ltx_text" id="bib.bib6.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Elise Bonnail, Wen-Jie Tseng, Mark Mcgill, Éric Lecolinet, Samuel Huron, and Jan Gugenheimer. 2023.

</span>
<span class="ltx_bibblock">Memory Manipulations in Extended Reality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.3.1">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:257952236" title="">https://api.semanticscholar.org/CorpusID:257952236</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brandt et al<span class="ltx_text" id="bib.bib7.2.2.1">.</span> (2007)</span>
<span class="ltx_bibblock">
Joel Brandt, Noah Weiss, and Scott R. Klemmer. 2007.

</span>
<span class="ltx_bibblock">txt 4 l8r: lowering the burden for diary studies under mobile conditions. In <em class="ltx_emph ltx_font_italic" id="bib.bib7.3.1">CHI ’07 Extended Abstracts on Human Factors in Computing Systems</em> (San Jose, CA, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib7.4.2">(CHI EA ’07)</em>. Association for Computing Machinery, New York, NY, USA, 2303–2308.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1240866.1240998" title="">https://doi.org/10.1145/1240866.1240998</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Brown et al<span class="ltx_text" id="bib.bib8.2.2.1">.</span> (2020)</span>
<span class="ltx_bibblock">
Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,
and Dario Amodei. 2020.

</span>
<span class="ltx_bibblock">Language Models are Few-Shot Learners.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2005.14165 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.14165" title="">https://arxiv.org/abs/2005.14165</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caprani et al<span class="ltx_text" id="bib.bib9.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Niamh Caprani, John Greaney, and Nicola Porter. 2006.

</span>
<span class="ltx_bibblock">A Review of Memory Aid Devices for an Ageing Population.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.3.1">PsychNology J.</em> 4 (2006), 205–243.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:9598075" title="">https://api.semanticscholar.org/CorpusID:9598075</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al<span class="ltx_text" id="bib.bib10.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Chi-Min Chan, Chunpu Xu, Ruibin Yuan, Hongyin Luo, Wei Xue, Yike Guo, and Jie Fu. 2024.

</span>
<span class="ltx_bibblock">RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.00610 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.00610" title="">https://arxiv.org/abs/2404.00610</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan (2020)</span>
<span class="ltx_bibblock">
Samantha WT Chan. 2020.

</span>
<span class="ltx_bibblock">Biosignal-Sensitive Memory Improvement and Support Systems. In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems</em>. 1–7.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan (2022)</span>
<span class="ltx_bibblock">
Wei Ting Samantha Chan. 2022.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Augmenting Human Prospective Memory through Cognition-Aware Technologies</em>.

</span>
<span class="ltx_bibblock">Ph. D. Dissertation. ResearchSpace@ Auckland.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib13.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017.

</span>
<span class="ltx_bibblock">Reading Wikipedia to Answer Open-Domain Questions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.3.1">ArXiv</em> abs/1704.00051 (2017).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:3618568" title="">https://api.semanticscholar.org/CorpusID:3618568</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al<span class="ltx_text" id="bib.bib14.2.2.1">.</span> (2022)</span>
<span class="ltx_bibblock">
Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. 2022.

</span>
<span class="ltx_bibblock">MuRAG: Multimodal Retrieval-Augmented Generator for Open Question Answering over Images and Text. In <em class="ltx_emph ltx_font_italic" id="bib.bib14.3.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:252735160" title="">https://api.semanticscholar.org/CorpusID:252735160</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chiang et al<span class="ltx_text" id="bib.bib15.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph E. Gonzalez, and Ion Stoica. 2024.

</span>
<span class="ltx_bibblock">Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2403.04132 [cs.AI]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.04132" title="">https://arxiv.org/abs/2403.04132</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubourg et al<span class="ltx_text" id="bib.bib16.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Lydia Dubourg, Ana Rita Silva, Christophe Fitamen, Chris J. A. Moulin, and Céline Souchay. 2016.

</span>
<span class="ltx_bibblock">SenseCam: A new tool for memory rehabilitation?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.3.1">Revue neurologique</em> 172 12 (2016), 735–747.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:9803824" title="">https://api.semanticscholar.org/CorpusID:9803824</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Edge et al<span class="ltx_text" id="bib.bib17.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Darren Edge, Ha Trinh, Newman Cheng, Joshua Bradley, Alex Chao, Apurva Mody, Steven Truitt, and Jonathan Larson. 2024.

</span>
<span class="ltx_bibblock">From Local to Global: A Graph RAG Approach to Query-Focused Summarization.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2404.16130 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2404.16130" title="">https://arxiv.org/abs/2404.16130</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Engel et al<span class="ltx_text" id="bib.bib18.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jakob Engel, Kiran Somasundaram, Michael Goesele, Albert Sun, Alexander Gamino, Andrew Turner, Arjang Talattof, Arnie Yuan, Bilal Souti, Brighid Meredith, Cheng Peng, Chris Sweeney, Cole Wilson, Dan Barnes, Daniel DeTone, David Caruso, Derek Valleroy, Dinesh Ginjupalli, Duncan Frost, Edward Miller, Elias Mueggler, Evgeniy Oleinik, Fan Zhang, Guruprasad Somasundaram, Gustavo Solaira, Harry Lanaras, Henry Howard-Jenkins, Huixuan Tang, Hyo Jin Kim, Jaime Rivera,
Ji Luo, Jing Dong, Julian Straub, Kevin Bailey, Kevin Eckenhoff, Lingni Ma, Luis Pesqueira, Mark Schwesinger, Maurizio Monge, Nan Yang, Nick Charron, Nikhil Raina, Omkar Parkhi, Peter Borschowa, Pierre Moulon, Prince Gupta, Raul Mur-Artal, Robbie Pennington, Sachin Kulkarni, Sagar Miglani, Santosh Gondi, Saransh Solanki, Sean Diener, Shangyi Cheng, Simon Green, Steve Saarinen, Suvam Patra, Tassos Mourikis, Thomas Whelan, Tripti Singh, Vasileios Balntas,
Vijay Baiyya, Wilson Dreewes, Xiaqing Pan, Yang Lou, Yipu Zhao, Yusuf Mansour, Yuyang Zou, Zhaoyang Lv, Zijian Wang, Mingfei Yan, Carl Ren, Renzo De Nardi, and Richard Newcombe. 2023.

</span>
<span class="ltx_bibblock">Project Aria: A New Tool for Egocentric Multi-Modal AI Research.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.13561 [cs.HC]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.13561" title="">https://arxiv.org/abs/2308.13561</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fan et al<span class="ltx_text" id="bib.bib19.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Yue Fan, Xiaojian Ma, Rujie Wu, Yuntao Du, Jiaqi Li, Zhi Gao, and Qing Li. 2024.

</span>
<span class="ltx_bibblock">VideoAgent: A Memory-augmented Multimodal Agent for Video Understanding.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.3.1">arXiv preprint arXiv:2403.11481</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al<span class="ltx_text" id="bib.bib20.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, and Haofen Wang. 2023.

</span>
<span class="ltx_bibblock">Retrieval-augmented generation for large language models: A survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.3.1">arXiv preprint arXiv:2312.10997</em> (2023).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al<span class="ltx_text" id="bib.bib21.2.2.1">.</span> (2016)</span>
<span class="ltx_bibblock">
Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2016.

</span>
<span class="ltx_bibblock">Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.3.1">International Journal of Computer Vision</em> 127 (2016), 398 – 414.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:8081284" title="">https://api.semanticscholar.org/CorpusID:8081284</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grauman et al<span class="ltx_text" id="bib.bib22.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Martin, Tushar Nagarajan, Ilija Radosavovic, Santhosh K. Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray, Mengmeng Xu, Eric Z. Xu, Chen Zhao, Siddhant Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichtenhofer, Adriano Fragomeni, Qichen Fu,
Christian Fuegen, Abrham Kahsay Gebreselasie, Cristina González, James M. Hillis, Xuhua Huang, Yifei Huang, Wenqi Jia, Weslie Khoo, Jáchym Kolár, Satwik Kottur, Anurag Kumar, Federico Landini, Chao Li, Yanghao Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu, Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari, Kiran K. Somasundaram, Audrey Southerland, Yusuke Sugano, Ruijie Tao, Minh Vo, Yuchen Wang,
Xindi Wu, Takuma Yagi, Yunyi Zhu, Pablo Arbeláez, David J. Crandall, Dima Damen, Giovanni Maria Farinella, Bernard Ghanem, Vamsi Krishna Ithapu, C. V. Jawahar, Hanbyul Joo, Kris Kitani, Haizhou Li, Richard A. Newcombe, Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato, Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo Torresani, Mingfei Yan, and Jitendra Malik. 2021.

</span>
<span class="ltx_bibblock">Ego4D: Around the World in 3,000 Hours of Egocentric Video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.3.1">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</em> (2021), 18973–18990.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:238856888" title="">https://api.semanticscholar.org/CorpusID:238856888</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gurrin et al<span class="ltx_text" id="bib.bib23.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Cathal Gurrin, Alan F Smeaton, Aiden R Doherty, et al<span class="ltx_text" id="bib.bib23.3.1">.</span> 2014.

</span>
<span class="ltx_bibblock">Lifelogging: Personal big data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.4.1">Foundations and Trends® in information retrieval</em> 8, 1 (2014), 1–125.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hayes et al<span class="ltx_text" id="bib.bib24.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
Gillian R. Hayes, Shwetak N. Patel, Khai Nhut Truong, Giovanni Iachello, Julie A. Kientz, Rob Farmer, and Gregory D. Abowd. 2004.

</span>
<span class="ltx_bibblock">The Personal Audio Loop: Designing a Ubiquitous Audio-Based Memory Aid. In <em class="ltx_emph ltx_font_italic" id="bib.bib24.3.1">Mobile HCI</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:11316625" title="">https://api.semanticscholar.org/CorpusID:11316625</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hodges et al<span class="ltx_text" id="bib.bib25.2.2.1">.</span> (2006)</span>
<span class="ltx_bibblock">
Steve Hodges, Lyndsay Williams, Emma Berry, Shahram Izadi, James Srinivasan, Alex Butler, Gavin Smyth, Narinder Kapur, and Ken Wood. 2006.

</span>
<span class="ltx_bibblock">SenseCam: A retrospective memory aid. In <em class="ltx_emph ltx_font_italic" id="bib.bib25.3.1">UbiComp 2006: Ubiquitous Computing: 8th International Conference, UbiComp 2006 Orange County, CA, USA, September 17-21, 2006 Proceedings 8</em>. Springer, 177–193.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hohman et al<span class="ltx_text" id="bib.bib26.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Fred Hohman, Mary Beth Kery, Donghao Ren, and Dominik Moritz. 2023.

</span>
<span class="ltx_bibblock">Model Compression in Practice: Lessons Learned from Practitioners Creating On-device Machine Learning Experiences.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:263829166" title="">https://api.semanticscholar.org/CorpusID:263829166</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Horvitz (1999)</span>
<span class="ltx_bibblock">
Eric Horvitz. 1999.

</span>
<span class="ltx_bibblock">Principles of mixed-initiative user interfaces. In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Proceedings of the SIGCHI conference on Human Factors in Computing Systems</em>. 159–166.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hou et al<span class="ltx_text" id="bib.bib28.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Zhijian Hou, Lei Ji, Difei Gao, Wanjun Zhong, Kun Yan, Chao Li, Wing-Kwong Chan, Chong-Wah Ngo, Nan Duan, and Mike Zheng Shou. 2023.

</span>
<span class="ltx_bibblock">GroundNLQ @ Ego4D ‘Natural Language Queries Challenge 2023.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2306.15255 [cs.CV]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2306.15255" title="">https://arxiv.org/abs/2306.15255</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib29.2.2.1">.</span> (2019a)</span>
<span class="ltx_bibblock">
Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. 2019a.

</span>
<span class="ltx_bibblock">Knowledge graph embedding based question answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib29.3.1">Proceedings of the twelfth ACM international conference on web search and data mining</em>. 105–113.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al<span class="ltx_text" id="bib.bib30.2.2.1">.</span> (2019b)</span>
<span class="ltx_bibblock">
Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. 2019b.

</span>
<span class="ltx_bibblock">Knowledge Graph Embedding Based Question Answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.3.1">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</em> (2019).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:59528287" title="">https://api.semanticscholar.org/CorpusID:59528287</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jaber et al<span class="ltx_text" id="bib.bib31.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Razan Jaber, Sabrina Zhong, Sanna Kuoppamäki, Aida Hosseini, Iona Gessinger, Duncan P Brumby, Benjamin R Cowan, and Donald McMillan. 2024.

</span>
<span class="ltx_bibblock">Cooking With Agents: Designing Context-aware Voice Interaction. In <em class="ltx_emph ltx_font_italic" id="bib.bib31.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>. 1–13.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jamieson et al<span class="ltx_text" id="bib.bib32.2.2.1">.</span> (2014)</span>
<span class="ltx_bibblock">
Matthew Jamieson, Breda Cullen, Marilyn McGee-Lennon, Stephen Brewster, and Jonathan J Evans. 2014.

</span>
<span class="ltx_bibblock">The efficacy of cognitive prosthetic technology for people with memory impairments: A systematic review and meta-analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib32.3.1">Neuropsychological rehabilitation</em> 24, 3-4 (2014), 419–444.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jamieson et al<span class="ltx_text" id="bib.bib33.2.2.1">.</span> (2017)</span>
<span class="ltx_bibblock">
Matthew Jamieson, Brian O’Neill, Breda Cullen, Marilyn Rose McGee-Lennon, Stephen Anthony Brewster, and Jonathan J. Evans. 2017.

</span>
<span class="ltx_bibblock">ForgetMeNot: Active Reminder Entry Support for Adults with Acquired Brain Injury.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.3.1">Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems</em> (2017).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:2298134" title="">https://api.semanticscholar.org/CorpusID:2298134</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al<span class="ltx_text" id="bib.bib34.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Jaewook Lee, Jun Wang, Elizabeth Brown, Liam Chu, Sebastian S. Rodriguez, and Jon E. Froehlich. 2024.

</span>
<span class="ltx_bibblock">GazePointAR: A Context-Aware Multimodal Voice Assistant for Pronoun Disambiguation in Wearable Augmented Reality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:268132284" title="">https://api.semanticscholar.org/CorpusID:268132284</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al<span class="ltx_text" id="bib.bib35.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen tau Yih, Tim Rocktäschel, Sebastian Riedel, and Douwe Kiela. 2021.

</span>
<span class="ltx_bibblock">Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2005.11401 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2005.11401" title="">https://arxiv.org/abs/2005.11401</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib36.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Franklin Mingzhe Li, Michael Xieyang Liu, Shaun K. Kane, and Patrick Carrington. 2024a.

</span>
<span class="ltx_bibblock">A Contextual Inquiry of People with Vision Impairments in Cooking.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:267897983" title="">https://api.semanticscholar.org/CorpusID:267897983</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al<span class="ltx_text" id="bib.bib37.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, and Michelle Li. 2024b.

</span>
<span class="ltx_bibblock">OmniActions: Predicting Digital Actions in Response to Real-World Multimodal Sensory Inputs with LLMs. In <em class="ltx_emph ltx_font_italic" id="bib.bib37.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (Honolulu, HI, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib37.4.2">(CHI ’24)</em>. Association for Computing Machinery, New York, NY, USA, Article 8, 22 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3613904.3642068" title="">https://doi.org/10.1145/3613904.3642068</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib38.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a.

</span>
<span class="ltx_bibblock">Improved Baselines with Visual Instruction Tuning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib39.2.2.1">.</span> (2024b)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. 2024b.

</span>
<span class="ltx_bibblock">LLaVA-NeXT: Improved reasoning, OCR, and world knowledge.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://llava-vl.github.io/blog/2024-01-30-llava-next/" title="">https://llava-vl.github.io/blog/2024-01-30-llava-next/</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib40.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b.

</span>
<span class="ltx_bibblock">Visual Instruction Tuning.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al<span class="ltx_text" id="bib.bib41.2.2.1">.</span> (2024a)</span>
<span class="ltx_bibblock">
Xingyu Bruce Liu, Jiahao Nick Li, David Kim, Xiang ’Anthony’ Chen, and Ruofei Du. 2024a.

</span>
<span class="ltx_bibblock">Human I/O: Towards a Unified Approach to Detecting Situational Impairments.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (2024).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:268203741" title="">https://api.semanticscholar.org/CorpusID:268203741</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Maaz et al<span class="ltx_text" id="bib.bib42.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Muhammad Maaz, Hanoona Abdul Rasheed, Salman H. Khan, and Fahad Shahbaz Khan. 2023.

</span>
<span class="ltx_bibblock">Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models. In <em class="ltx_emph ltx_font_italic" id="bib.bib42.3.1">Annual Meeting of the Association for Computational Linguistics</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:259108333" title="">https://api.semanticscholar.org/CorpusID:259108333</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Majeed and Lee (2021)</span>
<span class="ltx_bibblock">
Abdul Majeed and Sungchang Lee. 2021.

</span>
<span class="ltx_bibblock">Anonymization Techniques for Privacy Preserving Data Publishing: A Comprehensive Survey.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">IEEE Access</em> 9 (2021), 8512–8545.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:231616865" title="">https://api.semanticscholar.org/CorpusID:231616865</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mangalam et al<span class="ltx_text" id="bib.bib44.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Karttikeya Mangalam, Raiymbek Akshulakov, and Jitendra Malik. 2023.

</span>
<span class="ltx_bibblock">EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2308.09126 [cs.CV]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2308.09126" title="">https://arxiv.org/abs/2308.09126</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mann (1996)</span>
<span class="ltx_bibblock">
Steve Mann. 1996.

</span>
<span class="ltx_bibblock">Wearable Tetherless Computer-Mediated Reality: WearCam as a wearable face-recognizer, and other applications for the disabled.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:11838759" title="">https://api.semanticscholar.org/CorpusID:11838759</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mavi et al<span class="ltx_text" id="bib.bib46.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Vaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2024.

</span>
<span class="ltx_bibblock">Multi-hop Question Answering.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2204.09140 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2204.09140" title="">https://arxiv.org/abs/2204.09140</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McRae and Jones (2013)</span>
<span class="ltx_bibblock">
Ken McRae and Michael Jones. 2013.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">14 Semantic Memory</em>. Vol. 206.

</span>
<span class="ltx_bibblock">Oxford University Press Oxford.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nadeem and Javed (2005)</span>
<span class="ltx_bibblock">
Aamer Nadeem and Muhammad Younus Javed. 2005.

</span>
<span class="ltx_bibblock">A Performance Comparison of Data Encryption Algorithms.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">2005 International Conference on Information and Communication Technologies</em> (2005), 84–89.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:14441015" title="">https://api.semanticscholar.org/CorpusID:14441015</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nielsen (1994)</span>
<span class="ltx_bibblock">
Jakob Nielsen. 1994.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Usability engineering</em>.

</span>
<span class="ltx_bibblock">Morgan Kaufmann.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al<span class="ltx_text" id="bib.bib50.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021.

</span>
<span class="ltx_bibblock">Learning Transferable Visual Models From Natural Language Supervision.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2103.00020 [cs.CV]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2103.00020" title="">https://arxiv.org/abs/2103.00020</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sarthi et al<span class="ltx_text" id="bib.bib51.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Parth Sarthi, Salman Abdullah, Aditi Tuli, Shubh Khanna, Anna Goldie, and Christopher D. Manning. 2024.

</span>
<span class="ltx_bibblock">RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2401.18059 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.18059" title="">https://arxiv.org/abs/2401.18059</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sedlakova et al<span class="ltx_text" id="bib.bib52.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jana Sedlakova, Paola Daniore, Andrea Horn Wintsch, Markus Wolf, Mina Stanikic, Christina Haag, Chloé Sieber, Gerold Schneider, Kaspar Staub, Dominik Alois Ettlin, et al<span class="ltx_text" id="bib.bib52.3.1">.</span> 2023.

</span>
<span class="ltx_bibblock">Challenges and best practices for digital unstructured data enrichment in health research: a systematic narrative review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib52.4.1">PLOS Digital Health</em> 2, 10 (2023), e0000347.

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shin et al<span class="ltx_text" id="bib.bib53.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Youngsoo Shin, Ruth Barankevich, Jina Lee, and Saleh Kalantari. 2021.

</span>
<span class="ltx_bibblock">PENCODER: Design for Prospective Memory and Older Adults.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.3.1">Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems</em> (2021).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:233987041" title="">https://api.semanticscholar.org/CorpusID:233987041</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sohn et al<span class="ltx_text" id="bib.bib54.2.2.1">.</span> (2008)</span>
<span class="ltx_bibblock">
Timothy Sohn, Kevin A. Li, William G. Griswold, and James D. Hollan. 2008.

</span>
<span class="ltx_bibblock">A diary study of mobile information needs. In <em class="ltx_emph ltx_font_italic" id="bib.bib54.3.1">Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> (Florence, Italy) <em class="ltx_emph ltx_font_italic" id="bib.bib54.4.2">(CHI ’08)</em>. Association for Computing Machinery, New York, NY, USA, 433–442.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/1357054.1357125" title="">https://doi.org/10.1145/1357054.1357125</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Talmor et al<span class="ltx_text" id="bib.bib55.2.2.1">.</span> (2021)</span>
<span class="ltx_bibblock">
Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hannaneh Hajishirzi, and Jonathan Berant. 2021.

</span>
<span class="ltx_bibblock">MultiModalQA: Complex Question Answering over Text, Tables and Images.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2104.06039 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2104.06039" title="">https://arxiv.org/abs/2104.06039</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al<span class="ltx_text" id="bib.bib56.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023.

</span>
<span class="ltx_bibblock">LLaMA: Open and Efficient Foundation Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2302.13971 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2302.13971" title="">https://arxiv.org/abs/2302.13971</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tulving (2002)</span>
<span class="ltx_bibblock">
Endel Tulving. 2002.

</span>
<span class="ltx_bibblock">Episodic Memory: From Mind to Brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Annual Review of Psychology</em> 53, Volume 53, 2002 (2002), 1–25.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1146/annurev.psych.53.100901.135114" title="">https://doi.org/10.1146/annurev.psych.53.100901.135114</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vemuri et al<span class="ltx_text" id="bib.bib58.2.2.1">.</span> (2004)</span>
<span class="ltx_bibblock">
Sunil Vemuri, Chris Schmandt, Walter Bender, Stefanie Tellex, and Bradford Lassey. 2004.

</span>
<span class="ltx_bibblock">An Audio-Based Personal Memory Aid. In <em class="ltx_emph ltx_font_italic" id="bib.bib58.3.1">Ubiquitous Computing</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:309402" title="">https://api.semanticscholar.org/CorpusID:309402</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al<span class="ltx_text" id="bib.bib59.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Cunxiang Wang, Ruoxi Ning, Boqi Pan, Tonghui Wu, Qipeng Guo, Cheng Deng, Guangsheng Bao, Qian Wang, and Yue Zhang. 2024.

</span>
<span class="ltx_bibblock">Novelqa: A benchmark for long-range novel question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib59.3.1">arXiv preprint arXiv:2403.12766</em> (2024).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wei et al<span class="ltx_text" id="bib.bib60.2.2.1">.</span> (2023)</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023.

</span>
<span class="ltx_bibblock">Chain-of-Thought Prompting Elicits Reasoning in Large Language Models.

</span>
<span class="ltx_bibblock">
</span>
<span class="ltx_bibblock">arXiv:2201.11903 [cs.CL]

<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2201.11903" title="">https://arxiv.org/abs/2201.11903</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib61.2.2.1">.</span> (2003)</span>
<span class="ltx_bibblock">
G. Yang, Lekha Chaisorn, Yunlong Zhao, Shi-Yong Neo, and Tat-Seng Chua. 2003.

</span>
<span class="ltx_bibblock">VideoQA: question answering on news video.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.3.1">Proceedings of the eleventh ACM international conference on Multimedia</em> (2003).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:207716" title="">https://api.semanticscholar.org/CorpusID:207716</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al<span class="ltx_text" id="bib.bib62.2.2.1">.</span> (2018)</span>
<span class="ltx_bibblock">
Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018.

</span>
<span class="ltx_bibblock">HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib62.3.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:52822214" title="">https://api.semanticscholar.org/CorpusID:52822214</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et al<span class="ltx_text" id="bib.bib63.2.2.1">.</span> (2021a)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021a.

</span>
<span class="ltx_bibblock">QA-GNN: Reasoning with language models and knowledge graphs for question answering.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.3.1">arXiv preprint arXiv:2104.06378</em> (2021).

</span>
<span class="ltx_bibblock">
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yasunaga et al<span class="ltx_text" id="bib.bib64.2.2.1">.</span> (2021b)</span>
<span class="ltx_bibblock">
Michihiro Yasunaga, Hongyu Ren, Antoine Bosselut, Percy Liang, and Jure Leskovec. 2021b.

</span>
<span class="ltx_bibblock">QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering. In <em class="ltx_emph ltx_font_italic" id="bib.bib64.3.1">North American Chapter of the Association for Computational Linguistics</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:233219869" title="">https://api.semanticscholar.org/CorpusID:233219869</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib65.2.2.1">.</span> (2023b)</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing. 2023b.

</span>
<span class="ltx_bibblock">Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding. In <em class="ltx_emph ltx_font_italic" id="bib.bib65.3.1">Conference on Empirical Methods in Natural Language Processing</em>.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:259075356" title="">https://api.semanticscholar.org/CorpusID:259075356</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al<span class="ltx_text" id="bib.bib66.2.2.1">.</span> (2023a)</span>
<span class="ltx_bibblock">
Liangfu Zhang, Anwen Hu, Jing Zhang, Shuo Hu, and Qin Jin. 2023a.

</span>
<span class="ltx_bibblock">MPMQA: Multimodal Question Answering on Product Manuals.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.3.1">ArXiv</em> abs/2304.09660 (2023).

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:258212471" title="">https://api.semanticscholar.org/CorpusID:258212471</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zulfikar et al<span class="ltx_text" id="bib.bib67.2.2.1">.</span> (2024)</span>
<span class="ltx_bibblock">
Wazeer Deen Zulfikar, Samantha Chan, and Pattie Maes. 2024.

</span>
<span class="ltx_bibblock">Memoro: Using Large Language Models to Realize a Concise Interface for Real-Time Memory Augmentation. In <em class="ltx_emph ltx_font_italic" id="bib.bib67.3.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em> (Honolulu, HI, USA) <em class="ltx_emph ltx_font_italic" id="bib.bib67.4.2">(CHI ’24)</em>. Association for Computing Machinery, New York, NY, USA, Article 450, 18 pages.

</span>
<span class="ltx_bibblock">
<a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1145/3613904.3642450" title="">https://doi.org/10.1145/3613904.3642450</a>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Prompts for LLMs</h2>
<section class="ltx_subsection" id="A1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.1. </span>Identifying Composite Contexts</h3>
<div class="ltx_para" id="A1.SS1.p1">
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.SS1.p1.1">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,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">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx1">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.1">System</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx1.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx1.3">instruction</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx1.4">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx2">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.5">an</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.7">intelligent</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.9">agent</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.11">capable</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.13">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.15">generating</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.17">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.19">list</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.21">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.23">COMPOSITE</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.25">CONTEXTS</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.27">inferred</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.29">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.31">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.33">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx2.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx2.35">memory</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx2.36">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx3">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.1">Composite</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.3">context</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.5">refers</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.7">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.9">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.11">combination</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.13">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.15">time</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.16">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.18">location</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.19">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.21">people</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.22">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.24">objects</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.25">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.27">environment</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.29">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.31">activities</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.32">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.34">Such</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.36">composite</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.37"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.38">contexts</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.39"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.40">could</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.41"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.42">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.43"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.44">inferred</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.45"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.46">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.47"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.48">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.49"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.50">explicit</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.51"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.52">content</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.53"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.54">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.55">e</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.56">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.57">g</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.58">.,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.59"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.60">text</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.61"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.62">showing</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.63"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.64">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.65"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.66">event</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.67"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.68">info</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.69">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.70"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.71">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.72"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.73">implicit</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.74"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.75">cues</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.76"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.77">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.78">e</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.79">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.80">g</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.81">.,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.82"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.83">multiple</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.84"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.85">changes</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.86"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.87">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.88"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.89">location</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.90"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.91">indicating</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.92"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.93">travel</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.94">).</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.95"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.96">Focus</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.97"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.98">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.99"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.100">relatively</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.101"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.102">important</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.103"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.104">composites</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.105"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.106">such</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.107"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.108">as</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.109"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.110">travel</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.111">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.112"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.113">conferences</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.114">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.115"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.116">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.117"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.118">important</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.119"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.120">meetings</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.121"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.122">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.123"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.124">focus</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.125"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.126">less</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.127"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.128">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.129"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.130">trivial</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx3.131"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx3.132">events</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx3.133">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx4">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.1">For</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.3">each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.5">composite</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.7">context</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.8">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.10">identify</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.12">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.14">related</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.16">episodic</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.18">memory</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.20">ids</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.21">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.23">This</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.25">could</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.27">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.29">due</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.31">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.33">time</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.34"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.35">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.36">e</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.37">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.38">g</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.39">.,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.40"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.41">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.42"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.43">memory</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.44"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.45">occurs</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.46"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.47">during</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.48"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.49">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.51">event</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.52">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.53"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.54">location</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.55"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.56">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.57">e</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.58">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.59">g</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.60">.,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.61"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.62">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.63"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.64">memory</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.65"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.66">takes</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.67"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.68">place</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.69"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.70">at</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.71"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.72">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.73"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.74">event</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.75"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.76">location</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.77">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.78"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.79">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.80"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.81">specific</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.82"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.83">content</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.84"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.85">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.86">e</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.87">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.88">g</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.89">.,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.90"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.91">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.92"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.93">memory</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.94"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.95">mentions</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.96"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.97">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx4.98"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx4.99">event</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx4.100">).</span>
</div>
<div class="ltx_listingline" id="lstnumberx5">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.1">Additionally</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.2">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.4">rate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.6">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.8">importance</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.10">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.12">each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.14">event</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.16">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.18">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.20">scale</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.22">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.23"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.24">1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.26">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.27"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.28">3,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.29"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.30">where</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.31"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.32">3</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.34">denotes</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.36">very</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.37"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.38">major</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.39"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.40">events</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.41"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.42">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.43">e</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.44">.</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.45">g</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.46">.,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.47"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.48">multi</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.49">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.50">day</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.51"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.52">events</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.53"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.54">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.55"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.56">highly</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.57"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.58">important</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.59"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.60">events</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.61">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.62"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.63">2</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.64"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.65">denotes</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.66"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.67">moderately</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.68"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.69">important</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.70"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.71">events</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.72">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.73"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.74">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.75"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.76">1</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.77"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.78">denotes</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.79"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.80">less</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.81"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.82">important</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx5.83"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx5.84">events</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx5.85">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx6">
</div>
<div class="ltx_listingline" id="lstnumberx7">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.1">Exemplar</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.3">composite</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.5">context</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.7">types</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx7.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx7.9">include</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx7.10">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx8">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.1">An</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.3">academic</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.5">conference</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.6">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.7"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.8">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.9">An</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.11">academic</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx8.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx8.13">conference</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx8.14">";</span>
</div>
<div class="ltx_listingline" id="lstnumberx9">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.1">Recreational</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.3">travel</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.4">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.5"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.6">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.7">Trip</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.9">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.11">Salt</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.13">lake</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.15">city</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.16">",</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.17"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.18">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.19">Traveling</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.21">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.23">home</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx9.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx9.25">town</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx9.26">";</span>
</div>
<div class="ltx_listingline" id="lstnumberx10">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.1">Locational</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.3">change</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.4">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.5"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.6">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.7">Location</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.9">changed</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.11">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.13">Seattle</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.15">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx10.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx10.17">Irvine</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx10.18">";</span>
</div>
<div class="ltx_listingline" id="lstnumberx11">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.1">Outdoor</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.3">activities</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.4">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.5"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.6">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.7">Camping</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx11.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx11.9">trip</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx11.10">";</span>
</div>
<div class="ltx_listingline" id="lstnumberx12">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.1">Personal</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.3">milestones</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.4">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.5"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.6">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.7">Birthday</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.9">celebration</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.10">",</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.11"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.12">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.13">Graduation</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.15">ceremony</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.16">",</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.17"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.18">"</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.19">first</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.21">day</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.23">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx12.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx12.25">univeristy</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx12.26">";</span>
</div>
<div class="ltx_listingline" id="lstnumberx13">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx13.1">etc</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx13.2">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx14">
</div>
<div class="ltx_listingline" id="lstnumberx15">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.1">Output</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.5">list</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.7">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.9">composite</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.11">context</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.13">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.15">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.17">JSON</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.19">object</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.21">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.23">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.25">key</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.26"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.27">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.28">composite_context</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.29">’.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.31">Each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.33">event</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.35">should</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.36"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.37">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.38"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.39">represented</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.40"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.41">as</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.42"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.43">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.44"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.45">sub</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.46"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.47">JSON</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.48"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.49">object</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.51">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.52"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.53">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.54"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.55">following</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.56"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.57">keys</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.58">:</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.59"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.60">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.61">event_name</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.62">’</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.63"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.64">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.65">detailed</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.66"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.67">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.68"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.69">concise</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.70">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.71"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.72">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.73">memory_ids</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.74">’</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.75"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.76">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.77">list</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.78">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.79"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.80">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.81">start_date</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.82">’,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.83"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.84">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.85">end_date</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.86">’</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.87"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.88">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.89">could</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.90"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.91">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.92"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.93">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.94"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.95">same</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.96"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.97">as</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.98"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.99">start_date</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.100">),</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.101"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.102">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.103">location</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.104">’,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.105"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.106">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.107">is_multi_days</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.108">’,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.109"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.110">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx15.111"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.112">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx15.113">importance</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx15.114">’.</span>
</div>
<div class="ltx_listingline" id="lstnumberx16">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx16.1">+</span>
</div>
<div class="ltx_listingline" id="lstnumberx17">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx17.1">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.2">List</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.4">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.6">structured</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.8">captured</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx17.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx17.10">memories</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx17.11">&gt;</span>
</div>
</div>
</div>
<figure class="ltx_figure" id="A1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="235" id="A1.F1.g1" src="x9.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Structure of the baseline implementation.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A1.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.2. </span>Inferring Semantic Knowledge</h3>
<div class="ltx_para" id="A1.SS2.p1">
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.SS2.p1.1">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,U3lzdGVtIGluc3RydWN0aW9uOgpZb3UgYXJlIGFuIGludGVsbGlnZW50IGFnZW50IGNhcGFibGUgb2YgZ2VuZXJhdGluZyBhIGxpc3Qgb2YgRkFDVFMgb3IgS05PV0xFREdFIChyZWZlcnJlZCB0byBrbm93bGVkZ2UgaW4gdGhlIGZvbGxvd2luZykgdGhhdCBjYW4gYmUgaW5mZXJyZWQgZnJvbSB0aGUgZ2l2ZW4gbWVtb3J5IGFuZCB0aGUgcmVsYXRlZCBjb21wb3NpdGUgY29udGV4dHMuIEZvY3VzIG9uIHJlbGF0aXZlbHkgaW1wb3J0YW50IGhpZ2gtbGV2ZWwgc2VtYW50aWMga25vd2xlZGdlIGFuZCBmb2N1cyBsZXNzIG9uIHRyaXZpYWwgZXZlbnRzLiBBdm9pZCBzcGVjaWZpYyBkZXRhaWxzIGFib3V0IGluZGl2aWR1YWwgbWVkaWEKVGhlIGtub3dsZWRnZSBzaG91bGQgYmUgZGV0YWlsZWQgYW5kIHNlbGYtY29udGFpbmVkLgpFeGVtcGxhciBzZW1hbnRpYyBrbm93bGVkZ2UgaW5jbHVkZXM6CjxFeGFtcGxlcyBvZiBzZW1hbnRpYyBrbm93bGVkZ2U+CkFsc28gaWRlbnRpZnkgdGhlIG1vc3QgcmVwcmVzZW50YXRpdmUgZXBpc29kaWMgbWVtb3JpZXMgdGhhdCBjb250cmlidXRlIHRvIHRoZSB1bmRlcnN0YW5kaW5nIG9mIHRoZSBrbm93bGVkZ2UuCk91dHB1dCBhIEpTT04gb2JqZWN0IHdpdGggdGhlIGtleSAna25vd2xlZGdlJy4gRWFjaCBrbm93bGVkZ2UgaXRlbSBzaG91bGQgaW5jbHVkZSAna25vd2xlZGdlJywgJ21lbW9yeV9pZHMnIChsaXN0KQoKSW5wdXQ6CjxTdHJ1Y3R1cmVkIGNhcHR1cmVkIG1lbW9yaWVzIGluIHRoZSBzbGlkaW5nIHdpbmRvd3M+CisKPElkZW50aWZpZWQgY29tcG9zaXRlIGNvbnRleHRzIGlkZW50aWZpZWQgaW4gdGhlIHNsaWRpbmcgd2luZG93Pg==">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx18">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.1">System</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx18.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx18.3">instruction</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx18.4">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx19">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.1">You</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.3">are</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.5">an</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.7">intelligent</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.9">agent</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.11">capable</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.13">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.15">generating</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.17">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.19">list</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.21">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.23">FACTS</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.25">or</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.27">KNOWLEDGE</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.28"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.29">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.30">referred</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.31"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.32">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.33"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.34">knowledge</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.35"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.36">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.37"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.38">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.39"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.40">following</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.41">)</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.42"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.43">that</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.44"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.45">can</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.46"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.47">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.48"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.49">inferred</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.50"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.51">from</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.52"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.53">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.54"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.55">given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.56"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.57">memory</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.58"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.59">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.60"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.61">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.62"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.63">related</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.64"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.65">composite</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.66"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.67">contexts</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.68">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.69"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.70">Focus</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.71"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.72">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.73"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.74">relatively</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.75"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.76">important</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.77"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.78">high</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.79">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.80">level</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.81"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.82">semantic</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.83"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.84">knowledge</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.85"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.86">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.87"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.88">focus</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.89"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.90">less</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.91"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.92">on</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.93"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.94">trivial</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.95"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.96">events</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx19.97">.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.98"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.99">Avoid</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.100"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.101">specific</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.102"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.103">details</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.104"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.105">about</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.106"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.107">individual</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx19.108"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx19.109">media</span>
</div>
<div class="ltx_listingline" id="lstnumberx20">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.1">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.3">knowledge</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.5">should</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.7">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.9">detailed</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.11">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx20.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.13">self</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.14">-</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx20.15">contained</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx20.16">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx21">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.1">Exemplar</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.3">semantic</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.5">knowledge</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx21.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx21.7">includes</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx21.8">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx22">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx22.1">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.2">Examples</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.4">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.6">semantic</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx22.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx22.8">knowledge</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx22.9">&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx23">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.1">Also</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.3">identify</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.5">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.7">most</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.9">representative</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.11">episodic</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.13">memories</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.15">that</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.17">contribute</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.19">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.21">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.23">understanding</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.25">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.27">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx23.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx23.29">knowledge</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx23.30">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx24">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.1">Output</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.3">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.5">JSON</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.7">object</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.9">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.11">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.13">key</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.14"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.15">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.16">knowledge</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.17">’.</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.19">Each</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.21">knowledge</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.23">item</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.25">should</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.27">include</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.28"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.29">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.30">knowledge</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.31">’,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.32"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.33">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.34">memory_ids</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.35">’</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx24.36"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.37">(</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx24.38">list</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx24.39">)</span>
</div>
<div class="ltx_listingline" id="lstnumberx25">
</div>
<div class="ltx_listingline" id="lstnumberx26">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx26.1">Input</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx26.2">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx27">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx27.1">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.2">Structured</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.4">captured</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.6">memories</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.8">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.10">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.12">sliding</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx27.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx27.14">windows</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx27.15">&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx28">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx28.1">+</span>
</div>
<div class="ltx_listingline" id="lstnumberx29">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx29.1">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.2">Identified</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.4">composite</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.6">contexts</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.8">identified</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.10">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.12">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.14">sliding</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx29.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx29.16">window</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx29.17">&gt;</span>
</div>
</div>
</div>
</section>
<section class="ltx_subsection" id="A1.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">A.3. </span>Generating Answers Based on Retrieved Results</h3>
<div class="ltx_para" id="A1.SS3.p1">
<div class="ltx_listing ltx_lstlisting ltx_listing" id="A1.SS3.p1.1">
<div class="ltx_listing_data"><a download="" href="data:text/plain;base64,U3lzdGVtIGluc3RydWN0aW9uOgpHaXZlbiBhIHF1ZXJ5LCBhIGxpc3Qgb2YgbWVtb3JpZXMgYW5kIHBlcnNvbmFsIGtub3dsZWRnZSwgZ2VuZXJhdGUgYSBjb21wcmVoZW5zaXZlIGFuc3dlciB0byB0aGUgcXVlcnkuCklkZW50aWZ5IHRoZSBlcGlzb2RpYyBtZW1vcmllcyB0aGF0IGNhbiBwcm92aWRlIGV2aWRlbmNlIHRvIHRoZSBxdWVzdGlvbi4KSWYgdGhlIGFuc3dlciBpcyBub3QgZXhwbGljaXRseSBwcmVzZW50ZWQgaW4gdGhlIG1lbW9yaWVzLCBtYWtlIGEgcmVhc29uYWJsZSBpbmZlcmVuY2UuCk91dHB1dCBhIEpTT04gb2JqZWN0IHdpdGggdGhlIGtleSAnYW5zd2VyJywgJ2V4cGxhbmF0aW9uJyBhbmQgJ21lbW9yeV9pZHMnLgpUaGUgJ2Fuc3dlcicgc2hvdWxkIGJlIGEgc3RyaW5nIGFuZCAnbWVtb3J5X2lkcycgc2hvdWxkIGJlIGEgbGlzdCBvZiBtZW1vcnkgaWRzCgpJbnB1dDoKPFF1ZXJ5PgorCjxSZXRyaWV2ZWQgc2VtYW50aWMga25vd2xlZGdlPgorCjxSZXRyaWV2ZWQgc3RydWN0dXJlZCBrbm93bGVkZ2U+">⬇</a></div>
<div class="ltx_listingline" id="lstnumberx30">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.1">System</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx30.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx30.3">instruction</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx30.4">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx31">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.1">Given</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.3">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.5">query</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.6">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.7"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.8">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.9"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.10">list</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.11"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.12">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.13"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.14">memories</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.15"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.16">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.17"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.18">personal</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.19"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.20">knowledge</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.21">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.23">generate</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.25">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.27">comprehensive</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.29">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.31">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.33">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx31.34"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx31.35">query</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx31.36">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx32">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.1">Identify</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.5">episodic</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.7">memories</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.9">that</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.11">can</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.13">provide</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.15">evidence</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.17">to</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.19">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx32.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx32.21">question</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx32.22">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx33">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.1">If</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.3">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.5">answer</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.7">is</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.9">not</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.11">explicitly</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.13">presented</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.15">in</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.16"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.17">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.18"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.19">memories</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.20">,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.21"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.22">make</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.23"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.24">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.25"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.26">reasonable</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx33.27"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx33.28">inference</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx33.29">.</span>
</div>
<div class="ltx_listingline" id="lstnumberx34">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.1">Output</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.2"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.3">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.4"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.5">JSON</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.7">object</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.9">with</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.11">the</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.13">key</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.14"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.15">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.16">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.17">’,</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.18"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.19">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.20">explanation</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.21">’</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.23">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx34.24"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.25">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx34.26">memory_ids</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx34.27">’.</span>
</div>
<div class="ltx_listingline" id="lstnumberx35">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.1">The</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.2"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.3">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.4">answer</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.5">’</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.6"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.7">should</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.8"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.9">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.10"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.11">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.12"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.13">string</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.14"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.15">and</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.16"> </span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.17">’</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.18">memory_ids</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx35.19">’</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.20"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.21">should</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.22"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.23">be</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.24"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.25">a</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.26"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.27">list</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.28"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.29">of</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.30"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.31">memory</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx35.32"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx35.33">ids</span>
</div>
<div class="ltx_listingline" id="lstnumberx36">
</div>
<div class="ltx_listingline" id="lstnumberx37">
<span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx37.1">Input</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx37.2">:</span>
</div>
<div class="ltx_listingline" id="lstnumberx38">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx38.1">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx38.2">Query</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx38.3">&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx39">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx39.1">+</span>
</div>
<div class="ltx_listingline" id="lstnumberx40">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx40.1">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.2">Retrieved</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.4">semantic</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx40.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx40.6">knowledge</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx40.7">&gt;</span>
</div>
<div class="ltx_listingline" id="lstnumberx41">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx41.1">+</span>
</div>
<div class="ltx_listingline" id="lstnumberx42">
<span class="ltx_text ltx_font_typewriter" id="lstnumberx42.1">&lt;</span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx42.2">Retrieved</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx42.3"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx42.4">structured</span><span class="ltx_text ltx_lst_space ltx_font_typewriter" id="lstnumberx42.5"> </span><span class="ltx_text ltx_lst_identifier ltx_font_typewriter" id="lstnumberx42.6">knowledge</span><span class="ltx_text ltx_font_typewriter" id="lstnumberx42.7">&gt;</span>
</div>
</div>
</div>
</section>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Baseline Implementation</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">While there is no already-existing system designed for answering personal questions on captured memories, we manually designed and implemented a system as the baseline to comare with OmniQuery.</p>
</div>
<div class="ltx_para" id="A2.p2">
<p class="ltx_p" id="A2.p2.1">Similar to OmniQuery, the baseline system also adopts a RAG architecture to adapt to the large number of captured memories.
We utilized the basic structure of RAG illustrated in <cite class="ltx_cite ltx_citemacro_citep">(Gao et al<span class="ltx_text">.</span>, <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#bib.bib20" title="">2023</a>)</cite>, which involves (1) indexing the external data sources with embedding models, (2) leverage vector-based search to retrieve the top K relevant data instances (3) based-on the retrieved data, utilizing a powerful LLM to generate the final answer.
Note that typical RAG systems require a chunking phase, where long documents are split into smaller chunks for more precise matching and retrieval of relevant information.
In our case, each captured memory already represents a limited amount of information and is naturally separated.
Therefore, we treat each captured memory as an individual chunk.</p>
</div>
<div class="ltx_para" id="A2.p3">
<p class="ltx_p" id="A2.p3.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A2" title="Appendix B Baseline Implementation ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">B</span></a><a class="ltx_ref" href="https://arxiv.org/html/2409.08250v1#A1.F1" title="Figure 1 ‣ A.1. Identifying Composite Contexts ‣ Appendix A Prompts for LLMs ‣ OmniQuery: Contextually Augmenting Captured Multimodal Memory to Enable Personal Question Answering"><span class="ltx_text ltx_ref_tag">1</span></a> demonstrates the structure of the baseline system in our experiment.
The baseline also processes the captured memories by leveraging a multimodal model (<span class="ltx_text ltx_font_typewriter" id="A2.p3.1.1">GPT-4o</span>) to generate detailed captions for each memory.
Additionally, it extracts temporal and geographical information from the metadata and processes it in the same manner as OmniQuery.
This ensures that the processed memories include the temporal and geographical data, which are common components in users’ queries.
The temporal and geographical information is concatenated to the generated caption.
Then the concatenated text sequence is encoded into text embeddings using embedding models (<span class="ltx_text ltx_font_typewriter" id="A2.p3.1.2">text-embedding-3-small</span>).</p>
</div>
<div class="ltx_para" id="A2.p4">
<p class="ltx_p" id="A2.p4.1">In the retrieval stage, the query is first encoded into the text embeddings using the same embedding model, and then retrieve the top K (K=50) captured memories using vector-based similarity search.
The retrieved top K captured memories are then ordered in temporal sequence, and then sent to the LLM (<span class="ltx_text ltx_font_typewriter" id="A2.p4.1.1">GPT-4o)</span> for generating the answer.
The prompt used for the answer generation is the same as OmniQuery.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Thu Sep 12 17:48:30 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
