<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images</title>
<!--Generated on Tue Sep 17 20:23:43 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="image segmentation histology axon myelin" lang="en" name="keywords"/>
<base href="/html/2409.11552v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S1" title="In Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S1.SS1" title="In 1 Introduction ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1.1 </span>Contribution</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2" title="In Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS1" title="In 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Data</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS1.SSS1" title="In 2.1 Data ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Datasets Used</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS1.SSS2" title="In 2.1 Data ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Annotations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS1.SSS3" title="In 2.1 Data ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Preprocessing and Data Aggregation Strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS1.SSS4" title="In 2.1 Data ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.4 </span>Data availability</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS2" title="In 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Models</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS2.SSS1" title="In 2.2 Models ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.1 </span>Architecture and Training Details</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS2.SSS2" title="In 2.2 Models ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2.2 </span>Resolution-Ignorance</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS3" title="In 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS3.SSS1" title="In 2.3 Experiments ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.1 </span>Intra-Modality Aggregation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.SS3.SSS2" title="In 2.3 Experiments ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3.2 </span>Inter-Modality Aggregation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S3" title="In Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Results and Discussion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S3.SS1" title="In 3 Results and Discussion ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Intra- and Inter-Modality Aggregation Results</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S3.SS2" title="In 3 Results and Discussion ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Out-of-Distribution Generalization</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S4" title="In Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S4.SS0.SSS1" title="In 4 Conclusion ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.1 </span>Acknowledgements</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S4.SS0.SSS2" title="In 4 Conclusion ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.0.2 </span><span class="ltx_ERROR undefined">\discintname</span></span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line"><span class="ltx_note ltx_role_institutetext" id="id1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_note_type">institutetext: </span>NeuroPoly Lab, Institute of Biomedical Engineering, Polytechnique Montréal, Montréal, Québec, Canada </span></span></span><span class="ltx_note ltx_role_institutetext" id="id2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_note_type">institutetext: </span>Mila - Québec Artificial Intelligence Institute, Montréal, Québec, Canada</span></span></span>
<h1 class="ltx_title ltx_title_document">Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Armand Collin
</span><span class="ltx_author_notes">1122</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Arthur Boschet
</span><span class="ltx_author_notes">22</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Mathieu Boudreau
</span><span class="ltx_author_notes">11</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Julien Cohen-Adad
</span><span class="ltx_author_notes">1122</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Quantifying axon and myelin properties (e.g., axon diameter, myelin thickness, g-ratio) in histology images can provide useful information about microstructural changes caused by neurodegenerative diseases. Automatic tissue segmentation is an important tool for these datasets, as a single stained section can contain up to thousands of axons. Advances in deep learning have made this task quick and reliable with minimal overhead, but a deep learning model trained by one research group will hardly ever be usable by other groups due to differences in their histology training data. This is partly due to subject diversity (different body parts, species, genetics, pathologies) and also to the range of modern microscopy imaging techniques resulting in a wide variability of image features (i.e., contrast, resolution). There is a pressing need to make AI accessible to neuroscience researchers to facilitate and accelerate their workflow, but publicly available models are scarce and poorly maintained. Our approach is to aggregate data from multiple imaging modalities (bright field, electron microscopy, Raman spectroscopy) and species (mouse, rat, rabbit, human), to create an open-source, durable tool for axon and myelin segmentation. Our generalist model makes it easier for researchers to process their data and can be fine-tuned for better performance on specific domains. We study the benefits of different aggregation schemes. This multi-domain segmentation model performs better than single-modality dedicated learners (p=0.03077), generalizes better on out-of-distribution data and is easier to use and maintain. Importantly, we package the segmentation tool into a well-maintained open-source software ecosystem
<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url" href="https://axondeepseg.readthedocs.io/" title="">https://axondeepseg.readthedocs.io/</a></span></span></span>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Keywords: </h6>image segmentation histology axon myelin
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Neurological disorders constitute the most prevalent cause of physical and cognitive disability and the second highest cause of death <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib11" title="">11</a>]</cite>. They are also a major financial burden to society, given the associated medical costs and the reduced years of employment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib26" title="">26</a>]</cite>. Microscopy imaging techniques play an important role to understand neurological diseases. It can notably be used to quantify demyelination and remyelination, which are critically important to assess the efficiency of new drugs.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">To this end, automatic tissue segmentation is required because slices of the brain or the spinal cord, for example, can contain hundreds or thousands of axons. Typical metrics of interest include axon internal area, myelin thickness or g-ratio (ratio between inner and outer axon diameter). Collecting a meaningful amount of data cannot be done manually. As a result, researchers have been using automatic methods for more than a decade. Initial solutions consisted of a combination of thresholding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib29" title="">29</a>]</cite>, contour detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib2" title="">2</a>]</cite>, morphological operations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib27" title="">27</a>]</cite>, watershed algorithms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib2" title="">2</a>]</cite> or active contour models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib27" title="">27</a>]</cite>. These conventional image processing methods were effective but they relied on assumptions about the visual aspect of input images or the typical axon morphometry captured in the data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib22" title="">22</a>]</cite>. These solutions required a meticulous design and were specifically tailored for a data distribution, but typically would not be applicable to other domains (i.e., different histological staining, different microscopy imaging modalities).
Deep learning approaches, more specifically convolutional neural networks (CNNs), gained a lot of popularity due to the improved performance of GPU acceleration in the last decade and large dataset sizes that have become available. These methods now outperform traditional image processing solutions for a lot of medical imaging tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib18" title="">18</a>]</cite>, including axon and myelin segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib28" title="">28</a>]</cite>. Notably, the U-Net architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib24" title="">24</a>]</cite> quickly became a <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">de facto</span> standard for biomedical image segmentation, and is still widely used in the field <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib14" title="">14</a>]</cite>. For many axon and myelin segmentation methods, its encoder-decoder structure was a major design inspiration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib8" title="">8</a>]</cite>, and its original proposed architecture was also successfully applied to this task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib21" title="">21</a>]</cite>. Alternatively, transformers have gained a lot of traction in the deep learning community. Initially applied to language modelling, this efficient network architecture was quickly adapted for vision tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib9" title="">9</a>]</cite>. An outstanding application of transformers to image segmentation is the Segment-Anything-Model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib16" title="">16</a>]</cite>, a modular architecture that uses a Vision Transformer backbone. Intended to be prompted with points or bounding boxes, this model was trained on the largest annotated segmentation dataset ever released. In an effort to build a segmentation foundation model for the biomedical field, this framework was fine-tuned on various datasets (mostly CT and MRI) to create MedSAM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib19" title="">19</a>]</cite>. Despite its promising performance on microscopy images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib1" title="">1</a>]</cite>, SAM is not ideal for axon and myelin segmentation because it heavily relies on prompts, which need to be specified for every element to segment. Since our target images often contain large quantities of axons, automating the pipeline would require to generate accurate prompts which shifts the task from segmentation to object detection.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">All these conventional image processing and deep learning-based methods applied to axon and myelin segmentation share the same weakness: they were tailored for a specific image domain. As such, their performance is often impressive on the target dataset, but they perform poorly on out-of-distribution (OoD) data (i.e., different imaging modality or anatomical region). Moreover, they were often built for a specific research project, and become unmaintained a few years after the original publication. Thus, other researchers often cannot re-use existing models because these implementations are challenging to use without support from the original authors, or are not applicable to different image domains. As a result, a lot of redundant work is produced and little effort is made to make these methods easily accessible to researchers and durable in the medium- to long-term. There is a pressing need to make biomedical image segmentation models public and domain-agnostic, which is the main motivation behind this work.</p>
</div>
<section class="ltx_subsection" id="S1.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">1.1 </span>Contribution</h3>
<div class="ltx_para" id="S1.SS1.p1">
<p class="ltx_p" id="S1.SS1.p1.1">We contribute a publicly available multi-domain segmentation model for axon and myelin segmentation in neurological images, trained on diverse imaging modalities, resolutions, anatomical regions, species and pathologies. We show that given a collection of datasets from multiple domains, there is no performance advantage to train dedicated models on every dataset. Aggregating the data leads to equal or improved performance on all datasets. Additionally, we demonstrate that our multi-domain model is simpler to use than single-domain methods, and its monolithic nature makes it easier to maintain. The code and weights of our open-source model can be found in a GitHub release <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url" href="https://github.com/axondeepseg/model_seg_generalist/releases/tag/r20240224" title="">https://github.com/axondeepseg/model˙seg˙generalist/releases/tag/r20240224</a></span></span></span>. The model is also directly integrated into the AxonDeepSeg software, for a user-friendly experience with access to morphometrics extraction tools.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Methods</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Data</h3>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="180" id="S2.F1.g1" src="extracted/5861525/dataset_previews.png" width="449"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Dataset previews</figcaption>
</figure>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Dataset overview</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<table class="ltx_tabular ltx_centering ltx_figure_panel ltx_guessed_headers ltx_align_middle" id="S2.T1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S2.T1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.1.1.1.1">Dataset</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.2"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.2.1">TEM1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.3"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.3.1">TEM2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.4"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.4.1">SEM1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.5"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.5.1">SEM2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.6"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.6.1">SEM3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.7"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.7.1">CARS1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.8"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.8.1">BF1</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.9"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.9.1">BF2</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.10"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.10.1">BF3</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.1.1.11"><span class="ltx_text ltx_font_typewriter" id="S2.T1.1.1.1.11.1">BF4</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S2.T1.1.2.2.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.2.2.1.1">modality</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.2">TEM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.3">TEM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.4">SEM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.5">SEM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.6">SEM</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.7">CARS</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.8">BF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.9">BF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.10">BF</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.2.2.11">BF</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S2.T1.1.3.3.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.3.3.1.1">annotated</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.2"><span class="ltx_ERROR undefined" id="S2.T1.1.3.3.2.1">\faCheck</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.3">partially</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.4"><span class="ltx_ERROR undefined" id="S2.T1.1.3.3.4.1">\faCheck</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.5">partially</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T1.1.3.3.6"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.7"><span class="ltx_ERROR undefined" id="S2.T1.1.3.3.7.1">\faCheck</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.8"><span class="ltx_ERROR undefined" id="S2.T1.1.3.3.8.1">\faCheck</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.9"><span class="ltx_ERROR undefined" id="S2.T1.1.3.3.9.1">\faCheck</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.3.3.10"><span class="ltx_ERROR undefined" id="S2.T1.1.3.3.10.1">\faCheck</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T1.1.3.3.11"></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S2.T1.1.4.4.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.4.4.1.1">public</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.4.4.2"><span class="ltx_ERROR undefined" id="S2.T1.1.4.4.2.1">\faCheck</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.4.4.3"><span class="ltx_ERROR undefined" id="S2.T1.1.4.4.3.1">\faCheck</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.4.4.4"><span class="ltx_ERROR undefined" id="S2.T1.1.4.4.4.1">\faCheck</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T1.1.4.4.5"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.4.4.6"><span class="ltx_ERROR undefined" id="S2.T1.1.4.4.6.1">\faCheck</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.4.4.7"><span class="ltx_ERROR undefined" id="S2.T1.1.4.4.7.1">\faCheck</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.4.4.8"><span class="ltx_ERROR undefined" id="S2.T1.1.4.4.8.1">\faCheck</span></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T1.1.4.4.9"></td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S2.T1.1.4.4.10"></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.4.4.11"><span class="ltx_ERROR undefined" id="S2.T1.1.4.4.11.1">\faCheck</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S2.T1.1.5.5.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.5.5.1.1">species</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.2">mouse</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.3">macaque</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.4">rat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.5">human</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.6">dog</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.7">rat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.8">rat</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.9">rabbit</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.10">human</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.5.5.11">cat</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S2.T1.1.6.6.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.6.6.1.1">pathology*</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.2">H</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.3">H</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.4">H</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.5">H</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.6">H</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.7">H</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.8">H</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.9">MR</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.10">ND</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.6.6.11">H</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.7.7">
<th class="ltx_td ltx_th ltx_th_row ltx_border_l ltx_border_rr" id="S2.T1.1.7.7.1"></th>
<td class="ltx_td ltx_border_r" id="S2.T1.1.7.7.2"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.7.7.3"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.7.7.4"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.7.7.5"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.7.7.6"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.7.7.7"></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S2.T1.1.7.7.8">MR</td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.7.7.9"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.7.7.10"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.7.7.11"></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S2.T1.1.8.8.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.8.8.1.1">organ**</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.2">b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.3">b</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.4">sc</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.5">sc</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.6">sc</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.7">sc</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.8">pns</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.9">pns</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.10">b/pns/m</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.8.8.11">sc</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S2.T1.1.9.9.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.9.9.1.1">size</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.2">1360</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.3">98</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.4">14.8</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.5">31.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.6">592</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.7">2.6</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.8">280</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.9">12</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.10">20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.9.9.11">658</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr" id="S2.T1.1.10.10.1">(megapixel)</th>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.2"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.3"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.4"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.5"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.6"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.7"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.8"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.9"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.10"></td>
<td class="ltx_td ltx_border_r" id="S2.T1.1.10.10.11"></td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_rr ltx_border_t" id="S2.T1.1.11.11.1"><span class="ltx_text ltx_font_bold" id="S2.T1.1.11.11.1.1">pixel size</span></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.2">0.00236</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.3">0.009</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.4">0.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.5">0.13</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.6">0.26</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.7">0.225</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.8">0.1</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.9">0.211</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.10">0.226</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S2.T1.1.11.11.11">0.23</td>
</tr>
<tr class="ltx_tr" id="S2.T1.1.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_rr" id="S2.T1.1.12.12.1">(um/px)</th>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.2"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.3"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.4"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.5"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.6"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.7"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.8"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.9"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.10"></td>
<td class="ltx_td ltx_border_b ltx_border_r" id="S2.T1.1.12.12.11"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S2.T1.2"><span class="ltx_text ltx_font_bold" id="S2.T1.2.1">*</span> H: healthy, MR: myelin regeneration, ND: neurodegenerative diseases
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S2.T1.2.2">**</span> b: brain, sc: spinal cord, pns: peripheral nervous system, m: muscle</p>
</div>
</div>
</figure>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Datasets Used</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">The datasets used in this project cover the most popular microscopy modalities: transmission electron microscopy (TEM), scanning electron microscopy (SEM), bright-field optical microscopy (BF) and the less popular coherent anti-Stokes Raman spectroscopy (CARS). Although the main focus of this work is to produce a model that performs well across modalities, the imaging technique itself only accounts for some of the variability present in the data. Subject species or pathologies change the axon morphology. The axon density is not the same in the brain, in the spinal cord or in the peripheral nervous system. Different researchers have different hardware and experimental protocols, which creates variability with all other variables controlled, depending on the provenance of the data. For example, during sample preparation, tissues are sometimes damaged or slightly deformed, which leads to artifacts in the dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib25" title="">25</a>]</cite>. All these elements come into play to affect the visual aspect of the image, and our philosophy was to include as many of these factors as possible. The aggregated dataset spans different species (rat, mouse, human, rabbit), organs (brain, spinal cord, peripheral nerves, muscles), and were acquired using four imaging modalities as previously described. A wide range of pixel sizes are present, ranging from 2.36 nm/px to 0.26 um/px, as researchers use different magnifications based on their specific needs. This diversity is summarised in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.T1" title="Table 1 ‣ 2.1 Data ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_tag">1</span></a> and Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S2.F1" title="Figure 1 ‣ 2.1 Data ‣ 2 Methods ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_tag">1</span></a> shows visual examples. The datasets used for training were <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.1">TEM1</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.2">SEM1</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.3">CARS1</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.4">BF1</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.5">BF2</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.6">BF3</span>. Out-of-distribution evaluation was performed on datasets <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.7">TEM2</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.8">SEM2</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.9">SEM3</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS1.p1.1.10">BF4</span>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Annotations</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">Regardless of the image characteristics, the task we aim to perform is shared: segmenting the axon and the myelin. As such, the ground-truth labels for this supervised 2-class segmentation task consist of axon and myelin masks. Typically, preliminary segmentations are obtained using classical image processing or deep learning based methods. The predictions are then manually corrected by annotators with various degrees of medical expertise. Occasionally, due to limited resources, it is unrealistic to collect enough masks to effectively train a model. In such cases, to alleviate the annotator’s task, an active learning strategy is employed: the model is re-trained many times, and the masks are iteratively corrected by the annotator at every step, resulting in a progressively larger training set. This strategy has the advantage of requiring less annotations, because the masks chosen for correction are targeted towards mitigating the previous model checkpoint weaknesses. Many people from different medical backgrounds were involved in this process over the last decade. We would thus expect some level of inter-rater (and even intra-rater) variability in annotation quality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib17" title="">17</a>]</cite>. Although these variations are not characterised in this study because they are not deemed as problematic, our data aggregation strategy mitigates this bias. For example, a model trained on annotations with over-segmented myelin consistently reproduces this artifact in predictions, whereas a model trained on data coming from many different annotators will benefit from alternative interpretations of the data, assuming it is not overfitted.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Preprocessing and Data Aggregation Strategy</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">Minimal preprocessing was applied. Images were converted to grayscale when necessary, and their range was normalized to [0,1]. Every data aggregation described in this work is constructed identically. The testing set of all source datasets are combined into a large aggregated testing set. To ensure a representative validation set, we enforce the inclusion of samples from every source into the aggregated validation set. The aggregated test set is obtained by combining all source test sets.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.4 </span>Data availability</h4>
<div class="ltx_para" id="S2.SS1.SSS4.p1">
<p class="ltx_p" id="S2.SS1.SSS4.p1.1">Most of the data used in this project came from the publicly available <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS4.p1.1.1">White Matter Microscopy Database</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib5" title="">5</a>]</cite>, namely <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.2">TEM1</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.3">TEM2</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.4">SEM1</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.5">SEM2</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.6">SEM3</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.7">BF4</span>. <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.8">CARS1</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.9">BF1</span> respectively came from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib10" title="">10</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib7" title="">7</a>]</cite>, and are available upon request to the authors. <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.10">BF2</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS1.SSS4.p1.1.11">BF3</span> are not currently public, because the studies for which they were originally acquired are not yet published.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Models</h3>
<section class="ltx_subsubsection" id="S2.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.1 </span>Architecture and Training Details</h4>
<div class="ltx_para" id="S2.SS2.SSS1.p1">
<p class="ltx_p" id="S2.SS2.SSS1.p1.1">Two main criteria were considered to help decide the backbone for our experiments: an overall competitive performance and a durable implementation, to ensure support in the medium to long term. The latter is difficult to achieve, notably in the open-source community where project involvement and funding is often volatile. The nn-UNet framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib14" title="">14</a>]</cite> was selected for its consistency and popularity in the field. This project has been maintained for some years and was recently integrated into the MONAI project ecosystem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib3" title="">3</a>]</cite>. As such, it seemed like the most durable option. It leverages a typical encoder-decoder U-Net architecture, which is a well-known standard for biomedical image segmentation tasks. Other alternatives were considered, including transformer-based methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib19" title="">19</a>]</cite>, but preliminary results were not convincing and it was unclear if their implementation would still be actively maintained in the coming years. CNNs are still relevant for biomedical image segmentation because of their inherent inductive bias and they are less data-hungry than transformers <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib9" title="">9</a>]</cite>. Every model is trained based on a 5-fold cross-validation scheme for 1000 epochs. The generalist model is trained with a batch size of 13 and a patch size of 384x640. We discard the final model, which is often overfitted, and keep the checkpoint with the best validation score. All experiments were performed on a single 48 GB NVIDIA A6000 GPU.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.2.2 </span>Resolution-Ignorance</h4>
<div class="ltx_para" id="S2.SS2.SSS2.p1">
<p class="ltx_p" id="S2.SS2.SSS2.p1.1">An important design decision was to ignore the native resolution of input images. Typically, the input images fed to the network at train and test time are resampled to a common resolution, such that the model effectively works at a fixed resolution. When training on a single domain, this is not problematic because the resampling operation required to resize the train and test images is known. However, applying this model to an arbitrary image implies an appropriate resampling to the fixed internal resolution of the network. The end user needs to apply this transformation himself, or it can be done automatically based on the acquired image resolution and model target resolution. In both cases, this operation will either downsize the image, which causes information loss, or upsize it, which is computationally inefficient. Furthermore, for aggregation purposes, resampling is a liability because our data comes from a wide range of acquired resolution (spanning 2 orders of magnitude) and converting everything to the same resolution would inevitably cause catastrophic degradation in training label quality. Our proposed model is thus resolution-ignorant, as opposed to having a fixed resolution (see <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#bib.bib13" title="">13</a>]</cite>), but we claim its capacity is more than sufficient to efficiently generalize across scales.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Experiments</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Two types of models are compared: dedicated learners, exclusively trained on data from a specific domain, and generalist learners, trained on aggregated data. For both experiments, we select a collection of datasets, then train a dedicated model per dataset and a generalist model on the whole collection. A visual description of our experiments is included in the appendix (see Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#Pt0.Ax1.F4" title="Figure A ‣ Appendix ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_tag">A</span></a>).</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.1 </span>Intra-Modality Aggregation</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">To study the importance of intra-modality variability on model training, the intra-modality aggregation experiment uses 3 bright-field microscopy datasets (<span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS1.p1.1.1">BF1</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS1.p1.1.2">BF2</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS1.p1.1.3">BF3</span>). Despite a similar visual appearance and resolution, each dataset comes from a different species (rat, rabbit, human) and the data was acquired from multiple body parts (peripheral nervous system, brain, muscle). Additional variability comes from pathologies. Dedicated learners were trained on each dataset separately and a generalist model was trained on the concatenation of all three: <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS1.p1.1.4">BF_AGG</span>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.3.2 </span>Inter-Modality Aggregation</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">The second and most important experiment targets the impact of inter-modality variability on model performance. As such, we use datasets from 4 different modalities (<span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS2.p1.1.1">BF_AGG</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS2.p1.1.2">SEM1</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS2.p1.1.3">TEM1</span>, <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS2.p1.1.4">CARS1</span>). Note that in this context, the model trained on <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS2.p1.1.5">BF_AGG</span> is a dedicated learner, although it was considered the generalist learner of the intra-modality aggregation experiment. This task is more challenging, because the generalist model has to account for widely different image contrasts and resolutions in addition to the other factors of variability described in the previous experiment. Notably, myelin appears dark and axon light in BF/TEM images, whereas this pattern is inverted in SEM/CARS images. Moreover, the pixel sizes vary prominently, meaning that an axon with the same physical dimensions could appear to have a diameter of 10 pixels or 500 pixels depending on the magnification used. We expect the generalist model trained on the full aggregation <span class="ltx_text ltx_font_typewriter" id="S2.SS3.SSS2.p1.1.6">FULL_AGG</span> to learn an even more abstract representation of the structures of interest compared to dedicated single-modality models.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Results and Discussion</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">We report Dice scores for all experiments in heatmaps, where every row represents a target dataset and every column a model trained on the specified source dataset. All Dice values presented are obtained by ensembling the 5 folds of the cross-validation scheme. Results for both axon and myelin classes are presented. In 3.2, the generalist model is applied to unseen data.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Intra- and Inter-Modality Aggregation Results</h3>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="95" id="S3.F2.g1" src="extracted/5861525/intramodality_results_oneline.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Intra-Modality Aggregation Results: Performance of dedicated and generalist models on all BF datasets</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">As shown in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S3.F2" title="Figure 2 ‣ 3.1 Intra- and Inter-Modality Aggregation Results ‣ 3 Results and Discussion ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_tag">2</span></a>, the model trained on <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.1.1">BF_AGG</span> performs similarly to dedicated BF models. Dedicated learners generally work well across BF datasets, because these intra-modality image domains share a similar visual appearance. However, the generalist model consistently outperforms dedicated models on datasets they were not trained on.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="105" id="S3.F3.g1" src="extracted/5861525/intermodality_results_oneline.png" width="598"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Inter-Modality Aggregation Results: Performance of dedicated and generalist models on all imaging modalities.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">Expectedly, the heatmaps presented in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S3.F3" title="Figure 3 ‣ 3.1 Intra- and Inter-Modality Aggregation Results ‣ 3 Results and Discussion ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_tag">3</span></a> are sparse: dedicated models work poorly on image modalities they were not trained on. The only exception is the similar behavior of dedicated models trained on <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.1">CARS1</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p2.1.2">SEM1</span>, which makes sense given these two modalities are visually similar. The performance of the generalist and dedicated models for both classes are compared using a paired Student’s t-test on pairs of Dice score. For a fair comparison, we only include the performance of dedicated models on datasets they were trained on. The Dice scores of the generalist model are significantly greater than the ones of dedicated models (p=0.03077, N=8).</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Out-of-Distribution Generalization</h3>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Dice Scores on out-of-distribution data.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.1.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S3.T2.1.1.1.2">SEM2</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" colspan="2" id="S3.T2.1.1.1.3">TEM2</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2.2">
<th class="ltx_td ltx_th ltx_th_row ltx_border_r" id="S3.T2.1.2.2.1"></th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.2.2">Axon</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.2.3">Myelin</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.2.4">Axon</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.2.2.5">Myelin</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.3.3.1">Dedicated</th>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.3.2">0.824</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.3.3">0.774</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.3.4">0.640</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.1.3.3.5">0.604</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S3.T2.1.4.4.1">Generalist</th>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.4.4.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.4.2.1">0.834</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.4.4.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.4.3.1">0.783</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.4.4.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.4.4.1">0.697</span></td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r ltx_border_t" id="S3.T2.1.4.4.5"><span class="ltx_text ltx_font_bold" id="S3.T2.1.4.4.5.1">0.706</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#S3.T2" title="Table 2 ‣ 3.2 Out-of-Distribution Generalization ‣ 3 Results and Discussion ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_tag">2</span></a> compares dedicated models to the generalist model on OoD data. For <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.1">SEM2</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.2">TEM2</span>, we respectively used the SEM and TEM dedicated models. The generalist model trained on the full aggregation <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.3">FULL_AGG</span> outperforms both dedicated models on these datasets. Notably, the generalist model consistently detects more small axons, possibly due to its multi-resolution training set.
Our proposed model was also tested on unlabelled datasets <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.4">SEM3</span> and <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.5">BF4</span>. Examples of OoD predictions are included in the appendix for qualitative evaluation (see Figures <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#Pt0.Ax1.F7" title="Figure D ‣ Appendix ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_tag">D</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.11552v1#Pt0.Ax1.F8" title="Figure E ‣ Appendix ‣ Multi-Domain Data Aggregation for Axon and Myelin Segmentation in Histology Images"><span class="ltx_text ltx_ref_tag">E</span></a>).</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Conclusion</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our proposed generalist model produces better segmentations than single modality learners on in-distribution and out-of-distribution images. Our work shows that although intra-modality aggregation is useful, inter-modality data aggregation is the most beneficial. Our strategy is more sustainable than maintaining multiple dedicated systems, and leads to a single easy-to-use model. Models trained on aggregations <span class="ltx_text ltx_font_typewriter" id="S4.p1.1.1">BF_AGG</span> and <span class="ltx_text ltx_font_typewriter" id="S4.p1.1.2">FULL_AGG</span> are publicly available. We hope this project facilitates both the workflow of neuroscience researchers and the medium- to long-term maintenance of the method.</p>
</div>
<div class="ltx_para" id="S4.p2">
<span class="ltx_ERROR undefined" id="S4.p2.1">{credits}</span>
</div>
<section class="ltx_subsubsection" id="S4.SS0.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.1 </span>Acknowledgements</h4>
<div class="ltx_para" id="S4.SS0.SSS1.p1">
<p class="ltx_p" id="S4.SS0.SSS1.p1.1">We would like to thank Tanguy Duval and Daniel Côté for the CARS images, Simeon Christian Daeschler, Marie-Hélène Bourget, Tessa Gordon and Gregory Howard Borschel for the <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS1.p1.1.1">BF1</span> dataset, Charles R. Reiter and Geetanjanli Bendale for the <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS1.p1.1.2">BF2</span> dataset, and Osvaldo Delbono for the <span class="ltx_text ltx_font_typewriter" id="S4.SS0.SSS1.p1.1.3">BF3</span> dataset.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S4.SS0.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">4.0.2 </span><span class="ltx_ERROR undefined" id="S4.SS0.SSS2.1.1">\discintname</span>
</h4>
<div class="ltx_para" id="S4.SS0.SSS2.p1">
<p class="ltx_p" id="S4.SS0.SSS2.p1.1">The authors have no competing interests to declare that are relevant to the content of this article.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Archit, A., et al.: Segment Anything for Microscopy (Aug 2023).
https://doi.org/10.1101/2023.08.21.554208

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Bégin, S., Dupont-Therrien, O., Bélanger, E., Daradich, A., Laffray, S.,
De Koninck, Y., Côté, D.C.: Automated method for the segmentation and
morphometry of nerve fibers in large-scale CARS images of spinal cord
tissue. Biomedical Optics Express <span class="ltx_text ltx_font_bold" id="bib.bib2.1.1">5</span>(12), (Dec 2014).
https://doi.org/10.1364/BOE.5.004145

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Cardoso, M.J., Li, W., Ourselin, S., Feng, A., et al.: MONAI: An
open-source framework for deep learning in healthcare (Nov 2022).
https://doi.org/10.48550/arXiv.2211.02701

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Cheng, A., Zhao, G., Wang, L., Zhang, R.: AxonCallosumEM Dataset: Axon
Semantic Segmentation of Whole Corpus Callosum cross section from
EM Images (Jul 2023). https://doi.org/10.48550/arXiv.2307.02464

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Cohen Adad, J., et al.: White Matter Microscopy Database (Sep 2016).
https://doi.org/10.17605/OSF.IO/YP4QG, publisher: OSF

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
Couedic, T.L., Caillon, R., Rossant, F., Joutel, A., Urien, H., Rajani, R.M.:
Deep-learning based segmentation of challenging myelin sheaths. In: 2020
Tenth International Conference on Image Processing Theory,
Tools and Applications (IPTA) pp. (Nov 2020).
https://doi.org/10.1109/IPTA50016.2020.9286715

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Daeschler, S.C., Bourget, M.H., et al.: Rapid, automated nerve histomorphometry
through open-source artificial intelligence. Scientific Reports <span class="ltx_text ltx_font_bold" id="bib.bib7.1.1">12</span>,
 5975 (Apr 2022). https://doi.org/10.1038/s41598-022-10066-6

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Deng, W., Hedberg-Buenz, A., et al.: AxonDeep: Automated Optic Nerve
Axon Segmentation in Mice With Deep Learning. Translational
Vision Science &amp; Technology <span class="ltx_text ltx_font_bold" id="bib.bib8.1.1">10</span>(14),  22 (Dec 2021).
https://doi.org/10.1167/tvst.10.14.22

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Houlsby,
N., et al.: An Image is Worth 16x16 Words: Transformers for Image
Recognition at Scale (Jun 2021). https://doi.org/10.48550/arXiv.2010.11929

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Duval, T., Gasecka, A., Pouliot, P., Côté, D., Stikov, N., Cohen-Adad, J.:
Validation of MRI microstructure measurements with Coherent
Anti-Stokes Raman Scattering (CARS) (May 2015)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Feigin, V.L., Vos, T., Nichols, E., Owolabi, M.O., Carroll, W.M., Dichgans, M.,
Deuschl, G., Parmar, P., Brainin, M., Murray, C.: The global burden of
neurological disorders: translating evidence into policy. The Lancet.
Neurology <span class="ltx_text ltx_font_bold" id="bib.bib11.1.1">19</span>(3), (Mar 2020). https://doi.org/10.1016/S1474-4422(19)30411-9

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Gros, C., Lemay, A., Cohen-Adad, J.: SoftSeg: Advantages of soft versus
binary training for image segmentation (Nov 2020).
https://doi.org/10.48550/arXiv.2011.09041

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Henschel, L., Kügler, D., Reuter, M.: FastSurferVINN: Building
resolution-independence into deep learning segmentation methods—A
solution for HighRes brain MRI. NeuroImage <span class="ltx_text ltx_font_bold" id="bib.bib13.1.1">251</span>, 118933 (May
2022). https://doi.org/10.1016/j.neuroimage.2022.118933

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Isensee, F., Jaeger, P.F., Kohl, S.A.A., Petersen, J., Maier-Hein, K.H.:
nnU-Net: a self-configuring method for deep learning-based biomedical
image segmentation. Nature Methods <span class="ltx_text ltx_font_bold" id="bib.bib14.1.1">18</span>(2), (Feb 2021).
https://doi.org/10.1038/s41592-020-01008-z

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Janjic, P., Petrovski, K., Dolgoski, B., Smiley, J., Zdravkovski, P.,
Pavlovski, G., Jakjovski, Z., Davceva, N., Poposka, V., Stankov, A.,
Rosoklija, G., Petrushevska, G., Kocarev, L., Dwork, A.J.:
Measurement-oriented deep-learning workflow for improved segmentation of
myelin and axons in high-resolution images of human cerebral white matter.
Journal of Neuroscience Methods <span class="ltx_text ltx_font_bold" id="bib.bib15.1.1">326</span>, 108373 (Oct 2019).
https://doi.org/10.1016/j.jneumeth.2019.108373

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Kirillov, A., Mintun, E., Ravi, N., Mao, H., et al.: Segment Anything (Apr
2023). https://doi.org/10.48550/arXiv.2304.02643

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
Lemay, A., Gros, C., Karthik, E.N., Cohen-Adad, J.: Label fusion and training
methods for reliable representation of inter-rater uncertainty (Jan 2023).
https://doi.org/10.48550/arXiv.2202.07550

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Litjens, G., et al.: A survey on deep learning in medical image analysis.
Medical Image Analysis <span class="ltx_text ltx_font_bold" id="bib.bib18.1.1">42</span>, (Dec 2017).
https://doi.org/10.1016/j.media.2017.07.005

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment Anything in
Medical Images (Jul 2023). https://doi.org/10.48550/arXiv.2304.12306

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Mesbah, R., McCane, B., Mills, S.: Deep convolutional encoder-decoder for
myelin and axon segmentation. In: 2016 International Conference on
Image and Vision Computing New Zealand (IVCNZ) pp. (Nov 2016).
https://doi.org/10.1109/IVCNZ.2016.7804455

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Moiseev, D., Hu, B., Li, J.: Morphometric analysis of peripheral myelinated
nerve fibers through deep learning. Journal of the Peripheral Nervous System
<span class="ltx_text ltx_font_bold" id="bib.bib21.1.1">24</span>(1), (2019). https://doi.org/10.1111/jns.12293

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
More, H.L., Chen, J., et al.: A semi-automated method for identifying and
measuring myelinated nerve fibers in scanning electron microscope images.
Journal of Neuroscience Methods <span class="ltx_text ltx_font_bold" id="bib.bib22.1.1">201</span>(1), (Sep 2011).
https://doi.org/10.1016/j.jneumeth.2011.07.026

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Richerson, S., Condurache, A.P., Lohmeyer, J., Schultz, K., Ganske, P.: An
Initial Approach to Segmentation and Analysis of Nerve Cells
using Ridge Detection. In: 2008 IEEE Southwest Symposium on
Image Analysis and Interpretation pp. (Mar 2008).
https://doi.org/10.1109/SSIAI.2008.4512298

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional Networks for
Biomedical Image Segmentation (May 2015).
https://doi.org/10.48550/arXiv.1505.04597

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Saliani, A., Perraud, B., Duval, T., Stikov, N., Rossignol, S., Cohen-Adad, J.:
Axon and Myelin Morphology in Animal and Human Spinal Cord.
Frontiers in Neuroanatomy <span class="ltx_text ltx_font_bold" id="bib.bib25.1.1">11</span> (2017)

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Schependom, J.V., D’haeseleer, M.: Advances in Neurodegenerative
Diseases. Journal of Clinical Medicine <span class="ltx_text ltx_font_bold" id="bib.bib26.1.1">12</span>(5) (Mar 2023).
https://doi.org/10.3390/jcm12051709

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Zaimi, A., Duval, T., Gasecka, A., Côté, D., Stikov, N., Cohen-Adad, J.:
AxonSeg: Open Source Software for Axon and Myelin Segmentation
and Morphometric Analysis. Frontiers in Neuroinformatics <span class="ltx_text ltx_font_bold" id="bib.bib27.1.1">10</span>,
 37 (2016). https://doi.org/10.3389/fninf.2016.00037

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Zaimi, A., Wabartha, M., Herman, V., Antonsanti, P.L., Perone, C.S.,
Cohen-Adad, J.: AxonDeepSeg: automatic axon and myelin segmentation from
microscopy data using convolutional neural networks. Scientific Reports
<span class="ltx_text ltx_font_bold" id="bib.bib28.1.1">8</span>(1),  3816 (Feb 2018). https://doi.org/10.1038/s41598-018-22181-4

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Zhao, X., Pan, Z., Wu, J., Zhou, G., Zeng, Y.: Automatic identification and
morphometry of optic nerve fibers in electron microscopy images. Computerized
Medical Imaging and Graphics: The Official Journal of the Computerized
Medical Imaging Society <span class="ltx_text ltx_font_bold" id="bib.bib29.1.1">34</span>(3), (Apr 2010).
https://doi.org/10.1016/j.compmedimag.2009.08.009

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="Pt0.Ax1">
<h2 class="ltx_title ltx_title_appendix">Appendix</h2>
<figure class="ltx_figure" id="Pt0.Ax1.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="186" id="Pt0.Ax1.F4.g1" src="extracted/5861525/FIGURE_AGG_EXPERIMENTS.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure A: </span>Visual description of experiments</figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.Ax1.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="212" id="Pt0.Ax1.F5.g1" src="extracted/5861525/detailed_intramodality.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure B: </span>Dataset-wise results of individual folds for <span class="ltx_text ltx_font_bold" id="Pt0.Ax1.F5.2.1">intra-modality</span>.</figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.Ax1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="208" id="Pt0.Ax1.F6.g1" src="extracted/5861525/detailed_intermodality.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure C: </span>Dataset-wise results of individual folds for <span class="ltx_text ltx_font_bold" id="Pt0.Ax1.F6.2.1">inter-modality</span>.</figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.Ax1.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="Pt0.Ax1.F7.g1" src="extracted/5861525/segmentation_comparison_in_distribution.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure D: </span>In-Distribution predictions. All dedicated models used for the third row were trained on the corresponding dataset specified for every column.</figcaption>
</figure>
<figure class="ltx_figure" id="Pt0.Ax1.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="449" id="Pt0.Ax1.F8.g1" src="extracted/5861525/segmentation_comparison_ood.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure E: </span>Out-of-Distribution predictions. The dedicated models used for the second row were respectively trained on <span class="ltx_text ltx_font_typewriter" id="Pt0.Ax1.F8.5.1">BF_AGG</span>, <span class="ltx_text ltx_font_typewriter" id="Pt0.Ax1.F8.6.2">TEM1</span>, <span class="ltx_text ltx_font_typewriter" id="Pt0.Ax1.F8.7.3">SEM1</span> and <span class="ltx_text ltx_font_typewriter" id="Pt0.Ax1.F8.8.4">SEM1</span>. Note the remarkable performance of the BF generalist model on its OoD input.</figcaption>
</figure>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Tue Sep 17 20:23:43 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
